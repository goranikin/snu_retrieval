[
  {
    "corpusid": 252715594,
    "title": "PHENAKI: VARIABLE LENGTH VIDEO GENERATION FROM OPEN DOMAIN TEXTUAL DESCRIPTIONS",
    "abstract": "We present Phenaki, a model capable of realistic video synthesis, given a sequence of textual prompts. Generating videos from text is particularly challenging due to the computational cost, limited quantities of high quality text-video data and variable length of videos. To address these issues, we introduce a new model for learning video representation which compresses the video to a small representation of discrete tokens. This tokenizer uses causal attention in time, which allows it to work with variable-length videos. To generate video tokens from text we are using a bidirectional masked transformer conditioned on pre-computed text tokens. The generated video tokens are subsequently de-tokenized to create the actual video. To address data issues, we demonstrate how joint training on a large corpus of image-text pairs as well as a smaller number of video-text examples can result in generalization beyond what is available in the video datasets. Compared to the previous video generation methods, Phenaki can generate arbitrary long videos conditioned on a sequence of prompts (i.e. time variable text or a story) in open domain. To the best of our knowledge, this is the first time a paper studies generating videos from time variable prompts. In addition, compared to the perframe baselines, the proposed video encoder-decoder computes fewer tokens per video but results in better spatio-temporal consistency. ‡ Equal contribution.",
    "citations": [
      6628106,
      174802916,
      238582653
    ],
    "full_paper": "PHENAKI: VARIABLE LENGTH VIDEO GENERATION FROM OPEN DOMAIN TEXTUAL DESCRIPTIONS\n\n\nRuben Villegas \nUniversity of Michigan\nUniversity College London\n\n\nGoogle Brain \nUniversity of Michigan\nUniversity College London\n\n\nMohammad Babaeizadeh \nUniversity of Michigan\nUniversity College London\n\n\nGoogle Brain \nUniversity of Michigan\nUniversity College London\n\n\nPieter-Jan Kindermans \nUniversity of Michigan\nUniversity College London\n\n\nGoogle Brain \nUniversity of Michigan\nUniversity College London\n\n\nHernan Moraldo hmoraldo@google.com \nUniversity of Michigan\nUniversity College London\n\n\nGoogle Brain \nUniversity of Michigan\nUniversity College London\n\n\nHan Zhang zhanghan@google.com \nUniversity of Michigan\nUniversity College London\n\n\nGoogle Brain \nUniversity of Michigan\nUniversity College London\n\n\nMohammad Taghi \nUniversity of Michigan\nUniversity College London\n\n\nSaffar Google Brain \nUniversity of Michigan\nUniversity College London\n\n\nSantiago Castro sacastro@umich.edu \nUniversity of Michigan\nUniversity College London\n\n\nJulius Kunze \nUniversity of Michigan\nUniversity College London\n\n\nDumitru Erhan dumitru@google.com \nUniversity of Michigan\nUniversity College London\n\n\nGoogle Brain \nUniversity of Michigan\nUniversity College London\n\n\nPHENAKI: VARIABLE LENGTH VIDEO GENERATION FROM OPEN DOMAIN TEXTUAL DESCRIPTIONS\n\nWe present Phenaki, a model capable of realistic video synthesis, given a sequence of textual prompts. Generating videos from text is particularly challenging due to the computational cost, limited quantities of high quality text-video data and variable length of videos. To address these issues, we introduce a new model for learning video representation which compresses the video to a small representation of discrete tokens. This tokenizer uses causal attention in time, which allows it to work with variable-length videos. To generate video tokens from text we are using a bidirectional masked transformer conditioned on pre-computed text tokens. The generated video tokens are subsequently de-tokenized to create the actual video. To address data issues, we demonstrate how joint training on a large corpus of image-text pairs as well as a smaller number of video-text examples can result in generalization beyond what is available in the video datasets. Compared to the previous video generation methods, Phenaki can generate arbitrary long videos conditioned on a sequence of prompts (i.e. time variable text or a story) in open domain. To the best of our knowledge, this is the first time a paper studies generating videos from time variable prompts. In addition, compared to the perframe baselines, the proposed video encoder-decoder computes fewer tokens per video but results in better spatio-temporal consistency. ‡ Equal contribution.\n\nINTRODUCTION\n\nIt is now possible to generate realistic high resolution images given a description [34,35,32,38,59], but generating high quality videos from text remains challenging. In essence, videos are just a sequence of images, but this does not mean that generating a long coherent video is easy. In practice, it is a significantly harder task because there is much less high quality data available and the computational requirements are much more severe [9]. For image generation, there are datasets with billions of image-text pairs (such as LAION-5B [41] and JFT4B [60]) while the text-video datasets are substantially smaller e.g. WebVid [4] with ∼10M videos, which is not enough given the higher complexity of open domain videos. As for computation, training current state-of-theart image generation models is already pushing the state-of-the-art computational capabilities [59], leaving little to no room for generating videos, particularly videos of variable length.\n\nTo make the matters worse, one can argue that a single short text prompt is not sufficient to provide a complete description of a video (except for short clips), and instead, a generated video must be conditioned on a sequence of prompts, or a story, which narrates what happens over time. Ideally, Figure 1. Time variable text (i.e. story) conditional video generation. The entire figure is one continuous video generated auto-regressively. We start by generating the video conditioned on the first prompt and then after a couple of frames we change the prompt to the next one. Each row contains a selected number of frames (from left to right in order) while the model was conditioned on that particular prompt. The model manages to preserve the temporal coherence of the video while adapting to the new prompt, usually taking the shortest path for the adaption (notice the morphing of the teddy bear to the panda). Please note that the generated video has complex visual features such as reflections, occlusions, interactions and scene transitions. Full video is available at phenaki.github.io. a video generation model must be able to generate videos of arbitrary length, all the while having the capability of conditioning the generated frames at time t on prompts at time t that can vary over time. Such capability can clearly distinguish the video from a \"moving image\" and open up the way to real-world creative applications in art, design and content creation. To the best our knowledge, story based conditional video generation has never been explored before and this is the first paper to take early steps towards that goal. A traditional deep learning approach of simply learning this task from data is not possible, since there is no story-based dataset to learn from. Instead, to achieve this we rely on a model that is designed specifically with this capability in mind.\n\nIn this paper, we introduce Phenaki, a text to video model trained on both text to video and text to image data that can:\n\n-Generate temporally coherent and diverse videos conditioned on open domain prompts even when the prompt is a new composition of concepts (Fig. 3). The videos can be long (minutes) even though the model is trained on 1.4 seconds videos (at 8 fps).\n\n-Generate videos conditioned on a story (i.e. a sequence of prompts), e.g. Fig. 1   The embeddings of images and video patches from raw frames x are processed by a spatial and then a causal transformer (auto-regressive in time) to generate video tokens z. Center: MaskGiT is trained to reconstruct masked tokens z predicted by a frozen C-ViViT encoder and conditioned on T5X tokens of a given prompt p 0 . Right: How Phenaki can generate arbitrary long videos by freezing the past token and generating the future tokens. The prompt can change over time to enable time-variable prompt (i.e. story) conditional generation. The subscripts represent time (i.e. frame number).\n\nTo enable these capabilities, we could not rely on current video encoders, because they either can only decode fixed size videos or they encode frames independently. Hence, we introduce C-ViViT , a novel encoder-decoder architecture that:\n\n-Exploits temporal redundancy in videos to improve reconstruction quality over a per frame model while compressing the number of video tokens by 40% or more.\n\n-Allows encoding and decoding of variable length videos given its causal structure.\n\n\nTHE PHENAKI MODEL\n\nInspired by the previous work in auto-regressive text to image [34,59,38] and text to video [54,53,18], Phenaki is designed with two main components (see Figure 2): an encoder-decoder model which compresses videos to discrete embeddings (i.e. tokens) and a transformer model to translate text embeddings to video tokens. To get the text embeddings, Phenaki uses a pre-trained language model, T5X [37]. We will discuss each one of these components in the following subsections.\n\n\nENCODER-DECODER VIDEO MODEL: C-VIVIT\n\nOne of the primary challenges for generating video from text, is to get a compressed representation of videos. Previous work on text to video either use per-frame image encoders [18,54,57] such as VQ-GAN [12] or fixed length video encoders [52] such as VideoVQVAE [49]. The former allows for generating videos of arbitrary length, however in practice, the videos have to be short because the encoder does not compress the videos in time and the tokens are highly redundant in consecutive frames. The latter is more efficient in the number of tokens but it does not allow to generate variable length videos. In Phenaki, our goal is to generate videos of variable length while keeping the number of video tokens to a minimum so they can be modeled with a transformer within current computational limitations. To do so, we introduce C-ViViT , a causal variation of ViViT [1] with additional architectural changes for video generation, which can compress the videos in temporal and spatial dimensions, while staying auto-regressive in time, This capability allows for generating videos of arbitrary length auto-regressively.\n\nEncoder architecture: As illustrated in Figure 2, we start with a video sequence of t x + 1 frames with a resolution of w x × h x and c x channels: x ∈ R (tx+1)×hx×wx×cx . This sequence will be compressed into a token representation of size (t z + 1) × w z × h z where the first w z × h z tokens represent the first frame independently from the rest of the video, and the remaining tokens represent spatio-temporal video tokens that auto-regressively depend on previous frames. To do so, we extract non-overlapping image patches of size w p × h p × c p from the first frame and video patches of size t p × w p × h p × c p from the rest of the video. We typically use all channels at once such that the number of patches equals the number of video tokens t z = tx tp , w z = wx wp and h z = hx hp . Each of these patches is flattened and linearly projected into a d z dimensional space. We combine the spatial dimensions to have a tensor of shape (t z +1)×w z * h z ×d z where the spatial and temporal dimensions are separated. Then multiple transformer layers are applied along the spatial dimensions with allto-all attention. This is followed by multiple transformer layers over the temporal dimension with causal attention such that each spatial token only observes spatial tokens from previous frames in an auto-regressive manner. The effect of this is that the first frame can be completely independently encoded. This opens up the possibility of text to image training to be embedded naturally into our video model. The second advantage is that we can condition the video generation process on a number of starting frames. The resulting patch embeddings z of shape t z × w z × h z × d z are then tokenized into learned codewords c z by vector quantization. The codebook learning will be discussed later together with the losses.\n\nDecoder architecture: The C-ViViT decoder is simply an upside down version of the encoder. First tokens are transformed into embeddings. This is followed by the temporal transformer, then the spatial transformer. After the output of the spatial transformer, we apply a single linear projection without activation to map the tokens back to pixel space.\n\n\nQuantization and Losses:\n\nTo learn a discrete latent space, we quantize our encoder outputs into the entries of a learned codebook via the vector quantization (VQ) objective in VQVAEs [45],\nL VQ = sg(z) − e 2 2 + β z − sg(e) 2 2 ,(1)\nwhere sg(x) ≡ x, and d dx sg(x) ≡ 0 is the stop-gradient operator, β is the commitment loss weight, and e is a codebook vector from codebook E. The index to the codebook vector closest to z is found by i = argmin j z − E j 2 2 . In addition to the VQ objective, we adopt the factorized and 2normalized codes from ViT-VQGAN [58] to improve codebook usage and reconstruction quality.\n\nTo train our model, we use a combination of L 2 loss, image perceptual loss L IP [20,61], video perceptual loss L VP by using the I3D network [6] as feature extractor, and adversarial loss L Adv with StyleGAN architecture [21]. As training objective, we use the following\nL = L VQ + 0.1 × L Adv + 0.1 × L IP + 1.0 × L VP + 1.0 × L 2 .(2)\nNovelty over the ViViT architecture: While our proposed C-ViViT architecture is inspired by the factorized encoder in ViViT [1], we modify their architecture to enable self-supervised learning from unlabeled videos. We first remove the [CLS] tokens in the spatial and the temporal transformers. Next, we apply temporal transformer for all spatial tokens computed by the spatial encoder, in contrast to single run of the temporal transformer over the [CLS] tokens in ViViT. Most importantly, the ViViT encoder requires a fixed length video input due to the all-to-all attention in time. Therefore, we apply causal attention instead such that our C-ViViT encoder becomes autoregressive and allows for a variable number of input frames which are necessary to learn from image datasets, and auto-regressively extrapolate video or single frames into the future.\n\n\nTEXT-TO-VIDEO GENERATION WITH BIDIRECTIONAL TRANSFORMERS\n\nIn this stage, the text-to-video task can be formulated as a sequence-to-sequence problem to predict video tokens given the paired text embeddings. Most of recent methods [34,59,54,18] adopt a transformer model for these sequence-to-sequence tasks. In their models, they use an auto-regressive transformer which predicts the image or video tokens sequentially given the encoded text features. As a result, the sampling time scales linearly with the sequence length, even when caching is used. This becomes impractical for long video sequence generation.\n\n\nMasked bidirectional transformer:\n\nIn this work, we aim to reduce the sampling time by having a small and fixed sampling step disregarding different video sequence lengths. Inspired by previous work for image generation [8], we use a bidirectional transformer since it can predict different video tokens simultaneously. For training step i, we first sample a mask ratio γ i from 0 to 1 and randomly replace γ i · N tokens with the special token [MASK], where N is the video sequence length. Then we learn the model parameters by minimizing the cross entropy loss on those masked tokens given the encoded text embeddings and unmasked video tokens. During inference, we first label all of the video tokens as the special token [MASK]. Then, at each inference step, we predict all the masked (unknown) video tokens in parallel conditioned on the text embeddings and unmasked (predicted) video tokens. We keep a ratio β i of the predicted tokens at sampling step i and the remaining tokens are re-masked and re-predicted in the next step.\n\nAs discussed in MaskGIT [8], the masking schedule γ i and sampling schedule β i have a significant effect on the samples quality therefore we follow the same strategies. Compared to an autoregressive transformer, the number of sampling steps is an order-of-magnitude smaller (typically we use values in the range of 12 to 48). Generally speaking, more sampling steps improves the quality.\n\nLosses and training strategies: Given a pre-trained C-ViViT , videos are encoded into codebook ids a of shape (t z + 1) × w z × h z which are flattened into a long vector using the raster ordering from [58]. We then model the text-conditional video token distribution using Masked Visual Token Modeling (MVTM) [8]:\nL mask = − ∀i∈[1,N ],mi=1 log p(a i |aM , p),(3)\nwhere aM represents the masked version of a, m i is a binary variable indicating whether a i is masked or not, N is the number of video tokens, and p is the text condition embedding. In addition to the MVTM objective, we train using classifier-free guidance by dropping the text condition 10% of the time during training [16,59] . Finally, we dynamically adjust the MVTM objective during training to allow the use of image and video datasets as a single large dataset. We achieve this by only applying the masking ratio and objective on the first w z × h z tokens if only a single frame is given or over all video tokens if a full video is given. This mixed image and video dataset training strategy allows our models to learn concepts only present in image datasets, and transfer them to concepts present video datasets (e.g., the pencil drawing styled video of the panda in Figure.3).\n\nInference and auto-regressive generation of long videos: At inference time, we sample videos tokens by the same iterative process used in [8] with classifier-free guidance scale λ to control alignment between the generation and the text condition. Once the first video is generated, we can extrapolate additional frames auto-regressively by encoding the last K generated frames in the last video using C-ViViT , initializing MaskGIT with the tokens computed by our C-ViViT encoder, and proceed to generate the remaining video tokens conditioned on a text input. During video extrapolation, the text condition can be the same or a different one which enables our model to dynamically create visual transitions between the previous and current text condition visual content, effective generating a visual story an described by the input text.\n\n\nEXPERIMENTS\n\nTo evaluate Phenaki, we test it on the following tasks: 1) text conditional video generation, 2) textimage conditional video generation, 3) time variable text conditional video generation (i.e.) story mode, 4) video quantization and 5) image conditional video generation a.k.a. video prediction.\n\nTo the best of our knowledge, 3) time variable text conditional video generation has not been explored in prior work. Given the dynamic nature of videos, we highly encourage readers to visit phenaki.github.io to check the generated videos. The website also includes qualitative comparisons to a subset of the prompts from the CogVideo paper [18]. While the focus is on the text to video generation tasks, it is remarkable that Phenaki is still competitive on the more traditional video tasks despite not being developed explicitly for these tasks. We implemented Phenaki in JAX [? ] using FLAX [? ] library. \n\n\nTEXT CONDITIONAL VIDEO GENERATION\n\nCurrently there is no established benchmark for evaluating text to video methods. This makes comparing Phenaki to recent methods such as NUWA [54], CogVideo [18], NUWA-Infinity [53] and video diffusion models [17] difficult.\n\nUnless specified otherwise, we train a 1.8B parameter Phenaki model on a corpus of ∼15M textvideo pairs at 8 FPS mixed with ∼50M text-images plus ∼400M pairs of LAION-400M [41] (more details in Appendix B.3). The model used in the visualisations in this paper was trained for 1 million steps at a batch size of 512, which took less than 5 days. In this setup 80% of the training data came from the video dataset and each image dataset contributed 10%.\n\nQualitative evaluation: Samples from this model can be seen in Figure 3 and additional samples are provided at phenaki.github.io. We observe that there is a high degree of control over both the actors and the background dynamics in the videos. The appearance of the actors and the video style can be adjusted by the text prompt as well (e.g. a regular video, a cartoon or a pencil drawing).\n\nOn phenaki.github.io we provide examples from prompts that were provided in the CogVideo [18] demo. Since there are substantial differences between these methods it is hard to compare them on an equal footing. As an example, there are massive differences in scale: 9B parameters for CogVideo and 1.8B for our model. Additionally, the training data is different. Finally, we do not know how representative the prompts in the CogVideo demo are for the general performance of the CogVideo.\n\nQuantative comparison: The NUWA [54] paper provided a qualitative evaluation on Kinetics-400. Since the NUWA model is only 0.9B parameters we also use a model of the same size. Our model was trained on 50% video and 50% image data in this experiment. The NUWA model finetuned on Kinetics but the Phenaki model is not: it is evaluated in a zero shot setting. The results in Table 1 show that Phenaki achieves comparable generation quality, in a zero-shot setting, compared to previous text to video methods that were actually trained or finetuned on this dataset.\n\nOn the importance of joint text-to-image and text-to-video training While there are some textvideo datasets, text-image datasets dominate the internet in terms of quality and quantity [30]. Consequently, there is simply not enough video data available to cover all the concepts present in textimage datasets. For example using only our video data, concepts such as pencil drawings or different painting styles cannot be learned. To be able to learn a model that can combine video dynamics with these additional concepts we have to combine training on image and video data. In Table 2, we evaluate the performance of using different ratios of video and images. We start with data splits of only video, and vary the ratio of image and video datasets up to using 50% image and 50% video datasets. In our results, we find that there is a trade-off in performance between models trained with only video video (i.e., significantly better FVD), and models trained with more image data (i.e., better text-video and text-image alignment, and significantly better FID in image datasets). On phenaki.github.io we show samples from different models side by side where this trade-off between control over the content and the quality of the dynamics can be seen. We believe that the tradeoff between concepts and dynamics will be improved as the quality and size of text-video datasets increases in the future.\n\n\nTEXT-IMAGE CONDITIONAL VIDEO GENERATION\n\nGiven that Phenaki can be conditioned on both still images and text, an interesting setup is to animate existing images given a text prompt. For this experiment, we use the same model from Section 3.1 but conditioned on unseen pictures (captured with our phones from local subjects) and a related prompt. As it can be seen in Figure 4 the model can generate coherent videos starting from the given images, while following the given prompts. \n\n\nVISUAL STORY TELLING BY DYNAMIC TEXT INPUTS\n\nA notable and useful feature of Phenaki is that it is auto-regressive in time. This allows for generating long videos, while the prompt changes over time. Time variable prompts can be thought of as a story; a narration of the entire video where each prompt corresponds to a scene from the video. This allows for creating dynamically changing scenes. To the best our knowledge, this paper is the first work to generate such videos. An example of this can be seen in Fig. 1 and on phenaki.github.io. The way it works is that we generate a video with the first prompt and then extend it in time by conditioning a possibly new prompt and on the last N , typically 5, previously generated frames.\n\n\nVIDEO ENCODING\n\nTo evaluate the video encoding and reconstruction performance of C-ViViT , we use the Momentsin-Time (MiT) [29] dataset. MiT contains ∼802K training, ∼33K validation and ∼67K test videos at 25 FPS. The MiT dataset, in contrast to other publicly available video datasets, is a high quality balanced dataset with high coverage and density of verbs depicting moments of a few seconds [29]. We compare C-ViViT against per-frame image based encoder-decoders that have been used as video quantizers for conditional video generation [57,54,18,54,18,52]: a ViT [58] and a convolutional VQ-GAN [12]. The experimental details can be found in the Appendix B.1.  As demonstrated in Table 3, we evaluate the video reconstruction quality using FID [15] and FVD [44]. Both FID and FVD compare the distribution of generated videos (or images) to the ground truth distribution. The FID ignores temporal coherency, while the FVD measures how well the spatio-temporal dynamics of the videos are reconstructed. Results in Table 3 show that perframe image based methods slightly outperform our video method (indicated by marginally higher FID of C-ViViT ), however, they do poorly at modeling the spatio-temporal dynamics in video (significantly lower FVD of C-ViViT ). This is expected as C-ViViT has spatio-temporal connections between patches in each frame, allowing space and time to be modeled together. In addition, C-ViViT compresses the video into fewer tokens per video compared to the image based baselines. This is crucial as the number of tokens drastically impacts the computational cost of the transformer in downstream tasks. Furthermore, C-ViViT tokens are auto-regressive in time which enables variable length videos to be modeled with the same encoder which is important for video extrapolation conditioned on previously generated frames.\n\n\nIMAGE CONDITIONAL VIDEO GENERATION A.K.A VIDEO PREDICTION\n\nTo evaluate the learnt video representation of C-ViViT beyond reconstruction, we test it on the task of frame-conditioned video generation, also commonly known as video prediction [3]. In this experiment, we test Phenaki on BAIR Robot Pushing benchmark [11] where the task is to generate 15 frames conditioned on a given single frame. For open domain videos, we test Phenaki on Kinetics-600 [7] where the task is to predict 11 frames given 5 frames. More details about these experiments can be found in Appendix B.2. Tables 4 and 5 show the results of these experiments. Note that Table 4. Video prediction on Kinetics-600 [7]. While\n\nPhenaki is not designed for video prediction it achieves comparable results with SOTA video prediction models.\n\nMethod FVD ↓ Video Transformer [51] 170.0 ± 5.00 CogVideo [18] 109.2 DVD-GAN-FP [9] 69.1 ± 0.78 Video VQ-VAE [49] 64.3 ± 2.04 CCVS [28] 55.0 ± 1.00 TrIVD-GAN-FP [27] 25.7 ± 0.66 Transframer [31] 25.4 RaMViD [19] 16.5 Video Diffusion [17] 16.2 ± 0.34 Phenaki (Ours) 36.4 ± 0.19 Table 5. Video prediction on BAIR [11].\n\nMethod FVD ↓ DVD-GAN [9] 109.8 VideoGPT [55] 103.3 TrIVD-GAN [27] 103.3 Transframer [31] 100.0 HARP [57] 99.3 CCVS [28] 99.0 Video Transformer [51] 94.0 FitVid [3] 93.6 MCVD [47] 89.5 NUWA [54] 86.9 RaMViD [19] 84.2 Phenaki (Ours) 97.0\n\nPhenaki is not specifically designed for video prediction, therefore, it lacks components such as skip connections in U-Nets which are known to improve the performance for video prediction methods [10,46,3]. Nevertheless, our method is competitive on these benchmarks with SOTA video prediction methods. Overall, these experiments show that Phenaki is strong at modeling dynamics of the videos which is required for generating coherent videos from text.\n\n\nRELATED WORKS\n\nThis paper is closely related to auto-regressive methods for text conditioned image and video generation. DALL-E [34] translates text tokens to discrete image embeddings learnt using a VQVAE [45]. Parti [59] has a similar architecture but can generate higher quality images by predicting tokens from a ViT-VQGAN [58] using a 21B parameters transformer. Similar architectures have been used for generating videos as well. GODIVA [52] uses a transformer to map text tokens to video tokens from a image based VQVAE. Given the large number of tokens from multiple frames, GODIVA relied on a local-attention mechanism. Similarly, NUWA [54] and NUWA-Infinity [53] both employ auto-regressive architectures to generate videos and images from text. NUWA generates fixed size outputs, while NUWA-Infinity introduces a second layer of auto-regressive computation to support variable size videos. Likewise, CogVideo [18] argues the main reason behind low quality video generation is the scarcity of good text-video data and tried to leverage pre-trained text to images models to generate high quality video.\n\nWhile Phenaki sticks to the same architecture principles, it has major differences with previous work. Most notably, NUWA, NUWA-Infinity and CogVideo treat videos as a sequence of independent images. This can lead to poor modeling of dynamics and generate motion artifacts. To combat this, NUWA-infinity used the previous frame during decoding to combat this. In Phenaki, we go further and treat videos as a temporal sequence of images which substantially decreases the number of video tokens given the redundancy in video generation, and results in a much lower training cost. The auto-regressive nature of the Phenaki also allows us to effectively condition on previous frames and generates longer videos as detailed in Section 2.\n\nDiffusion models are another class of models which recently have been used for conditional and unconditional video generation, which we call VDM [17]. In VDM, authors proposed replacing the conventional U-Net architectures for 2D image modeling with a 3D space-time model to run the diffusion process directly on pixels. While this approach provides an effective formulation for modeling videos, it is limited to fixed size videos. To address this issue, VDM provides an autoregressive extension, which allows the model to generate longer videos but it is typically impractical due to high sampling time of diffusion models.\n\nText conditional video generation is a relatively new field of research, nonetheless, image conditional video generation, commonly known as video prediction, and unconditional video generation have been studied more comprehensively. These papers include deterministic methods using a combination of recurrent and convolutional networks [36,42,13,50], variational based stochastic methods [2,10,46,3] and more recently by learning a discrete representation [49,33,31], auto-regressive models [51,55,28,57], diffusion models [47,14,56,19] flow based models [24], and finally adversarial based methods [48,39,43,9,40,27]. These works mostly consider limited domain (e.g. robotic videos) prediction/generation, or short fixed size clips. Section 3 provides comparison with some of these models.\n\n\nCONCLUSION\n\nWe introduced Phenaki, a model which is capable of generating variable length videos conditioned on a sequence of open domain text prompts. Phenaki uses C-ViViT as video encoder. C-ViViT is a new model which provides temporal-spatial compression while being auto-regressive in time. The C-ViViT model is a crucial part of Phenaki that allows it to generate variable length videos. We demonstrate how joint training on images and videos can improve the generation quality, and diversity, given the existence of much larger image-text dataset with order of magnitude more samples. The Phenaki model achieves good performance on video prediction, it can be used as to generate long videos conditioned on a text prompt. Additionally it is able to condition on both text and a starting frame. Finally, Phenaki is not limited to generating a video depicting a single concept or caption. It is actually able to generate longer coherent video stories based on a sequence of text prompts. The more complex narratives it can visualize demonstrate how this can become a great creative tool for story telling.\n\n\nETHICS STATEMENT\n\nWhile we have not explored potential downstream applications of the generative models described in this work, we believe Phenaki can have a positive impact in a variety of creative settings. In general, many of the samples from the model will not perfectly correspond to the input caption or the user's intent; however, the end-user is likely to gain considerable time savings even if only one of the generated samples aligns with their intent. We thus foresee Phenaki being useful in eventually empowering users to accelerate their creativity, especially since the model can so quickly generate videos. Phenaki and similar models will be part of an ever-broad toolset for artists and non-artists alike, providing new and exciting ways to express creativity.\n\nThe flip-side of this acceleration and ease-of-use is the potential for harmful impact, as with many of the prior or concurrent work in generative modeling. An easy-to-use system like Phenaki can be repurposed for generating maliciously fake content and enable spreading of such content much easier. While the quality of the videos generated by Phenaki is not yet indistinguishable from real videos, getting to that bar for a specific set of samples is within the realm of possibility, even today. This can be particularly harmful if Phenaki is to be used to generate videos of someone without their consent and knowledge.\n\nLike DALLE-2 [35], Imagen [38], Parti [59] and others, Phenaki is trained on a collection of datasets that is known to encode a number of undesirable biases. LAION-400M [41] specifically has a variety of issues regarding violence, pornography, gore. While our primary image and video datasets have minimal traits like this, we did incorporate LAION-400M into our training and observed better results. In a currently training version of Phenaki, we use a set of datasets that minimizes such problems.\n\nTaken together, these issues contribute to our decision not to release the underlying models, code, data or interactive demo at this time. Before we can do that, we want to focus our efforts on better understanding of data, prompt and output filtering. We would also like to more explicitly measure the biases encoded in the outputs of Phenaki, so that we can further mitigate them actively, either in the data, models or pre/post-processing steps.\n\n\nACKNOWLEDGMENTS\n\nWe would like to thank Niki Parmar for initial discussions. Special thanks to Gabriel Bender and Thang Luong for reviewing the paper and providing constructive feedback. We appreciate the efforts of Kevin Murphy and David Fleet for advising the project and providing feedback throughout. We are grateful to Evan Rapoport, Douglas Eck and Zoubin Ghahramani for supporting this work in a variety of ways. The decoder architecture for all models is the same as the encoder but in reverse to put the latent embeddings back to image space. The VQ objective is trained with commitment loss of β = 0.25 and codebook size of 8192. The discriminator architecture is the StyleGAN [21] discriminator with blur resample, and channel multiplier of 1.\n\n\nB.1.2 TRAINING\n\nWe train all encoder-decoder baselines and with StyleGAN [21] discriminators with a batch size of 128 using Adam optimizer [23] with β 1 = 0.9 and β 2 = 0.99. We use a linear learning rate warmup to a peak value of 1 × 10 −4 over 100, 000 steps and then decaying over the remaining 900, 000 steps with a cosine schedule, and use a decoupled weight decay [26]  We use a similar setup as in Section B.1, but the video tokenization step is done over 4 × 4 spatial patches on the first image and 2 × 4 × 4 spatio-temporal patches in the rest of the video. The spatial encoder consists of 8 layers and the temporal encoder consists of 6 layers.\n\n\nB.2.2 KINETICS-600 C-VIVIT ARCHITECTURE\n\nWe use a similar setup as in Section B.2.1, but both the spatial encoder and temporal encoder consist of 8 layers.\n\n\nB.2.3 MASKGIT ARCHITECTURE\n\nTo perform video prediction in latent space in the BAIR Robot Push and Kinetics-600 datasets, we use an unconditional transformer architecture consisting of 24 layers, 768 hidden units, 16 attention heads, dropout and attention dropout rate of 0.1, 3072 mlp hidden units.\n\n\nB.2.4 TRAINING AND INFERENCE\n\nAs described in Table 7, we train C-ViViT with the same optimizer setup as in Sec B.1, but we do not downsample the FPS of any of the datasets in this section for fair comparison with the video prediction baselines. We train MaskGIT on the video tokens extracted using C-ViViT in an unconditional setting, that is, we do not assume frames or text inputs to be given. During training, we use the Adam [23] optimizer with β 1 = 0.9 and β 2 = 0.99. We use a linear learning rate warmup up to a peak value of 1 × 10 −4 over 10, 000 steps, and constant learning rate schedule for ∼2M steps. At inference time, we initialize MaskGIT given a number of input frames, and predict the rest of the frames depending on the dataset on which we evaluate.\n\n\nB.3 TEXT CONDITIONAL VIDEO GENERATION\n\n\nB.3.1 ARCHITECTURE\n\nIn our text conditional video generation, we use the same C-ViViT architecture and training described in Section B.1. To train MaskGIT, we include a text conditioning in the form of T5X embeddings [37] which are used as input through the use of cross attention with the video tokens. We reduce the number of parameters of our base model for fairness in the quantitative comparisons against NUWA. We use λ = 12, 48 MaskGIT iterations, and temperature of 8.0. \n\nFigure 2 .\n2The architecture of Phenaki. Left: C-ViViT encoder architecture.\n\nFigure 3 .\n3Text conditional video generation. Each row shows selected frames from a video generated given the prompt. The model is trained on a mix of images and videos. The video dataset does not include any stylized videos such as pencil drawings, however, the image dataset does. The model can generalize from still images to videos. This figure also demonstrate the capability of the model in generating new unseen compositions. Full videos are available at phenaki.github.io.\n\nFigure 4 .\n4Animating images conditioned on a prompt. Each row demonstrates multiple frames of a generated video conditioned on a given first frame as well as a given text prompt. The first frames are new (captured by author's phone) and not observed during the training. The model animates the given image while following the prompt. Full videos are available at phenaki.github.io.\n\nFigure 5 .\n5Another example of story conditional video generation. Full videos are available at phenaki.github.io.\n\n\nand Fig. 5.Empty \nTokens \n\nTokens \n\nPatch \nEmb \n\nPatch \nEmb \n\nPatch \nEmb \n\nSpatial \nTransformer \n\nSpatial \nTransformer \n\nSpatial \nTransformer \n\nCausal \nTransformer \n\nCausal \nTransformer \n\nCausal \nTransformer \n\n... \n\n... \n\n... \n\n... \n\nC-ViViT \nEncoder \nT5X \n\n... \n\nTransformer \n\nRandom Masking \n\n... \n\n... \n\nVideo \n\nTokens \n\nTokens \n\nMasked \n\nReconstructed \n\n... \n\nTransformer \n\n... \n\nShift Time \n\n... \n... \n\nTransformer \n\n... \n\nT5X \n\nT5X \n\n... \n\n\"Next Prompt\" \n\nTokens \n\nTokens \n\nPredicted \n\nFrozne Past \n\nPredicted \n\nFuture Tokens \n\nC-ViViT Encoder \nTraining Transformer \nVideo Generation \n\nToken \nMasked/Empty \nToken \nTransformer \nFrozen Model \nLinear \nEmbedding \nOperation \n\n\"1st Prompt\" \n\n\"Prompt\" \n\nDiscretize \nDiscretize \nDiscretize \n\n... \n\n\n\nTable 1 .\n1Text to video comparisons on Kinetics-400 [22].Table 2. Text to video and text to image results highlighting the importance of image datasets in video models. Text-to-image evaluation is done on ∼40K images of LAION-400M [41]. Data Split Text to Video Text to Image Vid% / Img% CLIP ↑ FID ↓ FVD ↓ CLIP ↑ FID ↓ 100% / 0% 0.298 19.2 168.9 0.240 53.9 80% / 20% 0.303 21.4 198.4 0.289 29.4 50% / 50% 0.302 21.4 239.7 0.287 30.5Method \nFID \nImage \n↓ \nFID \nVideo \n↓ \n\nT2V [25] \n82.13 14.65 \nSC [5] \n33.51 \n7.34 \nTFGAN [5] \n31.76 \n7.19 \nNUWA \n28.46 \n7.05 \nPhenaki [0-Shot] 37.74 \n3.84 \n\n\n\nTable 3 .\n3Video reconstruction results on Moments-in-Time. The number of tokens is computed for 10 frames with the exception of C-ViViT which is for 11, due to the isolated initial frame.Method \nFID ↓ FVD ↓ Number of Tokens ↓ \nConv VQ-GAN [12] \n7.5 \n306.1 \n2560 \nConv VQ-GAN + Video loss \n13.7 \n346.5 \n2560 \nViT VQ-GAN [58] \n3.4 \n166.6 \n2560 \nViT VQ-GAN + Video loss \n3.8 \n173.1 \n2560 \nC-ViViT VQ-GAN (Ours) \n4.5 \n65.78 \n1536 \n\n\n\n\nTim Salimans and Chitwan Saharia helped us with brainstorming and coming up with shared benchmarks. Jason Baldridge was instrumental for bouncing ideas. Alex Rizkowsky was very helpful in keeping things organized, while Erica Moreira and Victor Gomes ensured smooth resourcing for the project. Sarah Laszlo and Kathy Meier-Hellstern have greatly helped us incorporate important responsible AI practices into this project, which we are immensely grateful for. Finally, Blake Hechtman and Anselm Levskaya were generous in helping us debug a number of JAX issues.A HYPER-PARAMETERS Symbol Value Description t x , w x , h x , c x 11, 128, 128, 3 Video dimensions t p , w p , h p , c pTable 6. Hyperparamters used for C-ViViT architecture and optimizer.Table 7. Hyperparamters used for MaskGIT architecture and optimizer. B DETAILS OF EXPERIMENTS B.1 VIDEO QUANTIZATION B.1.1 NETWORK ARCHITECTURE All encoder-decoder baselines have approximately 50M parameters. The Convolutional baseline encoder architecture consists of 5 convolutional blocks with channel multipliers of [1, 1, 2, 2, 4], 2 residual layers and 128 hidden units per block, and embedding dimension of 256. The ViT baseline encoder architecture consists of an image patchification step over non-overlapping 8 × 8 spatial patches which are linearly transformed into image tokens. Next, we follow with 8 transformer layers with 512 hidden units, 8 attention heads, 2048 mlp units, and embedding dimension of 32. C-ViViT encoder architecture patches the first frame to non-overlapping 8 × 8 patches, and then the rest of the frames to non-overlapping 2 × 8 × 8 spatio-temporal patches which are linearly transformed into video embeddings. Next, C-ViViT encoder architecture consists of 4 spatial and 4 temporal transformer layers with 512 hidden units, 8 attention heads, 2048 mlp hidden units, and embedding dimension of 32.2, 8, 8, 3 \nPatches dimensions (all frames except the first one) \nt z , w z , h z \n6, 16, 16 \nVideo tokens dimension (before linear projection) \nh z \n512 \nHidden size in the transformer layer \nd z \n32 \nEmbedding dimension (after linear projection) \n− \n4 \nNumber of layers for spatial transformer \n− \n4 \nNumber of layers for temporal transformer \n− \n2048 \nMLP size \n|E| \n8192 \nCodebook size \n-\nAdamW \nOptimizer \nβ 1 \n0.9 \nfirst moment of gradient \nβ 2 \n0.99 \nsecond moment of gradient \n-\n1e-4 \nLearning rate \n-\n1e-4 \nWeight decay \n-\nCosine decay Learning rate scheduler \n-\n1M \nTarget number of training steps for learning rate scheduler \n-\n100K \nWarmup steps \n-\n10 \nGradient clipping magnitude \n-\n1028 \nBatch size \n\nSymbol \nValue \nDescription \n|z| \n1536 \nSequence Length \n-\n24 \nNumber of layer \n-\n2048 \nEmbedding dimension \n-\n8192 \nMLP dimension \n-\n32 \nNumber of heads \n-\nAdamW \nOptimizer \nβ 1 \n0.9 \nfirst moment of gradient \nβ 2 \n0.99 \nsecond moment of gradient \n-\n1e-4 \nLearning rate \n-\n1e-4 \nWeight decay \n-\nCosine decay Learning rate scheduler \n-\n4M \nTarget number of training steps for learning rate scheduler \n-\n10K \nWarmup steps \n-\n10 \nGradient clipping magnitude \n-\n512 \nBatch size \n\n\n\n\nof 1 × 10 −4 for the encoder-decoder and discriminator. To capture longer time horizons during training and better evaluate temporal coherence, we downsample the MiT dataset from 25 FPS to 6 FPS and evaluate on videos of 10 frames at spatial resolution of 128 × 128. B.2 IMAGE CONDITIONAL VIDEO GENERATION B.2.1 BAIR ROBOT PUSH C-VIVIT ARCHITECTURE\n\n\nThe MaskGIT architecture used against NUWA consists of 20 transformer layers with 1536 hidden units, 24 attention heads, and 6144 MLP hidden units, resulting in 0.9B parameters similar to NUWA. For the main experiments in this paper, we use a larger architecture that consists of consists of 24 transformer layers with 2048 hidden units, 32 attention heads, and 8192 mlp hidden units, resulting in 1.8B parameters.B.3.2 TRAINING AND INFERENCEFor all our text-conditional video generation, we use the training parametersTable 7. B.3.3 INFERENCE PARAMETERS AGAINST NUWA We use λ = 0.1, 12 MaskGIT iterations, and temperature of 4.0. B.3.4 INFERENCE PARAMETERS FOR ABLATION OF IMAGE AND VIDEO DATA FOR TRAINING. We use λ = 6, 24 MaskGIT iterations, and temperature of 4.0. B.3.5 INFERENCE PARAMETERS FOR ALL VIDEOS IN THE PAPER.\n\nVivit: A video vision transformer. Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, Cordelia Schmid, ICCV. 2021Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, and Cordelia Schmid. Vivit: A video vision transformer. In ICCV, 2021.\n\nStochastic variational video prediction. ICLR. Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, H Roy, Sergey Campbell, Levine, Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Roy H Campbell, and Sergey Levine. Stochastic variational video prediction. ICLR, 2018.\n\nMohammad Babaeizadeh, Mohammad Taghi Saffar, Suraj Nair, Sergey Levine, Chelsea Finn, Dumitru Erhan, Fitvid, arXiv:2106.13195Overfitting in pixel-level video prediction. arXiv preprintMohammad Babaeizadeh, Mohammad Taghi Saffar, Suraj Nair, Sergey Levine, Chelsea Finn, and Dumitru Erhan. Fitvid: Overfitting in pixel-level video prediction. arXiv preprint arXiv:2106.13195, 2020.\n\nFrozen in time: A joint video and image encoder for end-to-end retrieval. Max Bain, Arsha Nagrani, Gül Varol, Andrew Zisserman, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionMax Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1728-1738, 2021.\n\nConditional gan with discriminative filter generation for text-to-video synthesis. Yogesh Balaji, Bing Martin Renqiang Min, Rama Bai, Hans Peter Chellappa, Graf, IJCAI. Yogesh Balaji, Martin Renqiang Min, Bing Bai, Rama Chellappa, and Hans Peter Graf. Con- ditional gan with discriminative filter generation for text-to-video synthesis. In IJCAI, 2019.\n\nQuo vadis, action recognition? a new model and the kinetics dataset. Joao Carreira, Andrew Zisserman, CVPR. Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In CVPR, 2017.\n\nA short note about kinetics-600. Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, Andrew Zisserman, Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. A short note about kinetics-600, 2018.\n\nHuiwen Chang, Han Zhang, Lu Jiang, Ce Liu, William T Freeman, Maskgit, arXiv:2202.04200Masked generative image transformer. arXiv preprintHuiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. Maskgit: Masked generative image transformer. arXiv preprint arXiv:2202.04200, 2022.\n\nAdversarial video generation on complex datasets. Aidan Clark, Jeff Donahue, Karen Simonyan, arXiv:1907.06571arXiv preprintAidan Clark, Jeff Donahue, and Karen Simonyan. Adversarial video generation on complex datasets. arXiv preprint arXiv:1907.06571, 2019.\n\nStochastic video generation with a learned prior. Emily Denton, Rob Fergus, Proceedings of the 35th International Conference on Machine Learning. Jennifer Dy and Andreas Krausethe 35th International Conference on Machine Learning80Emily Denton and Rob Fergus. Stochastic video generation with a learned prior. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1174-1183, 2018.\n\nSelf-supervised visual planning with temporal skip connections. Frederik Ebert, Chelsea Finn, Alex X Lee, Sergey Levine, Frederik Ebert, Chelsea Finn, Alex X. Lee, and Sergey Levine. Self-supervised visual planning with temporal skip connections, 2017.\n\nTaming transformers for high-resolution image synthesis. Patrick Esser, Robin Rombach, Björn Ommer, Patrick Esser, Robin Rombach, and Björn Ommer. Taming transformers for high-resolution image synthesis, 2020.\n\nUnsupervised learning for physical interaction through video prediction. Chelsea Finn, Ian Goodfellow, Sergey Levine, Advances in neural information processing systems. Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical inter- action through video prediction. In Advances in neural information processing systems, pages 64-72, 2016.\n\nFlexible diffusion modeling of long videos. William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, Frank Wood, arXiv:2205.11495arXiv preprintWilliam Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, and Frank Wood. Flexible diffusion modeling of long videos. arXiv preprint arXiv:2205.11495, 2022.\n\nGans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter, 30Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.\n\nClassifier-free diffusion guidance. Jonathan Ho, Tim Salimans, Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance, 2021.\n\n. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, David J Fleet, arXiv:2204.03458Video diffusion models. arXiv preprintJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. arXiv preprint arXiv:2204.03458, 2022.\n\nCogvideo: Large-scale pretraining for text-to-video generation via transformers. Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Jie Tang, arXiv:2205.15868arXiv preprintWenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022.\n\nDiffusion models for video prediction and infilling. Tobias Höppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen, Andrea Dittadi, arXiv:2206.07696arXiv preprintTobias Höppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen, and Andrea Dittadi. Diffusion models for video prediction and infilling. arXiv preprint arXiv:2206.07696, 2022.\n\nJustin Johnson, Alexandre Alahi, Li Fei-Fei, arXiv:1603.08155Perceptual losses for real-time style transfer and super-resolution. arXiv preprintJustin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. arXiv preprint arXiv:1603.08155, 2016.\n\nAnalyzing and improving the image quality of stylegan. Jtero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila, CVPR. JTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In CVPR, 2020.\n\nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, Andrew Zisserman, The kinetics human action video dataset. Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijaya- narasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and An- drew Zisserman. The kinetics human action video dataset, 2017.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, ICLR. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n\nManoj Kumar, Mohammad Babaeizadeh, Dumitru Erhan, Chelsea Finn, Sergey Levine, arXiv:1903.01434Laurent Dinh, and Durk Kingma. Videoflow: A flow-based generative model for video. arXiv preprintManoj Kumar, Mohammad Babaeizadeh, Dumitru Erhan, Chelsea Finn, Sergey Levine, Lau- rent Dinh, and Durk Kingma. Videoflow: A flow-based generative model for video. arXiv preprint arXiv:1903.01434, 2019.\n\nVideo generation from text. Yitong Li, Martin Min, Dinghan Shen, David Carlson, Lawrence Carin, AAAI. Yitong Li, Martin Min, Dinghan Shen, David Carlson, and Lawrence Carin. Video generation from text. In AAAI, 2018.\n\nDecoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, ICLR. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.\n\nAlbin Cassirer, and Karen Simonyan. Transformation-based adversarial video prediction on large-scale data. Pauline Luc, Aidan Clark, Sander Dieleman, arXiv:2003.04035Yotam DoronarXiv preprintDiego de Las CasasPauline Luc, Aidan Clark, Sander Dieleman, Diego de Las Casas, Yotam Doron, Albin Cas- sirer, and Karen Simonyan. Transformation-based adversarial video prediction on large-scale data. arXiv preprint arXiv:2003.04035, 2019.\n\nCCVS: Context-aware controllable video synthesis. Guillaume Le Moing, Jean Ponce, Cordelia Schmid, NeurIPS. 2021Guillaume Le Moing, Jean Ponce, and Cordelia Schmid. CCVS: Context-aware controllable video synthesis. In NeurIPS, 2021.\n\nMoments in time dataset: one million videos for event understanding. Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ramakrishnan, Sarah Adel Bargal, Tom Yan, Lisa Brown, Quanfu Fan, Dan Gutfruend, Carl Vondrick, IEEE Transactions on Pattern Analysis and Machine Intelligence. Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ramakrishnan, Sarah Adel Bargal, Tom Yan, Lisa Brown, Quanfu Fan, Dan Gutfruend, Carl Vondrick, et al. Moments in time dataset: one million videos for event understanding. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019.\n\nLearning audio-video modalities from image captions. Arsha Nagrani, Paul Hongsuck Seo, Bryan Andrew Seybold, Anja Hauth, Santiago Manen, Chen Sun, Cordelia Schmid, ECCV. 2022Arsha Nagrani, Paul Hongsuck Seo, Bryan Andrew Seybold, Anja Hauth, Santiago Manen, Chen Sun, and Cordelia Schmid. Learning audio-video modalities from image captions. In ECCV, 2022.\n\nTransframer: Arbitrary frame prediction with generative models. Charlie Nash, João Carreira, Jacob Walker, Iain Barr, Andrew Jaegle, Mateusz Malinowski, Peter Battaglia, arXiv:2203.09494arXiv preprintCharlie Nash, João Carreira, Jacob Walker, Iain Barr, Andrew Jaegle, Mateusz Malinowski, and Peter Battaglia. Transframer: Arbitrary frame prediction with generative models. arXiv preprint arXiv:2203.09494, 2019.\n\nGlide: Towards photorealistic image generation and editing with text-guided diffusion models. Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mc-Grew, Ilya Sutskever, Mark Chen, arXiv:2112.10741arXiv preprintAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mc- Grew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\n\n. Ruslan Rakhimov, Denis Volkhonskiy, Alexey Artemov, Denis Zorin, Evgeny Burnaev, arXiv:2006.10704Latent video transformer. arXiv preprintRuslan Rakhimov, Denis Volkhonskiy, Alexey Artemov, Denis Zorin, and Evgeny Burnaev. Latent video transformer. arXiv preprint arXiv:2006.10704, 2020.\n\nZero-shot text-to-image generation. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever, International Conference on Machine Learning. PMLRAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821-8831. PMLR, 2021.\n\nHierarchical text-conditional image generation with clip latents. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen, arXiv:2204.06125arXiv preprintAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n\nVideo (language) modeling: a baseline for generative models of natural videos. Marcaurelio Ranzato, Arthur Szlam, Joan Bruna, Michael Mathieu, Ronan Collobert, Sumit Chopra, arXiv:1412.6604arXiv preprintMarcAurelio Ranzato, Arthur Szlam, Joan Bruna, Michael Mathieu, Ronan Collobert, and Sumit Chopra. Video (language) modeling: a baseline for generative models of natural videos. arXiv preprint arXiv:1412.6604, 2014.\n\nAfroz Mohiuddin, et al. Scaling up models and data with t5x and seqio. Adam Roberts, Hyung Won, Anselm Chung, Gaurav Levskaya, James Mishra, Daniel Bradbury, Sharan Andor, Brian Narang, Colin Lester, Gaffney, arXiv:2203.17189arXiv preprintAdam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, et al. Scal- ing up models and data with t5x and seqio. arXiv preprint arXiv:2203.17189, 2022.\n\nPhotorealistic text-to-image diffusion models with deep language understanding. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, ; S Sara Mahdavi, Rapha Gontijo Lopes, arXiv:2205.11487Burcu Karagol Ayan. arXiv preprintSeyed Kamyar Seyed GhasemipourChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022.\n\nTemporal generative adversarial nets with singular value clipping. Masaki Saito, Eiichi Matsumoto, Shunta Saito, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionMasaki Saito, Eiichi Matsumoto, and Shunta Saito. Temporal generative adversarial nets with singular value clipping. In Proceedings of the IEEE international conference on computer vision, pages 2830-2839, 2017.\n\nTrain sparsely, generate densely: Memory-efficient unsupervised training of high-resolution temporal gan. Masaki Saito, Shunta Saito, Masanori Koyama, Sosuke Kobayashi, International Journal of Computer Vision. 12810Masaki Saito, Shunta Saito, Masanori Koyama, and Sosuke Kobayashi. Train sparsely, gener- ate densely: Memory-efficient unsupervised training of high-resolution temporal gan. Interna- tional Journal of Computer Vision, 128(10):2586-2606, 2020.\n\nLaion-400m: Open dataset of clip-filtered 400 million image-text pairs. Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, Aran Komatsuzaki, arXiv:2111.02114arXiv preprintChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\n\nUnsupervised learning of video representations using lstms. Nitish Srivastava, Elman Mansimov, Ruslan Salakhudinov, International Conference on Machine Learning. Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video representations using lstms. In International Conference on Machine Learning, 2015.\n\nMocogan: Decomposing motion and content for video generation. Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, Jan Kautz, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionSergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1526-1535, 2018.\n\nTowards accurate generative models of video: A new metric & challenges. Thomas Unterthiner, Karol Sjoerd Van Steenkiste, Raphael Kurach, Marinier, arXiv:1812.01717arXiv preprintMarcin Michalski, and Sylvain GellyThomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michal- ski, and Sylvain Gelly. Towards accurate generative models of video: A new metric & chal- lenges. arXiv preprint arXiv:1812.01717, 2018.\n\nNeural discrete representation learning. Aaron Van Den Oord, Oriol Vinyals, Koray Kavukcuoglu, NeurIPS. Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In NeurIPS, 2018.\n\nHigh fidelity video prediction with large stochastic recurrent neural networks. Ruben Villegas, Arkanath Pathak, Harini Kannan, Dumitru Erhan, V Quoc, Honglak Le, Lee, Advances in Neural Information Processing Systems. Ruben Villegas, Arkanath Pathak, Harini Kannan, Dumitru Erhan, Quoc V Le, and Honglak Lee. High fidelity video prediction with large stochastic recurrent neural networks. In Ad- vances in Neural Information Processing Systems, pages 81-91, 2019.\n\nMcvd: Masked conditional video diffusion for prediction, generation, and interpolation. Vikram Voleti, Alexia Jolicoeur-Martineau, Christopher Pal, arXiv:2205.09853arXiv preprintVikram Voleti, Alexia Jolicoeur-Martineau, and Christopher Pal. Mcvd: Masked conditional video diffusion for prediction, generation, and interpolation. arXiv preprint arXiv:2205.09853, 2022.\n\nGenerating videos with scene dynamics. Carl Vondrick, Hamed Pirsiavash, Antonio Torralba, arXiv:1609.02612arXiv preprintCarl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dy- namics. arXiv preprint arXiv:1609.02612, 2016.\n\n. Jacob Walker, Ali Razavi, Aäron Van Den Oord, arXiv:2103.01950Predicting video with vqvae. arXiv preprintJacob Walker, Ali Razavi, and Aäron van den Oord. Predicting video with vqvae. arXiv preprint arXiv:2103.01950, 2019.\n\nPredrnn: Recurrent neural networks for predictive learning using spatiotemporal lstms. Advances in neural information processing systems. Yunbo Wang, Mingsheng Long, Jianmin Wang, Zhifeng Gao, Philip S Yu, 30Yunbo Wang, Mingsheng Long, Jianmin Wang, Zhifeng Gao, and Philip S Yu. Predrnn: Re- current neural networks for predictive learning using spatiotemporal lstms. Advances in neural information processing systems, 30, 2017.\n\nScaling autoregressive video models. Dirk Weissenborn, Oscar Täckström, Jakob Uszkoreit, ICLR. Dirk Weissenborn, Oscar Täckström, and Jakob Uszkoreit. Scaling autoregressive video mod- els. In ICLR, 2020.\n\nChenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, Nan Duan Godiva, arXiv:2104.14806Generating open-domain videos from natural descriptions. arXiv preprintChenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Generating open-domain videos from natural descriptions. arXiv preprint arXiv:2104.14806, 2021.\n\nChenfei Wu, Jian Liang, Xiaowei Hu, Zhe Gan, Jianfeng Wang, Lijuan Wang, Zicheng Liu, arXiv:2207.09814Yuejian Fang, and Nan Duan. Nuwa-infinity: Autoregressive over autoregressive generation for infinite visual synthesis. arXiv preprintChenfei Wu, Jian Liang, Xiaowei Hu, Zhe Gan, Jianfeng Wang, Lijuan Wang, Zicheng Liu, Yuejian Fang, and Nan Duan. Nuwa-infinity: Autoregressive over autoregressive generation for infinite visual synthesis. arXiv preprint arXiv:2207.09814, 2022.\n\nNÜwa: Visual synthesis pre-training for neural visual world creation. Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, Nan Duan, ECCV. 2022Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. NÜwa: Visual synthesis pre-training for neural visual world creation. In ECCV, 2022.\n\nVideogpt: Video generation using vq-vae and transformers. Wilson Yan, Yunzhi Zhang, Pieter Abbeel, Aravind Srinivas, arXiv:2104.10157arXiv preprintWilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2019.\n\nDiffusion probabilistic modeling for video generation. Ruihan Yang, Prakhar Srivastava, Stephan Mandt, arXiv:2203.09481arXiv preprintRuihan Yang, Prakhar Srivastava, and Stephan Mandt. Diffusion probabilistic modeling for video generation. arXiv preprint arXiv:2203.09481, 2022.\n\nHarp: Autoregressive latent video prediction with high-fidelity image generator. Fangchen Liu Stephen James Pieter Abbeel Younggyo Seo, Kimin Lee, arXiv:2209.07143arXiv preprintFangchen Liu Stephen James Pieter Abbeel Younggyo Seo, Kimin Lee. Harp: Autoregressive latent video prediction with high-fidelity image generator. arXiv preprint arXiv:2209.07143, 2022.\n\nVector-quantized image modeling with improved vqgan. Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, Yonghui Wu, ICLR. 2022Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. In ICLR, 2022.\n\nScaling autoregressive models for content-rich text-to-image generation. Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Ben Burcu Karagol Ayan, Wei Hutchinson, Zarana Han, Xin Parekh, Han Li, Jason Zhang, Yonghui Baldridge, Wu, arXiv:2206.10789arXiv preprintJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Va- sudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022.\n\nScaling vision transformers. Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, Lucas Beyer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision trans- formers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog- nition, pages 12104-12113, 2022.\n\nThe unreasonable effectiveness of deep features as a perceptual metric. Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang, CVPRRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, , and Oliver Wang. The unrea- sonable effectiveness of deep features as a perceptual metric. CVPR, 2018."
  },
  {
    "corpusid": 13002849,
    "title": "MODE REGULARIZED GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution, during the early phases of training and thus providing a unified solution to the missing modes problem. * Authors contributed equally.",
    "citations": [],
    "full_paper": "MODE REGULARIZED GENERATIVE ADVERSARIAL NETWORKS\n\n\n† Tong \nMontreal Institute for Learning Algorithms\nUniversité de Montréal\nH3T 1J4MontréalQCCanada\n\nDepartment of Computing\nSchool of Computer Science\nThe Hong Kong Polytechnic University\nUniversity Of WaterlooN2L 3G1Hong Kong, WaterlooONCanada\n\nChe \nYanran Li \nMontreal Institute for Learning Algorithms\nUniversité de Montréal\nH3T 1J4MontréalQCCanada\n\nAthul Paul Jacob ap.jacob@umontreal.ca \nMontreal Institute for Learning Algorithms\nUniversité de Montréal\nH3T 1J4MontréalQCCanada\n\nYoshua Bengio yoshua.bengio@umontreal.ca \nDepartment of Computing\nSchool of Computer Science\nThe Hong Kong Polytechnic University\nUniversity Of WaterlooN2L 3G1Hong Kong, WaterlooONCanada\n\nWenjie Li \nDavid R Cheriton \nMODE REGULARIZED GENERATIVE ADVERSARIAL NETWORKS\nPublished as a conference paper at ICLR 2017\nAlthough Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution, during the early phases of training and thus providing a unified solution to the missing modes problem. * Authors contributed equally.\n\nINTRODUCTION\n\nGenerative adversarial networks (GAN) (Goodfellow et al., 2014) have demonstrated their potential on various tasks, such as image generation, image super-resolution, 3D object generation, and video prediction (Radford et al., 2015;Ledig et al., 2016;Sønderby et al., 2016;Nguyen et al., 2016;Wu et al., 2016;Mathieu et al., 2015). The objective is to train a parametrized function (the generator) which maps noise samples (e.g., uniform or Gaussian) to samples whose distribution is close to that of the data generating distribution. The basic scheme of the GAN training procedure is to train a discriminator which assigns higher probabilities to real data samples and lower probabilities to generated data samples, while simultaneously trying to move the generated samples towards the real data manifold using the gradient information provided by the discriminator. In a typical setting, the generator and the discriminator are represented by deep neural networks.\n\nDespite their success, GANs are generally considered as very hard to train due to training instability and sensitivity to hyper-parameters. On the other hand, a common failure pattern observed while training GANs is the collapsing of large volumes of probability mass onto a few modes. Namely, although the generators produce meaningful samples, these samples are often from just a few modes (small regions of high probability under the data distribution). Behind this phenomenon is the missing modes problem, which is widely conceived as a major problem for training GANs: many modes of the data generating distribution are not at all represented in the generated samples, yielding a much lower entropy distribution, with less variety than the data generating distribution. This issue has been the subject of several recent papers proposing several tricks and new architectures to stabilize GAN's training and encourage its samples' diversity. However, we argue that a general cause behind these problems is the lack of control on the discriminator during GAN training. We would like to encourage the manifold of the samples produced by the generator to move towards that of real data, using the discriminator as a metric. However, even if we train the discriminator to distinguish between these two manifolds, we have no control over the shape of the discriminator function in between these manifolds. In fact, the shape of the discriminator function in the data Published as a conference paper at ICLR 2017 space can be very non-linear with bad plateaus and wrong maxima and this can therefore hurt the training of GANs (Figure 1). To remedy this problem, we propose a novel regularizer for the GAN training target. The basic idea is simple yet powerful: in addition to the gradient information provided by the discriminator, we want the generator to take advantage of other similarity metrics with much more predictable behavior, such as the L 2 norm. Differentiating these similarity metrics will provide us with more stable gradients to train our generator. Combining this idea with an approach meant to penalize the missing modes, we propose a family of additional regularizers for the GAN objective. We then design a set of metrics to evaluate the generated samples in terms of both the diversity of modes and the distribution fairness of the probability mass. These metrics are shown to be more robust in judging complex generative models, including those which are well-trained and collapsed ones.\n\nRegularizers usually bring a trade-off between model variance and bias. Our results have shown that, when correctly applied, our regularizers can dramatically reduce model variance, stabilize the training, and fix the missing mode problem all at once, with positive or at the least no negative effects on the generated samples. We also discuss a variant of the regularized GAN algorithm, which can even improve sample quality as compared to the DCGAN baseline.\n\n\nRELATED WORK\n\nThe GAN approach was initially proposed by Goodfellow et al. (2014) where both the generator and the discriminator are defined by deep neural networks.\n\nIn Goodfellow et al. (2014), the GAN is able to generate interesting local structure but globally incoherent images on various datasets. Mirza & Osindero (2014) enlarges GAN's representation capacity by introducing an extra vector to allow the generator to produce samples conditioned on other beneficial information. Motivated from this, several conditional variants of GAN has been applied to a wide range of tasks, including image prediction from a normal map Wang & Gupta (2016), image synthesis from text Reed et al. (2016) and edge map Isola et al. (2016), real-time image manipulation , temporal image generation Zhou & Berg (2016); Saito & Matsumoto (2016); Vondrick et al. (2016), texture synthesis, style transfer, and video stylization Li & Wand (2016).\n\nResearchers also aim at stretching GAN's limit to generate higher-resolution, photo-realistic images. Denton et al. (2015) initially apply a Laplacian pyramid framework on GAN to generate images of high resolution. At each level of their LAPGAN, both the generator and the discriminator are convolutional networks. As an alternative to LAPGAN, Radford et al. (2015) successfully designs a class of deep convolutional generative adversarial networks which has led to significant improvements on unsupervised image representation learning. Another line of work aimed at improving GANs are through feature learning, including features from the latent space and image space. The motivation is that features from different spaces are complementary for generating perceptual and natural-looking images. With this perspective, some researchers use distances between learned features as losses for training objectives for generative models. Larsen et al. (2015) combine a variational autoencoder objective with a GAN and utilize the learned features from the discriminator in the GANs for better image similarity metrics. It is shown that the learned distance from the discriminator is of great help for the sample visual fidelity. Recent literature have also shown impressive results on image super-resolution to infer photo-realistic natural images for 4x upscaling factors Ledig et al. (2016);Sønderby et al. (2016); Nguyen et al. (2016).\n\nDespite these promising successes, GANs are notably hard to train. Although Radford et al. (2015) provide a class of empirical architectural choices that are critical to stabilize GAN's training, it would be even better to train GANs more robustly and systematically. Salimans et al. (2016) propose feature matching technique to stabilize GAN's training. The generator is required to match the statistics of intermediate features of the discriminator. Similar idea is adopted by Zhao et al. (2016).\n\nIn addition to feature distances, Dosovitskiy & Brox (2016) found that the counterpart loss in image space further improves GAN's training stability. Furthermore, some researchers make use of information in both spaces in a unified learning procedure (Dumoulin et al., 2016;Donahue et al., 2016). In Dumoulin et al. (2016), one trains not just a generator but also an encoder, and the discriminator is trained to distinguish between two joint distributions over image and latent spaces produced either by the application of the encoder on the training data or by the application of the generator (decoder) to the latent prior. This is in contrast with the regular GAN training, in which the discriminator only attempts to separate the distributions in the image space. Parallelly, Metz et al. (2016) stabilize GANs by unrolling the optimization of discriminator, which can be considered as an orthogonal work with ours.\n\nOur work is related to VAEGAN (Larsen et al., 2015) in terms of training an autoencoder or VAE jointly with the GAN model. However, the variational autoencoder (VAE) in VAEGAN is used to generate samples whereas our autoencoder based losses serves as a regularizer to penalize missing modes and thus improving GAN's training stability and sample qualities. We demonstrate detailed differences from various aspects in Appendix D.\n\n\nMODE REGULARIZERS FOR GANS\n\nThe GAN training procedure can be viewed as a non-cooperative two player game, in which the discriminator D tries to distinguish real and generated examples, while the generator G tries to fool the discriminator by pushing the generated samples towards the direction of higher discrimination values. Training the discriminator D can be viewed as training an evaluation metric on the sample space. Then the generator G has to take advantage of the local gradient ∇ log D(G) provided by the discriminator to improve itself, namely to move towards the data manifold.\n\nWe now take a closer look at the root cause of the instabilities while training GANs. The discriminator is trained on both generated and real examples. As pointed out by Goodfellow et al. (2014);Denton et al. (2015); Radford et al. (2015), when the data manifold and the generation manifold are disjoint (which is true in almost all practical situations), it is equivalent to training a characteristic function to be very close to 1 on the data manifold, and 0 on the generation manifold. In order to pass good gradient information to the generator, it is important that the trained discriminator produces stable and smooth gradients. However, since the discriminator objective does not directly depend on the behavior of the discriminator in other parts of the space, training can easily fail if the shape of the discriminator function is not as expected. As an example,Denton et al. (2015) noted a common failure pattern for training GANs which is the vanishing gradient problem, in which the discriminator D perfectly classifies real and fake examples, such that around the fake examples, D is nearly zero. In such cases, the generator will receive no gradient to improve itself. 1 Another important problem while training GANs is mode missing. In theory, if the generated data and the real data come from the same low dimensional manifold, the discriminator can help the generator distribute its probability mass, because the missing modes will not have near-0 probability under the generator and so the samples in these areas can be appropriately concentrated towards regions where D is closer to 1. However, in practice since the two manifolds are disjoint, D tends to be near 1 on all the real data samples, so large modes usually have a much higher chance of attracting the gradient of discriminator. For a typical GAN model, since all modes have similar D values, there is no reason why the generator cannot collapse to just a few major modes. In other words, since the discriminator's output is nearly 0 and 1 on fake and real data respectively, the generator is not penalized for missing modes.\n\n\nGEOMETRIC METRICS REGULARIZER\n\nCompared with the objective for the GAN generator, the optimization targets for supervised learning are more stable from an optimization point of view. The difference is clear: the optimization target for the GAN generator is a learned discriminator. While in supervised models, the optimization targets are distance functions with nice geometric properties. The latter usually provides much easier training gradients than the former, especially at the early stages of training.\n\nInspired by this observation, we propose to incorporate a supervised training signal as a regularizer on top of the discriminator target. Assume the generator G(z) : Z → X generates samples by sampling first from a fixed prior distribution in space Z followed by a deterministic trainable transformation G into the sample space X. Together with G, we also jointly train an encoder E(x) : X → Z. Assume d is some similarity metric in the data space, we add E x∼p d [d(x, G•E(x))] as a regularizer, where p d is the data generating distribution. The encoder itself is trained by minimizing the same reconstruction error.\n\nIn practice, there are many options for the distance measure d. For instance, the pixel-wise L 2 distance, or the distance of learned features by the discriminator (Dumoulin et al., 2016) or by other networks, such as a VGG classifier. (Ledig et al., 2016) The geometric intuition for this regularizer is straight-forward. We are trying to move the generated manifold to the real data manifold using gradient descent. In addition to the gradient provided by the discriminator, we can also try to match the two manifolds by other geometric distances, say, L s metric. The idea of adding an encoder is equivalent to first training a point to point mapping G(E(x)) between the two manifolds and then trying to minimize the expected distance between the points on these two manifolds.\n\n\nMODE REGULARIZER\n\nIn addition to the metric regularizer, we propose a mode regularizer to further penalize missing modes. In traditional GANs, the optimization target for the generator is the empirical sum For most z, the gradient of the generator ∇ θ log D(G θ (z)) pushes the generator towards the major mode M 1 . Only when G(z) is very close to the mode M 2 can the generator get gradients to push itself towards the minor mode M 2 . However, it is possible that such z is of low or zero probability in the prior distribution p 0 .\n\nGiven this observation, consider a regularized GAN model with the metric regularizer. Assume M 0 is a minor mode of the data generating distribution. For x ∈ M 0 , we know that if G • E is a good autoencoder, G(E(x)) will be located very close to mode M 0 . Since there are sufficient training examples of mode M 0 in the training data, we add the mode regularizer E x∼p d [log D(G • E(x))] to our optimization target for the generator, to encourage G(E(x)) to move towards a nearby mode of the data generating distribution. In this way, we can achieve fair probability mass distribution across different modes.\n\nIn short, our regularized optimization target for the generator and the encoder becomes:\nT G = −E z [log D(G(z))] + E x∼p d [λ 1 d(x, G • E(x)) + λ 2 log D(G • E(x))]\n(1)\nT E = E x∼p d [λ 1 d(x, G • E(x)) + λ 2 log D(G • E(x))](2)\n\nMANIFOLD-DIFFUSION TRAINING FOR REGULARIZED GANS\n\nOn some large scale datasets, CelebA for example, the regularizers we have discussed do improve the diversity of generated samples, but the quality of samples may not be as good without carefully tuning the hyperparameters. Here we propose a new algorithm for training metric-regularized GANs, which is very stable and much easier to tune for producing good samples.\n\nThe proposed algorithm divides the training procedure of GANs into two steps: a manifold step and a diffusion step. In the manifold step, we try to match the generation manifold and the real data manifold with the help of an encoder and the geometric metric loss. In the diffusion step, we try to distribute the probability mass on the generation manifold fairly according to the real data distribution.\n\nAn example of manifold-diffusion training of GAN (MDGAN for short) is as follows: we train a discriminator D 1 which separates between the samples x and G • E(x), for x from the data, and we optimize G with respect to the regularized GAN loss E[log D 1 (G•E(x))+λd(x, G•E(x))] in order to match the two manifolds. In the diffusion step we train a discriminator D 2 between distributions G(z) and G • E(x), and we train G to maximize log D 2 (G(z)). Since these two distributions are now nearly on the same low dimensional manifold, the discriminator D 2 provides much smoother and more stable gradients. The detailed training procedure is given in Appendix A. See Figure 6 for the quality of generated samples.\n\n\nEVALUATION METRICS FOR MODE MISSING\n\nIn order to estimate both the missing modes and the sample qualities in our experiments, we used several different metrics for different experiments instead of human annotators.\n\nThe inception score (Salimans et al., 2016) was considered as a good assessment for sample quality from a labelled dataset:\nexp (E x KL(p(y|x)||p * (y)))(3)\nWhere x denotes one sample, p(y|x) is the softmax output of a trained classifier of the labels, and p * (y) is the overall label distribution of generated samples. The intuition behind this score is that a strong classifier usually has a high confidence for good samples. However, the inception score is sometimes not a good metric for our purpose. Assume a generative model that collapse to a very bad image. Although the model is very bad, it can have a perfect inception score, because p(y|x) can have a high entropy and p * (y) can have a low entropy. So instead, for labelled datasets, we propose another assessment for both visual quality and variety of samples, the MODE score:\n\nexp (E x KL(p(y|x)||p(y)) − KL(p * (y)||p(y)))\n\nwhere p(y) is the distribution of labels in the training data. According to our human evaluation experiences, the MODE score successfully measures two important aspects of generative models, i.e., variety and visual quality, in one metric.\n\nHowever, in datasets without labels (LSUN) or where the labels are not sufficient to characterize every data mode (CelebA), the above metric does not work well. We instead train a third party discriminator between the real data and the generated data from the model. It is similar to the GAN discriminator but is not used to train the generator. We can view the output of the discriminator as an estimator for the quantity (See (Goodfellow et al., 2014) for proof):\nD * (s) ≈ p g (s) p g (s) + p d (s)(5)\nWhere p g is the probability density of the generator and p d is the density of the data distribution.\n\nTo prevent D * from learning a perfect 0-1 separation of p g and p d , we inject a zero-mean Gaussian noise to the inputs when training D * . After training, we test D * on the test set T of the real dataset.\n\nIf for any test sample t ∈ T , the discrimination value D(t) is close to 1, we can conclude that the mode corresponding to t is missing. In this way, although we cannot measure exactly the number of modes that are missing, we have a good estimator of the total probability mass of all the missing modes. We perform two classes of experiments on MNIST.\n\n\nEXPERIMENTS\n\n\nMNIST\n\nFor the MNIST dataset, we can assume that the data generating distribution can be approximated with ten dominant modes, if we define the term \"mode\" here as a connected component of the data manifold.\n\n\nGRID SEARCH FOR MNIST GAN MODELS\n\nIn order to systemically explore the effect of our proposed regularizers on GAN models in terms of improving stability and sample quality, we use a large scale grid search of different GAN hyper-parameters on the MNIST dataset. The grid search is based on a pair of randomly selected loss weights: λ 1 = 0.2 and λ 2 = 0.4. We use the same hyper-parameter settings for both GAN and Regularized GAN, and list the search ranges in Table 1. Our grid search is similar to those proposed in Zhao et al. (2016). Please refer to it for detailed explanations regarding these hyper-parameters.\n\nFor evaluation, we first train a 4-layer CNN classifier on the MNIST digits, and then apply it to compute the MODE scores for the generated samples from all these models. The resulting distribution of MODE score is shown in Figure 3. Clearly, our proposed regularizer significantly improves the MODE scores and thus demonstrates its benefits on stabilizing GANs and improving sample qualities. To illustrate the effect of regularizers with different coefficients, we randomly pick an architecture and train it with different λ 1 = λ 2 . The results are shown in Figure 4. \n\n\nCOMPOSITIONAL MNIST DATA WITH 1000 MODES\n\nIn order to quantitatively study the effect of our regularizers on the missing modes, we concatenate three MNIST digits to a number in [0,999] in a single 64x64 image, and then train DCGAN as a baseline model on the 1000 modes dataset. The digits on the image are sampled with different probabilities, in order to test the model's capability to preserve small modes in generation. We again use a pre-trained classifier for MNIST instead of a human to evaluate the models. The performances on the compositional experiment are measured by two metrics. #Miss represents the classifier-reported number of missing modes, which is the size of the set of numbers that the model never generates. KL stands for the KL divergence between the classifier-reported distribution of generated numbers and the distribution of numbers in the training data (as for the Inception score). The results are shown in Table 2. With the help of our proposed regularizer, both the number of missing modes and KL divergence drop dramatically among all the sets of the compositional MNIST dataset, which again proves the effectiveness of our regularizer for preventing the missing modes problem.\n\n\nCELEBA\n\nTo test the effectiveness of our proposal on harder problems, we implement an encoder for the DCGAN algorithm and train our model with different hyper-parameters together with the DCGAN baseline on the CelebA dataset. We provide the detailed architecture of our regularized DCGAN in Appendix B.\n\n\nMISSING MODES ESTIMATION ON CELEBA\n\nWe also employ a third party discriminator trained with injected noise as a metric for missing mode estimation. To implement this, we add noise in the input layer in the discriminator network. For each GAN model to be estimated, we independently train this noisy discriminator, as mode estimator, with the same architecture and hyper-parameters on the generated data and the training data. We then apply the mode estimator to the test data. The images which have high mode estimator outputs can be viewed as on the missing modes.  The comparison result is shown in Table 3. Both our proposed Regularized-GAN and MDGAN outperform baseline DCGAN models on all settings. Especially, MDGAN suppresses other models, showing its superiority on modes preserving. We also find that, although sharing the same architecture, the DCGAN with 200-dimensional noise performs quite worse than that with 100-dimensional noise as input. On the contrary, our regularized GAN performs more consistently.\n\nTo get a better understanding of the models' performance, we want to figure out when and where these models miss the modes. Visualizing the test images associated with missed modes is instructive. In Figure 5, the left three images are missed by all models. It is rare to see in the training data the cap in the second image and the type of background in the third, which thus can be viewed as small modes under this situation. These three images should be considered as the hardest test data for GAN to learn. Nonetheless, our best model, MDGAN still capture certain small modes. The seven images on the right in Figure 5 are only missed by DCGAN. The sideface, paleface, black, and the berets are special attributes among these images, but our proposed MDGAN performs well on all of them. \n\n\nQUALITATIVE EVALUATION OF GENERATED SAMPLES\n\nAfter quantitative evaluation, we manually examine the generated samples by our regularized GAN to see whether the proposed regularizer has side-effects on sample quality. We compare our model with ALI (Dumoulin et al., 2016), VAEGAN (Larsen et al., 2015), and DCGAN (Radford et al., 2015) in terms of sample visual quality and mode diversity. Samples generated from these models are shown in Figure 6 2 . Figure 6: Samples generated from different generative models. For each compared model, we directly take ten decent samples reported in their corresponding papers and code repositories. Note how MDGAN samples are both globally more coherent and locally have sharp textures.\n\nBoth MDGAN and Regularized-GAN generate clear and natural-looking face images. Although ALI's samples are plausible, they are sightly deformed in comparison with those from MDGAN. The samples from VAEGAN and DCGAN seem globally less coherent and locally less sharp.\n\nAs to sample quality, it is worth noting that the samples from MDGAN enjoy fewer distortions. With all four other models, the majority of generated samples suffer from some sort of distortion. However, for the samples generated by MDGAN, the level of distortion is lower compared with the other four compared models. We attribute it to the help of the autoencoder as the regularizer to alter the generation manifolds. In this way, the generator is able to learn fine-grained details such as face edges. As a result, MDGAN is able to reduce distortions. In terms of missing modes problem, we instructed five individuals to conduct human evaluation on the generated samples. They achieve consensus that MDGAN wins in terms of mode diversities. Two people pointed out that MDGAN generates a larger amount of samples with side faces than other models. We select several of these side face samples in Figure 7. Clearly, our samples maintain acceptable visual fidelity meanwhile share diverse modes. Combined with the above quantitative results, it is convincing that our regularizers bring benefits for both training stability and mode variety without the loss of sample quality.\n\n\nCONCLUSIONS\n\nAlthough GANs achieve state-of-the-art results on a large variety of unsupervised learning tasks, training them is considered highly unstable, very difficult and sensitive to hyper-parameters, all the while, missing modes from the data distribution or even collapsing large amounts of probability mass on some modes. Successful GAN training usually requires large amounts of human and computing efforts to fine tune the hyper-parameters, in order to stabilize training and avoid collapsing.\n\nResearchers usually rely on their own experience and published tricks and hyper-parameters instead of systematic methods for training GANs.\n\nWe provide systematic ways to measure and avoid the missing modes problem and stabilize training with the proposed autoencoder-based regularizers. The key idea is that some geometric metrics can provide more stable gradients than trained discriminators, and when combined with the encoder, they can be used as regularizers for training. These regularizers can also penalize missing modes and encourage a fair distribution of probability mass on the generation manifold.\n\n\nA APPENDIX: PSEUDO CODE FOR MDGAN\n\nIn this Appendix, we give the detailed training procedure of an MDGAN example we discuss in Section 3.3.\n\n\nManifold\n\nStep: 1. Sample {x 1 , x 2 , · · · x m } from data generating distribution p data (x). 2. Update discriminator D 1 using SGD with gradient ascent:\n∇ θ 1 d 1 m m i=1 [log D 1 (x i ) + log(1 − D 1 (G(E(x i ))))]\n3. Update generator G using SGD with gradient ascent:\n∇ θg 1 m m i=1 [λ log D 1 (G(E(x i ))) − ||x i − G(E(x i ))|| 2 ] Diffusion\nStep: 4. Sample {x 1 , x 2 , · · · x m } from data generating distribution p data (x). 5. Sample {z 1 , z 2 , · · · z m } from prior distribution p σ (z). 6. Update discriminator D 2 using SGD with gradient ascent:\n∇ θ 2 d 1 m m i=1 [log D 2 (G(E(x i ))) + log(1 − D 2 (z i ))]\n7. Update generator G using SGD with gradient ascent: \n∇ θg 1 m m i=1 [log D 2 (G(z i ))]\n\nB APPENDIX: ARCHITECTURE FOR EXPERIMENTS\n\nWe use similar architectures for Compositional MNIST and CelebA experiments. The architecture is based on that found in DCGAN Radford et al. (2015). Apart from the discriminator and generator which are the same as DCGAN, we add an encoder which is the \"inverse\" of the generator, by reversing the order of layers and replacing the de-convolutional layers with convolutional layers.\n\nOne has to pay particular attention to batch normalization layers. In DCGAN, there are batch normalization layers both in the generator and the discriminator. However, two classes of data go through the batch normalization layers in the generator. One come from sampled noise z, the other one come from the encoder. In our implementation, we separate the batch statistics for these two classes of data in the generator, while keeping the parameters of BN layer to be shared. In this way, the batch statistics of these two kinds of batches cannot interfere with each other.\n\n\nC APPENDIX: ADDITIONAL SYNTHESIZED EXPERIMENTS\n\nTo demonstrate the effectiveness of mode-regularized GANs proposed in this paper, we train a very simple GAN architecture on synthesized 2D dataset, following Metz et al. (2016).\n\nThe data is sampled from a mixture of 6 Gaussians, with standard derivation of 0.1. The means of the Gaussians are placed around a circle with radius 5. The generator network has two ReLU hidden layers with 128 neurons. It generates 2D output samples from 3D uniform noise from [0,1]. The discriminator consists of only one fully connected layer of ReLU neurons, mapping the 2D input to a real 1D number. Both networks are optimized with the Adam optimizer with the learning rate of 1e-4.\n\nIn the regularized version, we choose λ 1 = λ 2 = 0.005. The comparison between the generator distribution from standard GAN and our proposed regularized GAN are shown in Figure 9. Figure 9: Comparison results on a toy 2D mixture of Gaussians dataset. The columns on the left shows heatmaps of the generator distributions as the number of training epochs increases, whereas the rightmost column presents the target, the original data distribution. The top row shows standard GAN result. The generator has a hard time oscillating among the modes of the data distribution, and is only able to \"recover\" a single data mode at once. In contrast, the bottom row shows results of our regularized GAN. Its generator quickly captures the underlying multiple modes and fits the target distribution.\n\n\nD APPENDIX: COMPARISON WITH VAEGAN\n\nIn this appendix section, we demonstrate the effectiveness and uniqueness of mode-regularized GANs proposed in this paper as compared to Larsen et al. (2015) in terms of its theoretical difference, sample quality and number of missing modes.\n\nWith regard to the theoretical difference, the optimization of VAEGAN relies on the probabilistic variational bound, namely p(x) ≥ E q(z|x) [log p(x|z)] − KL(q(z|x)||p(z)). This variational bound together with a GAN loss is optimized with several assumptions imposed in VAEGAN:\n\n1. In general, VAE is based on the assumption that the true posterior p(z|x) can be well approximated by factorized Gaussian distribution q.\n\n2. As to VAEGAN, It is also assumed that the maximum likelihood objectives does not conflict with GAN objective in terms of probabilistic framework.\n\nThe first assumption does not necessarily hold for GANs. We have found that in some trained models of DCGANs, the real posterior p(z|x) is even not guaranteed to have only one mode, not to mention it is anything close to factorized Gaussian. We believe that this difference in probabilistic framework is an essential obstacle when one tries to use the objective of VAEGAN as a regularizer. However, in our algorithm, where we use a plain auto-encoder instead of VAE as the objective. Plain auto-encooders works better than VAE for our purposes because as long as the model G(z) is able to generate training samples, there always exists a function E * (x) such that G(E(x)) = x. Our encoder can therefore be viewed as being trained to approximate this real encoder E * . There are no conflicts between a good GAN generator and our regularization objective. Hence, our objectives can be used as regularizers for encoding the prior knowledge that good models should be able to generate the training samples. This is why our work is essentially different from VAEGAN. In our experiments, we also believe that this is the reason why VAEGAN generates worse samples than a carefully tuned regularized GANs.\n\nIn terms of sample quality and missing modes, we run the official code of VAEGAN 3 with their default setting. We train VAEGAN for 30 epochs 4 and our models for only 20 epochs. For fairness, their model was run 3 times and the trained model with the best sample visual quality was taken for the comparison.\n\nThe generated samples are shown in Figure 10. The most obvious difference between our samples and VAEGAN's samples is the face distortion, which is consistent with our experimental results in Section 4.2.2. We conjecture that the distortions of VAEGAN's samples are due to the conflicts between the two objectives, as we present above. In other words, the way we introduce auto-encoders as regularizers for GAN models is different from VAEGAN's. The difference is that the second assumption mentioned above is not required in our approaches. In our framework, the auto-encoders helps alter the generation manifolds, leading to fewer distortions in fine-grained details in our generated samples. Figure 10: Samples generated by our models and VAEGAN. The third line are samples generated by our self-trained VAEGAN model, with default settings. The last line are generated samples reported in the original VAEGAN paper. We depict both of them here for a fair comparison.\n\nIn terms of the missing modes problem, we use the same method described in Section 4.2.1 for computing the number of images with missing modes. The results are shown below. Table 4: Number of images on the missing modes on CelebA estimated by a third-party discriminator. The numbers in the brackets indicate the dimension of prior z. σ denotes the standard deviation of the added Gaussian noise applied at the input of the discriminator to regularize it. MDGAN achieves a very high reduction in the number of missing modes, in comparison to VAEGAN. We see that using our proposed regularizers results in a huge drop in the number of missing modes. We conjecture that the reason why VAEGAN performs very bad in our metric for missing modes is because the samples generated are of low quality, so the discriminator classifies the samples as \"not on mode\". Namely, the data generated is too far away from many real data modes. Essentially if a model generates very bad samples, we can say that the model misses all or most modes.\n\nTo conduct more fair evaluation between VAEGAN and our methods, we also perform a blind human evaluation. Again we instructed five individuals to conduct this evaluation of sample variability. Without telling them which is generated by VAEGAN and which is generated by our methods, four people agree that our method wins in terms of sample diversity. One person thinks the samples are equally diverse.\n\nIn conclusion, we demonstrate that our proposed mode-regularized GANs, i.e., Reg-GAN and MDGAN, are different from VAEGAN theoretically as discussed above. Such differences empirically result in better sample quality and mode preserving ability, which are our main contributions.\n\nFigure 1 :\n1Samples with very high discrimination values (D=1.0) in DCGAN model trained on CelebA dataset.\n\nFigure 2 :\n2Illustration of missing modes problem. As an example, consider the situation in Figure 2.\n\nFigure 3 :\n3The distributions of MODE scores for GAN and regularized GAN.\n\nFigure 4 :\n4(Left 1-5) Different hyperparameters for MNIST generation. The values of the λ 1 and λ 2 in our Regularized GAN are listed below the corresponding samples. (Right 6-7) Best samples through grid search for GAN and Regularized GAN.\n\n3 :\n3Number of images on the missing modes on CelebA estimated by a third-party discriminator. The numbers in the brackets indicate the dimension of prior z. σ denotes the standard deviation of the added Gaussian noise applied at the input of the discriminator to regularize it. MDGAN achieves a very high reduction in the number of missing modes, in comparison to other methods .σ DCGAN (100) DCGAN (200) Reg-GAN (100) Reg-GAN (200)\n\nFigure 5 :\n5Test set images that are on missing mode. Left: Both MDGAN and DCGAN missing. Right: Only DCGAN missing.\n\nFigure 7 :\n7Sideface samples generated by Regularized-GAN and MDGAN.\n\nFigure 8 :\n8The detailed training procedure of an MDGAN example.\n\nTable 1 :\n1Grid Search for Hyperparameters.nLayerG [2,3,4] \nnLayerD [2,3,4] \nsizeG \n[400,800,1600,3200] \nsizeD \n[256, 512, 1024] \ndropoutD [True,False] \noptimG \n[SGD,Adam] \noptimD \n[SGD,Adam] \nlr \n[1e-2,1e-3,1e-4] \n\n\n\nTable 2 :\n2Results for Compositional MNIST with 1000 modes. The proposed regularization (Reg-DCGAN) allows to substantially reduce the number of missed modes as well as the KL divergence that measures the plausibility of the generated samples (like in the Inception score).Set 1 \nSet 2 \nSet 3 \nSet 4 \n#Miss KL #Miss KL #Miss KL #Miss KL \n\nDCGAN 204.7 77.9 204.3 60.2 103.4 75.9 \n89.3 \n77.8 \n\nReg-DCGAN \n32.1 \n62.3 \n71.5 \n58.9 \n42.7 \n68.4 \n31.6 \n67.8 \n\n\n\nTable\n\nThis problem exists even when we use log D(G(z)) as target for the generator, as noted byDenton et al. (2015) and our experiments.\ni ∇ θ log D(G θ (z i )). The missing mode problem is caused by the conjunction of two facts: (1) the areas near missing modes are rarely visited by the generator, by definition, thus providing very few examples to improve the generator around those areas, and (2) both missing modes and nonmissing modes tend to correspond to a high value of D, because the generator is not perfect so that the discriminator can take strong decisions locally and obtain a high value of D even near non-missing modes.\nFor fair comparison, we also recommend readers to refer to the original papers Dumoulin et al. (2016); Larsen et al. (2015); Radford et al. (2015) for the reported samples of the compared. The ALI samples are from https://github.com/IshmaelBelghazi/ALI/blob/master/paper/celeba_ samples.png and we reverted them to the original 64x64 size. The DCGAN samples are from https: //github.com/Newmu/dcgan_code/\nhttps://github.com/andersbll/autoencoding_beyond_pixels 4 Note that we also trained 20-epoch version of VAEGAN, however the samples seemed worse.\nACKNOWLEDGEMENTSWe thank Naiyan Wang, Jianbo Ye, Yuchen Ding, Saboya Yang for their GPU support. We also want to thank Huiling Zhen for helpful discussions, Junbo Zhao for providing the details of grid search experiments on the EBGAN model, as well as Anders Boesen Lindbo Larsen for kindly helping us on running VAEGAN experiments. We appreciate for the valuable suggestions and comments from the anonymous reviewers. The work described in this paper was partially supported by NSERC, Calcul Quebec, Compute Canada, the Canada Research Chairs, CIFAR, National Natural Science Foundation of China (61672445 and 61272291), Research Grants Council of Hong Kong (PolyU 152094/14E), and The Hong Kong Polytechnic University (G-YBP6).\nDeep generative image models using a laplacian pyramid of adversarial networks. Soumith Emily L Denton, Rob Chintala, Fergus, Advances in neural information processing systems. Emily L Denton, Soumith Chintala, Rob Fergus, et al. Deep generative image models using a laplacian pyramid of adversarial networks. In Advances in neural information processing systems, pp. 1486-1494, 2015.\n\nAdversarial feature learning. Jeff Donahue, Philipp Krähenbühl, Trevor Darrell, arXiv:1605.09782arXiv preprintJeff Donahue, Philipp Krähenbühl, and Trevor Darrell. Adversarial feature learning. arXiv preprint arXiv:1605.09782, 2016.\n\nGenerating images with perceptual similarity metrics based on deep networks. Alexey Dosovitskiy, Thomas Brox, arXiv:1602.02644arXiv preprintAlexey Dosovitskiy and Thomas Brox. Generating images with perceptual similarity metrics based on deep networks. arXiv preprint arXiv:1602.02644, 2016.\n\nIshmael Vincent Dumoulin, Ben Belghazi, Alex Poole, Martin Lamb, Olivier Arjovsky, Aaron Mastropietro, Courville, arXiv:1606.00704Adversarially learned inference. arXiv preprintVincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mastropi- etro, and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.\n\nGenerative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Advances in Neural Information Processing Systems. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor- mation Processing Systems, pp. 2672-2680, 2014.\n\nImage-to-image translation with conditional adversarial networks. arxiv. Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A Efros, Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. arxiv, 2016.\n\nAutoencoding beyond pixels using a learned similarity metric. Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Ole Winther, arXiv:1512.09300arXiv preprintAnders Boesen Lindbo Larsen, Søren Kaae Sønderby, and Ole Winther. Autoencoding beyond pixels using a learned similarity metric. arXiv preprint arXiv:1512.09300, 2015.\n\nPhoto-realistic single image super-resolution using a generative adversarial network. Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi, arXiv:1609.04802arXiv preprintChristian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero, Andrew Aitken, Alykhan Tejani, Jo- hannes Totz, Zehan Wang, and Wenzhe Shi. Photo-realistic single image super-resolution using a generative adversarial network. arXiv preprint arXiv:1609.04802, 2016.\n\nPrecomputed real-time texture synthesis with markovian generative adversarial networks. Chuan Li, Michael Wand, arXiv:1604.04382arXiv preprintChuan Li and Michael Wand. Precomputed real-time texture synthesis with markovian generative adversarial networks. arXiv preprint arXiv:1604.04382, 2016.\n\nDeep multi-scale video prediction beyond mean square error. Michael Mathieu, Camille Couprie, Yann Lecun, arXiv:1511.05440arXiv preprintMichael Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond mean square error. arXiv preprint arXiv:1511.05440, 2015.\n\nLuke Metz, Ben Poole, David Pfau, Jascha Sohl-Dickstein, arXiv:1611.02163Unrolled generative adversarial networks. arXiv preprintLuke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial networks. arXiv preprint arXiv:1611.02163, 2016.\n\nMehdi Mirza, Simon Osindero, arXiv:1411.1784Conditional generative adversarial nets. arXiv preprintMehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.\n\nPlug & play generative networks: Conditional iterative generation of images in latent space. Anh Nguyen, Jason Yosinski, Yoshua Bengio, Alexey Dosovitskiy, Jeff Clune, arXiv:1612.00005arXiv preprintAnh Nguyen, Jason Yosinski, Yoshua Bengio, Alexey Dosovitskiy, and Jeff Clune. Plug & play generative networks: Conditional iterative generation of images in latent space. arXiv preprint arXiv:1612.00005, 2016.\n\nUnsupervised representation learning with deep convolutional generative adversarial networks. Alec Radford, Luke Metz, Soumith Chintala, arXiv:1511.06434arXiv preprintAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\n\nScott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, Honglak Lee, arXiv:1605.05396Generative adversarial text to image synthesis. arXiv preprintScott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis. arXiv preprint arXiv:1605.05396, 2016.\n\nMasaki Saito, Eiichi Matsumoto, arXiv:1611.06624Temporal generative adversarial nets. arXiv preprintMasaki Saito and Eiichi Matsumoto. Temporal generative adversarial nets. arXiv preprint arXiv:1611.06624, 2016.\n\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, arXiv:1606.03498Improved techniques for training gans. arXiv preprintTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. arXiv preprint arXiv:1606.03498, 2016.\n\nAmortised map inference for image super-resolution. Casper Kaae, Jose Sønderby, Lucas Caballero, Wenzhe Theis, Ferenc Shi, Huszár, arXiv:1610.04490arXiv preprintCasper Kaae Sønderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc Huszár. Amortised map inference for image super-resolution. arXiv preprint arXiv:1610.04490, 2016.\n\nGenerating videos with scene dynamics. Carl Vondrick, Hamed Pirsiavash, Antonio Torralba, Advances In Neural Information Processing Systems. Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics. In Advances In Neural Information Processing Systems, pp. 613-621, 2016.\n\nGenerative image modeling using style and structure adversarial networks. Xiaolong Wang, Abhinav Gupta, ECCV. Xiaolong Wang and Abhinav Gupta. Generative image modeling using style and structure adversar- ial networks. In ECCV, 2016.\n\nLearning a probabilistic latent space of object shapes via 3d generative-adversarial modeling. Jiajun Wu, Chengkai Zhang, Tianfan Xue, T William, Joshua B Freeman, Tenenbaum, Neural Information Processing Systems (NIPS). Jiajun Wu, Chengkai Zhang, Tianfan Xue, William T Freeman, and Joshua B Tenenbaum. Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling. In Neural Information Processing Systems (NIPS), 2016.\n\nEnergy-based generative adversarial network. Junbo Zhao, Michael Mathieu, Yann Lecun, arXiv:1609.03126arXiv preprintJunbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial network. arXiv preprint arXiv:1609.03126, 2016.\n\nLearning temporal transformations from time-lapse videos. Yipin Zhou, Tamara L Berg, European Conference on Computer Vision. SpringerYipin Zhou and Tamara L Berg. Learning temporal transformations from time-lapse videos. In European Conference on Computer Vision, pp. 262-277. Springer, 2016.\n\nGenerative visual manipulation on the natural image manifold. Jun-Yan Zhu, Philipp Krähenbühl, Eli Shechtman, Alexei A Efros, Proceedings of European Conference on Computer Vision (ECCV). European Conference on Computer Vision (ECCV)Jun-Yan Zhu, Philipp Krähenbühl, Eli Shechtman, and Alexei A. Efros. Generative visual manipula- tion on the natural image manifold. In Proceedings of European Conference on Computer Vision (ECCV), 2016."
  },
  {
    "corpusid": 239998253,
    "title": "What Do We Mean by Generalization in Federated Learning?",
    "abstract": "Federated learning data is drawn from a distribution of distributions: clients are drawn from a meta-distribution, and their data are drawn from local data distributions. Thus generalization studies in federated learning should separate performance gaps from unseen client data (out-of-sample gap) from performance gaps from unseen client distributions (participation gap). In this work, we propose a framework for disentangling these performance gaps. Using this framework, we observe and explain differences in behavior across natural and synthetic federated datasets, indicating that dataset synthesis strategy can be important for realistic simulations of generalization in federated learning. We propose a semantic synthesis strategy that enables realistic simulation without naturally-partitioned data. Informed by our findings, we call out community suggestions for future federated learning works.",
    "citations": [
      235613568,
      231924480,
      211678094,
      195798643,
      43964415
    ],
    "full_paper": "What Do We Mean by Generalization in Federated Learning?\n\n\nHonglin Yuan \nWarren Morningstar \nLin Ning \nKaran Singhal \nWhat Do We Mean by Generalization in Federated Learning?\n\nFederated learning data is drawn from a distribution of distributions: clients are drawn from a meta-distribution, and their data are drawn from local data distributions. Thus generalization studies in federated learning should separate performance gaps from unseen client data (out-of-sample gap) from performance gaps from unseen client distributions (participation gap). In this work, we propose a framework for disentangling these performance gaps. Using this framework, we observe and explain differences in behavior across natural and synthetic federated datasets, indicating that dataset synthesis strategy can be important for realistic simulations of generalization in federated learning. We propose a semantic synthesis strategy that enables realistic simulation without naturally-partitioned data. Informed by our findings, we call out community suggestions for future federated learning works.\n\nIntroduction\n\nFederated learning (FL) enables distributed clients to train a machine learning model collaboratively via focused communication with a coordinating server. In cross-device FL settings, clients are sampled from a population for participation in each round of training Li et al., 2020a). Each participating client possesses its own data distribution, from which finite samples are drawn for federated training.\n\nGiven this problem framing, defining generalization in FL is not as obvious as in centralized learning. Existing works generally characterize the difference between empirical and expected risk for clients participating in training Yagli et al., 2020;Karimireddy et al., 2020;. However, in cross-device settings, which we focus on in this work, clients are sampled from a large population with unreliable availability. Many or most clients may never participate in training Singhal et al., 2021). Thus it is crucial to better understand expected performance for non-participating clients.\n\nIn this work, we model clients' data distributions as drawn from a meta population distribution , an assumption we argue is reasonable in real-world FL settings. We use this framing to define two generalization gaps to study in FL: the out-of-sample gap, or the difference between empirical and expected risk for participating clients, and the participation gap, or the difference in expected risk between participating and non-participating clients. Previous works generally ignore the participation gap or fail to disentangle it from the out-of-sample gap, but we observe significant participation gaps in practice across six federated datasets (see Figure 1), indicating that the participation gap is an important but neglected feature of generalization in FL.\n\nWe present a systematic study of generalization in FL across six tasks. We observe that focusing only on out-of-sample gaps misses important effects, including differences in generalization behavior across naturally-partitioned and synthetically-partitioned federated datasets. We use our results to inform a series of recommendations for future works studying generalization in FL. We conduct experiments on four image classification tasks and two text prediction tasks. As described in Section 3.1, the participation gap can be estimated as the difference in metrics between participating validation and unparticipating data (defined in Figure 2). Prior works either ignore the participation gap or fail to separate it from other generalization gaps, indicating the participation gap is a neglected feature of generalization in FL.\n\nOur contributions:\n\n• Propose a three-way split for measuring out-of-sample and participation gaps in centralized and FL settings where data is drawn from a distribution of distributions (see Figure 2).\n\n• Observe significant participation gaps across six different tasks (see Figure 1) and perform empirical studies on how various factors, e.g., number of clients and client diversity, affect generalization performance (see Section 5).\n\n• Observe significant differences in generalization behavior across naturally-partitioned and syntheticallypartitioned federated datasets, and propose semantic partitioning, a dataset synthesis strategy that enables more realistic simulations of generalization behavior in FL without requiring naturally-partitioned data (see Section 4).\n\n• Present a model to define the participation gap (Section 2), reveal its connection with data heterogeneity (Section 3.2), and explain differences in generalization behavior between label-based partitioning and semantic partitioning (Section 4.2).\n\n• Present recommendations for future FL works, informed by our findings (see Section 6).\n\n• Release an extensible open-source code library for studying generalization in FL (see Reproducibility Statement).\n\n\nRelated work\n\nWe briefly discuss primary related work here and provide a detailed review in Appendix A. We refer readers to ;  for a more comprehensive introduction to federated learning in general.\n\nDistributional heterogeneity in FL. Distributional heterogeneity is one of the most important patterns in federated learning . Existing literature on FL heterogeneity is mostly focused on the impact of heterogeneity on the training efficiency (convergence and communication) of federated optimizers (Karimireddy et al., 2020;. In this work, we identify that the participation gap is another major outcome of the heterogeneity in FL, and recommend using the participation gap as a natural measurement for dataset heterogeneity.\n\nPersonalized FL. In this work, we propose to evaluate and distinguish the generalization performance of clients participating and non-participating in training. Throughout this work, we focus on the classic FL setting  in which a single global model is learned from and served to all clients. In the personalized FL setting (Hanzely and Richtárik, 2020;Singhal et al., 2021), the goal is to learn and serve different models for different clients. While related, our focus and contribution is orthogonal to personalization. In fact, our three-way split framework can be readily applied in various personalized FL settings. For example, for personalization via fine-tuning , the participating clients can be defined as the clients that contribute to the training of the base model. The participation gap can then be defined as the difference in post-fine-tuned performance between participating clients and unparticipating clients.\n\nOut-of-distribution generalization. In this work, we propose to train models using a set of participating clients and examine their performance on heldout data from these clients as well as an additional set of non-participating clients. Because each client has a different data distribution, unparticipating clients' data exhibits distributional shift compared to the participating clients' validation data. Therefore, our work is related to the field of domain adaptation (Daumé III, 2009;Ben-David et al., 2007;Shimodaira, 2000;Patel et al., 2015), where a model is explicitly adapted to make predictions on a test set that is not identically distributed to the training set. The participation gap that we observe is consistent with findings from the out-of-distribution research community (Ovadia et al., 2019;Amodei et al., 2016;Lakshminarayanan et al., 2016), which shows on centrally trained (non-federated) models that even small deviations in the morphology of deployment examples can lead to systematic degradations in performance. Our setting differs from these other settings in that our problem framing assumes data is drawn from a distribution of client distributions, meaning that the training and deployment distributions eventually converge as more clients participate in training. In contrast, the typical OOD setup assumes that the distributions will never converge (since the deployment data is out-of-distribution, by definition it does not contribute to training). Our meta-distribution assumption makes the problem of generalizing to unseen distributions potentially more tractable.\n\n\nSetup for Generalization in FL\n\nWe model each FL client as a data source associated with a local distribution and the overall population as a meta-distribution over all possible clients.\n\nDefinition 2.1 (Federated Learning Problem). 1. Let Ξ be the (possibly infinite) collection of all the possible data elements, e.g., image-label pairs. For any parameters w in parameter space Θ, we use f (w, ξ) to denote the loss at element ξ ∈ Ξ with parameter w.\n\n2. Let C be the (possibly infinite) collection of all the possible clients. Every client c ∈ C is associated with a local distribution D c supported on Ξ.\n\n3. Further, we assume there is a meta-distribution P supported on client set C, and each client c is associated with a weight ρ c for aggregation.\n\nThe goal is to optimize the following two-level expected loss as follows:\nF (w) := E c∼P [ρ c · E ξ∼Dc [f (w; ξ)]] .(1)\nSimilar formulations as in Equation (1) have been proposed in existing literature Reisizadeh et al., 2020;Charles and Konečnỳ, 2020). To understand Equation (1), consider a random procedure that repeatedly draws clients c from the meta-distribution P and then evaluates the loss on samples ξ drawn from the local data distribution D c . Equation (1) then characterizes the weighted-average limit of the above process.\n\nRemark. The selection of client weights {ρ c : c ∈ C} depends on the desired aggregation pattern. For example, setting ρ c ≡ 1 will equalize the performance share across all clients. Another common example is setting ρ c to be proportional to the training dataset size contributed by client c.\n\nIntuitive Justification. The formulation in Equation (1) is especially natural in cross-device FL settings, where the number of clients is generally large and modeling clients' local distributions as sampled from a meta-distribution is reasonable. This assumption also makes the problem of generalization to non-participating client distributions more tractable since samples from the meta-distribution are seen during training.\n\nDiscretization. While the ultimate goal is to optimize the expected loss over the entire meta-distribution P and client local distributions {D c : c ∈ C}, only finite training data and a finite number of clients are accessible during training. We call the subset of clients that contributes training data the participating clients, denoted asĈ. We assumeĈ is drawn from the meta-distribution P. For each participating client c ∈Ĉ, we denoteΞ c the training data contributed by client c. We call these data participating training client data and assumeΞ c satisfies the local distribution D c .\n\nDefinition 2.2. The empirical risk on the participating training client data is defined by\nFpart_train(w) := 1 |Ĉ| c∈Ĉ   ρc ·   1 |Ξc| ξ∈|Ξc| f (w; ξ)     .(2)\nEquation (2) characterizes the performance of the model (at parameter w) on the observed data possessed by observed clients.\n\nThere are two levels of generalization between Equation (2) and Equation (1): (i) the generalization from finite training data to unseen data, and (ii) the generalization from finite participating clients to unseen clients. To disentangle the effect of the two levels, a natural intermediate stage is to consider the performance on unseen data of participating (seen) clients.\n\nDefinition 2.3. The semi-empirical risk on the participating validation client data is defined by\nF part_val (w) := 1 |Ĉ| c∈Ĉ [ρc · (E ξ∼Dc f (w; ξ))] .(3)\nEquation (3) differs from Equation (2) by replacing the intra-client empirical loss with the expected loss over D c . We shall also call F (w) defined in Equation (1) the unparticipating expected risk and denote it as F unpart (w) for consistency. Now we are ready to define the two levels of generalization gaps formally.\n\nDefinition 2.4. The out-of-sample gap is defined as F part_val (w) − F part_train (w).\n\nDefinition 2.5. The participation gap is defined as F unpart (w) − F part_val (w).\n\nNote that these gaps are also meaningful in centralized learning settings where data is sampled from a distribution of distributions.\n\n\nUnderstanding Generalization Gaps\n\n\nEstimating Risks and Gaps via the Three-Way Split\n\nBoth F part_val and F unpart take an expectation over the distribution of clients or data. To estimate these two risks in practice, we propose splitting datasets into three blocks. The procedure is demonstrated in Figure 2. Given a dataset with client assignment, we first hold out a percentage of clients (e.g., 20%) as unparticipating clients, as shown in the rightmost two columns (in purple). The remaining clients are participating clients. We refer to this split as inter-client split. Within each participating client, we hold out a percentage of data (e.g., 20%) as participating validation data, as shown in the upper left block (in orange). The remaining data is the participating training client data, as shown in the lower left block (in blue). We refer to this second split as intra-client split. Figure 2: Illustration of the three-way split via a visualization of the EMNIST digits dataset. Each column corresponds to the dataset of one client. A dataset is split into participating training, participating validation, and unparticipating data, which enables separate measurement of out-of-sample and participation gaps (unlike other works). Note we only present the digit \"6\" for illustrative purposes.\n\nExisting FL literature and benchmarks typically conduct either an inter-client or intra-client train-validation split. However, neither inter-client nor intra-client split alone can reveal the participation gap. 1 To the best of our knowledge, this is the first work that conducts both splits simultaneously.\n\n\nWhy is the Participation Gap Interesting?\n\nParticipation gap is an intrinsic property of FL due to heterogeneity. Heterogeneity across clients is one of the most important phenomena in FL. We identify that the participation gap is another outcome of heterogeneity in FL, in that the gap will not exist if data is homogeneous. Formally, we can establish the following proposition.\n\nProposition 3.1. If D c ≡ D for any c ∈ C and ρ c ≡ ρ, then for any participating clientsĈ ⊂ C and w in domain, the participation gap is always zero in that F unpart (w) ≡ F part_val (w).\n\n\nProposition 3.1 holds by definition as\nF part_val (w) = 1 |Ĉ| c∈Ĉ [ρ · (E ξ∼Dc f (w; ξ))] = ρ · (E ξ∼D f (w; ξ)) = Ec∼P [ρ · E ξ∼D [f (w; ξ)]] = Funpart(w).\nRemark. We assume unweighted risk with ρ c ≡ ρ for ease of exposition. Even if ρ c are different, one can still show Funpart(w) F part_val (w) is always equal to a constant independent of w. Therefore the triviality of the participation gap for homogeneous data still holds in the logarithmitic sense.\n\nParticipation gap can quantify client diversity. The participation gap can provide insight into a federated dataset since it provides a quantifiable measure of client diversity / heterogeneity. With other aspects controlled, a federated dataset with larger participation gap tends to have greater heterogeneity. For example, using the same model and hyperparameters, we observe in Section 5 that CIFAR-100 exhibits a larger participation gap than CIFAR-10. Unlike other indirect measures (such as the degradation of federated performance relative to centralized performance), the participation gap is intrinsic in federated datasets and more consistent with respect to training hyperparameters.\n\nParticipation gap can measure overfitting on the population distribution. Just as a generalization gap that increases over time in centralized training can indicate overfitting on training samples, a large or increasing participation gap can indicate a training process is overfitting on participating clients. We observe this effect in Figure 1 for Shakespeare and Stack Overflow tasks. Thus measuring this gap can be important for researchers developing models or algorithms to reduce overfitting.\n\nParticipation gap can quantify model robustness to unseen clients. From a modeler's perspective, the participation gap quantifies the loss of performance incurred by switching from seen clients to unseen clients. The smaller the participation gap is, the more robust the model might be when deployed. Therefore, estimating participation gap may guide modelers to design more robust models, regularizers, and training algorithms.\n\nParticipation gap can quantify the incentive for clients to participate. From a client's perspective, the participation gap offers a measure of the performance gain realized by switching from unparticipating (not contributing training data) to participating (contributing training data). This is a fair comparison since both F part_val and F unpart are estimated on unseen data. When the participation gap is large (e.g., if only a few clients participate), modelers might report the participation gap as a well-justified incentive to encourage more clients to join a federated learning process.\n\n\nReflections on Client Heterogeneity and Synthetic Partitioning\n\nSince participation gaps can quantify client dataset heterogeneity, we study how participation gaps vary for different types of federated datasets. Many prior works Zhao et al., 2018; have created synthetic federated versions of centralized datasets. These centralized datasets do not have naturally-occurring client partitions and thus need to be synthetically partitioned into clients. Due to the importance of heterogeneity in FL, partitioning schemes generally ensure client datasets are heterogeneous in some respect. Previous works typically impose heterogeneity at the label level. For example,  create heterogeneous federated datasets by assigning each client a distribution over labels, where each local distribution is drawn from a Dirichlet meta-distribution. Once conditioned on labels, the drawing process is homogeneous. We refer to these schemes as label-based partitioning. 2 While label heterogeneity is generally observed in natural federated datasets, it is not the only observed form of heterogeneity. In particular, each client in a natural federated dataset has its own separate data generating process. For example, for Federated EMNIST (Cohen et al., 2017), different clients write characters using different handwriting. Label-based partitioning does not account for this form of heterogeneity. To show this, in Figure 3 we visualize the clustering of client data between natural and label-based partitioning  for Federated EMNIST. We project clients from each partitioning into a 2D space using T-SNE (Van der Maaten and Hinton, 2008) applied to the raw pixel data. Naturally partitioned examples clearly cluster more than label-based partitioned examples, which appear to be distributed similarly to the full data distribution.\n\n\nNatural Partitioning\n\nLabel-Based Partitioning Figure 3: T-SNE projection of different partitionings of EMNIST. The top panel shows the naturally-partitioned dataset (partitioned by writer), the bottom panel shows the label-based synthetic dataset. The gray points are the projections of examples from each dataset, obtained by aggregating the data from 100 clients each. The blue points show projections of data from a single client. The naturallypartitioned client data appears much more tightly clustered, whereas the label-based partitioned data appears similarly distributed as the overall dataset, indicating that label-based partitioning may not fully represent realistic client heterogeneity.\n\nInterestingly, differences in heterogeneity also significantly affect generalization behavior. In Figure 4, we compare the training progress of the naturally-partitioned EMNIST dataset with a label-based partitioning following the scheme by . Despite showing greater label heterogeneity ( Fig. 4(a)), the label-based partitioning does not recover any significant participation gap, in sharp contrast to the natural partitioning ( Fig. 4(d)). In Figure 5, we also see minimal participation gap in label-based partitioning for CIFAR. This motivates a client partitioning approach that better preserves the generalization behavior of naturally-partitioned datasets. Observe that label-based partitioning shows greater label heterogeneity (a) than natural partitioning (c), while the participation gap (part_val − unpart) for label-based synthetic partitioning (b) is significantly smaller than that for the natural partitioning (d).\n\n\nSemantic Client Partitioning and the Participation Gap\n\nTo explore and remediate differences in client heterogeneity across natural and synthetic datasets, we propose a semantics-based framework to assign semantically similar examples to clients during federated dataset partitioning. We instantiate this framework via an example of an image classification task.\n\nOur goal is to reverse-engineer the federated dataset-generating process described in Equation (1) so that each client possesses semantically similar data. For example, for the EMNIST dataset, we expect every client (writer) to (i) write in a consistent style for each digit (intra-client intra-label similarity) and (ii) use a consistent writing style across all digits (intra-client inter-label similarity). A simple approach might be to cluster similar examples together and sample client data from clusters. However, if one directly clusters the entire dataset, the resulting clusters may end up largely correlated to labels. To disentangle the effect of label heterogeneity and semantic heterogeneity, we propose the following algorithm to enforce intra-client intra-label similarity and intra-client inter-label similarity in two separate stages.\n\n• Stage 1: For each label, we embed examples using a pretrained neural network (extracting semantic features), and fit a Gaussian Mixture Model to cluster pretrained embeddings into groups. Note that this results in multiple groups per label. This stage enforces intra-client intra-label consistency.\n\n• Stage 2: To package the clusters from different labels into clients, we aim to compute an optimal multi-partite matching with cost-matrix defined by KL-divergence between the Gaussian clusters. To reduce complexity, we heuristically solve the optimal multi-partite matching by progressively solving the optimal bipartite matching at each time for randomly-chosen label pairs. This stage enforces intra-client inter-label consistency.\n\nWe relegate the detailed setup to Appendix D. Using this procedure we can generate clients which have similar example semantics. We show in Figure 5 that this method of partitioning preserves the participation gap. In Appendix D, we visualize several examples of our semantic partitioning on various datasets, which can serve as benchmarks for future works. \n\n\nExplaining differences between label-based and semantic partitioning\n\nTo explain the above behavior, we revisit our mathematical setup and the definition of the participation gap.\n\nRecall that the participation gap is defined as (we omit the weights by setting ρ c ≡ 1 for simplicity):\nIpart_gap(w) := Funpart(w) − F part_val (w) = Ec∼P [E ξ∼Dc [f (w; ξ)]] − 1 |Ĉ| c∈Ĉ [E ξ∼Dc [f (w; ξ)]](4)\nIn order to express the ideas without diving into details of measure theory, we assume without loss of generality that the meta-distribution P is a continuous distribution supported on C with probability density function p P (c). We also assume that for each client c ∈ C, the local distribution D c is a continuous distribution supported on Ξ with probability density function p Dc (ξ). Therefore, the participation gap becomes\nIparticipation(w) = ξ∈Ξ f (w; ξ) ·   c∈C pD c (ξ)pP (c)dc − 1 |Ĉ| c∈Ĉ pD c (ξ)   dξ.(5)\nTherefore the scale of participation gap could depend (negatively) on the concentration speed from 1 |Ĉ| c∈Ĉ p Dc (ξ) to c∈C p Dc (ξ)p P (c)dc as |Ĉ| → ∞. 3 We hypothesize that for label-based partitioning, the concentration is fast because each client has a large entropy as it can cover the entire distribution of a given label. On the other hand, for natural or semantic partitioning, the concentration is slower as the local distribution of each client has lower entropy due to the (natural or synthetic) semantic clustering.\n\nWe validate our hypothesis with an empirical estimation of local dataset entropy, shown in Figure 6. We observe that the clients generated by label-based partitioning demonstrate much higher entropy than the natural ones. Notably, our proposed semantic partitioning has a very similar entropy distribution across clients as the natural partitioning. This indicates that the heterogeneity in EMNIST is mostly attributed to semantic heterogeneity.\n\n\nExperimental Evaluation\n\nWe conduct experiments in six settings, including four image classification tasks: EMNIST-10 (digits only), EMNIST-62 (digits and characters) (Cohen et al., 2017;Caldas et al., 2019), CIFAR-10 and CIFAR-100 (Krizhevsky et al., 2009); and two next character/word prediction tasks: Shakespeare (Caldas et al., 2019) and StackOverflow . We use FedAvgM for image classification tasks and While naturally and semantically partitioned clients appear to have approximately the same distribution of client entropies, the synthetically partitioned clients are distributed differently and have higher average entropy (48 Nats) than the other forms of partitioning (44 Nats). We refer readers to Appendix E for the detailed methodology for the estimation of the entropy. for text-based tasks . 4 The detailed setups (including model, dataset preprocessing, hyperparameter tuning) are relegated to Appendix C. We summarize our main results in Figure 1 and Table 1.\n\nIn the following subsections, we provide more detailed ablation studies exploring how various aspects of training affect generalization performance.\n\n\nEffect of the number of participating clients\n\nIn this subsection we study the effect of the number of participating clients on the generalization performance on various tasks. To this end, we randomly sample subsets of clients of different scales as participating clients, and perform federated training with the same settings otherwise. The results are shown in Figure 7. As the number of participating clients increases, the unparticipating accuracy monotonically improves, and the participation gap tends to decrease. This is consistent with our theoretical understanding, as the participating clients can be interpreted as a discretization of the overall client population distribution.\n\n\nEffect of client diversity\n\nIn this subsection, we study the effect of client diversity on generalization performance. Recall that in the previous subsection, we vary the number of participating clients while keeping the amount of training data per client unchanged. As a result, the total amount of training data will grow proportionally with the number of participating clients.    Figure 8: Effect of diversity on generalization. We fix the total amount of training data while varying the concentration across clients. The concentration varies from taking only 5% clients as participating clients where each client contributes 128 training data, to the most diverse distribution with 80% clients as participating clients but each client only contributes 8 training data. Observe that while the total amount of training data is identical, the more diverse settings exhibit better performance in terms of both participating validation and unparticipating accuracy.\n\nTo disentangle the effect of diversity and the growth of training data size, in the following experiment, we instead fix the total amount of the training data, while varying the concentration across clients. The experiment is conducted on the EMNIST digits dataset. As shown in Figure 8, the training data from a new participating client can be more valuable than those contributed by the existing participating clients. The intuition can also be justified by our model in that the data from a new participating client is drawn from the overall population distribution c∈C p P (c)p Dc (ξ)dc, whereas the data from existing clients are drawn from the distribution aggregated by existing clients 1 |Ĉ| c∈Ĉ p Dc (ξ). This reveals the importance of client diversity in federated training.\n\n\nOverview of Additional Experiments in Appendices\n\nWe provide more detailed analysis and further experiments in Appendices B and C, including:\n\n• Training progress of centralized optimizers on six tasks, see Appendix B.1.\n\n• Detailed analysis of metrics distributions across clients, see Appendices B.2 and B.3. We observe that the unparticipating clients tend to exhibit longer tails on the lower side of accuracy.\n\n• Results on alternative hyperparameter choices, see Appendices C. \n\n\nCommunity Suggestions\n\nIn this work we have used the three-way-split, dataset partitioning strategies, and distributions of metrics to systematically study generalization behavior in FL. Our results inform the following suggestions for the FL community:\n\n• Researchers can use the three-way split to disentangle out-of-sample and participation gaps in empirical studies of FL algorithms.\n\n• When proposing new federated algorithms, researchers might prefer using naturally-partitioned or semantically-partitioned datasets for more realistic simulations of generalization behavior.\n\n• Distributions of metrics across clients (e.g., percentiles, variance) may vary across groups in the threeway split (see Table 2 and Figure 10). We suggest researchers report the distribution of metrics across clients, instead of just the average, when reporting metrics for participating and non-participating clients. We encourage researchers to pay attention to the difference of two distributions (participating validation and unparticipating) as it may have fairness implications.\n\n\nReproducibility Statement\n\nWe provide complete descriptions of experimental setups, including dataset preparation and preprocessing, model configurations, and hyperparameter tuning in Appendix C. Appendix D describes the detailed procedure for semantic partitioning, and Appendix E presents the detailed approach for estimating the entropy that generates Figure 6.\n\nWe are also releasing an extensible code framework for measuring out-of-sample and participation gaps and distributions of metrics (e.g., percentiles) for federated algorithms across several tasks. 5 We include all tasks reported in this work; the framework is easily extended with additional tasks. We also include libraries for performing label-based and semantic dataset partitioning (enabling new benchmark datasets for future works, see Appendix D). This framework enables easy reproduction of our results and facilitates future work. The framework is implemented using TensorFlow Federated (Ingerman and Ostrowski, 2019). The code is released under Apache License 2.0. We hope that the release of this code encourages researchers to take up our suggestions presented in Section 6. \n\n\nAppendices\n\n\nList of Appendices\n\n\nB Additional Experimental Results\n\nIn this section, we present several experimental results omitted from the main body due to space constraints. Additional task-specific ablation experiments can be found in Appendix C.\n\n\nB.1 Training progress of centralized optimizers\n\nIn this subsection, we repeat the experiment in Figure 1 with centralized training. The results are shown in Figure 9. Observe that participation gap still exists with centralized optimizers. This is because the participation gap is an intrinsic outcome of the heterogeneity of federated dataset. Observe that the participation gap still exists even with centralized optimizers. We refer readers to Table 1 for a quantitative comparison between federated optimizers and centralized optimizers.\n\n\nB.2 Percentiles of metrics across clients\n\nIn this subsection, we report the detailed statistics of metrics across clients. Recall that in Table 1, we aggregated the metrics across clients by weighted averaging, where the weights are determined by the number of elements contributed by each client. In the following Table 2, we report five percentiles of metrics across clients: 95 th , 75 th , 50 th (a.k.a. median), 25 th , and 5 th . These statistics provide a detailed characterization on the metrics distribution across clients. 6\n\n\nB.3 Federated training progress at the 25 th percentile acorss clients\n\nTo further inspect the distribution of metrics across clients, we plot the 25 th percentile of accuracy across clients versus communication rounds (training progress). The results are shown in Figure 10.  Percentiles of metrics across clients on six federated tasks. We observe that the unparticipating clients tend to exhibit longer tails on the lower side of accuracy. For example, the participating clients of EMNIST-10 have perfect (100%) accuracy even for clients at the 5 th percentile, whereas the unparticipating clients only achieve 91.7%.  \n\n\nC Additional Details on Experimental Setup and Task-Specific Experiments\n\nIn this section we provide details of the experimental setup, including dataset preparation/preprocessing, model choice and hyperparameter tuning. We also include task-specific experiments with ablations.\n\nFor every setting, unless otherwise stated, we tune the learning rate(s) to achieve the best sum of participating validation accuracy and unparticipating accuracy (so that the result will not be biased towards one of the accuracies).\n\n\nC.1 EMNIST Hand-written Character Recognition Task\n\nFederated Dataset Description and Proprocessing. The EMNIST dataset (Cohen et al., 2017) is a hand-written character recognition dataset derived from the NIST Special Database 19 (Grother and Flanagan, 1995). We used the Federated version of EMNIST (Caldas et al., 2019) dataset, which is partitioned based on the writer identification. We consider both the full version (62 classes) as well as the numbers-only version (10 classes). We adopt the federated EMNIST hosted by Tensorflow Federated (TFF). In TFF, federated EMNIST has a default intra-client split, namely all the clients appeared in both the \"training\" and \"validation\" dataset. To construct a three-way split, we hold out 20% of the total clients as unparticipating clients. Within each participating client, we keep the original training/validation split, i.e., the original training data that are assigned to participating clients will become participating training data. We tested the performance under various number of participating clients, as shown in Figure 7. The results reported in Table 1 are for the case with 272 participating clients.\n\nModel, Optimizer, and Hyperparameters. We train a shallow convolutional neural network with approximately one million trainable parameters as in . For centralized training, we run 200 epochs of SGD with momentum = 0.9 with constant learning rate with batch size 50. The (centralized) learning rate is tuned from {10 −2.5 , 10 −2 , . . . , 10 −0.5 }. For federated training, we run 3000 rounds of FedAvgM  with server momentum = 0.9 and constant server and client learning rates. For each communication round, we uniformly sample 20 clients to train for 1 epoch with client batch size 20. The client and server learning rates are both tuned from {10 −2 , 10 −1.5 , . . . , 1}.\n\n\nC.1.1 Consistency across various hyperparameters choices\n\nIn Table 1, we only presented the best hyperparameter choice (learning rate combinations). In this subsubsection, we show that the pattern of generalization gap is consistent across various hyperparameter choices. The result is shown in Figure 11.\n\n\nC.1.2 Effect of multiple local epochs per communication round\n\nIn the main experiments we by default let each sampled client run one local epoch every communication round. In this subsubsection, we evaluate the effect of multiple local epochs on the generalization performance. The result is shown in Figure 12.\n\n\nC.2 CIFAR 10/100 Image Classification Task\n\nFederated Dataset Preprocessing. The CIFAR-10 and CIFAR-100 datasets (Krizhevsky et al., 2009) are datasets of natural images distributed into 10 and 100 classes respectively. Since the dataset does not come with user assignment, we first shuffle the original dataset and assign to clients by applying our proposed semantic synthesized partitioning. The CIFAR-10 and CIFAR-100 dataset are partitioned into 300 and 100 clients, respectively. For three-way split, we hold out 20% (60 for CIFAR-10, 20 for CIFAR-100) clients as unparticipating clients, and leave the remaining client as participating clients. Within each participating client, we hold out 20% of data as (participating) validation data. participating training participating validation unparticipating Figure 11: Consistency of participation gaps across hyperparameter choice (learning rates configuration). We present the best four (4) combination of learning rates for federated training of EMNIST-10.\n\nHere η c stands for client learning rate, and η s stands for server learning rate. We observe that the participation gap is consistent across various configurations of learning rates. participating training participating validation unparticipating Figure 12: Effect of multiple client epochs per round on EMNIST-62. We repeat the experiment on EMNIST-62 but instead let each sampled client run multiple local epochs per communication round. The other settings (including the total communication rounds) remain the same. We observe that the participation gap is consistent across various settings of local epochs.\n\n\nModel, Optimizer, and Hyperparameters\n\nWe train a ResNet-18 (He et al., 2016) in which the batch normalization is replaced by group normalization (Wu and He, 2018) for improved stability in federated setting, as recommended by Hsieh et al. (2019). For centralized training, we run 200 epochs of SGD with momentum = 0.9 with batch size 50, and decay the learning rate by 5x every 60 epochs. The initial learning rate is tuned from {10 −2.5 , 10 −2 , . . . , 10 −0.5 }. For federated training, we run 2,000 rounds of FedAvgM  with server momentum = 0.9, and decay the server learning rate by 5x every 600 communication rounds. For each communication round, we uniformly sample 10 clients (for CIFAR-100) or 30 clients (for CIFAR-10), and let each client train for 1 local epoch with batch size 20. The client learning rate is tuned from {10 −2 , 10 −1.5 , . . . , 1}; the server learning rate is tuned from {10 −1.5 , 10 −1 , . . . , 10 0.5 }.\n\n\nC.2.1 Consistency across various hyperparameters choice\n\nIn the main result Table 1 we only present the best hyperparameter choice (learning rate combinations). In this subsubsection, we show that the pattern of generalization gap is consistent across hyperparameter choice. The result is shown in Figures 13 and 14. participating training participating validation unparticipating Figure 13: Consistency of participation gaps across hyperparameter choice (learning rates configuration). We present the best four (4) combination of learning rates for federated training of CIFAR-10.\n\nHere η c stands for client learning rate, and η s stands for server learning rate. We observe that the participation gap is largely consistent across various configurations of learning rates. participating training participating validation unparticipating Figure 14: Consistency of participation gaps across hyperparameter choice (learning rates configuration). We present the best four (4) combination of learning rates for federated training of CIFAR-100.\n\nHere η c stands for client learning rate, and η s stands for server learning rate. We observe that the participation gap is consistent across various configurations of learning rates.\n\n\nC.2.2 Effect of Weight Decay Strength\n\nIn the main experiments we by default set the weight decay of ResNet-18 to be 10 −4 . In this subsubsection, we experiment various other options of weight decay from 10 −5 to 10 −2\n\nThe result is shown in Figure 15. We observe that a moderate scale of weight decay might improve the unparticipating accuracy and therefore decrease the participation gap. However, an overlarge weight decay might hurt both participating validation and unparticipating performance.\n\n\nC.2.3 Effect of Model Depth\n\nIn the main experiments we by default train a ResNet-18 for the CIFAR task. In this subsubsection, we experiment a deeper model (ResNet-50) for the CIFAR-100. The result is shown in Figure 16. We federatedly train a ResNet-50 for CIFAR-100 to compare with our default choice (ResNet-18). We apply a constant learning rate (instead of step decay learning rate) for easy comparison. We observe that while using a deeper model improves the overall accuracy, the participation gap is still reasonably large for ResNet-50.\n\n\nC.3 Shakespeare Next Character Prediction Task\n\nFederated Dataset Description and Preprocessing. The Shakespeare dataset (Caldas et al., 2019) is a next character prediction dataset containing lines from the Complete Works of William Shakespeare where each client is a different character from one of the plays. We adopt the federated shakespeare dataset hosted by Tensorflow Federated (TFF). In TFF, the federated shakespeare dataset was by default split intra-cliently, namely all the clients appeared in both the \"training\" and \"validation\" dataset. To construct a three-way split, we hold out 20% of the total clients as unparticipating clients, and leave the remaining (80%) clients as participating clients (which gives the result reported in Table 1). Within each participating client, we keep the original training/validation split, e.g., the original training data that are assigned to these participating clients will become participating training data. We also tested the performance under other numbers of participating clients, as shown in Figure 7.\n\nModel, Optimizer, and Hyperparameters We train the same recurrent neural network as in . For centralized training, we run 30 epochs of Adam (with = 10 −4 ) with batch size 20. We tune the centralized learning rate from {10 −3 , 10 −2.5 , . . . , 10 −1 }. For federated training, we run 3,000 rounds of FedAdam  with server = 10 −4 . For each communictaion round, we uniformly sample 10 clients, and let each client train for 1 local epoch with batch size 10. Both client and server learning rates are tuned from {10 −2 , 10 −1.5 , . . . , 1}.\n\n\nC.4 Stackoverflow Next Word Prediction Task\n\nFederated Dataset Description and Preprocessing. The Stack Overflow dataset consists of questions and answers taken from the website Stack Overflow. Each client is a different user of the website. We adopt the stackoverflow dataset hosted by Tensorflow Federated (TFF). In TFF, the federated stackoverflow dataset is splitted inter-cliently, namely the training data and validation data belong to two disjoint subsets of clients.\n\nTo construct a three-way split, we will treat the original \"validation\" clients as unparticipating clients. Within each participating client, we randomly hold out the max of 20% or 1000 elements as (participating) validation data, and the max of 80% or 1000 elements as (participating) training data. Due to the abundance of stackoverflow data, we randomly sample a subset of clients from the original \"training\" clients as participating clients. The result shown in Table 1 is for the case with 3425 participating clients. We also tested other various levels of participating clients, shown in Figure 7.\n\nModel, Optimizer, and Hyperparameters We train the same recurent neural network as in . For centralized training, we run 30 epochs of Adam (with = 10 −4 ) with batch size 200. We tune the centralized learning rate from {10 −3 , 10 −2.5 , . . . , 10 −1.5 }. For federated training, we run 6,000 rounds of FedAdam  with server = 10 −4 . For each communictaion round, we randomly sample 100 clients, and let each client train for 1 local epoch with batch size 50. Both client and server learning rates are tuned from {10 −2 , 10 −1.5 , . . . , 1}. The client learning rate is tuned from {10 −3 , 10 −1.5 , . . . , 10 −1 }; the server learning rate is tuned from {10 −2 , 10 −1.5 , . . . , 1}.\n\n\nD Details of Semanticically Partitioned Federated Dataset\n\n\nD.1 Details of the Semantic Partitioning Scheme\n\nIn this section we provide the details of the proposed algorithm to semantically partition a federated dataset for CIFAR-10 and CIFAR-100. For clarify, we use K to denote the number of classes, and C to denote the number of clients partitioned into.\n\nThe first stage aims to cluster each label into C clusters.\n\n1. Embed the original inputs of dataset using a pretrained EfficientNetB3. This gives a embedding of dimension 1280 for each input.\n\n2. Reduce the dimension of the above embeddings to 256 dimensions via PCA. 7\n\n3. For each label, fit the corresponding input with a Gaussian mixture model with C clusters. This step yields C gaussian distribution for each of the K labels. Formally, we let D k c denote the (Gaussian) distribution of the cluster c of label k.\n\nThe second stage will package the clusters from different labels across clients. We aim to compute an optimal multi-partite matching with cost-matrix defined by KL-divergence between the Gaussian clusters. To reduce complexity, we heuristically solve the optimal multi-partite matching by progressively solving the optimal bipartite matching at each time for some randomly-chosen label pairs. Formally, we run the following procedure 1: Initialize S unmatched ← {1, . . . , K} 2: Randomly sample a label k from S unmatched , and remove k from S unmatched . 3: while S unmatched = ∅ do 4:\n\nRandomly sample a label k from S unmatched , and remove k from S unmatched .\n\n\n5:\n\nCompute a cost matrix A of dimension C × C, where A ij ← D KL (D k i ||D k j ).\n\n\n6:\n\nSolve and record the optimal bipartite matching with cost matrix A.\n\n\n7:\n\nSet k ← k 8: return the aggregation of all the bipartite matchings computed.\n\nD.2 Visualization of Semanticically Partitioned CIFAR-100 Dataset Figure 17: Visualization of semantic partitioning of CIFAR-100. We partition the CIFAR-100 dataset into 100 clients without resorting to external user information (such as writer identification). Here we show 10 out of 100 clients featuring the label \"apple\". Figure 18: Visualization of semantic partitioning of MNIST. We partition the (classic) MNIST dataset into 300 clients without resorting to external user information (such as writer identification). Here we show 5 out of 300 clients. Observe that the images within each client demonstrates consistent writing styles both within label and across labels.\n\n\nD.3 Visualization of Semantically Partitioned MNIST Dataset\n\n\nE Methodology for Computing Entropy\n\nWe hypothesize that a participation gap exists for naturally partitioned datasets and not for synthetically partitioned datasets because the naturally partitioned datasets inherently contain correlated inputs not drawn IID from the full data generating distribution. Put another way, the entropy of the input data for a given label from a naturally partitioned client is lower than the entropy for that same label from a synthetically partitioned client. To evaluate this claim, we need to (approximately) infer the data generating distribution for each client, and then measure the entropy of this distribution, defined as:\nH(q) = −E x∼q(x) log q(x)(6)\nTo infer the client data generating distribution, we used deep generative models. Because our clients possess relatively few training examples (O(10) for a particular class), many deep generative models such as Glow (Kingma and Dhariwal, 2018) or PixelCNN (Salimans et al., 2017) will not be able to learn a reasonable density model. We instead used a Variational Autoencoder (Kingma and Welling, 2013) to approximate the deep generative process. This model is significantly easier to train compared to the much larger generative models, but does not have tractable log-evidence measurement. Instead, models are trained by minimizing the negative Evidence Lower Bound (ELBO).\n\nWe filtered each client to contain data only for a single label. Because of the sparseness of the data after filtering, we found that a 2 dimensional latent space was sufficient to compress our data without significant losses. We used a Multivariate Normal distribution for our posterior and prior, and an Independent Bernoulli distribution for our likelihood. The posterior was given a full covariance matrix to account for correlations in the latent variable. All models were trained for 10 4 training steps.\n\nIn order to evaluate our models, we used a stochastic approximation to the log-evidence, given by a 1000 sample IWAE (Burda et al., 2015). IWAE is a lower bound on the Bayesian Evidence that becomes asymptotically tight when computed with a large number of samples. We evaluated the entropy for 100 clients from naturally partitioned, syntactically partitioned, and synthetically partitioned datasets, and computed the average across clients as our estimate for the client data entropy. We find that synthetic partitioning results in an average client entropy of 50 Nats, while Natural partitioning results in clients with only 40 Nats of entropy. Syntactic partitioning falls in between these two, having 45 Nats of entropy.\n\nFigure 1 :\n1Federated training results demonstrating participation gaps for six different tasks.\n\nFigure 4 :\n4Comparison of label-based synthetic partitioning and natural partitioning of EMNIST-10.\n\nFigure 5 :\n5Comparison of label-based partitioning and semantic partitioning (ours). Results for CIFAR-10 and CIFAR-100 are shown. Observe that semantic partitioning recovers the participation gap typically observed in naturally-partitioned data.\n\nFigure 6 :\n6Kernel density estimates of the distribution of client entropy for naturally-partitioned clients (top), semantic-partitioned clients (middle), and label-based partitioned clients (bottom).\n\nFigure 7 :\n7Effect of the number of participating clients. See Section 5.1 for discussion.\n\nFigure 9 :\n9Centralized training progress on six different federated tasks.\n\nFigure 10 :\n10Accuracies of the client at the 25 th percentile versus the communication rounds.\n\nFigure 15 :\n15Effect of 2 weight decay on CIFAR-100 training. We federated train the ResNet-18 networks for CIFAR-100 with various levels of weight decay ranging from 10 −5 to 10 −2 .\n\nFigure 16 :\n16Effect of a deeper ResNet on CIFAR-100 training.\n\nTable 1 :\n1Summary of experimental results.We perform federated and centralized training across six \n\n\n\nTraining progress of centralized optimizers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 B.2 Percentiles of metrics across clients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 B.3 Federated training progress at the 25 th percentile acorss clients . . . . . . . . . . . . . . . . . 18 C Additional Details on Experimental Setup and Task-Specific Experiments 20 C.1 EMNIST Hand-written Character Recognition Task . . . . . . . . . . . . . . . . . . . . . . . 20 C.1.1 Consistency across various hyperparameters choices . . . . . . . . . . . . . . . . . . . 20 C.1.2 Effect of multiple local epochs per communication round . . . . . . . . . . . . . . . . . 20 C.2 CIFAR 10/100 Image Classification Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 C.2.1 Consistency across various hyperparameters choice . . . . . . . . . . . . . . . . . . . . 22 C.2.2 Effect of Weight Decay Strength . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 C.2.3 Effect of Model Depth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 C.3 Shakespeare Next Character Prediction Task . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 C.4 Stackoverflow Next Word Prediction Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 D Details of Semanticically Partitioned Federated Dataset 24 D.1 Details of the Semantic Partitioning Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 D.2 Visualization of Semanticically Partitioned CIFAR-100 Dataset . . . . . . . . . . . . . . . . . 25 D.3 Visualization of Semantically Partitioned MNIST Dataset . . . . . . . . . . . . . . . . . . . . 25A Additional Related Work \n17 \n\nB Additional Experimental Results \n17 \nB.1 E Methodology for Computing Entropy \n26 \n\nA Additional Related Work \net al., 2021) for a more comprehensive survey on the recent progress in Federated Learning. \n\n\n\nTable 2 :\n2\nTo see this, observe that inter-client split can only estimate F part_train and Funpart, and intra-client split can only estimate F part_train and F part_val .\nTo avoid confusion, throughout this work, we use the term \"partition\" to refer to assigning data with no client assignment into synthetic clients. The term \"split\" refers to splitting a federated dataset (with existing client assignments) to measure different metrics (e.g., three-way-split).\nOne can make the above claim rigorous with standard learning theory approaches such as uniform convergence and Rademacher complexity(Vapnik, 1998).\nIn addition, we experimented with FedYogi on these tasks. The performance is comparable (in terms of both participating validation and unparticipating metrics). We also experimented with vanilla FedAvg and FedAdagrad, which are less effective than the other adaptive optimizers in these settings, but the participation gaps are generally consistent.\nPlease visit https://bit.ly/fl-generalization for the code repository.\nTo make the percentiles comparable, we ensure the un-participating clients and participating validation clients have the same scale of elements per client.\nReducing the dimension is purely a computational issue since the original embedding dimension (1280) is too large for downstream procedures such as GMM fitting and optimal matching (measured by KL divergence). While there may be other complicated dimension reduction technique, we found PCA to be simple enough to generate reasonable results. The dimension of 256 is a trade-off of (down-stream) computational complexity and embedding information.\nAcknowledgementsWe would like to thank Zachary Charles, Zachary Garrett, Zheng Xu, Keith Rush, Hang Qi, Brendan  McMahan, Josh Dillon, and Sushant  Prakash for helpful discussions at various stages of this work.ReferencesAlekh Agarwal, John Langford, and Chen-Yu Wei. Federated Residual Learning. arXiv:2003.12880 [cs, stat]\nProvable guarantees for gradient-based meta-learning. Maria-Florina Balcan, Mikhail Khodak, Ameet Talwalkar, PMLRProceedings of the 36th International Conference on Machine Learning. the 36th International Conference on Machine Learning97Maria-Florina Balcan, Mikhail Khodak, and Ameet Talwalkar. Provable guarantees for gradient-based meta-learning. In Proceedings of the 36th International Conference on Machine Learning, volume 97. PMLR, 2019.\n\nOm Dipakbhai Thakkar, and Abhradeep Thakurta. Privacy amplification via random check-ins. Borja Balle, Peter Kairouz, Brendan Mcmahan, Advances in Neural Information Processing Systems. 33Borja Balle, Peter Kairouz, Brendan McMahan, Om Dipakbhai Thakkar, and Abhradeep Thakurta. Privacy amplification via random check-ins. In Advances in Neural Information Processing Systems 33, volume 33, 2020.\n\nQsparse-local-sgd: Distributed SGD with quantization, sparsification and local computations. Debraj Basu, Deepesh Data, Can Karakus, Suhas Diggavi, Advances in Neural Information Processing Systems. Curran Associates, Inc32Debraj Basu, Deepesh Data, Can Karakus, and Suhas Diggavi. Qsparse-local-sgd: Distributed SGD with quantization, sparsification and local computations. In Advances in Neural Information Processing Systems 32. Curran Associates, Inc., 2019.\n\nAnalysis of representations for domain adaptation. Shai Ben-David, John Blitzer, Koby Crammer, Fernando Pereira, Advances in neural information processing systems. 19137Shai Ben-David, John Blitzer, Koby Crammer, Fernando Pereira, et al. Analysis of representations for domain adaptation. Advances in neural information processing systems, 19:137, 2007.\n\nOn Biased Compression for Distributed Learning. Aleksandr Beznosikov, Samuel Horváth, Peter Richtárik, Mher Safaryan, arXiv:2002.124102020cs, math, statAleksandr Beznosikov, Samuel Horváth, Peter Richtárik, and Mher Safaryan. On Biased Compression for Distributed Learning. arXiv:2002.12410 [cs, math, stat], 2020.\n\nDistributed Distillation for On-Device Learning. Ilai Bistritz, Ariana Mann, Nicholas Bambos, Advances in Neural Information Processing Systems. 33Ilai Bistritz, Ariana Mann, and Nicholas Bambos. Distributed Distillation for On-Device Learning. In Advances in Neural Information Processing Systems 33, volume 33, 2020.\n\nGavin Brown, Mark Bun, Vitaly Feldman, Adam Smith, Kunal Talwar, arXiv:2012.06421When is Memorization of Irrelevant Training Data Necessary for High-Accuracy Learning. 2020Gavin Brown, Mark Bun, Vitaly Feldman, Adam Smith, and Kunal Talwar. When is Memorization of Irrelevant Training Data Necessary for High-Accuracy Learning? arXiv:2012.06421 [cs], 2020.\n\n. Yuri Burda, Roger Grosse, Ruslan Salakhutdinov, arXiv:1509.00519Importance weighted autoencoders. arXiv preprintYuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv preprint arXiv:1509.00519, 2015.\n\nLEAF: A Benchmark for Federated Settings. Sebastian Caldas, Sai Meher Karthik, Peter Duddu, Tian Wu, Jakub Li, H Brendan Konečný, Virginia Mcmahan, Ameet Smith, Talwalkar, NeurIPS 2019 Workshop on Federated Learning for Data Privacy and Confidentiality. Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konečný, H. Brendan McMahan, Virginia Smith, and Ameet Talwalkar. LEAF: A Benchmark for Federated Settings. In NeurIPS 2019 Workshop on Federated Learning for Data Privacy and Confidentiality, 2019.\n\nOn the outsized importance of learning rates in local update methods. Zachary Charles, Jakub Konečnỳ, arXiv:2007.00878arXiv preprintZachary Charles and Jakub Konečnỳ. On the outsized importance of learning rates in local update methods. arXiv preprint arXiv:2007.00878, 2020.\n\n. Fei Chen, Mi Luo, Zhenhua Dong, Zhenguo Li, Xiuqiang He, arXiv:1802.07876Federated Meta-Learning with Fast Convergence and Efficient Communication. Fei Chen, Mi Luo, Zhenhua Dong, Zhenguo Li, and Xiuqiang He. Federated Meta-Learning with Fast Convergence and Efficient Communication. arXiv:1802.07876 [cs], 2019.\n\nFedBE: Making Bayesian Model Ensemble Applicable to Federated Learning. Hong-You Chen, Wei-Lun Chao, International Conference on Learning Representations. Hong-You Chen and Wei-Lun Chao. FedBE: Making Bayesian Model Ensemble Applicable to Federated Learning. In International Conference on Learning Representations, 2021.\n\nUnderstanding gradient clipping in private SGD: A geometric perspective. Xiangyi Chen, Steven Z Wu, Mingyi Hong, Advances in Neural Information Processing Systems. 33Xiangyi Chen, Steven Z. Wu, and Mingyi Hong. Understanding gradient clipping in private SGD: A geometric perspective. In Advances in Neural Information Processing Systems 33, 2020.\n\nEMNIST: An extension of MNIST to handwritten letters. Gregory Cohen, Saeed Afshar, Jonathan Tapson, André Van Schaik, abs/1702.05373CoRRGregory Cohen, Saeed Afshar, Jonathan Tapson, and André van Schaik. EMNIST: An extension of MNIST to handwritten letters. CoRR, abs/1702.05373, 2017.\n\nHal Daumé, Iii , arXiv:0907.1815Frustratingly easy domain adaptation. arXiv preprintHal Daumé III. Frustratingly easy domain adaptation. arXiv preprint arXiv:0907.1815, 2009.\n\n. Yuyang Deng, Mohammad Mahdi Kamani, Mehrdad Mahdavi, arXiv:2003.13461Adaptive Personalized Federated Learning. 2020cs, statYuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive Personalized Federated Learning. arXiv:2003.13461 [cs, stat], 2020.\n\nHeterofl: Computation and communication efficient federated learning for heterogeneous clients. Enmao Diao, Jie Ding, Vahid Tarokh, arXiv:2010.01264arXiv preprintEnmao Diao, Jie Ding, and Vahid Tarokh. Heterofl: Computation and communication efficient federated learning for heterogeneous clients. arXiv preprint arXiv:2010.01264, 2020.\n\nAdaptive gradient quantization for data-parallel SGD. Fartash Faghri, Iman Tabrizian, Ilia Markov, Dan Alistarh, M Daniel, Ali Roy, Ramezani-Kebrya, Advances in Neural Information Processing Systems. Curran Associates, Inc33Fartash Faghri, Iman Tabrizian, Ilia Markov, Dan Alistarh, Daniel M Roy, and Ali Ramezani-Kebrya. Adaptive gradient quantization for data-parallel SGD. In Advances in Neural Information Processing Systems, volume 33. Curran Associates, Inc., 2020.\n\nPersonalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach. Alireza Fallah, Aryan Mokhtari, Asuman E Ozdaglar, Advances in Neural Information Processing Systems. 33Alireza Fallah, Aryan Mokhtari, and Asuman E. Ozdaglar. Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach. In Advances in Neural Information Processing Systems 33, 2020.\n\nInverting Gradients -How easy is it to break privacy in federated learning?. Jonas Geiping, Hartmut Bauermeister, Hannah Dröge, Michael Moeller, Advances in Neural Information Processing Systems. 33Jonas Geiping, Hartmut Bauermeister, Hannah Dröge, and Michael Moeller. Inverting Gradients -How easy is it to break privacy in federated learning? In Advances in Neural Information Processing Systems 33, 2020.\n\nSharp bounds for federated averaging (local sgd) and continuous perspective. Margalit Glasgow, Honglin Yuan, Tengyu Ma, arXiv:2111.03741arXiv preprintMargalit Glasgow, Honglin Yuan, and Tengyu Ma. Sharp bounds for federated averaging (local sgd) and continuous perspective. arXiv preprint arXiv:2111.03741, 2021.\n\nLinearly converging error compensated SGD. Eduard Gorbunov, Dmitry Kovalev, Dmitry Makarenko, Peter Richtárik, Advances in Neural Information Processing Systems. 33Eduard Gorbunov, Dmitry Kovalev, Dmitry Makarenko, and Peter Richtárik. Linearly converging error compensated SGD. In Advances in Neural Information Processing Systems 33, volume 33, 2020.\n\nNIST Handprinted Forms and Characters, NIST Special Database 19. J Patrick, Patricia A Grother, Flanagan, Patrick J. Grother and Patricia A. Flanagan. NIST Handprinted Forms and Characters, NIST Special Database 19., 1995.\n\nTrading redundancy for communication: Speeding up distributed SGD for non-convex optimization. Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, Viveck Cadambe, PMLRProceedings of the 36th International Conference on Machine Learning. the 36th International Conference on Machine Learning97Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Viveck Cadambe. Trading redundancy for communication: Speeding up distributed SGD for non-convex optimization. In Proceedings of the 36th International Conference on Machine Learning, volume 97. PMLR, 2019a.\n\nLocal SGD with periodic averaging: Tighter analysis and adaptive synchronization. Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, Viveck Cadambe, Advances in Neural Information Processing Systems. Curran Associates, Inc32Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Viveck Cadambe. Local SGD with periodic averaging: Tighter analysis and adaptive synchronization. In Advances in Neural Information Processing Systems 32. Curran Associates, Inc., 2019b.\n\nFederated Learning of a Mixture of Global and Local Models. Filip Hanzely, Peter Richtárik, arXiv:2002.055162020cs, math, statFilip Hanzely and Peter Richtárik. Federated Learning of a Mixture of Global and Local Models. arXiv:2002.05516 [cs, math, stat], 2020.\n\nLower bounds and optimal algorithms for personalized federated learning. Filip Hanzely, Slavomír Hanzely, Samuel Horváth, Peter Richtárik, Advances in Neural Information Processing Systems. 33Filip Hanzely, Slavomír Hanzely, Samuel Horváth, and Peter Richtárik. Lower bounds and optimal algorithms for personalized federated learning. In Advances in Neural Information Processing Systems 33, 2020.\n\nWeituo Hao, Nikhil Mehta, Kevin J Liang, Pengyu Cheng, Mostafa El-Khamy, Lawrence Carin, arXiv:2008.05687WAFFLe: Weight Anonymized Factorization for Federated Learning. 2020cs, statWeituo Hao, Nikhil Mehta, Kevin J. Liang, Pengyu Cheng, Mostafa El-Khamy, and Lawrence Carin. WAFFLe: Weight Anonymized Factorization for Federated Learning. arXiv:2008.05687 [cs, stat], 2020.\n\nGroup Knowledge Transfer: Federated Learning of Large CNNs at the Edge. Chaoyang He, Murali Annavaram, Salman Avestimehr, Advances in Neural Information Processing Systems. 33Chaoyang He, Murali Annavaram, and Salman Avestimehr. Group Knowledge Transfer: Federated Learning of Large CNNs at the Edge. In Advances in Neural Information Processing Systems 33, volume 33, 2020.\n\nDeep Residual Learning for Image Recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n\nA Better Alternative to Error Feedback for Communication-Efficient Distributed Learning. Samuel Horváth, Peter Richtárik, arXiv:2006.110772020cs, statSamuel Horváth and Peter Richtárik. A Better Alternative to Error Feedback for Communication-Efficient Distributed Learning. arXiv:2006.11077 [cs, stat], 2020.\n\nKevin Hsieh, Amar Phanishayee, Onur Mutlu, Phillip B Gibbons, arXiv:1910.00189The Non-IID Data Quagmire of Decentralized Machine Learning. cs, statKevin Hsieh, Amar Phanishayee, Onur Mutlu, and Phillip B. Gibbons. The Non-IID Data Quagmire of Decentralized Machine Learning. arXiv:1910.00189 [cs, stat], 2019.\n\nMeasuring the Effects of Non-Identical Data Distribution for Federated Visual Classification. Tzu-Ming Harry Hsu, Hang Qi, Matthew Brown, arXiv:1909.06335cs, statTzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the Effects of Non-Identical Data Distribution for Federated Visual Classification. arXiv:1909.06335 [cs, stat], 2019.\n\nFL-NTK: A neural tangent kernel-based framework for federated learning analysis. Baihe Huang, Xiaoxiao Li, Zhao Song, Xin Yang, PMLRProceedings of the 38th International Conference on Machine Learning. the 38th International Conference on Machine Learning139Baihe Huang, Xiaoxiao Li, Zhao Song, and Xin Yang. FL-NTK: A neural tangent kernel-based framework for federated learning analysis. In Proceedings of the 38th International Conference on Machine Learning, volume 139. PMLR, 2021.\n\nIntroducing TensorFlow Federated. Alex Ingerman, Krzys Ostrowski, Alex Ingerman and Krzys Ostrowski. Introducing TensorFlow Federated, 2019.\n\nDistributed Second Order Methods with Fast Rates and Compressed Communication. Rustem Islamov, Xun Qian, Peter Richtárik, ICML 2021. Rustem Islamov, Xun Qian, and Peter Richtárik. Distributed Second Order Methods with Fast Rates and Compressed Communication. In ICML 2021, 2021.\n\nImproving Federated Learning Personalization via Model Agnostic Meta Learning. Yihan Jiang, Jakub Konečný, Keith Rush, Sreeram Kannan, arXiv:1909.12488cs, statYihan Jiang, Jakub Konečný, Keith Rush, and Sreeram Kannan. Improving Federated Learning Personalization via Model Agnostic Meta Learning. arXiv:1909.12488 [cs, stat], 2019.\n\nYuang Jiang, Shiqiang Wang, Victor Valls, Bong Jun Ko, Wei-Han Lee, Kin K Leung, Leandros Tassiulas, arXiv:1909.12326Model Pruning Enables Efficient Federated Learning on Edge Devices. 2020cs, statYuang Jiang, Shiqiang Wang, Victor Valls, Bong Jun Ko, Wei-Han Lee, Kin K. Leung, and Leandros Tassiulas. Model Pruning Enables Efficient Federated Learning on Edge Devices. arXiv:1909.12326 [cs, stat], 2020.\n\nH Brendan Peter Kairouz, Brendan Mcmahan, Aurélien Avent, Mehdi Bellet, Arjun Nitin Bennis, Keith Bhagoji, Zachary Bonawitz, Graham Charles, Rachel Cormode, Cummings, G L Rafael, Salim El Oliveira, David Rouayheb, Josh Evans, Zachary Gardner, Adrià Garrett, Badih Gascón, Phillip B Ghazi, Marco Gibbons, Zaid Gruteser, Chaoyang Harchaoui, Lie He, Zhouyuan He, Ben Huo, Justin Hutchinson, Martin Hsu, Tara Jaggi, Gauri Javidi, Mikhail Joshi, Jakub Khodak, Aleksandra Konečný, Farinaz Korolova, Sanmi Koushanfar, Tancrède Koyejo, Yang Lepoint, Prateek Liu, Mehryar Mittal, Richard Mohri, ; Jianyu Nock, Li Wang, Zheng Xiong, Qiang Xu, Felix X Yang, Han Yu, Sen Yu, Zhao, arXiv:1912.04977Advances and Open Problems in Federated Learning. Rasmus Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian TramèrAyfer ÖzgürPraneeth Vepakomma,. cs, statPeter Kairouz, H. Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D'Oliveira, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adrià Gascón, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Konečný, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancrède Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer Özgür, Rasmus Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramèr, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances and Open Problems in Federated Learning. arXiv:1912.04977 [cs, stat], 2019.\n\nSCAFFOLD: Stochastic Controlled Averaging for Federated Learning. Satyen Sai Praneeth Karimireddy, Mehryar Kale, Mohri, J Sashank, Sebastian U Reddi, Ananda Theertha Stich, Suresh, Proceedings of the International Conference on Machine Learning 1 Pre-Proceedings (ICML 2020. the International Conference on Machine Learning 1 Pre-Proceedings (ICML 20202020Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J. Reddi, Sebastian U. Stich, and Ananda Theertha Suresh. SCAFFOLD: Stochastic Controlled Averaging for Federated Learning. In Proceedings of the International Conference on Machine Learning 1 Pre-Proceedings (ICML 2020), 2020.\n\nTighter Theory for Local SGD on Identical and Heterogeneous Data. Ahmed Khaled, Konstantin Mishchenko, Peter Richtárik, PMLRProceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics. the Twenty Third International Conference on Artificial Intelligence and Statistics108Ahmed Khaled, Konstantin Mishchenko, and Peter Richtárik. Tighter Theory for Local SGD on Identical and Heterogeneous Data. In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108. PMLR, 2020.\n\nAdaptive Gradient-Based Meta-Learning Methods. Mikhail Khodak, Maria-Florina F Balcan, Ameet S Talwalkar, Advances in Neural Information Processing Systems. Curran Associates, Inc32Mikhail Khodak, Maria-Florina F Balcan, and Ameet S Talwalkar. Adaptive Gradient-Based Meta-Learning Methods. In Advances in Neural Information Processing Systems 32, volume 32. Curran Associates, Inc., 2019.\n\nP Diederik, Prafulla Kingma, Dhariwal, arXiv:1807.03039Glow: Generative flow with invertible 1x1 convolutions. arXiv preprintDiederik P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. arXiv preprint arXiv:1807.03039, 2018.\n\nAuto-encoding variational bayes. P Diederik, Max Kingma, Welling, arXiv:1312.6114arXiv preprintDiederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\n\nA Unified Theory of Decentralized SGD with Changing Topology and Local Updates. Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, Sebastian U Stich, Proceedings of the International Conference on Machine Learning 1 Pre-Proceedings (ICML 2020. the International Conference on Machine Learning 1 Pre-Proceedings (ICML 20202020Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian U. Stich. A Unified Theory of Decentralized SGD with Changing Topology and Local Updates. In Proceedings of the International Conference on Machine Learning 1 Pre-Proceedings (ICML 2020), 2020.\n\nJakub Konečný, H Brendan Mcmahan, Daniel Ramage, Peter Richtárik, arXiv:1610.02527Federated Optimization: Distributed Machine Learning for On-Device Intelligence. Jakub Konečný, H. Brendan McMahan, Daniel Ramage, and Peter Richtárik. Federated Optimization: Distributed Machine Learning for On-Device Intelligence. arXiv:1610.02527 [cs], 2016.\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, Geoffrey Hinton, Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\nSimple and scalable predictive uncertainty estimation using deep ensembles. Balaji Lakshminarayanan, Alexander Pritzel, Charles Blundell, arXiv:1612.01474arXiv preprintBalaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. arXiv preprint arXiv:1612.01474, 2016.\n\nFederated Learning: Challenges, Methods, and Future Directions. Tian Li, Anit Kumar Sahu, Ameet Talwalkar, Virginia Smith, IEEE Signal Processing Magazine. 3732020Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated Learning: Challenges, Methods, and Future Directions. IEEE Signal Processing Magazine, 37(3), 2020a.\n\nFederated optimization in heterogeneous networks. Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, Virginia Smith, Proceedings of Machine Learning and Systems 2020. Machine Learning and Systems 2020Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. In Proceedings of Machine Learning and Systems 2020, 2020b.\n\nFair resource allocation in federated learning. Tian Li, Maziar Sanjabi, Ahmad Beirami, Virginia Smith, International Conference on Learning Representations. Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. Fair resource allocation in federated learning. In International Conference on Learning Representations, 2020c.\n\nOn the convergence of FedAvg on non-iid data. Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, Zhihua Zhang, International Conference on Learning Representations. Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of FedAvg on non-iid data. In International Conference on Learning Representations, 2020d.\n\nFedBN: Federated learning on Non-IID features via local batch normalization. Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, Qi Dou, International Conference on Learning Representations. Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. FedBN: Federated learning on Non-IID features via local batch normalization. In International Conference on Learning Representations, 2021.\n\nTerrance Paul Pu Liang, Liu Liu, Nicholas B Ziyin, Randy P Allen, David Auerbach, Ruslan Brent, Louis-Philippe Salakhutdinov, Morency, arXiv:2001.01523Think Locally, Act Globally: Federated Learning with Local and Global Representations. 2020cs, statPaul Pu Liang, Terrance Liu, Liu Ziyin, Nicholas B. Allen, Randy P. Auerbach, David Brent, Ruslan Salakhutdinov, and Louis-Philippe Morency. Think Locally, Act Globally: Federated Learning with Local and Global Representations. arXiv:2001.01523 [cs, stat], 2020.\n\nEnsemble Distillation for Robust Model Fusion in Federated Learning. Tao Lin, Lingjing Kong, Sebastian U Stich, Martin Jaggi, Advances in Neural Information Processing Systems. 33Tao Lin, Lingjing Kong, Sebastian U. Stich, and Martin Jaggi. Ensemble Distillation for Robust Model Fusion in Federated Learning. In Advances in Neural Information Processing Systems 33, 2020.\n\nPAC Identifiability in Federated Personalization. Ben London, NeurIPS 2020 Workshop on Scalability, Privacy and Security in Federated Learning (SpicyFL). 2020Ben London. PAC Identifiability in Federated Personalization. In NeurIPS 2020 Workshop on Scalability, Privacy and Security in Federated Learning (SpicyFL), 2020.\n\nCommunication-efficient learning of deep networks from decentralized data. Brendan Mcmahan, Eider Moore, Daniel Ramage, Seth Hampson, Blaise Aguera Y Arcas, Proceedings of the 20th International Conference on Artificial Intelligence and Statistics. the 20th International Conference on Artificial Intelligence and StatisticsPMLR54Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54. PMLR, 2017.\n\nAgnostic federated learning. Mehryar Mohri, Gary Sivek, Ananda Theertha Suresh, PMLRProceedings of the 36th International Conference on Machine Learning. the 36th International Conference on Machine Learning97Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In Proceedings of the 36th International Conference on Machine Learning, volume 97. PMLR, 2019.\n\n. Alex Nichol, cs]Joshua Achiam, cs]John Schulman, cs]arXiv:1803.02999On First-Order Meta-Learning Algorithms. Alex Nichol, Joshua Achiam, and John Schulman. On First-Order Meta-Learning Algorithms. arXiv:1803.02999 [cs], 2018.\n\nCan you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua V Dillon, Balaji Lakshminarayanan, Jasper Snoek, arXiv:1906.02530arXiv preprintYaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua V Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. arXiv preprint arXiv:1906.02530, 2019.\n\nVisual domain adaptation: A survey of recent advances. M Vishal, Raghuraman Patel, Ruonan Gopalan, Rama Li, Chellappa, IEEE signal processing magazine. 323Vishal M Patel, Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Visual domain adaptation: A survey of recent advances. IEEE signal processing magazine, 32(3):53-69, 2015.\n\nFedSplit: An algorithmic framework for fast federated optimization. Reese Pathak, Martin J Wainwright, Advances in Neural Information Processing Systems. 33Reese Pathak and Martin J. Wainwright. FedSplit: An algorithmic framework for fast federated optimization. In Advances in Neural Information Processing Systems 33, 2020.\n\nAdaptive Federated Optimization. Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konečný, Sanjiv Kumar, H Brendan Mcmahan, International Conference on Learning Representations. Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konečný, Sanjiv Kumar, and H. Brendan McMahan. Adaptive Federated Optimization. In International Conference on Learning Representations, 2021.\n\nRobust federated learning: The case of affine distribution shifts. Amirhossein Reisizadeh, Farzan Farnia, Ramtin Pedarsani, Ali Jadbabaie, arXiv:2006.08907arXiv preprintAmirhossein Reisizadeh, Farzan Farnia, Ramtin Pedarsani, and Ali Jadbabaie. Robust federated learning: The case of affine distribution shifts. arXiv preprint arXiv:2006.08907, 2020.\n\nPixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. Tim Salimans, Andrej Karpathy, Xi Chen, Diederik P Kingma, arXiv:1701.05517arXiv preprintTim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. arXiv preprint arXiv:1701.05517, 2017.\n\nImproving predictive inference under covariate shift by weighting the log-likelihood function. Hidetoshi Shimodaira, Journal of statistical planning and inference. 902Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood function. Journal of statistical planning and inference, 90(2):227-244, 2000.\n\nFederated Reconstruction: Partially Local Federated Learning. Karan Singhal, Hakim Sidahmed, Zachary Garrett, Shanshan Wu, Keith Rush, Sushant Prakash, Advances in Neural Information Processing Systems. Karan Singhal, Hakim Sidahmed, Zachary Garrett, Shanshan Wu, Keith Rush, and Sushant Prakash. Federated Reconstruction: Partially Local Federated Learning. Advances in Neural Information Processing Systems, 2021.\n\nFederated multi-task learning. Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, Ameet S Talwalkar, Advances in Neural Information Processing Systems. Curran Associates, Inc30Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. Federated multi-task learning. In Advances in Neural Information Processing Systems 30. Curran Associates, Inc., 2017.\n\nA Scalable Approach for Privacy-Preserving Collaborative Machine Learning. Jinhyun So, Basak Guler, Salman Avestimehr, Advances in Neural Information Processing Systems. 33Jinhyun So, Basak Guler, and Salman Avestimehr. A Scalable Approach for Privacy-Preserving Collaborative Machine Learning. In Advances in Neural Information Processing Systems 33, 2020.\n\nElection coding for distributed learning: Protecting SignSGD against byzantine attacks. Dong-Jun Jy-Yong Sohn, Beongjun Han, Jaekyun Choi, Moon, Advances in Neural Information Processing Systems. 33Jy-yong Sohn, Dong-Jun Han, Beongjun Choi, and Jaekyun Moon. Election coding for distributed learning: Protecting SignSGD against byzantine attacks. In Advances in Neural Information Processing Systems 33, 2020.\n\nLocal SGD converges fast and communicates little. U Sebastian, Stich, International Conference on Learning Representations. Sebastian U. Stich. Local SGD converges fast and communicates little. In International Conference on Learning Representations, 2019.\n\nPersonalized Federated Learning with Moreau Envelopes. T Canh, Nguyen Dinh, Tuan Dung Tran, Nguyen, Advances in Neural Information Processing Systems. 33Canh T. Dinh, Nguyen Tran, and Tuan Dung Nguyen. Personalized Federated Learning with Moreau Envelopes. In Advances in Neural Information Processing Systems 33, 2020.\n\nVisualizing data using t-sne. Laurens Van Der Maaten, Geoffrey Hinton, Journal of machine learning research. 911Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9 (11), 2008.\n\nVladimir Naumovich Vapnik, Statistical Learning Theory. WileyVladimir Naumovich Vapnik. Statistical Learning Theory. Wiley, 1998.\n\nCooperative SGD: A unified Framework for the Design and Analysis of Communication-Efficient SGD Algorithms. Jianyu Wang, Gauri Joshi, arXiv:1808.07576cs, stat]Jianyu Wang and Gauri Joshi. Cooperative SGD: A unified Framework for the Design and Analysis of Communication- Efficient SGD Algorithms. arXiv:1808.07576 [cs, stat], 2018.\n\nSlowMo: Improving communication-efficient distributed SGD with slow momentum. Jianyu Wang, Vinayak Tantia, Nicolas Ballas, Michael Rabbat, International Conference on Learning Representations. Jianyu Wang, Vinayak Tantia, Nicolas Ballas, and Michael Rabbat. SlowMo: Improving communication-efficient distributed SGD with slow momentum. In International Conference on Learning Representations, 2020a.\n\nJianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, Maruan H Brendan Mcmahan, Galen Al-Shedivat, Salman Andrew, Katharine Avestimehr, Daly, arXiv:2107.06917Deepesh Data, et al. A field guide to federated optimization. arXiv preprintJianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A field guide to federated optimization. arXiv preprint arXiv:2107.06917, 2021.\n\nFederated Evaluation of On-device Personalization. Kangkang Wang, Rajiv Mathews, Chloé Kiddon, Hubert Eichner, Françoise Beaufays, Daniel Ramage, arXiv:1910.10252cs, statKangkang Wang, Rajiv Mathews, Chloé Kiddon, Hubert Eichner, Françoise Beaufays, and Daniel Ramage. Federated Evaluation of On-device Personalization. arXiv:1910.10252 [cs, stat], 2019.\n\nA Principled Approach to Data Valuation for Federated Learning. Tianhao Wang, Johannes Rausch, Ce Zhang, Ruoxi Jia, Dawn Song, Federated Learning. Springer International Publishing12500Tianhao Wang, Johannes Rausch, Ce Zhang, Ruoxi Jia, and Dawn Song. A Principled Approach to Data Valuation for Federated Learning. In Federated Learning, volume 12500. Springer International Publishing, 2020b.\n\nMinibatch vs Local SGD for Heterogeneous Distributed Learning. Blake Woodworth, Nathan Kumar Kshitij Patel, Srebro, Advances in Neural Information Processing Systems. 33Blake Woodworth, Kumar Kshitij Patel, and Nathan Srebro. Minibatch vs Local SGD for Heterogeneous Distributed Learning. In Advances in Neural Information Processing Systems 33, 2020.\n\nGroup normalization. Yuxin Wu, Kaiming He, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European Conference on Computer Vision (ECCV), 2018.\n\nInformation-Theoretic Bounds on the Generalization Error and Privacy Leakage in Federated Learning. Semih Yagli, Alex Dytso, H Vincent Poor, 2020 IEEE 21st International Workshop on Signal Processing Advances in Wireless Communications (SPAWC). IEEESemih Yagli, Alex Dytso, and H. Vincent Poor. Information-Theoretic Bounds on the Generalization Error and Privacy Leakage in Federated Learning. In 2020 IEEE 21st International Workshop on Signal Processing Advances in Wireless Communications (SPAWC). IEEE, 2020.\n\nOn the computation and communication complexity of parallel SGD with dynamic batch sizes for stochastic non-convex optimization. Hao Yu, Rong Jin, PMLRProceedings of the 36th International Conference on Machine Learning. the 36th International Conference on Machine Learning97Hao Yu and Rong Jin. On the computation and communication complexity of parallel SGD with dynamic batch sizes for stochastic non-convex optimization. In Proceedings of the 36th International Conference on Machine Learning, volume 97. PMLR, 2019.\n\nOn the linear speedup analysis of communication efficient momentum SGD for distributed non-convex optimization. Hao Yu, Rong Jin, Sen Yang, PMLRProceedings of the 36th International Conference on Machine Learning. the 36th International Conference on Machine Learning97Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication efficient momentum SGD for distributed non-convex optimization. In Proceedings of the 36th International Conference on Machine Learning, volume 97. PMLR, 2019.\n\nTao Yu, Eugene Bagdasaryan, Vitaly Shmatikov, arXiv:2002.04758Salvaging Federated Learning by Local Adaptation. 2020cs, statTao Yu, Eugene Bagdasaryan, and Vitaly Shmatikov. Salvaging Federated Learning by Local Adaptation. arXiv:2002.04758 [cs, stat], 2020.\n\nFederated Accelerated Stochastic Gradient Descent. Honglin Yuan, Tengyu Ma, Advances in Neural Information Processing Systems. 33Honglin Yuan and Tengyu Ma. Federated Accelerated Stochastic Gradient Descent. In Advances in Neural Information Processing Systems 33, 2020.\n\nFederated Composite Optimization. Honglin Yuan, Manzil Zaheer, Sashank Reddi, Proceedings of the 38th International Conference on Machine Learning. the 38th International Conference on Machine LearningHonglin Yuan, Manzil Zaheer, and Sashank Reddi. Federated Composite Optimization. In Proceedings of the 38th International Conference on Machine Learning, 2021.\n\nFedPD: A Federated Learning Framework with Optimal Rates and Adaptivity to Non-IID Data. Xinwei Zhang, Mingyi Hong, Sairaj Dhople, Wotao Yin, Yang Liu, arXiv:2005.114182020cs, statXinwei Zhang, Mingyi Hong, Sairaj Dhople, Wotao Yin, and Yang Liu. FedPD: A Federated Learning Framework with Optimal Rates and Adaptivity to Non-IID Data. arXiv:2005.11418 [cs, stat], 2020.\n\nYue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, Vikas Chandra, arXiv:1806.00582Federated Learning with Non-IID Data. cs, stat]Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated Learning with Non-IID Data. arXiv:1806.00582 [cs, stat], 2018.\n\nOn the convergence properties of a k-step averaging stochastic gradient descent algorithm for nonconvex optimization. Fan Zhou, Guojing Cong, Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence. the Twenty-Seventh International Joint Conference on Artificial IntelligenceFan Zhou and Guojing Cong. On the convergence properties of a k-step averaging stochastic gradient descent algorithm for nonconvex optimization. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, 2018.\n\nFederated Heavy Hitters Discovery with Differential Privacy. Wennan Zhu, Peter Kairouz, Brendan Mcmahan, Haicheng Sun, Wei Li, PMLRProceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics. the Twenty Third International Conference on Artificial Intelligence and Statistics108Wennan Zhu, Peter Kairouz, Brendan McMahan, Haicheng Sun, and Wei Li. Federated Heavy Hitters Discovery with Differential Privacy. In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108. PMLR, 2020.\n\nMcmahan, Recent years have observed a booming interest in various aspects of Federated Learning. Yuan and Maincluding communicationefficient learningRecent years have observed a booming interest in various aspects of Federated Learning, including communication- efficient learning (McMahan et al., 2017; Konečný et al., 2016; Zhou and Cong, 2018; Haddadpour et al., 2019a; Wang and Joshi, 2018; Yu and Jin, 2019; Yu et al., 2019; Basu et al., 2019; Stich, 2019; Khaled et al., 2020; Yuan and Ma, 2020; Woodworth et al., 2020; Yuan et al., 2021; Li et al., 2021; Huang et al., 2021;\n\n. Glasgow, model ensembling. integration with compressionGlasgow et al., 2021), model ensembling (Bistritz et al., 2020; He et al., 2020; Lin et al., 2020; Chen and Chao, 2021), integration with compression (Faghri et al., 2020; Gorbunov et al., 2020; Sohn et al., 2020;\n\n2020), data (distributional) heterogeneity. Beznosikov, 2021), systems heterogeneityBeznosikov et al., 2020; Horváth and Richtárik, 2020; Albasyoni et al., 2020; Jiang et al., 2020; Islamov et al., 2021), systems heterogeneity (Smith et al., 2017; Diao et al., 2020), data (distributional) heterogeneity (Haddadpour et al., 2019b; Khaled et al., 2020; Li et al., 2020d; Koloskova et al., 2020; Woodworth et al., 2020; Mohri et al., 2019; Zhang et al., 2020; Li et al., 2020b; Wang et al., 2020a; Karimireddy et al., 2020;\n\n. Wainwright Pathak, Pathak and Wainwright, 2020;\n\n. Al-Shedivat, Nichol et al. Al-Shedivat et al., 2021), fairness (Wang et al., 2020b; Li et al., 2020c; Mohri et al., 2019), personalization (Smith et al., 2017; Nichol et al., 2018; Khodak et al., 2019; Balcan et al., 2019;\n\n. Jiang, LondonJiang et al., 2019; Wang et al., 2019; Chen et al., 2019; Fallah et al., 2020; Hanzely et al., 2020; London, 2020;\n\n. T Dinh, Hanzely and Richtárik. T. Dinh et al., 2020; Yu et al., 2020; Hanzely and Richtárik, 2020; Agarwal et al., 2020; Deng et al., 2020;\n\n. Hao, Hao et al., 2020; Liang et al., 2020), and privacy (Balle et al., 2020; Chen et al., 2020; Geiping et al., 2020;\n\nHuang et al. (2021) studied the generalization of Federated Learning in Neural-tangent kernel regime. But, to our knowledge, there is no existing work that disentangles out-of-sample and participation gaps in federated training. We refer readers to. ; London, So, WangThese works often study generalization and convergence for newly proposed algorithmsLondon, 2020; So et al., 2020; Zhu et al., 2020; Brown et al., 2020). These works often study generalization and convergence for newly proposed algorithms. Huang et al. (2021) studied the generalization of Federated Learning in Neural-tangent kernel regime. But, to our knowledge, there is no existing work that disentangles out-of-sample and participation gaps in federated training. We refer readers to (Kairouz et al., 2019; Wang"
  },
  {
    "corpusid": 62841605,
    "title": "SPREADING VECTORS FOR SIMILARITY SEARCH",
    "abstract": "Discretizing multi-dimensional data distributions is a fundamental step of modern indexing methods. State-of-the-art techniques learn parameters of quantizers on training data for optimal performance, thus adapting quantizers to the data. In this work, we propose to reverse this paradigm and adapt the data to the quantizer: we train a neural net which last layer forms a fixed parameter-free quantizer, such as pre-defined points of a hyper-sphere. As a proxy objective, we design and train a neural network that favors uniformity in the spherical latent space, while preserving the neighborhood structure after the mapping. We propose a new regularizer derived from the Kozachenko-Leonenko differential entropy estimator to enforce uniformity and combine it with a locality-aware triplet loss. Experiments show that our end-to-end approach outperforms most learned quantization methods, and is competitive with the state of the art on widely adopted benchmarks. Furthermore, we show that training without the quantization step results in almost no difference in accuracy, but yields a generic catalyzer that can be applied with any subsequent quantizer. The code is available online 1 .",
    "citations": [],
    "full_paper": "SPREADING VECTORS FOR SIMILARITY SEARCH\n\n\nAlexandre Sablayrolles \nFacebook AI Research Inria\n\n\nMatthijs Douze \nFacebook AI Research Inria\n\n\nCordelia Schmid \nFacebook AI Research Inria\n\n\nHervé Jégou \nFacebook AI Research Inria\n\n\nSPREADING VECTORS FOR SIMILARITY SEARCH\nPublished as a conference paper at ICLR 2019\nDiscretizing multi-dimensional data distributions is a fundamental step of modern indexing methods. State-of-the-art techniques learn parameters of quantizers on training data for optimal performance, thus adapting quantizers to the data. In this work, we propose to reverse this paradigm and adapt the data to the quantizer: we train a neural net which last layer forms a fixed parameter-free quantizer, such as pre-defined points of a hyper-sphere. As a proxy objective, we design and train a neural network that favors uniformity in the spherical latent space, while preserving the neighborhood structure after the mapping. We propose a new regularizer derived from the Kozachenko-Leonenko differential entropy estimator to enforce uniformity and combine it with a locality-aware triplet loss. Experiments show that our end-to-end approach outperforms most learned quantization methods, and is competitive with the state of the art on widely adopted benchmarks. Furthermore, we show that training without the quantization step results in almost no difference in accuracy, but yields a generic catalyzer that can be applied with any subsequent quantizer. The code is available online 1 .\n\nINTRODUCTION\n\nRecent work (Kraska et al., 2017) proposed to leverage the pattern-matching ability of machine learning algorithms to improve traditional index structures such as B-trees or Bloom filters, with encouraging results. In their one-dimensional case, an optimal B-Tree can be constructed if the cumulative density function (CDF) of the indexed value is known, and thus they approximate this CDF using a neural network. We emphasize that the CDF itself is a mapping between the indexed value and a uniform distribution in [0,1]. In this work, we wish to generalize such an approach to multi-dimensional spaces. More precisely, as illustrated by Figure 1, we aim at learning a function that maps real-valued vectors to a uniform distribution over a d-dimensional sphere, such that a fixed discretizing structure, for example a fixed binary encoding (sign of components) or a regular lattice quantizer, offers competitive coding performance.\n\nOur approach is evaluated in the context of similarity search, where methods often rely on various forms of learning machinery (Gong et al., 2013;Wang et al., 2014b); in particular there is a substantial body of literature on methods producing compact codes (Jégou et al., 2011a). Yet the problem of jointly optimizing a coding stage and a neural network remains essentially unsolved, partly because FC catalyzer discretization Figure 1: Our method learns a network that encodes the input space R d into a code c(x). It is learned end-to-end, yet the part of the network in charge of the discretization operation is fixed in advance, thereby avoiding optimization problems. The learnable function f , namely the \"catalyzer\", is optimized to increase the quality of the subsequent coding stage. Published as a conference paper at ICLR 2019 input λ = 0 λ = 0.01 λ = 0.1 λ → ∞ Figure 2: Illustration of our method, which takes as input a set of samples from an unknown distribution. We learn a neural network that aims at preserving the neighborhood structure in the input space while best covering the output space (uniformly). This trade-off is controlled by a parameter λ. The case λ = 0 keeps the locality of the neighbors but does not cover the output space. On the opposite, when the loss degenerates to the differential entropic regularizer (λ → ∞), the neighbors are not maintained by the mapping. Intermediate values offer different trade-offs between neighbor fidelity and uniformity, which is proper input for an efficient lattice quantizer (depicted here by the hexagonal lattice A 2 ).\nBN RELU FC BN RELU FC x 2 R d c(x) f (x) 2\nit is difficult to optimize through a discretization function. For this reason, most efforts have been devoted to networks producing binary codes, for which optimization tricks exist, such as soft binarization or stochastic relaxation, which are used in conjunction with neural networks (Liong et al., 2015;Jain et al., 2017). However it is difficult to improve over more powerful codes such as those produced by product quantization (Jégou et al., 2011a), and recent solutions addressing product quantization require complex optimization procedures (Klein & Wolf, 2017;Ozan et al., 2016).\n\nIn order to circumvent this problem, we propose a drastic simplification of learning algorithms for indexing. We learn a mapping such that the output follows the distribution under which the subsequent discretization method, either binary or a more general quantizer, performs better. In other terms, instead of trying to adapt an indexing structure to the data, we adapt the data to the index.\n\nOur technique requires to jointly optimize two antithetical criteria. First, we need to ensure that neighbors are preserved by the mapping, using a vanilla ranking loss (Usunier et al., 2009;Chechik et al., 2010;Wang et al., 2014a). Second, the training must favor a uniform output. This suggests a regularization similar to maximum entropy (Pereyra et al., 2017), except that in our case we consider a continuous output space. We therefore propose to cast an existing differential entropy estimator into a regularization term, which plays the same \"distribution-matching\" role as the Kullback-Leiber term of variational auto-encoders (Doersch, 2016).\n\nAs a side note, many similarity search methods are implicitly designed for the range search problem (or near neighbor, as opposed to nearest neighbor (Indyk & Motwani, 1998;Andoni & Indyk, 2006)), that aims at finding all vectors whose distance to the query vector is below a fixed threshold. For real-world high-dimensional data, range search usually returns either no neighbors or too many. The discrepancy between near-and nearest-neighbors is significantly reduced by our technique, see Section 3.3 and Appendix C for details.\n\nOur method is illustrated by Figure 2. We summarize our contributions as follows:\n\n• We introduce an approach for multi-dimensional indexing that maps the input data to an output space in which indexing is easier. It learns a neural network that plays the role of an adapter for subsequent similarity search methods. • For this purpose we introduce a loss derived from the Kozachenko-Leonenko differential entropy estimator to favor uniformity in the spherical output space. • Our learned mapping makes it possible to leverage spherical lattice quantizers with competitive quantization properties and efficient algebraic encoding. • Our ablation study shows that our network can be trained without the quantization layer and used as a plug-in for processing features before using standard quantizers. We show quantitatively that our catalyzer improves performance by a significant margin for quantization-based (OPQ (Ge et al., 2013)) and binary (LSH (Charikar, 2002)) method.\n\nThis paper is organized as follows. Section 2 discusses related works. Section 3 introduces our neural network model and the optimization scheme. Section 4 details how we combine this strategy with lattice assignment to produce compact codes. The experimental section 5 evaluates our approach.\n\n\nRELATED WORK\n\nGenerative modeling. Recent models such as Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) or Variational Auto-Encoders (VAEs) (Kingma & Welling, 2013) learn a mapping between an isotropic Gaussian distribution and the empirical distribution of a training set. Our approach maps an empirical input distribution to a uniform distribution on the spherical output space. Another distinction is that GANs learn a unidirectional mapping from the latent code to an image (decoder), whereas VAEs learn a bidirectional mapping (encoder -decoder). In our work, we focus on learning the encoder, whose goal is to pre-process input vectors for subsequent indexing.\n\nDimensionality reduction and representation learning. There is a large body of literature on the topic of dimensionality reduction, see for instance the review by Van Der Maaten et al. (2009). Relevant work includes self-organizing maps (Kohonen et al., 2001), the stochastic neighbor embedding (Hinton & Roweis, 2003) and the subsequent t-SNE approach (van der Maaten & Hinton, 2008), which is tailored to low-dimensional spaces for visualisation purposes. Both works are non-linear dimensionality reduction aiming at preserving the neighborhood in the output space.\n\nLearning to index and quantize. The literature on product compact codes for indexing is most relevant to our work, see Wang et al. (2014b;2016) for an overview of the topic. Early popular highdimensional approximate neighbor methods, such as Locality Sensitive Hashing (Indyk & Motwani, 1998;Gionis et al., 1999;Charikar, 2002;Andoni & Indyk, 2006), were mostly relying on statistical guarantees without any learning stage. This lack of data adaptation was subsequently addressed by several works. The Iterative quantization (ITQ) (Gong et al., 2013) modifies the coordinate system to improve binarization, while methods inspired by Vector Quantization and compression (Jégou et al., 2011a;Babenko & Lempitsky, 2014;Zhang et al., 2015;Jain et al., 2016) have gradually emerged as strong competitors for estimating distances or similarities with compact codes. While most of these works aim at reproducing target (dis-)similarity, some recent works directly leverage semantic information in a supervised manner with neural networks (Liong et al., 2015;Jain et al., 2017;Klein & Wolf, 2017;Sablayrolles et al., 2017).\n\nLattices, also known as Euclidean networks, are discrete subsets of the Euclidean space that are of particular interest due to their space covering and sphere packing properties (Conway & Sloane, 2013). They also have excellent discretization properties under some assumptions about the distribution, and most interestingly the closest point of a lattice is determined efficiently thanks to algebraic properties (Ran & Snyders, 1998). This is why lattices have been proposed (Andoni & Indyk, 2006;Jégou et al., 2008) as hash functions in LSH. However, for real-world data, lattices waste capacity because they assume that all regions of the space have the same density (Paulevé et al., 2010). In this paper, we are interested in spherical lattices because of their bounded support.\n\nEntropy regularization appears in many areas of machine learning and indexing. For instance, Pereyra et al. (2017) argue that penalizing confident output distributions is an effective regularization. Cuturi (2013) use entropy regularization to speed up computation of optimal transport distances. Another proposal by Bojanowski & Joulin (2017) in an unsupervised learning context, is to spread the output by enforcing input images to map to points drawn uniformly on a sphere. Interestingly, most recent works on binary hashing introduce some form of entropic regularization. Deep hashing (Liong et al., 2015) employs a regularization term that increases the marginal entropy of each bit. SUBIC (Jain et al., 2017) extends this idea to one-hot codes.\n\n\nOUR APPROACH: LEARNING THE CATALYZER\n\nOur proposal is inspired by prior work for one-dimensional indexing (Kraska et al., 2017). However their approach based on unidimensional density estimation can not be directly translated to the multidimensional case. Our strategy is to train a neural network f that maps vectors from a d in -dimensional space to the hypersphere of a d out -dimensional space S dout .\n\n\nKOLEO: DIFFERENTIAL ENTROPY REGULARIZER\n\nLet us first introduce our regularizer, which we design to spread out points uniformly across S dout . With the knowledge of the density of points p, we could directly maximize the differential entropy − p(u) log(p(u))du. Given only samples (f (x 1 ), ..., f (x n )), we instead use an estimator of the  Figure 3: Histograms of the distance between a query point and its 1st (resp. 100 th ) nearest neighbors, in the original space (left) and after our catalyzer (right). In the original space, the two histograms have a significant overlap, which means that a 100-th nearest neighbor for a query has often a distance lower that the 1st neighbor for another query. This gap is significantly reduced by our catalyzer.\n\ndifferential entropy as a proxy. It was shown by Kozachenko and Leononenko (see e.g. (Beirlant et al., 1997)) that defining ρ n,i = min j =i f (x i ) − f (x j ) , the differential entropy of the distribution can be estimated by\nH n = α n n n i=1 log(ρ n,i ) + β n ,(1)\nwhere α n and β n are two constants that depend on the number of samples n and the dimensionality of the data d out . Ignoring the affine components, we define our entropic regularizer as\nL KoLeo = − 1 n n i=1 log(ρ n,i ).(2)\nThis loss also has a satisfactory geometric interpretation: closest points are pushed away, with a strength that is non-decreasing and concave. This ensures diminishing returns: as points get away from each other, the marginal impact of increasing the distance becomes smaller.\n\n\nRANK PRESERVING LOSS\n\nWe enforce the outputs of the neural network to follow the same neighborhood structure as in the input space by adopting the triplet loss (Chechik et al., 2010;Wang et al., 2014a)\nL rank = max 0, f (x) − f (x + ) 2 − f (x) − f (x − ) 2 ,(3)\nwhere x is a query, x + a positive match, x − a negative match. The positive matches are obtained by computing the k pos nearest neighbors of each point x in the training set in the input space. The negative matches are generated by taking the k neg -th nearest neighbor of\nf (x) in (f (x 1 ), ..., f (x n )).\nIn order to speed up the learning, we compute the k neg -th nearest neighbor of every point in the dataset at the beginning of each epoch and use these throughout the epoch. Note that we do not need to use a margin, as its effect is essentially superseded by our regularizer. Our overall loss combines the triplet loss and the entropy regularizer, as\nL model = L rank + λL KoLeo ,(4)\nwhere the parameter λ ≥ 0 controls the trade-off between ranking quality and uniformity.\n\n\nDISCUSSION\n\nChoice of λ. Figure 2 was produced by our method on a toy dataset adapted to the disk as the output space. Without the KoLeo regularization term, neighboring points tend to collapse and most of the output space is not exploited. If we quantize this output with a regular quantizer, many Voronoi cells are empty and we waste coding capacity. In contrast, if we solely rely on the entropic regularizer, the neighbors are poorly preserved. Interesting trade-offs are achieved with intermediate values of λ. Figure 4: Impact of the regularizer on the output distribution. Each column corresponds to a different amount of regularization (left: λ = 0, middle: λ = 0.02, right: λ = 1). Each line corresponds to a different random projection of the empirical distribution, parametrized by an angle in [0, 2π]. The marginal distributions for these two views are much more uniform with our KoLeo regularizer, which is a consequence of the higher uniformity in the high-dimensional latent space.\n\nQualitative evaluation of the uniformity. Figure 3 shows the histogram of the distance to the nearest (resp. 100 th nearest) neighbor, before applying the catalyzer (left) and after (right). The overlap between the two distributions is significantly reduced by the catalyzer. We evaluate this quantitatively by measuring the probability that the distance between a point and its nearest neighbor is larger than the distance between another point and its 100 th nearest neighbor. In a very imbalanced space, this value is 50%, whereas in a uniform space it should approach 0%. In the input space, this probability is 20.8%, and it goes down to 5.0% in the output space thanks to our catalyzer.\n\nVisualization of the output distribution. While Figure 2 illustrates our method with the 2D disk as an output space, we are interested in mapping input samples to a higher dimensional hyper-sphere. Figure 4 proposes a visualization of the high-dimensional density from a different viewpoint, with the Deep1M dataset mapped in 8 dimensions. We sample 2 planes randomly in R dout and project the dataset points (f (x 1 ), ..., f (x n )) on them. For each column, the 2 figures are the angular histograms of the points with a polar parametrization of this plane. The area inside the curve is constant and proportional to the number of samples n. A uniform angular distribution produces a centered disk, and less uniform distributions look like unbalanced potatoes.\n\nThe densities we represent are marginalized, so if the distribution looks non-uniform then it is non-uniform in d out -dimensional space, but the reverse is not true. Yet one can compare the results obtained for different regularization coefficients, which shows that our regularizer has a strong uniformizing effect on the mapping, ultimately resembling that of a uniform distribution for λ = 1.\n\n\nCATALYZER WITH DISCRETIZATION\n\nIn this section we describe how our method interplays with discretization, at training and at search time. We consider two parameter-free coding methods: binarization and defining a fixed set of points on the unit sphere provided by a lattice spherical quantizer. A key advantage of a fixed coding structure like ours is that compressed-domain distance computations between codes do not depend on external meta-data. This is in contrast with quantization-based methods like product quantization, which require centroids to be available at search time.\n\n\nBINARIZATION\n\nBinary features are obtained by applying the sign function to the coordinates. We relax this constraint at train time by replacing the sign with the identity function, and the binarization is used only to cross-validate the regularization parameter on the validation set.\n\n\nLATTICES\n\nAs discussed by Paulevé et al. (2010), lattices impose a rigid partitioning of the feature space, which is suboptimal for arbitrary distributions, see Figure 2. In contrast, lattices offer excellent quantization properties for a uniform distribution (Conway & Sloane, 2013). Thanks to our regularizer, we are closer to uniformity in the output space, making lattices an attractive choice.\n\nWe consider the simplest spherical lattice, integer points of norm r, a set we denote S r d . Given a vector x ∈ R din , we compute its catalyzed features f (x), and find the nearest lattice point on S r d using the assignment operation, which formally minimizes q(f (x)) = min c∈S r\nd r × f (x) − c 2 2 .\nPublished as a conference paper at ICLR 2019\n\nThis assignment can be computed very efficiently (see Appendix B for details). Given a query y and its representation f (y), we approximate the similarity between y and x using the code: f (y) − f (x) 2 ≈ f (y) − q(f (x))/r 2 , This is an asymmetric comparison, because the query vectors are not quantized (Jégou et al., 2011a).\n\nWhen used as a layer, it takes a vector in R d and returns the quantized version of this vector in the forward pass, and passes the gradient to the previous layer in the backward pass. This heuristic is referred to as the straight-through estimator in the literature, and is often used for discretization steps, see e.g., van den Oord et al. (2017).\n\n\nEXPERIMENTS\n\nThis section presents our experimental results. We focus on the class of similarity search methods that represents the database vectors with a compressed representation (Charikar, 2002;Jégou et al., 2011a;Gong et al., 2013;Ge et al., 2013), which enables to store very large dataset in memory (Lv et al., 2004;Torralba et al., 2008).\n\n\nEXPERIMENTAL SETUP\n\nAll experiments have two phases. In the first phase (encoding), all vectors of a database are encoded into a representation (e.g. 32, 64 bits). Encoding consists in a vector transformation followed by a quantization or binarization stage. The second phase is the search phase: a set of query vectors is transformed, then the codes are scanned exhaustively and compared with the transformed query vector, and the top-k nearest vectors are returned.\n\nDatasets and metrics. We use two benchmark datasets Deep1M and BigAnn1M. Deep1M consists of the first million vectors of the Deep1B dataset (Babenko & Lempitsky, 2016). The vectors were obtained by running a convnet on an image collection, reduced to 96 dimensions by principal component analysis and subsequently 2 -normalized. We also experiment with the BigAnn1M (Jégou et al., 2011b), which consists of SIFT descriptors (Lowe, 2004). Both datasets contain 1M vectors that serve as a reference set, 10k query vectors and a very large training set of which we use 500k elements for training, and 1M vectors that we use a base to cross-validate the hyperparameters d out and λ. We also experiment on the full Deep1B and BigAnn datasets, that contain 1 billion elements. We evaluate methods with the recall at k performance measure, which is the proportion of results that contain the ground truth nearest neighbor when returning the top k candidates (for k ∈ {1, 10, 100}).\n\nTraining. For all methods, we train our neural network on the training set, cross-validate d out and λ on the validation set, and use a different set of vectors for evaluation. In contrast, some works carry out training on the database vectors themselves (Muja & Lowe, 2014;Malkov & Yashunin, 2016;Gong et al., 2013), in which case the index is tailored to a particular fixed set of database vectors.\n\n\nMODEL ARCHITECTURE AND OPTIMIZATION\n\nOur model is a 3 -layer perceptron, with ReLU non-linearity and hidden dimension 1024. The final linear layer projects the dataset to the desired output dimension d out , along with 2 -normalization. We use batch normalization (Ioffe & Szegedy, 2015) and train our model for 300 epochs with Stochastic Gradient Descent, with an initial learning rate of 0.1 and a momentum of 0.9. The learning rate is decayed to 0.05 (resp. 0.01) at the 80-th epoch (resp. 120-th).\n\n\nSIMILARITY SEARCH WITH LATTICE VECTOR QUANTIZERS\n\nWe evaluate the lattice-based indexing proposed in Section 4, and compare it to more conventional methods based on quantization, namely PQ (Jégou et al., 2011a) and Optimized Product Quantization (OPQ) (Ge et al., 2013). We use the Faiss (Johnson et al., 2017) implementation of PQ and OPQ and assign one byte per sub-vector (each individual quantizer has 256 centroids). For our lattice, we vary the value of r to increase the quantizer size, hence generating curves for each value of d out . Figure 5 provides a comparison of these methods. On both datasets, the lattice quantizer strongly outperforms PQ and OPQ for most code sizes. 1-recall at 10 bits per indexed vector d=16 d=24 d=32 d=40 PQ OPQ Figure 5: Comparison of the performance of the product lattice vs OPQ on Deep1M (left) and BigAnn1M (right). Our method maps the input vectors to a d out -dimensional space, that is then quantized with a lattice of radius r. We obtain the curves by varying the radius r.\n\nImpact of the hyperparameters. Varying the rank parameters k pos and k neg did not impact significantly the performance, so we fixed them respectively to k pos = 10 and k neg = 50. For a fixed number of bits, varying the dimension d out is a trade-off between a good representation and an easily compressible one. When d out is small, we can use a large r for a very small quantization error, but there are not enough dimensions to represent the degrees of freedom of the underlying data. A larger d out allows for better representations but suffers from a coarser approximation. Figure 5 shows that for low bitrates, small dimensions perform better because the approximation quality dominates, whereas for higher bitrates, larger dimensions are better because the representation quality dominates. Similarly, the regularizer λ needs to be set to a large value for small dimensions and low bitrates, but higher dimensions and higher bitrates require lower values of λ (cf. Appendix A for details).\n\nLarge-scale experiments. We experiment with the full Deep1B (resp. BigAnn) dataset, that contains 1 billion vectors, with 64 bits codes. At that scale, the recall at 10 drops to 26.1% for OPQ and to 37.8% for the lattice quantizer (resp. 21.3% and 36.5%). As expected, the recall performance is lower than for the 1 million vectors database, but the precision advantage of the lattice quantizer is maintained at large scale.\n\nComparison to the state of the art. Additive quantization variants (Babenko & Lempitsky, 2014;Martinez et al., 2018;Ozan et al., 2016) are currently state-of-the art encodings for vectors in terms of accuracy. However, their encoding stage involves an iterative optimization process that is prohibitively slow for practical use cases. For example, Competitive quantization's reported complexity is 15×   (Martinez et al., 2018), a recent variant that is close to the state of the art and for which open-source code is available. We show that our Catalyst + Lattice variant method is 14× times faster for an accuracy that is competitive or well above that of LSQ. To our knowledge, this is the first time that such competitive results are reported for a method that can be used in practice at a large scale. Our search time is a bit slower: computing 1M asymmetric distances takes 7.5 ms with the Catalyzer+Lattice instead of 4.9 ms with PQ. This is due to our decoding procedure, which does not rely on precomputed tables as used in PQ.\n\n\nA UNIVERSAL CATALYZER?\n\nAblation study. As a sanity check, we first replace our catalyzer by a PCA that reduces the dimensionality to the same size as our catalyzer, followed by 2 -normalization. This significantly decreases the performance of the lattice quantizer, as can be seen in Table 1.\n\nWe also evaluate the impact of training end-to-end, compared to training without the quantization layer. Table 1 shows that end-to-end training has a limited impact on the overall performance for 64 bits, sometimes even decreasing performance. This may be partly due to the approximation induced by the straight-through estimation, which handicaps end-to-end training. Another reason is that the KoLeo regularizer narrows the performance gap induced by discretization. In other terms, our method trained without the discretization layer trains a general-purpose network (hence the name catalyzer), on which we can apply any binarization or quantization method. Table 1 shows that OPQ is improved when applied on top of catalyzed features, for example increasing the recall@10 from 63.6 to 71.1.\n\nBinary hashing. We also show the interest of our method as a catalyzer for binary hashing, compared to two popular methods (Charikar, 2002;Gong et al., 2013):\n\nLSH maps Euclidean vectors to binary codes that are then compared with Hamming distance. A set of m fixed projection directions are drawn randomly and isotropically in d in , and each vector is encoded into m bits by taking the sign of the dot product with each direction.\n\nITQ is another popular hashing method, that improves LSH by using an orthogonal projection that is optimized to maximize correlation between the original vectors and the bits. \n\n\nCONCLUDING REMARKS\n\nWe train a neural network that maps input features to a uniform output distribution on a unit hypersphere, making high-dimensional indexing more accurate, in particular with fast and rigid lattice quantizers or a trivial binary encoding. To the best of our knowledge, this is the first work on multi-dimensional data that demonstrates that it is competitive to adapt the data distribution to a rigid quantizer, instead of adapting the quantizer to the input data. This has several benefits: rigid quantizers are fast at encoding time; and vectors can be decoded without carrying around codebooks or auxiliary tables. We open-sourced the code corresponding to the experiments at https://github.com/facebookresearch/spreadingvectors.\n\n\nAPPENDIX A VALUES OF THE REGULARIZATION PARAMETER\n\nThe optimal value of the regularizer λ decreases with the dimension, as shown by  Table 3: Optimal values of the regularization parameter λ for Deep1M, using a fixed radius of r = 10.\n\n\nAPPENDIX B FAST DISCRETIZATION WITH A LATTICE ON THE SPHERE.\n\nWe consider the set of integer points z = (z 1 , ..., z d ) ∈ Z d such that d i=1 z 2 i = r 2 , that we denote S r d . This set is the intersection of the hyper-cubic lattice Z d with the hyper-sphere of radius r. For example to extract a 64−bit representation in 24D we use r 2 = 79. Quantizing a vector y ∈ R d amounts to solving the following optimization problem:\nargmin z∈S r d y − z 2 = argmax z∈S r d yz .(5)\nAtoms. We define a \"normalization\" function N of vectors: it consists in taking the absolute value of their coordinates, and sorting them by decreasing coordinates. We call \"atoms\" the set of vectors that can be obtained by normalizing the vectors of S r d .\n\nFor example, the atoms of S √ 10 8 are:\n\n3 1 0 0 0 0 0 0 2 2 1 1 0 0 0 0 2 1 1 1 1 1 1 0 (6) All vectors of S r d can be represented as permutations of an atom, with sign flips. Figure 6 reports the number of vectors of S k d and the corresponding number of atoms.\n\nEncoding and enumerating. To solve Equation 5, we apply the following steps:\n\n1. normalize y with N , store the permutation σ that sorts coordinates of |y| 2. exhaustively search the atom z that maximizes N (y) z 3. apply the inverse permutation σ −1 that sorts y to z to obtain z 4. the nearest vector (z 1 , .., z d ) is z i = sign(y i )z i ∀i = 1..d.\n\nTo encode a vector of z ∈ S r d we proceed from N (z):\n\n1. each atom is assigned a range of codes, so z is encoded relative to the start of N (z)'s range 2. encode the permutation using combinatorial number systems Knuth (2005). There are d! permutations, but the permutation of equal components is irrelevant, which divides the number combinations. For example atom (2, 2, 1, 1, 0, 0, 0, 0) is the normalized form of 8!/(2!2!4!) = 240 vectors of S √ 10 8 . 3. encode the sign of non-zero elements. In the example above, there are 4 sign bits.\n\nDecoding proceeds in the reverse order.\n\nEncoding 1M vectors takes about 0.5 s on our reference machine, which is faster than PQ (1.9 s). In other terms, he quantization time is negligible w.r.t. the preprocessing by the catalyzer.   Figure 7 shows how our method achieves a better agreement between range search and k-nearest neighbors search on Deep1M. In this experiment, we consider different thresholds ε for the range search and perform a set of queries for each ε. Then we measure how many vectors we must return, on average, to achieve a certain recall in terms of the nearest neighbors in the original space. Without our mapping, there is a large variance on the number of results for a given ε. In contrast, after the mapping it is possible to use a unique threshold to find most neighbors.  Figure 7: Agreement between nearest neighbor and range search: average number of results per query for given values of ε (indicated on the curve), and corresponding recall values. For example: to obtain 80% recall, the search in the original space requires to set ε = 0.54, which returns 700 results per query on average, while in the transformed space ε = 0.38 returns just 200 results. Observe the much better agreement in the latent spherical space.\n\n\nAPPENDIX C EPSILON-SEARCH\n\nFigure 6 :\n6Number of atoms of the hyper-sphere of S r 24 . (linear scale), and the corresponding number of points on the hyper-sphere (log scale).\n\n\n1 https://github.com/facebookresearch/spreadingvectors arXiv:1806.03198v2 [stat.ML] 16 Feb 20191 \n\n\n\nTable 2 :\n2Performance (1-recall at 10, %) with LSH, on Deep1M and BigAnn1M, as a function of the \nnumber of bits per index vector. All results are averaged over 5 runs with different random seeds. \nOur catalyzer gets a large improvement in binary codes over LSH and ITQ. \n\nslower than OPQ. Table 1 compares our results with LSQ \n\nTable 2\n2compares our catalyzer to LSH and ITQ. Note that a simple sign function is applied to the catalyzed features. The catalyzer improves the performance by 2-9 percentage points in all settings, from 32 to 128 bits.\n\nTable 3 .\n3d out λ \n\n16 \n0.05 \n24 \n0.02 \n32 \n0.01 \n40 \n0.005 \n\n\n\nNear-optimal hashing algorithms for approximate nearest neighbor in high dimensions. Alexandr Andoni, Piotr Indyk, Symposium on the Foundations of Computer Science. Alexandr Andoni and Piotr Indyk. Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. In Symposium on the Foundations of Computer Science, 2006.\n\nAdditive quantization for extreme vector compression. Artem Babenko, Victor Lempitsky, Conference on Computer Vision and Pattern Recognition. Artem Babenko and Victor Lempitsky. Additive quantization for extreme vector compression. In Conference on Computer Vision and Pattern Recognition, 2014.\n\nEfficient indexing of billion-scale datasets of deep descriptors. Artem Babenko, Victor Lempitsky, Conference on Computer Vision and Pattern Recognition. Artem Babenko and Victor Lempitsky. Efficient indexing of billion-scale datasets of deep descriptors. In Conference on Computer Vision and Pattern Recognition, 2016.\n\nNonparametric entropy estimation: An overview. Jan Beirlant, E J Dudewicz, E C Gyor, Meulen, International Journal of Mathematical and Statistical Sciences. 6Jan Beirlant, E J. Dudewicz, L Gyor, and E.C. Meulen. Nonparametric entropy estimation: An overview. International Journal of Mathematical and Statistical Sciences, 6, 1997.\n\nUnsupervised learning by predicting noise. Piotr Bojanowski, Armand Joulin, International Conference on Machine Learning. Piotr Bojanowski and Armand Joulin. Unsupervised learning by predicting noise. In International Conference on Machine Learning, 2017.\n\nSimilarity estimation techniques from rounding algorithms. Moses Charikar, ACM symposium on Theory of computing. Moses Charikar. Similarity estimation techniques from rounding algorithms. In ACM symposium on Theory of computing, 2002.\n\nLarge scale online learning of image similarity through ranking. Gal Chechik, Varun Sharma, Uri Shalit, Samy Bengio, Journal of Machine Learning Research. 11Gal Chechik, Varun Sharma, Uri Shalit, and Samy Bengio. Large scale online learning of image similarity through ranking. Journal of Machine Learning Research, 11(Mar), 2010.\n\nSphere packings, lattices and groups. John Horton, Conway , Neil James Alexander Sloane, Springer Science & Business Media290John Horton Conway and Neil James Alexander Sloane. Sphere packings, lattices and groups, volume 290. Springer Science & Business Media, 2013.\n\nSinkhorn distances: Lightspeed computation of optimal transport. Marco Cuturi, Advances in Neural Information Processing Systems. Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in Neural Information Processing Systems. 2013.\n\nCarl Doersch, arXiv:1606.05908Tutorial on variational autoencoders. arXiv preprintCarl Doersch. Tutorial on variational autoencoders. arXiv preprint arXiv:1606.05908, 2016.\n\nOptimized product quantization for approximate nearest neighbor search. Tiezheng Ge, Kaiming He, Qifa Ke, Jian Sun, Conference on Computer Vision and Pattern Recognition. Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. Optimized product quantization for approximate nearest neighbor search. In Conference on Computer Vision and Pattern Recognition, 2013.\n\nSimilarity search in high dimension via hashing. Arisitides Gionis, Piotr Indyk, Rajeev Motwani, International Conference on Very Large DataBases. Arisitides Gionis, Piotr Indyk, and Rajeev Motwani. Similarity search in high dimension via hashing. In International Conference on Very Large DataBases, pp. 518-529, 1999.\n\nIterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval. Yunchao Gong, Svetlana Lazebnik, Albert Gordo, Florent Perronnin, IEEE Transactions on Pattern Analysis and Machine Intelligence. 3512Yunchao Gong, Svetlana Lazebnik, Albert Gordo, and Florent Perronnin. Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(12), 2013.\n\nGenerative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Advances in Neural Information Processing Systems. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems. 2014.\n\nStochastic neighbor embedding. Geoffrey Hinton, Sam Roweis, Advances in Neural Information Processing Systems. Geoffrey Hinton and Sam Roweis. Stochastic neighbor embedding. In Advances in Neural Information Processing Systems, 2003.\n\nApproximate nearest neighbors: towards removing the curse of dimensionality. Piotr Indyk, Rajeev Motwani, ACM symposium on Theory of computing. Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In ACM symposium on Theory of computing, 1998.\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. Sergey Ioffe, Christian Szegedy, International Conference on Machine Learning. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, 2015.\n\nApproximate search with quantized sparse representations. Himalaya Jain, Patrick Pérez, Rémi Gribonval, Joaquin Zepeda, Hervé Jégou, European Conference on Computer Vision. Himalaya Jain, Patrick Pérez, Rémi Gribonval, Joaquin Zepeda, and Hervé Jégou. Approximate search with quantized sparse representations. In European Conference on Computer Vision, October 2016.\n\nSUBIC: A supervised, structured binary code for image search. Himalaya Jain, Joaquin Zepeda, Patrick Perez, Remi Gribonval, International Conference on Computer Vision. Himalaya Jain, Joaquin Zepeda, Patrick Perez, and Remi Gribonval. SUBIC: A supervised, structured binary code for image search. In International Conference on Computer Vision, 2017.\n\nQuery adaptative locality sensitive hashing. Hervé Jégou, Laurent Amsaleg, Cordelia Schmid, Patrick Gros, International Conference on Acoustics, Speech, and Signal Processing. Hervé Jégou, Laurent Amsaleg, Cordelia Schmid, and Patrick Gros. Query adaptative locality sensitive hashing. In International Conference on Acoustics, Speech, and Signal Processing, 2008.\n\nProduct Quantization for Nearest Neighbor Search. Hervé Jégou, Matthijs Douze, Cordelia Schmid, IEEE Transactions on Pattern Analysis and Machine Intelligence. Hervé Jégou, Matthijs Douze, and Cordelia Schmid. Product Quantization for Nearest Neighbor Search. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011a.\n\nSearching in one billion vectors: re-rank with source coding. Hervé Jégou, Romain Tavenard, Matthijs Douze, Laurent Amsaleg, International Conference on Acoustics, Speech, and Signal Processing. Hervé Jégou, Romain Tavenard, Matthijs Douze, and Laurent Amsaleg. Searching in one billion vectors: re-rank with source coding. In International Conference on Acoustics, Speech, and Signal Processing, 2011b.\n\nBillion-scale similarity search with gpus. Jeff Johnson, Matthijs Douze, Hervé Jégou, arXiv:1702.08734arXiv preprintJeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. arXiv preprint arXiv:1702.08734, 2017.\n\nAuto-encoding variational bayes. P Diederik, Max Kingma, Welling, arXiv:1312.6114arXiv preprintDiederik P. Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\n\nBenjamin Klein, Lior Wolf, arXiv:1711.08589defense of product quantization. arXiv preprintBenjamin Klein and Lior Wolf. In defense of product quantization. arXiv preprint arXiv:1711.08589, 2017.\n\nGenerating all combinations and partitions. E Donald, Knuth, 4The art of computer programmingDonald E Knuth. The art of computer programming, volume 4: Generating all combinations and partitions, fascicle 3, 2005.\n\nSelf-Organizing Maps. T Kohonen, M R Schroeder, T. S. HuangT. Kohonen, M. R. Schroeder, and T. S. Huang (eds.). Self-Organizing Maps. 2001.\n\nThe case for learned index structures. Tim Kraska, Alex Beutel, Ed H Chi, Jeff Dean, Neoklis Polyzotis, arXiv:1712.01208arXiv preprintTim Kraska, Alex Beutel, Ed H. Chi, Jeff Dean, and Neoklis Polyzotis. The case for learned index structures. arXiv preprint arXiv:1712.01208, 2017.\n\nDeep hashing for compact binary codes learning. Venice Erin Liong, Jiwen Lu, Gang Wang, Pierre Moulin, Jie Zhou, Conference on Computer Vision and Pattern Recognition. 1Venice Erin Liong, Jiwen Lu, Gang Wang, Pierre Moulin, Jie Zhou, et al. Deep hashing for compact binary codes learning. In Conference on Computer Vision and Pattern Recognition, volume 1, 2015.\n\nDistinctive image features from scale-invariant keypoints. David G Lowe, International journal of Computer Vision. 602David G. Lowe. Distinctive image features from scale-invariant keypoints. International journal of Computer Vision, 60(2), 2004.\n\nImage similarity search with compact data structures. Qin Lv, Moses Charikar, Kai Li, International Conference on Information and Knowledge. Qin Lv, Moses Charikar, and Kai Li. Image similarity search with compact data structures. In International Conference on Information and Knowledge, pp. 208-217, November 2004.\n\nEfficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. A Yu, Malkov, Da Yashunin, arXiv:1603.09320arXiv preprintYu A Malkov and DA Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. arXiv preprint arXiv:1603.09320, 2016.\n\nLSQ++: Lower running time and higher recall in multi-codebook quantization. Julieta Martinez, Shobhit Zakhmi, H Holger, James J Hoos, Little, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Julieta Martinez, Shobhit Zakhmi, Holger H Hoos, and James J Little. LSQ++: Lower running time and higher recall in multi-codebook quantization. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 491-506, 2018.\n\nScalable nearest neighbor algorithms for high dimensional data. Marius Muja, David G Lowe, IEEE Transactions on Pattern Analysis and Machine Intelligence. 36Marius Muja and David G. Lowe. Scalable nearest neighbor algorithms for high dimensional data. IEEE Transactions on Pattern Analysis and Machine Intelligence, 36, 2014.\n\nCompetitive quantization for approximate nearest neighbor search. E C Ozan, S Kiranyaz, M Gabbouj, IEEE Transactions on Knowledge and Data Engineering. E. C. Ozan, S. Kiranyaz, and M. Gabbouj. Competitive quantization for approximate nearest neighbor search. IEEE Transactions on Knowledge and Data Engineering, 2016.\n\nLocality sensitive hashing: A comparison of hash function types and querying mechanisms. Loïc Paulevé, Hervé Jégou, Laurent Amsaleg, Pattern recognition letters. 3111Loïc Paulevé, Hervé Jégou, and Laurent Amsaleg. Locality sensitive hashing: A comparison of hash function types and querying mechanisms. Pattern recognition letters, 31(11), 2010.\n\nRegularizing neural networks by penalizing confident output distributions. Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz Kaiser, Geoffrey Hinton, arXiv:1701.06548arXiv preprintGabriel Pereyra, George Tucker, Jan Chorowski, Łukasz Kaiser, and Geoffrey Hinton. Regularizing neural networks by penalizing confident output distributions. arXiv preprint arXiv:1701.06548, 2017.\n\nEfficient decoding of the Gosset, Coxeter-Todd and the Barnes-Wall lattices. Moshe Ran, Jakov Snyders, International Symposium on Information Theory. 92Moshe Ran and Jakov Snyders. Efficient decoding of the Gosset, Coxeter-Todd and the Barnes-Wall lattices. In International Symposium on Information Theory, pp. 92, 1998.\n\nHow should we evaluate supervised hashing. Alexandre Sablayrolles, Matthijs Douze, Nicolas Usunier, Hervé Jégou, International Conference on Acoustics, Speech, and Signal Processing. Alexandre Sablayrolles, Matthijs Douze, Nicolas Usunier, and Hervé Jégou. How should we evaluate supervised hashing? In International Conference on Acoustics, Speech, and Signal Processing, 2017.\n\nSmall codes and large image databases for recognition. Antonio Torralba, Rob Fergus, Yair Weiss, Conference on Computer Vision and Pattern Recognition. Antonio Torralba, Rob Fergus, and Yair Weiss. Small codes and large image databases for recognition. In Conference on Computer Vision and Pattern Recognition, 2008.\n\nRanking with ordered weighted pairwise classification. Nicolas Usunier, David Buffoni, Patrick Gallinari, International Conference on Machine Learning. Nicolas Usunier, David Buffoni, and Patrick Gallinari. Ranking with ordered weighted pairwise classification. In International Conference on Machine Learning, 2009.\n\nNeural discrete representation learning. Aaron Van Den Oord, Oriol Vinyals, Koray Kavukcuoglu, Advances in Neural Information Processing Systems. Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In Advances in Neural Information Processing Systems. 2017.\n\nVisualizing data using t-SNE. Laurens Van Der Maaten, Geoffrey Hinton, Journal of Machine Learning Research. Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine Learning Research, 2008.\n\nDimensionality reduction: a comparative review. Laurens Van Der Maaten, Eric Postma, Jaap Van Den, Herik, Journal of Machine Learning Research. 10Laurens Van Der Maaten, Eric Postma, and Jaap Van den Herik. Dimensionality reduction: a comparative review. Journal of Machine Learning Research, 10, 2009.\n\nLearning fine-grained image similarity with deep ranking. Jiang Wang, Yang Song, Thomas Leung, Chuck Rosenberg, Jingbin Wang, James Philbin, Bo Chen, Ying Wu, Conference on Computer Vision and Pattern Recognition. Jiang Wang, Yang Song, Thomas Leung, Chuck Rosenberg, Jingbin Wang, James Philbin, Bo Chen, and Ying Wu. Learning fine-grained image similarity with deep ranking. In Conference on Computer Vision and Pattern Recognition, 2014a.\n\nHashing for similarity search: A survey. Jingdong Wang, Jingkuan Heng Tao Shen, Jianqiu Song, Ji, arXiv:1408.2927arXiv preprintJingdong Wang, Heng Tao Shen, Jingkuan Song, and Jianqiu Ji. Hashing for similarity search: A survey. arXiv preprint arXiv:1408.2927, 2014b.\n\nLearning to hash for indexing big data: a survey. Jun Wang, Wei Liu, Sanjiv Kumar, Shih-Fu Chang, Proceedings of the IEEE. 1041Jun Wang, Wei Liu, Sanjiv Kumar, and Shih-Fu Chang. Learning to hash for indexing big data: a survey. Proceedings of the IEEE, 104(1), 2016.\n\nSparse composite quantization. Ting Zhang, Guo-Jun Qi, Jinhui Tang, Jingdong Wang, Conference on Computer Vision and Pattern Recognition. Ting Zhang, Guo-Jun Qi, Jinhui Tang, and Jingdong Wang. Sparse composite quantization. In Conference on Computer Vision and Pattern Recognition, June 2015."
  },
  {
    "corpusid": 253237531,
    "title": "MACHINE UNLEARNING OF FEDERATED CLUSTERS",
    "abstract": "Federated clustering (FC) is an unsupervised learning problem that arises in a number of practical applications, including personalized recommender and healthcare systems. With the adoption of recent laws ensuring the \"right to be forgotten\", the problem of machine unlearning for FC methods has become of significant importance. We introduce, for the first time, the problem of machine unlearning for FC, and propose an efficient unlearning mechanism for a customized secure FC framework. Our FC framework utilizes special initialization procedures that we show are well-suited for unlearning. To protect client data privacy, we develop the secure compressed multiset aggregation (SCMA) framework that addresses sparse secure federated learning (FL) problems encountered during clustering as well as more general problems. To simultaneously facilitate low communication complexity and secret sharing protocols, we integrate Reed-Solomon encoding with special evaluation points into our SCMA pipeline, and prove that the client communication cost is logarithmic in the vector dimension. Additionally, to demonstrate the benefits of our unlearning mechanism over complete retraining, we provide a theoretical analysis for the unlearning performance of our approach. Simulation results show that the new FC framework exhibits superior clustering performance compared to previously reported FC baselines when the cluster sizes are highly imbalanced. Compared to completely retraining K-means++ locally and globally for each removal request, our unlearning procedure offers an average speed-up of roughly 84x across seven datasets. Our implementation for the proposed method is available at https://github.com/thupchnsky/mufc. * Equal contribution. arXiv:2210.16424v2 [cs.LG] 1 Jul 2023 Published as a conference paper at ICLR 2023 Nir Ailon, Ragesh Jaiswal, and Claire Monteleoni. Streaming k-means approximation. Advances in neural information processing systems, 22, 2009.Constance Beguier, Mathieu Andreux, and Eric W Tramel. Efficient sparse secure aggregation for federated learning. arXiv preprint arXiv:",
    "citations": [],
    "full_paper": "MACHINE UNLEARNING OF FEDERATED CLUSTERS\n\n\nChao Pan chaopan2@illinois.edu \nDepartment of Electrical and Computer Engineering\nUniversity of Illinois Urbana-Champaign\nUSA\n\nJin Sima jsima@illinois.edu \nDepartment of Electrical and Computer Engineering\nUniversity of Illinois Urbana-Champaign\nUSA\n\nSaurav Prakash sauravp2@illinois.edu \nDepartment of Electrical and Computer Engineering\nUniversity of Illinois Urbana-Champaign\nUSA\n\nVishal Rana vishalr@illinois.edu \nDepartment of Electrical and Computer Engineering\nUniversity of Illinois Urbana-Champaign\nUSA\n\nOlgica Milenkovic milenkov@illinois.edu \nDepartment of Electrical and Computer Engineering\nUniversity of Illinois Urbana-Champaign\nUSA\n\nMACHINE UNLEARNING OF FEDERATED CLUSTERS\nPublished as a conference paper at ICLR 2023\nFederated clustering (FC) is an unsupervised learning problem that arises in a number of practical applications, including personalized recommender and healthcare systems. With the adoption of recent laws ensuring the \"right to be forgotten\", the problem of machine unlearning for FC methods has become of significant importance. We introduce, for the first time, the problem of machine unlearning for FC, and propose an efficient unlearning mechanism for a customized secure FC framework. Our FC framework utilizes special initialization procedures that we show are well-suited for unlearning. To protect client data privacy, we develop the secure compressed multiset aggregation (SCMA) framework that addresses sparse secure federated learning (FL) problems encountered during clustering as well as more general problems. To simultaneously facilitate low communication complexity and secret sharing protocols, we integrate Reed-Solomon encoding with special evaluation points into our SCMA pipeline, and prove that the client communication cost is logarithmic in the vector dimension. Additionally, to demonstrate the benefits of our unlearning mechanism over complete retraining, we provide a theoretical analysis for the unlearning performance of our approach. Simulation results show that the new FC framework exhibits superior clustering performance compared to previously reported FC baselines when the cluster sizes are highly imbalanced. Compared to completely retraining K-means++ locally and globally for each removal request, our unlearning procedure offers an average speed-up of roughly 84x across seven datasets. Our implementation for the proposed method is available at https://github.com/thupchnsky/mufc. * Equal contribution. arXiv:2210.16424v2 [cs.LG] 1 Jul 2023 Published as a conference paper at ICLR 2023 Nir Ailon, Ragesh Jaiswal, and Claire Monteleoni. Streaming k-means approximation. Advances in neural information processing systems, 22, 2009.Constance Beguier, Mathieu Andreux, and Eric W Tramel. Efficient sparse secure aggregation for federated learning. arXiv preprint arXiv:\n\nINTRODUCTION\n\nThe availability of large volumes of user training data has contributed to the success of modern machine learning models. For example, most state-of-the-art computer vision models are trained on large-scale image datasets including Flickr (Thomee et al., 2016) and ImageNet (Deng et al., 2009). Organizations and repositories that collect and store user data must comply with privacy regulations, such as the recent European Union General Data Protection Regulation (GDPR), the California Consumer Privacy Act (CCPA), and the Canadian Consumer Privacy Protection Act (CPPA), all of which guarantee the right of users to remove their data from the datasets (Right to be Forgotten). Data removal requests frequently arise in practice, especially for sensitive datasets pertaining to medical records (numerous machine learning models in computational biology are trained using UK Biobank (Sudlow et al., 2015) which hosts a collection of genetic and medical records of roughly half a million patients (Ginart et al., 2019)). Removing user data from a dataset is insufficient to ensure sufficient privacy, since training data can often be reconstructed from trained models (Fredrikson et al., 2015;Veale et al., 2018). This motivates the study of machine unlearning (Cao & Yang, 2015) which aims to efficiently eliminate the influence of certain data points on a model. Naively, one can retrain the model from scratch to ensure complete removal, yet retraining comes at a high computational cost and is thus not practical when accommodating frequent removal requests. To avoid complete retraining, specialized approaches have to be developed for each unlearning application (Ginart et al., 2019;Guo et al., 2020;Bourtoule et al., 2021;Sekhari et al., 2021). Figure 1: Overview of our proposed FC framework. K-means++ initialization and quantization are performed at each client in parallel. The SCMA procedure ensures that only the server knows the aggregated statistics of clients, without revealing who contributed the points in each individual cluster. The server generates points from the quantization bins with prescribed weights and performs full K-means++ clustering to infer the global model. At the same time, federated learning (FL) has emerged as a promising approach to enable distributed training over a large number of users while protecting their privacy (McMahan et al., 2017;Chen et al., 2020;Kairouz et al., 2021;Wang et al., 2021;Bonawitz et al., 2021). The key idea of FL is to keep user data on their devices and train global models by aggregating local models in a communicationefficient and secure manner. Due to model inversion attacks (Zhu et al., 2019;Geiping et al., 2020), secure local model aggregation at the server is a critical consideration in FL, as it guarantees that the server cannot get specific information about client data based on their local models (Bonawitz et al., 2017;Bell et al., 2020;So et al., 2022;Chen et al., 2022). Since data privacy is the main goal in FL, it should be natural for a FL framework to allow for frequent data removal of a subset of client data in a cross-silo setting (e.g., when several patients request their data to be removed in the hospital database), or the entire local dataset for clients in a cross-device setting (e.g., when users request apps not to track their data on their phones). This leads to the largely unstudied problem termed federated unlearning (Liu et al., 2021;Wu et al., 2022;Wang et al., 2022). However, existing federated unlearning methods do not come with theoretical performance guarantees after model updates, and often, they are vulnerable to adversarial attacks.\n\nOur contributions are summarized as follows. 1) We introduce the problem of machine unlearning in FC, and design a new end-to-end system ( Fig. 1) that performs highly efficient FC with privacy and low communication-cost guarantees, which also enables, when needed, simple and effective unlearning. 2) As part of the FC scheme with unlearning features, we describe a novel one-shot FC algorithm that offers order-optimal approximation for the federated K-means clustering objective, and also outperforms the handful of existing related methods (Dennis et al., 2021;Ginart et al., 2019), especially for the case when the cluster sizes are highly imbalanced. 3) For FC, we also describe a novel sparse compressed multiset aggregation (SCMA) scheme which ensures that the server only has access to the aggregated counts of points in individual clusters but has no information about the point distributions at individual clients. SCMA securely recovers the exact sum of the input sparse vectors with a communication complexity that is logarithmic in the vector dimension, outperforming existing sparse secure aggregation works (Beguier et al., 2020;Ergun et al., 2021), which have a linear complexity. 4) We theoretically establish the unlearning complexity of our FC method and show that it is significantly lower than that of complete retraining. 5) We compile a collection of datasets for benchmarking unlearning of federated clusters, including two new datasets containing methylation patterns in cancer genomes and gut microbiome information, which may be of significant importance to computational biologists and medical researchers that are frequently faced with unlearning requests. Experimental results reveal that our one-shot algorithm offers an average speed-up of roughly 84x compared to complete retraining across seven datasets.\n\n\nRELATED WORKS\n\nDue to space limitations, the complete discussion about related works is included in Appendix A.\n\nFederated clustering. The goal of this learning task is to perform clustering using data that resides at different edge devices. Most of the handful of FC methods are centered around the idea of sending exact (Dennis et al., 2021) or quantized client (local) centroids (Ginart et al., 2019) directly to the server, which may not ensure desired levels of privacy as they leak the data statistics or cluster information of each individual client. To avoid sending exact centroids, Li et al. (2022) proposes sending distances between data points and centroids to the server without revealing the membership of data points to any of the parties involved, but their approach comes with large computational and communication overhead. Our work introduces a novel communication-efficient secure FC framework, with a new privacy criterion that is intuitively appealing as it involves communicating obfuscated point counts of the clients to the server and frequently used in FL literature (Bonawitz et al., 2017).\n\nMachine unlearning. Two types of unlearning requirements were proposed in previous works: exact unlearning (Cao & Yang, 2015;Ginart et al., 2019;Bourtoule et al., 2021;Chen et al., 2021) and approximate unlearning (Guo et al., 2020;Golatkar et al., 2020a;b;Sekhari et al., 2021;Fu et al., 2022;Chien et al., 2022). For exact unlearning, the unlearned model is required to perform identically as a completely retrained model. For approximate unlearning, the \"differences\" in behavior between the unlearned model and the completely retrained model should be appropriately bounded. A limited number of recent works also investigated data removal in the FL settings (Liu et al., 2021;Wu et al., 2022;Wang et al., 2022); however, most of them are empirical methods and do not come with theoretical guarantees for model performance after removal and/or for the unlearning efficiency. In contrast, our proposed FC framework not only enables efficient data removal in practice, but also provides theoretical guarantees for the unlearned model performance and for the expected time complexity of the unlearning procedure.\n\n\nPRELIMINARIES\n\nWe start with a formal definition of the centralized K-means problem. Given a set of n points in R d X arranged into a matrix X ∈ R n×d , and the number of clusters K, the K-means problem asks for finding a set of points\nC = {c 1 , ..., c K }, c k ∈ R d , ∀k ∈ [K] that minimizes the objective φ c (X ; C) = X − C 2 F ,(1)\nwhere || · || F denotes the Frobenius norm of a matrix, · denotes the 2 norm of a vector, and C ∈ R n×d records the closest centroid in C to each data point x i ∈ X (i.e., c i = arg min cj ∈C x i − c j ). Without loss of generality, we make the assumption that the optimal solution is unique in order to facilitate simpler analysis and discussion, and denote the optimum by C * = {c * 1 , ..., c * K }. The set of centroids C * induces an optimal partition\nK k=1 C * k over X , where ∀k ∈ [K], C * k = {x i : ||x i − c * k || ≤ ||x i − c * j || ∀i ∈ [n], j ∈ [K]}.\nWe use φ * c (X ) to denote the optimal value of the objective function for the centralized K-means problem. With a slight abuse of notation, we also use φ * c (C * k ) to denote the objective value contributed by the optimal cluster C * k . A detailed description of a commonly used approach for solving the K-means problem, K-means++, is available in Appendix B.\n\nIn FC, the dataset X is no longer available at the centralized server. Instead, data is stored on L edge devices (clients) and the goal of FC is to learn a global set of K centroids C s at the server based on the information sent by clients. For simplicity, we assume that there exists no identical data points across clients, and that the overall dataset X is the union of the datasets X (l) arranged as X (l) ∈ R n (l) ×d on device l, ∀l ∈ [L]. The server will receive the aggregated cluster statistics of all clients in a secure fashion, and generate the set C s . In this case, the federated K-means problem asks for finding K global centroids C s that minimize the objective\nφ f (X ; C s ) = L l=1 X (l) − C (l) s 2 F ,(2)\nwhere C (l) s ∈ R n (l) ×d records the centroids of the induced global clusters that data points {x\n(l) i } n (l) i=1\non client l belong to. Note that the definition of the assignment matrix C for the centralized K-means is different from that obtained through federated K-means C (l) s : the i-th row of C only depends on the location of x i while the row in C (l) s corresponding to x i depends on the induced global clusters that x i belongs to (for a formal definition see 3.1). In Appendix L, we provide a simple example that further illustrates the difference between C and C (l) s . Note that the notion of induced global clusters was also used in Dennis et al. (2021).\n\nDefinition 3.1. Suppose that the local clusters at client l are denoted by C (l) k , ∀k ∈ [K], l ∈ [L], and that the clusters at the server are denoted by C s k , ∀k ∈ [K]. The global clustering equals\nP k = {x (l) i |x (l) i ∈ C (l) j , c (l) j ∈ C s k , ∀j ∈ [K], l ∈ [L]}, where c (l) j is the centroid of C (l)\nj on client l. Note that (P 1 , . . . , P K ) forms a partition of the entire dataset X , and the representative centroid for P k is defined as c s,k ∈ C s . Exact unlearning. For clustering problems, the exact unlearning criterion may be formulated as follows. Let X be a given dataset and A a (randomized) clustering algorithm that trains on X and outputs a set of centroids C ∈ M, where M is the chosen space of models. Let U be an unlearning algorithm that is applied to A(X ) to remove the effects of one data point x ∈ X . Then U is an exact unlearning algorithm if ∀C ∈ M, x ∈ X , P(U(A(X ), X , x) = C) = P(A(X \\x) = C). To avoid confusion, in certain cases, this criterion is referred to as probabilistic (model) equivalence.\n\nPrivacy-accuracy-efficiency trilemma. How to trade-off data privacy, model performance, communication and computational efficiency is a long-standing problem in distributed learning (Acharya & Sun, 2019;Chen et al., 2020;Gandikota et al., 2021) that also carries over to FL and FC. Solutions that simultaneously address all these challenges in the latter context are still lacking. For example, Dennis et al. (2021) proposed a one-shot algorithm that takes model performance and communication efficiency into consideration by sending the exact centroids of each client to the server in a nonanonymous fashion. This approach may not be desirable under stringent privacy constraints as the server can gain information about individual client data. On the other hand, privacy considerations were addressed in Li et al. (2022) by performing K-means Lloyd's iterations anonymously via distribution of computations across different clients. Since the method relies on obfuscating pairwise distances for each client, it incurs computational overheads to hide the identity of contributing clients at the server and communication overheads due to interactive computations. None of the above methods is suitable for unlearning applications. To simultaneously enable unlearning and address the trilemma in the unlearning context, our privacy criterion involves transmitting the number of client data points within local client clusters in such a manner that the server cannot learn the data statistics of any specific client, but only the overall statistics of the union of client datasets. In this case, computations are limited and the clients on their end can perform efficient unlearning, unlike the case when presented with data point/centroid distances.\n\nAlgorithm 1 Secure Federated Clustering 1: input: Dataset X distributed on L clients (X (1) , . . . , X (L) ). 2: Run K-means++ initialization on each client l in parallel, obtain the initial centroid sets C (l) , and record the corresponding cluster sizes (|C with the aggregated vector denoted as q. 6: For index j ∈ {t : q t = 0}, sample q j points based on pre-defined distribution and denote their union as new dataset X s at server. 7: Run full K-means++ clustering at server with X s to obtain the centroid set C s at server. 8: return Each client retains its own centroid set C (l) , server retains X s , q and C s .\n\nRandom and adversarial removal. Most unlearning literature focuses on the case when all data points are equally likely to be removed, a setting known as random removal. However, adversarial data removal requests may arise when users are malicious in unlearning certain points that are critical for model training (i.e., boundary points in optimal clusters). We refer to such a removal request as adversarial removal. In Section 5, we provide theoretical analysis for both types of removal.\n\n\nFEDERATED CLUSTERING WITH SECURE MODEL AGGREGATION\n\nThe block diagram of our FC (Alg. 1) is depicted in Fig. 1. It comprises five components: a client-side clustering, client local information processing, secure compressed aggregation, server data generation, and server-side clustering module. We explain next the role of each component of the system.\n\nFor client-and server-side clustering (line 2 and 7 of Alg. 1), we adopt K-means++ as it lends it itself to highly efficient unlearning, as explained in Section 5. Specifically, we only run the K-means++ initialization procedure at each client but full K-means++ clustering (initialization and Lloyd's algorithm) at the server.\n\nLine 3 and 4 of Alg. 1 describe the procedure used to process the information of local client clusters. As shown in Fig. 1, we first quantize the local centroids to their closest centers of the quantization bins, and the spatial locations of quantization bins naturally form a tensor, in which we store the sizes of local clusters. A tensor is generated for each client l, and subsequently flattened to form a vector q (l) . For simplicity, we use uniform quantization with step size γ for each dimension (line 3 of Alg. 1, with more details included in Appendix H). The parameter γ > 0 determines the number of quantization bins in each dimension. If the client data is not confined to the unit hypercube centered at the origin, we scale the data to meet this requirement. Then the number of quantization bins in each dimension equals B = γ −1 , while the total number of quantization bins for d dimensions is\nB d = γ −d .\nLine 5 of Alg. 1 describes how to aggregate information efficiently at the server without leaking individual client data statistics. This scheme is discussed in Section 4.1. Line 6 pertains to generating q j points for the j-th quantization bin based on its corresponding spatial location. The simplest idea is to choose the center of the quantization bin as the representative point and assign weight q j to it. Then, in line 7, we can use the weighted K-means++ algorithm at the server to further reduce the computational complexity.\n\nA simplified version of Alg. 1 is discussed in Appendix I, for applications where the privacy criterion is not an imperative.\n\n\nSCMA AT THE SERVER\nAlgorithm 2 SCMA 1: input: L different vectors q (l) of length B d to be securely aggregated, a finite field F p . 2: Each client l ∈ [L] communicates (S (l) 1 , . . . , S (l) 2KL ) to the server, where S (l) i = ( j:q (l) j =0 q (l) j ·j i−1 +z (l) i ) mod p, i ∈ [2KL] and z (l)\ni is a random key uniformly distributed over F p and hidden from the server. The keys {z i ) mod p. Given S i , the server computes the coefficients of the polynomial g(x) = j:qj =0 (1 − j · x) using the Berlekamp-Massey algorithm (Berlekamp, 1968;Massey, 1969). Then, the server factorizes g(x) over the field F p to determine the roots j −1 , q j = 0, using the polynomial factorizing algorithm (Kedlaya & Umans, 2011). Finally, the server solves a set of\n2KL linear equations S i = l∈[L] S (l) i = j:qj =0 q j · j i−1 for i ∈ [2KL]\n, by considering q j as unknowns and j i−1 as known coefficients for q j = 0. 4: return q reconstructed at the server.\n\nOnce the vector representations q (l) of length B d for client l are generated (line 4 of Alg. 1), we can use standard secure model aggregation methods (Bonawitz et al., 2017;Bell et al., 2020;So et al., 2022) to sum up all q (l) securely and obtain the aggregated results q at the server. However, since the length of each vector q (l) is B d , securely aggregating the whole vector would lead to an exponential communication complexity for each client. Moreover, each q (l) is a sparse vector since the number of client centroids is much smaller than the number of quantization bins (i.e., K B d ). It is inefficient and unnecessary for each client to send out the entire q (l) with noisy masks for aggregation. This motivates us to first compress the vectors and then perform the secure aggregation, and we refer to this process as SCMA (Alg. 2), with one example illustrated in Fig. 2.\n\nBy observing that there can be at most K nonzero entries in q (l) , ∀l ∈ [L] and at most KL nonzero entries in q, we invoke the Reed-Solomon code construction (Reed & Solomon, 1960) for designing SCMA. Let F p = {0, 1, . . . , p − 1} be a finite field of prime order p ≥ max{n, B d }. We treat the indices of the quantization bins as distinct elements from the underlying finite field, and use them as evaluation points of the encoder polynomial. In addition, we treat a nonzero entry q (l) j in vector q (l) as a substitution error at the j-th entry in a codeword. Then, we use our SCMA scheme shown in Alg. 2, where the messages that the clients send to server can be treated as syndromes in Reed-Solomon decoding. Note that in our scheme, the server does not know q (l) , l ∈ [L] beyond the fact that l∈[L] q (l) = q, which fits into our privacy criterion. This follows because z \n\n\nPERFORMANCE ANALYSIS\n\nWe describe next the performance guarantees of Alg. 1 w.r.t. the objective defined in Eq. (2).\n\nTheorem 4.1. Suppose that we performed uniform quantization with step size γ in Algorithm 1. Then we have E (φ f (X ; C s )) < O(log 2 K) · φ * c (X ) + O(ndγ 2 log K). The performance guarantee in Theorem 4.1 pertains to two terms: the approximation of the optimal objective value and the quantization error (line 3 of Alg. 1). For the first term, the approximation factor O(log 2 K) is order-optimal for one-shot FC algorithms since one always needs to perform two rounds of clustering and each round will contribute a factor of O(log K). To make the second term a constant w.r.t. n, we can choose γ = Θ(1/ √ n), which is a good choice in practice for the tested datasets as well. The above conclusions hold for any distribution of data across clients. Note that SCMA does not contribute to the distortion as it always returns the exact sum, while other methods for sparse secure aggregation based on sparsification (Han et al., 2020) may introduce errors and degrade the FC objective. See Appendix D for more details.\n\n\nCOMPLEXITY ANALYSIS\n\nWe derived a cohort of in-depth analysis pertaining to the computational and communication complexity for our proposed FC framework (Alg. 1). Due to space limitations, these results are summarized in Appendix C.\n\n\nMACHINE UNLEARNING VIA SPECIALIZED SEEDING\n\nWe first describe an intuitive exact unlearning mechanism (Alg. 3) for K-means clustering in the centralized setting, which will be used later on as the unlearning procedure on the client-sides of the FC framework described in Section 5.3. The idea behind Alg. 3 is straightforward: one needs to rerun the K-means++ initialization, corresponding to retraining only if the current centroid set C contains at least one point requested for removal. This follows from two observations. First, since the centroids chosen through K-means++ initialization are true data points, the updated centroid set C returned by Alg. 3 is guaranteed to contain no information about the data points that have been removed. Second, as we will explain in the next section, Alg. 3 also satisfies the exact unlearning criterion (defined in Section 3).\n\n\nPERFORMANCE ANALYSIS\n\nTo verify that Alg. 3 is an exact unlearning method, we need to check that C is probabilistically equivalent to the models generated by rerunning the K-means++ initialization process on X , the set of point remaining after removal. This is guaranteed by Lemma 5.1, and a formal proof is provided in Appendix E. Lemma 5.1. For any set of data points X and removal set X R , assuming that the remaining dataset is X = X \\X R and the centroid set returned by Algorithm 3 is C , we have\nP(U(A(X ), X , X R ) = C) = P(A(X ) = C); E(φ c (X ; C )) ≤ 8(ln K + 2)φ * c (X )\n, where A represents Algorithm 1 and U represents the unlearning mechanism in Algorithm 3.\n\n\nCOMPLEXITY ANALYSIS\n\nWe present next analytical results for the expected time complexity of removing a batch of R data points simultaneously by our Alg. 3. For this, we consider both random and adversarial removal scenarios. While the analysis for random removal is fairly straightforward, the analysis for adversarial removal requests requires us to identify which removals force frequent retraining from scratch. In this regard, we state two assumptions concerning optimal cluster sizes and outliers, which will allow us to characterize the worst-case scenario removal setting.\n\nAssumption 5.2. Let 1 = n Ksmin be a constant denoting cluster size imbalance, where s min equals the size of the smallest cluster in the optimal clustering; when 1 = 1, all clusters are of size n K . Assumption 5.3. Assume that 2 ≥ 1 is a fixed constant. An outlier\nx i in X satisfies x i − c * j ≤ x i − c * k , ∀k ∈ [K] and x i − c * j > 2 φ * c (C * j )/|C * j |. Algorithm 3 Unlearning via K-means++ Init. 1: input: Dataset X , centroid set C obtained by K-means++ initialization on X , re- moval request set X R = {x r1 , . . . , x r R }. 2: if c j / ∈ X R ∀c j ∈ C then 3: C ← C 4: else 5: i ← (arg min j c j ∈ X R ) − 1 6: if i = 0 then 7: C ← ∅, X ← X \\X R . 8: else 9: C ← {c 1 , . . . , c i }, X ← X \\X R . 10: end if 11: for j = i + 1, . . . , K do 12: Sample x from X with prob d 2 (x,C ) φc(X ;C ) . 13: C ← C ∪ {x}.\n14:\n\nend for 15: end if 16: return C Under Assumptions 5.2 and 5.3, we arrive at an estimate for the expected removal time presented in Theorem 5.4 below. Notably, the expected removal time does not depend on the data set size n. Theorem 5.4. Assume that the number of data points in X is n and the probability of the data set containing at least one outlier is upper bounded by O (1/n). Algorithm 3 supports removing R points within one single request with expected time min{O(RK 2 d), O(nKd)} for random removals, and expected time min{O(RK 3 1 2 d), O(nKd)} in expectation for adversarial removals. The complexity for complete retraining equals O(nKd). Remark. Due to the distance-based K-means++ initialization procedure, the existence of outliers in the dataset inevitably leads to higher retraining probability. This is the case since outliers are more likely to lie in the initial set of centroids. Hence, for analytical purposes, we assume in Theorem 5.4 that the probability of the data set containing at least one outlier is upper bounded by O (1/n). This is not an overly restrictive assumption as there exist many different approaches for removing outliers before clustering Chawla & Gionis (2013); Gan & Ng (2017); Hautamäki et al. (2005), which effectively make the probability of outliers negligible.\n\n\nUNLEARNING FEDERATED CLUSTERS\n\nWe describe next the complete unlearning algorithm for the new FC framework which uses Alg. 3 for client-level clustering. In the FL setting, data resides on client storage devices, and thus the basic assumption of federated unlearning is that the removal requests will only appear at the client side, and the removal set will not be known to other unaffected clients and the server. We consider two types of removal requests in the FC setting: removing R points from one client l (cross-silo, single-client removal), and removing all data points from R clients l 1 , . . . , l R (cross-device, multi-client removal). For the case where multiple clients want to unlearn only a part of their data, the approach is similar to that of single-client removal and can be handled via simple union bounds.\n\nThe unlearning procedure is depicted in Alg. 4. For single-client data removal, the algorithm will first perform unlearning at the client (say, client l) following Alg. 3. If the client's local clustering changes (i.e., client l reruns the initialization), one will generate a new vector q (l) and send it to the server via SCMA. The server will rerun the clustering procedure with the new aggregated vector q and generate a new set of global centroids C s . Note that other clients do not need to perform additional computations during this stage. For multi-client removals, we follow a similar strategy, except that no client needs to perform additional computations. Same as centralized unlearning described in Lemma 5.1, we can show that Alg. 4 is also an exact unlearning method.\n\n\nRemoval time complexity.\n\nFor single-client removal, we know from Theorem 5.4 that the expected removal time complexity of client l is min{O(RK 2 d), O(n (l) Kd)} and min{O(RK 3 1 2 d), O(n (l) Kd)} for random and adversarial removals, respectively. n (l) denotes the number of data points on client l. Other clients do not require additional computations, since their centroids will not be affected by the removal requests. Meanwhile, the removal time complexity for the server is upper bounded by O(K 2 LT d), where T is the maximum number of iterations of Lloyd's algorithm at the server before convergence. For multi-client removal, no client needs to perform additional computations, and the removal time complexity for the server equals O((L − R)K 2 T d).\n\n\nAlgorithm 4 Unlearning of Federated Clusters\n1: input: Dataset X distributed on L clients (X (1) , . . . , X (L) ), (C (l) , X s , q, C s ) obtained by Algorithm 1 on X , removal request set X (l)\nR for single-client removal or L R for multiclient removal. 2: if single-client removal then 3:\n\nRun Algorithm 3 on client l and update q (l) if client l has to perform retraining. 4: else 5: q (l) ← 0 on client l, ∀l ∈ L R . 6: end if 7: Securely sum up q (l) at server by Algorithm 2, with the aggregated vector denoted as q . 8: if q = q then 9:\n\nC s ← C s . 10: else 11:\n\nGenerate X s with q .\n\n\n12:\n\nRun full K-means++ at the server with X s to obtain C s . 13: end if 14: return Client centroid sets C (l) , server data X s , q and centroids C s .\n\nTo empirically characterize the trade-off between the efficiency of data removal and performance of our newly proposed FC method, we compare it with baseline methods on both synthetic and real datasets. Due to space limitations, more in-depth experiments and discussions are delegated to Appendix M.\n\nDatasets and baselines. We use one synthetic dataset generated by a Gaussian Mixture Model (Gaussian) and six real datasets (Celltype, Covtype, FEMNIST, Postures, TMI, TCGA) in our experiments. We preprocess the datasets such that the data distribution is non-i.i.d. across different clients. The symbol K in Fig. 3 represents the maximum number of (true) clusters among clients, while K represents the number of true clusters in the global dataset. A detailed description of the data statistics and the preprocessing procedure is available in Appendix M.\n\nSince there is currently no off-the-shelf algorithm designed for unlearning federated clusters, we adapt DC-Kmeans (DC-KM) from Ginart et al. (2019) to apply to our problem setting, and use complete retraining as the baseline comparison method. To evaluate FC performance on the complete dataset (before data removals), we also include the K-FED algorithm from Dennis et al. (2021) as the baseline method. In all plots, our Alg. 1 is referred to as MUFC. Note that in FL, clients are usually trained in parallel so that the estimated time complexity equals the sum of the longest processing time of a client and the processing time of the server.\n\nClustering performance. The clustering performance of all methods on the complete dataset is shown in the first row of Tab. 1. The loss ratio is defined as φ f (X ; C s )/φ * c (X ) 1 , which is the metric used to evaluate the quality of the obtained clusters. For the seven datasets, MUFC offered the best performance on TMI and Celltype, datasets for which the numbers of data points in different clusters are highly imbalanced. This can be explained by pointing out an important difference between MUFC and K-FED/DC-KM: the quantized centroids sent by the clients may have non-unit weights, and MUFC is essentially performing weighted K-means++ at the server. In contrast, both K-FED and DC-KM assign equal unit weights to the client's centroids. Note that assigning weights to the client's centroids based on local clusterings not only enables a simple analysis of the scheme but also improves the empirical performance, especially for datasets with highly imbalanced cluster distributions. For all other datasets except Gaussian, MUFC obtained competitive clustering performance compared to K-FED/DC-KM. The main reason why DC-KM outperforms MUFC on Gaussian data is that all clusters are of the same size in this case. Also note that DC-KM runs full K-means++ clustering for each client while MUFC only performs initialization. Although running full K-means++ clustering at the client side can improve the empirical performance on certain datasets, it also greatly increases the computational complexity during training and the retraining probability during unlearning, which is shown in Fig. 3. Nevertheless, we also compare the performance of MUFC with K-FED/DC-KM when running full K-means++ clustering on clients for MUFC in Appendix M.\n\nWe also investigated the influence of K and γ on the clustering performance. Fig. 3(a) shows that MUFC can obtain a lower loss ratio when K < K, indicating that data is non-i.i.d. distributed across clients. Fig. 3(b) shows that the choice of γ does not seem to have a strong influence on the clustering performance of Gaussian datasets, due to the fact that we use uniform sampling in Step 6 of Alg. 1 to generate the server dataset. Meanwhile, Fig. 3(c) shows that γ can have a significant influence on the clustering performance of real-world datasets, which agrees with our analysis in Theorem 4.1. Loss ratio MUFC 1.24 ± 0.10 1.14 ± 0.03 1.25 ± 0.02 1.18 ± 0.05 1.10 ± 0.01 1.20 ± 0.00 1.03 ± 0.02 K-FED 1.84 ± 0.07 1.72 ± 0.24 1.25 ± 0.01 1.56 ± 0.11 1.13 ± 0.01 1.21 ± 0.00 1.60 ± 0.01 DC-KM 1.54 ± 0.13 1.46 ± 0.01 1.02 ± 0.00 1.15 ± 0.02 1.03 ± 0.00 1.18 ± 0.00 1.03 ± 0.02 Unlearning performance. Since K-FED does not support data removal, has high computational complexity, and its empirical clustering performance is worse than DC-KM (see Tab. 1), we only compare the unlearning performance of MUFC with that of DC-KM. For simplicity, we consider removing one data point from a uniformly at random chosen client l at each round of unlearning. The second row of Tab. 1 records the speed-up ratio w.r.t. complete retraining for one round of MUFC unlearning (Alg. 4) when the removed point does not lie in the centroid set selected at client l. Fig. 3(e) shows the accumulated removal time on the TMI dataset for adversarial removals, which are simulated by removing the data points with the highest contribution to the current value of the objective function at each round, while Fig. 3(f)-(l) shows the accumulated removal time on different datasets for random removals. The results show that MUFC maintains high unlearning efficiency compared to all other baseline approaches, and offers an average speed-up ratio of 84x when compared to complete retraining for random removals across seven datasets. We also report the change in the loss ratio of MUFC during unlearning in Fig. 3(d). The loss ratio remains nearly constant after each removal, indicating that our unlearning approach does not significantly degrade clustering performance. Similar conclusions hold for other tested datasets, as shown in Appendix M.\nSpeed-up of MUFC (if no retraining is performed) 151x 1535x 2074x 483x 613x 53x 267x\n\nETHICS STATEMENT\n\nThe seven datasets used in our simulations are all publicly available. Among these datasets, TCGA and TMI contain potentially sensitive biological data and are downloaded after logging into the database. We adhered to all regulations when handling this anonymized data and will only release the data processing pipeline and data that is unrestricted at TCGA and TMI. Datasets that do not contain sensitive information can be downloaded directly from their open-source repositories.\n\n\nREPRODUCIBILITY STATEMENT\n\nOur implementation is available at https://github.com/thupchnsky/mufc. Detailed instructions are included in the source code. \n\n\nACKNOWLEDGMENT\n\n\nA RELATED WORKS\n\nFederated clustering. The idea of FC is to perform clustering using data that resides at different edge devices. It is closely related to clustered FL (Sattler et al., 2020), whose goal is to learn several global models simultaneously, based on the cluster structure of the dataset, as well as personalization according to the cluster assignments of client data in FL (Mansour et al., 2020). One difference between FC and distributed clustering (Guha et al., 2003;Ailon et al., 2009) (2022) proposes sending distances between data points and centroids to the server without revealing the membership of data points to any of the parties involved. Note that there is currently no formal definition of computational or information-theoretic secrecy/privacy for FC problems, making it hard to compare methods addressing different aspects of FL. Our method introduces a simple-to-unlearn clustering process and new privacy mechanism that is intuitively appealing as it involves communicating obfuscated point counts of the clients to the server.\n\nSparse secure aggregation. Sparse secure aggregation aims to securely aggregate local models in a communication-efficient fashion for the case that the local models are high-dimensional but sparse. In comparison, our SCMA scheme can securely recover the exact sum of the input sparse models with a communication complexity that is logarithmic in the model dimension.\n\nPrivate set union. The private set union (Kissner & Song, 2005;Frikken, 2007;Seo et al., 2012) is a related but different problem compared to sparse secure aggregation. It requires multiple parties to communicate with each other to securely compute the union of their sets. In SCMA we aggregate multisets, which include the frequency of each element that is not considered in the private set union problem. In addition, our scheme includes only one round of communication from the clients to the server, while there is no server in the private set union problem but multi-round client to client communication is needed.\n\nMachine unlearning. For centralized machine unlearning problems, two types of unlearning requirements were proposed in previous works: exact unlearning and approximate unlearning. For exact unlearning, the unlearned model is required to perform identically as a completely retrained model. To achieve this, Cao & Yang (2015)  . Although obtaining the exact optimal solution for the K-means problem is difficult, there are many methods that can obtain quality approximations for the optimal centroids. For example, a randomized initialization algorithm (K-means++) was introduced in Vassilvitskii & Arthur (2006) and the expected objective value after initialization is a (log K)-approximation to the optimal objective (E(φ) ≤ (8 ln K + 16)φ * ). K-means++ initialization works as follows: initially, the centroid set C is assumed to be empty. Then, a point is sampled uniformly at random from X for the first centroid and added to C. For the following K − 1 rounds, a point x from X is sampled with probability d 2 (x, C)/φ c (X ; C) for the new centroid and added to C. Here, d(x, C) denotes the minimum 2 distance between x and the centroids in C chosen so far. After the initialization step, we arrive at K initial centroids in C used for running Lloyd's algorithm.\n\n\nC COMPLEXITY ANALYSIS OF ALGORITHM 1\n\nComputational complexity of client-side clustering. Client-side clustering involves running K-means++ initialization procedure, which is of complexity O(nKd).\n\nComputational complexity of server-side clustering. Server-side clustering involves running K-means++ initialization procedure followed by Lloyd's algorithm with T iterations, which is of complexity O(K 2 LT d).\n\nComputational complexity of SCMA at the client end. The computation of S (l) i on client l requires at most O(K log i) multiplications over F p , i ∈ [2KL]. The total computational complexity equals O(K 2 L log(KL)) multiplication and addition operations over F p .\n\nComputational complexity of SCMA at the server end. The computational complexity at the server is dominated by the complexity of the Berlekamp-Massey decoding algorithm (Berlekamp, 1968;Massey, 1969), factorizing the polynomial g(x) over F p (Kedlaya & Umans, 2011), and solving the linear equations S i = l∈[L] S (l) i = j:qj =0 q j · j i−1 with known j, q j = 0. The complexity of Berlekamp-Massey decoding over F p is O(K 2 L 2 ). The complexity of factorizing a polynomial g(x) over F p using the algorithm in Kedlaya & Umans (2011) is O((KL) 1.5 log p + KL log 2 p) operations over F p . The complexity of solving for S i = l∈[L] S (l) i equals that of finding the inverse of a Vandermonde matrix, which takes O(K 2 L 2 ) operations over F p (Eisinberg & Fedele, 2006). Hence, the total computational complexity at the server side is max{O(K 2 L 2 ), O((KL) 1.5 log p + KL log 2 p)} operations over F p .\n\nCommunication complexity of SCMA at the client end. Since each S = max{O(KL log n), O(KLd log B)} bits, which implies that our scheme is order-optimal w.r.t. the communication cost. Note that following standard practice in the area, we do not take into account the complexity of noise generation in secure model aggregation, as it can be done offline and independently of the Reed-Solomon encoding procedure.\n\n\nD PROOF OF THEOREM 4.1\n\nProof. We first consider the case where no quantization is performed (Algorithm 5). The performance guarantees for the federated objective value in this setting are provided in Lemma D.1.\n\nLemma D.1. Suppose that the entire data set across clients is denoted by X , and the set of server centroids returned by Algorithm 5 is C s . Then we have\nE (φ f (X ; C s )) < O(log 2 K) · φ * c (X ).\nProof. Let C * denote the optimal set of centroids that minimize the objective (1) for the entire dataset X ∈ R n×d , let C * ∈ R n×d be the matrix that records the closest centroid in C * to each data point, C s the set of centroids returned by Alg. 1, and C s ∈ R n×d the matrix that records the corresponding centroid in C s for each data point based on the global clustering defined in Definition 3.1. Since we perform K-means++ initialization on each client dataset, for client l it holds\nE X (l) − C (l) 2 F ≤ (8 ln K + 16) X (l) − C (l) * 2 F , ∀l ∈ [L] ≤ (8 ln K + 16) X (l) − C * ,(l) 2 F (3)\nwhere C (l) ∈ R n (l) ×d records the closest centroid in C (l) to each data point x i in X (l) , C (l) * is the optimal solution that can minimize the local K-means objective for client l, and C * ,(l) denotes the row in C * that corresponds to client l. Summing up (3) over all clients gives\nE L l=1 X (l) − C (l) 2 F ≤ (8 ln K + 16) L l=1 X (l) − C * ,(l) 2 F .(4)\nAt the server side the client centroids are reorganized into a matrix X s ∈ R n×d . The weights of the client centroids are converted to replicates of rows in X s . Since we perform full K-means++ clustering at the server, it follows that\nE X s − C s 2 F = E L l=1 C (l) − C (l) s 2 F (a) ≤ (8 ln K + 16) L l=1 E C (l) − C (l) s, * 2 F ≤ (8 ln K + 16) L l=1 E C (l) − C * ,(l) 2 F ,(5)\nwhere C s, * ∈ R n×d is the optimal solution that minimizes the K-means objective at the server. It is worth pointing out that C s, * is different from C * , as they are optimal solutions for different optimization objectives. Note that we still keep the expectation on RHS for (a). The randomness comes from the fact that C (l) is obtained by K-means++ initialization, which is a probabilistic procedure.\n\nCombining (4) and (5) results in\nE (φ f (X ; C s )) = E L l=1 X (l) − C (l) s 2 F ≤ 2 · E L l=1 X (l) − C (l) 2 F + C (l) − C (l) s 2 F ≤ (16 ln K + 32) L l=1 X (l) − C * ,(l) 2 F + E C (l) − C * ,(l) 2 F . (6) For E C (l) − C * ,(l) 2 F , we have E C (l) − C * ,(l) 2 F ≤ 2 · E C (l) − X (l) 2 F + X (l) − C * ,(l) 2 F = 2 · X (l) − C * ,(l) 2 F + 2 · E C (l) − X (l) 2 F .(7)\nReplacing (7) into (6) shows that E (φ f (X ; C s )) < O(log 2 K) · φ * c (X), which completes the proof.\n\nIf we are only concerned with the performance of non-outlier points over the entire dataset, we can upper bound the term E\nL l=1 C (l) − C * ,(l) 2 F by E L l=1 C (l) − C * ,(l) 2 F ≤ 2 φ * c (X ).(8)\nHere, we used the fact that rows of C (l) are all real data points sampled by the K-means++ initialization procedure. For each data point\nx i , it holds that x i − c * i 2 |C * i | ≤ 2 φ * c (C * i ), where x i ∈ C * i . In this case, we arrive at E (φ f (X t ; C s )) < O( 2 log K) · φ * c (X t ),\nwhere X t corresponds to all non-outlier points.\n\nRemark. In Theorem 4 of Guha et al. (2003) the authors show that for the distributed K-median problem, if we use a O(b)-approximation algorithm (i.e., φ ≤ O(b) · φ * ) for the K-median problem with subdatasets on distributed machines, and use a O(c)-approximation algorithm for the K-median problem on the centralized machine, the overall distributed algorithm achieves effectively a O(bc)approximation of the optimal solution to the centralized K-median problem. This is consistent with our observation that Alg. 5 can offer in expectation a O(log 2 K)-approximation to the optimal solution of the centralized K-means problem, since K-means++ initialization achieves a O(log K)approximation on both the client and server side.\n\nWe also point out that in Dennis et al. (2021) the authors assume that the exact number of clusters from the global optimal clustering on client l is known and equal to K (l) , and propose the K-FED algorithm which performs well when K = max l∈[L] K (l) ≤ √ K. The difference between K and K represents the data heterogeneity across different clients. With a slight modifications of the proof, we can also obtain E (φ f (X ; C s )) < O(log K · log K ) · φ * c (X ), when K (l) is known for each client beforehand, and perform K (l) -means++ on client l instead of K-means++ in Alg. 1. For the extreme setting where each client safeguards data of one entire cluster (w.r.t. the global optimal clustering (L = K, K = 1)), the performance guarantee for Alg. 1 becomes E (φ f (X ; C s )) < O(1) · φ * c (X ), which is the same as seeding each optimal cluster by a data point sampled uniformly at random from that cluster. From Lemma 3.1 of Vassilvitskii & Arthur (2006) we see that we can indeed have E (φ f (X ; C s )) = 2φ * c (X ), where the approximation factor does not depend on K. This shows that data heterogeneity across different clients can benefit the entire FC framework introduced.\n\nNext we show the proof for Theorem 4.1. Following the same idea as the one used in the proof of Lemma D.1, we arrive at\nE (φ f (X ; C s )) ≤ 3 · E L l=1 X (l) − C (l) 2 F + C (l) − C (l) 2 F + C (l) − C (l) s 2 F ,(9)\nwhere C (l) is the quantized version of C (l) . The first term can be upper bounded in the same way as in Lemma D.1. For the second term, the distortion introduced by quantizing one point is bounded by √ dγ 2 , if we choose the center of the quantization bin as the reconstruction point. Therefore,\nE L l=1 C (l) − C (l) 2 F ≤ n √ dγ 2 2 = ndγ 2 4 .(10)\nThe third term can be bounded as\nE L l=1 C (l) − C (l) s 2 F ≤ (8 ln K + 16) L l=1 E C (l) − C * ,(l) 2 F E C (l) − C * ,(l) 2 F ≤ 3 · E C (l) − C (l) 2 F + C (l) − X (l) 2 F + X (l) − C * ,(l) 2 F . (11)\nReplacing (10) and (11) into (9) leads to E (φ f (X ; C s )) < O(log 2 K) · φ * c (X ) + O(ndγ 2 log K), which completes the proof. Similar as in Lemma D.1, we can have that for non-outlier points X t ,\nE (φ f (X t ; C s )) < O( 2 log K) · φ * c (X t ) + O(ndγ 2 log K).\n\nE PROOF OF LEMMA 5.1\n\nProof. Assume that the number of data points in X is n, the size of X R is R, and the initial centroid set for X is C. We use induction to prove that C returned by Alg. 3 is probabilistically equivalent to rerunning the K-means++ initialization on X = X \\X R .\n\nThe base case of induction amounts to investigating the removal process for c 1 , the first point selected by K-means++. There are two possible scenarios: c 1 ∈ X R and c 1 / ∈ X R . In the first case, we will rerun the initialization process over X , which is equivalent to retraining the model. In the second case, since we know c 1 / ∈ X R , the probability of choosing c 1 from X as the first centroid equals the conditional probability 1 n − R = P(choose c 1 from X as the first centroid|c 1 / ∈ X R ) = P(choose c 1 from X as the first centroid).\n\nNext suppose that K > 1, i = (arg min j c j ∈ X R )−1. The centroids C i−1 = {c 1 = c 1 , . . . , c i−1 = c i−1 } returned by Alg. 3 can be viewed probabilistically equivalent to the model obtained from rerunning the initialization process over X for the first i − 1 rounds. Then we have\nP(choose c i from X as i-th centroid|c i / ∈ X R ) = P(choose c i from X as i-th centroid ∩ c i / ∈ X R ) P(c i / ∈ X R ) (a) = P(choose c i from X as i-th centroid) P(c i / ∈ X R ) = d 2 (c i , C i−1 )/φ c (X ; C i−1 ) 1 − x∈X R d 2 (x, C i−1 )/φ c (X ; C i−1 ) = d 2 (c i , C i−1 )/φ c (X ; C i−1 ) φ c (X ; C i−1 )/φ c (X ; C i−1 ) = d 2 (c i , C i−1 ) φ c (X ; C i−1 ) = P(choose c i from X as i-th centroid),\nwhere (a) holds based on the definition of i, indicating that the i-th centroid is not in X R . Therefore, the centroid c i = c i returned by Alg. 3 can be seen as if obtained from rerunning the initialization process over X in the i-th round. Again based on the definition of i, it is clear that for j > i, c j are the centroids chosen by the K-means++ procedure over X . This proves our claim that C returned by Alg. 3 is probabilistic equivalent to the result obtained by rerunning the K-means++ initialization on X .\n\nTheorem 1.1 of Vassilvitskii & Arthur (2006) then establishes that\nE(φ c (X ; C )) ≤ 8(ln K + 2)φ * c (X ),(12)\nwhich completes the proof.\n\n\nF PROOF OF THEOREM 5.4\n\nProof. We first analyze the probability of rerunning K-means++ initialization based on Alg. 3. Assumptions 5.2 and 5.3 can be used to derive an expression for the probability of x i ∈ C (where x i is the point that needs to be unlearned), which also equals the probability of retraining.\n\nLemma F.1. Assume that the number of data points in X is n and that the probability of the data set containing at least one outlier is upper bounded by O (1/n). Let C be the centroid set obtained by running K-means++ on X . For an arbitrary removal set X R ⊆ X of size R, we have for random removals: P(X R ∩ C = ∅) < O (RK/n) ;\n\nfor adversarial removals: P(X R ∩ C = ∅) < O RK 2 1 2 /n .\n\nProof. Since outliers can be arbitrarily far from all true cluster points based on definition, during initialization they may be sampled as centroids with very high probability. For simplicity of analysis, we thus assume that outliers are sampled as centroids with probability 1 if they exist in the dataset, meaning that we will always need to rerun the K-means++ initialization when outliers exist in the complete dataset before any removals.\n\nFor random removals, where the point requested for unlearning, x i , is drawn uniformly at random from X , it is clear that P(x i ∈ C) = K n , since C contains K distinct data points in X . For adversarial removals, we need to analyze the probability of choosing x i as the (k + 1)-th centroid, given that the first k centroids have been determined and x i / ∈ C k = {c 1 , . . . , c k }. For simplicity we first assume that there is no outlier in X . Then we have\nP(choose x i from X as the (k + 1)-th centroid|C k ) = d 2 (x i , C k ) y =xi d 2 (y, C k ) + d 2 (x i , C k )(13)\nFor the denominator y =xi d 2 (y, C k ) + d 2 (x i , C k ), the following three observations are in place\ny =xi d 2 (y, C k ) + d 2 (x i , C k ) ≥ φ * c (X ) ≥ φ * c (C * i ), x i ∈ C * i y =xi d 2 (y, C k ) + d 2 (x i , C k ) ≥ y =xi d 2 (y, C * ) y =xi d 2 (y, C k ) + d 2 (x i , C k ) ≥ y =xi d 2 (y, C k ).\nTherefore,\ny =xi d 2 (y, C k ) + d 2 (x i , C k ) ≥ φ * c (C * i ) 5 + 2 5   y =xi d 2 (y, C * ) + d 2 (y, C k )   (a) ≥ 1 5   φ * c (C * i ) + y =xi c y − c * y 2   ,(14)\nwhere c y , c * y are the closest centroid in C k and C * to y, respectively. Here, (a) is a consequence of the fact that a − b 2 = a − c + c − b 2 ≤ 2( a − c 2 + b − c 2 ). Since x i is not an outlier for C * i based on our assumption, we have\nφ * c (C * i ) ≥ |C * i | 2 x i − c * i 2 ≥ n K 1 2 x i − c * i 2 .\nConsequently,\nφ * c (C * i ) + y =xi c y − c * y 2 ≥ |C * i | 2 x i − c * i 2 + y∈C * i c y − c * y 2 = |C * i | 2 x i − c * i 2 + y∈C * i c y − c * i 2 .(15)\nFor\n∀y ∈ C * i , it hold x i − c * i 2 + c y − c * i 2 ≥ 1 2 x i − c y 2 ≥ 1 2 d 2 (x i , C k ).\nThus, (15) can be lower bounded by\n|C * i | 2 x i − c * i 2 + y∈C * i c y − c * i 2 ≥ |C * i | 2 2 d 2 (x i , C k ) ≥ n 2K 1 2 d 2 (x i , C k ).(16)\nCombining (16) and (14) we obtain y =xi\nd 2 (y, C k ) + d 2 (x i , C k ) ≥ n 10K 1 2 d 2 (x i , C k ).\nUsing this expression in (13) results in P(choose x i from X as the (k + 1)-th centroid|C k ) ≤ 10K 1 2 n ,\n\nwhich holds for ∀k ∈ [K]. Thus, the probability P(x i ∈ C) can be computed as\nP(x i ∈ C) = K−1 k=0 P(choose x i from X as the (k + 1)-th centroid|C k )P(C k ) ≤ K−1 k=0 P(choose x i from X as the (k + 1)-th centroid|C k ) ≤ 1 n + 10K(K − 1) 1 2 n < O K 2 1 2 n .(18)\nHere, we assumed that C 0 = ∅.\n\nFor the case where outliers are present in the dataset, we have\nP(x i ∈ C) = P(x i ∈ C|x i is outlier)P(x i is outlier) + P(x i ∈ C|x i is not outlier)P(x i is not outlier) ≤ 1 · O 1 n + O K 2 1 2 n · 1 < O K 2 1 2 n ,\nwhich completes the proof for the adversarial removal scenario. Finally, by union bound we can have that for the removal set X R of size R, random removals: P(X R ∩ C = ∅) < O RK n ;\n\nadversarial removals:\nP(X R ∩ C = ∅) < O RK 2 1 2 n .\nAlso, the probability naturally satisfies that P(X R ∩ C = ∅) ≤ 1.\n\nNext we show the proof for Theorem 5.4. The expected removal time for random removals can be upper bounded by E(Removal time) = E(Removal time|new initialization needed)P(new initialization needed)+ E(Removal time|new initialization not needed)P(new initialization not needed)\n≤ O(nKd + RK) · O RK n + O(RK) · 1 < O(RK 2 d).\nFollowing a similar argument, we can also show that the expected removal time for adversarial removals can be upper bounded by O(RK 3 1 2 d). And based on our Algorithm 3, the unlearning complexity for both types of removal requests would be always upper bounded by the retraining complexity O(nKd) as well, which completes the proof.\n\n\nG COMPARISON BETWEEN ALGORITHM 3 AND QUANTIZED K-MEANS\n\nIn Ginart et al. (2019), quantized K-means were proposed to solve a similar problem of machine unlearning in the centralized setting. However, that approach substantially differs from Alg. 3. First, the intuition behind quantized K-means is that the centroids are computed by taking an average, and the effect of a small number of points is negligible when there are enough terms left in the clusters after removal. Therefore, if we quantize all centroids after each Lloyd's iteration, the quantized centroids will not change with high probability when we remove a small number of points from the dataset. Meanwhile, the intuition behind Alg. 3 is as described in Lemma F.1. Second, the expected removal time complexity for quantized K-means equals O R 2 K 3 T 2 d 2.5 / , which is high since one needs to check if all quantized centroids remain unchanged after removal at each iteration, where T denotes the maximum number of Lloyd's iteration before convergence and is some intrinsic parameter. In contrast, Alg. 3 only needs O(RK 3 1 2 d) even for adversarial removals. Also note that the described quantized K-means algorithm does not come with performance guarantees on removal time complexity unless it is randomly initialized.\n\n\nH QUANTIZATION\n\nFor uniform quantization, we setŷ = γ · a(y), where a(y) = arg min j∈Z |y − γj|, y ∈ R 2 . The parameter γ > 0 determines the number of quantization bins in each dimension. Suppose all client data lie in the unit hypercube centered at the origin, and that if needed, pre-processing is performed to meet this requirement. Then the number of quantization bins in each dimension equals B = γ −1 , while the total number of quantization bins for d dimensions is B d = γ −d .\n\nIn Section 4, we remarked that one can generate q j points by choosing the center of the quantization bin as the representative point and endow it with a weight equal to q j . Then, in line 7, we can use the weighted K-means++ algorithm at the server to further reduce the computational complexity, since the effective problem size at the server reduces from n to KL. However, in practice we find that when the computational power of the server is not the bottleneck in the FL system, generating data points uniformly at random within the quantization bins can often lead to improved clustering performance. Thus, this is the default approach for our subsequent numerical simulations. Algorithm 5 Simplified Federated K-means Clustering 1: input: Dataset X distributed on L clients (X (1) , . . . , X (L) ). 2: Run K-means++ initialization on each client l in parallel, obtain the initial centroid sets C (l) , and record the corresponding cluster sizes |C K | as the weights for the corresponding rows, ∀l ∈ [L]. 5: Run full weighted K-means++ clustering at server with X s to obtain the centroid set at server C s . 6: return Each client retains their own centroid set C (l) while the server retains X s and C s .\n\nIn line 5 of Alg. 5, weighted K-means++ would assign weights to data points when computing the sampling probability during the initialization procedure and when computing the average of clusters during the Lloyd's iterations. Since the weights we are considering here are always positive integers, a weighted data point can also be viewed as there exist identical data points in the dataset with multiplicity equals to the weight.\n\n\nJ THE UNIQUENESS OF THE VECTOR q GIVEN {S i } i∈[2KL]\n\nTo demonstrate that the messages generated by Alg. 2 can be uniquely decoded, we prove that there exists a unique q that produces the aggregated values {S i } i∈ [2KL] at the server. The proof is by contradiction. Assume that there exist two different vectors q and q that result in the same {S i } i∈ [2KL] . In this case, we have the following set of linear equations j:qj =0 q j ·j i−1 − j:q j =0 q j · j i−1 = 0, i ∈ [2KL]. Given that {q j : q j = 0} and {q j : q j = 0} represent at most 2KL unknowns and j i−1 coefficients, the linear equations can be described using a square Vandermonde matrix for the coefficients, with the columns of the generated by the indices of the nonzero entries in q. This leads to a contradiction since a square Vandermonde matrix with different column generators is invertible, which we show below. Hence, the aggregated values {S i } must be different for different q. Similarly, the sums j:\nq (l) j =0 q (l) j · j i−1 are distinct for different choices of vectors q (l) , i ∈ [2KL], l ∈ [L].\nIf two vectors q and q result in the same {S i } i∈[2KL] , then j:qj =0 q j ·j i−1 − j:q j =0 q j ·j i−1 = 0, for all i ∈ [2KL]. Let {i 1 , . . . , i u } = ({j : q j = 0} ∪ {j : q j = 0}) be the set of integers such that at least one of q im and q im is nonzero for m ∈ [u]. Note that u ≤ 2KL. Rewrite this equation as\n    1 · · · 1 i 1 · · · i u . . . . . . . . . i 2KL−1 1 · · · i 2KL−1 u        q i1 − q i1 . . . q iu − q iu    = 0.(19)\nSince u ≤ 2KL, we take the first u equations in (19) and rewrite them as\nBv = 0, where B =     1 · · · 1 i 1 · · · i u . . . . . . . . . i 2KL−1 1 · · · i 2KL−1 u     is a square Vandermonde matrix and v =    q i1 − q i1 . . . q iu − q iu   \nis a nonzero vector since q = q . It is known that the determinant of a square Vandermonde matrix B is given by m1<m2,m1,m2∈[u] (i m2 − i m1 ), which in our case is nonzero since all the i 1 , . . . , i u are different. Therefore, B is invertible and does not admit a non-zero solution, which contradicts the equation Bv = 0.\n\n\nK A DETERMINISTIC LOW-COMPLEXITY ALGORITHM FOR SCMA AT THE SERVER\n\nIn the SCMA scheme we described in Alg. 1, the goal of the server is to reconstruct the vector q, given values S i = j:qj =0 q j · j i−1 mod p for i ∈ [2KL]. To this end, we first use the Berlekamp-Massey algorithm to compute the polynomial g(x) = j:qj =0 (1 − j · x). Then, we factorize g(x) over the finite field F p using the algorithm described in Kedlaya & Umans (2011). The complexity O((KL) 1.5 log p + KL log 2 p) referred to in Section 4.3 corresponds to the average complexity (finding a deterministic algorithm that factorizes a polynomial over finite fields with poly(log p) worst-case complexity is an open problem). The complexity max{O(K 2 L 2 ), O((KL) 1.5 log p + KL log 2 p)} referred to in Appendix C for the SCMA scheme represents an average complexity.\n\nWe show next that the SCMA scheme has small worst-case complexity under a deterministic decoding algorithm at the server as well. To this end, we replace the integer p in Alg. 2 with a large number p ≥ max{KLB 2dKL , n} + 1 such that p is larger than the largest possible S i and there is no overflow when applying the modulo p operation on S i . It is known (Bertrand's postulate) that there exists a prime number between any integer n > 3 and 2n−2, and hence there must be a prime number lower-bounded by max{KLB 2dKL , n} + 1 and twice the lower bound 2(max{KLB 2dKL , n} + 1). However, since searching for a prime number of this size can be computationally intractable, we remove the requirement that p is prime. Correspondingly, F p is not necessarily a finite field. Then, instead of sending S\n(l) i = ( j:q (l) j =0 q (l) j · j i−1 + z (l) i ) mod p, client l, l ∈ [L], will send S (l) i = ( j:q (l) j =0 q (l) j · j i−1 + z (l) i ) mod p to the server, i ∈ [2KL], where random keys z (l)\ni are independently and uniformly distributed over {0, . . . , p − 1} and hidden from the server. After obtaining S i , i ∈ [2KL], the server can continue performing operations over the field of reals since there is no overflow in computing S i mod p . We note that though p is exponentially large, the computation of S We now present a low complexity secure aggregation algorithm at the server. After reconstructing S i , we have S i = j:qj =0 q j · j i−1 . The server switches to computations over the real field. First, it uses the Berlekamp-Massey algorithm to find the polynomial g(x) = j:qj =0 (1 − j · x) (the algorithm was originally proposed for decoding of BCH codes over finite fields, but it applies to arbitrary fields). Let m be the degree of g(x). Then h(x) = x m g(1/x) = j:qj =0 (x − j). The goal is to factorize h(x) over the field of reals, where the roots are known to be integers in [B d ] and the multiplicity of each root is one.\n\nIf the degree of h(x) is odd, then h(0) < 0 and h(B d ) > 0. Then we can use bisection search to find a root of h(x), which requires O(log B d ) polynomial evaluations of h(x), and thus O(M K log B d ) multiplication and addition operations of integers of size at most log p . After finding one root j, we can divide h(x) by x − j and start the next root-finding iteration.\n\nIf the degree of h(x) is even, then the degree of h (x) is odd, and the roots of h (x) are different and confined to [B d ]. We use bisection search to find a root j of h (x). If h(j ) < 0, then we use bisection search on [0, j ] = {0, 1, . . . , j } to find a root of h(x) and start a new iteration as described above when the degree of h(x) is odd. If h(j ) > 0, then h (j − 1) > 0 and h (0) < 0. We use bisection search to find another root of h (x) in [j − 1]. Note that for every two roots j 1 and j 2 (j 1 < j 2 ) of h (x) satisfying h(j 1 ) > 0 and h(j 2 ) > 0 we can always find another root j 3 of h (x) in [j 1 + 1, j 2 − 1]. We keep iterating the search for every two such roots j 1 , j 2 until we find a list of roots r 1 , . . . , r 2R+1 of h (x) such that h(r i ) < 0 for odd i in [2R + 1] and h(r i ) > 0 for even i ∈ [2R + 1]. Then we can run bisection search on the sets [0, r 1 ], [r 1 , r 2 ], . . . , [r 2R , r 2R+1 ], [r 2R+1 , B d ], to find 2R + 2 roots of h(x). Note that during the iteration we need 2R + 1 bisection search iterations to find the roots r 1 , . . . , r 2R+1 for h (x) and 2R + 2 bisection search iterations to find 2R + 2 roots for h(x). L DIFFERENCE BETWEEN THE ASSIGNMENT MATRICES C AND C s One example that explains the difference between these two assignment matrices is as follows. Suppose the global data sets and centroid sets are the same for the centralized and FC settings, i.e.,\nX =   X (1) · · · X (L)   , C = C s = {c 1 , . . . , c K }.\nSuppose that for x 1 , which is the first row of X, we have\nd(x 1 , c 1 ) < d(x 1 , c j ), ∀j ∈ [K], j = 1.\nThen, the first row of C equals c 1 . However, if x 1 resides on the memory of client l and belongs to the local cluster C (l)\n\ni , and the recorded local centroid c\n(l) i satisfies d c (l) i , c 2 < d c (l) i , c j , ∀j ∈ [K], j = 2,\nthen the first row of C s is c 2 , even if d(x 1 , c 1 ) < d(x 1 , c 2 ). Here C s is the row concatenation of the matrices C (l) s on client l. This example shows that the assignment matrices C and C s are different, which also implies that φ f and φ c are different.\n\n\nM EXPERIMENTAL SETUP AND ADDITIONAL RESULTS\n\n\nM.1 DATASETS\n\nIn what follows, we describe the datasets used in our numerical experiments. Note that we preprocessed all datasets such that the absolute value of each element in the data matrix is smaller than 1. Each dataset has an intrinsic parameter K for the number of optimal clusters, and these are used in the centralized K-means++ algorithm to compute the approximation of the optimal objective value. We use φ * c (X) in subsequent derivation to denote the objective value returned by the K-means++ algorithm. Besides K, we set an additional parameter K ∼ √ K for each client data so that the number of true clusters at the client level is not larger than K . This non-i.i.d. data distribution across clients is discussed in Dennis et al. (2021). For small datasets (e.g., TCGA, TMI), we consider the number of clients L as 10, and set L = 100 for all other datasets. Covtype [n = 15120, d = 52, K = 7] (Blackard & Dean, 1999) comprises digital spatial data for seven forest cover types obtained from the US Forest Service (USFS) and the US Geological Survey (USGS). There are 52 cartographic variables including slope, elevation, and aspect. The dataset has 15120 samples. The sizes of the seven clusters are 3742, 3105, 2873, 2307, 1482, 886, 725. Gaussian [n = 30000, d = 10, K = 10] comprises ten clusters, each generated from a 10-variate Gaussian distribution centered at uniformly at random chosen locations in the unit hypercube. From each cluster, 3000 samples are taken, for a total of 30000 samples. Each Gaussian cluster is spherical with variance 0.5. TMI [n = 1126, d = 984, K = 4] contains samples from human gut microbiomes. We retrieved 1126 human gut microbiome samples from the NIH Human Gut Microbiome (Peterson et al., 2009). Each data point is of dimension 983, capturing the frequency (concentration) of identified bacterial species or genera in the sample. The dataset can be roughly divided into four classes based on gender and age. The sizes of the four clusters are 934, 125, 46, 21.\n\n\nM.2 BASELINE SETUPS.\n\nWe use the publicly available implementation of K-FED and DC-KM as our baseline methods. For DC-KM, we set the height of the computation tree to 2, and observe that the leaves represent the clients. Since K-FED does not originally support data removal, has high computational complexity, and its clustering performance is not comparable with that of DC-KM (see Tab. 1), we thus only compare the unlearning performance of MUFC with DC-KM. During training, the clustering parameter K is set to be the same in both clients and server for all methods, no matter how the data was distributed across the clients. Experiments on all datasets except FEMNIST were repeated 5 times to obtain the mean and standard deviations, and experiments on FEMNIST were repeated 3 times due to the high complexity of training. Note that we used the same number of repeated experiments as in Ginart et al.\n\n.\n\n\nM.3 ENABLING COMPLETE CLIENT TRAINING FOR MUFC\n\nNote that both K-FED and DC-KM allow clients to perform full K-means++ clustering to improve the clustering performance at the server. Thus it is reasonable to enable complete client training for MUFC as well to compare the clustering performance on the full datasets. Although in this case we need to retrain affected clients and the server for MUFC upon each removal request, leading to a similar unlearning complexity as DC-KM, the clustering performance of MUFC is consistently better than that of the other two baseline approaches (see Tab. 2). This is due to the fact that we utilize information about the aggregated weights of client centroids. Loss ratio MUFC 1.05 ± 0.01 1.03 ± 0.00 1.02 ± 0.00 1.02 ± 0.01 1.02 ± 0.00 1.12 ± 0.00 1.02 ± 0.00 K-FED 1.84 ± 0.07 1.72 ± 0.24 1.25 ± 0.01 1.56 ± 0.11 1.13 ± 0.01 1.21 ± 0.00 1.60 ± 0.01 DC-KM 1.54 ± 0.13 1.46 ± 0.01 1.02 ± 0.00 1.15 ± 0.02 1.03 ± 0.00 1.18 ± 0.00 1.03 ± 0.02\n\n\nM.4 LOSS RATIO AND UNLEARNING EFFICIENCY\n\nIn Fig. 4 we plot results pertaining to the change of loss ratio after each removal request and the accumulated removal time when the removal requests are adversarial. The conclusion is consistent with the results in Section 6.\n\n\nM.5 BATCH REMOVAL\n\nIn Fig. 5 we plot the results pertaining to removing multiple points within one removal request (batch removal). Since in this case the affected client is more likely to rerun the K-means++ initialization for each request, it is expected that the performance (i.e., accumulated removal time) of our algorithm would behave more similar to retraining when we remove more points within one removal request, compared to the case in Fig. 3 where we only remove one point within one removal request.  \n\nK\n|), ∀l ∈ [L]. 3: Perform uniform quantization of C (l) on each dimension, and flatten the quantization bins into a vector q (l) , ∀l ∈ [L]. sum up q (l) at server by Algorithm 2,\n\n\nl∈[L],i∈[2KL] are generated offline using standard secure model aggregation so that ( l z (l) i ) mod p = 0. 3: The server first computes the sum S i = ( l∈[L] S (l)\n\n\nuniformly distributed over F p and independently chosen for different l ∈ [L], i ∈ [2KL]. For details, please refer to Appendix J.\n\nFigure 2 :\n2Example of the SCMA procedure for K = 2, L = 2, B d = 4, n = 12, p = 13.\n\nFigure 3 :\n3The shaded areas represent the standard deviation of results from different trails. (a) Influence of data heterogeneity on the clustering performance of MUFC: K represents the maximum number of (global) clusters covered by the data at the clients, while K = 10 indicates that the data points are i.i.d. distributed across clients. (b)(c) Influence of the quantization step size γ on the clustering performance of MUFC. The red vertical line indicates the default choice of γ = 1/ √ n, where n is the total number of data points across clients. (d) The change in the loss ratio after each round of unlearning. (e) The accumulated removal time for adversarial removals. (f)-(l) The accumulated removal time for random removals. The red vertical line in both figures indicates the default choice of γ = 1/ √ n, where n stands for the number of total data points across clients.\n\n\nExisting works on sparse secure aggregation (Beguier et al., 2020; Ergun et al., 2021) either have a communication complexity that is linear in the model dimension, or they can only generate an approximation of the aggregated model based on certain sparsification procedures (Han et al., 2020).\n\n\nintroduced distributed learners, Bourtoule et al. (2021) proposed sharding-based methods, Ginart et al. (2019) used quantization to eliminate the effect of removed data in clustering problems, and Chen et al. (2021) applied sharding-based methods to Graph Neural Networks. For approximate unlearning, the \"differences\" in behavior between the unlearned model and the completely retrained model should be appropriately bounded, similarly to what is done in the context of differential privacy. Following this latter direction, Guo et al. (2020) introduced the inverse Newton update for linear models, Sekhari et al. (2021) studied the generalization performance of approximately unlearned models, Fu et al. (2022) proposed an MCMC unlearning algorithm for sampling-based Bayesian inference, Golatkar et al. (2020a;b) designed model update mechanisms for deep neural networks based on Fisher Information and Neural Tangent Kernel, while Chien et al. (2022; 2023); Pan et al. (2023) extended the analysis to Graph Neural Networks. A limited number of recent works also investigated data removal in the FL settings: Liu et al. (2021) proposed to use fewer iterations during retraining for federated unlearning,Wu et al. (2022) introduced Knowledge Distillation into the unlearning procedure to eliminate the effect of data points requested for removal, andWang et al. (2022) considered removing all data from one particular class via inspection of the internal influence of each channel in Convolutional Neural Networks. These federated unlearning methods are (mostly) empirical and do not come with theoretical guarantees for model performance after removal and/or for the unlearning efficiency. In contrast, our proposed FC framework not only enables efficient data removal in practice, but also provides theoretical guarantees for the unlearned model performance and for the expected time complexity of the unlearning procedure.B K-MEANS++ INITIALIZATIONThe K-means problem is NP-hard even for K = 2, and when the points lie in a two-dimensional Euclidean space(Mahajan et al., 2012). Heuristic algorithms for solving the problem, includingLloyd's (Lloyd, 1982)  and Hartigan's method(Hartigan & Wong, 1979), are not guaranteed to obtain the global optimal solution unless further assumptions are made on the point and cluster structures(Lee et al., 2017)\n\n\nbe represented by log p bits, the information {S (l) i } i∈[2KL] sent by each client l can be represented by 2KL log p ≤ max{2KL log n, 2KLd log B} + 1 bits. Note that there are at most k∈[vectors of length B d . Hence, the cost for communicating q (l) from the client to server l is at least log\n\nI\nSIMPLIFIED FEDERATED K-MEANS CLUSTERING When privacy criterion like the one stated in Section 3 is not enforced, and as done in the framework of Dennis et al. (2021), one can skip line 3-6 in Alg. 1 and send the centroid set C (l) obtained by client l along with the cluster sizes (|C (l) 1 |, . . . , |C (l) K |) directly to the server. Then, one can run the weighted K-means++ algorithm at the server on the aggregated centroid set to obtain C s . The pseudocode for this simplified case is shown in Alg. 5. It follows a similar idea as the divide-and-conquer schemes of Guha et al. (2003); Ailon et al. (2009), developed for distributed clustering.\n\n\nS i , l ∈ [L] and i ∈ [2KL] is still manageable, and achieved by computing and storing S (l) i and S i using O(KL) floating point numbers, instead of computing and storing S (l) i in a single floating point number. Note that j i can be computed using O(i) floating point numbers with complexity almost linear in i (i.e., O(i log c i) for some constant c).\n\n\nThe total computations complexity is hence at most O(M K log B d ) evaluations of polynomials with degree at most O(M K) and at most O(M K) polynomial divisions, which requires at most O((M K) 2 log B d ) multiplications and additions for integers of size at most log p . This results in an overall complexity of O((M K) 3 d 2 log c (M K) log B), for some constant c < 2.\n\n\nCelltype [n = 12009, d = 10, K = 4] (Han et al., 2018; Gardner et al., 2014b) comprises single cell RNA sequences belonging to a mixture of four cell types: fibroblasts, microglial cells, endothelial cells and mesenchymal stem cells. The data, retrieved from the Mouse Cell Atlas, consists of 12009 data points and each sample has 10 feature dimensions, reduced from an original dimension of 23, 433 using Principal Component Analysis (PCA). The sizes of the four clusters 3 are 6506, 2328, 2201, 974. Postures [n = 74975, d = 15, K = 5] (Gardner et al., 2014b;a) comprises images obtained via a motion capture system and a glove for 12 different users performing five hand postures -fist, pointing with one finger, pointing with two fingers, stop (hand flat), and grab (fingers curled). For establishing a rotation and translation invariant local coordinate system, a rigid unlabeled pattern on the back of the glove was utilized. There are a total of 74975 samples in the dataset and the data dimension is 15. The sizes of the given clusters are 19772, 17340, 15141, 12225, 10497.\n\nFEMNIST\n[n = 36725, d = 784, K = 62] (Caldas et al., 2018) is a popular FL benchmark dataset comprising images of digits (0-9) and letters from the English alphabet (both upper and lower cases) from over 3500 users. It dataset is essentially built from the Extended MNIST repository (Cohen et al., 2017) by partitioning it on the basis of the writer of the digit/character. We extract data corresponding to 100 different clients, each of which contributed at least 350 data points. Each image has dimension 784. The size of the largest cluster is 1234, and that of the smallest cluster is 282. TCGA [n = 1904, d = 57, K = 4] methylation consists of methylation microarray data for 1904 samples from The Cancer Genome Atlas (TCGA) (Hutter & Zenklusen, 2018) corresponding to four different cancer types: Low Grade Glioma (LGG), Lung Adenocarcinoma (LUAD), Lung Squamous Cell Carcinoma (LUSC) and Stomach Adenocarcinoma (STAD). The observed features correspond to a subset of β values, representing the coverage of the methylated sites, at 57 locations on the promoters of 11 different genes(ATM, BRCA1, CASP8, CDH1, IGF2, KRAS, MGMT, MLH1, PTEN, SFRP5 and TP53). This subset of genes was chosen for its relevance in carcinogenesis. The sizes of the four clusters are 735, 503, 390, 276.\n\nFigure 4 :\n4The shaded areas represent the standard deviation of results from different trails for all subplots. (a)-(d) The change of loss ratio φ f (X ; C s )/φ * c (X) after each round of unlearning procedure. (e)-(h) The accumulated removal time for adversarial removals.\n\nFigure 5 :\n5The shaded areas represent the standard deviation of results from different trails for all subplots. (a), (c) Remove 10 points within one batch removal request. (b), (d) Remove 30 points within one batch removal request.\n\nTable 1 :\n1Clustering performance of different FC algorithms compared to centralized K-means++ clustering.TMI \nCelltype \nGaussian \nTCGA \nPostures \nFEMNIST \nCovtype \n\n\n\n\nThis work was funded by NSF grants 1816913 and 1956384. The authors thank Eli Chien for the helpful discussion. Jichan Chung, Kangwook Lee, and Kannan Ramchandran. Federated unsupervised clustering with generative models. In AAAI 2022 International Workshop on Trustable, Verifiable and Auditable Federated Learning, 2022. Keith Frikken. Privacy-preserving set union. In International Conference on Applied Cryptography and Network Security, pp. 237-252. Springer, 2007. Shaopeng Fu, Fengxiang He, and Dacheng Tao. Knowledge removal in sampling-based bayesian inference. In International Conference on Learning Representations, 2022. URL https:// openreview.net/forum?id=dTqOcTUOQO. Andrew Gardner, Christian A Duncan, Jinko Kanno, and Rastko Selmic. 3D hand posture recognition from small unlabeled point sets. In 2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC), pp. 164-169. IEEE, 2014a. Andrew Gardner, Jinko Kanno, Christian A Duncan, and Rastko Selmic. Measuring distance between unordered sets of different sizes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 137-143, 2014b.Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. EMNIST: Extending MNIST \nto handwritten letters. In 2017 international joint conference on neural networks (IJCNN), pp. \n2921-2926. IEEE, 2017. \n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale \nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, \npp. 248-255. Ieee, 2009. \n\nDon Kurian Dennis, Tian Li, and Virginia Smith. Heterogeneity for the win: One-shot federated \nclustering. In International Conference on Machine Learning, pp. 2611-2620. PMLR, 2021. \n\nAlfredo Eisinberg and Giuseppe Fedele. On the inversion of the vandermonde matrix. Applied \nmathematics and computation, 174(2):1384-1397, 2006. \n\nIrem Ergun, Hasin Us Sami, and Basak Guler. Sparsified secure aggregation for privacy-preserving \nfederated learning. arXiv preprint arXiv:2112.12872, 2021. \n\nMatt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence \ninformation and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC conference on \ncomputer and communications security, pp. 1322-1333, 2015. \n\nGuojun Gan and Michael Kwok-Po Ng. K-means clustering with outlier removal. Pattern Recognition \nLetters, 90:8-14, 2017. \n\nVenkata Gandikota, Daniel Kane, Raj Kumar Maity, and Arya Mazumdar. vqsgd: Vector quantized \nstochastic gradient descent. In International Conference on Artificial Intelligence and Statistics, \npp. 2197-2205. PMLR, 2021. \n\nJonas Geiping, Hartmut Bauermeister, Hannah Dröge, and Michael Moeller. Inverting gradients-how \neasy is it to break privacy in federated learning? Advances in Neural Information Processing \nSystems, 33:16937-16947, 2020. \n\nAvishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework for \nclustered federated learning. Advances in Neural Information Processing Systems, 33:19586-\n19597, 2020. \n\nAntonio Ginart, Melody Guan, Gregory Valiant, and James Y Zou. Making AI forget you: Data \ndeletion in machine learning. Advances in neural information processing systems, 32, 2019. \n\nAditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net: \nSelective forgetting in deep networks. In Proceedings of the IEEE/CVF Conference on Computer \nVision and Pattern Recognition, pp. 9304-9312, 2020a. \n\nAditya Golatkar, Alessandro Achille, and Stefano Soatto. Forgetting outside the box: Scrubbing deep \nnetworks of information accessible from input-output observations. In European Conference on \nComputer Vision, pp. 383-398. Springer, 2020b. \n\n\nTable 2 :\n2Clustering performance of different FC algorithms compared to centralized K-means++ clustering.TMI \nCelltype \nGaussian \nTCGA \nPostures \nFEMNIST \nCovtype \n\n\nφ * c (X) is approximated by running K-means++ multiple times and selecting the smallest objective value.\nWe can also add random shifts during quantization as proposed inGinart et al. (2019)  to make the data appear more uniformly distributed within the quantization bins.\nThe clusters are obtained by running centralized K-means++ clustering multiple times and selecting the one inducing the lowest objective value.\n\nCommunication complexity in locally private distribution estimation and heavy hitters. Jayadev Acharya, Ziteng Sun, International Conference on Machine Learning. PMLRJayadev Acharya and Ziteng Sun. Communication complexity in locally private distribution estimation and heavy hitters. In International Conference on Machine Learning, pp. 51-60. PMLR, 2019.\n\nUnlearning graph classifiers with limited data resources. Eli Chao Pan, Olgica Chien, Milenkovic, The Web Conference. Chao Pan, Eli Chien, and Olgica Milenkovic. Unlearning graph classifiers with limited data resources. In The Web Conference, 2023.\n\nThe NIH Human Microbiome Project. Jane Peterson, Susan Garges, Maria Giovanni, Pamela Mcinnes, Lu Wang, A Jeffery, Vivien Schloss, Jean E Bonazzi, Kris A Mcewen, Carolyn Wetterstrand, Deal, Genome research. 1912Jane Peterson, Susan Garges, Maria Giovanni, Pamela McInnes, Lu Wang, Jeffery A Schloss, Vivien Bonazzi, Jean E McEwen, Kris A Wetterstrand, Carolyn Deal, et al. The NIH Human Microbiome Project. Genome research, 19(12):2317-2323, 2009.\n\nPolynomial codes over certain finite fields. S Irving, Gustave Reed, Solomon, Journal of the society for industrial and applied mathematics. 82Irving S Reed and Gustave Solomon. Polynomial codes over certain finite fields. Journal of the society for industrial and applied mathematics, 8(2):300-304, 1960.\n\nClustered federated learning: Modelagnostic distributed multitask optimization under privacy constraints. Felix Sattler, Klaus-Robert Müller, Wojciech Samek, IEEE transactions on neural networks and learning systems. 32Felix Sattler, Klaus-Robert Müller, and Wojciech Samek. Clustered federated learning: Model- agnostic distributed multitask optimization under privacy constraints. IEEE transactions on neural networks and learning systems, 32(8):3710-3722, 2020.\n\nRemember what you want to forget: Algorithms for machine unlearning. Ayush Sekhari, Jayadev Acharya, Gautam Kamath, Ananda Theertha Suresh, Advances in Neural Information Processing Systems. 34Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh. Remember what you want to forget: Algorithms for machine unlearning. Advances in Neural Information Processing Systems, 34:18075-18086, 2021.\n\nConstant-round multi-party private set union using reversed laurent series. Jae Hong Seo, Jung Hee Cheon, Jonathan Katz, International Workshop on Public Key Cryptography. SpringerJae Hong Seo, Jung Hee Cheon, and Jonathan Katz. Constant-round multi-party private set union using reversed laurent series. In International Workshop on Public Key Cryptography, pp. 398-412. Springer, 2012.\n\nLightsecagg: a lightweight and versatile design for secure aggregation in federated learning. Jinhyun So, J Corey, Chien-Sheng Nolet, Songze Yang, Qian Li, Yu, E Ramy, Basak Ali, Salman Guler, Avestimehr, Proceedings of Machine Learning and Systems. Machine Learning and Systems4Jinhyun So, Corey J Nolet, Chien-Sheng Yang, Songze Li, Qian Yu, Ramy E Ali, Basak Guler, and Salman Avestimehr. Lightsecagg: a lightweight and versatile design for secure aggregation in federated learning. Proceedings of Machine Learning and Systems, 4:694-720, 2022.\n\nUk biobank: an open access resource for identifying the causes of a wide range of complex diseases of middle and old age. Cathie Sudlow, John Gallacher, Naomi Allen, Valerie Beral, Paul Burton, John Danesh, Paul Downey, Paul Elliott, Jane Green, Martin Landray, PLoS medicine. 1231001779Cathie Sudlow, John Gallacher, Naomi Allen, Valerie Beral, Paul Burton, John Danesh, Paul Downey, Paul Elliott, Jane Green, Martin Landray, et al. Uk biobank: an open access resource for identifying the causes of a wide range of complex diseases of middle and old age. PLoS medicine, 12(3): e1001779, 2015.\n\nYfcc100m: The new data in multimedia research. Bart Thomee, A David, Gerald Shamma, Benjamin Friedland, Karl Elizalde, Douglas Ni, Damian Poland, Li-Jia Borth, Li, Communications of the ACM. 592Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):64-73, 2016.\n\nk-means++: The advantages of careful seeding. Sergei Vassilvitskii, David Arthur, Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. the eighteenth annual ACM-SIAM symposium on Discrete algorithmsSergei Vassilvitskii and David Arthur. k-means++: The advantages of careful seeding. In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pp. 1027-1035, 2006.\n\nAlgorithms that remember: model inversion attacks and data protection law. Michael Veale, Reuben Binns, Lilian Edwards, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences. 37620180083Michael Veale, Reuben Binns, and Lilian Edwards. Algorithms that remember: model inversion attacks and data protection law. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 376(2133):20180083, 2018.\n\nJianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, Maruan H Brendan Mcmahan, Galen Al-Shedivat, Salman Andrew, Katharine Avestimehr, Daly, arXiv:2107.06917Deepesh Data, et al. A field guide to federated optimization. arXiv preprintJianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A field guide to federated optimization. arXiv preprint arXiv:2107.06917, 2021.\n\nFederated unlearning via class-discriminative pruning. Junxiao Wang, Song Guo, Xin Xie, Heng Qi, Proceedings of the ACM Web Conference 2022. the ACM Web Conference 2022Junxiao Wang, Song Guo, Xin Xie, and Heng Qi. Federated unlearning via class-discriminative pruning. In Proceedings of the ACM Web Conference 2022, pp. 622-632, 2022.\n\nFederated unlearning with knowledge distillation. Chen Wu, Sencun Zhu, Prasenjit Mitra, arXiv:2201.09441arXiv preprintChen Wu, Sencun Zhu, and Prasenjit Mitra. Federated unlearning with knowledge distillation. arXiv preprint arXiv:2201.09441, 2022.\n\nDeep leakage from gradients. Ligeng Zhu, Zhijian Liu, Song Han, Advances in neural information processing systems. 32Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. Advances in neural information processing systems, 32, 2019."
  }
]