[
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"Are there any research papers on methods to compress large-scale language models using task-agnostic knowledge distillation techniques?",
    "positive_ctxs":[
      {
        "title":"TinyBERT: Distilling BERT for Natural Language Understanding",
        "text":"Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resourcerestricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large \"teacher\" BERT can be effectively transferred to a small \"student\" Tiny-BERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pretraining and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT.TinyBERT 4 1 with 4 layers is empirically effective and achieves more than 96.8% the performance of its teacher BERT BASE on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT 4 is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only ∼28% parameters and ∼31% inference time of them. Moreover, TinyBERT 6 with 6 layers performs on-par with its teacher BERT BASE . * Authors contribute equally. † This work is done when Xiaoqi Jiao is an intern at Huawei Noah's Ark Lab. ‡ Corresponding authors. 1  The code and models are publicly available at https: \/\/github.com\/huawei-noah\/Pretrained-Language-Model\/tree\/ master\/TinyBERT",
        "id":202719327
      }
    ],
    "negative_ctxs":[
      {
        "title":"A Frame-based Sentence Representation for Machine Reading Comprehension",
        "text":"Sentence representation (SR) is the most crucial and challenging task in Machine Reading Comprehension (MRC). MRC systems typically only utilize the information contained in the sentence itself, while human beings can leverage their semantic knowledge. To bridge the gap, we proposed a novel Frame-based Sentence Representation (FSR) method, which employs frame semantic knowledge to facilitate sentence modelling. Specifically, different from existing methods that only model lexical units (LUs), Frame Representation Models, which utilize both LUs in frame and Frame-to-Frame (F-to-F) relations, are designed to model frames and sentences with attention schema. Our proposed FSR method is able to integrate multiple-frame semantic information to get much better sentence representations. Our extensive experimental results show that it performs better than state-of-the-art technologies on machine reading comprehension task.",
        "id":220047836
      },
      {
        "title":"Factuality Detection on the Cheap: Inferring Factuality for Increased Precision in Detecting Negated Events",
        "text":"This paper describes a system for discriminating between factual and non-factual contexts, trained on weakly labeled data by taking advantage of information implicit in annotations of negated events. In addition to evaluating factuality detection in isolation, we also evaluate its impact on a system for event detection. The two components for factuality detection and event detection form part of a system for identifying negative factual events, or counterfacts, with top-ranked results in the *SEM 2012 shared task.",
        "id":1649240
      },
      {
        "title":"Humanly Certifying Superhuman Classifiers",
        "text":"Estimating the performance of a machine learning system is a longstanding challenge in artificial intelligence research.Today, this challenge is especially relevant given the emergence of systems which appear to increasingly outperform human beings.In some cases, this \"superhuman\" performance is readily demonstrated; for example by defeating legendary human players in traditional two player games.On the other hand, it can be challenging to evaluate classification models that potentially surpass human performance.Indeed, human annotations are often treated as a ground truth, which implicitly assumes the superiority of the human over any models trained on human annotations.In reality, human annotators can make mistakes and be subjective.Evaluating the performance with respect to a genuine oracle may be more objective and reliable, even when querying the oracle is expensive or impossible.In this paper, we first raise the challenge of evaluating the performance of both humans and models with respect to an oracle which is unobserved.We develop a theory for estimating the accuracy compared to the oracle, using only imperfect human annotations for reference.Our analysis provides a simple recipe for detecting and certifying superhuman performance in this setting, which we believe will assist in understanding the stage of current research on classification.We validate the convergence of the bounds and the assumptions of our theory on carefully designed toy experiments with known oracles.Moreover, we demonstrate the utility of our theory by meta-analyzing large-scale natural language processing tasks, for which an oracle does not exist, and show that under our assumptions a number of models from recent years are with high probability superhuman.",
        "id":237532482
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Are there any resources available for translating Tunisian Arabic dialect that contain both manually translated comments by native speakers and additional data augmented through methods like segmentation at stop words level?",
    "positive_ctxs":[
      {
        "title":"Parallel resources for Tunisian Arabic dialect translation",
        "text":"The difficulty of processing dialects is clearly observed in the high cost of building representative corpus, in particular for machine translation. Indeed, all machine translation systems require a huge amount and good management of training data, which represents a challenge in a lowresource setting such as the Tunisian Arabic dialect. In this paper, we present a data augmentation technique to create a parallel corpus for Tunisian Arabic dialect written in social media and standard Arabic in order to build a Machine Translation (MT) model. The created corpus was used to build a sentence-based translation model. This model reached a BLEU score of 15.03% on a test set, while it was limited to 13.27% utilizing the corpus without augmentation.",
        "id":227231792
      }
    ],
    "negative_ctxs":[
      {
        "title":"Comparing Sanskrit Texts for Critical Editions *",
        "text":"Traditionally Sanskrit is written without blank, sentences can make thousands of characters without any separation. A critical edition takes into account all the different known versions of the same text in order to show the differences between any two distinct versions, in term of words missing, changed or omitted. This paper describes the Sanskrit characteristics that make text comparisons different from other languages, and will present different methods of comparison of Sanskrit texts which can be used for the elaboration of computer assisted critical edition of Sanskrit texts. It describes two sets of methods used to obtain the alignments needed. The first set is using the L.C.S., the second one the global alignment algorithm. One of the methods of the second set uses a classical technique in the field of artificial intelligence, the A* algorithm to obtain the suitable alignment. We conclude by comparing our different results in term of adequacy as well as complexity.",
        "id":1399747
      },
      {
        "title":"Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access",
        "text":"Most prior work on task-oriented dialogue systems are restricted to a limited coverage of domain APIs, while users oftentimes have domain related requests that are not covered by the APIs. In this paper, we propose to expand coverage of task-oriented dialogue systems by incorporating external unstructured knowledge sources. We define three sub-tasks: knowledge-seeking turn detection, knowledge selection, and knowledge-grounded response generation, which can be modeled individually or jointly. We introduce an augmented version of MultiWOZ 2.1, which includes new out-of-API-coverage turns and responses grounded on external knowledge sources. We present baselines for each sub-task using both conventional and neural approaches. Our experimental results demonstrate the need for further research in this direction to enable more informative conversational systems.",
        "id":219401758
      },
      {
        "title":"Rapid Adaptation of Neural Machine Translation to New Languages",
        "text":"This paper examines the problem of adapting neural machine translation systems to new, low-resourced languages (LRLs) as effectively and rapidly as possible. We propose methods based on starting with massively multilingual \"seed models\", which can be trained ahead-of-time, and then continuing training on data related to the LRL. We contrast a number of strategies, leading to a novel, simple, yet effective method of \"similar-language regularization\", where we jointly train on both a LRL of interest and a similar high-resourced language to prevent over-fitting to small LRL data. Experiments demonstrate that massively multilingual models, even without any explicit adaptation, are surprisingly effective, achieving BLEU scores of up to 15.5 with no data from the LRL, and that the proposed similarlanguage regularization method improves over other adaptation methods by 1.7 BLEU points average over 4 LRL settings. 1",
        "id":51976920
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"Are there any studies that explore post-hoc techniques for hallucination detection at both the token- and sentence-level in neural sequence generation tasks?",
    "positive_ctxs":[
      {
        "title":"Detecting Hallucinated Content in Conditional Neural Sequence Generation",
        "text":"Neural sequence models can generate highly fluent sentences, but recent studies have also shown that they are also prone to hallucinate additional content not supported by the input. These variety of fluent but wrong outputs are particularly problematic, as it will not be possible for users to tell they are being presented incorrect content. To detect these errors, we propose a task to predict whether each token in the output sequence is hallucinated (not contained in the input) and collect new manually annotated evaluation sets for this task. We also introduce a method for learning to detect hallucinations using pretrained language models fine tuned on synthetic data that includes automatically inserted hallucinations Experiments on machine translation (MT) and abstractive summarization demonstrate that our proposed approach consistently outperforms strong baselines on all benchmark datasets. We further demonstrate how to use the token-level hallucination labels to define a fine-grained loss over the target sequence in low-resource MT and achieve significant improvements over strong baseline methods.We also apply our method to word-level quality estimation for MT and show its effectiveness in both supervised and unsupervised settings 1 .",
        "id":226254579
      },
      {
        "title":"Evaluating the Factual Consistency of Abstractive Text Summarization",
        "text":"The most common metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and generated summaries. Training data is generated by applying a series of rule-based transformations to the sentences of source documents. The factual consistency model is then trained jointly for three tasks: 1) predict whether each summary sentence is factually consistent or not, 2) in either case, extract a span in the source document to support this consistency prediction, 3) for each summary sentence that is deemed inconsistent, extract the inconsistent span from it. Transferring this model to summaries generated by several neural models reveals that this highly scalable approach outperforms previous models, including those trained with strong supervision using datasets from related domains, such as natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency. We also release a manually annotated dataset for factual consistency verification, code for training data generation, and trained model weights at https:\/\/github.com\/salesforce\/factCC.",
        "id":204976362
      }
    ],
    "negative_ctxs":[
      {
        "title":"Mining the Web for Discourse Markers",
        "text":"This paper proposes a methodology for obtaining sentences containing discourse markers from the World Wide Web. The proposed methodology is particularly suitable for collecting large numbers of discourse marker tokens. It relies on the automatic identification of discourse markers, and we show that this can be done with an accuracy within 9% of that of human performance. We also show that the distribution of discourse markers on the web correlates highly with those in a conventional balanced corpus.",
        "id":16720635
      },
      {
        "title":"Behavior Analysis of NLI Models: Uncovering the Influence of Three Factors on Robustness",
        "text":"Natural Language Inference is a challenging task that has received substantial attention, and state-of-the-art models now achieve impressive test set performance in the form of accuracy scores. Here, we go beyond this single evaluation metric to examine robustness to semantically-valid alterations to the input data. We identify three factorsinsensitivity, polarity and unseen pairs -and compare their impact on three SNLI models under a variety of conditions. Our results demonstrate a number of strengths and weaknesses in the models' ability to generalise to new in-domain instances. In particular, while strong performance is possible on unseen hypernyms, unseen antonyms are more challenging for all the models. More generally, the models suffer from an insensitivity to certain small but semantically significant alterations, and are also often influenced by simple statistical correlations between words and training labels. Overall, we show that evaluations of NLI models can benefit from studying the influence of factors intrinsic to the models or found in the dataset used.",
        "id":21677829
      },
      {
        "title":"A Text-based Method for Detection and Filtering of Commercial Segments in Broadcast News",
        "text":"Story segmentation is an important problem in multimedia indexing and retrieval and includes detection of commercials as one of its component problems. Commercials appear regularly in television data and are usually treated as noise. Hence, filtering of commercials is an important task. This paper presents a system that detects and filters commercials from broadcast news data. While previous work in the area relies largely on features from audio, video and captions, the system described in this paper uses just closed caption text to perform this task. An evaluation of this system is also presented which shows comparable performance with other methods.",
        "id":2041910
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Are there any tools or studies that have focused on building a morphological analyzer specifically for handling multiple Arabic dialects?",
    "positive_ctxs":[
      {
        "title":"Learning from Relatives: Unified Dialectal Arabic Segmentation",
        "text":"Arabic dialects do not just share a common koiné, but there are shared pandialectal linguistic phenomena that allow computational models for dialects to learn from each other. In this paper we build a unified segmentation model where the training data for different dialects are combined and a single model is trained. The model yields higher accuracies than dialect-specific models, eliminating the need for dialect identification before segmentation. We also measure the degree of relatedness between four major Arabic dialects by testing how a segmentation model trained on one dialect performs on the other dialects. We found that linguistic relatedness is contingent with geographical proximity. In our experiments we use SVM-based ranking and bi-LSTM-CRF sequence labeling.",
        "id":10961392
      },
      {
        "title":"A Neural Architecture for Dialectal Arabic Segmentation",
        "text":"The automated processing of Arabic dialects is challenging due to the lack of spelling standards and the scarcity of annotated data and resources in general. Segmentation of words into their constituent tokens is an important processing step for natural language processing. In this paper, we show how a segmenter can be trained on only 350 annotated tweets using neural networks without any normalization or reliance on lexical features or linguistic resources. We deal with segmentation as a sequence labeling problem at the character level. We show experimentally that our model can rival state-of-the-art methods that heavily depend on additional resources.",
        "id":12160022
      }
    ],
    "negative_ctxs":[
      {
        "title":"The KiezDeutsch Korpus (KiDKo) Release 1.0",
        "text":"This paper presents the first release of the KiezDeutsch Korpus (KiDKo), a new language resource with multiparty spoken dialogues of Kiezdeutsch, a newly emerging language variety spoken by adolescents from multiethnic urban areas in Germany. The first release of the corpus includes the transcriptions of the data as well as a normalisation layer and part-of-speech annotations. In the paper, we describe the main features of the new resource and then focus on automatic POS tagging of informal spoken language. Our tagger achieves an accuracy of nearly 97% on KiDKo. While we did not succeed in further improving the tagger using ensemble tagging, we present our approach to using the tagger ensembles for identifying error patterns in the automatically tagged data.",
        "id":16455108
      },
      {
        "title":"Exploring Substitutability through Discourse Adverbials and Multiple Judgments",
        "text":"In his systematic analysis of discourse connectives,Knott (1996)introduced the notion of substitutability and the conditions under which one connective (e.g., when) can substitute for another (e.g., if ) to express the same meaning. Knott only uses examples which he constructed and judged himself. This paper describes a new multi-judgment study on naturally occurring passages, on which substitutability claims can be tested. While some of our findings support Knott's claims, other pairs of connectives that Knott predicts to be exclusive are in fact judged to substitute felicitously for one another. These findings show that discourse adverbials in the immediate context play a role in connective choice.",
        "id":4613427
      },
      {
        "title":"Machine Translation Enhanced Computer Assisted Translation",
        "text":"MateCat aims to integrate statistical Machine Translation (MT) and collaborative Translation Memories (TM) within the human translation workflow. The objective is to increase the productivity of professional translators and to enhance their work experience with MT.MateCat will go beyond the state-of-the-art by investigating new research issues related to the integration of MT into CAT, namely: self-tuning MT, user adaptive MT, and informative MT.",
        "id":20057270
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Are there papers that propose contextualized calibration for the probability of answers in language models?",
    "positive_ctxs":[
      {
        "title":"Surface Form Competition: Why the Highest Probability Answer Isn't Always Right",
        "text":"Large language models have shown promising results in zero-shot settings (Brown et al.,  2020; Radford et al., 2019). For example, they can perform multiple choice tasks simply by conditioning on a question and selecting the answer with the highest probability. * Code is available at https:\/\/github.com\/ peterwestuw\/surface-form-competition",
        "id":233296182
      }
    ],
    "negative_ctxs":[
      {
        "title":"DeepCx: A transition-based approach for shallow semantic parsing with complex constructional triggers",
        "text":"This paper introduces the SURFACE CON-STRUCTION LABELING (SCL) task, which expands the coverage of Shallow Semantic Parsing (SSP) to include frames triggered by complex constructions. We present DeepCx, a neural, transition-based system for SCL. As a test case for the approach, we apply DeepCx to the task of tagging causal language in English, which relies on a wider variety of constructions than are typically addressed in SSP. We report substantial improvements over previous tagging efforts on a causal language dataset. We also propose ways DeepCx could be extended to still more difficult constructions and to other semantic domains once appropriate datasets become available.",
        "id":53080192
      },
      {
        "title":"A Corpus of Human-written Summaries of Line Graphs",
        "text":"We describe a corpus of human-written English language summaries of line graphs. This corpus is intended to help develop a system to automatically generate summaries capturing the most salient information conveyed by line graphs in popular media, as well as to evaluate the output of such a system.",
        "id":15878323
      },
      {
        "title":"Normalising Audio Transcriptions for Unwritten Languages",
        "text":"The task of documenting the world's languages is a mainstream activity in linguistics which is yet to spill over into computational linguistics. We propose a new task of transcription normalisation as an algorithmic method for speeding up the process of transcribing audio sources, leading to text collections of usable quality. We report on the application of sentence and word alignment algorithms to this task, before describing a new algorithm. All of the algorithms are evaluated over synthetic datasets. Although the results are nuanced, the transcription normalisation task is suggested as an NLP contribution to the grand challenge of documenting the world's languages.",
        "id":6605351
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Are there studies that combine convolutional and recurrent neural network approaches to extract multiple types of features for relation extraction? If so, could you point me to one of them?",
    "positive_ctxs":[
      {
        "title":"Combining Recurrent and Convolutional Neural Networks for Relation Classification",
        "text":"This paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networks. For both models, we demonstrate the effect of different architectural choices. We present a new context representation for convolutional neural networks for relation classification (extended middle context). Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization. Finally, we show that combining convolutional and recurrent neural networks using a simple voting scheme is accurate enough to improve results. Our neural models achieve state-of-the-art results on the SemEval 2010 relation classification task.",
        "id":17297069
      }
    ],
    "negative_ctxs":[
      {
        "title":"Czech MWE Database",
        "text":"In this paper we deal with a recently developed large Czech MWE database containing at the moment 160 000 MWEs (treated as lexical units). It was compiled from various resources such as encyclopedias and dictionaries, public databases of proper names and toponyms, collocations obtained from Czech WordNet, lists of botanical and zoological terms and others. We describe the structure of the database and give basic types of MWEs according to domains they belong to. We compare the built MWEs database with the corpus data from Czech National Corpus (approx. 100 mil. tokens) and present results of this comparison in the paper. These MWEs have not been obtained from the corpus since their frequencies in it are rather low. To obtain a more complete list of MWEs we propose and use a technique exploiting the Word Sketch Engine, which allows us to work with statistical parameters such as frequency of MWEs and their components as well as with the salience for the whole MWEs. We also discuss exploitation of the database for working out a more adequate tagging and lemmatization. The final goal is to be able to recognize MWEs in corpus text and lemmatize them as complete lexical units, i. e. to make tagging and lemmatization more adequate.",
        "id":13443762
      },
      {
        "title":"What We Know About The Voynich Manuscript",
        "text":"The Voynich Manuscript is an undeciphered document from medieval Europe. We present current knowledge about the manuscript's text through a series of questions about its linguistic properties.",
        "id":9925138
      },
      {
        "title":"LEARNING RECURRENT BINARY\/TERNARY WEIGHTS",
        "text":"Recurrent neural networks (RNNs) have shown excellent performance in processing sequence data. However, they are both complex and memory intensive due to their recursive nature. These limitations make RNNs difficult to embed on mobile devices requiring real-time processes with limited hardware resources. To address the above issues, we introduce a method that can learn binary and ternary weights during the training phase to facilitate hardware implementations of RNNs. As a result, using this approach replaces all multiply-accumulate operations by simple accumulations, bringing significant benefits to custom hardware in terms of silicon area and power consumption. On the software side, we evaluate the performance (in terms of accuracy) of our method using long short-term memories (LSTMs) on various sequential models including sequence classification and language modeling. We demonstrate that our method achieves competitive results on the aforementioned tasks while using binary\/ternary weights during the runtime. On the hardware side, we present custom hardware for accelerating the recurrent computations of LSTMs with binary\/ternary weights. Ultimately, we show that LSTMs with binary\/ternary weights can achieve up to 12× memory saving and 10× inference speedup compared to the full-precision implementation on an ASIC platform.",
        "id":52894096
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"Can you direct me to research that explores methods for transforming multi-hop questions into single-hop sub-questions to leverage existing single-hop answer models?",
    "positive_ctxs":[
      {
        "title":"Multi-hop Reading Comprehension through Question Decomposition and Rescoring",
        "text":"Multi-hop Reading Comprehension (RC) requires reasoning and aggregation across several paragraphs. We propose a system for multi-hop RC that decomposes a compositional question into simpler sub-questions that can be answered by off-the-shelf single-hop RC models. Since annotations for such decomposition are expensive, we recast subquestion generation as a span prediction problem and show that our method, trained using only 400 labeled examples, generates sub-questions that are as effective as humanauthored sub-questions. We also introduce a new global rescoring approach that considers each decomposition (i.e. the sub-questions and their answers) to select the best final answer, greatly improving overall performance. Our experiments on HOTPOTQA show that this approach achieves the state-of-the-art results, while providing explainable evidence for its decision making in the form of sub-questions. . 2018. Neural models for reasoning over multiple mentions using coreference. In NAACL.Albert Gatt and Emiel Krahmer. 2018. Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. Artificial Intelligence Research.",
        "id":174801080
      }
    ],
    "negative_ctxs":[
      {
        "title":"MONODISTILL: LEARNING SPATIAL FEATURES FOR MONOCULAR 3D OBJECT DETECTION",
        "text":"3D object detection is a fundamental and challenging task for 3D scene understanding, and the monocular-based methods can serve as an economical alternative to the stereo-based or LiDAR-based methods. However, accurately detecting objects in the 3D space from a single image is extremely difficult due to the lack of spatial cues. To mitigate this issue, we propose a simple and effective scheme to introduce the spatial information from LiDAR signals to the monocular 3D detectors, without introducing any extra cost in the inference phase. In particular, we first project the LiDAR signals into the image plane and align them with the RGB images. After that, we use the resulting data to train a 3D detector (LiDAR Net) with the same architecture as the baseline model. Finally, this LiDAR Net can serve as the teacher to transfer the learned knowledge to the baseline model. Experimental results show that the proposed method can significantly boost the performance of the baseline model and ranks the 1 st place among all monocularbased methods on the KITTI benchmark. Besides, extensive ablation studies are conducted, which further prove the effectiveness of each part of our designs and illustrate what the baseline model has learned from the LiDAR Net. Our code will be released at https:\/\/github.com\/monster-ghost\/MonoDistill.Published as a conference paper at ICLR 2022RGB-based method Depth-based method OursRGB Data Depth Data 3D Box Pseudo-LiDAR T (d) 3D Box RGB Data Depth Data (a) RGB Data 3D Box D D DE DE D RGB Data 3D Box Depth Distribution Image Feature B DE D RGB Data LiDAR Depth 3D Box 3D Box Feature Guidance Response Guidance (e) D D Inference Training T Data Transformation Outer Product DE D Depth Estimator Detector B Backbone",
        "id":246285537
      },
      {
        "title":"",
        "text":"",
        "id":229365786
      },
      {
        "title":"Using Machine Learning for System-Internal Evaluation of Transferred Linguistic Representations",
        "text":"We present an automated, system-internal evaluation technique for linguistic representations in a large-scale, multilingual MT system. We use machine-learned classifiers to recognize the differences between linguistic representations generated from transfer in an MT context from representations that are produced by \"native\" analysis of the target language. In the MT scenario, convergence of the two is the desired result. Holding the feature set and the learning algorithm constant, the accuracy of the classifiers provides a measure of the overall difference between the two sets of linguistic representations: classifiers with higher accuracy correspond to more pronounced differences between representations. More importantly, the classifiers yield the basis for error-analysis by providing a ranking of the importance of linguistic features. The more salient a linguistic criterion is in discriminating transferred representations from \"native\" representations, the more work will be needed in order to get closer to the goal of producing native-like MT. We present results from using this approach on the Microsoft MT system and discuss its advantages and possible extensions.",
        "id":11540517
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"Can you direct me to studies that explore techniques like question answering and passage retrieval for mitigating the effects of clickbait headlines?",
    "positive_ctxs":[
      {
        "title":"Clickbait Spoiling via Question Answering and Passage Retrieval",
        "text":"We introduce and study the task of clickbait spoiling: generating a short text that satisfies the curiosity induced by a clickbait post. Clickbait links to a web page and advertises its contents by arousing curiosity instead of providing an informative summary. Our contributions are approaches to classify the type of spoiler needed (i.e., a phrase or a passage), and to generate appropriate spoilers. A large-scale evaluation and error analysis on a new corpus of 5,000 manually spoiled clickbait poststhe Webis Clickbait Spoiling Corpus 2022shows that our spoiler type classifier achieves an accuracy of 80%, while the question answering model DeBERTa-large outperforms all others in generating spoilers for both types.",
        "id":247593812
      }
    ],
    "negative_ctxs":[
      {
        "title":"Copenhagen at CoNLL-SIGMORPHON 2018: Multilingual Inflection in Context with Explicit Morphosyntactic Decoding",
        "text":"This paper documents the Team Copenhagen system which placed first in the CoNLL-SIGMORPHON 2018 shared task on universal morphological reinflection, Task 2 with an overall accuracy of 49.87. Task 2 focuses on morphological inflection in context: generating an inflected word form, given the lemma of the word and the context it occurs in. Previous SIGMORPHON shared tasks have focused on context-agnostic inflection-the \"inflection in context\" task was introduced this year. We approach this with an encoder-decoder architecture over character sequences with three core innovations, all contributing to an improvement in performance: (1) a wide context window; (2) a multi-task learning approach with the auxiliary task of MSD prediction; (3) training models in a multilingual fashion.",
        "id":52164624
      },
      {
        "title":"MATAPHORICAL EXTENSION AND LEXICAL MEANING",
        "text":"Metonymy and metaphor reflect an important part of the way people ordinarily conceptualize of themselves, events, and everyday world [1]. We will argue for this position via the lexicalization process of two linguistic items in Chinese: shang and zai, and demonstrate that grammatical meaning develops from lexical meaning by a process of \"generalization or weakening of semantic content,\" which is in fact metaphorical in nature. The purpose of this paper is to ascertain into the nature of metaphorical extension (via metaphor and metonymy) and the creation of lexical meaning as they are seen in the two lexical items mentioned. Though data gathered from corpus, dictionaries and native speaker intuition, we wish to examine the relationship between conversion, metaphor and metonymy, and understand better 1) the driving force for polysemy in Chinese lexicon; 2) the different driving forces, concerning metaphor and metonymy, for prototypical categories and grammaticalization.",
        "id":16593363
      },
      {
        "title":"A Novel Approach towards Cross Lingual Sentiment Analysis using Transliteration and Character Embedding",
        "text":"Sentiment analysis with deep learning in resource-constrained languages is a challenging task. In this paper, we introduce a novel approach for sentiment analysis in resource-constrained scenarios using character embedding and cross-lingual sentiment analysis with transliteration. We use this method to introduce the novel task of inducing sentiment polarity of words and sentences and aspect term sentiment analysis in the no-resource scenario. We formulate this task by taking a metalingual approach whereby we transliterate data from closely related languages and transform it into a meta language. We also demonstrated the efficacy of using character-level embedding for sentence representation. We experimented with 4 Indian languages -Bengali, Hindi, Tamil, and Telugu, and obtained encouraging results. We also presented new state-of-the-art results on the Hindi sentiment analysis dataset leveraging our metalingual character embeddings.",
        "id":257767699
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"Can you list some publications that discuss the evaluation metrics used in semantic role labeling tasks, specifically focusing on precision and evaluation scripts used in shared tasks like SemEval?",
    "positive_ctxs":[
      {
        "title":"SemEval-2007 Task 17: English Lexical Sample, SRL and All Words",
        "text":"This paper describes our experience in preparing the data and evaluating the results for three subtasks of SemEval-2007 Task-17 -Lexical Sample, Semantic Role Labeling (SRL) and All-Words respectively. We tabulate and analyze the results of participating systems.",
        "id":17553490
      },
      {
        "title":"SENSEVAL-3 TASK Automatic Labeling of Semantic Roles",
        "text":"The SENSEVAL-3 task to perform automatic labeling of semantic roles was designed to encourage research into and use of the FrameNet dataset. The task was based on the considerable expansion of the FrameNet data since the baseline study of automatic labeling of semantic roles by Gildea and Jurafsky. The FrameNet data provide an extensive body of \"gold standard\" data that can be used in lexical semantics research, as the basis for its further exploitation in NLP applications. Eight teams participated in the task, with a total of 20 runs. Discussions among participants during development of the task and the scoring of their runs contributed to a successful task. Participants used a wide variety of techniques, investigating many aspects of the FrameNet data. They achieved results showing considerable improvements from Gildea and Jurafsky's baseline study. Importantly, their efforts have contributed considerably to making the complex FrameNet dataset more accessible. They have amply demonstrated that FrameNet is a substantial lexical resource that will permit extensive further research and exploitation in NLP applications in the future.",
        "id":561429
      },
      {
        "title":"Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling",
        "text":"In this paper we describe the CoNLL-2005 shared task on Semantic Role Labeling. We introduce the specification and goals of the task, describe the data sets and evaluation methods, and present a general overview of the 19 systems that have contributed to the task, providing a comparative description and results.",
        "id":16509032
      }
    ],
    "negative_ctxs":[
      {
        "title":"GEOMETRY OF POLYSEMY",
        "text":"Vector representations of words have heralded a transformational approach to classical problems in NLP; the most popular example is word2vec. However, a single vector does not suffice to model the polysemous nature of many (frequent) words, i.e., words with multiple meanings. In this paper, we propose a three-fold approach for unsupervised polysemy modeling: (a) context representations, (b) sense induction and disambiguation and (c) lexeme (as a word and sense pair) representations. A key feature of our work is the finding that a sentence containing a target word is well represented by a low rank subspace, instead of a point in a vector space. We then show that the subspaces associated with a particular sense of the target word tend to intersect over a line (one-dimensional subspace), which we use to disambiguate senses using a clustering algorithm that harnesses the Grassmannian geometry of the representations. The disambiguation algorithm, which we call K-Grassmeans, leads to a procedure to label the different senses of the target word in the corpus -yielding lexeme vector representations, all in an unsupervised manner starting from a large (Wikipedia) corpus in English. Apart from several prototypical target (word,sense) examples and a host of empirical studies to intuit and justify the various geometric representations, we validate our algorithms on standard sense induction and disambiguation datasets and present new state-of-the-art results.",
        "id":18114929
      },
      {
        "title":"Standoff Coordination for Multi-Tool Annotation in a Dialogue Corpus",
        "text":"The LUNA corpus is a multi-lingual, multidomain spoken dialogue corpus currently under development that will be used to develop a robust natural spoken language understanding toolkit for multilingual dialogue services. The LUNA corpus will be annotated at multiple levels to include annotations of syntactic, semantic, and discourse information; specialized annotation tools will be used for the annotation at each of these levels. In order to synchronize these multiple layers of annotation, the PAULA standoff exchange format will be used. In this paper, we present the corpus and its PAULA-based architecture. 1",
        "id":6618630
      },
      {
        "title":"PANDORA Talks: Personality and Demographics on Reddit",
        "text":"Personality and demographics are important variables in social sciences and computational sociolinguistics. However, datasets with both personality and demographic labels are scarce. To address this, we present PANDORA, the first dataset of Reddit comments of 10k users partially labeled with three personality models and demographics (age, gender, and location), including 1.6k users labeled with the wellestablished Big 5 personality model. We showcase the usefulness of this dataset on three experiments, where we leverage the more readily available data from other personality models to predict the Big 5 traits, analyze gender classification biases arising from psychodemographic variables, and carry out a confirmatory and exploratory analysis based on psychological theories. Finally, we present benchmark prediction models for all personality and demographic variables.",
        "id":215547988
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"Can you point me to a paper that discussed transformer-based sentence embeddings?",
    "positive_ctxs":[
      {
        "title":"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
        "text":"BERT(Devlin et al., 2018)and RoBERTa (Liu  et al., 2019)  has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT \/ RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods. 1",
        "id":201646309
      }
    ],
    "negative_ctxs":[
      {
        "title":"PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction",
        "text":"Joint extraction of entities and relations from unstructured texts is a crucial task in information extraction. Recent methods achieve considerable performance but still suffer from some inherent limitations, such as redundancy of relation prediction, poor generalization of span-based extraction and inefficiency. In this paper, we decompose this task into three subtasks, Relation Judgement, Entity Extraction and Subject-object Alignment from a novel perspective and then propose a joint relational triple extraction framework based on Potential Relation and Global Correspondence (PRGC). Specifically, we design a component to predict potential relations, which constrains the following entity extraction to the predicted relation subset rather than all relations; then a relation-specific sequence tagging component is applied to handle the overlapping problem between subjects and objects; finally, a global correspondence component is designed to align the subject and object into a triple with low-complexity. Extensive experiments show that PRGC achieves state-of-the-art performance on public benchmarks with higher efficiency and delivers consistent performance gain on complex scenarios of overlapping triples.",
        "id":235485451
      },
      {
        "title":"Meeting Decision Tracker: Making Meeting Minutes with De-Contextualized Utterances",
        "text":"Meetings are a universal process to make decisions in business and project collaboration. The capability to automatically itemize the decisions in daily meetings allows for extensive tracking of past discussions. To that end, we developed Meeting Decision Tracker, a prototype system to construct decision items comprising decision utterance detector (DUD) and decision utterance rewriter (DUR). We show that DUR makes a sizable contribution to improving the user experience by dealing with utterance collapse in natural conversation. An introduction video of our system is also available at https:\/\/youtu.be\/TG1pJJo0Iqo.",
        "id":253018848
      },
      {
        "title":"UQeResearch: Semantic Textual Similarity Quantification",
        "text":"This paper presents an approach for estimating the Semantic Textual Similarity of full English sentences as specified in Shared Task 2 of SemEval-2015. The semantic similarity of sentence pairs is quantified from three perspectives -structural, syntactical, and semantic. The numerical representations of the derived similarity measures are then applied to train a regression ensemble. Although none of these three sets of measures is able to represent the semantic similarity of two sentences individually, our experimental results show that the combination of these features can precisely assess the semantic similarity of the sentences. In the English subtask our system's best result ranked 35 among 73 system runs with 0.7189 average Pearson correlation over five test sets. This was 0.08 correlation points less than the best submitted run.",
        "id":8035413
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Can you point me to a work that uses diagnostic tools to detect depression from online posts, and investigates strategies that address common temporal and topical artifacts that plague these models?",
    "positive_ctxs":[
      {
        "title":"Do Models of Mental Health Based on Social Media Data Generalize?",
        "text":"Proxy-based methods for annotating mental health status in social media have grown popular in computational research due to their ability to gather large training samples. However, an emerging body of literature has raised new concerns regarding the validity of these types of methods for use in clinical applications. To further understand the robustness of distantly supervised mental health models, we explore the generalization ability of machine learning classifiers trained to detect depression in individuals across multiple social media platforms. Our experiments not only reveal that substantial loss occurs when transferring between platforms, but also that there exist several unreliable confounding factors that may enable researchers to overestimate classification performance. Based on these results, we enumerate recommendations for future mental health dataset construction.",
        "id":226283450
      }
    ],
    "negative_ctxs":[
      {
        "title":"A Formal Proof of Strong Equivalence for a Grammar Conversion from LTAG to HPSG-style",
        "text":"",
        "id":119887
      },
      {
        "title":"Learning Non-Monotonic Automatic Post-Editing of Translations from Human Orderings",
        "text":"Recent research in neural machine translation has explored flexible generation orders, as an alternative to left-to-right generation. However, training non-monotonic models brings a new complication: how to search for a good ordering when there is a combinatorial explosion of orderings arriving at the same final result? Also, how do these automatic orderings compare with the actual behaviour of human translators? Current models rely on manually built biases or are left to explore all possibilities on their own. In this paper, we analyze the orderings produced by human post-editors and use them to train an automatic postediting system. We compare the resulting system with those trained with left-to-right and random post-editing orderings. We observe that humans tend to follow a nearly left-to-right order, but with interesting deviations, such as preferring to start by correcting punctuation or verbs.",
        "id":216641902
      },
      {
        "title":"Analysis of Zero-Shot Crosslingual Learning between English and Korean for Named Entity Recognition",
        "text":"This paper presents a English-Korean parallel dataset that collects 381K news articles where 1,400 of them, comprising 10K sentences, are manually labeled for crosslingual named entity recognition (NER). The annotation guidelines for the two languages are developed in parallel, that yield the inter-annotator agreement scores of 91 and 88% for English and Korean respectively, indicating sublime quality annotation in our dataset. Three types of crosslingual learning approaches, direct model transfer, embedding projection, and annotation projection, are used to develop zero-shot Korean NER models. Our best model gives the F1-score of 51% that is very encouraging, considering the extremely distinct natures of these two languages. This is pioneering work that explores zero-shot crosslingual learning between English and Korean and provides rich parallel annotation for a core NLP task such as named entity recognition.",
        "id":241583519
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"Can you point me to studies discussing methods for evaluating text generation models on various dimensions? I'm particularly interested in models like T5 and FLAN-T5, and how to assess their performance on summary-level and turn-level tasks.",
    "positive_ctxs":[
      {
        "title":"Towards a Unified Multi-Dimensional Evaluator for Text Generation",
        "text":"Multi-dimensional evaluation is the dominant paradigm for human evaluation in Natural Language Generation (NLG), i.e., evaluating the generated text from multiple explainable dimensions, such as coherence and fluency. However, automatic evaluation in NLG is still dominated by similarity-based metrics, and we lack a reliable framework for a more comprehensive evaluation of advanced models. In this paper, we propose a unified multi-dimensional evaluator UNIEVAL for NLG. We re-frame NLG evaluation as a Boolean Question Answering (QA) task, and by guiding the model with different questions, we can use one evaluator to evaluate from multiple dimensions. Furthermore, thanks to the unified Boolean QA format, we are able to introduce an intermediate learning phase that enables UNIEVAL to incorporate external knowledge from multiple related tasks and gain further improvement. Experiments on three typical NLG tasks show that UNIEVAL correlates substantially better with human judgments than existing metrics. Specifically, compared to the top-performing unified evaluators, UNIEVAL achieves a 23% higher correlation on text summarization, and over 43% on dialogue response generation. Also, UNIEVAL demonstrates a strong zero-shot learning ability for unseen evaluation dimensions and tasks. Source code, data and all pre-trained evaluators are available on our GitHub repository 1 .2023Generated Summary: Harry Kane is nominated for both the PFA player and young player of the season. The Spurs striker has been released from the awards ceremony on Sunday. The Tottenham striker features in a new animation. Reference Summary: Harry Kane has been in superb form for Tottenham this season. The 21-year-old has scored 30 goals in all competitions for Spurs. Kane also made his England debut and scored within two minutes. Document: Harry Kane's celebrations this season have always shown him to be an animated young man . . .Similarity-based EvaluatorsROUGE-1: 0.44 ROUGE-2: 0.25 ROUGE-L: 0.42 BERTScore: 0.24 Single-dimensional Evaluators (predicted by two different evaluators (Deng et al., 2021)) Consistency: 0.87 Relevance: 0.74Unified Evaluator (predicted by BARTScore, and the scoring range is negative infinity to 0)",
        "id":252873117
      }
    ],
    "negative_ctxs":[
      {
        "title":"EXCOM : Plate-forme d'annotation sémantique de textes multilingues",
        "text":"Nous proposons une plateforme d\"annotation sémantique, appelée « EXCOM ».Basée sur la méthode de l\" « Exploration Contextuelle », elle permet, à travers une diversité de langues, de procéder à des annotations automatiques de segments textuels par l'analyse des formes de surface dans leur contexte. Les textes sont traités selon des « points de vue » discursifs dont les valeurs sont organisées dans une « carte sémantique ». L\"annotation se base sur un ensemble de règles linguistiques, écrites par un analyste, qui permettent d\"identifier les représentations textuelles sous-jacentes aux différentes catégories de la carte. Le système offre, à travers deux types d\"interfaces (développeur ou utilisateur), une chaîne de traitements automatiques de textes qui comprend la segmentation, l\"annotation et d\"autres fonctionnalités de post-traitement. Les documents annotés peuvent être utilisés, par exemple, pour des systèmes de recherche d\"information, de veille, de classification ou de résumé automatique.AbstractWe propose a platform for semantic annotation, called \"EXCOM\". Based on the \"Contextual Exploration\" method, it enables, across a great range of languages, to perform automatic annotations of textual segments by analyzing surface forms in their context. Texts are approached through discursive \"points of view\", of which values are organized into a \"semantic map\". The annotation is based on a set of linguistic rules, manually constructed by an analyst, and that enables to automatically identify the textual representations underlying the different semantic categories of the map. The system provides through two sorts of userfriendly interfaces (analyst or end-user) a complete pipeline of automatic text processing which consists of segmentation, annotation and other post-processing functionalities. Annotated documents can be used, for instance, for information retrieval systems, classification or automatic summarization.",
        "id":60424971
      },
      {
        "title":"An Unsupervised Dynamic Bayesian Network Approach to Measuring Speech Style Accommodation",
        "text":"Speech style accommodation refers to shifts in style that are used to achieve strategic goals within interactions. Models of stylistic shift that focus on specific features are limited in terms of the contexts to which they can be applied if the goal of the analysis is to model socially motivated speech style accommodation. In this paper, we present an unsupervised Dynamic Bayesian Model that allows us to model stylistic style accommodation in a way that is agnostic to which specific speech style features will shift in a way that resembles socially motivated stylistic variation. This greatly expands the applicability of the model across contexts. Our hypothesis is that stylistic shifts that occur as a result of social processes are likely to display some consistency over time, and if we leverage this insight in our model,we will achieve a model that better captures inherent structure within speech.",
        "id":15242587
      },
      {
        "title":"Title = {Automatic and Human Evaluation of Local Topic Automatic and Human Evaluation of Local Topic Quality",
        "text":"Topic models are typically evaluated with respect to the global topic distributions that they generate, using metrics such as coherence, but without regard to local (token-level) topic assignments. Token-level assignments are important for downstream tasks such as classification. Recent models, which claim to improve token-level topic assignments, are only validated on global metrics. We elicit human judgments of token-level topic assignments: over a variety of topic model types and parameters, global metrics agree poorly with human assignments. Since human evaluation is expensive we propose automated metrics to evaluate topic models at a local level. Finally, we correlate our proposed metrics with human judgments: an evaluation based on the percent of topic switches correlates most strongly with human judgment of local topic quality. This new metric, which we call consistency, should be adopted alongside global metrics such as topic coherence.",
        "id":170079259
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Can you point me to studies that explore the impact of different data augmentation strategies, such as feature\/token\/span cutoff or dropout, in the context of contrastive learning for sentence representations?",
    "positive_ctxs":[
      {
        "title":"ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer",
        "text":"Learning high-quality sentence representations benefits a wide range of natural language processing tasks. Though BERT-based pretrained language models achieve high performance on many downstream tasks, the native derived sentence representations are proved to be collapsed and thus produce a poor performance on the semantic textual similarity (STS) tasks. In this paper, we present ConSERT, a Contrastive Framework for Self-Supervised SEntence Representation Transfer, that adopts contrastive learning to fine-tune BERT in an unsupervised and effective way. By making use of unlabeled texts, ConSERT solves the collapse issue of BERT-derived sentence representations and make them more applicable for downstream tasks. Experiments on STS datasets demonstrate that ConSERT achieves an 8% relative improvement over the previous state-of-the-art, even comparable to the supervised SBERT-NLI. And when further incorporating NLI supervision, we achieve new stateof-the-art performance on STS tasks. Moreover, ConSERT obtains comparable results with only 1000 samples available, showing its robustness in data scarcity scenarios.",
        "id":235187266
      },
      {
        "title":"SimCSE: Simple Contrastive Learning of Sentence Embeddings",
        "text":"This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using \"entailment\" pairs as positives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2% and 2.2% improvement compared to previous best results. We also show-both theoretically and empirically-that contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available. 1",
        "id":233296292
      }
    ],
    "negative_ctxs":[
      {
        "title":"The Effects of Formal Schema on Reading Comprehension-An Experiment with Chinese EFL Readers",
        "text":"This study attempts to explore the effects of formal schemata or rhetorical patterns on reading comprehension through detailed analysis of a case study of 45 non-English majors from X University. The subjects were selected from three classes of comparable English level and were divided into three groups. Each group was asked to recall the text and finish a cloze test after reading one of three versions of a passage with identical content but different formal schemata: description schema, comparison and contrast schema, and problem-solution schema. Both quantitative and qualitative analyses of the recall protocol indicate that subjects displayed better recall of the text with highly structured schema than the one with loosely controlled schema, which suggests that formal schemata has a significant effect on written communication and the teaching of formal schemata to students is necessary to enhance their writing ability.",
        "id":17205703
      },
      {
        "title":"Exploring Phoneme-Level Speech Representations for End-to-End Speech Translation",
        "text":"Previous work on end-to-end translation from speech has primarily used frame-level features as speech representations, which creates longer, sparser sequences than text. We show that a naïve method to create compressed phoneme-like speech representations is far more effective and efficient for translation than traditional frame-level speech features. Specifically, we generate phoneme labels for speech frames and average consecutive frames with the same label to create shorter, higher-level source sequences for translation. We see improvements of up to 5 BLEU on both our high and low resource language pairs, with a reduction in training time of 60%. Our improvements hold across multiple data sizes and two language pairs.",
        "id":174798018
      },
      {
        "title":"FFB: A Fair Fairness Benchmark for In-Processing Group Fairness Methods",
        "text":"This paper introduces the Fair Fairness Benchmark (FFB), a benchmarking framework for in-processing group fairness methods. Ensuring fairness in machine learning is critical for ethical and legal compliance. However, there exist challenges in comparing and developing of fairness methods due to inconsistencies in experimental settings, lack of accessible algorithmic implementations, and limited extensibility of current fairness packages and tools. To address these issues, we introduce an open-source, standardized benchmark for evaluating in-processing group fairness methods and provide a comprehensive analysis of state-of-the-art methods to ensure different notions of group fairness. This work offers the following key contributions: the provision of flexible, extensible, minimalistic, and research-oriented opensource code; the establishment of unified fairness method benchmarking pipelines; and extensive benchmarking, which yields key insights from 45, 079 experiments. We believe our work will significantly facilitate the growth and development of the fairness research community. The benchmark, including code and running logs, is available at https:\/\/github.com\/ahxt\/fair_fairness_benchmark. * This work was done while the first author was an intern at Meta.2We use utility to represent the performance of the downstream task.Preprint. Under review.",
        "id":259187750
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Can you point me towards research on contrastive learning methods used for fine-tuning sentence representations, where in-batch negatives may sometimes unintentionally be similar to the positive examples?",
    "positive_ctxs":[
      {
        "title":"ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer",
        "text":"Learning high-quality sentence representations benefits a wide range of natural language processing tasks. Though BERT-based pretrained language models achieve high performance on many downstream tasks, the native derived sentence representations are proved to be collapsed and thus produce a poor performance on the semantic textual similarity (STS) tasks. In this paper, we present ConSERT, a Contrastive Framework for Self-Supervised SEntence Representation Transfer, that adopts contrastive learning to fine-tune BERT in an unsupervised and effective way. By making use of unlabeled texts, ConSERT solves the collapse issue of BERT-derived sentence representations and make them more applicable for downstream tasks. Experiments on STS datasets demonstrate that ConSERT achieves an 8% relative improvement over the previous state-of-the-art, even comparable to the supervised SBERT-NLI. And when further incorporating NLI supervision, we achieve new stateof-the-art performance on STS tasks. Moreover, ConSERT obtains comparable results with only 1000 samples available, showing its robustness in data scarcity scenarios.",
        "id":235187266
      },
      {
        "title":"SimCSE: Simple Contrastive Learning of Sentence Embeddings",
        "text":"This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using \"entailment\" pairs as positives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2% and 2.2% improvement compared to previous best results. We also show-both theoretically and empirically-that contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available. 1",
        "id":233296292
      }
    ],
    "negative_ctxs":[
      {
        "title":"Building and curating conversational corpora for diversity-aware language science and technology",
        "text":"We present an analysis pipeline and best practice guidelines for building and curating corpora of everyday conversation in diverse languages. Surveying language documentation corpora and other resources that cover 67 languages and varieties from 28 phyla, we describe the compilation and curation process, specify minimal properties of a unified format for interactional data, and develop methods for quality control that take into account turn-taking and timing. Two case studies show the broad utility of conversational data for (i) charting human interactional infrastructure and (ii) tracing challenges and opportunities for current ASR solutions. Linguistically diverse conversational corpora can provide new insights for the language sciences and stronger empirical foundations for language technology.",
        "id":247362437
      },
      {
        "title":"NLP Corpus Observatory -Looking for Constellations in Parallel Corpora to Improve Learners' Collocational Skills",
        "text":"The use of corpora in language learning, both in classroom and self-study situations, has proven useful. Investigations into technology use show a benefit for learners that are able to work with corpus data using easily accessible technology. But relatively little work has been done on exploring the possibilities of parallel corpora for language learning applications.Our work described in this paper explores the applicability of a parallel corpus enhanced with several layers generated by NLP techniques for extracting collocations that are noncompositional and thus indispensable to learn. We identify constellations, i.e. combinations of intra-and interlingual relations, calculate association scores on each relation and, based thereon, a joint score for each constellation. This way, we are able to find relevant collocations for different types of constellations.We evaluate our approach and discuss scenarios in which language learners can playfully explore collocations. Our explorative web tool is freely accessible, generates collocation dictionaries on the fly, and links them to example sentences to ensure context embedding.Gerold Schneider and Johannes Graën 2018. NLP Corpus Observatory -Looking for constellations in parallel corpora to improve learners' collocational skills.",
        "id":108381061
      },
      {
        "title":"Generating Syntactic Paraphrases",
        "text":"We study the automatic generation of syntactic paraphrases using four different models for generation: data-to-text generation, textto-text generation, text reduction and text expansion, We derive training data for each of these tasks from the WebNLG dataset and we show (i) that conditioning generation on syntactic constraints effectively permits the generation of syntactically distinct paraphrases for the same input and (ii) that exploiting different types of input (data, text or data+text) further increases the number of distinct paraphrases that can be generated for a given input.",
        "id":53083307
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"Can you point to studies or tasks focused on detecting patronizing and condescending language, particularly in contexts involving vulnerable communities?",
    "positive_ctxs":[
      {
        "title":"SemEval-2022 Task 4: Patronizing and Condescending Language Detection",
        "text":"This paper presents an overview of Task 4 at SemEval-2022, which was focused on detecting Patronizing and Condescending Language (PCL) towards vulnerable communities. Two sub-tasks were considered: a binary classification task, where participants needed to classify a given paragraph as containing PCL or not, and a multi-label classification task, where participants needed to identify which types of PCL are present (if any). The task attracted 77 teams. We provide an overview of how the task was organized, discuss the techniques that were employed by the different participants, and summarize the main resulting insights about PCL detection and categorization.",
        "id":250390607
      }
    ],
    "negative_ctxs":[
      {
        "title":"A Deep Transfer Learning Method for Cross-Lingual Natural Language Inference",
        "text":"Natural Language Inference (NLI), also known as Recognizing Textual Entailment (RTE), has been one of the central tasks in Artificial Intelligence (AI) and Natural Language Processing (NLP). RTE between the two pieces of texts is a crucial problem, and it adds further challenges when involving two different languages, i.e., in the cross-lingual scenario. This paper proposes an effective transfer learning approach for cross-lingual NLI. We perform experiments on English-Hindi language pairs in the cross-lingual setting to find out that our novel loss formulation could enhance the performance of the baseline model by up to 2%. To assess the effectiveness of our method further, we perform additional experiments on every possible language pair using four European languages, namely French, German, Bulgarian, and Turkish, on top of XNLI dataset. Evaluation results yield up to 10% performance improvement over the respective baseline models, in some cases surpassing the state-of-the-art (SOTA). It is also to be noted that our proposed model has 110M parameters which is much lesser than the SOTA model having 220M parameters. Finally, we argue that our transfer learning-based loss objective is model agnostic and thus can be used with other deep learning-based architectures for cross-lingual NLI.",
        "id":250163947
      },
      {
        "title":"Determining the Sentiment of Opinions",
        "text":"Identifying sentiments (the affective parts of opinions) is a challenging problem. We present a system that, given a topic, automatically finds the people who hold opinions about that topic and the sentiment of each opinion. The system contains a module for determining word sentiment and another for combining sentiments within a sentence. We experiment with various models of classifying and combining sentiment at word and sentence levels, with promising results.",
        "id":5690545
      },
      {
        "title":"Constructional Intensifying Adjectives in Italian",
        "text":"Grading is a primary cognitive operation that has an important expressive function.",
        "id":2628500
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Can you recommend a conversational QA dataset where the human questioner does not have access to the evidence passage to simulate a more real-world information-seeking environment?",
    "positive_ctxs":[
      {
        "title":"QuAC : Question Answering in Context",
        "text":"We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-ofthe-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is significant room for future work on this data. Dataset, baseline, and leaderboard available at http:\/\/quac.ai.",
        "id":52057510
      }
    ],
    "negative_ctxs":[
      {
        "title":"Data-driven Model Generalizability in Crosslinguistic Low-resource Morphological Segmentation",
        "text":"Common designs of model evaluation typically focus on monolingual settings, where different models are compared according to their performance on a single data set that is assumed to be representative of all possible data for the task at hand. While this may be reasonable for a large data set, this assumption is difficult to maintain in low-resource scenarios, where artifacts of the data collection can yield data sets that are outliers, potentially making conclusions about model performance coincidental. To address these concerns, we investigate model generalizability in crosslinguistic low-resource scenarios. Using morphological segmentation as the test case, we compare three broad classes of models with different parameterizations, taking data from 11 languages across 6 language families. In each experimental setting, we evaluate all models on a first data set, then examine their performance consistency when introducing new randomly sampled data sets with the same size and when applying the trained models to unseen test sets of varying sizes. The results demonstrate that the extent of model generalization depends on the characteristics of the data set, and does not necessarily rely heavily on the data set size. Among the characteristics that we studied, the ratio of morpheme overlap and that of the average number of morphemes per word between the training and test sets are the two most prominent factors. Our findings suggest that future work should adopt random sampling to construct data sets with different sizes in order to make more responsible claims about model evaluation.393",
        "id":245769658
      },
      {
        "title":"IIT Gandhinagar at SemEval-2020 Task 9: Code-Mixed Sentiment Classification Using Candidate Sentence Generation and Selection",
        "text":"Code-mixing is the phenomenon of using multiple languages in the same utterance of a text or speech. It is a frequently used pattern of communication on various platforms such as social media sites, online gaming, product reviews, etc. Sentiment analysis of the monolingual text is a well-studied task. Code-mixing adds to the challenge of analyzing the sentiment of the text due to the non-standard writing style. We present a candidate sentence generation and selection based approach on top of the Bi-LSTM based neural classifier to classify the Hinglish code-mixed text into one of the three sentiment classes positive, negative, or neutral. The proposed approach shows an improvement in the system performance as compared to the Bi-LSTM based neural classifier. The results present an opportunity to understand various other nuances of code-mixing in the textual data, such as humor-detection, intent classification, etc.",
        "id":220055815
      },
      {
        "title":"Zero Alignment of Verb Arguments in a Parallel Treebank",
        "text":"This paper analyses several points of interlingual dependency mismatch on the material of a parallel Czech-English dependency treebank. Particularly, the points of alignment mismatch between the valency frame arguments of the corresponding verbs are observed and described. The attention is drawn to the question whether such mismatches stem from the inherent semantic properties of the individual languages, or from the character of the used linguistic theory. Comments are made on the possible shifts in meaning. The authors use the findings to make predictions about possible machine translation implementation of the data.",
        "id":13998481
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Can you recommend a foundational paper that provides a scalable framework for generating English sentences with controllable semantic and syntactic attributes for the purpose of augmenting datasets in NLP tasks?",
    "positive_ctxs":[
      {
        "title":"Control, Generate, Augment: A Scalable Framework for Multi-Attribute Text Generation",
        "text":"We introduce CGA, a conditional VAE architecture, to control, generate, and augment text. CGA is able to generate natural English sentences controlling multiple semantic and syntactic attributes by combining adversarial learning with a context-aware loss and a cyclical word dropout routine. We demonstrate the value of the individual model components in an ablation study. The scalability of our approach is ensured through a single discriminator, independently of the number of attributes. We show high quality, diversity and attribute control in the generated sentences through a series of automatic and human assessments. As the main application of our work, we test the potential of this new NLG model in a data augmentation scenario. In a downstream NLP task, the sentences generated by our CGA model show significant improvements over a strong baseline, and a classification performance often comparable to adding same amount of additional real data.",
        "id":216914087
      }
    ],
    "negative_ctxs":[
      {
        "title":"InfoSurgeon: Cross-Media Fine-grained Information Consistency Checking for Fake News Detection",
        "text":"To defend against neural system-generated fake news, an effective mechanism is urgently needed. We contribute a novel benchmark for fake news detection at the knowledge element level, as well as a solution for this task which incorporates cross-media consistency checking to detect the fine-grained knowledge elements making news articles misinformative. Due to training data scarcity, we also formulate a novel data synthesis method by manipulating knowledge elements within the knowledge graph to generate noisy training data with specific, hard to detect, known inconsistencies. Our detection approach outperforms the state-of-the-art (up to 16.8% absolute accuracy gain), and more critically, yields fine-grained explanations.",
        "id":236460326
      },
      {
        "title":"Local Space-Time Smoothing for Version Controlled Documents",
        "text":"Unlike static documents, version controlled documents are continuously edited by one or more authors. Such collaborative revision process makes traditional modeling and visualization techniques inappropriate. In this paper we propose a new representation based on local spacetime smoothing that captures important revision patterns. We demonstrate the applicability of our framework using experiments on synthetic and real-world data.",
        "id":811939
      },
      {
        "title":"Search with Synonyms: Problems and Solutions",
        "text":"Search with synonyms is a challenging problem for Web search, as it can easily cause intent drifting. In this paper, we propose a practical solution to this issue, based on co-clicked query analysis, i.e., analyzing queries leading to clicking the same documents. Evaluation results on Web search queries show that synonyms obtained from this approach considerably outperform the thesaurus based synonyms, such as WordNet, in terms of keeping search intent.",
        "id":7643917
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Can you recommend a paper that uses an NLI model for sentence-level relation extraction using hypothesis generation and verification with entity-type constraints?",
    "positive_ctxs":[
      {
        "title":"Label Verbalization and Entailment for Effective Zero-and Few-Shot Relation Extraction",
        "text":"Relation extraction systems require large amounts of labeled examples which are costly to annotate. In this work we reformulate relation extraction as an entailment task, with simple, hand-made, verbalizations of relations produced in less than 15 minutes per relation. The system relies on a pretrained textual entailment engine which is run as-is (no training examples, zero-shot) or further fine-tuned on labeled examples (few-shot or fully trained). In our experiments on TACRED we attain 63% F1 zero-shot, 69% with 16 examples per relation (17% points better than the best supervised system on the same conditions), and only 4 points short of the state-of-the-art (which uses 20 times more training data). We also show that the performance can be improved significantly with larger entailment models, up to 12 points in zero-shot, giving the best results to date on TACRED when fully trained. The analysis shows that our few-shot systems are especially effective when discriminating between relations, and that the performance difference in low data regimes comes mainly from identifying no-relation cases.",
        "id":237442211
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":219300751
      },
      {
        "title":"",
        "text":"",
        "id":170476009
      },
      {
        "title":"Associating Collocations with WordNet Senses Using Hybrid Models",
        "text":"In this paper, we introduce a hybrid method to associate English collocations with sense class members chosen from WordNet. Our combinational approach includes a learning-based method, a paraphrase-based method and a sense frequency ranking method. At training time, a set of collocations with their tagged senses is prepared. We use the sentence information extracted from a large corpus and cross-lingual information to train a learning-based model. At run time, the corresponding senses of an input collocation will be decided via majority voting. The three outcomes participated in voting are as follows: 1. the result from a learning-based model; 2. the result from a paraphrase-based model; 3. the result from sense frequency ranking method. The sense with most votes will be associated with the input collocation. Evaluation shows that the hybrid model achieves significant improvement when comparing with the other method described in evaluation time. Our method provides more reliable result on associating collocations with senses that can help lexicographers in compilation of collocations dictionaries and assist learners to understand collocation usages.",
        "id":2908106
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"Can you recommend some literature that focuses on dependency-based models for relation extraction, especially those that utilize dependency parsing to capture non-local syntactic relations?",
    "positive_ctxs":[
      {
        "title":"N -ary Relation Extraction using Graph State LSTM",
        "text":"Cross-sentence n-ary relation extraction detects relations among n entities across multiple sentences. Typical methods formulate an input as a document graph, integrating various intra-sentential and inter-sentential dependencies. The current state-of-the-art method splits the input graph into two DAGs, adopting a DAG-structured LSTM for each. Though being able to model rich linguistic knowledge by leveraging graph edges, important information can be lost in the splitting procedure. We propose a graph-state LSTM model, which uses a parallel state to model each word, recurrently enriching state values via message passing. Compared with DAG LSTMs, our graph LSTM keeps the original graph structure, and speeds up computation by allowing more parallelization. On a standard benchmark, our model shows the best result in the literature.",
        "id":52115592
      },
      {
        "title":"Bidirectional Recurrent Convolutional Neural Network for Relation Classification",
        "text":"Relation classification is an important semantic processing task in the field of natural language processing (NLP). In this paper, we present a novel model BRCNN to classify the relation of two entities in a sentence. Some state-of-the-art systems concentrate on modeling the shortest dependency path (SDP) between two entities leveraging convolutional or recurrent neural networks. We further explore how to make full use of the dependency relations information in the SDP, by combining convolutional neural networks and twochannel recurrent neural networks with long short term memory (LSTM) units. We propose a bidirectional architecture to learn relation representations with directional information along the SDP forwards and backwards at the same time, which benefits classifying the direction of relations. Experimental results show that our method outperforms the state-of-theart approaches on the SemEval-2010 Task 8 dataset.",
        "id":1774259
      },
      {
        "title":"Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Paths",
        "text":"Relation classification is an important research arena in the field of natural language processing (NLP). In this paper, we present SDP-LSTM, a novel neural network to classify the relation of two entities in a sentence. Our neural architecture leverages the shortest dependency path (SDP) between two entities; multichannel recurrent neural networks, with long short term memory (LSTM) units, pick up heterogeneous information along the SDP. Our proposed model has several distinct features: (1) The shortest dependency paths retain most relevant information (to relation classification), while eliminating irrelevant words in the sentence. (2) The multichannel LSTM networks allow effective information integration from heterogeneous sources over the dependency paths.(3) A customized dropout strategy regularizes the neural network to alleviate overfitting. We test our model on the SemEval 2010 relation classification task, and achieve an F 1 -score of 83.7%, higher than competing methods in the literature.",
        "id":5403702
      },
      {
        "title":"End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures",
        "text":"We present a novel end-to-end neural model to extract entities and relations between them. Our recurrent neural network based model captures both word sequence and dependency tree substructure information by stacking bidirectional treestructured LSTM-RNNs on bidirectional sequential LSTM-RNNs. This allows our model to jointly represent both entities and relations with shared parameters in a single model. We further encourage detection of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sampling. Our model improves over the stateof-the-art feature-based model on end-toend relation extraction, achieving 12.1% and 5.7% relative error reductions in F1score on ACE2005 and ACE2004, respectively. We also show that our LSTM-RNN based model compares favorably to the state-of-the-art CNN based model (in F1-score) on nominal relation classification (SemEval-2010 Task 8). Finally, we present an extensive ablation analysis of several model components.",
        "id":2476229
      }
    ],
    "negative_ctxs":[
      {
        "title":"Maltilex: A Computational Lexicon for Maltese",
        "text":"The project described in this paper, which is still in the preliminary phase, concerns the design and implementation of a computational lexicon for Maltese, a language very much in current use but so far lacking most of the infrastructure required for NLP. One of the main characteristics of Maltese, a source of many difficulties, is that it is an amalgam of different language types (chiefly Semitic and Romance), as illustrated in the first part of the paper. The latter part of the paper describes our general approach to the problem of constructing the lexicon.",
        "id":4013259
      },
      {
        "title":"An Application of Latent Semantic Analysis to Word Sense Discrimination for Words with Related and Unrelated Meanings",
        "text":"We present an application of Latent Semantic Analysis to word sense discrimination within a tutor for English vocabulary learning. We attempt to match the meaning of a word in a document with the meaning of the same word in a fill-in-the-blank question. We compare the performance of the Lesk algorithm to Latent Semantic Analysis. We also compare the performance of Latent Semantic Analysis on a set of words with several unrelated meanings and on a set of words having both related and unrelated meanings.",
        "id":296903
      },
      {
        "title":"The Character-based CRF Segmenter of MSRA&NEU for the 4th Bakeoff",
        "text":"This paper describes the Chinese Word Segmenter for the fourth International Chinese Language Processing Bakeoff. Base on Conditional Random Field (CRF) model, a basic segmenter is designed as a problem of character-based tagging. To further improve the performance of our segmenter, we employ a word-based approach to increase the in-vocabulary (IV) word recall and a post-processing to increase the out-of-vocabulary (OOV) word recall. We participate in the word segmentation closed test on all five corpora and our system achieved four second best and one the fifth in all the five corpora.",
        "id":18174510
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Can you refer me to research that adapts the concept of Word Mover's Distance to sentences, addressing the limitations of bag-of-words approaches and considering the order of words for text similarity?",
    "positive_ctxs":[
      {
        "title":"Sentence Mover's Similarity: Automatic Evaluation for Multi-Sentence Texts",
        "text":"For evaluating machine-generated texts, automatic methods hold the promise of avoiding collection of human judgments, which can be expensive and time-consuming. The most common automatic metrics, like BLEU and ROUGE, depend on exact word matching, an inflexible approach for measuring semantic similarity. We introduce methods based on sentence mover's similarity; our automatic metrics evaluate text in a continuous space using word and sentence embeddings. We find that sentence-based metrics correlate with human judgments significantly better than ROUGE, both on machine-generated summaries (average length of 3.4 sentences) and human-authored essays (average length of 7.5). We also show that sentence mover's similarity can be used as a reward when learning a generation model via reinforcement learning; we present both automatic and human evaluations of summaries learned in this way, finding that our approach outperforms ROUGE.1 For readability, we scale ROUGE scores by a factor of 100 and sentence mover's metrics by a factor of 1000.ReferencesMohammed Alshahrani, Spyridon Samothrakis, and Maria Fasli. 2017. Word mover's distance for affect detection.",
        "id":192530110
      }
    ],
    "negative_ctxs":[
      {
        "title":"Utilizing Temporal Information for Taxonomy Construction",
        "text":"Taxonomies play an important role in many applications by organizing domain knowledge into a hierarchy of 'is-a' relations between terms. Previous work on automatic construction of taxonomies from text documents either ignored temporal information or used fixed time periods to discretize the time series of documents. In this paper, we propose a time-aware method to automatically construct and effectively maintain a taxonomy from a given series of documents preclustered for a domain of interest. The method extracts temporal information from the documents and uses a timestamp contribution function to score the temporal relevance of the evidence from source texts when identifying the taxonomic relations for constructing the taxonomy. Experimental results show that our proposed method outperforms the stateof-the-art methods by increasing F-measure up to 7%-20%. Furthermore, the proposed method can incrementally update the taxonomy by adding fresh relations from new data and removing outdated relations using an information decay function. It thus avoids rebuilding the whole taxonomy from scratch for every update and keeps the taxonomy effectively up-to-date in order to track the latest information trends in the rapidly evolving domain.551",
        "id":16168839
      },
      {
        "title":"Misspelling Oblivious Word Embeddings",
        "text":"In this paper we present a method to learn word embeddings that are resilient to misspellings.Existing word embeddings have limited applicability to malformed texts, which contain a non-negligible amount of outof-vocabulary words. We propose a method combining FastText with subwords and a supervised task of learning misspelling patterns. In our method, misspellings of each word are embedded close to their correct variants. We train these embeddings on a new dataset we are releasing publicly. Finally, we experimentally show the advantages of this approach on both intrinsic and extrinsic NLP tasks using public test sets.",
        "id":162184257
      },
      {
        "title":"An Analytical Study of Synonymy in Assamese Language Using WorldNet: Classification and Structure",
        "text":"The present paper aims to categorize different types of synonymous words and also to highlight their synonymic pattern as well as grammatical categories found in Wordnet of Assamese language. Synonymy is an important component of vocabulary of the language. It establishes lexical relation between words. In fact, the term 'synonymy' is applied to the two or more words which share the same semantic features. WorldNet is a lexical database consisting of synsets. A synset is constructed by assembling a set of synonyms that together define a unique sense and synset is the basic foundation of Wordnet. Assamese language is rich in synonyms. In Assamese WorldNet, more than 20,000 synsets are entered under the categories of Noun, Verb, Adverb and Adjective. These synsets can of different types according to their semantic similarity, connotation, denotation, stylistic variations etc.",
        "id":15673301
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Can you suggest a corpus that contains French encyclopedia documents with semantic annotations and includes a test set of manually written question\/answer triplets that align with the constraints of FrameNet semantic analysis?",
    "positive_ctxs":[
      {
        "title":"Semantic Frame Parsing for Information Extraction : the CALOR corpus",
        "text":"This paper presents a publicly available corpus of French encyclopedic history texts annotated according to the Berkeley FrameNet formalism. The main difference in our approach compared to previous works on semantic parsing with FrameNet is that we are not interested here in full text parsing but rather on partial parsing. The goal is to select from the FrameNet resources the minimal set of frames that are going to be useful for the applicative framework targeted, in our case Information Extraction from encyclopedic documents. Such an approach leverages the manual annotation of larger corpora than those obtained through full text parsing and therefore opens the door to alternative methods for Frame parsing than those used so far on the FrameNet 1.5 benchmark corpus. The approaches compared in this study rely on an integrated sequence labeling model which jointly optimizes frame identification and semantic role segmentation and identification. The models compared are CRFs and multitasks bi-LSTMs.",
        "id":21725691
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":218977408
      },
      {
        "title":"DEEPDSL: A COMPILATION-BASED DOMAIN- SPECIFIC LANGUAGE FOR DEEP LEARNING",
        "text":"In recent years, Deep Learning (DL) has found great success in domains such as multimedia understanding. However, the complex nature of multimedia data makes it difficult to develop DL-based software. The state-of-the-art tools, such as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their applicable domains, are programming libraries with fixed user interface, internal representation, and execution environment. This makes it difficult to implement portable and customized DL applications. In this paper, we present DeepDSL, a domain specific language (DSL) embedded in Scala, that compiles deep networks written in DeepDSL to Java source code. Deep DSL provides (1) intuitive constructs to support compact encoding of deep networks; (2) symbolic gradient derivation of the networks; (3) static analysis for memory consumption and error detection; and (4) DSL-level optimization to improve memory and runtime efficiency. DeepDSL programs are compiled into compact, efficient, customizable, and portable Java source code, which operates the CUDA and CUDNN interfaces running on Nvidia GPU via a Java Native Interface (JNI) library. We evaluated DeepDSL with a number of popular DL networks. Our experiments show that the compiled programs have very competitive runtime performance and memory efficiency compared to the existing libraries.",
        "id":11345245
      },
      {
        "title":"",
        "text":"",
        "id":237155090
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"Can you suggest any literature that explores the idea of training neural networks to translate text passages into related questions?",
    "positive_ctxs":[
      {
        "title":"Learning to Ask: Neural Question Generation for Reading Comprehension",
        "text":"We study automatic question generation for sentences from text passages in reading comprehension. We introduce an attention-based sequence learning model for the task and investigate the effect of encoding sentence-vs. paragraph-level information. In contrast to all previous work, our model does not rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead trainable end-to-end via sequenceto-sequence learning. Automatic evaluation results show that our system significantly outperforms the state-of-the-art rule-based system. In human evaluations, questions generated by our system are also rated as being more natural (i.e., grammaticality, fluency) and as more difficult to answer (in terms of syntactic and lexical divergence from the original text and reasoning needed to answer).",
        "id":2172129
      }
    ],
    "negative_ctxs":[
      {
        "title":"How \"open\" are the conversations with open-domain chatbots? A proposal for Speech Event based evaluation",
        "text":"Open-domain chatbots are supposed to converse freely with humans without being restricted to a topic, task or domain. However, the boundaries and\/or contents of opendomain conversations are not clear. To clarify the boundaries of \"openness\", we conduct two studies: First, we classify the types of \"speech events\" encountered in a chatbot evaluation data set (i.e., Meena by Google) and find that these conversations mainly cover the \"small talk\" category and exclude the other speech event categories encountered in real life human-human communication. Second, we conduct a small-scale pilot study to generate online conversations covering a wider range of speech event categories between two humans vs. a human and a state-of-the-art chatbot (i.e., Blender by Facebook). A human evaluation of these generated conversations indicates a preference for human-human conversations, since the human-chatbot conversations lack coherence in most speech event categories. Based on these results, we suggest (a) using the term \"small talk\" instead of \"opendomain\" for the current chatbots which are not that \"open\" in terms of conversational abilities yet, and (b) revising the evaluation methods to test the chatbot conversations against other speech events.",
        "id":237099289
      },
      {
        "title":"Sparks: Inspiration for Science Writing using Language Models",
        "text":"Large-scale language models are rapidly improving, performing well on a variety of tasks with little to no customization. In this work we investigate how language models can support science writing, a challenging writing task that is both open-ended and highly constrained. We present a system for generating \"sparks\", sentences related to a scientific concept intended to inspire writers. We run a user study with 13 STEM graduate students and find three main use cases of sparks-inspiration, translation, and perspective-each of which correlates with a unique interaction pattern. We also find that while participants were more likely to select higher quality sparks, the overall quality of sparks seen by a given participant did not correlate with their satisfaction with the tool. 1",
        "id":239009871
      },
      {
        "title":"Improving Reading Comprehension Question Generation with Data Augmentation and Overgenerate-and-rank",
        "text":"Reading comprehension is a crucial skill in many aspects of education, including language learning, cognitive development, and fostering early literacy skills in children. Automated answer-aware reading comprehension question generation has significant potential to scale up learner support in educational activities. One key technical challenge in this setting is that there can be multiple questions, sometimes very different from each other, with the same answer; a trained question generation method may not necessarily know which question human educators would prefer. To address this challenge, we propose 1) a data augmentation method that enriches the training dataset with diverse questions given the same context and answer and 2) an overgenerate-and-rank method to select the best question from a pool of candidates. We evaluate our method on the FairytaleQA dataset, showing a 5% absolute improvement in ROUGE-L over the best existing method. We also demonstrate the effectiveness of our method in generating harder, \"implicit\" questions, where the answers are not contained in the context as text spans.",
        "id":259165390
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Can you suggest literature on a dataset that categorizes various emotions like anger, anticipation, fear, joy, and sadness in Facebook posts across multiple languages?",
    "positive_ctxs":[
      {
        "title":"Universal Joy A Data Set and Results for Classifying Emotions Across Languages",
        "text":"While emotions are universal aspects of human psychology, they are expressed differently across different languages and cultures. We introduce a new data set of over 530k anonymized public Facebook posts across 18 languages, labeled with five different emotions. Using multilingual BERT embeddings, we show that emotions can be reliably inferred both within and across languages. Zero-shot learning produces promising results for lowresource languages. Following established theories of basic emotions, we provide a detailed analysis of the possibilities and limits of crosslingual emotion classification. We find that structural and typological similarity between languages facilitates cross-lingual learning, as well as linguistic diversity of training data. Our results suggest that there are commonalities underlying the expression of emotion in different languages. We publicly release the anonymized data for future research.",
        "id":233364964
      }
    ],
    "negative_ctxs":[
      {
        "title":"Kyoto: An Integrated System for Specific Domain WSD",
        "text":"This document describes the preliminary release of the integrated Kyoto system for specific domain WSD. The system uses concept miners (Tybots) to extract domain-related terms and produces a domain-related thesaurus, followed by knowledge-based WSD based on wordnet graphs (UKB). The resulting system can be applied to any language with a lexical knowledge base, and is based on publicly available software and resources. Our participation in Semeval task #17 focused on producing running systems for all languages in the task, and we attained good results in all except Chinese. Due to the pressure of the time-constraints in the competition, the system is still under development, and we expect results to improve in the near future.",
        "id":15865150
      },
      {
        "title":"What do tokens know about their characters and how do they know it?",
        "text":"Pre-trained language models (PLMs) that use subword tokenization schemes can succeed at a variety of language tasks that require characterlevel information, despite lacking explicit access to the character composition of tokens. Here, studying a range of models (e.g., GPT-J, BERT, RoBERTa, GloVe), we probe what word pieces encode about character-level information by training classifiers to predict the presence or absence of a particular alphabetical character in a token, based on its embedding (e.g., probing whether the model embedding for \"cat\" encodes that it contains the character \"a\"). We find that these models robustly encode character-level information and, in general, larger models perform better at the task. We show that these results generalize to characters from non-Latin alphabets (Arabic, Devanagari, and Cyrillic). Then, through a series of experiments and analyses, we investigate the mechanisms through which PLMs acquire English-language character information during training and argue that this knowledge is acquired through multiple phenomena, including a systematic relationship between particular characters and particular parts of speech, as well as natural variability in the tokenization of related strings.",
        "id":249394509
      },
      {
        "title":"End-to-End Synthetic Data Generation for Domain Adaptation of Question Answering Systems",
        "text":"We propose an end-to-end approach for synthetic QA data generation. Our model comprises a single transformer-based encoderdecoder network that is trained end-to-end to generate both answers and questions. In a nutshell, we feed a passage to the encoder and ask the decoder to generate a question and an answer token-by-token. The likelihood produced in the generation process is used as a filtering score, which avoids the need for a separate filtering model. Our generator is trained by finetuning a pretrained LM using maximum likelihood estimation. The experimental results indicate significant improvements in the domain adaptation of QA models outperforming current state-of-the-art methods. * *equal contribution. † Siamak Shakeri is currently with Google. The work was done when he was at AWS AI.",
        "id":222310116
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"Can you suggest literature on enhanced semantic parsing methods that focus on generating high-quality meaning representations and utilize knowledge-constrained decoding under specific grammar rules?",
    "positive_ctxs":[
      {
        "title":"A Syntactic Neural Model for General-Purpose Code Generation",
        "text":"We consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like Python. Existing datadriven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language. Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge. Experiments find this an effective way to scale up to generation of complex programs from natural language descriptions, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.",
        "id":12718048
      },
      {
        "title":"Neural Semantic Parsing with Type Constraints for Semi-Structured Tables",
        "text":"We present a new semantic parsing model for answering compositional questions on semi-structured Wikipedia tables. Our parser is an encoder-decoder neural network with two key technical innovations:(1) a grammar for the decoder that only generates well-typed logical forms; and (2) an entity embedding and linking module that identifies entity mentions while generalizing across tables. We also introduce a novel method for training our neural model with question-answer supervision. On the WIKITABLEQUESTIONS data set, our parser achieves a state-of-theart accuracy of 43.3% for a single model and 45.9% for a 5-model ensemble, improving on the best prior score of 38.7% set by a 15-model ensemble. These results suggest that type constraints and entity linking are valuable components to incorporate in neural semantic parsers.",
        "id":1675452
      },
      {
        "title":"Sequence-based Structured Prediction for Semantic Parsing",
        "text":"We propose an approach for semantic parsing that uses a recurrent neural network to map a natural language question into a logical form representation of a KB query. Building on recent work by(Wang et al., 2015), the interpretable logical forms, which are structured objects obeying certain constraints, are enumerated by an underlying grammar and are paired with their canonical realizations. In order to use sequence prediction, we need to sequentialize these logical forms. We compare three sequentializations: a direct linearization of the logical form, a linearization of the associated canonical realization, and a sequence consisting of derivation steps relative to the underlying grammar. We also show how grammatical constraints on the derivation sequence can easily be integrated inside the RNNbased sequential predictor. Our experiments show important improvements over previous results for the same dataset, and also demonstrate the advantage of incorporating the grammatical constraints.",
        "id":16911296
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"In this paper, we explore a new approach for automated chess commentary generation, which aims to generate chess commentary texts in different categories (e.g., description, comparison, planning, etc.). We introduce a neural chess engine into text generation models to help with encoding boards, predicting moves, and analyzing situations. By jointly training the neural chess engine and the generation models for different categories, the models become more effective. We conduct experiments on 5 categories in a benchmark Chess Commentary dataset and achieve inspiring results in both automatic and human evaluations.",
        "id":196203256
      },
      {
        "title":"",
        "text":"",
        "id":221373808
      },
      {
        "title":"Variabilité des syllabes réalisées par des apprenants de l'anglais",
        "text":"Cette contribution analyse la segmentation syllabique des francophones du corpus d'apprenant d'anglais ANGLISH (Tortel 2009). A partir d'une méthode d'alignement par alignement forcé, on montre la pertinence d'une analyse de l'interlangue fondée sur la comparaison des durées des syllabes. La comparaison des réalisations est ici centrée sur une typologie des syllabes fondée sur des propriétés distributionnelles, accentuelles et où l'interlangue tient sa place (risques d'isosyllabicité les plus manifestes pour les réalisations des francophones). La variabilité des réalisations des syllabes est appréciée en fonction des propriétés positionnelles, accentuelles et structurelles des syllabes. L'étude démontre l'intérêt d'une approche fonctionnelle des syllabes, plus pertinente que les intervalles interconsonantiques et intervocaliques inspirés deRamus et al. (1999)pour la discrimination du niveau des locuteurs.ABSTRACTAnalysing syllable variability in a French learner corpus of English.This paper suggests an alternative method to classify native and non-native rhythmic realisations. Learner phonetic output has been automatically aligned on a native template of English syllables. Syllables have been classified according to positional, distributional and accentual properties. These syllable types differ significantly in their realisations between native and non-native speakers of English.MOTS-CLÉS : syllabation, interphonologie français\/anglais, alignement forcé, durée.",
        "id":64813718
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"Can you suggest recent studies that have integrated prompt fine-tuning into semi-supervised learning workflows for natural language understanding tasks?",
    "positive_ctxs":[
      {
        "title":"Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference",
        "text":"Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with \"task descriptions\" in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in lowresource settings by a large margin. 1",
        "id":210838924
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":219310164
      },
      {
        "title":"Automated Extrac-tion of Tree Adjoining Grammars from a Treebank for Vietnamese",
        "text":"Tóm tắt nội dungBài báo này giới thiệu hệ văn phạm kết nối cây LTAG (Lexicalized Tree Adjoining Grammars -LTAG) và các thuật toán trích rút tự động LTAG từ kho văn bản gán nhãn cú pháp (treebank). Chúng tôi trình bày kết quả trích rút một văn phạm LTAG cho tiếng Việt. Chương trình trích rút tự động các văn phạm LTAG độc lập với ngôn ngữ và được phân phối dưới dạng mã nguồn mở.Từ khoá: trích rút, LTAG, treebank, tiếng Việt.AbstractIn this paper, we present a system that automatically extracts lexicalized tree adjoining grammars (LTAG) from treebanks. We first discuss in detail extraction algorithms and compare them to previous works. We then report the first LTAG extraction result for Vietnamese, using a recently released Vietnamese treebank. The implementation of an open source and language independent system for automatic extraction of LTAG grammars is also discussed.",
        "id":13647606
      },
      {
        "title":"UNBNLP at SemEval-2019 Task 5 and 6: Using Language Models to Detect Hate Speech and Offensive Language",
        "text":"In this paper we apply a range of approaches to language modeling -including wordlevel n-gram and neural language models, and character-level neural language models -to the problem of detecting hate speech and offensive language. Our findings indicate that language models are able to capture knowledge of whether text is hateful or offensive. However, our findings also indicate that moreconventional approaches to text classification often perform similarly or better.",
        "id":184483194
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Can you suggest some literature that evaluates the ability of context-aware machine translation systems to handle discourse phenomena such as deixis and lexical cohesion?",
    "positive_ctxs":[
      {
        "title":"When a Good Translation is Wrong in Context: Context-Aware Machine Translation Improves on Deixis, Ellipsis, and Lexical Cohesion",
        "text":"Though machine translation errors caused by the lack of context beyond one sentence have long been acknowledged, the development of context-aware NMT systems is hampered by several problems. Firstly, standard metrics are not sensitive to improvements in consistency in document-level translations. Secondly, previous work on context-aware NMT assumed that the sentence-aligned parallel data consisted of complete documents while in most practical scenarios such document-level data constitutes only a fraction of the available parallel data. To address the first issue, we perform a human study on an English-Russian subtitles dataset and identify deixis, ellipsis and lexical cohesion as three main sources of inconsistency. We then create test sets targeting these phenomena. To address the second shortcoming, we consider a set-up in which a much larger amount of sentence-level data is available compared to that aligned at the document level. We introduce a model that is suitable for this scenario and demonstrate major gains over a context-agnostic baseline on our new benchmarks without sacrificing performance as measured with BLEU. 1",
        "id":155089628
      }
    ],
    "negative_ctxs":[
      {
        "title":"A Transformer-based joint-encoding for Emotion Recognition and Sentiment Analysis",
        "text":"Understanding expressed sentiment and emotions are two crucial factors in human multimodal language. This paper describes a Transformer-based joint-encoding (TBJE) for the task of Emotion Recognition and Sentiment Analysis. In addition to use the Transformer architecture, our approach relies on a modular co-attention and a glimpse layer to jointly encode one or more modalities. The proposed solution has also been submitted to the ACL20: Second Grand-Challenge on Multimodal Language to be evaluated on the CMU-MOSEI dataset. The code to replicate the presented experiments is open-source 1 .",
        "id":220250648
      },
      {
        "title":"A Grain of Salt for the WMT Manual Evaluation *",
        "text":"The Workshop on Statistical Machine Translation (WMT) has become one of ACL's flagship workshops, held annually since 2006. In addition to soliciting papers from the research community, WMT also features a shared translation task for evaluating MT systems. This shared task is notable for having manual evaluation as its cornerstone. The Workshop's overview paper, playing a descriptive and administrative role, reports the main results of the evaluation without delving deep into analyzing those results. The aim of this paper is to investigate and explain some interesting idiosyncrasies in the reported results, which only become apparent when performing a more thorough analysis of the collected annotations. Our analysis sheds some light on how the reported results should (and should not) be interpreted, and also gives rise to some helpful recommendation for the organizers of WMT. . We are grateful to our students, colleagues, and the three reviewers for various observations and suggestions.",
        "id":6395516
      },
      {
        "title":"Cryptocurrency Day Trading and Framing Prediction in Microblog Discourse",
        "text":"With 56 million people actively trading and in-001 vesting in cryptocurrency online and globally 002 in 2020, there is an increasing need for auto-003 matic social media analysis tools to help under-004 stand trading discourse and behavior. In this 005 work, we present a dual natural language mod-006 eling pipeline which leverages language and 007 social network behaviors for the prediction of 008 cryptocurrency day trading actions and their 009 associated framing patterns. This pipeline first 010 predicts if tweets can be used to guide day trad-011 ing behavior, specifically if a cryptocurrency 012 investor should buy, sell, or hold their cryp-013 tocurrencies in order to make a profit. Next, 014 tweets are input to an unsupervised deep clus-015 tering approach to automatically detect trad-016 ing framing patterns. Our contributions in-017 clude the modeling pipeline for this novel task, 018 a new Cryptocurrency Tweets Dataset com-019 piled from influential accounts, and a Histor-020 ical Price Dataset. Our experiments show that 021 our approach achieves an 88.78% accuracy for 022 day trading behavior prediction and reveals 023 framing fluctuations prior to and during the 024 COVID-19 pandemic that could be used to 025 guide investment actions. 026 1 Introduction 027 Beginning with the 2008 introduction of Bitcoin 028 (BTC) (Nakamoto, 2008), a cryptocurrency for a 029 Peer-to-Peer cash system, the use of cryptocurren-030 cies and their corresponding blockchains have in-031 creasingly gained in popularity. In 2019, the num-032 ber of Americans owning cryptocurrency doubled 033 from 7% in 2018 to 14%, representing about 35 034 million people trading and investing with cryptocur-035 rency (Partz, 2019). 036 This increase is largely due to the capability 037 of cryptocurrency to improve various applications 038 ranging from increased security of smart contracts 039 to facilitating less expensive, faster cross-border in-040 ternational payments. Another contributing factor 041 to this growth is that digital coins fulfill the prop-042 erty of storing value similar to other fiat currencies, 043 which are government-issued currencies not backed 044 by physical commodities, e.g., the American dollar 045 or euro. Finally, cryptocurrency popularity can be 046 associated with its high day trading volume. As of 047 January 2021, the combined worth of all cryptocur-048 rencies was $1 trillion 1 , with Bitcoin accounting 049 for $650 billion of this amount. To put this in per-050 spective, the average trading volume of Amazon 051 Inc. is $13 billion per day -less than one-fifth of 052 the BTC daily volume of $70 billion. 2 053 Cryptocurrencies were born on the internet, 054 gained their visibility through online and social 055 media coverage, and many investors follow the 056 advice of well-known cryptocurrency experts on 057 Twitter to guide their personal investment strate-058 gies (Mone, 2019). Because cryptocurrency prices 059 can fluctuate quickly, resulting in real-life financial 060 gains or losses, models that can rapidly analyze 061 trending discourse on Twitter can be harnessed to 062 guide and benefit investors. 063 Additionally, work in computational linguistics 064 and the social sciences have shown the benefit of 065 studying framing, which is how someone discusses 066 a topic in order to influence or alter the opinion of 067 the public, for understanding microblog discourse 068 (Card et al., 2015; Johnson et al., 2017; Li and 069 Goldwasser, 2019). Specifically, framing in Twit-070 ter can be used to understand social phenomena, 071 such as political maneuvering or epidemiology cov-072 erage. However, few works exist which study the 073 relationship between framing and cryptocurrency 074 trading, especially during times of economic stress. 075 Currently, it is estimated that the COVID-19 pan-076 demic has negatively impacted the global economy 077 by hindering economic growth worldwide between 078 4.5% and 6.0%, with the potential for future global 079 trade to fall up to 9.2% (CRS, 2020). Similar to 080 1 https:\/\/coinmarketcap.com\/charts\/ 2 https:\/\/finance.yahoo.com\/quote\/AMZN Figure 1: Cryptocurrency Day Trading Prediction and Framing Discovery Pipeline. the pandemic's effect on Wall Street (i.e., the New 081 York Stock Exchange and NASDAQ), the cryp-082 tocurrency market reflected a drastic 47.8% drop 083 on March 12, 2020, one day after the World Health 084 Organization (WHO) announced that COVID-19 085 could be characterized as a pandemic. Around the 086 same time, a similar drop occurred in stocks world-087 wide. Therefore, we hypothesized that microblog 088 discourse about day trading behaviors (i.e., buy, 089 sell, hold) would be a useful predictive feature in 090 understanding cryptocurrency trading and framing. 091 Our contributions include a cryptocurrency day 092 trading behavior modeling pipeline (Figure 1) that 093 leverages language and social network behavior 094 extracted from tweets to implement: (1) a pre-095 dictive model for investment actions, specifically, 096 whether to buy, sell, or hold cryptocurrency based 097 on daily discussions on Twitter, and (2) an unsuper-098 vised deep-learning clustering model to determine 099 the underlying framing patterns used to discuss 100 these investment actions. We have also compiled a 101 Cryptocurrency Tweets Dataset (divided into Pre-102 COVID and during COVID portions) and Bitcoin 103 Historical Price Dataset. 3 Lastly, we propose novel 104 frames for economic and financial analysis. Our 105 models are able to predict trading actions with 106 88.78% accuracy, while also revealing a distinc-107 tion between how day trading behaviors are framed 108 before and during the COVID-19 pandemic.",
        "id":241583406
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Can you suggest some recent datasets that have been used for studying stance detection in tweets, particularly those targeting specific individuals and events since 2020?",
    "positive_ctxs":[
      {
        "title":"P-Stance: A Large Dataset for Stance Detection in Political Domain",
        "text":"Stance detection determines whether the author of a text is in favor of, against or neutral to a specific target and provides valuable insights into important events such as presidential election. However, progress on stance detection has been hampered by the absence of large annotated datasets. In this paper, we present P-STANCE, a large stance detection dataset in the political domain, which contains 21,574 labeled tweets. We provide a detailed description of the newly created dataset and develop deep learning models on it. Our best model achieves a macro-average F1-score of 80.53%, which we improve further by using semi-supervised learning. Moreover, our P-STANCE dataset can facilitate research in the fields of cross-domain stance detection such as cross-target stance detection where a classifier is adapted from a different but related target. We publicly release our dataset and code. 1",
        "id":236477909
      },
      {
        "title":"Stance Detection in COVID-19 Tweets",
        "text":"The prevalence of the COVID-19 pandemic in day-to-day life has yielded large amounts of stance detection data on social media sites, as users turn to social media to share their views regarding various issues related to the pandemic, e.g. stay at home mandates and wearing face masks when out in public. We set out to make use of this data by collecting the stance expressed by Twitter users, with respect to topics revolving around the pandemic. We annotate a new stance detection dataset, called COVID-19-Stance. Using this newly annotated dataset, we train several established stance detection models to ascertain a baseline performance for this specific task. To further improve the performance, we employ self-training and domain adaptation approaches to take advantage of large amounts of unlabeled data and existing stance detection datasets. The dataset, code, and other resources are available on GitHub. 1",
        "id":236460157
      }
    ],
    "negative_ctxs":[
      {
        "title":"Benchmarking Automated Review Response Generation for the Hospitality Domain",
        "text":"Online customer reviews are of growing importance for many businesses in the hospitality industry, particularly restaurants and hotels. Managerial responses to such reviews provide businesses with the opportunity to influence the public discourse and to attain improved ratings over time. However, responding to each and every review is a time-consuming endeavour. Therefore, we investigate automatic generation of review responses in the hospitality domain for two languages, English and German.We apply an existing system, originally proposed for review response generation for smartphone apps. This approach employs an extended neural network sequence-to-sequence architecture and performs well in the original domain. However, as shown through our experiments, when applied to a new domain, such as hospitality, performance drops considerably. Therefore, we analyse potential causes for the differences in performance and provide evidence to suggest that review response generation in the hospitality domain is a more challenging task and thus requires further study and additional domain adaptation techniques.",
        "id":227230307
      },
      {
        "title":"WikiWalk: Random walks on Wikipedia for Semantic Relatedness",
        "text":"Computing semantic relatedness of natural language texts is a key component of tasks such as information retrieval and summarization, and often depends on knowledge of a broad range of real-world concepts and relationships. We address this knowledge integration issue by computing semantic relatedness using personalized PageRank (random walks) on a graph derived from Wikipedia. This paper evaluates methods for building the graph, including link selection strategies, and two methods for representing input texts as distributions over the graph nodes: one based on a dictionary lookup, the other based on Explicit Semantic Analysis. We evaluate our techniques on standard word relatedness and text similarity datasets, finding that they capture similarity information complementary to existing Wikipedia-based relatedness measures, resulting in small improvements on a stateof-the-art measure.",
        "id":805379
      },
      {
        "title":"",
        "text":"",
        "id":218974163
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Could you direct me towards a study that explores the potential to predict a reader's native language based on their eye movement patterns while reading English texts?",
    "positive_ctxs":[
      {
        "title":"Predicting Native Language from Gaze",
        "text":"A fundamental question in language learning concerns the role of a speaker's first language in second language acquisition. We present a novel methodology for studying this question: analysis of eye-movement patterns in second language reading of free-form text. Using this methodology, we demonstrate for the first time that the native language of English learners can be predicted from their gaze fixations when reading English. We provide analysis of classifier uncertainty and learned features, which indicates that differences in English reading are likely to be rooted in linguistic divergences across native languages. The presented framework complements production studies and offers new ground for advancing research on multilingualism. 1",
        "id":14515265
      }
    ],
    "negative_ctxs":[
      {
        "title":"Published as a conference paper at ICLR 2020 LEARNING TO GROUP: A BOTTOM-UP FRAMEWORK FOR 3D PART DISCOVERY IN UNSEEN CATEGORIES",
        "text":"We address the problem of discovering 3D parts for objects in unseen categories.",
        "id":211132680
      },
      {
        "title":"Published as a conference paper at ICLR 2023 DOES DEEP LEARNING LEARN TO ABSTRACT? A SYSTEMATIC PROBING FRAMEWORK",
        "text":"Abstraction is a desirable capability for deep learning models, which means to induce abstract concepts from concrete instances and flexibly apply them beyond the learning context. At the same time, there is a lack of clear understanding about both the presence and further characteristics of this capability in deep learning models. In this paper, we introduce a systematic probing framework to explore the abstraction capability of deep learning models from a transferability perspective. A set of controlled experiments are conducted based on this framework, providing strong evidence that two probed pre-trained language models (PLMs), T5 and GPT2, have the abstraction capability. We also conduct in-depth analysis, thus shedding further light: (1) the whole training phase exhibits a \"memorize-thenabstract\" two-stage process; (2) the learned abstract concepts are gathered in a few middle-layer attention heads, rather than evenly distributed throughout the model;(3) the probed abstraction capabilities exhibit robustness against concept mutations, and are more robust to low-level\/source-side mutations than high-level\/target-side ones; (4) generic pre-training is critical to the emergence of abstraction capability, and PLMs exhibit better abstraction with larger model sizes and data scales. * Work done during an internship at Microsoft Research.Abstract ConceptsInput Output… …Input Output… …Surface PatternsInput Output… …Abstract ConceptsFigure 1: Motivating example: the abstract concepts learned in task A can be effectively reused in task B, but surface patterns are useless. Unused patterns or concepts are whitened after the update.generally reused. We consider designing multiple tasks with shared abstract concepts and totally different surface patterns, then tracing whether the learning on one task can boost the performance on another.Figure 1demonstrates a motivating example.Published as a conference paper at ICLR 2023 Aiming Task Train Set ഥ Probing Task Test Set ഥ Transfer Set Abstract Concepts Same Task-Specific Characteristics Different Contrast Task Contrast Set ഥ Abstract Concepts Broken Main Exp ⟹ Pretrain Finetune Test ഥ Control Exp ⇑ ഥ",
        "id":257102348
      },
      {
        "title":"Introduction to Frontiers in Corpus Annotation",
        "text":"A new annotated corpus can have a pivotal role in the future of computational linguistics. Corpus annotation can define new NLP tasks and set new standards. This may put many of the papers presented at this workshop on the cutting edge of our field.A standard, however, is a double edged sword. A standard corpus urges users to accept the theory of how to represent things that underlie that corpus. For example, a Penn Treebank theory of grammar is implicit in Penn-Treebank-based parsers. This can be a problem if one rejects some aspects of that theory. Also one may object to a particular system of annotation because some theories generalize to cover new ground (e.g., new languages) better than others. Nevertheless, advantages of accepting a corpus as standard include the following:It is straight-forward to compare the performance of the set of systems that produce the same form of output, e.g., Penn Treebank-based parsers can be compared in terms of how well they reproduce the Penn Treebank.Alternative systems based on a standard are largely interchangeable. Thus a system that uses one Penn-Treebank-based parser as a component can easily be adapted to use another better performing Penn-Treebank-based parser.",
        "id":32824591
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Could you point me to research on binary classification systems that predict whether sentences within a context require clarification, particularly using the wikiHowToImprove dataset?",
    "positive_ctxs":[
      {
        "title":"UnImplicit Shared Task Report: Detecting Clarification Requirements in Instructional Text",
        "text":"This paper describes the data, task setup, and results of the shared task at the First Workshop on Understanding Implicit and Underspecified Language (UnImplicit). The task requires computational models to predict whether a sentence contains aspects of meaning that are contextually unspecified and thus require clarification. Two teams participated and the best scoring system achieved an accuracy of 68%.",
        "id":236486314
      }
    ],
    "negative_ctxs":[
      {
        "title":"Logical metonymies and qualia structures: an annotated database of logical metonymies for German",
        "text":"Logical metonymies like The author began the book involve the interpretation of events that are not realized in the sentence (covert events: → writing the book). The Generative Lexicon (Pustejovsky, 1995) provides a qualia-based account of covert event interpretation, claiming that the covert event is retrieved from the qualia structure of the object. Such a theory poses the question of to what extent covert events in logical metonymies can be accounted for by qualia structures. Building on previous work on English, we present a corpus study for German verbs (anfangen (mit), aufhören (mit), beenden, beginnen (mit), geniessen), based on data obtained from the deWaC corpus. We built a corpus of logical metonymies, which were manually annotated and compared with the qualia structures of their objects, then we contrasted annotation results from two expert annotators for metonymies (The author began the book) and long forms (The author began reading the book) across verbs. Our annotation was evaluated on a sample of sentences annotated by a group of naive annotators on a crowdsourcing platform. The logical metonymy database (2661 metonymies and 1886 long forms) with two expert annotations is freely available for scientific research purposes.",
        "id":14484317
      },
      {
        "title":"INFERENCING ON LINGUISTICALLY BASED ZZ~IANTIC STRUCTUR~F",
        "text":"The paper characterizes natural language inferencing in the TIBAQ method of question-answering, focussing on three aspects: ~i) specification of the structures on which the inference rules operate, (ii) classification of the rules that have been formulated and implemented up to now, according to the kind of modification of the input structure ti~e rules invoke, an~ (iii) discussion of some points in which a proverly designed inference procedure may help the searc~ of the answer, and vice versa.",
        "id":1069475
      },
      {
        "title":"Sudachi: a Japanese Tokenizer for Business",
        "text":"Tokenization, or morphological analysis, is a fundamental and important technology for processing a Japanese text, especially for industrial applications. However, we often face many obstacles, such as the inconsistency of token unit in different resources, notation variations, discontinued maintenance of the resources, and various issues with the existing tokenizer implementations. In order to improve this situation, we develop a tokenizer called Sudachi and its accompanying dictionary with features such as multi-granular output and normalization of notation variations. In addition to this, we continuously maintain our software and language resources in long-term as a part of the company business. We release the resulting tokenizer software and language resources freely available to the public as an open source software. You can access them at https:\/\/github.com\/WorksApplications\/Sudachi.",
        "id":21686399
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"Could you point me to studies that discuss the development of open information extraction systems with lexical and syntactic constraints to ensure the extraction is coherent and informative?",
    "positive_ctxs":[
      {
        "title":"Identifying Relations for Open Information Extraction",
        "text":"Open Information Extraction (IE) is the task of extracting assertions from massive corpora without requiring a pre-specified vocabulary. This paper shows that the output of state-ofthe-art Open IE systems is rife with uninformative and incoherent extractions. To overcome these problems, we introduce two simple syntactic and lexical constraints on binary relations expressed by verbs. We implemented the constraints in the REVERB Open IE system, which more than doubles the area under the precision-recall curve relative to previous extractors such as TEXTRUNNER and WOE pos . More than 30% of REVERB's extractions are at precision 0.8 or highercompared to virtually none for earlier systems. The paper concludes with a detailed analysis of REVERB's errors, suggesting directions for future work. 1",
        "id":10318045
      }
    ],
    "negative_ctxs":[
      {
        "title":"G h oSt-PV: A Representative Gold Standard of German Particle Verbs",
        "text":"German particle verbs represent a frequent type of multi-word-expression that forms a highly productive paradigm in the lexicon. Similarly to other multi-word expressions, particle verbs exhibit various levels of compositionality. One of the major obstacles for the study of compositionality is the lack of representative gold standards of human ratings. In order to address this bottleneck, this paper presents such a gold standard data set containing 400 randomly selected German particle verbs. It is balanced across several particle types and three frequency bands, and accomplished by human ratings on the degree of semantic compositionality.",
        "id":1701031
      },
      {
        "title":"The Personal Name Modeling in Mandarin ASR System",
        "text":"摘要 本論文主要有兩個目的：一是訓練一個高效能的中文語音辨識系統；二是改善因人 名而造成的 OOV(Out-Of-Vocabulary)問題，並將其辨認出來，以便日後自動轉寫不同類 型的語音訊息並產生逐字稿。而人名之辨識對於將來自然語言處理也是一重要的訓練資 料。 本論文使用 Kaldi speech recognition toolkit 的環境為基礎，在聲學模型的方面，本 實驗使用類神經網路 TDNN 以達到聲音資訊轉成音素序列(phone sequence)的目的；在 語言模型方面，本論文透過加入中文特有的語言資訊如形音義詞的合併、專有名詞的拆 解，並使用 n-gram 語言模型的訓練，以達到音素序列轉成詞序列(word sequence)的目 的，並於解碼過程中調整參數與權重，找出最佳操作點，以得到即時性與辨識率兼顧的 語音辨識系統，此外，針對以往人名無法辨認出來的問題，本論文建立特別的人名語言 模型以類似 class-based model 的方式置換原 word-based model 中的人名，以達到辨識人 名的目的。AbstractThere are two purposes in the paper, one is training an efficient ASR system, the other is improving the OOV problem caused by the personal name, and we want to recognize it for the purpose of making transcription of different kind of speech data. Name recognition data is also an important training data for the NLP.The paper base on the environment of Kaldi speech recognition toolkit. In the acoustic model part, we use many different kind of neural network such as TDNN to transform the speech information into phone sequence. In the language part, we add Chinese special",
        "id":235482430
      },
      {
        "title":"Dialog Generation Using Multi-turn Reasoning Neural Networks",
        "text":"In this paper, we propose a generalizable dialog generation approach that adapts multiturn reasoning, one recent advancement in the field of document comprehension, to generate responses (\"answers\") by taking current conversation session context as a \"document\" and current query as a \"question\". The major idea is to represent a conversation session into memories upon which attention-based memory reading mechanism can be performed multiple times, so that (1) user's query is properly extended by contextual clues and (2) optimal responses are step-by-step generated. Considering that the speakers of one conversation are not limited to be one, we separate the single memory used for document comprehension into different groups for speaker-specific topic and opinion embedding. Namely, we utilize the queries' memory, the responses' memory, and their unified memory, following the time sequence of the conversation session. Experiments on Japanese 10-sentence (5-round) conversation modeling show impressive results on how multi-turn reasoning can produce more diverse and acceptable responses than stateof-the-art single-turn and non-reasoning baselines.",
        "id":44113253
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"Could you point me to studies that have investigated dialectal variations and normalized text in non-Latin scripts, such as Arabic or Japanese?",
    "positive_ctxs":[
      {
        "title":"",
        "text":"",
        "id":218974489
      },
      {
        "title":"Multi-dialect Neural Machine Translation and Dialectometry",
        "text":"We present a multi-dialect neural machine translation (NMT) model tailored to Japanese. While the surface forms of Japanese dialects differ from those of standard Japanese, most of the dialects share fundamental properties such as word order, and some also use many of the same phonetic correspondence rules. To take advantage of these properties, we integrate multilingual, syllable-level, and fixed-order translation techniques into a general NMT model. Our experimental results demonstrate that this model can outperform a baseline dialect translation model. In addition, we show that visualizing the dialect embeddings learned by the model can facilitate geographical and typological analyses of dialects.",
        "id":198918735
      }
    ],
    "negative_ctxs":[
      {
        "title":"Towards an Ontology for Art and Colours",
        "text":"To meet a variety of needs in information modeling, software development and integration as well as knowledge management and reuse, various groups within industry, academia, and government have been developing and deploying sharable and reusable models known as ontologies. Ontologies play an important role in knowledge representation. In this paper, we address the problem of capturing knowledge needed for indexing and retrieving art resources. We describe a case study in which we attempt to construct an ontology for a subset of art. The aim of the present ontology is to build an extensible repository of knowledge and information about artists, their works and materials used in artistic creations. Influenced by the recent interest in colours and colouring materials, mainly shared by French researchers and linguists, an ontology prototype has been developed using Protégé. It allows to organize and catalogue information about artists, art works, colouring materials and related colours.",
        "id":18043043
      },
      {
        "title":"EmoTag1200 : Understanding the Association between Emojis and Emotions",
        "text":"Given the growing ubiquity of emojis in language, there is a need for methods and resources that shed light on their meaning and communicative role. One conspicuous aspect of emojis is their use to convey affect in ways that may otherwise be non-trivial to achieve. In this paper, we seek to explore the connection between emojis and emotions by means of a new dataset consisting of human-solicited association ratings. We additionally conduct experiments to assess to what extent such associations can be inferred from existing data in an unsupervised manner. Our experiments show that this succeeds when high-quality wordlevel information is available.",
        "id":226262234
      },
      {
        "title":"Ferryman at SemEval-2020 Task 3: Bert with TFIDF-Weighting for Predicting the Effect of Context in Word Similarity",
        "text":"Word similarity is widely used in machine learning applications like searching engine and recommendation. Measuring the changing meaning of the same word between two different sentences is not only a way to handle complex features in word usage (such as sentence syntax and semantics), but also an important method for different word polysemy modeling. In this paper, we present the methodology proposed by team Ferryman. Our system is based on the Bidirectional Encoder Representations from Transformers (BERT) model combined with term frequency-inverse document frequency (TF-IDF), applying the method on the provided datasets called CoSimLex, which covers four different languages including English, Croatian, Slovene, and Finnish. Our team Ferryman wins the the first position for English task and the second position for Finnish in the subtask 1. * All the corresponding to Yan Wang. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:\/\/ creativecommons.org\/licenses\/by\/4.0\/.",
        "id":227231766
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Could you point me toward some large-scale multilingual Amazon customer review data?",
    "positive_ctxs":[
      {
        "title":"The Multilingual Amazon Reviews Corpus",
        "text":"We present the Multilingual Amazon Reviews Corpus (MARC), a large-scale collection of Amazon reviews for multilingual text classification. The corpus contains reviews in English, Japanese, German, French, Spanish, and Chinese, which were collected between 2015 and 2019. Each record in the dataset contains the review text, the review title, the star rating, an anonymized reviewer ID, an anonymized product ID, and the coarse-grained product category (e.g., 'books', 'appliances', etc.) The corpus is balanced across the 5 possible star ratings, so each rating constitutes 20% of the reviews in each language. For each language, there are 200,000, 5,000, and 5,000 reviews in the training, development, and test sets, respectively. We report baseline results for supervised text classification and zero-shot crosslingual transfer learning by fine-tuning a multilingual BERT model on reviews data. We propose the use of mean absolute error (MAE) instead of classification accuracy for this task, since MAE accounts for the ordinal nature of the ratings.",
        "id":222141483
      }
    ],
    "negative_ctxs":[
      {
        "title":"FERMAT: An Alternative to Accuracy for Numerical Reasoning",
        "text":"While pre-trained language models achieve impressive performance on various NLP benchmarks, they still struggle with tasks that require numerical reasoning. Recent advances in improving numerical reasoning are mostly achieved using very large language models that contain billions of parameters and are not accessible to everyone. In addition, numerical reasoning is measured using a single score on existing datasets. As a result, we do not have a clear understanding of the strengths and shortcomings of existing models on different numerical reasoning aspects and therefore, potential ways to improve them apart from scaling them up. Inspired by CheckList (Ribeiro et al., 2020), we introduce a multi-view evaluation set for numerical reasoning in English, called FERMAT. Instead of reporting a single score on a whole dataset, FERMAT evaluates models on various key numerical reasoning aspects such as number understanding, mathematical operations, and training dependency. Apart from providing a comprehensive evaluation of models on different numerical reasoning aspects, FERMAT enables a systematic and automated generation of an arbitrarily large training or evaluation set for each aspect.The datasets and codes are publicly available to generate further multi-view data for ulterior tasks and languages. 1",
        "id":258959201
      },
      {
        "title":"Unsupervised Event Coreference Resolution with Rich Linguistic Features",
        "text":"This paper examines how a new class of nonparametric Bayesian models can be effectively applied to an open-domain event coreference task. Designed with the purpose of clustering complex linguistic objects, these models consider a potentially infinite number of features and categorical outcomes. The evaluation performed for solving both within-and cross-document event coreference shows significant improvements of the models when compared against two baselines for this task.",
        "id":739867
      },
      {
        "title":"Structural Ambiguity and Conceptual Relations",
        "text":"Lexical co-occurrence statistics are becoming widely used in the syntactic analysis of unconstrained text. However, analyses based solely on lexical relationships suffer from sparseness of data: it is sometimes necessary to use a less informed model in order to reliably estimate statistical parameters. For example, the \"lexical association\" strategy for resolving ambiguous prepositional phrase attachments[Hindle and Rooth. 1991]takes into account only the attachment site (a verb or its direct object) and the preposition, ignoring the object of the preposition.We investigated an extension of the lexical association strategy to make use of noun class information, thus permitting a disambiguation strategy to take more information into account. Although in preliminary experiments the extended strategy did not yield improved performance over lexical association alone. a qualitative analysis of the results suggests that the problem lies not in the noun class information, but rather in the multiplicity of classes available for each noun in the absence of sense disambiguation. This suggests several possible revisions of our proposal.",
        "id":16000644
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Could you provide me with a reference that discusses the development of classifiers for suicide risk detection in a low-resource language, with a specific focus on using explicit suicide-related terminology?",
    "positive_ctxs":[
      {
        "title":"Detecting Suicide Risk in Online Counseling Services: A Study in a Low-Resource Language",
        "text":"With the increased awareness of situations of mental crisis and their societal impact, online services providing emergency support are becoming commonplace in many countries. Computational models, trained on discussions between help-seekers and providers, can support suicide prevention by identifying at-risk individuals. However, the lack of domain-specific models, especially in low-resource languages, poses a significant challenge for the automatic detection of suicide risk. We propose a model that combines pre-trained language models (PLM) with a fixed set of manually crafted (and clinically approved) set of suicidal cues, followed by a two-stage fine-tuning process. Our model achieves 0.91 ROC-AUC and an F2-score of 0.55, significantly outperforming an array of strong baselines even early on in the conversation, which is critical for real-time detection in the field. Moreover, the model performs well across genders and age groups.",
        "id":252199533
      }
    ],
    "negative_ctxs":[
      {
        "title":"Relation Schema Induction using Tensor Factorization with Side Information",
        "text":"Given a set of documents from a specific domain (e.g., medical research journals), how do we automatically build a Knowledge Graph (KG) for that domain? Automatic identification of relations and their schemas, i.e., type signature of arguments of relations (e.g., undergo(Patient, Surgery)), is an important first step towards this goal. We refer to this problem as Relation Schema Induction (RSI). In this paper, we propose Schema Induction using Coupled Tensor Factorization (SICTF), a novel tensor factorization method for relation schema induction. SICTF factorizes Open Information Extraction (OpenIE) triples extracted from a domain corpus along with additional side information in a principled way to induce relation schemas. To the best of our knowledge, this is the first application of tensor factorization for the RSI problem. Through extensive experiments on multiple real-world datasets, we find that SICTF is not only more accurate than state-of-the-art baselines, but also significantly faster (about 14x faster).",
        "id":12051021
      },
      {
        "title":"",
        "text":"",
        "id":204793182
      },
      {
        "title":"Simple Semi-supervised Dependency Parsing",
        "text":"We present a simple and effective semisupervised method for training dependency parsers. We focus on the problem of lexical representation, introducing features that incorporate word clusters derived from a large unannotated corpus. We demonstrate the effectiveness of the approach in a series of dependency parsing experiments on the Penn Treebank and Prague Dependency Treebank, and we show that the cluster-based features yield substantial gains in performance across a wide range of conditions. For example, in the case of English unlabeled second-order parsing, we improve from a baseline accuracy of 92.02% to 93.16%, and in the case of Czech unlabeled second-order parsing, we improve from a baseline accuracy of 86.13% to 87.13%. In addition, we demonstrate that our method also improves performance when small amounts of training data are available, and can roughly halve the amount of supervised data required to reach a desired level of performance.",
        "id":1916754
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Could you recommend datasets that include SQL annotations over WikiTQ?",
    "positive_ctxs":[
      {
        "title":"On the Potential of Lexico-logical Alignments for Semantic Parsing to SQL Queries",
        "text":"Large-scale semantic parsing datasets annotated with logical forms have enabled major advances in supervised approaches. But can richer supervision help even more? To explore the utility of fine-grained, lexical-level supervision, we introduce SQUALL, a dataset that enriches 11,276 WIKITABLEQUESTIONS English-language questions with manually created SQL equivalents plus alignments between SQL and question fragments. Our annotation enables new training possibilities for encoderdecoder models, including approaches from machine translation previously precluded by the absence of alignments. We propose and test two methods: (1) supervised attention;(2) adopting an auxiliary objective of disambiguating references in the input queries to table columns. In 5-fold cross validation, these strategies improve over strong baselines by 4.4% execution accuracy. Oracle experiments suggest that annotated alignments can support further accuracy gains of up to 23.9%.",
        "id":225039884
      }
    ],
    "negative_ctxs":[
      {
        "title":"Open Data Vocabularies for Assigning Usage Rights to Translation Memories",
        "text":"An assessment of the intellectual property requirements for data used in machine-aided translation is provided based on a recent EC-funded legal review. This is compared against the capabilities offered by current linked open data standards from the W3C for publishing and sharing translation memories from translation projects, and proposals for adequately addressing the intellectual property needs of stakeholders in translation projects using open data vocabularies are suggested.",
        "id":37536762
      },
      {
        "title":"臺灣口音中英雙語之多語者影音合成系統 Taiwanese-Accented Mandarin and English Multi-Speaker Talking-Face Synthesis System",
        "text":"This paper proposes a multi-speaker talking-face synthesis system. The system incorporates voice cloning and lipsyncing technology to achieve text-totalking-face generation by acquiring audio and video clips of any speaker and using zero-shot transfer learning. In addition, we used open-source corpora to train several Taiwanese-accented models and proposed using Mandarin Phonetic Symbols (Bopomofo) as the character embedding of the synthesizer to improve the system's ability to synthesize Chinese-English codeswitched sentences. Through our system, users can create rich applications. Also, the research on this technology is novel in the audiovisual speech synthesis field.關鍵字：多語者語音合成、語者驗證、語音 複製、語碼轉換、嘴型同步、人物說話影像",
        "id":253628242
      },
      {
        "title":"Integrating a Large-scale, Reusable Lexicon with a Natural Language Generator",
        "text":"This paper presents the integration of a largescale, reusable lexicon for generation with the FUF\/SURGE unification-based syntactic realizer.The lexicon was combined from multiple existing resources in a semi-automatic process. The integration is a multi-step unification process. This integration allows the reuse of lexical, syntactic, and semantic knowledge encoded in the lexicon in the development of lexical chooser module in a generation system. The lexicon also brings other benefits to a generation system: for example, the ability to generate many lexical and syntactic paraphrases and the ability to avoid non-grammatical output.",
        "id":1891268
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Could you recommend studies that provide a baseline for experiments using supervised constituency parsers with a focus on few-shot learning settings and have also reported on the use of pre-training and data augmentation techniques for parser performance improvement?",
    "positive_ctxs":[
      {
        "title":"On the Role of Supervision in Unsupervised Constituency Parsing",
        "text":"We analyze several recent unsupervised constituency parsing models, which are tuned with respect to the parsing F 1 score on the Wall Street Journal (WSJ) development set (1,700 sentences). We introduce strong baselines for them, by training an existing supervised parsing model (Kitaev and Klein, 2018) on the same labeled examples they access. When training on the 1,700 examples, or even when using only 50 examples for training and 5 for development, such a few-shot parsing approach can outperform all the unsupervised parsing methods by a significant margin. Fewshot parsing can be further improved by a simple data augmentation method and selftraining. This suggests that, in order to arrive at fair conclusions, we should carefully consider the amount of labeled data used for model development. We propose two protocols for future work on unsupervised parsing: (i) use fully unsupervised criteria for hyperparameter tuning and model selection; (ii) use as few labeled examples as possible for model development, and compare to few-shot parsing trained on the same labeled examples. 1",
        "id":222140735
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":227230629
      },
      {
        "title":"A Knowledge-based Approach to Text Classification",
        "text":"The paper presents a simple and effective knowledge-based approach for the task of text classification. The approach uses topic identification algorithm named FIFA to text classification. In this paper the basic process of text classification task and FIFA algorithm are described in detail. At last some results of experiment and evaluations are discussed.",
        "id":8700588
      },
      {
        "title":"Le projet BabyTalk : génération de texte à partir de données hétérogènes pour la prise de décision en unité néonatale",
        "text":"Notre société génère une masse d'information toujours croissante, que ce soit en médecine, en météorologie, etc. La méthode la plus employée pour analyser ces données est de les résumer sous forme graphique. Cependant, il a été démontré qu'un résumé textuel est aussi un mode de présentation efficace. L'objectif du prototype BT-45, développé dans le cadre du projet Babytalk, est de générer des résumés de 45 minutes de signaux physiologiques continus et d'événements temporels discrets en unité néonatale de soins intensifs (NICU). L'article présente l'aspect génération de texte de ce prototype. Une expérimentation clinique a montré que les résumés humains améliorent la prise de décision par rapport à l'approche graphique, tandis que les textes de BT-45 donnent des résultats similaires à l'approche graphique. Une analyse a identifié certaines des limitations de BT-45 mais en dépit de cellesci, notre travail montre qu'il est possible de produire automatiquement des résumés textuels efficaces de données complexes.Abstract Nowadays large amount of data is produced every day in medicine, meteorology and other areas and the most common approach to analyse such data is to present it graphically. However, it has been shown that textual summarisation is also an effective approach. As part of the BabyTalk project, the prototype BT-45 was developed to generate summaries of 45 minutes of continuous physiological signals and discrete temporal events in a neonatal intensive care unit (NICU). The paper presents its architecture with an emphasis on its natural language generation part. A clinical experiment showed that human textual summaries led to better decision making than graphical presentation, whereas BT-45 texts led to similar results as visualisations. An analysis identified some of the reasons for the BT-45 texts inferiority, but, despite these deficiencies, our work shows that it is possible for computer systems to generate effective textual summaries of complex data.Mots-clés : Traitement automatique des langues naturelles ; Génération de texte ; Analyse de données ; Unité de soins intensifs ; Systèmes d'aide à la décision",
        "id":69085348
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Could you suggest a paper that introduces an approach to relation extraction that involves learning syntax dependency structures using a tree LSTM model?",
    "positive_ctxs":[
      {
        "title":"Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks",
        "text":"Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).",
        "id":3033526
      }
    ],
    "negative_ctxs":[
      {
        "title":"Logic Against Bias: Textual Entailment Mitigates Stereotypical Sentence Reasoning",
        "text":"Due to their similarity-based learning objectives, pretrained sentence encoders often internalize stereotypical assumptions that reflect the social biases that exist within their training corpora. In this paper, we describe several kinds of stereotypes concerning different communities that are present in popular sentence representation models, including pretrained next sentence prediction and contrastive sentence representation models. We compare such models to textual entailment models that learn language logic for a variety of downstream language understanding tasks. By comparing strong pretrained models based on text similarity with textual entailment learning, we conclude that the explicit logic learning with textual entailment can significantly reduce bias and improve the recognition of social communities, without an explicit de-biasing process. The code, model, and data associated with this work are publicly available at https: \/\/github.com\/luohongyin\/ESP.git.",
        "id":257482682
      },
      {
        "title":"Improving Variational Autoencoder for Text Modelling with Timestep-Wise Regularisation",
        "text":"The Variational Autoencoder (VAE) is a popular and powerful model applied to text modelling to generate diverse sentences. However, an issue known as posterior collapse (or KL loss vanishing) happens when the VAE is used in text modelling, where the approximate posterior collapses to the prior, and the model will totally ignore the latent variables and be degraded to a plain language model during text generation. Such an issue is particularly prevalent when RNN-based VAE models are employed for text modelling. In this paper, we propose a simple, generic architecture called Timestep-Wise Regularisation VAE (TWR-VAE), which can effectively avoid posterior collapse and can be applied to any RNN-based VAE models. The effectiveness and versatility of our model are demonstrated in different tasks, including language modelling and dialogue response generation. * Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:\/\/creativecommons. org\/licenses\/by\/4.0\/.",
        "id":226236766
      },
      {
        "title":"Optimizing Features in Active Machine Learning for Complex Qualitative Content Analysis",
        "text":"We propose a semi-automatic approach for content analysis that leverages machine learning (ML) being initially trained on a small set of hand-coded data to perform a first pass in coding, and then have human annotators correct machine annotations in order to produce more examples to retrain the existing model incrementally for better performance. In this \"active learning\" approach, it is equally important to optimize the creation of the initial ML model given less training data so that the model is able to capture most if not all positive examples, and filter out as many negative examples as possible for human annotators to correct. This paper reports our attempt to optimize the initial ML model through feature exploration in a complex content analysis project that uses a multidimensional coding scheme, and contains codes with sparse positive examples. While different codes respond optimally to different combinations of features, we show that it is possible to create an optimal initial ML model using only a single combination of features for codes with at least 100 positive examples in the gold standard corpus.",
        "id":11104974
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"Could you suggest studies that employ novel methods for capturing data, specifically in the context of sarcasm detection on social media platforms like Twitter?",
    "positive_ctxs":[
      {
        "title":"Reactive Supervision: A New Method for Collecting Sarcasm Data",
        "text":"Sarcasm detection is an important task in affective computing, requiring large amounts of labeled data. We introduce reactive supervision, a novel data collection method that utilizes the dynamics of online conversations to overcome the limitations of existing data collection techniques. We use the new method to create and release a first-of-its-kind large dataset of tweets with sarcasm perspective labels and new contextual features. The dataset is expected to advance sarcasm detection research. Our method can be adapted to other affective computing domains, thus opening up new research opportunities.",
        "id":221970454
      }
    ],
    "negative_ctxs":[
      {
        "title":"Published as a conference paper at ICLR 2021 LEARNING TO RECOMBINE AND RESAMPLE DATA FOR COMPOSITIONAL GENERALIZATION",
        "text":"Flexible neural sequence models outperform grammar-and automaton-based counterparts on a variety of tasks. However, neural models perform poorly in settings requiring compositional generalization beyond the training data-particularly to rare or unseen subsequences. Past work has found symbolic scaffolding (e.g. grammars or automata) essential in these settings. We describe R&R, a learned data augmentation scheme that enables a large category of compositional generalizations without appeal to latent symbolic structure. R&R has two components: recombination of original training examples via a prototype-based generative model and resampling of generated examples to encourage extrapolation. Training an ordinary neural sequence model on a dataset augmented with recombined and resampled examples significantly improves generalization in two language processing problems-instruction following (SCAN) and morphological analysis (SIGMORPHON 2018)-where R&R enables learning of new constructions and tenses from as few as eight initial examples. . Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In",
        "id":222208634
      },
      {
        "title":"Improving Coverage of an Inuktitut Morphological Analyzer Using a Segmental Recurrent Neural Network",
        "text":"Languages such as Inuktitut are particularly challenging for natural language processing because of polysynthesis, abundance of grammatical features represented via morphology, morphophonemics, dialect variation, and noisy data. We make use of an existing morphological analyzer, the Uqailaut analyzer, and a dataset, the Nunavut Hansards, and experiment with improving the analyzer via bootstrapping of a segmental recurrent neural network onto it. We present results of the accuracy of this approach which works better for a coarse-grained analysis than a fine-grained analysis. We also report on accuracy of just the \"closed-class\" suffix parts of the Inuktitut words, which are better than the overall accuracy on the full words.",
        "id":7806031
      },
      {
        "title":"",
        "text":"",
        "id":218973787
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Has there been any recent work or competitions focused on the development of methods to counteract clickbait through spoiling, such as revealing key information upfront?",
    "positive_ctxs":[
      {
        "title":"SemEval-2023 Task 5: Clickbait Spoiling",
        "text":"In this overview paper, we report on the second PAN Clickbait Challenge hosted as Task 5 at SemEval 2023. The challenge's focus is to better support social media users by automatically generating short spoilers that close the curiosity gap induced by a clickbait post. We organized two subtasks: (1) spoiler type classification to assess what kind of spoiler a clickbait post warrants (e.g., a phrase), and (2) spoiler generation to generate an actual spoiler for a clickbait post. 2 github.com\/pan-webis-de\/SEMEVAL-2023 2275",
        "id":259376639
      }
    ],
    "negative_ctxs":[
      {
        "title":"An Evaluation Exercise for Romanian Word Sense Disambiguation",
        "text":"This paper presents the task definition, resources, participating systems, and comparative results for a Romanian Word Sense Disambiguation task, which was organized as part of the SENSEVAL-3 evaluation exercise. Five teams with a total of seven systems were drawn to this task.",
        "id":752448
      },
      {
        "title":"Using J-K-fold Cross Validation to Reduce Variance When Tuning NLP Models",
        "text":"K-fold cross validation (CV) is a popular method for estimating the true performance of machine learning models, allowing model selection and parameter tuning. However, the very process of CV requires random partitioning of the data and so our performance estimates are in fact stochastic, with variability that can be substantial for natural language processing tasks. We demonstrate that these unstable estimates cannot be relied upon for effective parameter tuning. The resulting tuned parameters are highly sensitive to how our data is partitioned, meaning that we often select sub-optimal parameter choices and have serious reproducibility issues.Instead, we propose to use the less variable J-K-fold CV, in which J independent K-fold cross validations are used to assess performance. Our main contributions are extending J-K-fold CV from performance estimation to parameter tuning and investigating how to choose J and K. We argue that variability is more important than bias for effective tuning and so advocate lower choices of K than are typically seen in the NLP literature, instead use the saved computation to increase J. To demonstrate the generality of our recommendations we investigate a wide range of case-studies: sentiment classification (both general and target-specific), part-of-speech tagging and document classification.",
        "id":49311178
      },
      {
        "title":"The SuperARV Language Model: Investigating the Effectiveness of Tightly Integrating Multiple Knowledge Sources",
        "text":"A new almost-parsing language model incorporating multiple knowledge sources that is based upon the concept of Constraint Dependency Grammars is presented in this paper. Lexical features and syntactic constraints are tightly integrated into a uniform linguistic structure called a SuperARV that is associated with a word in the lexicon. The Super-ARV language model reduces perplexity and word error rate compared to trigram, part-of-speech-based, and parser-based language models. The relative contributions of the various knowledge sources to the strength of our model are also investigated by using constraint relaxation at the level of the knowledge sources. We have found that although each knowledge source contributes to language model quality, lexical features are an outstanding contributor when they are tightly integrated with word identity and syntactic constraints. Our investigation also suggests possible reasons for the reported poor performance of several probabilistic dependency grammar models in the literature.Johnson(2001)andLafferty et al. (2001)provide insight into why a joint model is superior to a conditional model.Recently, there has been good progress in developing structured models (Chelba, 2000; Charniak, Association for Computational Linguistics.",
        "id":5820758
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"I am exploring state-of-the-art techniques in language representation models that are trained to understand context from both the preceding and succeeding text. Where can I find foundational research on this topic, including information about the Transformer architecture, and the specific tasks such models are pre-trained on?",
    "positive_ctxs":[
      {
        "title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "text":"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
        "id":52967399
      }
    ],
    "negative_ctxs":[
      {
        "title":"Fast Yet Rich Morphological Analysis",
        "text":"Implementations of models of morphologically rich languages such as Arabic typically achieve speed and small memory footprint at the cost of abandoning linguistically abstract and elegant representations. We present a solution to modeling rich morphologies that is both fast and based on linguistically rich representations. In our approach, we convert a linguistically complex and abstract implementation of Arabic verbs in finite-state machinery into a simple precompiled tabular representation.",
        "id":11301067
      },
      {
        "title":"Polly Want a Cracker: Analyzing Performance of Parroting on Paraphrase Generation Datasets",
        "text":"Paraphrase generation is an interesting and challenging NLP task which has numerous practical applications. In this paper, we analyze datasets commonly used for paraphrase generation research, and show that simply parroting input sentences surpasses state-of-theart models in the literature when evaluated on standard metrics. Our findings illustrate that a model could be seemingly adept at generating paraphrases, despite only making trivial changes to the input sentence or even none at all.",
        "id":201126736
      },
      {
        "title":"FASTIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging",
        "text":"Influence functions approximate the \"influences\" of training data-points for test predictions and have a wide variety of applications. Despite the popularity, their computational cost does not scale well with model and training data size. We present FASTIF, a set of simple modifications to influence functions that significantly improves their run-time. We use k-Nearest Neighbors (kNN) to narrow the search space down to a subset of good candidate data points, identify the configurations that best balance the speed-quality trade-off in estimating the inverse Hessian-vector product, and introduce a fast parallel variant. Our proposed method achieves about 80X speedup while being highly correlated with the original influence values. With the availability of the fast influence functions, we demonstrate their usefulness in four applications. First, we examine whether influential data-points can \"explain\" test time behavior using the framework of simulatability. Second, we visualize the influence interactions between training and test data-points. Third, we show that we can correct model errors by additional fine-tuning on certain influential data-points, improving the accuracy of a trained MultiNLI model by 2.5% on the HANS dataset. Finally, we experiment with a similar setup but fine-tuning on datapoints not seen during training, improving the model accuracy by 2.8% and 1.7% on HANS and ANLI datasets respectively. Overall, our fast influence functions can be efficiently applied to large models and datasets, and our experiments demonstrate the potential of influence functions in model interpretation and correcting model errors. 1",
        "id":229923196
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"I am looking for research that has explored topic and frame conditioning in transformer language models to enhance the quality of generated argument claims. Is there a paper discussing this approach?",
    "positive_ctxs":[
      {
        "title":"Aspect-Controlled Neural Argument Generation",
        "text":"We rely on arguments in our daily lives to deliver our opinions and base them on evidence, making them more convincing in turn. However, finding and formulating arguments can be challenging. In this work, we present the Arg-CTRL-a language model for argument generation that can be controlled to generate sentence-level arguments for a given topic, stance, and aspect. We define argument aspect detection as a necessary method to allow this fine-granular control and crowdsource a dataset with 5,032 arguments annotated with aspects. Our evaluation shows that the Arg-CTRL is able to generate high-quality, aspectspecific arguments, applicable to automatic counter-argument generation. We publish the model weights and all datasets and code to train the Arg-CTRL. 1",
        "id":218470025
      }
    ],
    "negative_ctxs":[
      {
        "title":"Clinical Outcome Prediction from Admission Notes using Self-Supervised Knowledge Integration",
        "text":"Outcome prediction from clinical text can prevent doctors from overlooking possible risks and help hospitals to plan capacities. We simulate patients at admission time, when decision support can be especially valuable, and contribute a novel admission to discharge task with four common outcome prediction targets: Diagnoses at discharge, procedures performed, in-hospital mortality and length-of-stay prediction. The ideal system should infer outcomes based on symptoms, pre-conditions and risk factors of a patient. We evaluate the effectiveness of language models to handle this scenario and propose clinical outcome pretraining to integrate knowledge about patient outcomes from multiple public sources. We further present a simple method to incorporate ICD code hierarchy into the models. We show that our approach improves performance on the outcome tasks against several baselines. A detailed analysis reveals further strengths of the model, including transferability, but also weaknesses such as handling of vital values and inconsistencies in the underlying data.",
        "id":231846970
      },
      {
        "title":"Multi-layered Annotation of Conversation-like Narratives in German",
        "text":"This work presents two corpora based on excerpts from two German novels with an informal narration style. We performed fine-grained multi-layer annotations of animate referents, assigning local and global prominence-lending features to the annotated referring expressions. In addition, our corpora include annotations of intra-sentential segments, which can serve as a more reliable unit of length measurement. Furthermore, we present two exemplary studies demonstrating how to use these corpora.",
        "id":259376874
      },
      {
        "title":"Fusion of linguistic, neural and sentence-transformer features for improved term alignment",
        "text":"Crosslingual terminology alignment task has many practical applications. In this work, we propose an aligning method for the shared task of the 15th Workshop on Building and Using Comparable Corpora. Our method combines several different approaches into one cohesive machine learning model, based on SVM. From shared-task specific and external sources, we crafted four types of features: cognate-based, dictionary-based, embedding-based, and combined features, which combine aspects of the other three types. We added a post-processing re-scoring method, which reduces the effect of hubness, where some terms are nearest neighbours of many other terms. We achieved the average precision score of 0.833 on the English-French training set of the shared task.",
        "id":252624481
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"I am looking to understand more about sequence-to-sequence pre-training and its applications in natural language tasks. Can you suggest a significant paper that describes the denoising process for such models?",
    "positive_ctxs":[
      {
        "title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
        "text":"We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance. 1",
        "id":204960716
      }
    ],
    "negative_ctxs":[
      {
        "title":"A Geometric View on Bilingual Lexicon Extraction from Comparable Corpora",
        "text":"We present a geometric view on bilingual lexicon extraction from comparable corpora, which allows to re-interpret the methods proposed so far and identify unresolved problems. This motivates three new methods that aim at solving these problems. Empirical evaluation shows the strengths and weaknesses of these methods, as well as a significant gain in the accuracy of extracted lexicons.",
        "id":459519
      },
      {
        "title":"UC3M-PUCPR at SemEval-2022 Task 11: An Ensemble Method of Transformer-based Models for Complex Named Entity Recognition",
        "text":"This study introduces the system submitted to the SemEval 2022 Task 11: MultiCoNER (Multilingual Complex Named Entity Recognition) by the UC3M-PUCPR team. We proposed an ensemble of transformer-based models for entity recognition in cross-domain texts. Our deep learning method benefits from the transformer architecture, which adopts the attention mechanism to handle the long-range dependencies of the input text. Also, the ensemble approach for named entity recognition (NER) improved the results over baselines based on individual models on two of the three tracks we participated in. The ensemble model for the codemixed task achieves an overall performance of 76.36% F1-score, a 2.85 percentage point increase upon our individually best model for this task, XLM-RoBERTa-large (73.51%), outperforming the baseline provided for the shared task by 18.26 points. Our preliminary results suggest that contextualized language models ensembles can, even if modestly, improve the results in extracting information from unstructured data.",
        "id":250391088
      },
      {
        "title":"",
        "text":"",
        "id":232021830
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"I would like to understand the theoretical basis for using the nuclear norm of a weight matrix as a measure of complexity in linear models for probing tasks. Which paper should I refer to?",
    "positive_ctxs":[
      {
        "title":"Information-Theoretic Probing for Linguistic Structure",
        "text":"The success of neural networks on a diverse set of NLP tasks has led researchers to question how much do these networks actually know about natural language. Probes are a natural way of assessing this. When probing, a researcher chooses a linguistic task and trains a supervised model to predict annotation in that linguistic task from the network's learned representations. If the probe does well, the researcher may conclude that the representations encode knowledge related to the task. A commonly held belief is that using simpler models as probes is better; the logic is that such models will identify linguistic structure, but not learn the task itself. We propose an information-theoretic formalization of probing as estimating mutual information that contradicts this received wisdom: one should always select the highest performing probe one can, even if it is more complex, since it will result in a tighter estimate. The empirical portion of our paper focuses on obtaining tight estimates for how much information BERT knows about parts of speech in a set of five typologically diverse languages that are often underrepresented in parsing research, plus English, totaling six languages. We find BERT accounts for only at most 5% more information than traditional, type-based word embeddings.",
        "id":215238965
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":164835508
      },
      {
        "title":"Enhanced Multi-Channel Graph Convolutional Network for Aspect Sentiment Triplet Extraction",
        "text":"Aspect Sentiment Triplet Extraction (ASTE) is an emerging sentiment analysis task. Most of the existing studies focus on devising a new tagging scheme that enables the model to extract the sentiment triplets in an end-to-end fashion. However, these methods ignore the relations between words for ASTE task. In this paper, we propose an Enhanced Multi-Channel Graph Convolutional Network model (EMC-GCN) to fully utilize the relations between words. Specifically, we first define ten types of relations for ASTE task, and then adopt a biaffine attention module to embed these relations as an adjacent tensor between words in a sentence. After that, our EMC-GCN transforms the sentence into a multi-channel graph by treating words and the relation adjacent tensor as nodes and edges, respectively. Thus, relationaware node representations can be learnt. Furthermore, we consider diverse linguistic features to enhance our EMC-GCN model. Finally, we design an effective refining strategy on EMC-GCN for word-pair representation refinement, which considers the implicit results of aspect and opinion extraction when determining whether word pairs match or not. Extensive experimental results on the benchmark datasets demonstrate that the effectiveness and robustness of our proposed model, which outperforms state-of-the-art methods significantly.",
        "id":248780173
      },
      {
        "title":"An annotation scheme and Gold Standard for Dutch-English word alignment",
        "text":"The importance of sentence-aligned parallel corpora has been widely acknowledged. Reference corpora in which sub-sentential translational correspondences are indicated manually are more labour-intensive to create, and hence less wide-spread. Such manually created reference alignments -also called Gold Standards -have been used in research projects to develop or test automatic word alignment systems. In most translations, translational correspondences are rather complex; for example word-by-word correspondences can be found only for a limited number of words. A reference corpus in which those complex translational correspondences are aligned manually is therefore also a useful resource for the development of translation tools and for translation studies. In this paper, we describe how we created a Gold Standard for the Dutch-English language pair. We present the annotation scheme, annotation guidelines, annotation tool and inter-annotator results. To cover a wide range of syntactic and stylistic phenomena that emerge from different writing and translation styles, our Gold Standard data set contains texts from different text types. The Gold Standard will be publicly available as part of the Dutch Parallel Corpus.",
        "id":9828572
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"I'm conducting research on computational humor and looking at various approaches to detect it within texts. What are some articles that explore features like repetition or use language models like GPT-2 for humor recognition?",
    "positive_ctxs":[
      {
        "title":"Humor Recognition and Humor Anchor Extraction",
        "text":"Humor is an essential component in personal communication. How to create computational models to discover the structures behind humor, recognize humor and even extract humor anchors remains a challenge.In this work, we first identify several semantic structures behind humor and design sets of features for each structure, and next employ a computational approach to recognize humor. Furthermore, we develop a simple and effective method to extract anchors that enable humor in a sentence. Experiments conducted on two datasets demonstrate that our humor recognizer is effective in automatically distinguishing between humorous and non-humorous texts and our extracted humor anchors correlate quite well with human annotations.",
        "id":11128248
      },
      {
        "title":"Uncertainty and Surprisal Jointly Deliver the Punchline: Exploiting Incongruity-Based Features for Humor Recognition",
        "text":"Humor recognition has been widely studied as a text classification problem using data-driven approaches. However, most existing work does not examine the actual joke mechanism to understand humor. We break down any joke into two distinct components: the set-up and the punchline, and further explore the special relationship between them. Inspired by the incongruity theory of humor, we model the setup as the part developing semantic uncertainty, and the punchline disrupting audience expectations. With increasingly powerful language models, we were able to feed the set-up along with the punchline into the GPT-2 language model, and calculate the uncertainty and surprisal values of the jokes. By conducting experiments on the SemEval 2021 Task 7 dataset, we found that these two features have better capabilities of telling jokes from non-jokes, compared with existing baselines.",
        "id":229349316
      }
    ],
    "negative_ctxs":[
      {
        "title":"Harvesting Multi-Word Expressions from Parallel Corpora",
        "text":"The paper presents a set of approaches to extend the automatically created Slovene wordnet with nominal multiword expressions. In the first approach multiword expressions from Princeton WordNet are translated with a technique that is based on wordalignment and lexicosyntactic patterns. This is followed by extracting new terms from a monolingual corpus using keywordness ranking and contextual patterns. Finally, the multiword expressions are assigned a hypernym and added to our wordnet. Manual evaluation and comparison of the results shows that the translation approach is the most straightforward and accurate. However, it is successfully complemented by the two monolingual approaches which are able to identify more term candidates in the corpus that would otherwise go unnoticed. Some weaknesses of the proposed wordnet extension techniques are also addressed.",
        "id":12731859
      },
      {
        "title":"Sinica-IASL Chinese Spelling Check System at SIGHAN-7",
        "text":"We developed a Chinese spelling check system for error detection and error correction subtasks in the 2013 SIGHAN-7 Chinese Spelling Check Bake-off. By using the resources of Chinese phonology and orthographic components, our system contains four parts: high confidence pattern matcher, the detection module, the correction module, and the merger. We submitted 2 official runs for both subtasks. The evaluation result show that our system achieved 0.6016 in error detection F-score of subtask 1, and 0.448 in correction accuracy of subtask 2. 1",
        "id":11928977
      },
      {
        "title":"Transliteration and Alignment of Parallel Texts from Cyrillic to Latin",
        "text":"This article describes a methodology of recovering and preservation of old Romanian texts and problems related to their recognition. Our focus is to create a gold corpus for Romanian language (the novella Sania), for both alphabets used in Transnistria -Cyrillic and Latin. The resource is available for similar researches. This technology is based on transliteration and semiautomatic alignment of parallel texts at the level of letter\/lexem\/multiwords. We have analysed every text segment present in this corpus and discovered other conventions of writing at the level of transliteration, academic norms and editorial interventions. These conventions allowed us to elaborate and implement some new heuristics that make a correct automatic transliteration process. Sometimes the words of Latin script are modified in Cyrillic script from semantic reasons (for instance, editor's interpretation). Semantic transliteration is seen as a good practice in introducing multiwords from Cyrillic to Latin. Not only does it preserve how a multiwords sound in the source script, but also enables the translator to modify in the original text (here, choosing the most common sense of an expression). Such a technology could be of interest to lexicographers, but also to specialists in computational linguistics to improve the actual transliteration standards.",
        "id":10202628
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"I'm exploring efficient transformer architectures for language embeddings and came across some work that utilizes advanced pre-trained models. Which paper should I reference to learn more about the use of XLM-R for multilingual representation learning in a transformer-based setting?",
    "positive_ctxs":[
      {
        "title":"Unsupervised Cross-lingual Representation Learning at Scale",
        "text":"This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of crosslingual transfer tasks. We train a Transformerbased masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing perlanguage performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available. 1",
        "id":207880568
      }
    ],
    "negative_ctxs":[
      {
        "title":"LAGRANGIAN NEURAL NETWORKS",
        "text":"Accurate models of the world are built upon notions of its underlying symmetries. In physics, these symmetries correspond to conservation laws, such as for energy and momentum. Yet even though neural network models see increasing use in the physical sciences, they struggle to learn these symmetries. In this paper, we propose Lagrangian Neural Networks (LNNs), which can parameterize arbitrary Lagrangians using neural networks. In contrast to models that learn Hamiltonians, LNNs do not require canonical coordinates, and thus perform well in situations where canonical momenta are unknown or difficult to compute. Unlike previous approaches, our method does not restrict the functional form of learned energies and will produce energy-conserving models for a variety of tasks. We test our approach on a double pendulum and a relativistic particle, demonstrating energy conservation where a baseline approach incurs dissipation and modeling relativity without canonical coordinates where a Hamiltonian approach fails. Finally, we show how this model can be applied to graphs and continuous systems using a Lagrangian Graph Network, and demonstrate it on the 1D wave equation. * Also affiliated with Princeton University",
        "id":212644628
      },
      {
        "title":"《全唐詩》的分析、探勘與應用-風格、對仗、社會網路與對聯 Textual Analysis of Complete Tang Poems for Discoveries and Applications -Style, Antitheses, Social Networks, and Couplets",
        "text":"1The Complete Tang Poems (CTP) is the most important collection for studying Tang poetry, which in turn is arguably a very influential part of the Chinese literature. Our analyzing the CTP from the perspectives of antithesis 2 , collocation and distributional semantics offers some interesting overviews of the styles and imageries embedded in the works of some representative Tang poets. Our analyses include (1) a quantitative comparison of the uses of \"wind\" and \"moon\" in Li Bai's and Du Fu's works and (2) the functions of colors in Tang poems. In particular, we explored the appearances of \"white\" color, which is the most frequent color in Tang poems. Colors in static poems are like audios in motion pictures, so we thought the analyses could lead us to an important facet of the poems. In addition, we extracted social networks of poets from the poems, and built a simple couplet suggestion kit based on the textual analysis of the poems.關鍵詞：數位人文、中國文學、全唐詩、詞彙語意、共現分析、文本分析、語料庫 分析、中國歷代人物傳記資料庫",
        "id":12980287
      },
      {
        "title":"Generalised Unsupervised Domain Adaptation of Neural Machine Translation with Cross-Lingual Data Selection",
        "text":"This paper considers the unsupervised domain adaptation problem for neural machine translation (NMT), where we assume the access to only monolingual text in either the source or target language in the new domain. We propose a cross-lingual data selection method to extract in-domain sentences in the missing language side from a large generic monolingual corpus. Our proposed method trains an adaptive layer on top of multilingual BERT by contrastive learning to align the representation between the source and target language. This then enables the transferability of the domain classifier between the languages in a zero-shot manner. Once the in-domain data is detected by the classifier, the NMT model is then adapted to the new domain by jointly learning translation and domain discrimination tasks. We evaluate our cross-lingual data selection method on NMT across five diverse domains in three language pairs, as well as a real-world scenario of translation for COVID-19. The results show that our proposed method outperforms other selection baselines up to +1.5 BLEU score.",
        "id":237453654
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"I'm exploring research that utilizes large datasets for the task of sentence simplification. Are there any prominent datasets sourced from Wikipedia that I could look into?",
    "positive_ctxs":[
      {
        "title":"Sentence Simplification with Deep Reinforcement Learning",
        "text":"Sentence simplification aims to make sentences easier to read and understand. Most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simple sentences. We address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework. Our model, which we call DRESS (as shorthand for Deep REinforcement Sentence Simplification), explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple, fluent, and preserve the meaning of the input. Experiments on three datasets demonstrate that our model outperforms competitive simplification systems. 1",
        "id":7473831
      }
    ],
    "negative_ctxs":[
      {
        "title":"IMBALANCED SEMI-SUPERVISED LEARNING WITH BIAS ADAPTIVE CLASSIFIER",
        "text":"Pseudo-labeling has proven to be a promising semi-supervised learning (SSL) paradigm. Existing pseudo-labeling methods commonly assume that the class distributions of training data are balanced. However, such an assumption is far from realistic scenarios and thus severely limits the performance of current pseudolabeling methods under the context of class-imbalance. To alleviate this problem, we design a bias adaptive classifier that targets the imbalanced SSL setups. The core idea is to automatically assimilate the training bias caused by class imbalance via the bias adaptive classifier, which is composed of a novel bias attractor and the original linear classifier. The bias attractor is designed as a light-weight residual network and optimized through a bi-level learning framework. Such a learning strategy enables the bias adaptive classifier to fit imbalanced training data, while the linear classifier can provide unbiased label prediction for each class. We conduct extensive experiments under various imbalanced semi-supervised setups, and the results demonstrate that our method can be applied to different pseudo-labeling models and is superior to current state-of-the-art methods. Raffel. Mixmatch: A holistic approach to semi-supervised learning. NeurIPS, 32, 2019b. Paula Branco, Luís Torgo, and Rita P Ribeiro. A survey of predictive modeling on imbalanced domains. ACM Computing Surveys (CSUR), 49(2):1-50, 2016. Mateusz Buda, Atsuto Maki, and Maciej A Mazurowski. A systematic study of the class imbalance problem in convolutional neural networks. -supervised learning (chapelle, o. et al., eds.; 2006)[book reviews]. IEEE Transactions on Neural Networks, 20(3):542-542, 2009. . Smote: synthetic minority over-sampling technique. . Model-agnostic meta-learning for fast adaptation of deep networks.",
        "id":257279756
      },
      {
        "title":"Cohesion, Entrainment and Task Success in Educational Dialog",
        "text":"Researchers often study dialog corpora to better understand what makes some dialogs more successful than others. In this talk I will examine the relationship between coherence\/entrainment and task success, in several types of educational dialog corpora: 1) one-on-one tutoring, where students use dialog to interact with a human tutor in the physics domain, 2) one-on-one tutoring, where students instead interact with a spoken dialog system, and 3) engineering design, where student teams engage in multi-party dialog to complete a group project. I will first introduce several corpus-based measures of both lexical and acousticprosodic dialog cohesion and entrainment, and extend them to handle multi-party conversations. I will then show that the amount of cohesion and\/or entrainment positively correlates with measures of educational task success in all of our corpora. Finally, I will discuss how we are using our findings to build better tutorial dialog systems.197",
        "id":1538910
      },
      {
        "title":"Improving Generation and Evaluation of Visual Stories via Semantic Consistency",
        "text":"Story visualization is an underexplored task that falls at the intersection of many important research directions in both computer vision and natural language processing. In this task, given a series of natural language captions which compose a story, an agent must generate a sequence of images that correspond to the captions. Prior work has introduced recurrent generative models which outperform text-to-image synthesis models on this task. However, there is room for improvement of generated images in terms of visual quality, coherence and relevance. We present a number of improvements to prior modeling approaches, including (1) the addition of a dual learning framework that utilizes video captioning to reinforce the semantic alignment between the story and generated images, (2) a copy-transform mechanism for sequentiallyconsistent story visualization, and (3) MARTbased transformers to model complex interactions between frames. We present ablation studies to demonstrate the effect of each of these techniques on the generative power of the model for both individual images as well as the entire narrative. Furthermore, due to the complexity and generative nature of the task, standard evaluation metrics do not accurately reflect performance. Therefore, we also provide an exploration of evaluation metrics for the model, focused on aspects of the generated frames such as the presence\/quality of generated characters, the relevance to captions, and the diversity of the generated images. We also present correlation experiments of our proposed automated metrics with human evaluations.",
        "id":235097275
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"I'm exploring ways to enhance question answering systems through domain adaptation. Could you point me towards research that specifically focuses on synthetic data generation for this purpose?",
    "positive_ctxs":[
      {
        "title":"End-to-End Synthetic Data Generation for Domain Adaptation of Question Answering Systems",
        "text":"We propose an end-to-end approach for synthetic QA data generation. Our model comprises a single transformer-based encoderdecoder network that is trained end-to-end to generate both answers and questions. In a nutshell, we feed a passage to the encoder and ask the decoder to generate a question and an answer token-by-token. The likelihood produced in the generation process is used as a filtering score, which avoids the need for a separate filtering model. Our generator is trained by finetuning a pretrained LM using maximum likelihood estimation. The experimental results indicate significant improvements in the domain adaptation of QA models outperforming current state-of-the-art methods. * *equal contribution. † Siamak Shakeri is currently with Google. The work was done when he was at AWS AI.",
        "id":222310116
      }
    ],
    "negative_ctxs":[
      {
        "title":"SESSION 11 -NATURAL LANGUAGE III",
        "text":"",
        "id":27270383
      },
      {
        "title":"Identifying Appropriate Support for Propositions in Online User Comments",
        "text":"The ability to analyze the adequacy of supporting information is necessary for determining the strength of an argument. 1 This is especially the case for online user comments, which often consist of arguments lacking proper substantiation and reasoning. Thus, we develop a framework for automatically classifying each proposition as UNVERIFIABLE, VERIFIABLE NON-EXPERIENTIAL, or VERIFIABLE EXPE-RIENTIAL 2 , where the appropriate type of support is reason, evidence, and optional evidence, respectively 3 . Once the existing support for propositions are identified, this classification can provide an estimate of how adequately the arguments have been supported. We build a goldstandard dataset of 9,476 sentences and clauses from 1,047 comments submitted to an eRulemaking platform and find that Support Vector Machine (SVM) classifiers trained with n-grams and additional features capturing the verifiability and experientiality exhibit statistically significant improvement over the unigram baseline, achieving a macro-averaged F 1 of 68.99%.",
        "id":14764893
      },
      {
        "title":"Published as a conference paper at ICLR 2023 EQUIVARIANT HYPERGRAPH DIFFUSION NEURAL OP- ERATORS",
        "text":"Hypergraph neural networks (HNNs) using neural networks to encode hypergraphs provide a promising way to model higher-order relations in data and further solve relevant prediction tasks built upon such higher-order relations. However, higher-order relations in practice contain complex patterns and are often highly irregular. So, it is often challenging to design an HNN that suffices to express those relations while keeping computational efficiency. Inspired by hypergraph diffusion algorithms, this work proposes a new HNN architecture named ED-HNN, which provably approximates any continuous equivariant hypergraph diffusion operators that can model a wide range of higher-order relations. ED-HNN can be implemented efficiently by combining star expansions of hypergraphs with standard message passing neural networks. ED-HNN further shows great superiority in processing heterophilic hypergraphs and constructing deep models. We evaluate ED-HNN for node classification on nine real-world hypergraph datasets. ED-HNN uniformly outperforms the best baselines over these nine datasets and achieves more than 2%↑ in prediction accuracy over four datasets therein. Our",
        "id":250526398
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"I'm interested in understanding how perplexity is utilized in identifying misinformation or fact-checking. Are there studies discussing this application of perplexity?",
    "positive_ctxs":[
      {
        "title":"Towards Few-Shot Fact-Checking via Perplexity",
        "text":"Few-shot learning has drawn researchers' attention to overcome the problem of data scarcity. Recently, large pre-trained language models have shown great performance in fewshot learning for various downstream tasks, such as question answering and machine translation. Nevertheless, little exploration has been made to achieve few-shot learning for the fact-checking task. However, fact-checking is an important problem, especially when the amount of information online is growing exponentially every day. In this paper, we propose a new way of utilizing the powerful transfer learning ability of a language model via a perplexity score. The most notable strength of our methodology lies in its capability in fewshot learning. With only two training samples, our methodology can already outperform the Major Class baseline by more than an absolute 10% on the F1-Macro metric across multiple datasets. Through experiments, we empirically verify the plausibility of the rather surprising usage of the perplexity score in the context of fact-checking and highlight the strength of our few-shot methodology by comparing it to strong fine-tuning-based baseline models. Moreover, we construct and publicly release two new fact-checking datasets related to",
        "id":232258000
      }
    ],
    "negative_ctxs":[
      {
        "title":"Dimensions of Abusive Language on Twitter",
        "text":"In this paper, we use a new categorical form of multidimensional register analysis to identify the main dimensions of functional linguistic variation in a corpus of abusive language, consisting of racist and sexist Tweets. By analysing the use of a wide variety of parts-ofspeech and grammatical constructions, as well as various features related to Twitter and computer-mediated communication, we discover three dimensions of linguistic variation in this corpus, which we interpret as being related to the degree of interactive, antagonistic and attitudinal language exhibited by individual Tweets. We then demonstrate that there is a significant functional difference between racist and sexist Tweets, with sexists Tweets tending to be more interactive and attitudinal than racist Tweets.",
        "id":39055707
      },
      {
        "title":"HCS at SemEval-2017 Task 5: Sentiment Detection in Business News Using Convolutional Neural Networks",
        "text":"Task 5 of SemEval-2017 involves finegrained sentiment analysis on financial microblogs and news. Our solution for determining the sentiment score extends an earlier convolutional neural network for sentiment analysis in several ways. We explicitly encode a focus on a particular company, we apply a data augmentation scheme, and use a larger data collection to complement the small training data provided by the task organizers. The best results were achieved by training a model on an external dataset and then tuning it using the provided training dataset.",
        "id":26958313
      },
      {
        "title":"Linear Context-Free Rewriting Systems and Deterministic Tree-Walking Transducers*",
        "text":"We show that the class of string languages generated by linear context-free rewriting systems is equal to the class of output languages of deterministic treewalking transducers. From equivalences that have previously been established we know that this class of languages is also equal to the string languages generated by context-free hypergraph grammars, multicomponent tree-adjoining grammars, and multiple contextfree grammars and to the class of yields of images of the regular tree languages under finite-copying topdown tree transducers.",
        "id":18592508
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"I'm looking for a comprehensive dataset that has been influential in fact verification research",
    "positive_ctxs":[
      {
        "title":"FEVER: a large-scale dataset for Fact Extraction and VERification",
        "text":"In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from.The claims are classified as SUPPORTED, RE-FUTED or NOTENOUGHINFO by annotators achieving 0.6841 in Fleiss κ. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87%, while if we ignore the evidence we achieve 50.91%. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.",
        "id":4711425
      }
    ],
    "negative_ctxs":[
      {
        "title":"Zhestyatsky at SemEval-2021 Task 2: ReLU over Cosine Similarity for BERT Fine-tuning",
        "text":"This paper presents our contribution to SemEval-2021 Task 2: Multilingual and Cross-lingual Word-in-Context Disambiguation (MCL-WiC). Our experiments cover English (EN-EN) sub-track from the multilingual setting of the task. We experiment with several pre-trained language models and investigate an impact of different top-layers on finetuning. We find the combination of Cosine Similarity and ReLU activation leading to the most effective fine-tuning procedure. Our best model results in accuracy 92.7%, which is the fourth-best score in EN-EN sub-track.",
        "id":233231621
      },
      {
        "title":"",
        "text":"",
        "id":218977390
      },
      {
        "title":"Cross-Lingual Dependency Parsing Using Code-Mixed TreeBank",
        "text":"Treebank translation is a promising method for cross-lingual transfer of syntactic dependency knowledge. The basic idea is to map dependency arcs from a source treebank to its target translation according to word alignments. This method, however, can suffer from imperfect alignment between source and target words. To address this problem, we investigate syntactic transfer by code mixing, translating only confident words in a source treebank. Cross-lingual word embeddings are leveraged for transferring syntactic knowledge to the target from the resulting code-mixed treebank. Experiments on University Dependency Treebanks show that code-mixed treebanks are more effective than translated treebanks, giving highly competitive performances among cross-lingual parsing methods.",
        "id":202541341
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"I'm looking for a paper that discusses improvements in constituency parsing performance by applying a partition strategy for content embedding and positional embedding within self-attention and label attention layers.",
    "positive_ctxs":[
      {
        "title":"Constituency Parsing with a Self-Attentive Encoder",
        "text":"We demonstrate that replacing an LSTM encoder with a self-attentive architecture can lead to improvements to a state-ofthe-art discriminative constituency parser. The use of attention makes explicit the manner in which information is propagated between different locations in the sentence, which we use to both analyze our model and propose potential improvements. For example, we find that separating positional and content information in the encoder can lead to improved parsing accuracy. Additionally, we evaluate different approaches for lexical representation. Our parser achieves new state-ofthe-art results for single models trained on the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset.",
        "id":19206893
      }
    ],
    "negative_ctxs":[
      {
        "title":"Evaluating the morphological competence of Machine Translation Systems",
        "text":"While recent changes in Machine Translation state-of-the-art brought translation quality a step further, it is regularly acknowledged that the standard automatic metrics do not provide enough insights to fully measure the impact of neural models. This paper proposes a new type of evaluation focused specifically on the morphological competence of a system with respect to various grammatical phenomena. Our approach uses automatically generated pairs of source sentences, where each pair tests one morphological contrast. This methodology is used to compare several systems submitted at WMT'17 for English into Czech and Latvian.",
        "id":11877258
      },
      {
        "title":"An Extensible Framework for Efficient Document Management Using RDF and OWL",
        "text":"In this paper, we describe an integrated approach towards dealing with various semantic and structural issues associated with document management. We provide motivations for using XML, RDF and OWL in building a seamless architecture to serve not only as a document exchange service but also to enable higher level services such as annotations, metadata access and querying. The key idea is to manifest differential treatments for the actual document structure, semantic content of the document and ontological document organization. The deployment of this architecture in the PROTEUS project 1 provides an industrial setting for evaluation and further specification.",
        "id":18471591
      },
      {
        "title":"Annotation of Events and Temporal Expressions in French Texts",
        "text":"We present two modules for the recognition and annotation of temporal expressions and events in French texts according to the TimeML specification language. The Temporal Expression Tagger we have developed is based on a large coverage cascade of finite state transducers and our Event Tagger on a set of simple heuristics applied over local context in a chunked text. We present results of a preliminary evaluation and compare them with those obtained by a similar system.",
        "id":17196199
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"I'm looking for innovative approaches to data annotation on platforms like Amazon Mechanical Turk that focus on maximizing document coverage. Is there any research discussing strategies that balance the trade-off between full annotation and broader document coverage?",
    "positive_ctxs":[
      {
        "title":"Partial Or Complete, That Is The Question",
        "text":"For many structured learning tasks, the data annotation process is complex and costly. Existing annotation schemes usually aim at acquiring completely annotated structures, under the common perception that partial structures are of low quality and could hurt the learning process. This paper questions this common perception, motivated by the fact that structures consist of interdependent sets of variables. Thus, given a fixed budget, partly annotating each structure may provide the same level of supervision, while allowing for more structures to be annotated. We provide an information theoretic formulation for this perspective and use it, in the context of three diverse structured learning tasks, to show that learning from partial structures can sometimes outperform learning from complete ones. Our findings may provide important insights into structured data annotation schemes and could support progress in learning protocols for structured tasks.",
        "id":174800300
      }
    ],
    "negative_ctxs":[
      {
        "title":"Nearest Neighbor Knowledge Distillation for Neural Machine Translation",
        "text":"k-nearest-neighbor machine translation (kNN-MT), proposed byKhandelwal et al. (2021), has achieved many state-of-the-art results in machine translation tasks. Although effective, kNN-MT requires conducting kNN searches through the large datastore for each decoding step during inference, prohibitively increasing the decoding cost and thus leading to the difficulty for the deployment in real-world applications. In this paper, we propose to move the time-consuming kNN search forward to the preprocessing phase, and then introduce k Nearest Neighbor Knowledge Distillation (kNN-KD) that trains the base NMT model to directly learn the knowledge of kNN. Distilling knowledge retrieved by kNN can encourage the NMT model to take more reasonable target tokens into consideration, thus addressing the overcorrection problem. Extensive experimental results show that, the proposed method achieves consistent improvement over the stateof-the-art baselines including kNN-MT, while maintaining the same training and decoding speed as the standard NMT model. 1",
        "id":248496823
      },
      {
        "title":"Dialogue Management based on Multi-domain Corpus",
        "text":"Dialogue Management (DM) is a key issue in Spoken Dialogue System. Most of the existing data-driven DM schemes train the dialogue policy for some specific domain (or vertical domain), only using the dialogue corpus in this domain, which might suffer from the scarcity of dialogue corpus in some domains. In this paper, we divide Dialogue Act (DA), as semantic representation of utterance, into DA type and slot parameter, where the former one is domain-independent and the latter one is domain-specific. Firstly, based on multiple-domain dialogue corpus, the DA type prediction model is trained via Recurrent Neutral Networks (RNN). Moreover, DA type decision problem is modeled as a multi-order POMDP, and transformed to be a one-order MDP with continuous states, which is solved by Natural Actor Critic (NAC) algorithm and applicable for every domain. Furthermore, a slot parameter selection scheme is designed to generate a complete machine DA according to the features of specific domain, which yields the Multi-domain Corpus based Dialogue Management (MCD-M) scheme. Finally, extensive experimental results illustrate the performance improvement of the MCDM scheme, compared with the existing schemes.",
        "id":9281432
      },
      {
        "title":"NTUA-SLP at SemEval-2018 Task 2: Predicting Emojis using RNNs with Context-aware Attention",
        "text":"In this paper we present a deep-learning model that competed at SemEval-2018 Task 2 \"Multilingual Emoji Prediction\". We participated in subtask A, in which we are called to predict the most likely associated emoji in English tweets. The proposed architecture relies on a Long Short-Term Memory network, augmented with an attention mechanism, that conditions the weight of each word, on a \"context vector\" which is taken as the aggregation of a tweet's meaning. Moreover, we initialize the embedding layer of our model, with word2vec word embeddings, pretrained on a dataset of 550 million English tweets. Finally, our model does not rely on hand-crafted features or lexicons and is trained end-to-end with back-propagation. We ranked 2 nd out of 48 teams.",
        "id":4943905
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"I'm looking into morphological embedding algorithms that build upon the word2vec model by utilizing character n-grams. Which papers should I read to learn more about this approach?",
    "positive_ctxs":[
      {
        "title":"Enriching Word Vectors with Subword Information",
        "text":"Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Many popular models to learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for morphologically rich languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skip-gram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram, words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpus quickly. We evaluate the obtained word representations on five different languages, on word similarity and analogy tasks.",
        "id":207556454
      }
    ],
    "negative_ctxs":[
      {
        "title":"WIT: A Toolkit for Building Robust and Real-Time Spoken Dialogue Systems",
        "text":"This paper describes WI'I; a toolkit for building spoken dialogue systems. WIT features an incremental understanding mechanism that enables robust utterance understanding and realtime responses. WIT's ability to compile domain-dependent system specifications into internal knowledge sources makes building spoken dialogue systems much easier than :it is from scratch.157 the anonymous reviewers were of' great help.",
        "id":228130
      },
      {
        "title":"AUTOMATED QUALITY MONITORING FOR CALL CENTERS USING SPEECH AND NLP TECHNOLOGIES",
        "text":"This paper describes an automated system for assigning quality scores to recorded call center conversations. The system combines speech recognition, pattern matching, and maximum entropy classification to rank calls according to their measured quality. Calls at both ends of the spectrum are flagged as \"interesting\" and made available for further human monitoring. In this process, the ASR transcript is used to answer a set of standard quality control questions such as \"did the agent use courteous words and phrases,\" and to generate a question-based score. This is interpolated with the probability of a call being \"bad,\" as determined by maximum entropy operating on a set of ASR-derived features such as \"maximum silence length\" and the occurrence of selected n-gram word sequences. The system is trained on a set of calls with associated manual evaluation forms. We present precision and recall results from IBM's North American Help Desk indicating that for a given amount of listening effort, this system triples the number of bad calls that are identified, over the current policy of randomly sampling calls. The application that will be demonstrated is a research prototype that was built in conjunction with IBM's North American call centers.",
        "id":18047189
      },
      {
        "title":"Annotating Expressions of Opinion and Emotion in the Italian Content Annotation Bank",
        "text":"In this paper we describe the result of manually annotating I-CAB, the Italian Content Annotation Bank, by expressions of private state (EPSs), i.e., expressions that denote the presence of opinions, emotions, and other cognitive states. The aim of this effort was the generation of a standard resource for supporting the development of opinion extraction algorithms for Italian, and of a benchmark for testing such algorithms. To this end we have employed a previously existing annotation language (here dubbed WWC, from the initials of its proponents). We here describe the results of this annotation effort, including the results of a thorough inter-annotator agreement test. We conclude by discussing how WWC can be adapted to the specificities of a Romance language such as Italian.",
        "id":9520585
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"I'm looking into the distillation process of language models and would like to examine studies that specifically discuss the attention mechanism alignment in the teacher-student model architecture. Are there any papers you can suggest?",
    "positive_ctxs":[
      {
        "title":"MINILMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers",
        "text":"We generalize deep self-attention distillation in MINILM (Wang et al., 2020) by only using self-attention relation distillation for taskagnostic compression of pretrained Transformers. In particular, we define multi-head selfattention relations as scaled dot-product between the pairs of query, key, and value vectors within each self-attention module. Then we employ the above relational knowledge to train the student model. Besides its simplicity and unified principle, more favorably, there is no restriction in terms of the number of student's attention heads, while most previous work has to guarantee the same head number between teacher and student. Moreover, the fine-grained self-attention relations tend to fully exploit the interaction knowledge learned by Transformer. In addition, we thoroughly examine the layer selection strategy for teacher models, rather than just relying on the last layer as in MINILM. We conduct extensive experiments on compressing both monolingual and multilingual pretrained models. Experimental results demonstrate that our models 1 distilled from base-size and large-size teachers (BERT, RoBERTa and XLM-R) outperform the state-of-the-art.",
        "id":229923069
      }
    ],
    "negative_ctxs":[
      {
        "title":"Innovators@SMM4H'22: An Ensembles Approach for self-reporting of COVID-19 Vaccination Status Tweets",
        "text":"With the Surge in COVID-19, the number of social media postings related to the vaccine has grown, specifically tracing the confirmed reports by the users regarding the COVID-19 vaccine dose termed as Vaccine Surveillance.To mitigate this research problem, we present our novel ensembled approach for self-reporting COVID-19 vaccination status tweets into two labels, namely Vaccine Chatter and Self Report. We utilize state-of-the-art models, namely BERT, RoBERTa, and XLNet. Our model provides promising results with 0.77, 0.93, and 0.66 as precision, recall, and F1-score (respectively), comparable to the corresponding median scores of 0.77, 0.9, and 0.68 (respectively). The model gave an overall accuracy of 93.43. We also present an empirical analysis of the results to present how well the tweet was able to classify and report. We release our code base here https:\/\/github.com\/Zohair0209\/ SMM4H-2022-Task6.git",
        "id":252818969
      },
      {
        "title":"Improving word alignment for low resource languages using English monolingual SRL",
        "text":"We introduce a new statistical machine translation approach specifically geared to learning translation from low resource languages, that exploits monolingual English semantic parsing to bias inversion transduction grammar (ITG) induction. We show that in contrast to conventional statistical machine translation (SMT) training methods, which rely heavily on phrase memorization, our approach focuses on learning bilingual correlations that help translating low resource languages, by using the output language semantic structure to further narrow down ITG constraints. This approach is motivated by previous research which has shown that injecting a semantic frame based objective function while training SMT models improves the translation quality. We show that including a monolingual semantic objective function during the learning of the translation model leads towards a semantically driven alignment which is more efficient than simply tuning loglinear mixture weights against a semantic frame based evaluation metric in the final stage of statistical machine translation training. We test our approach with three different language pairs and demonstrate that our model biases the learning towards more semantically correct alignments. Both GIZA++ and ITG based techniques fail to capture meaningful bilingual constituents, which is required when trying to learn translation models for low resource languages. In contrast, our proposed model not only improve translation by injecting a monolingual objective function to learn bilingual correlations during early training of the translation model, but also helps to learn more meaningful correlations with a relatively small data set, leading to a better alignment compared to either conventional ITG or traditional GIZA++ based approaches.",
        "id":10060918
      },
      {
        "title":"Ordering Phrases with Function Words",
        "text":"This paper presents a Function Word centered, Syntax-based (FWS) solution to address phrase ordering in the context of statistical machine translation (SMT). Motivated by the observation that function words often encode grammatical relationship among phrases within a sentence, we propose a probabilistic synchronous grammar to model the ordering of function words and their left and right arguments. We improve phrase ordering performance by lexicalizing the resulting rules in a small number of cases corresponding to function words. The experiments show that the FWS approach consistently outperforms the baseline system in ordering function words' arguments and improving translation quality in both perfect and noisy word alignment scenarios.",
        "id":390966
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"I'm researching insertion-based decoding methods for semantic parsing and language modeling, and I'm looking for works that discuss alternatives to traditional loss functions such as cross-entropy, particularly those using Kullback–Leibler divergence in this context. Could you point me to some studies on this?",
    "positive_ctxs":[
      {
        "title":"Don't Parse, Insert: Multilingual Semantic Parsing with Insertion Based Decoding",
        "text":"Semantic parsing is one of the key components of natural language understanding systems. A successful parse transforms an input utterance to an action that is easily understood by the system. Many algorithms have been proposed to solve this problem, from conventional rulebased or statistical slot-filling systems to shiftreduce based neural parsers. For complex parsing tasks, the state-of-the-art method is based on autoregressive sequence to sequence models to generate the parse directly. This model is slow at inference time, generating parses in O(n) decoding steps (n is the length of the target sequence). In addition, we demonstrate that this method performs poorly in zero-shot cross-lingual transfer learning settings. In this paper, we propose a non-autoregressive parser which is based on the insertion transformer to overcome these two issues. Our approach 1) speeds up decoding by 3x while outperforming the autoregressive model and 2) significantly improves cross-lingual transfer in the low-resource setting by 37% compared to autoregressive baseline. We test our approach on three well-known monolingual datasets: ATIS, SNIPS and TOP. For cross lingual semantic parsing, we use the MultiATIS++ and the multilingual TOP datasets.",
        "id":222208998
      }
    ],
    "negative_ctxs":[
      {
        "title":"Universal Morphological Analysis using Structured Nearest Neighbor Prediction",
        "text":"In this paper, we consider the problem of unsupervised morphological analysis from a new angle. Past work has endeavored to design unsupervised learning methods which explicitly or implicitly encode inductive biases appropriate to the task at hand. We propose instead to treat morphological analysis as a structured prediction problem, where languages with labeled data serve as training examples for unlabeled languages, without the assumption of parallel data. We define a universal morphological feature space in which every language and its morphological analysis reside. We develop a novel structured nearest neighbor prediction method which seeks to find the morphological analysis for each unlabeled language which lies as close as possible in the feature space to a training language. We apply our model to eight inflecting languages, and induce nominal morphology with substantially higher accuracy than a traditional, MDLbased approach. Our analysis indicates that accuracy continues to improve substantially as the number of training languages increases.",
        "id":43278
      },
      {
        "title":"SYNTACTIC APPROACHES TO AUTOMATIC BOOK INDEXING",
        "text":"Automatic book indexing systems are based on the generation of phrase structures capable of reflecting text content. • Some approaches are given for the automatic construction of back-of-book indexes using a syntactic analysis of the available texts, followed by the identification of nominal constructions, the assignment of importance weights to the term phrases, and the choice of phrases as indexing units.",
        "id":16378264
      },
      {
        "title":"Universal Dependencies for Finnish",
        "text":"There has been substantial recent interest in annotation schemes that can be applied consistently to many languages. Building on several recent efforts to unify morphological and syntactic annotation, the Universal Dependencies (UD) project seeks to introduce a cross-linguistically applicable part-of-speech tagset, feature inventory, and set of dependency relations as well as a large number of uniformly annotated treebanks. We present Universal Dependencies for Finnish, one of the ten languages in the recent first release of UD project treebank data. We detail the mapping of previously introduced annotation to the UD standard, describing specific challenges and their resolution. We additionally present parsing experiments comparing the performance of a stateof-the-art parser trained on a languagespecific annotation schema to performance on the corresponding UD annotation. The results show improvement compared to the source annotation, indicating that the conversion is accurate and supporting the feasibility of UD as a parsing target. The introduced tools and resources are available under open licenses from",
        "id":7696189
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"I'm researching on the efficacy of recurrent networks in language modeling and CCG supertagging. Could you point me to studies that explore LSTM architectures and model comparisons in these tasks?",
    "positive_ctxs":[
      {
        "title":"Colorless green recurrent networks dream hierarchically",
        "text":"Recurrent neural networks (RNNs) have achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate here to what extent RNNs learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues (\"The colorless green ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas I ate with the chair sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep furiously\"), and, for Italian, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about long-distance agreement, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallowpattern extractors, but they also acquire deeper grammatical competence. * The work was conducted during the internship at Facebook AI Research, Paris.",
        "id":4460159
      },
      {
        "title":"Supertagging with LSTMs",
        "text":"In this paper we present new state-of-the-art performance on CCG supertagging and parsing. Our model outperforms existing approaches by an absolute gain of 1.5%. We analyze the performance of several neural models and demonstrate that while feed-forward architectures can compete with bidirectional LSTMs on POS tagging, models that encode the complete sentence are necessary for the long range syntactic information encoded in supertags.",
        "id":11771220
      }
    ],
    "negative_ctxs":[
      {
        "title":"Evaluating Feature Extraction Methods for Knowledge-based Biomedical Word Sense Disambiguation",
        "text":"In this paper, we present an analysis of feature extraction methods via dimensionality reduction for the task of biomedical Word Sense Disambiguation (WSD). We modify the vector representations in the 2-MRD WSD algorithm, and evaluate four dimensionality reduction methods: Word Embeddings using Continuous Bag of Words and Skip Gram, Singular Value Decomposition (SVD), and Principal Component Analysis (PCA). We also evaluate the effects of vector size on the performance of each of these methods. Results are evaluated on five standard evaluation datasets (Abbrev.100, Abbrev.200, Abbrev.300, NLM-WSD, and MSH-WSD). We find that vector sizes of 100 are sufficient for all techniques except SVD, for which a vector size of 1500 is preferred. We also show that SVD performs on par with Word Embeddings for all but one dataset.",
        "id":1841504
      },
      {
        "title":"",
        "text":"",
        "id":227231871
      },
      {
        "title":"Grammar-based tools for the creation of tagging resources for an unresourced language: the case of Northern Sotho",
        "text":"We describe an architecture for the parallel construction of a tagger lexicon and an annotated reference corpus for the part-of-speech tagging of Nothern Sotho, a Bantu language of South Africa, for which no tagged resources have been available so far. Our tools make use of grammatical properties (morphological and syntactic) of the language. We use symbolic pretagging, followed by stochastic tagging, an architecture which proves useful not only for the bootstrapping of tagging resources, but also for the tagging of any new text. We discuss the tagset design, the tool architecture and the current state of our ongoing effort.",
        "id":14356537
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"I'm searching for studies that explore advancements in dependency parsing, particularly using graph-to-graph transformers with iterative refinement processes. Which publications should I look into?",
    "positive_ctxs":[
      {
        "title":"Recursive Non-Autoregressive Graph-to-Graph Transformer for Dependency Parsing with Iterative Refinement",
        "text":"We propose the Recursive Non-autoregressive Graph-to-Graph Transformer architecture (RNGTr) for the iterative refinement of arbitrary graphs through the recursive application of a non-autoregressive Graph-to-Graph Transformer and apply it to syntactic dependency parsing. We demonstrate the power and effectiveness of RNGTr on several dependency corpora, using a refinement model pre-trained with BERT. We also introduce Syntactic Transformer (SynTr), a non-recursive parser similar to our refinement model. RNGTr can improve the accuracy of a variety of initial parsers on 13 languages from the Universal Dependencies Treebanks, English and Chinese Penn Treebanks, and the German CoNLL2009 corpus, even improving over the new state-ofthe-art results achieved by SynTr, significantly improving the state-of-the-art for all corpora tested.",
        "id":214713480
      }
    ],
    "negative_ctxs":[
      {
        "title":"Quantifying lexical influence: Giving direction to context",
        "text":"The relevance of context in disambiguating natural language input has been widely acknowledged in the literature. However, most attempts at formalising the intuitive notion of context tend to treat the word and its context symmetrically. We demonstrate here that traditional measures such as mutual information score are likely to overlook a significant fraction of all co-occurrence phenomena in natural language. We also propose metrics for measuring directed lexical influence and compare performances.",
        "id":10921956
      },
      {
        "title":"Context-Enhanced Adaptive Entity Linking",
        "text":"More and more knowledge bases are publicly available as linked data. Since these knowledge bases contain structured descriptions of real-world entities, they can be exploited by entity linking systems that anchor entity mentions from text to the most relevant resources describing those entities. In this paper, we investigate adaptation of the entity linking task using contextual knowledge. The key intuition is that entity linking can be customized depending on the textual content, as well as on the application that would make use of the extracted information. We present an adaptive approach that relies on contextual knowledge from text to enhance the performance of ADEL, a hybrid linguistic and graph-based entity linking system. We evaluate our approach on a domain-specific corpus consisting of annotated WikiNews articles.",
        "id":16060148
      },
      {
        "title":"CBR-Tagger: a case-based reasoning approach to the gene\/protein mention problem",
        "text":"This work proposes a case-based classifier to tackle the gene\/protein mention problem in biomedical literature. The so called gene mention problem consists of the recognition of gene and protein entities in scientific texts. A classification process aiming at deciding if a term is a gene mention or not is carried out for each word in the text. It is based on the selection of the best or most similar case in a base of known and unknown cases. The approach was evaluated on several datasets for different organisms and results show the suitability of this approach for the gene mention problem.",
        "id":14247203
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"In discourse parsing literature, which works have explored parser performance by adopting the original Parseval procedure and reporting micro-averaged F1 scores?",
    "positive_ctxs":[
      {
        "title":"A Top-Down Neural Architecture towards Text-Level Parsing of Discourse Rhetorical Structure",
        "text":"Due to its great importance in deep natural language understanding and various down-stream applications, text-level parsing of discourse rhetorical structure (DRS) has been drawing more and more attention in recent years. However, all the previous studies on text-level discourse parsing adopt bottom-up approaches, which much limit the DRS determination on local information and fail to well benefit from global information of the overall discourse. In this paper, we justify from both computational and perceptive points-of-view that the top-down architecture is more suitable for textlevel DRS parsing. On the basis, we propose a top-down neural architecture toward text-level DRS parsing. In particular, we cast discourse parsing as a recursive split point ranking task, where a split point is classified to different levels according to its rank and the elementary discourse units (EDUs) associated with it are arranged accordingly. In this way, we can determine the complete DRS as a hierarchical tree structure via an encoder-decoder with an internal stack. Experimentation on both the English RST-DT corpus and the Chinese CDTB corpus shows the great effectiveness of our proposed top-down approach towards textlevel DRS parsing.",
        "id":218516743
      }
    ],
    "negative_ctxs":[
      {
        "title":"COLLATE: Competence Center in Speech and Language Technology",
        "text":"This paper presents the structure and activitities of the recently established Competence Center in Speech and Language Technology in Saarbrücken. The objectives of the Competence Center are to provide a comprehensive information service about speech and language technologies, including live demonstrations of the most important language technology (LT) systems, and to advance the state of the art in the evaluation of LT systems for real-world applications. The Competence Center comprises the following components:1. the Virtual Information Center \"Language Technology World\" (www.lt-world.org), the world's most comprehensive information resource about speech and language technology, 2. the Demonstration Center in Saarbrücken, which offers interested parties the possibility to play and experiment with different speech and language technologies, or to attend guided demonstrations, 3. the Evaluation Center, which conducts evaluations of the overall usability of language technology systems and advances knowledge of relevant usability issues and evaluation methods.",
        "id":814070
      },
      {
        "title":"Multi-Component Word Sense Disambiguation",
        "text":"This paper describes the system MC-WSD presented for the English Lexical Sample task. The system is based on a multicomponent architecture. It consists of one classifier with two components. One is trained on the data provided for the task. The second is trained on this data and, additionally, on an external training set extracted from the Wordnet glosses. The goal of the additional component is to lessen sparse data problems by exploiting the information encoded in the ontology.",
        "id":7192080
      },
      {
        "title":"A Hybrid System for Patent Translation",
        "text":"This work presents a HMT system for patent translation. The system exploits the high coverage of SMT and the high precision of an RBMT system based on GF to deal with specific issues of the language. The translator is specifically developed to translate patents and it is evaluated in the English-French language pair. Although the number of issues tackled by the grammar are not extremely numerous yet, both manual and automatic evaluations consistently show their preference for the hybrid system in front of the two individual translators.",
        "id":39619605
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"In researching metrics for human-interaction with computer-assisted translation tools, which studies have analyzed parameters such as the keystroke ratio and mouse action ratio to quantify user effort or engagement?",
    "positive_ctxs":[
      {
        "title":"Statistical Approaches to Computer-Assisted Translation",
        "text":"Universitat Jaume I Current machine translation (MT) systems are still not perfect. In practice, the output from these systems needs to be edited to correct errors. A way of increasing the productivity of the whole translation process (MT plus human work) is to incorporate the human correction activities within the translation process itself, thereby shifting the MT paradigm to that of computer-assisted translation. This model entails an iterative process in which the human translator activity is included in the loop: In each iteration, a prefix of the translation is validated (accepted or amended) by the human and the system computes its best (or n-best) translation suffix hypothesis to complete this prefix. A successful framework for MT is the so-called statistical (or pattern recognition) framework. Interestingly, within this framework, the adaptation of MT systems to the interactive scenario affects mainly the search process, allowing a great reuse of successful techniques and models. In this article, alignment templates, phrase-based models, and stochastic finite-state transducers are used to develop computer-assisted translation systems. These systems were assessed in a European project (TransType2) in two real tasks: The translation of printer manuals; manuals and the translation of the Bulletin of the European Union. In each task, the following three pairs of languages were involved (in both translation directions):",
        "id":92327
      },
      {
        "title":"Statistical phrase-based models for interactive computer-assisted translation",
        "text":"Obtaining high-quality machine translations is still a long way off. A postediting phase is required to improve the output of a machine translation system. An alternative is the so called computerassisted translation. In this framework, a human translator interacts with the system in order to obtain high-quality translations. A statistical phrase-based approach to computer-assisted translation is described in this article. A new decoder algorithm for interactive search is also presented, that combines monotone and nonmonotone search. The system has been assessed in the TransType-2 project for the translation of several printer manuals, from (to) English to (from) Spanish, German and French.",
        "id":16308735
      }
    ],
    "negative_ctxs":[
      {
        "title":"Temporal information extraction from clinical text",
        "text":"In this paper, we present a method for temporal relation extraction from clinical narratives in French and in English. We experiment on two comparable corpora, the MERLOT corpus for French and the THYME corpus for English, and show that a common approach can be used for both languages.",
        "id":16711873
      },
      {
        "title":"Joint Multi-modal Aspect-Sentiment Analysis with Auxiliary Cross-modal Relation Detection",
        "text":"Aspect terms extraction (ATE) and aspect sentiment classification (ASC) are two fundamental and fine-grained sub-tasks in aspect-level sentiment analysis (ALSA). In the textual analysis, jointly extracting both aspect terms and sentiment polarities has been drawn much attention due to the better applications than individual sub-task. However, in the multimodal scenario, the existing studies are limited to handle each sub-task independently, which fails to model the innate connection between the above two objectives and ignores the better applications. Therefore, in this paper, we are the first to jointly perform multi-modal ATE (MATE) and multi-modal ASC (MASC), and we propose a multi-modal joint learning approach with auxiliary cross-modal relation detection for multi-modal aspect-level sentiment analysis (MALSA). Specifically, we first build an auxiliary text-image relation detection module to control the proper exploitation of visual information. Second, we adopt the hierarchical framework to bridge the multi-modal connection between MATE and MASC, as well as separately visual guiding for each sub module. Finally, we can obtain all aspect-level sentiment polarities dependent on the jointly extracted specific aspects. Extensive experiments show the effectiveness of our approach against the joint textual approaches, pipeline and collapsed multi-modal approaches.",
        "id":243865127
      },
      {
        "title":"Recognizing English Learners' Native Language from Their Writings",
        "text":"Native Language Identification (NLI), which tries to identify the native language (L1) of a second language learner based on their writings, is helpful for advancing second language learning and authorship profiling in forensic linguistics. With the availability of relevant data resources, much work has been done to explore the native language of a foreign language learner. In this report, we present our system for the first shared task in Native Language Identification (NLI). We use a linear SVM classifier and explore features of words, word and character n-grams, style, and metadata. Our official system achieves accuracy of 0.773, which ranks it 18 th among the 29 teams in the closed track.",
        "id":18504651
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"In the area of argument mining, could you point to literature that uses of dependency parsers to determine the argumentativeness of texts in dialogue systems?",
    "positive_ctxs":[
      {
        "title":"Dialo-AP: A Dependency Parsing Based Argument Parser for Dialogues",
        "text":"While neural approaches to argument mining (AM) have advanced considerably, most of the recent work has been limited to parsing monologues. With an urgent interest in the use of conversational agents for broader societal applications, there is a need to advance the stateof-the-art in argument parsers for dialogues. This enables progress towards more purposeful conversations involving persuasion, debate and deliberation. This paper discusses Dialo-AP, an end-to-end argument parser that constructs argument graphs from dialogues. We formulate AM as dependency parsing of elementary and argumentative discourse units; the system is trained using extensive pre-training and curriculum learning comprising nine diverse corpora. Dialo-AP is capable of generating argument graphs from dialogues by performing all subtasks of AM. Compared to existing state-ofthe-art baselines, Dialo-AP achieves significant improvements across all tasks, which is further validated through rigorous human evaluation.",
        "id":252818918
      }
    ],
    "negative_ctxs":[
      {
        "title":"Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them",
        "text":"Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between \"gender-neutralized\" words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.",
        "id":73729169
      },
      {
        "title":"Statistical methods for retrieving most significant paragraphs in newspaper articles • jea@dt fct uni pt",
        "text":"Retrieving a most stgulficant paragraph m a newspaper arUcle can act as a kind of surnmanzatmn It can gwe the human reader some hints on the contents of the arucle and help him to decide whether It deseei'ves a full readmg or not It may also act as a filter for a robust natural language understanding system, to extract relevant mformatton from that paragraph m order to enable conceptual mformauon retrieval Talang a newspaper arUcle and a base corpus, word co-occurrences w3th higher resolving power are ~dent~fied These co-occurrences are used to estabhsh hnks between the paragraphs of the arUcle The paragraph which presents the larger number of hnks tO other paragraphs ~s considered a most slgmficant one Though designed and tested for the Portuguese language, the staUshcal nature of our proposal should ensure ns portabtlny to other languages",
        "id":14181397
      },
      {
        "title":"DISTRIBUTIONAL CLUSTERING OF ENGLISH WORDS",
        "text":"We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical \"soft\" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data.",
        "id":6713452
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"In the context of Named Entity Recognition tasks across multiple languages, which work highlights the necessity of retrieving related knowledge to aid in the annotation of ambiguous named entities?",
    "positive_ctxs":[
      {
        "title":"CrossWeigh: Training Named Entity Tagger from Imperfect Annotations",
        "text":"Everyone makes mistakes. So do human annotators when curating labels for named entity recognition (NER). Such label mistakes might hurt model training and interfere model comparison. In this study, we dive deep into one of the widely-adopted NER benchmark datasets, CoNLL03 NER. We are able to identify label mistakes in about 5.38% test sentences, which is a significant ratio considering that the state-of-the-art test F 1 score is already around 93%. Therefore, we manually correct these label mistakes and form a cleaner test set. Our re-evaluation of popular models on this corrected test set leads to more accurate assessments, compared to those on the original test set. More importantly, we propose a simple yet effective framework, CrossWeigh, to handle label mistakes during NER model training. Specifically, it partitions the training data into several folds and train independent NER models to identify potential mistakes in each fold. Then it adjusts the weights of training data accordingly to train the final NER model. Extensive experiments demonstrate significant improvements of plugging various NER models into our proposed framework on three datasets. All implementations and corrected test set are available at our Github repo 1 . * Equal Contributions. 1 https:\/\/github.com\/ZihanWangKi\/ CrossWeigh",
        "id":202540591
      }
    ],
    "negative_ctxs":[
      {
        "title":"WASSERSTEIN AUTO-ENCODED MDPS FORMAL VERIFICATION OF EFFICIENTLY DISTILLED RL POLICIES WITH MANY- SIDED GUARANTEES",
        "text":"Although deep reinforcement learning (DRL) has many success stories, the largescale deployment of policies learned through these advanced techniques in safetycritical scenarios is hindered by their lack of formal guarantees. Variational Markov Decision Processes (VAE-MDPs) are discrete latent space models that provide a reliable framework for distilling formally verifiable controllers from any RL policy. While the related guarantees address relevant practical aspects such as the satisfaction of performance and safety properties, the VAE approach suffers from several learning flaws (posterior collapse, slow learning speed, poor dynamics estimates), primarily due to the absence of abstraction and representation guarantees to support latent optimization. We introduce the Wasserstein auto-encoded MDP (WAE-MDP), a latent space model that fixes those issues by minimizing a penalized form of the optimal transport between the behaviors of the agent executing the original policy and the distilled policy, for which the formal guarantees apply. Our approach yields bisimulation guarantees while learning the distilled policy, allowing concrete optimization of the abstraction and representation model quality. Our experiments show that, besides distilling policies up to 10 times faster, the latent model quality is indeed better in general. Moreover, we present experiments from a simple time-to-failure verification algorithm on the latent space. The fact that our approach enables such simple verification techniques highlights its applicability.",
        "id":257663689
      },
      {
        "title":"An End-to-End Approach for Full Bridging Resolution",
        "text":"In this article, we describe our submission to the CODI-CRAC 2021 Shared Task on Anaphora Resolution in Dialogues -Track BR (Gold) 1 . We demonstrate the performance of an end-to-end transformer-based higher-order coreference model finetuned for the task of full bridging. We find that while our approach is not effective at modeling the complexities of the task, it performs well on bridging resolution, suggesting a need for investigations into a robust anaphor identification model for future improvements.",
        "id":241583697
      },
      {
        "title":"Purity Homophily in Social Networks -Invited Talk",
        "text":"Does sharing moral values encourage people to connect and form communities? The importance of moral homophily (love of same) has been recognized by social scientists, but the types of moral similarities that drive this phenomenon are still unknown. In this talk, I will present a series of experiments (both large-scale, observational social-media analyses and behavioral lab experiments) that investigate which types of moral similarities influence tie formations. Our results indicate that social network processes reflect moral selection, and both online and offline differences in moral purity concerns are particularly predictive of social distance.Dr. Morteza Dehghani is an Assistant Professor of psychology, computer science and the Brain and Creativity Institute at University of Southern California. His research spans the boundary between psychology and artificial intelligence, as does his education. His work investigates properties of cognition by using documents of the social discourse, such as narratives, social media, transcriptions of speeches and news articles, in conjunction to behavioral studies.16",
        "id":34363067
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"In the context of machine translation, can you point me towards literature discussing the specifications for setting up encoder\/decoder layers, attention heads, and other hyperparameters for a neural network model?",
    "positive_ctxs":[
      {
        "title":"Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation",
        "text":"Massively multilingual models for neural machine translation (NMT) are theoretically attractive, but often underperform bilingual models and deliver poor zero-shot translations. In this paper, we explore ways to improve them. We argue that multilingual NMT requires stronger modeling capacity to support language pairs with varying typological characteristics, and overcome this bottleneck via language-specific components and deepening NMT architectures. We identify the off-target translation issue (i.e. translating into a wrong target language) as the major source of the inferior zero-shot performance, and propose random online backtranslation to enforce the translation of unseen training language pairs. Experiments on OPUS-100 (a novel multilingual dataset with 100 languages) show that our approach substantially narrows the performance gap with bilingual models in both oneto-many and many-to-many settings, and improves zero-shot performance by ∼10 BLEU, approaching conventional pivot-based methods. 1",
        "id":216144650
      }
    ],
    "negative_ctxs":[
      {
        "title":"Methods to Optimize Wav2Vec with Language Model for Automatic Speech Recognition in Resource Constrained Environment",
        "text":"Automatic Speech Recognition (ASR) on resource constrained environment is a complex task since most of the State-Of-The-Art models are combination of multilayered convolutional neural network (CNN) and Transformer models which itself requires huge resources such as GPU or TPU for training as well as inference. The accuracy as a performance metric of an ASR system depends upon the efficiency of phonemes to word translation of an Acoustic Model and context correction of the Language model. However, inference as a performance metric is also an important aspect, which mostly depends upon the resources. Also, most of the ASR models uses transformer models at its core and one caveat of transformers is that it usually has a finite amount of sequence length it can handle. Either because it uses position encodings or simply because the cost of attention in transformers is actually O(n²) in sequence length, meaning that using very large sequence length explodes in complexity\/memory. So you cannot run the system with finite hardware even a very high-end GPU, because if we inference even a one hour long audio with Wav2Vec the system will crash. In this paper, we used some state-of-the-art methods to optimize the Wav2Vec model for better accuracy of predictions in resource constrained systems. In addition, we have performed tests with other SOTA models such as Citrinet and Quartznet for the comparative analysis.",
        "id":257767721
      },
      {
        "title":"Exploring Verb Frames for Sentence Simplification in Hindi",
        "text":"Systems processing on natural language text encounters fatal problems due to long and complex sentences. Their performance degrades as the complexity of the sentence increases. This paper addresses the task of simplifying complex sentences in Hindi into multiple simple sentences, using a rule based approach. Our approach utilizes two linguistic resources viz. verb demand frames and conjuncts' list. We performed automatic as well as human evaluation of our system.",
        "id":8405027
      },
      {
        "title":"Unsupervised Separation of Transliterable and Native Words for Malayalam",
        "text":"Differentiating intrinsic language words from transliterable words is a key step aiding text processing tasks involving different natural languages. We consider the problem of unsupervised separation of transliterable words from native words for text in Malayalam language. Outlining a key observation on the diversity of characters beyond the word stem, we develop an optimization method to score words based on their nativeness. Our method relies on the usage of probability distributions over character n-grams that are refined in step with the nativeness scorings in an iterative optimization formulation. Using an empirical evaluation, we illustrate that our method, DTIM, provides significant improvements in nativeness scoring for Malayalam, establishing DTIM as the preferred method for the task.",
        "id":4391686
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"In the context of natural language processing, I am looking for research that explores the relationship between a model's prediction entropy and its tendencies to copy existing text versus generating novel content. Can you recommend a paper?",
    "positive_ctxs":[
      {
        "title":"Understanding Neural Abstractive Summarization Models via Uncertainty",
        "text":"An advantage of seq2seq abstractive summarization models is that they generate text in a free-form manner, but this flexibility makes it difficult to interpret model behavior. In this work, we analyze summarization decoders in both blackbox and whitebox ways by studying on the entropy, or uncertainty, of the model's token-level predictions. For two strong pretrained models, PEGASUS (Zhang et al., 2020) and BART (Lewis et al., 2020) on two summarization datasets, we find a strong correlation between low prediction entropy and where the model copies tokens rather than generating novel text. The decoder's uncertainty also connects to factors like sentence position and syntactic distance between adjacent pairs of tokens, giving a sense of what factors make a context particularly selective for the model's next output token. Finally, we study the relationship of decoder uncertainty and attention behavior to understand how attention gives rise to these observed effects in the model. We show that uncertainty is a useful perspective for analyzing summarization and text generation models more broadly. 1",
        "id":222378161
      }
    ],
    "negative_ctxs":[
      {
        "title":"Robust Multi-bit Natural Language Watermarking through Invariant Features",
        "text":"Recent years have witnessed a proliferation of valuable original natural language contents found in subscription-based media outlets, web novel platforms, and outputs of large language models. However, these contents are susceptible to illegal piracy and potential misuse without proper security measures. This calls for a secure watermarking system to guarantee copyright protection through leakage tracing or ownership identification. To effectively combat piracy and protect copyrights, a multi-bit watermarking framework should be able to embed adequate bits of information and extract the watermarks in a robust manner despite possible corruption. In this work, we explore ways to advance both payload and robustness by following a well-known proposition from image watermarking and identify features in natural language that are invariant to minor corruption. Through a systematic analysis of the possible sources of errors, we further propose a corruption-resistant infill model. Our full method improves upon the previous work on robustness by +16.8% point on average on four datasets, three corruption types, and two corruption ratios. 1",
        "id":259129912
      },
      {
        "title":"Space Efficient Context Encoding for Non-Task-Oriented Dialogue Generation with Graph Attention Transformer",
        "text":"To improve the coherence and knowledge retrieval capabilities of non-task-oriented dialogue systems, recent Transformer-based models aim to integrate fixed background context. This often comes in the form of knowledge graphs, and the integration is done by creating pseudo utterances through paraphrasing knowledge triples, added into the accumulated dialogue context. However, the context length is fixed in these architectures, which restricts how much background or dialogue context can be kept. In this work, we propose a more concise encoding for background context structured in the form of knowledge graphs, by expressing the graph connections through restrictions on the attention weights. The results of our human evaluation show that this encoding reduces space requirements without negative effects on the precision of reproduction of knowledge and perceived consistency. Further, models trained with our proposed context encoding generate dialogues that are judged to be more comprehensive and interesting.ReferencesKurt Bollacker, Robert Cook, and Patrick Tufts. 2007.Freebase: A shared database of structured general human knowledge.",
        "id":236317402
      },
      {
        "title":"NTUA-SLP at SemEval-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning",
        "text":"In this paper we present deep-learning models that submitted to the SemEval-2018 Task 1 competition: \"Affect in Tweets\". We participated in all subtasks for English tweets. We propose a Bi-LSTM architecture equipped with a multi-layer self attention mechanism. The attention mechanism improves the model performance and allows us to identify salient words in tweets, as well as gain insight into the models making them more interpretable. Our model utilizes a set of word2vec word embeddings trained on a large collection of 550 million Twitter messages, augmented by a set of word affective features. Due to the limited amount of task-specific training data, we opted for a transfer learning approach by pretraining the Bi-LSTMs on the dataset of Semeval 2017, Task 4A. The proposed approach ranked 1 st in Subtask E \"Multi-Label Emotion Classification\", 2 nd in Subtask A \"Emotion Intensity Regression\" and achieved competitive results in other subtasks.",
        "id":4933708
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"In the context of simultaneous machine translation, which tool or technique could I use to generate ground-truth alignments for training models to interpret and generate translations incrementally?",
    "positive_ctxs":[
      {
        "title":"A Simple, Fast, and Effective Reparameterization of IBM Model 2",
        "text":"We present a simple log-linear reparameterization of IBM Model 2 that overcomes problems arising from Model 1's strong assumptions and Model 2's overparameterization.Efficient inference, likelihood evaluation, and parameter estimation algorithms are provided. Training the model is consistently ten times faster than Model 4. On three large-scale translation tasks, systems built using our alignment model outperform IBM Model 4.An open-source implementation of the alignment model described in this paper is available from http:\/\/github.com\/clab\/fast align .",
        "id":8476273
      }
    ],
    "negative_ctxs":[
      {
        "title":"Proceedings of SSST, NAACL-HLT 2007 \/ AMTA Workshop on Syntax and Structure in Statistical Generation in Machine Translation from Deep Syntactic Trees",
        "text":"In this paper we explore a generative model for recovering surface syntax and strings from deep-syntactic tree structures. Deep analysis has been proposed for a number of language and speech processing tasks, such as machine translation and paraphrasing of speech transcripts. In an effort to validate one such formalism of deep syntax, the Praguian Tectogrammatical Representation (TR), we present a model of synthesis for English which generates surface-syntactic trees as well as strings. We propose a generative model for function word insertion (prepositions, definite\/indefinite articles, etc.) and subphrase reordering. We show by way of empirical results that this model is effective in constructing acceptable English sentences given impoverished trees.",
        "id":12088021
      },
      {
        "title":"MULTI-AGENT INTERACTIONS MODELING WITH COR- RELATED POLICIES",
        "text":"In multi-agent systems, complex interacting behaviors arise due to the high correlations among agents. However, previous work on modeling multi-agent interactions from demonstrations is primarily constrained by assuming the independence among policies and their reward structures. In this paper, we cast the multi-agent interactions modeling problem into a multi-agent imitation learning framework with explicit modeling of correlated policies by approximating opponents' policies, which can recover agents' policies that can regenerate similar interactions. Consequently, we develop a Decentralized Adversarial Imitation Learning algorithm with Correlated policies (CoDAIL), which allows for decentralized training and execution. Various experiments demonstrate that CoDAIL can better regenerate complex interactions close to the demonstrators and outperforms state-of-theart multi-agent imitation learning methods.",
        "id":210157251
      },
      {
        "title":"Neural Discourse Structure for Text Categorization",
        "text":"We show that discourse structure, as defined by Rhetorical Structure Theory and provided by an existing discourse parser, benefits text categorization. Our approach uses a recursive neural network and a newly proposed attention mechanism to compute a representation of the text that focuses on salient content, from the perspective of both RST and the task. Experiments consider variants of the approach and illustrate its strengths and weaknesses.Although the food was amazing] A [and I was in love with the spicy pork burrito,] B [the service was really awful.] C [We watched our waiter serve himself many drinks.] D [He kept running into the bathroom] E [instead of grabbing our bill.] F",
        "id":5914002
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"In the field of reinforcement learning models for multi-hop reasoning, what issue involves an agent erroneously correlating a successful outcome with irrelevant or coincidental actions, and are there any papers discussing this phenomenon?",
    "positive_ctxs":[
      {
        "title":"From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood",
        "text":"Our goal is to learn a semantic parser that maps natural language utterances into executable programs when only indirect supervision is available: examples are labeled with the correct execution result, but not the program itself. Consequently, we must search the space of programs for those that output the correct result, while not being misled by spurious programs: incorrect programs that coincidentally output the correct result. We connect two common learning paradigms, reinforcement learning (RL) and maximum marginal likelihood (MML), and then present a new learning algorithm that combines the strengths of both. The new algorithm guards against spurious programs by combining the systematic search traditionally employed in MML with the randomized exploration of RL, and by updating parameters such that probability is spread more evenly across consistent programs. We apply our learning algorithm to a new neural semantic parser and show significant gains over existing state-of-theart results on a recent context-dependent semantic parsing task.",
        "id":9268430
      }
    ],
    "negative_ctxs":[
      {
        "title":"Flat Multi-modal Interaction Transformer for Named Entity Recognition",
        "text":"Multi-modal named entity recognition (MNER) aims at identifying entity spans and recognizing their categories in social media posts with the aid of images. However, in dominant MNER approaches, the interaction of different modalities is usually carried out through the alternation of self-attention and cross-attention or over-reliance on the gating machine, which results in imprecise and biased correspondence between fine-grained semantic units of text and image. To address this issue, we propose a Flat Multi-modal Interaction Transformer (FMIT) for MNER. Specifically, we first utilize noun phrases in sentences and general domain words to obtain visual cues. Then, we transform the fine-grained semantic representation of the vision and text into a unified lattice structure and design a novel relative position encoding to match different modalities in Transformer. Meanwhile, we propose to leverage entity boundary detection as an auxiliary task to alleviate visual bias. Experiments show that our methods achieve the new state-of-the-art performance on two benchmark datasets.",
        "id":251741236
      },
      {
        "title":"LANGUAGE SYSTEMS INC : DESCRIPTION OF THE DBG SYSTE M AS USED FOR MUC-5 1 INTRODUCTIO N",
        "text":"Language Systems, Inc . (LSI) believes that the best system for producing a complete and accurate automate d analysis of natural language text is an in-depth text understanding system that employs linguistic as wel l as other analytical techniques to interpret the text . Our DBG (Data Base Generation) natural language processing system performs full-scale linguistic analysis of text in order to produce a system-internal textlevel representation of the content of the text . This representation is composed of a set of entity and event frame structures, interrelated to reflect the organization and content of the text . This representation of the text can then be mapped into any data structure required by a downstream application, such as th e templates specified for the MUC-5\/Tipster applications . DBG has been designed as a single core syste m for handling texts of different types in different domains for a variety of applications . Application types for which DBG has provided the input include information extraction and database generation tasks suc h as MUC-5, message fusion (the combination of information derived from various kinds of sources, includin g text ; see [1]), and the translation of text into another language using spoken input and output ([2]) .",
        "id":7259211
      },
      {
        "title":"Finding viable seed URLs for web corpora: A scouting approach and comparative study of available sources",
        "text":"The conventional tools of the \"web as corpus\" framework rely heavily on URLs obtained from search engines. Recently, the corresponding querying process became much slower or impossible to perform on a low budget. I try to find acceptable substitutes, i.e. viable link sources for web corpus construction. To this end, I perform a study of possible alternatives, including social networks as well as the Open Directory Project and Wikipedia. Four different languages (Dutch, French, Indonesian and Swedish) taken as examples show that complementary approaches are needed. My scouting approach using open-source software leads to a URL directory enriched with metadata which may be used to start a web crawl. This is more than a drop-in replacement for existing tools since said metadata enables researchers to filter and select URLs that fit particular needs, as they are classified according to their language, their length and a few other indicators such as host-and markup-based data.",
        "id":16763441
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"What approaches have been used to address the limitations of the beam search method in neural machine translation systems in terms of considering the full target context?",
    "positive_ctxs":[
      {
        "title":"Discriminative Reranking for Neural Machine Translation",
        "text":"Reranking models enable the integration of rich features to select a better output hypothesis within an n-best list or lattice. These models have a long history in NLP, and we revisit discriminative reranking for modern neural machine translation models by training a large transformer architecture. This takes as input both the source sentence as well as a list of hypotheses to output a ranked list. The reranker is trained to predict the observed distribution of a desired metric, e.g. BLEU, over the n-best list. Since such a discriminator contains hundreds of millions of parameters, we improve its generalization using pre-training and data augmentation techniques. Experiments on four WMT directions show that our discriminative reranking approach is effective and complementary to existing generative reranking approaches, yielding improvements of up to 4 BLEU over the beam search output.",
        "id":236460293
      },
      {
        "title":"Simple and Effective Noisy Channel Modeling for Neural Machine Translation",
        "text":"Previous work on neural noisy channel modeling relied on latent variable models that incrementally process the source and target sentence. This makes decoding decisions based on partial source prefixes even though the full source is available. We pursue an alternative approach based on standard sequence to sequence models which utilize the entire source. These models perform remarkably well as channel models, even though they have neither been trained on, nor designed to factor over incomplete target sentences. Experiments with neural language models trained on billions of words show that noisy channel models can outperform a direct model by up to 3.2 BLEU on WMT'17 German-English translation. We evaluate on four language-pairs and our channel models consistently outperform strong alternatives such right-to-left reranking models and ensembles of direct models. 1",
        "id":201058550
      }
    ],
    "negative_ctxs":[
      {
        "title":"D-STAG : un formalisme d'analyse automatique de discours fondé sur les TAG synchrones",
        "text":"Nous proposons D-STAG, un nouveau formalisme pour l'analyse automatique de la structure discursive des textes. Les analyses produites par D-STAG sont des structures de discours hiérarchiques annotées de relations de discours, qui sont compatibles avec les structures de discours produites en SDRT. L'analyse discursive prolonge l'analyse phrastique, sans modifier celle-ci, ce qui rend envisageable la mise en oeuvre d'un analyseur de discours.ABSTRACT. We propose D-STAG, a new formalism for the automatic analysis of the discourse structure of texts. The analyses computed by D-STAG are hierarchical discourse structures annotated with discourse relations, that are compatible with discourse structures computed in SDRT. The discourse analysis extends the sententiall analysis, without modifying it, which makes conceivable the realization of a discourse analyzer.",
        "id":12480349
      },
      {
        "title":"This Phrase-Based SMT System is Out of Order: Generalised Word Reordering in Machine Translation",
        "text":"Many natural language processes have some degree of preprocessing of data: tokenisation, stemming and so on. In the domain of Statistical Machine Translation it has been shown that word reordering as a preprocessing step can help the translation process.Recently, hand-written rules for reordering in German-English translation have shown good results, but this is clearly a labour-intensive and language pair-specific approach. Two possible sources of the observed improvement are that (1) the reordering explicitly matches the syntax of the source language more closely to that of the target language, or that (2) it fits the data better to the mechanisms of phrasal SMT; but it is not clear which. In this paper, we apply a general principle based on dependency distance minimisation to produce reorderings. Our languageindependent approach achieves half of the improvement of a reimplementation of the handcrafted approach, and suggests that reason (2) is a possible explanation for why that reordering approach works.Help you I can, yes.Jedi Master Yoda",
        "id":15272106
      },
      {
        "title":"Semantics, Discourse and Statistical Machine Translation",
        "text":"In the past decade, statistical machine translation (SMT) has been advanced from word-based SMT to phrase-and syntax-based SMT. Although this advancement produces significant improvements in BLEU scores, crucial meaning errors and lack of cross-sentence connections at discourse level still hurt the quality of SMT-generated translations. More recently, we have witnessed two active movements in SMT research: one towards combining semantics and SMT in attempt to generate not only grammatical but also meaningpreserved translations, and the other towards exploring discourse knowledge for document-level machine translation in order to capture intersentence dependencies.The emergence of semantic SMT are due to the combination of two factors: the necessity of semantic modeling in SMT and the renewed interest of designing models tailored to relevant NLP\/SMT applications in the semantics community. The former is represented by recent numerous studies on exploring word sense disambiguation, semantic role labeling, bilingual semantic representations as well as semantic evaluation for SMT. The latter is reflected in CoNLL shared tasks, SemEval and SenEval exercises in recent years.The need of capturing cross-sentence dependencies for document-level SMT triggers the resurgent interest of modeling translation from the perspective of discourse. Discourse phenomena, such as coherent relations, discourse topics, lexical cohesion that are beyond the scope of conventional sentence-level n-grams, have been recently considered and explored in the context of SMT.This tutorial aims at providing a timely and combined introduction of such recent work along these two trends as discourse is inherently connected with semantics. The tutorial has three parts. The first part critically reviews the phrase-and syntax-based SMT. The second part is devoted to the lines of research oriented to semantic SMT, including a brief introduction of semantics, lexical and shallow semantics tailored to SMT, semantic representations in SMT, semantically motivated evaluation as well as advanced topics on deep semantic learning for SMT. The third part is dedicated to recent work on SMT with discourse, including a brief review on discourse studies from linguistics and computational viewpoints, discourse research from monolingual to multilingual, discourse-based SMT and a few advanced topics.The tutorial is targeted for researchers in the SMT, semantics and discourse communities. In particular, the expected audience comes from two groups: 1) Researchers and students in the SMT community who want to design cutting-edge models and algorithms for semantic SMT with various semantic knowledge and representations, and who would like to advance SMT from sentence-bysentence translation to document-level translation with discourse information; 2) Researchers and students from the semantics and discourse community who are interested in developing models and methods and adapting them to SMT.OutlineSMT Overall Review (30 minutes)• SMT architecture • phrase-and syntax-based SMT 2. Semantics and SMT (1 hour and 15 minutes)",
        "id":6864375
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"What are some approaches to generating sports news reports from event data and what models have been used for this task in Finnish language NLP?",
    "positive_ctxs":[
      {
        "title":"Template-free Data-to-Text Generation of Finnish Sports News",
        "text":"News articles such as sports game reports are often thought to closely follow the underlying game statistics, but in practice they contain a notable amount of background knowledge, interpretation, insight into the game, and quotes that are not present in the official statistics. This poses a challenge for automated data-totext news generation with real-world news corpora as training data. We report on the development of a corpus of Finnish ice hockey news, edited to be suitable for training of end-to-end news generation methods, as well as demonstrate generation of text, which was judged by journalists to be relatively close to a viable product. The new dataset and system source code are available for research purposes. 1",
        "id":203734629
      }
    ],
    "negative_ctxs":[
      {
        "title":"Combining fast align with Hierarchical Sub-sentential Alignment for Better Word Alignments",
        "text":"fast align is a simple and fast word alignment tool which is widely used in state-of-the-art machine translation systems. It yields comparable results in the end-to-end translation experiments of various language pairs. However, fast align does not perform as well as GIZA++ when applied to language pairs with distinct word orders, like English and Japanese. In this paper, given the lexical translation table output by fast align, we propose to realign words using the hierarchical sub-sentential alignment approach. Experimental results show that simple additional processing improves the performance of word alignment, which is measured by counting alignment matches in comparison with fast align. We also report the result of final machine translation in both English-Japanese and Japanese-English. We show our best system provided significant improvements over the baseline as measured by BLEU and RIBES. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details:",
        "id":10441638
      },
      {
        "title":"Classifying Arabic Crisis Tweets using Data Selection and Pre-trained Language Models",
        "text":"User-generated Social Media (SM) content has been explored as a valuable and accessible source of data about crises to enhance situational awareness and support humanitarian response efforts. However, the timely extraction of crisis-related SM messages is challenging as it involves processing large quantities of noisy data in real-time. Supervised machine learning methods have been successfully applied to this task but such approaches require human-labelled data, which are unlikely to be available from novel and emerging crises. Supervised machine learning algorithms trained on labelled data from past events did not usually perform well when classifying a new disaster due to data variations across events. Using the BERT embeddings, we propose and investigate an instance distance-based data selection approach for adaptation to improve classifiers' performance under a domain shift. The K-nearest neighbours algorithm selects a subset of multi-event training data that is most similar to the target event. Results show that fine-tuning a BERT model on a selected subset of data to classify crisis tweets outperforms a model that has been fine-tuned on all available source data. We demonstrated that our approach generally works better than the self-training adaptation method. Combing the self-training with our proposed classifier does not enhance the performance.",
        "id":252624552
      },
      {
        "title":"DIP: Dead code Insertion based Black-box Attack for Programming Language Model",
        "text":"Automatic processing of source code, such as code clone detection and software vulnerability detection, is very helpful to software engineers. Large pre-trained Programming Language (PL) models (such as CodeBERT, Graph-CodeBERT, CodeT5, etc.), show very powerful performance on these tasks. However, these PL models are vulnerable to adversarial examples that are generated with slight perturbation. Unlike natural language, an adversarial example of code must be semantic-preserving and compilable. Due to the requirements, it is hard to directly apply the existing attack methods for natural language models. In this paper, we propose DIP (Dead code Insertion based Blackbox Attack for Programming Language Model), a high-performance and efficient black-box attack method to generate adversarial examples using dead code insertion. We evaluate our proposed method on 9 victim downstream-task large code models. Our method outperforms the state-of-the-art black-box attack in both attack efficiency and attack quality, while generated adversarial examples are compiled preserving semantic functionality.",
        "id":259370836
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"What are some good datasets for conversational question answering?",
    "positive_ctxs":[
      {
        "title":"CoQA: A Conversational Question Answering Challenge",
        "text":"Humans gather information through conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets (e.g., coreference and pragmatic reasoning). We evaluate strong dialogue and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4%, which is 23.4 points behind human performance (88.8%), indicating that there is ample room for improvement. We present CoQA as a challenge to the community at https:\/\/stanfordnlp.github. io\/coqa.",
        "id":52055325
      },
      {
        "title":"QuAC : Question Answering in Context",
        "text":"We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-ofthe-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is significant room for future work on this data. Dataset, baseline, and leaderboard available at http:\/\/quac.ai.",
        "id":52057510
      }
    ],
    "negative_ctxs":[
      {
        "title":"MARS: MARKOV MOLECULAR SAMPLING FOR MULTI-OBJECTIVE DRUG DISCOVERY",
        "text":"Searching for novel molecules with desired chemical properties is crucial in drug discovery. Existing work focuses on developing neural models to generate either molecular sequences or chemical graphs. However, it remains a big challenge to find novel and diverse compounds satisfying several properties. In this paper, we propose MARS, a method for multi-objective drug molecule discovery. MARS is based on the idea of generating the chemical candidates by iteratively editing fragments of molecular graphs. To search for high-quality candidates, it employs Markov chain Monte Carlo sampling (MCMC) on molecules with an annealing scheme and an adaptive proposal. To further improve sample efficiency, MARS uses a graph neural network (GNN) to represent and select candidate edits, where the GNN is trained on-the-fly with samples from MCMC. Experiments show that MARS achieves state-of-the-art performance in various multi-objective settings where molecular bio-activity, drug-likeness, and synthesizability are considered. Remarkably, in the most challenging setting where all four objectives are simultaneously optimized, our approach outperforms previous methods significantly in comprehensive evaluations. The code is available at",
        "id":232290577
      },
      {
        "title":"Generating Diverse Descriptions from Semantic Graphs",
        "text":"Text generation from semantic graphs is traditionally performed with deterministic methods, which generate a unique description given an input graph. However, the generation problem admits a range of acceptable textual outputs, exhibiting lexical, syntactic and semantic variation. To address this disconnect, we present two main contributions. First, we propose a stochastic graph-to-text model, incorporating a latent variable in an encoder-decoder model, and its use in an ensemble. Second, to assess the diversity of the generated sentences, we propose a new automatic evaluation metric which jointly evaluates output diversity and quality in a multi-reference setting. We evaluate the models on WebNLG datasets in English and Russian, and show an ensemble of stochastic models produces diverse sets of generated sentences, while retaining similar quality to state-of-the-art models.",
        "id":236986922
      },
      {
        "title":"Understanding Dataset Design Choices for Multi-hop Reasoning",
        "text":"Learning multi-hop reasoning has been a key challenge for reading comprehension models, leading to the design of datasets that explicitly focus on it. Ideally, a model should not be able to perform well on a multi-hop question answering task without doing multi-hop reasoning. In this paper, we investigate two recently proposed datasets, WikiHop (Welbl  et al., 2018)  and HotpotQA (Yang et al., 2018). First, we explore sentence-factored models for these tasks; by design, these models cannot do multi-hop reasoning, but they are still able to solve a large number of examples in both datasets. Furthermore, we find spurious correlations in the unmasked version of WikiHop, which make it easy to achieve high performance considering only the questions and answers. Finally, we investigate one key difference between these datasets, namely spanbased vs. multiple-choice formulations of the QA task. Multiple-choice versions of both datasets can be easily gamed, and two models we examine only marginally exceed a baseline in this setting. Overall, while these datasets are useful testbeds, high-performing models may not be learning as much multi-hop reasoning as previously thought.",
        "id":123758373
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"What are some of the key papers to look at for understanding how attention mechanisms have been used to enhance bidirectional recurrent neural networks in relation classification tasks?",
    "positive_ctxs":[
      {
        "title":"Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification",
        "text":"Relation classification is an important semantic processing task in the field of natural language processing (NLP). State-ofthe-art systems still rely on lexical resources such as WordNet or NLP systems like dependency parser and named entity recognizers (NER) to get high-level features. Another challenge is that important information can appear at any position in the sentence. To tackle these problems, we propose Attention-Based Bidirectional Long Short-Term Memory Networks(Att-BLSTM) to capture the most important semantic information in a sentence. The experimental results on the SemEval-2010 relation classification task show that our method outperforms most of the existing methods, with only word vectors.",
        "id":9870160
      }
    ],
    "negative_ctxs":[
      {
        "title":"Hybrid Approach to Zero Subject Resolution for multilingual MT -Spanish-to-Korean Cases",
        "text":"The current paper proposes a novel approach to Spanish",
        "id":5236680
      },
      {
        "title":"Character-level Representations Improve DRS-based Semantic Parsing Even in the Age of BERT",
        "text":"We combine character-level and contextual language model representations to improve performance on Discourse Representation Structure parsing. Character representations can easily be added in a sequence-to-sequence model in either one encoder or as a fully separate encoder, with improvements that are robust to different language models, languages and data sets. For English, these improvements are larger than adding individual sources of linguistic information or adding non-contextual embeddings. A new method of analysis based on semantic tags demonstrates that the character-level representations improve performance across a subset of selected semantic phenomena.",
        "id":226308548
      },
      {
        "title":"Universal Semantic Parsing",
        "text":"Universal Dependencies (UD) offer a uniform cross-lingual syntactic representation, with the aim of advancing multilingual applications. Recent work shows that semantic parsing can be accomplished by transforming syntactic dependencies to logical forms. However, this work is limited to English, and cannot process dependency graphs, which allow handling complex phenomena such as control. In this work, we introduce UDEPLAMBDA, a semantic interface for UD, which maps natural language to logical forms in an almost language-independent fashion and can process dependency graphs. We perform experiments on question answering against Freebase and provide German and Spanish translations of the WebQuestions and GraphQuestions datasets to facilitate multilingual evaluation. Results show that UDEPLAMBDA outperforms strong baselines across languages and datasets. For English, it achieves a 4.9 F 1 point improvement over the state-of-the-art on Graph-Questions.",
        "id":10436313
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"What are some recent advancements in training systems to parse complex multi-hop questions into a sequence of simpler query steps for improved question answering?",
    "positive_ctxs":[
      {
        "title":"BREAK It Down: A Question Understanding Benchmark",
        "text":"Understanding natural language questions entails the ability to break down a question into the requisite steps for computing its answer. In this work, we introduce a Question Decomposition Meaning Representation (QDMR) for questions. QDMR constitutes the ordered list of steps, expressed through natural language, that are necessary for answering a question. We develop a crowdsourcing pipeline, showing that quality QDMRs can be annotated at scale, and release the BREAK dataset, containing over 83K pairs of questions and their QDMRs. We demonstrate the utility of QDMR by showing that (a) it can be used to improve open-domain question answering on the HOTPOTQA dataset, (b) it can be deterministically converted to a pseudo-SQL formal language, which can alleviate annotation in semantic parsing applications. Last, we use BREAK to train a sequence-to-sequence model with copying that parses questions into QDMR structures, and show that it substantially outperforms several natural baselines.",
        "id":211003735
      }
    ],
    "negative_ctxs":[
      {
        "title":"Exploring Miscommunication and Collaborative Behaviour in Hu- man-Robot Interaction",
        "text":"This paper presents the first step in designing a speech-enabled robot that is capable of natural management of miscommunication. It describes the methods and results of two WOz studies, in which dyads of naïve participants interacted in a collaborative task. The first WOz study explored human miscommunication management. The second study investigated how shared visual space and monitoring shape the processes of feedback and communication in task-oriented interactions. The results provide insights for the development of human-inspired and robust natural language interfaces in robots.",
        "id":11259766
      },
      {
        "title":"Empathy Identification Systems are not Accurately Accounting for Context",
        "text":"Understanding empathy in text dialogue data is a difficult, yet critical, skill for effective human-machine interaction. In this work, we ask whether systems are making meaningful progress on this challenge. We consider a simple model that checks if an input utterance is similar to a small set of empathetic examples. Crucially, the model does not look at what the utterance is a response to, i.e., the dialogue context. This model performs comparably to prior work on standard benchmarks and even outperforms state-of-the-art models for empathetic rationale extraction by 16.7 points on T-F1 and 4.3 on IOU-F1. This indicates that current systems rely on the surface form of the response, rather than whether it is suitable in context. To confirm this, we create examples with dialogue contexts that change the interpretation of the response and show that current systems continue to label utterances as empathetic. We discuss the implications of our findings, including improvements for empathetic benchmarks and how our model can be an informative baseline.",
        "id":258378294
      },
      {
        "title":"An Open Web Platform for Rule-Based Speech-to-Sign Translation",
        "text":"We present an open web platform for developing, compiling, and running rulebased speech to sign language translation applications. Speech recognition is performed using the Nuance Recognizer 10.2 toolkit, and signed output, including both manual and non-manual components, is rendered using the JASigning avatar system. The platform is designed to make the component technologies readily accessible to sign language experts who are not necessarily computer scientists. Translation grammars are written in a version of Synchronous Context-Free Grammar adapted to the peculiarities of sign language. All processing is carried out on a remote server, with content uploaded and accessed through a web interface. Initial experiences show that simple translation grammars can be implemented on a time-scale of a few hours to a few days and produce signed output readily comprehensible to Deaf informants. Overall, the platform drastically lowers the barrier to entry for researchers interested in building applications that generate high-quality signed language.",
        "id":15635164
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"What are some soft-constrained methods proposed in the literature for terminology translation in neural machine translation systems, and how do they differ from hard-constrained decoding methods that might degrade translation quality or increase complexity?",
    "positive_ctxs":[
      {
        "title":"Training Neural Machine Translation To Apply Terminology Constraints",
        "text":"This paper proposes a novel method to inject custom terminology into neural machine translation at run time. Previous works have mainly proposed modifications to the decoding algorithm in order to constrain the output to include run-time-provided target terms. While being effective, these constrained decoding methods add, however, significant computational overhead to the inference step, and, as we show in this paper, can be brittle when tested in realistic conditions. In this paper we approach the problem by training a neural MT system to learn how to use custom terminology when provided with the input. Comparative experiments show that our method is not only more effective than a state-of-the-art implementation of constrained decoding, but is also as fast as constraint-free decoding.",
        "id":174798321
      },
      {
        "title":"PROMT Systems for WMT21 Terminology Translation Task",
        "text":"This paper describes the PROMT submissions for the WMT21 Terminology Translation Task. We participate in two directions: English to French and English to Russian. Our final submissions are MarianNMT-based neural systems. We present two technologies for terminology translation: a modification of the Dinu et al.(2019)soft-constrained approach and our own approach called PROMT Smart Neural Dictionary (SmartND). We achieve good results in both directions.",
        "id":245855898
      }
    ],
    "negative_ctxs":[
      {
        "title":"Valency Frames of Czech Verbs in VALLEX 1.0",
        "text":"The Valency Lexicon of Czech Verbs, Version 1.0 (VALLEX 1.0) is a collection of linguistically annotated data and documentation, resulting from an attempt at formal description of valency frames of Czech verbs. VALLEX 1.0 is closely related to Prague Dependency Treebank. In this paper, the context in which VALLEX came into existence is briefly outlined, and also three similar projects for English verbs are mentioned. The core of the paper is the description of the logical structure of the VALLEX data. Finally, we suggest a few directions of the future research.2 Besides VALLEX, a larger valency lexicon (called PDT-VALLEX,(Hajič et al., 2003)) has been created during the annotation of PDT. PDT-VALLEX contains more verbs (5200 verbs), but only frames occuring in PDT, whereas in VALLEX the verbs are analyzed in the whole complexity, in all their meanings. Moreover, richer information is assigned to particular valency frames in VALLEX.3",
        "id":1801774
      },
      {
        "title":"A New Minimally-Supervised Framework for Domain Word Sense Disambiguation",
        "text":"We present a new minimally-supervised framework for performing domain-driven Word Sense Disambiguation (WSD). Glossaries for several domains are iteratively acquired from the Web by means of a bootstrapping technique. The acquired glosses are then used as the sense inventory for fullyunsupervised domain WSD. Our experiments, on new and gold-standard datasets, show that our wide-coverage framework enables highperformance results on dozens of domains at a coarse and fine-grained level.",
        "id":16502540
      },
      {
        "title":"Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning",
        "text":"Unlike the problems of part-of-speech tagging and parsing, where commonly utilized training and test sets such as the Brown Corpus and Penn Treebank have existed for a number of years, evaluation of word sense disambiguation sytems is not yet standardized. In fact, most previous work in sense disambiguation has tended to use different sets of polysemous words, different sense inventories, different evaluation metrics and different test corpora. This working session will address these problems and seek solutions to them. Examples of issues for discussion include:• How should part-of-speech-level distinctions be treated when evaluating WSD systems?• How should sense inventories be defined so as not to be biased in favor of certain disambiguation methods, such as those based on selectional restriction, topic codes, hierarchical ontologies, or aligned multilingual corpora? Or are such biases ok?• What evaluation metrics are appropriate for the WSD ta~k?• What characteristics should common test suites exhibit? How and by whom should they be developed?• Would a MUC-style competitive evaluation program be beneficial or detrimental to progress in the WSD field?• What special problems exist when evaluating WSD performance on verbs?• What special problems exist when evaluating WSD performance in a multi-lingual setting?o What additional issues arise in evaluating more complex semantic tagging, going beyond sense disambiguation as traditionally defined?• How should regular polysemy and metaphor be treated in WSD evaluation?• Can a common evaluation framework satisfy the needs and limitations of both supervised and unsupervised sense disambiguation methods?ReferencesC. Leacock, G. Towell and E. Voorhees. 1993. Corpus-based statistical sense resolution.",
        "id":41603009
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"What are some studies that leverage statistical machine translation methodologies, like GIZA++, to improve the alignment process between words and grammar rules in the context of semantic parsing and generation of meaning representations?",
    "positive_ctxs":[
      {
        "title":"Learning for Semantic Parsing with Statistical Machine Translation",
        "text":"We present a novel statistical approach to semantic parsing, WASP, for constructing a complete, formal meaning representation of a sentence. A semantic parser is learned given a set of sentences annotated with their correct meaning representations. The main innovation of WASP is its use of state-of-the-art statistical machine translation techniques. A word alignment model is used for lexical acquisition, and the parsing model itself can be seen as a syntax-based translation model. We show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods requiring similar amount of supervision, and shows better robustness to variations in task complexity and word order.",
        "id":7785983
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":233365154
      },
      {
        "title":"Gene-disease association extraction by text mining and network analysis",
        "text":"Biomedical relations play an important role in biological processes. In this work, we combine information filtering, grammar parsing and network analysis for gene-disease association extraction. The proposed method first extracts sentences potentially containing information about gene-diseases interactions based on maximum entropy classifier with topic features. And then Probabilistic Context-Free Grammars is applied for gene-disease association extraction. The network of genes and the disease is constituted by the extracted interactions, network centrality metrics are used for calculating the importance of each gene. We used breast cancer as testing disease for system evaluation. The 31 top ranked genes and diseases by the weighted degree, betweenness, and closeness centralities have been checked relevance with breast cancer through NCBI database. The evaluation showed 83.9% accuracy for the testing genes and diseases, 74.2% accuracy for the testing genes.",
        "id":18012852
      },
      {
        "title":"",
        "text":"",
        "id":219302391
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"What are some techniques or tools used in machine learning for matching and grounding annotated data to an existing knowledge base, particularly using sentence embedding-based cosine similarity or clustering algorithms?",
    "positive_ctxs":[
      {
        "title":"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
        "text":"BERT(Devlin et al., 2018)and RoBERTa (Liu  et al., 2019)  has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT \/ RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods. 1",
        "id":201646309
      }
    ],
    "negative_ctxs":[
      {
        "title":"Subword-based Compact Reconstruction of Word Embeddings",
        "text":"The idea of subword-based word embeddings has been proposed in the literature, mainly for solving the out-of-vocabulary (OOV) word problem observed in standard word-based word embeddings. In this paper, we propose a method of reconstructing pre-trained word embeddings using subword information that can effectively represent a large number of subword embeddings in a considerably small fixed space. The key techniques of our method are twofold: memory-shared embeddings and a variant of the key-value-query self-attention mechanism. Our experiments show that our reconstructed subword-based embeddings can successfully imitate well-trained word embeddings in a small fixed space while preventing quality degradation across several linguistic benchmark datasets, and can simultaneously predict effective embeddings of OOV words. We also demonstrate the effectiveness of our reconstruction method when we apply them to downstream tasks 1 .",
        "id":174799852
      },
      {
        "title":"Selective-LAMA: Selective Prediction for Confidence-Aware Evaluation of Language Models",
        "text":"Recent studies have suggested that neural language models learn and store a large amount of facts and commonsense knowledge from training data. The ability of language models to restore such knowledge is often evaluated via zero-shot cloze-style QA tasks. However, such evaluations rely only on prediction accuracy without punishing the systems for their mistakes, e.g., simply guessing or hallucinating likely answers. Selective prediction is a more informative evaluation framework that takes the confidence of predictions into account. Under the selective prediction setting, a model is evaluated not only by the number of correct predictions, but also by the ability to filter out dubious predictions by estimating the confidence of individual predictions. Such confidence-aware evaluation is crucial for determining whether to trust zero-shot predictions of language models. In this paper, we apply the selective prediction setting to an existing benchmark, LAMA probe, and conduct extensive experiments with recent neural language models and different confidence functions. We empirically show that our Selective-LAMA evaluation is more robust to the effect of simple guesses than the conventional accuracy-based evaluation. Our evaluation reveals the importance of the choice of confidence functions by showing that simply relying on token probabilities is not always the best choice. Further analysis shows that various confidence functions exhibit different preferences over predicted tokens for a given context.",
        "id":258378336
      },
      {
        "title":"THE KANERVA MACHINE: A GENERATIVE DISTRIBUTED MEMORY",
        "text":"We present an end-to-end trained memory system that quickly adapts to new data and generates samples like them. Inspired by Kanerva's sparse distributed memory, it has a robust distributed reading and writing mechanism. The memory is analytically tractable, which enables optimal on-line compression via a Bayesian update-rule. We formulate it as a hierarchical conditional generative model, where memory provides a rich data-dependent prior distribution. Consequently, the top-down memory and bottom-up perception are combined to produce the code representing an observation. Empirically, we demonstrate that the adaptive memory significantly improves generative models trained on both the Omniglot and CIFAR datasets. Compared with the Differentiable Neural Computer (DNC) and its variants, our memory model has greater capacity and is significantly easier to train.",
        "id":3524564
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"What are the recent developments in evaluating the flow or 'streaming degree' of the translation processes in simultaneous machine translation (SiMT), and which metric has proven useful for this purpose?",
    "positive_ctxs":[
      {
        "title":"Learning to Translate in Real-time with Neural Machine Translation",
        "text":"Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively. 1",
        "id":2782776
      }
    ],
    "negative_ctxs":[
      {
        "title":"Sentiment Analysis on Naija-Tweets",
        "text":"Examining sentiments in social media poses a challenge to natural language processing because of the intricacy and variability in the dialect articulation, noisy terms in form of slang, abbreviation, acronym, emoticon, and spelling error coupled with the availability of real-time content. Moreover, most of the knowledgebased approaches for resolving slang, abbreviation, and acronym do not consider the issue of ambiguity that evolves in the usage of these noisy terms. This research work proposes an improved framework for social media feed pre-processing that leverages on the combination of integrated local knowledge bases and adapted Lesk algorithm to facilitate pre-processing of social media feeds. The results from the experimental evaluation revealed an improvement over existing methods when applied to supervised learning algorithms in the task of extracting sentiments from Nigeria-origin tweets with an accuracy of 99.17%.",
        "id":196205333
      },
      {
        "title":"A Linear Programming Formulation for Global Inference in Natural Language Tasks",
        "text":"Given a collection of discrete random variables representing outcomes of learned local predictors in natural language, e.g., named entities and relations, we seek an optimal global assignment to the variables in the presence of general (non-sequential) constraints. Examples of these constraints include the type of arguments a relation can take, and the mutual activity of different relations, etc. We develop a linear programming formulation for this problem and evaluate it in the context of simultaneously learning named entities and relations. Our approach allows us to efficiently incorporate domain and task specific constraints at decision time, resulting in significant improvements in the accuracy and the \"human-like\" quality of the inferences.",
        "id":10048734
      },
      {
        "title":"",
        "text":"",
        "id":218974364
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"What paper should I look at if I am interested in the challenges of compositional generalization in the context of semantic parsing, especially regarding the impact of unseen local structures in program outputs?",
    "positive_ctxs":[
      {
        "title":"Unobserved Local Structures Make Compositional Generalization Hard",
        "text":"While recent work has shown that sequence-tosequence models struggle to generalize to new compositions (termed compositional generalization), little is known on what makes compositional generalization hard on a particular test instance. In this work, we investigate the factors that make generalization to certain test instances challenging. We first substantiate that some examples are more difficult than others by showing that different models consistently fail or succeed on the same test instances. Then, we propose a criterion for the difficulty of an example: a test instance is hard if it contains a local structure that was not observed at training time. We formulate a simple decision rule based on this criterion and empirically show it predicts instance-level generalization well across 5 different semantic parsing datasets, substantially better than alternative decision rules. Last, we show local structures can be leveraged for creating difficult adversarial compositional splits and also to improve compositional generalization under limited training budgets by strategically selecting examples for the training set.",
        "id":246015393
      }
    ],
    "negative_ctxs":[
      {
        "title":"An Effective, Performant Named Entity Recognition System for Noisy Business Telephone Conversation Transcripts",
        "text":"We present a simple yet effective method to train a named entity recognition (NER) model that operates on business telephone conversation transcripts that contain noise due to the nature of spoken conversation and artifacts of automatic speech recognition. We first fine-tune LUKE, a state-of-the-art Named Entity Recognition (NER) model, on a limited amount of transcripts, then use it as the teacher model to teach a smaller DistilBERT-based student model using a large amount of weakly labeled data and a small amount of human-annotated data. The model achieves high accuracy while also satisfying the practical constraints for inclusion in a commercial telephony product: realtime performance when deployed on costeffective CPUs rather than GPUs.",
        "id":252568200
      },
      {
        "title":"Representational and architectural issues in a limited-domain medical speech translator Mots-clefs : reconnaissance de la parole, traduction de la parole, aide au diagnostic médical",
        "text":"Cet article dresse un aperçu du système MedSLT, un système de traduction de la parole dans le domaine médical pour un vocabulaire limité. Il met l'accent sur le problème du choix du type de représentation pour les constructions temporelles et causales. Nous montrons que celles-ci ne peuvent pas être représentées par des structures plates, généralement utilisées pour ce type d'application, mais qu'elles nécessitent des stuctures plus riches, enchâssées, qui permettent d'obtenir une traduction plus adéquate. Nous expliquons comment produire ces représentations et écrire des règles de traduction économiques qui mettent en correspondance les représentations sources dans la représentation interlingue correspondante Abstract We present an overview of MedSLT, a medium-vocabulary medical speech translation system, focussing on the representational issues that arise when translating temporal and causal concepts. Although flat key\/value structures are strongly preferred as semantic representations in speech understanding systems, we argue that it is infeasible to handle the necessary range of concepts using only flat structures. By exploiting the specific nature of the task, we show that it is possible to implement a solution which only slightly extends the representational complexity of the semantic representation language, by permitting an optional single nested level representing a subordinate clause construct. We sketch our solutions to the key problems of producing minimally nested representations using phrase-spotting methods, and writing cleanly structured rule-sets that map temporal and phrasal representations into a canonical interlingual form.Rayner, Bouillon, Santaholma and Nakao",
        "id":15377337
      },
      {
        "title":"Passing a USA National Bar Exam: a First Corpus for Experimentation",
        "text":"Bar exams provide a key watershed by which legal professionals demonstrate their knowledge of the law and its application. Passing the bar entitles one to practice the law in a given jurisdiction. The bar provides an excellent benchmark for the performance of legal information systems since passing the bar would arguably signal that the system has acquired key aspects of legal reason on a par with a human lawyer. The paper provides a corpus and experimental results with material derived from a real bar exam, treating the problem as a form of textual entailment from the question to an answer. The providers of the bar exam material set the Gold Standard, which is the answer key. The experiments carried out using the 'out of the box' the Excitement Open Platform for textual entailment. The results and evaluation show that the tool can identify wrong answers (non-entailment) with a high F1 score, but it performs poorly in identifying the correct answer (entailment). The results provide a baseline performance measure against which to evaluate future improvements. The reasons for the poor performance are examined, and proposals are made to augment the tool in the future. The corpus facilitates experimentation by other researchers.",
        "id":12705175
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"What papers should I refer to if I want to explore datasets and shared tasks designed for the purpose of fact verification using evidence from Wikipedia?",
    "positive_ctxs":[
      {
        "title":"FEVER: a large-scale dataset for Fact Extraction and VERification",
        "text":"In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from.The claims are classified as SUPPORTED, RE-FUTED or NOTENOUGHINFO by annotators achieving 0.6841 in Fleiss κ. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87%, while if we ignore the evidence we achieve 50.91%. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.",
        "id":4711425
      }
    ],
    "negative_ctxs":[
      {
        "title":"A Large Scale Arabic Sentiment Lexicon for Arabic Opinion Mining",
        "text":"Most opinion mining methods in English rely successfully on sentiment lexicons, such as English SentiWordnet (ESWN). While there have been efforts towards building Arabic sentiment lexicons, they suffer from many deficiencies: limited size, unclear usability plan given Arabic's rich morphology, or nonavailability publicly. In this paper, we address all of these issues and produce the first publicly available large scale Standard Arabic sentiment lexicon (Ar-SenL) using a combination of existing resources: ESWN, Arabic WordNet, and the Standard Arabic Morphological Analyzer (SAMA). We compare and combine two methods of constructing this lexicon with an eye on insights for Arabic dialects and other low resource languages. We also present an extrinsic evaluation in terms of subjectivity and sentiment analysis.",
        "id":16203230
      },
      {
        "title":"Zipfian corruptions for robust POS tagging",
        "text":"Inspired by robust generalization and adversarial learning we describe a novel approach to learning structured perceptrons for part-ofspeech (POS) tagging that is less sensitive to domain shifts. The objective of our method is to minimize average loss under random distribution shifts. We restrict the possible target distributions to mixtures of the source distribution and random Zipfian distributions. Our algorithm is used for POS tagging and evaluated on the English Web Treebank and the Danish Dependency Treebank with an average 4.4% error reduction in tagging accuracy.",
        "id":6328989
      },
      {
        "title":"On the Structural Complexity of Natural Language Sentences",
        "text":"Artificial I ntclligcn cc I,a, bovatory Ma.ss~cclmsetts Insdtttt(; of :l~cttnology l{m 7(57, 545 Technology Square (~atnbridgc, Massac.husetts, USA, 02] 239AbstractThe objective of this pal)er is to [brmalize the intuition al)out l,he comph;xity of syntactic structures. We propose a definition of structm:al COml)h'xity such that sentences ranked by our definition as more COml)h;x are gen(;rally more diI'ficult lbr humans to process. We justify the definition by showing how it is ahle to account for several seemingly unrelated phenomena in natural languages.",
        "id":7424558
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"What prior works suggested that exposure bias could lead to hallucinations in neural machine translation models?",
    "positive_ctxs":[
      {
        "title":"On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation",
        "text":"The standard training algorithm in neural machine translation (NMT) suffers from exposure bias, and alternative algorithms have been proposed to mitigate this. However, the practical impact of exposure bias is under debate. In this paper, we link exposure bias to another well-known problem in NMT, namely the tendency to generate hallucinations under domain shift. In experiments on three datasets with multiple test domains, we show that exposure bias is partially to blame for hallucinations, and that training with Minimum Risk Training, which avoids exposure bias, can mitigate this. Our analysis explains why exposure bias is more problematic under domain shift, and also links exposure bias to the beam search problem, i.e. performance deterioration with increasing beam size. Our results provide a new justification for methods that reduce exposure bias: even if they do not increase performance on in-domain test sets, they can increase model robustness to domain shift.",
        "id":218538004
      },
      {
        "title":"Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation",
        "text":"In Neural Machine Translation (and, more generally, conditional language modeling), the generation of a target token is influenced by two types of context: the source and the prefix of the target sequence. While many attempts to understand the internal workings of NMT models have been made, none of them explicitly evaluates relative source and target contributions to a generation decision. We argue that this relative contribution can be evaluated by adopting a variant of Layerwise Relevance Propagation (LRP). Its underlying 'conservation principle' makes relevance propagation unique: differently from other methods, it evaluates not an abstract quantity reflecting token importance, but the proportion of each token's influence. We extend LRP to the Transformer and conduct an analysis of NMT models which explicitly evaluates the source and target relative contributions to the generation process. We analyze changes in these contributions when conditioning on different types of prefixes, when varying the training objective or the amount of training data, and during the training process. We find that models trained with more data tend to rely on source information more and to have more sharp token contributions; the training process is non-monotonic with several stages of different nature. 1",
        "id":224818197
      }
    ],
    "negative_ctxs":[
      {
        "title":"Domain-Specific Image Captioning",
        "text":"We present a data-driven framework for image caption generation which incorporates visual and textual features with varying degrees of spatial structure. We propose the task of domain-specific image captioning, where many relevant visual details cannot be captured by off-the-shelf general-domain entity detectors. We extract previously-written descriptions from a database and adapt them to new query images, using a joint visual and textual bag-of-words model to determine the correctness of individual words. We implement our model using a large, unlabeled dataset of women's shoes images and natural language descriptions (Berg et al., 2010). Using both automatic and human evaluations, we show that our captioning method effectively deletes inaccurate words from extracted captions while maintaining a high level of detail in the generated output.Query Image GIST Nearest-NeighborExtraction: Classic ballet flats with decorative canvas strap and patent leather covered buckle.Compression: Classic ballet flats covered.Query Image GIST Nearest-NeighborExtraction: This shoe is the perfect shoe for you , featuring an open toe and a lace up upper with a high heel , and a two tone color .Compression: This shoe is the shoe , featuring an open toe and upper with a high heel .",
        "id":16430728
      },
      {
        "title":"Some Properties of Preposition and Subordinate Conjunction Attachments*",
        "text":"Determining the attachments of prepositions and subordinate conjunctions is a key problem in parsing natural language. This paper presents a trainable approach to making these attachments through transformation sequences and error-driven learning. Our approach is broad coverage, and accounts for roughly three times the attachment cases that have previously been handled by corpus-based techniques. In addition, our approach is based on a simplified model of syntax that is more consistent with the practice in current state-of-the-art language processing systems. This paper sketches syntactic and algorithmic details, and presents experimental results on data sets derived from the Penn Treebank. We obtain an attachment accuracy of 75.4% for the general case, the first such corpus-based result to be reported. For the restricted cases previously studied with corpusbased methods, our approach yields an accuracy comparable to current work (83.1%).",
        "id":3264475
      },
      {
        "title":"Prepositions in Applications: A Survey and Introduction to the Special Issue",
        "text":"",
        "id":6602375
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"What research could I reference to understand the methodology for binarizing non-binary subtrees in the context of discourse parsing?",
    "positive_ctxs":[
      {
        "title":"A Classifier-Based Parser with Linear Run-Time Complexity",
        "text":"We present a classifier-based parser that produces constituent trees in linear time. The parser uses a basic bottom-up shiftreduce algorithm, but employs a classifier to determine parser actions instead of a grammar. This can be seen as an extension of the deterministic dependency parser ofNivre and Scholz (2004)to full constituent parsing. We show that, with an appropriate feature set used in classification, a very simple one-path greedy parser can perform at the same level of accuracy as more complex parsers. We evaluate our parser on section 23 of the WSJ section of the Penn Treebank, and obtain precision and recall of 87.54% and 87.61%, respectively.",
        "id":173611
      }
    ],
    "negative_ctxs":[
      {
        "title":"Unsupervised Information Extraction: Regularizing Discriminative Approaches with Relation Distribution Losses",
        "text":"Unsupervised relation extraction aims at extracting relations between entities in text. Previous unsupervised approaches are either generative or discriminative. In a supervised setting, discriminative approaches, such as deep neural network classifiers, have demonstrated substantial improvement. However, these models are hard to train without supervision. To overcome this limitation, we introduce two losses on the predicted relations distribution. These losses improve the performance of discriminative based models, and enable us to train deep neural networks satisfactorily, surpassing current state of the art on three different datasets.",
        "id":196179979
      },
      {
        "title":"Automatic Detection of Well Recognized Words in Automatic Speech Transcriptions",
        "text":"This work adresses the use of confidence measures for extracting well recognized words with very low error rate from automatically transcribed segments in a unsupervised way. We present and compare several confidence measures and propose a method to merge them into a new one. We study its capabilities on extracting correct recognized word-segments compared to the amount of rejected words. We apply this fusion measure to select audio segments composed of words with a high confidence score. These segments come from an automatic transcription of french broadcast news given by our speech recognition system based on the CMU Sphinx3.3 decoder. Injecting new data resulting from unsupervised treatments of raw audio recordings in the training corpus of acoustic models gives statistically significant improvement (95% confident interval) in terms of word error rate. Experiments have been carried out on the corpus used during ESTER, the french evaluation campaign.",
        "id":18656920
      },
      {
        "title":"",
        "text":"",
        "id":199022739
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"What research has been done on annotating user comments with claim verifiability",
    "positive_ctxs":[
      {
        "title":"Identifying Appropriate Support for Propositions in Online User Comments",
        "text":"The ability to analyze the adequacy of supporting information is necessary for determining the strength of an argument. 1 This is especially the case for online user comments, which often consist of arguments lacking proper substantiation and reasoning. Thus, we develop a framework for automatically classifying each proposition as UNVERIFIABLE, VERIFIABLE NON-EXPERIENTIAL, or VERIFIABLE EXPE-RIENTIAL 2 , where the appropriate type of support is reason, evidence, and optional evidence, respectively 3 . Once the existing support for propositions are identified, this classification can provide an estimate of how adequately the arguments have been supported. We build a goldstandard dataset of 9,476 sentences and clauses from 1,047 comments submitted to an eRulemaking platform and find that Support Vector Machine (SVM) classifiers trained with n-grams and additional features capturing the verifiability and experientiality exhibit statistically significant improvement over the unigram baseline, achieving a macro-averaged F 1 of 68.99%.",
        "id":14764893
      }
    ],
    "negative_ctxs":[
      {
        "title":"Wronging a Right: Generating Better Errors to Improve Grammatical Error Detection",
        "text":"Grammatical error correction, like other machine learning tasks, greatly benefits from large quantities of high quality training data, which is typically expensive to produce. While writing a program to automatically generate realistic grammatical errors would be difficult, one could learn the distribution of naturallyoccurring errors and attempt to introduce them into other datasets. Initial work on inducing errors in this way using statistical machine translation has shown promise; we investigate cheaply constructing synthetic samples, given a small corpus of human-annotated data, using an off-the-rack attentive sequence-to-sequence model and a straight-forward post-processing procedure. Our approach yields error-filled artificial data that helps a vanilla bi-directional LSTM to outperform the previous state of the art at grammatical error detection, and a previously introduced model to gain further improvements of over 5% F 0.5 score. When attempting to determine if a given sentence is synthetic, a human annotator at best achieves 39.39 F 1 score, indicating that our model generates mostly human-like instances.",
        "id":52896498
      },
      {
        "title":"SJTU-NLP at SemEval-2018 Task 9: Neural Hypernym Discovery with Term Embeddings",
        "text":"This paper describes a hypernym discovery system for our participation in the SemEval-2018 Task 9, which aims to discover the best (set of) candidate hypernyms for input concepts or entities, given the search space of a pre-defined vocabulary. We introduce a neural network architecture for the concerned task and empirically study various neural network models to build the representations in latent space for words and phrases. The evaluated models include convolutional neural network, long-short term memory network, gated recurrent unit and recurrent convolutional neural network. We also explore different embedding methods, including word embedding and sense embedding for better performance.",
        "id":44130548
      },
      {
        "title":"",
        "text":"",
        "id":3194221
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"What research has been done on improving named entity recognition tasks by augmenting them with retrieval of external contexts?",
    "positive_ctxs":[
      {
        "title":"Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning",
        "text":"Recent advances in Named EntityRecognition (NER)show that document-level contexts can significantly improve model performance. In many application scenarios, however, such contexts are not available. In this paper, we propose to find external contexts of a sentence by retrieving and selecting a set of semantically relevant texts through a search engine, with the original sentence as the query. We find empirically that the contextual representations computed on the retrieval-based input view, constructed through the concatenation of a sentence and its external contexts, can achieve significantly improved performance compared to the original input view based only on the sentence. Furthermore, we can improve the model performance of both input views by Cooperative Learning, a training method that encourages the two input views to produce similar contextual representations or output label distributions. Experiments show that our approach can achieve new state-of-the-art performance on 8 NER data sets across 5 domains. 1",
        "id":234337605
      }
    ],
    "negative_ctxs":[
      {
        "title":"ETH-DS3Lab at SemEval-2018 Task 7: Effectively Combining Recurrent and Convolutional Neural Networks for Relation Classification and Extraction",
        "text":"Reliably detecting relevant relations between entities in unstructured text is a valuable resource for knowledge extraction, which is why it has awaken significant interest in the field of Natural Language Processing. In this paper, we present a system for relation classification and extraction based on an ensemble of convolutional and recurrent neural networks that ranked first in 3 out of the 4 Subtasks at Se-mEval 2018 Task 7. We provide detailed explanations and grounds for the design choices behind the most relevant features and analyze their importance.",
        "id":4696194
      },
      {
        "title":"Reorder and then Parse, Fast and Accurate Discontinuous Constituency Parsing",
        "text":"Discontinuous constituency parsing is still kept developing for its efficiency and accuracy are far behind its continuous counterparts. Motivated by the observation that a discontinuous constituent tree can be simply transformed into a pseudo-continuous one by artificially reordering words in the sentence, we propose a novel reordering method, thereby construct fast and accurate discontinuous constituency parsing systems working in continuous way. Specifically, we model the relative position changes of words as a list of actions. By parsing and performing this actions, the corresponding pseudocontinuous sequence is derived. Discontinuous parse tree can be further inferred via integrating a high-performance pseudo-continuous constituency parser. Our systems are evaluated on three classical discontinuous constituency treebanks, achieving new state-of-the-art on two treebanks and showing a distinct advantage in speed.",
        "id":256460951
      },
      {
        "title":"Methods to Optimize Wav2Vec with Language Model for Automatic Speech Recognition in Resource Constrained Environment",
        "text":"Automatic Speech Recognition (ASR) on resource constrained environment is a complex task since most of the State-Of-The-Art models are combination of multilayered convolutional neural network (CNN) and Transformer models which itself requires huge resources such as GPU or TPU for training as well as inference. The accuracy as a performance metric of an ASR system depends upon the efficiency of phonemes to word translation of an Acoustic Model and context correction of the Language model. However, inference as a performance metric is also an important aspect, which mostly depends upon the resources. Also, most of the ASR models uses transformer models at its core and one caveat of transformers is that it usually has a finite amount of sequence length it can handle. Either because it uses position encodings or simply because the cost of attention in transformers is actually O(n²) in sequence length, meaning that using very large sequence length explodes in complexity\/memory. So you cannot run the system with finite hardware even a very high-end GPU, because if we inference even a one hour long audio with Wav2Vec the system will crash. In this paper, we used some state-of-the-art methods to optimize the Wav2Vec model for better accuracy of predictions in resource constrained systems. In addition, we have performed tests with other SOTA models such as Citrinet and Quartznet for the comparative analysis.",
        "id":257767721
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"What research should I explore to understand methods for matching word senses in contexts using embeddings without relying on external lexical resources?",
    "positive_ctxs":[
      {
        "title":"Making Sense of Word Embeddings",
        "text":"We present a simple yet effective approach for learning word sense embeddings. In contrast to existing techniques, which either directly learn sense representations from corpora or rely on sense inventories from lexical resources, our approach can induce a sense inventory from existing word embeddings via clustering of ego-networks of related words. An integrated WSD mechanism enables labeling of words in context with learned sense vectors, which gives rise to downstream applications. Experiments show that the performance of our method is comparable to state-of-the-art unsupervised WSD systems.Related WorkOur method learns multi-prototype word embeddings and applies them to WSD. Below we briefly review both strains of research.Multi-Prototype Word Vector SpacesIn his pioneering work, Schütze (1998) induced sparse sense vectors by clustering context vectors using the EM algorithm. This approach is fitted with a similarity-based WSD mechanism. Later, Reisinger and Mooney (2010) presented a multiprototype vector space. Sparse TF-IDF vectors are clustered using a parametric method fixing the same number of senses for all words. Sense vectors are centroids of the clusters.While most dense word vector models represent a word with a single vector and thus conflate senses(Mikolov et al., 2013;Pennington et al., 2014), there are several approaches that produce word sense embeddings. Huang et al.(2012)learn arXiv:1708.03390v1 [cs.CL] 10 Aug 2017 Calculate Word Similarity Graph Learning Word Vectors Word Sense Induction Text Corpus Word Vectors Word Similarity Graph Pooling of Word Vectors Sense Inventory Sense Vectors 1 2 4 3",
        "id":5999791
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":218977370
      },
      {
        "title":"A Language Model for Spell Checking of Educational Texts in Kurdish (Sorani)",
        "text":"Spell checkers have become regular features of most word processing applications. They assist us in writing more correctly in various digital environments. However, this assistance does not exist for all languages equally. The Kurdish language, which still is considered a less-resourced language, currently, lacks well-known and well-tested spell checkers. We present a language model for the Kurdish (Sorani) based on educational texts written in the Persian\/Arabic script. We also showcase a spell checker as a testing environment for the language model. Primarily, we use a probabilistic method and our language model with Stupid Backoff smoothing for the spell-checking algorithm. We test for spelling errors on a word and context basis. The spell checker suggests a list of corrections for misspelled words. The results show 88.54% accuracy on the texts in the related context, an F1 score of 43.33%, and correct suggestions of an 85% chance of being in the top three positions of the corrections.",
        "id":252182458
      },
      {
        "title":"Arabizi Detection and Conversion to Arabic",
        "text":"Arabizi is Arabic text that is written using Latin characters. Arabizi is used to present both Modern Standard Arabic (MSA) or Arabic dialects. It is commonly used in informal settings such as social networking sites and is often with mixed with English. In this paper we address the problems of: identifying Arabizi in text and converting it to Arabic characters. We used word and sequence-level features to identify Arabizi that is mixed with English. We achieved an identification accuracy of 98.5%. As for conversion, we used transliteration mining with language modeling to generate equivalent Arabic text. We achieved 88.7% conversion accuracy, with roughly a third of errors being spelling and morphological variants of the forms in ground truth.",
        "id":9911858
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"When using pretrained transformer models for generating sentence embeddings, I've heard different strategies such as mean pooling and using the CLS token's embedding. What study shows that mean pooling outperforms CLS in semantic similarity tasks?",
    "positive_ctxs":[
      {
        "title":"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
        "text":"BERT(Devlin et al., 2018)and RoBERTa (Liu  et al., 2019)  has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT \/ RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods. 1",
        "id":201646309
      }
    ],
    "negative_ctxs":[
      {
        "title":"FontLex: A Typographical Lexicon based on Affective Associations",
        "text":"The task of selecting suitable fonts for a given text is non-trivial, as tens of thousands of fonts are available, and the choice of font has been shown to affect the perception of the text as well as of the author or of the brand being advertized. Aiming to support the development of font recommendation tools, we create a typographical lexicon providing associations between words and fonts. We achieve this by means of affective evocations, making use of font-emotion and word-emotion relationships. For this purpose, we first determine font vectors for a set of ten emotion attributes, based on word similarities and antonymy information. We evaluate these associations through a user study via Mechanical Turk, which, for eight of the ten emotions, shows a strong user preference towards the fonts that are found to be congruent by our predicted data. Subsequently, this data is used to calculate font vectors for specific words, by relying on the emotion associations of a given word. This leads to a set of font associations for 6.4K words. We again evaluate the resulting dataset using Mechanical Turk, on 25 randomly sampled words. For the majority of these words, the responses indicate that fonts with strong associations are preferred, and for all except 2 words, fonts with weak associations are dispreferred. Finally, we further extend the dataset using synonyms of font attributes and emotion names. The resulting FontLex resource provides mappings between 6.7K words and 200 fonts.",
        "id":21691078
      },
      {
        "title":"A Multiword Expression Data Set: Annotating Non-Compositionality and Conventionalization for English Noun Compounds",
        "text":"Scarcity of multiword expression data sets raises a fundamental challenge to evaluating the systems that deal with these linguistic structures. In this work we attempt to address this problem for a subclass of multiword expressions by producing a large data set annotated by experts and validated by common statistical measures. We present a set of 1048 noun-noun compounds annotated as non-compositional, compositional, conventionalized and not conventionalized. We build this data set following common trends in previous work while trying to address some of the well known issues such as small number of annotated instances, quality of the annotations, and lack of availability of true negative instances.",
        "id":345344
      },
      {
        "title":"Character-Level Linguistic Features Extraction for Text-to-Speech System",
        "text":"摘要 優良的語言文脈訊息是語音合成的關鍵部分，傳統的文脈訊息都是依賴於自然語言處理 (Natural Language Processing，NLP)，使用 parser 分析文字。但是 parser 設計困難無法 專門為語音合成設計；所以我們想直接以字元為處理單元建立一個 end-to-end 的語音合 成系統，在這想法下我們改用字元層級(character-level)的 word2vec 與遞迴類神經網路， 直接將輸入字元序列轉換成隱藏特徵向量當做語言合成的文脈訊息。最後我們利用一中 英夾雜語音合成系統測試此想法，語音合成的實驗的結果表明，我們提出的方式的確比 傳統使用 parse 的方式有更好的性能。 關鍵詞：語音合成、語言特徵、文脈訊息",
        "id":23967599
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"Where can I find a corpus of CCG annotations for natural language processing tasks, and what notable work has leveraged this corpus specifically in the domain of supertagging?",
    "positive_ctxs":[
      {
        "title":"Supertagging with LSTMs",
        "text":"In this paper we present new state-of-the-art performance on CCG supertagging and parsing. Our model outperforms existing approaches by an absolute gain of 1.5%. We analyze the performance of several neural models and demonstrate that while feed-forward architectures can compete with bidirectional LSTMs on POS tagging, models that encode the complete sentence are necessary for the long range syntactic information encoded in supertags.",
        "id":11771220
      },
      {
        "title":"CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank",
        "text":"This article presents an algorithm for translating the Penn Treebank into a corpus ofCombinatory Categorial Grammar (CCG)derivations augmented with local and long-range word-word dependencies. The resulting corpus, CCGbank, includes 99.4% of the sentences in the Penn Treebank. It is available from the Linguistic Data Consortium, and has been used to train widecoverage statistical parsers that obtain state-of-the-art rates of dependency recovery.In order to obtain linguistically adequate CCG analyses, and to eliminate noise and inconsistencies in the original annotation, an extensive analysis of the constructions and annotations in the Penn Treebank was called for, and a substantial number of changes to the Treebank were necessary. We discuss the implications of our findings for the extraction of other linguistically expressive grammars from the Treebank, and for the design of future treebanks.subcorpus of Wall Street Journal text that has become the de facto standard training and test data for statistical parsers. Its annotation, which is based on generic phrasestructure grammar (with coindexed traces and other null elements indicating non-local dependencies) and function tags on nonterminal categories providing (a limited degree of) syntactic role information, is designed to facilitate the extraction of the underlying predicate-argument structure. Statistical parsing on the Penn Treebank has made great progress by focusing on the machine-learning or algorithmic aspects(Magerman 1994;",
        "id":1331239
      }
    ],
    "negative_ctxs":[
      {
        "title":"Robustness Challenges in Model Distillation and Pruning for Natural Language Understanding",
        "text":"Recent work has focused on compressing pretrained language models (PLMs) like BERT where the major focus has been to improve the in-distribution performance for downstream tasks. However, very few of these studies have analyzed the impact of compression on the generalizability and robustness of compressed models for out-of-distribution (OOD) data. Towards this end, we study two popular model compression techniques including knowledge distillation and pruning and show that the compressed models are significantly less robust than their PLM counterparts on OOD test sets although they obtain similar performance on in-distribution development sets for a task. Further analysis indicates that the compressed models overfit on the shortcut samples and generalize poorly on the hard ones. We further leverage this observation to develop a regularization strategy for robust model compression based on sample uncertainty. Experimental results on several natural language understanding tasks demonstrate that our bias mitigation framework improves the OOD generalization of the compressed models, while not sacrificing the in-distribution task performance.",
        "id":257219883
      },
      {
        "title":"Integrating Unsupervised Data Generation into Self-Supervised Neural Machine Translation for Low-Resource Languages",
        "text":"For most language combinations, parallel data is either scarce or simply unavailable. To address this, unsupervised machine translation (UMT) exploits large amounts of monolingual data by using synthetic data generation techniques such as back-translation and noising, while self-supervised NMT (SSNMT) identifies parallel sentences in smaller comparable data and trains on them. To date, the inclusion of UMT data generation techniques in SSNMT has not been investigated. We show that including UMT techniques into SSNMT significantly outperforms SSNMT and UMT on all tested language pairs, with improvements of up to +4.3 BLEU, +50.8 BLEU, +51.5 over SSNMT, statistical UMT and hybrid UMT, respectively, on Afrikaans to English. We further show that the combination of multilingual denoising autoencoding, SSNMT with backtranslation and bilingual finetuning enables us to learn machine translation even for distant language pairs for which only small amounts of monolingual data are available, e.g. yielding BLEU scores of 11.6 (English to Swahili).",
        "id":236088076
      },
      {
        "title":"Deep Reinforcement Learning for Chinese Zero Pronoun Resolution",
        "text":"Deep neural network models for Chinese zero pronoun resolution learn semantic information for zero pronoun and candidate antecedents, but tend to be short-sightedthey often make local decisions. They typically predict coreference chains between the zero pronoun and one single candidate antecedent one link at a time, while overlooking their long-term influence on future decisions. Ideally, modeling useful information of preceding potential antecedents is critical when later predicting zero pronoun-candidate antecedent pairs. In this study, we show how to integrate local and global decision-making by exploiting deep reinforcement learning models. With the help of the reinforcement learning agent, our model learns the policy of selecting antecedents in a sequential manner, where useful information provided by earlier predicted antecedents could be utilized for making later coreference decisions. Experimental results on OntoNotes 5.0 dataset show that our technique surpasses the state-of-the-art models. * Corresponding author.([Litigant LiYading] not only shows φ1 willing of acception, but also φ2 hopes that there should be someone in charge of it.)",
        "id":47019459
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Where can I find a detailed discussion on automating the assessment of clarifications in instructional text, including tasks for grading these clarifications as plausible, implausible, or neutral and ranking them on a scale?",
    "positive_ctxs":[
      {
        "title":"SemEval-2022 Task 7: Identifying Plausible Clarifications of Implicit and Underspecified Phrases in Instructional Texts",
        "text":"We describe SemEval-2022 Task 7, a shared task on rating the plausibility of clarifications in English-language instructional texts. The dataset for this task consists of manually clarified how-to guides for which we generated alternative clarifications and collected human plausibility judgements. 1 The task of participating systems was to automatically determine the plausibility of a clarification in the respective context. In total, 21 participants took part in this task, with the best system achieving an accuracy of 68.9%. This report summarizes the results and findings from 8 teams and their system descriptions. Finally, we show in an additional evaluation that predictions by the top participating team make it possible to identify contexts with multiple plausible clarifications with an accuracy of 75.2%.",
        "id":250390720
      }
    ],
    "negative_ctxs":[
      {
        "title":"EMERGENCE OF GRID-LIKE REPRESENTATIONS BY TRAINING RECURRENT NEURAL NETWORKS TO PERFORM SPATIAL LOCALIZATION",
        "text":"Decades of research on the neural code underlying spatial navigation have revealed a diverse set of neural response properties. The Entorhinal Cortex (EC) of the mammalian brain contains a rich set of spatial correlates, including grid cells which encode space using tessellating patterns. However, the mechanisms and functional significance of these spatial representations remain largely mysterious. As a new way to understand these neural representations, we trained recurrent neural networks (RNNs) to perform navigation tasks in 2D arenas based on velocity inputs. Surprisingly, we find that grid-like spatial response patterns emerge in trained networks, along with units that exhibit other spatial correlates, including border cells and band-like cells. All these different functional types of neurons have been observed experimentally. The order of the emergence of grid-like and border cells is also consistent with observations from developmental studies. Together, our results suggest that grid cells, border cells and others as observed in EC may be a natural solution for representing space efficiently given the predominant recurrent connections in the neural circuits. * equal contribution arXiv:1803.07770v1 [q-bio.NC]",
        "id":3536139
      },
      {
        "title":"",
        "text":"",
        "id":232021604
      },
      {
        "title":"Probabilistic Graph Reasoning for Natural Proof Generation",
        "text":"In this paper, we investigate the problem of reasoning over natural language statements. Prior neural based approaches do not explicitly consider the inter-dependency among answers and their proofs. In this paper, we propose PROBR, a novel approach for joint answer prediction and proof generation. PROBR defines a joint probabilistic distribution over all possible proof graphs and answers via an induced graphical model. We then optimize the model using variational approximation on top of neural textual representation. Experiments on multiple datasets under diverse settings (fully supervised, few-shot and zero-shot evaluation) verify the effectiveness of PROBR, e.g., achieving 10%-30% improvement on QA accuracy in few\/zero-shot evaluation. Our codes and models can be found at https:\/\/github.com\/ changzhisun\/PRobr\/.",
        "id":235742855
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Where can I find a discourse treebank tailored to Chinese newswire articles that's large enough to make training, development, and test splits?",
    "positive_ctxs":[
      {
        "title":"Building Chinese Discourse Corpus with Connective-driven Dependency Tree Structure",
        "text":"In this paper, we propose a Connectivedriven Dependency Tree (CDT) scheme to represent the discourse rhetorical structure in Chinese language, with elementary discourse units as leaf nodes and connectives as non-leaf nodes, largely motivated by the Penn Discourse Treebank and the Rhetorical Structure Theory. In particular, connectives are employed to directly represent the hierarchy of the tree structure and the rhetorical relation of a discourse, while the nuclei of discourse units are globally determined with reference to the dependency theory. Guided by the CDT scheme, we manually annotate a Chinese Discourse Treebank (CDTB) of 500 documents. Preliminary evaluation justifies the appropriateness of the CDT scheme to Chinese discourse analysis and the usefulness of our manually annotated CDTB corpus.",
        "id":1781329
      }
    ],
    "negative_ctxs":[
      {
        "title":"Briefly Noted Referential Communication Tasks",
        "text":"",
        "id":208906
      },
      {
        "title":"TRAINING SOCIALLY ALIGNED LANGUAGE MODELS ON SIMULATED SOCIAL INTERACTIONS",
        "text":"Social alignment in AI systems aims to ensure that these models behave according to established societal values.However, unlike humans, who derive consensus on value judgments through social interaction, current language models (LMs) are trained to rigidly replicate their training corpus in isolation, leading to subpar generalization in unfamiliar scenarios and vulnerability to adversarial attacks.This work presents a novel training paradigm that permits LMs to learn from simulated social interactions.In comparison to existing methodologies, our approach is considerably more scalable and efficient, demonstrating superior performance in alignment benchmarks and human evaluations.This paradigm shift in the training of LMs brings us a step closer to developing AI systems that can robustly and accurately reflect societal norms and values.",
        "id":264590778
      },
      {
        "title":"Discovering Relations among Named Entities by Detecting Community Structure",
        "text":"This paper proposes a networked data mining method for relations discovery from large corpus. The key idea is representing the named entities pairs and their contexts as the network structure and detecting the communities from the network. Then each community relates to a relation the named entities pairs in the same community have the same relation. Finally, we labeled the relations. Our experiment using the corpus of People's Daily reveals not only that the relations among named entities could be detected with high precision, but also that appropriate labels could be automatically provided for the relations.",
        "id":14409919
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Where can I find a large corpus of annotated social media posts concerning a variety of health conditions?",
    "positive_ctxs":[
      {
        "title":"RedHOT: A Corpus of Annotated Medical Questions, Experiences, and Claims on Social Media",
        "text":"We present Reddit Health Online Talk (RedHOT), a corpus of 22,000 richly annotated social media posts from Reddit spanning 24 health conditions. Annotations include demarcations of spans corresponding to medical claims, personal experiences, and questions. We collect additional granular annotations on identified claims. Specifically, we mark snippets that describe patient Populations, Interventions, and Outcomes (PIO elements) within these. Using this corpus, we introduce the task of retrieving trustworthy evidence relevant to a given claim made on social media. We propose a new method to automatically derive (noisy) supervision for this task which we use to train a dense retrieval model; this outperforms baseline models. Manual evaluation of retrieval results performed by medical doctors indicate that while our system performance is promising, there is considerable room for improvement. We release all annotations collected (and scripts to assemble the dataset), and all code necessary to reproduce the results in this paper at: https:\/\/sominw. com\/redhot. -Olano. 2019. Learning dense representations for entity retrieval. In",
        "id":252846763
      }
    ],
    "negative_ctxs":[
      {
        "title":"Towards Emotion Prediction in Spoken Tutoring Dialogues",
        "text":"Human tutors detect and respond to student emotional states, but current machine tutors do not. Our preliminary machine learning experiments involving transcription, emotion annotation and automatic feature extraction from our human-human spoken tutoring corpus indicate that the spoken tutoring system we are developing can be enhanced to automatically predict and adapt to student emotional states.",
        "id":14981345
      },
      {
        "title":"Deep Neural Networks for Coreference Resolution for Polish",
        "text":"The paper presents several configurations of deep neural networks aimed at the task of coreference resolution for Polish. Starting with the basic feature set and standard word embedding vector size we examine the setting with larger vectors, more extensive sets of mention features, increased number of negative examples, Siamese network architecture and a global mention clustering algorithm. The highest results are achieved by the system combining our best deep neural architecture with the sieve-based approach -the cascade of rule-based coreference resolvers ordered from most to least precise. All systems are evaluated on the data of the Polish Coreference Corpus featuring 540K tokens and 180K mentions. The best variant improves the state of the art for Polish by 0.53 F1 points, reaching 81.23 points of the CoNLL metric.",
        "id":21685089
      },
      {
        "title":"Temporal Expression Recognition for Cell Cycle Phase Concepts in Biomedical Literature",
        "text":"In this paper, we present a system for recognizing temporal expressions related to cell cycle phase (CCP) concepts in biomedical literature. We identified 11 classes of cell cycle related temporal expressions, for which we made extensions to TIMEX3, arranging them in an ontology derived from the Gene Ontology. We annotated 310 abstracts from PubMed. Annotation guidelines were developed, consistent with existing time-related annotation guidelines for TimeML. Two annotators participated in the annotation. We achieved an inter-annotator agreement of 0.79 for an exact span match and 0.82 for relaxed constraints. Our approach is a hybrid of machine learning to recognize temporal expressions and a rule-based approach to map them to the ontology. We trained a named entity recognizer using Conditional Random Fields (CRF) models. An off-the-shelf implementation of the linear chain CRF model was used. We obtained an F-score of 0.77 for temporal expression recognition. We achieved 0.79 macro-averagee F-score and 0.78 microaveraged F-score for mapping to the ontology.",
        "id":653250
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Where can I find a multilingual corpus that includes reviews on DVDs, music, and books for the purposes of cross-language sentiment classification research?",
    "positive_ctxs":[
      {
        "title":"Cross-Language Text Classification using Structural Correspondence Learning",
        "text":"We present a new approach to crosslanguage text classification that builds on structural correspondence learning, a recently proposed theory for domain adaptation. The approach uses unlabeled documents, along with a simple word translation oracle, in order to induce taskspecific, cross-lingual word correspondences. We report on analyses that reveal quantitative insights about the use of unlabeled data and the complexity of interlanguage correspondence modeling.We conduct experiments in the field of cross-language sentiment classification, employing English as source language, and German, French, and Japanese as target languages. The results are convincing; they demonstrate both the robustness and the competitiveness of the presented ideas.",
        "id":12360898
      }
    ],
    "negative_ctxs":[
      {
        "title":"Beyond Linguistic Equivalence. An Empirical Study of Translation Evaluation in a Translation Learner Corpus",
        "text":"The realisation that fully automatic translation in many settings is still far from producing output that is equal or superior to human translation has lead to an intense interest in translation evaluation in the MT community. However, research in this field, by now, has not only largely ignored the tremendous amount of relevant knowledge available in a closely related discipline, namely translation studies, but also failed to provide a deeper understanding of the nature of \"translation errors\" and \"translation quality\". This paper presents an empirical take on the latter concept, translation quality, by comparing human and automatic evaluations of learner translations in the KOPTE corpus. We will show that translation studies provide sophisticated concepts for translation quality estimation and error annotation. Moreover, by applying well-established MT evaluation scores, namely BLEU and Meteor, to KOPTE learner translations that were graded by a human expert, we hope to shed light on properties (and potential shortcomings) of these scores.",
        "id":15966179
      },
      {
        "title":"A Direction of MT Development 1",
        "text":"",
        "id":16735270
      },
      {
        "title":"FinSim4-ESG Shared Task: Learning Semantic Similarities for the Financial Domain. Extended edition to ESG insights",
        "text":"This paper describes FinSim4-ESG 1 shared task organized in the 4th FinNLP workshopwhich is held in conjunction with the IJCAI-ECAI-2022 confer-enceThis year, the FinSim4 is extended to the Environment, Social and Government (ESG) insights and proposes two subtasks, one for ESG Taxonomy Enrichment and the other for Sustainable Sentence Prediction. Among the 28 teams registered to the shared task, a total of 8 teams submitted their systems results and 6 teams also submitted a paper to describe their method. The winner of each subtask shows good performance results of 0.85% and 0.95% in terms of accuracy, respectively.",
        "id":256460896
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Where can I find a paper that discusses annotating events in text with degrees of factuality, including categories such as certain, probable, and underspecified?",
    "positive_ctxs":[
      {
        "title":"Are You Sure That This Happened? Assessing the Factuality Degree of Events in Text",
        "text":"Identifying the veracity, or factuality, of event mentions in text is fundamental for reasoning about eventualities in discourse. Inferences derived from events judged as not having happened, or as being only possible, are different from those derived from events evaluated as factual. Event factuality involves two separate levels of information. On the one hand, it deals with polarity, which distinguishes between positive and negative instantiations of events. On the other, it has to do with degrees of certainty (e.g., possible, probable), an information level generally subsumed under the category of epistemic modality. This article aims at contributing to a better understanding of how event factuality is articulated in natural language. For that purpose, we put forward a linguistic-oriented computational model which has at its core an algorithm articulating the effect of factuality relations across levels of syntactic embedding. As a proof of concept, this model has been implemented in De Facto, a factuality profiler for eventualities mentioned in text, and tested against a corpus built specifically for the task, yielding an F 1 of 0.70 (macro-averaging) and 0.80 (micro-averaging). These two measures mutually compensate for an over-emphasis present in the other (either on the lesser or greater populated categories), and can therefore be interpreted as the lower and upper bounds of the De Facto's performance.",
        "id":2239324
      }
    ],
    "negative_ctxs":[
      {
        "title":"Consistent CCG Parsing over Multiple Sentences for Improved Logical Reasoning",
        "text":"In formal logic-based approaches to Recognizing Textual Entailment (RTE), a Combinatory Categorial Grammar (CCG) parser is used to parse input premises and hypotheses to obtain their logical formulas. Here, it is important that the parser processes the sentences consistently; failing to recognize a similar syntactic structure results in inconsistent predicate argument structures among them, in which case the succeeding theorem proving is doomed to failure. In this work, we present a simple method to extend an existing CCG parser to parse a set of sentences consistently, which is achieved with an inter-sentence modeling with Markov Random Fields (MRF). When combined with existing logic-based systems, our method always shows improvement in the RTE experiments on English and Japanese languages.",
        "id":4937809
      },
      {
        "title":"Workshop track -ICLR 2018 EFFICIENT RECURRENT NEURAL NETWORKS USING STRUCTURED MATRICES IN FPGAS",
        "text":"Recurrent Neural Networks (RNNs) are becoming increasingly important for time series-related applications which require efficient and real-time implementations. The recent pruning based work ESE (Han et al., 2017) suffers from degradation of performance\/energy efficiency due to the irregular network structure after pruning. We propose block-circulant matrices for weight matrix representation in RNNs, thereby achieving simultaneous model compression and acceleration. We aim to implement RNNs in FPGA with highest performance and energy efficiency, with certain accuracy requirement (negligible accuracy degradation). Experimental results on actual FPGA deployments shows that the proposed framework achieves a maximum energy efficiency improvement of 35.7× compared with ESE.",
        "id":4347685
      },
      {
        "title":"Results of the WMT19 Metrics Shared Task: Segment-Level and Strong MT Systems Pose Big Challenges",
        "text":"This paper presents the results of the WMT19 Metrics Shared Task.Participants were asked to score the outputs of the translations systems competing in the WMT19 News Translation Task with automatic metrics. 13 research groups submitted 24 metrics, 10 of which are reference-less \"metrics\" and constitute submissions to the joint task with WMT19 Quality Estimation Task, \"QE as a Metric\". In addition, we computed 11 baseline metrics, with 8 commonly applied baselines (BLEU, SentBLEU, NIST, WER, PER, TER, CDER, and chrF) and 3 reimplementations (chrF+, sacreBLEU-BLEU, and sacreBLEU-chrF). Metrics were evaluated on the system level, how well a given metric correlates with the WMT19 official manual ranking, and segment level, how well the metric correlates with human judgements of segment quality. This year, we use direct assessment (DA) as our only form of manual evaluation.",
        "id":201742578
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"Where can I find guidelines on standard practices for probing machine learning models to discern what information the models have captured without training them on a new task?",
    "positive_ctxs":[
      {
        "title":"Does My Representation Capture X? Probe-Ably",
        "text":"Probing (or diagnostic classification) has become a popular strategy for investigating whether a given set of intermediate features is present in the representations of neural models. Probing studies may have misleading results, but various recent works have suggested more reliable methodologies that compensate for the possible pitfalls of probing. However, these best practices are numerous and fast-evolving. To simplify the process of running a set of probing experiments in line with suggested methodologies, we introduce Probe-Ably: an extendable probing framework which supports and automates the application of probing methods to the user's inputs.",
        "id":233219699
      }
    ],
    "negative_ctxs":[
      {
        "title":"Improved Pattern Learning for Bootstrapped Entity Extraction",
        "text":"Bootstrapped pattern learning for entity extraction usually starts with seed entities and iteratively learns patterns and entities from unlabeled text. Patterns are scored by their ability to extract more positive entities and less negative entities. A problem is that due to the lack of labeled data, unlabeled entities are either assumed to be negative or are ignored by the existing pattern scoring measures. In this paper, we improve pattern scoring by predicting the labels of unlabeled entities. We use various unsupervised features based on contrasting domain-specific and general text, and exploiting distributional similarity and edit distances to learned entities. Our system outperforms existing pattern scoring algorithms for extracting drug-andtreatment entities from four medical forums.",
        "id":1367004
      },
      {
        "title":"Cross-lingual Information Extraction System Evaluation",
        "text":"In this paper, we discuss the performance of crosslingual information extraction systems employing an automatic pattern acquisition module. This module, which creates extraction patterns starting from a user's narrative task description, allows rapid customization to new extraction tasks. We compare two approaches: (1) acquiring patterns in the source language, performing source language extraction, and then translating the resulting templates to the target language, and (2) translating the texts and performing pattern discovery and extraction in the target language. We demonstrate an average of 8-10% more recall using the first approach. We discuss some of the problems with machine translation and their effect on pattern discovery which lead to this difference in performance.",
        "id":18312172
      },
      {
        "title":"",
        "text":"",
        "id":199560526
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"Where can I find information on self-attentive parsers that have been trained in a few-shot learning setting, including their official code and hyperparameters?",
    "positive_ctxs":[
      {
        "title":"Constituency Parsing with a Self-Attentive Encoder",
        "text":"We demonstrate that replacing an LSTM encoder with a self-attentive architecture can lead to improvements to a state-ofthe-art discriminative constituency parser. The use of attention makes explicit the manner in which information is propagated between different locations in the sentence, which we use to both analyze our model and propose potential improvements. For example, we find that separating positional and content information in the encoder can lead to improved parsing accuracy. Additionally, we evaluate different approaches for lexical representation. Our parser achieves new state-ofthe-art results for single models trained on the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset.",
        "id":19206893
      }
    ],
    "negative_ctxs":[
      {
        "title":"Discriminative Reasoning for Document-level Relation Extraction",
        "text":"Document-level relation extraction (DocRE) models generally use graph networks to implicitly model the reasoning skill (i.e., pattern recognition, logical reasoning, coreference reasoning, etc.) related to the relation between",
        "id":235313469
      },
      {
        "title":"Does BERT Learn as Humans Perceive? Understanding Linguistic Styles through Lexica",
        "text":"People convey their intention and attitude through linguistic styles of the text that they write. In this study, we investigate lexicon usages across styles throughout two lenses: human perception and machine word importance, since words differ in the strength of the stylistic cues that they provide. To collect labels of human perception, we curate a new dataset, HUMMINGBIRD, on top of benchmarking style datasets. We have crowd workers highlight the representative words in the text that makes them think the text has the following styles: politeness, sentiment, offensiveness, and five emotion types. We then compare these human word labels with word importance derived from a popular fine-tuned style classifier like BERT. Our results show that the BERT often finds content words not relevant to the target style as important words used in style prediction, but humans do not perceive the same way even though for some styles (e.g., positive sentiment and joy) humanand machine-identified words share significant overlap for some styles.",
        "id":237433537
      },
      {
        "title":"Cross-lingual Parse Disambiguation based on Semantic Correspondence",
        "text":"We present a system for cross-lingual parse disambiguation, exploiting the assumption that the meaning of a sentence remains unchanged during translation and the fact that different languages have different ambiguities. We simultaneously reduce ambiguity in multiple languages in a fully automatic way. Evaluation shows that the system reliably discards dispreferred parses from the raw parser output, which results in a pre-selection that can speed up manual treebanking.",
        "id":474999
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"Where can I find interdisciplinary research that investigates how creative natural language generation (NLG) systems are evaluated?",
    "positive_ctxs":[
      {
        "title":"Human Evaluation of Creative NLG Systems: An Interdisciplinary Survey on Recent Papers",
        "text":"We survey human evaluation in papers presenting work on creative natural language generation that have been published in INLG 2020 and ICCC 2020. The most typical human evaluation method is a scaled survey, typically on a 5 point scale, while many other less common methods exist. The most commonly evaluated parameters are meaning, syntactic correctness, novelty, relevance and emotional value, among many others. Our guidelines for future evaluation include clearly defining the goal of the generative system, asking questions as concrete as possible, testing the evaluation setup, using multiple different evaluation setups, reporting the entire evaluation process and potential biases clearly, and finally analyzing the evaluation results in a more profound way than merely reporting the most typical statistics.",
        "id":236486131
      }
    ],
    "negative_ctxs":[
      {
        "title":"Matrix and Tensor Factorization Methods for Natural Language Processing",
        "text":"Tutorial ObjectivesTensor and matrix factorization methods have attracted a lot of attention recently thanks to their successful applications to information extraction, knowledge base population, lexical semantics and dependency parsing. In the first part, we will first cover the basics of matrix and tensor factorization theory and optimization, and then proceed to more advanced topics involving convex surrogates and alternative losses. In the second part we will discuss recent NLP applications of these methods and show the connections with other popular methods such as transductive learning, topic models and neural networks. The aim of this tutorial is to present in detail applied factorization methods, as well as to introduce more recently proposed methods that are likely to be useful to NLP applications.Tutorial OverviewMatrix\/Tensor Factorization BasicsIn this part, we first remind essential results on bilinear forms, spectral representations of matrices and low-rank approximation theorems, which are often omitted in undergraduate linear algebra courses. This includes the link between eigenvalue decomposition and singular value decomposition and the trace-norm (a.k.a. nuclear norm) as a convex surrogate of the low-rank constraint on optimization problems. Then, an overview of the most efficient algorithms to solve low-rank constrained problems is made, from the power iteration method, the Lanczos algorithm and the implicitly restarted Arnoldi method that is implemented in the LAPACK library(Anderson et al., 1999). We show how to interpret low-rank models as probabilistic models(Bishop, 1999)and how we can extend SVD algorithms that can factor-ize non-standard matrices (i.e. with non-Gaussian noise and missing data) using gradient descent, reweighted SVD or Frank-Wolfe algorithms. We then show that combining different convex objectives can be a powerful tool, and we illustrate it by deriving the robust PCA algorithm by adding an L 1 penalty term in the objective function(Candès and Recht, 2009). Furthermore, we introduce Bayesian Personalized Ranking (BPR) for matrix and tensor factorization which deals with implicit feedback in ranking tasks(Rendle et al., 2009). Finally, will introduce the collective matrix factorization model (Singh and Gordon, 2008) and tensor extensions (Nickel et al., 2011) for relational learning.Applications in NLPIn this part we will discuss recent work applying matrix\/tensor factorization methods in the context of NLP. We will review the Universal Schema paradigm for knowledge base construction(Riedel et al., 2013)which relies on matrix factorization and BPR, as well as recent extensions of the RESCAL tensor factorization(Nickel et al., 2011)approach and methods of injecting logic into the embeddings learned (Rocktäschel et al.,  2015). These applications will motivate the connections between matrix factorization and transductive learning(Goldberg et al., 2010), as well as tensor factorization and multi-task learning (Romera-Paredes et al., 2013). Furthermore, we will review work on applying matrix and tensor factorization to sparsity reduction in syntactic dependency parsing(Lei et al., 2014)and word representation learning(Pennington et al., 2014). In addition, we will discuss the connections between matrix factorization, latent semantic analysis and topic modeling(Stevens et al., 2012).16",
        "id":27681954
      },
      {
        "title":"",
        "text":"",
        "id":202734912
      },
      {
        "title":"Published as a conference paper at ICLR 2023 BETTER GENERATIVE REPLAY FOR CONTINUAL FEDERATED LEARNING",
        "text":"Federated Learning (FL) aims to develop a centralized server that learns from distributed clients via communications without accessing the clients' local data. However, existing works mainly focus on federated learning in a single task scenario. with static data. In this paper, we introduce the continual federated learning (CFL) problem, where clients incrementally learn new tasks and history data cannot be stored due to certain reasons, such as limited storage and data retention policy 1 . Generative replay (GR) based methods are effective for continual learning without storing history data. However, we fail when trying to intuitively adapt GR models for this setting. By analyzing the behaviors of clients during training, we find the unstable training process caused by distributed training on non-IID data leads to a notable performance degradation. To address this problem, we propose our FedCIL model with two simple but effective solutions: 1. model consolidation and 2. consistency enforcement. Experimental results on multiple benchmark datasets demonstrate that our method significantly outperforms baselines.Published as a conference paper at ICLR 2023 traditional continual learning settings that only involve one model, our problem is more complex because there are multiple models including one server and many clients.",
        "id":257220031
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Where can I find multilingual datasets used for the task of intended sarcasm detection, particularly involving English and Arabic?",
    "positive_ctxs":[
      {
        "title":"SemEval-2022 Task 6: iSarcasmEval, Intended Sarcasm Detection in English and Arabic",
        "text":"iSarcasmEval is the first shared task to target intended sarcasm detection: the data for this task was provided and labelled by the authors of the texts themselves. Such an approach minimises the downfalls of other methods to collect sarcasm data, which rely on distant supervision or third-party annotations. The shared task contains two languages, English and Arabic, and three subtasks: sarcasm detection, sarcasm category classification, and pairwise sarcasm identification given a sarcastic sentence and its non-sarcastic rephrase. The task received submissions from 60 different teams, with the sarcasm detection task being the most popular. Most of the participating teams utilised pre-trained language models. In this paper, we provide an overview of the task, data, and participating teams.",
        "id":250391089
      }
    ],
    "negative_ctxs":[
      {
        "title":"Toward an Underspecifiable Corpus Annotation Scheme",
        "text":"The Wall Street Journal corpora provided for the Workshop on Cross-Framework and Cross-Domain Parser Evaluation Shared Task are investigated in order to see how the structures that are difficult for an annotator of dependency structure are encoded in the different schemes. Non-trivial differences among the schemes are found. The paper also investigates the possibility of merging the information encoded in the different corpora.",
        "id":3518648
      },
      {
        "title":"",
        "text":"",
        "id":218974550
      },
      {
        "title":"Domain Independent Authorship Attribution without Domain Adaptation",
        "text":"Automatic authorship attribution, by its nature, is much more advantageous if it is domain (i.e., topic and\/or genre) independent. That is, many real world problems that require authorship attribution may not have in-domain training data readily available. However, most previous work based on machine learning techniques focused only on in-domain text for authorship attribution. In this paper, we present comprehensive evaluation of various stylometric techniques for cross-domain authorship attribution. From the experiments based on the Project Gutenberg book archive, we discover that extremely simple techniques based on stopwords are surprisingly robust against domain change, essentially ridding the need for domain adaptation when supplied with a large amount of data.",
        "id":18999067
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Where can I find research about automatic evaluation metrics in summarization tasks disagree with each other?",
    "positive_ctxs":[
      {
        "title":"Metrics also Disagree in the Low Scoring Range: Revisiting Summarization Evaluation Metrics",
        "text":"In text summarization, evaluating the efficacy of automatic metrics without human judgments has become recently popular. One exemplar work (Peyrard, 2019) concludes that automatic metrics strongly disagree when ranking high-scoring summaries. In this paper, we revisit their experiments and find that their observations stem from the fact that metrics disagree in ranking summaries from any narrow scoring range. We hypothesize that this may be because summaries are similar to each other in a narrow scoring range and are thus, difficult to rank. Apart from the width of the scoring range of summaries, we analyze three other properties that impact inter-metric agreement -Ease of Summarization, Abstractiveness, and Coverage. To encourage reproducible research, we make all our analysis code and data publicly available. 1 1 https:\/\/github.com\/manikbhandari\/RevisitSummEvalMetrics 2 Peyrard (2019) uses three experiments to reach their conclusion. Due to limitations of space, we focus on the first one here. Please see the appendix for a detailed analysis of the other two experiments. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:\/\/ creativecommons.org\/licenses\/by\/4.0\/.",
        "id":226282456
      }
    ],
    "negative_ctxs":[
      {
        "title":"Finding Non-Arbitrary Form-Meaning Systematicity Using String-Metric Learning for Kernel Regression",
        "text":"Arbitrariness of the sign-the notion that the forms of words are unrelated to their meanings-is an underlying assumption of many linguistic theories. Two lines of research have recently challenged this assumption, but they produce differing characterizations of non-arbitrariness in language. Behavioral and corpus studies have confirmed the validity of localized form-meaning patterns manifested in limited subsets of the lexicon. Meanwhile, global (lexicon-wide) statistical analyses instead find diffuse form-meaning systematicity across the lexicon as a whole.We bridge the gap with an approach that can detect both local and global formmeaning systematicity in language. In the kernel regression formulation we introduce, form-meaning relationships can be used to predict words' distributional semantic vectors from their forms. Furthermore, we introduce a novel metric learning algorithm that can learn weighted edit distances that minimize kernel regression error. Our results suggest that the English lexicon exhibits far more global form-meaning systematicity than previously discovered, and that much of this systematicity is focused in localized formmeaning patterns.",
        "id":908501
      },
      {
        "title":"Generating student feedback from time-series data using Reinforcement Learning",
        "text":"We describe a statistical Natural Language Generation (NLG) method for summarisation of time-series data in the context of feedback generation for students. In this paper, we initially present a method for collecting time-series data from students (e.g. marks, lectures attended) and use example feedback from lecturers in a datadriven approach to content selection. We show a novel way of constructing a reward function for our Reinforcement Learning agent that is informed by the lecturers' method of providing feedback. We evaluate our system with undergraduate students by comparing it to three baseline systems: a rule-based system, lecturerconstructed summaries and a Brute Force system. Our evaluation shows that the feedback generated by our learning agent is viewed by students to be as good as the feedback from the lecturers. Our findings suggest that the learning agent needs to take into account both the student and lecturers' preferences.",
        "id":5552542
      },
      {
        "title":"How few is too few? Determining the minimum acceptable number of LSA dimensions to visualise text cohesion with Lex Abstract",
        "text":"",
        "id":39661249
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"Where might I find a dataset annotated specifically for patronizing and condescending language to use in computational linguistics research?",
    "positive_ctxs":[
      {
        "title":"Don't Patronize Me! An Annotated Dataset with Patronizing and Condescending Language towards Vulnerable Communities",
        "text":"In this paper, we introduce a new annotated dataset which is aimed at supporting the development of NLP models to identify and categorize language that is patronizing or condescending towards vulnerable communities (e.g. refugees, homeless people, poor families). While the prevalence of such language in the general media has long been shown to have harmful effects, it differs from other types of harmful language, in that it is generally used unconsciously and with good intentions. We furthermore believe that the often subtle nature of patronizing and condescending language (PCL) presents an interesting technical challenge for the NLP community. Our analysis of the proposed dataset shows that identifying PCL is hard for standard NLP models, with language models such as BERT achieving the best results.",
        "id":226976077
      }
    ],
    "negative_ctxs":[
      {
        "title":"Integrating on-line MT services into monolingual web-sites for dissemination purposes: an evaluation perspective",
        "text":"On-line machine translation (MT) services are becoming increasingly popular among Internet users. In particular, over the last few years there has been a dramatic increase in the number of monolingual web-sites that rely on Internet-based MT systems to disseminate their contents in a variety of languages, which seems to be one of the most interesting areas in the current use of MT technology. This paper is based on preliminary observations of these recent developments and reports on how on-line MT services are actually integrated into a sample of monolingual web-sites only available in English, attempting to evaluate the success of the strategies used to incorporate webbased MT technology for dissemination purposes. The discussion suggests in conclusion that the overall lack of a user-oriented approach and the limited consideration of issues of user-friendliness make the integration of on-line MT into the sample of monolingual web-sites largely ineffective.",
        "id":2980252
      },
      {
        "title":"Learning the Optimal use of Dependency-parsing Information for Finding Translations with Comparable Corpora",
        "text":"Using comparable corpora to find new word translations is a promising approach for extending bilingual dictionaries (semi-) automatically. The basic idea is based on the assumption that similar words have similar contexts across languages. The context of a word is often summarized by using the bag-of-words in the sentence, or by using the words which are in a certain dependency position, e.g. the predecessors and successors. These different context positions are then combined into one context vector and compared across languages. However, previous research makes the (implicit) assumption that these different context positions should be weighted as equally important. Furthermore, only the same context positions are compared with each other, for example the successor position in Spanish is compared with the successor position in English. However, this is not necessarily always appropriate for languages like Japanese and English. To overcome these limitations, we suggest to perform a linear transformation of the context vectors, which is defined by a matrix. We define the optimal transformation matrix by using a Bayesian probabilistic model, and show that it is feasible to find an approximate solution using Markov chain Monte Carlo methods. Our experiments demonstrate that our proposed method constantly improves translation accuracy.",
        "id":17019457
      },
      {
        "title":"",
        "text":"",
        "id":192986113
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"Where might I find research on the evaluation of consistency in generated summaries?",
    "positive_ctxs":[
      {
        "title":"SUMMAC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization",
        "text":"In the summarization domain, a key requirement for summaries is to be factually consistent with the input document. Previous work has found that natural language inference (NLI) models do not perform competitively when applied to inconsistency detection. In this work, we revisit the use of NLI for inconsistency detection, finding that past work suffered from a mismatch in input granularity between NLI datasets (sentence-level), and inconsistency detection (document level). We provide a highly effective and light-weight method called SUMMAC CONV that enables NLI models to be successfully used for this task by segmenting documents into sentence units and aggregating scores between pairs of sentences. We furthermore introduce a new benchmark called SUMMAC (Summary Consistency) which consists of six large inconsistency detection datasets. On this dataset, SUMMAC Conv obtains state-of-the-art results with a balanced accuracy of 74.4%, a 5% improvement compared with prior work. *",
        "id":244345901
      },
      {
        "title":"Compression, Transduction, and Creation: A Unified Framework for Evaluating Natural Language Generation",
        "text":"Natural language generation (NLG) spans a broad range of tasks, each of which serves for specific objectives and desires different properties of generated text. The complexity makes automatic evaluation of NLG particularly challenging. Previous work has typically focused on a single task and developed individual evaluation metrics based on specific intuitions. In this paper, we propose a unifying perspective based on the nature of information change in NLG tasks, including compression (e.g., summarization), transduction (e.g., text rewriting), and creation (e.g., dialog). Information alignment between input, context, and output text plays a common central role in characterizing the generation. With automatic alignment prediction models, we develop a family of interpretable metrics that are suitable for evaluating key aspects of different NLG tasks, often without need of gold reference data. Experiments show the uniformly designed metrics achieve stronger or comparable correlations with human judgement compared to state-of-the-art metrics in each of diverse tasks, including text summarization, style transfer, and knowledgegrounded dialog. 1",
        "id":237507028
      }
    ],
    "negative_ctxs":[
      {
        "title":"Enhancing Automatic ICD-9-CM Code Assignment for Medical Texts with PubMed",
        "text":"Assigning a standard ICD-9-CM code to disease symptoms in medical texts is an important task in the medical domain. Automating this process could greatly reduce the costs. However, the effectiveness of an automatic ICD-9-CM code classifier faces a serious problem, which can be triggered by unbalanced training data. Frequent diseases often have more training data, which helps its classification to perform better than that of an infrequent disease. However, a diseases frequency does not necessarily reflect its importance. To resolve this training data shortage problem, we propose to strategically draw data from PubMed to enrich the training data when there is such need. We validate our method on the CMC dataset, and the evaluation results indicate that our method can significantly improve the code assignment classifiers' performance at the macro-averaging level.",
        "id":11045020
      },
      {
        "title":"Cross-document Event Identity via Dense Annotation",
        "text":"In this paper, we study the identity of textual events from different documents. While the complex nature of event identity is previously studied (Hovy et al., 2013), the case of events across documents is unclear. Prior work on cross-document event coreference has two main drawbacks. First, they restrict the annotations to a limited set of event types. Second, they insufficiently tackle the concept of event identity. Such annotation setup reduces the pool of event mentions and prevents one from considering the possibility of quasiidentity relations. We propose a dense annotation approach for cross-document event coreference, comprising a rich source of event mentions and a dense annotation effort between related document pairs. To this end, we design a new annotation workflow with careful quality control and an easy-to-use annotation interface. In addition to the links, we further collect overlapping event contexts, including time, location, and participants, to shed some light on the relation between identity decisions and context. We present an open-access dataset for cross-document event coreference, CDEC-WN, collected from English Wikinews and open-source our annotation toolkit to encourage further research on cross-document tasks. 1",
        "id":237503515
      },
      {
        "title":"",
        "text":"",
        "id":232021911
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Which corpora are frequently used in research to benchmark English readability assessment tools?",
    "positive_ctxs":[
      {
        "title":"The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 163-173, On Improving the Accuracy of Readability Classification using Insights from Second Language Acquisition",
        "text":"We investigate the problem of readability assessment using a range of lexical and syntactic features and study their impact on predicting the grade level of texts. As empirical basis, we combined two web-based text sources, Weekly Reader and BBC Bitesize, targeting different age groups, to cover a broad range of school grades. On the conceptual side, we explore the use of lexical and syntactic measures originally designed to measure language development in the production of second language learners. We show that the developmental measures from Second Language Acquisition (SLA) research when combined with traditional readability features such as word length and sentence length provide a good indication of text readability across different grades. The resulting classifiers significantly outperform the previous approaches on readability classification, reaching a classification accuracy of 93.3%.",
        "id":10919200
      },
      {
        "title":"OneStopEnglish corpus: A new corpus for automatic readability assessment and text simplification",
        "text":"This paper describes the collection and compilation of the OneStopEnglish corpus of texts written at three reading levels, and demonstrates its usefulness for through two applications -automatic readability assessment and automatic text simplification. The corpus consists of 189 texts, each in three versions (567 in total). The corpus is now freely available under a CC by-SA 4.0 license 1 and we hope that it would foster further research on the topics of readability assessment and text simplification.",
        "id":46937952
      }
    ],
    "negative_ctxs":[
      {
        "title":"Evaluating Agent Interactions Through Episodic Knowledge Graphs",
        "text":"We present a new method based on episodic Knowledge Graphs (eKGs) for evaluating (multimodal)  conversational agents in open domains. This graph is generated by interpreting raw signals during conversation and is able to capture the accumulation of knowledge over time. We apply structural and semantic analysis of the resulting graphs and translate the properties into qualitative measures. We compare these measures with existing automatic and manual evaluation metrics commonly used for conversational agents. Our results show that our Knowledge-Graph-based evaluation provides more qualitative insights into interaction and the agent's behavior.",
        "id":252519245
      },
      {
        "title":"Multi-dimensional abstractness in cross-domain mappings",
        "text":"Metaphor is a cognitive process that shapes abstract target concepts by mapping them to concrete source concepts. Thus, many computational approaches to metaphor make reference, directly or indirectly, to the abstractness of words and concepts. The property of abstractness, however, remains theoretically and empirically unexplored. This paper implements a multi-dimensional definition of abstractness and tests the usefulness of each dimension for detecting cross-domain mappings.",
        "id":14826968
      },
      {
        "title":"Colors in Context: A Pragmatic Neural Model for Grounded Language Understanding",
        "text":"We present a model of pragmatic referring expression interpretation in a grounded communication task (identifying colors from descriptions) that draws upon predictions from two recurrent neural network classifiers, a speaker and a listener, unified by a recursive pragmatic reasoning framework. Experiments show that this combined pragmatic model interprets color descriptions more accurately than the classifiers from which it is built, and that much of this improvement results from combining the speaker and listener perspectives. We observe that pragmatic reasoning helps primarily in the hardest cases: when the model must distinguish very similar colors, or when few utterances adequately express the target color. Our findings make use of a newly-collected corpus of human utterances in color reference games, which exhibit a variety of pragmatic behaviors. We also show that the embedded speaker model reproduces many of these pragmatic behaviors.",
        "id":1854889
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Which paper specifies the typical configurations used in fine-tuning deep bidirectional transformers like BERT and RoBERTa for language understanding tasks?",
    "positive_ctxs":[
      {
        "title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "text":"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
        "id":52967399
      }
    ],
    "negative_ctxs":[
      {
        "title":"The Effects of Word Prediction on Communication Rate for AAC",
        "text":"Individuals using an Augmentative and Alternative Communication (AAC) device communicate at less than 10% of the speed of \"traditional\" speech, creating a large communication gap. In this user study, we compare the communication rate of pseudo-impaired individuals using two different word prediction algorithms and a system without word prediction. Our results show that word prediction can increase AAC communication rate and that more accurate predictions significantly improve communication rate.",
        "id":16511639
      },
      {
        "title":"Multimodal Joint Attribute Prediction and Value Extraction for E-commerce Product",
        "text":"Product attribute values are essential in many e-commerce scenarios, such as customer service robots, product recommendations, and product retrieval. While in the real world, the attribute values of a product are usually incomplete and vary over time, which greatly hinders the practical applications. In this paper, we propose a multimodal method to jointly predict product attributes and extract values from textual product descriptions with the help of the product images. We argue that product attributes and values are highly correlated, e.g., it will be easier to extract the values on condition that the product attributes are given. Thus, we jointly model the attribute prediction and value extraction tasks from multiple aspects towards the interactions between attributes and values. Moreover, product images have distinct effects on our tasks for different product attributes and values. Thus, we selectively draw useful visual information from product images to enhance our model. We annotate a multimodal product attribute value dataset that contains 87,194 instances, and the experimental results on this dataset demonstrate that explicitly modeling the relationship between attributes and values facilitates our method to establish the correspondence between them, and selectively utilizing visual product information is necessary for the task. Our code and dataset are available 1 .",
        "id":221703022
      },
      {
        "title":"Mapping product descriptions to a large ontology",
        "text":"In this paper we describe an information retrieval approach for mapping online business information texts to concepts in a large ontology. We adopt the traditional vector space model by representing the texts as queries and the concept labels in the ontology as documents.Because of the size of the ontology and the fact that concept labels are very sparse and generic, we conducted additional experiments for reducing the set of concepts, as well as the enrichment and enlargement of concept labels.The documents in our collection were of too poor quality for this task, and although we show that our enrichment technique did provide us with an ontology with good overall similarity to our query collection, individual concepts did not include enough terms for our method to achieve good results.",
        "id":8323291
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"Which papers should I refer to for learning about the application of transformer language models to the generation of argumentative text conclusions, including the assessment of their novelty and validity?",
    "positive_ctxs":[
      {
        "title":"Explainable Unsupervised Argument Similarity Rating with Abstract Meaning Representation and Conclusion Generation",
        "text":"When assessing the similarity of arguments, researchers typically use approaches that do not provide interpretable evidence or justifications for their ratings. Hence, the features that determine argument similarity remain elusive. We address this issue by introducing novel argument similarity metrics that aim at high performance and explainability. We show that Abstract Meaning Representation (AMR) graphs can be useful for representing arguments, and that novel AMR graph metrics can offer explanations for argument similarity ratings. We start from the hypothesis that similar premises often lead to similar conclusionsand extend an approach for AMR-based argument similarity rating by estimating, in addition, the similarity of conclusions that we automatically infer from the arguments used as premises. We show that AMR similarity metrics make argument similarity judgements more interpretable and may even support argument quality judgements. Our approach provides significant performance improvements over strong baselines in a fully unsupervised setting. Finally, we make first steps to address the problem of reference-less evaluation of argumentative conclusion generations.",
        "id":241583548
      },
      {
        "title":"Generating Informative Conclusions for Argumentative Texts",
        "text":"The purpose of an argumentative text is to support a certain conclusion. Yet, they are often omitted, expecting readers to infer them rather. While appropriate when reading an individual text, this rhetorical device limits accessibility when browsing many texts (e.g., on a search engine or on social media). In these scenarios, an explicit conclusion makes for a good candidate summary of an argumentative text. This is especially true if the conclusion is informative, emphasizing specific concepts from the text. With this paper we introduce the task of generating informative conclusions: First, Webis-ConcluGen-21 is compiled, a large-scale corpus of 136,996 samples of argumentative texts and their conclusions. Second, two paradigms for conclusion generation are investigated; one extractive, the other abstractive in nature. The latter exploits argumentative knowledge that augment the data via control codes and finetuning the BART model on several subsets of the corpus. Third, insights are provided into the suitability of our corpus for the task, the differences between the two generation paradigms, the trade-off between informativeness and conciseness, and the impact of encoding argumentative knowledge. The corpus, code, and the trained models are publicly available. 1",
        "id":235294159
      },
      {
        "title":"Assessing the Sufficiency of Arguments through Conclusion Generation",
        "text":"The premises of an argument give evidence or other reasons to support a conclusion. However, the amount of support required depends on the generality of a conclusion, the nature of the individual premises, and similar. An argument whose premises make its conclusion rationally worthy to be drawn is called sufficient in argument quality research. Previous work tackled sufficiency assessment as a standard text classification problem, not modeling the inherent relation of premises and conclusion. In this paper, we hypothesize that the conclusion of a sufficient argument can be generated from its premises. To study this hypothesis, we explore the potential of assessing sufficiency based on the output of large-scale pre-trained language models. Our best model variant achieves an F 1 -score of .885, outperforming the previous state-of-the-art and being on par with human experts. While manual evaluation reveals the quality of the generated conclusions, their impact remains low ultimately.",
        "id":239885913
      }
    ],
    "negative_ctxs":[
      {
        "title":"Rethinking Data Augmentation in Text-to-text Paradigm",
        "text":"As manually labelling data can be costly, some recent studies tend to augment the training data for improving the generalization power of machine learning models, known as data augmentation (DA). With the arise of pre-trained language models (PLMs), some recent works on DA try to synthesize new samples benefiting from the knowledge learned from PLM's pre-training. Along the same direction, we in this paper propose to integrate text-to-text language models and construct a new two-phase framework for augmentation: 1) a fine-tuning phase where PLMs are well adapted to downstream classification with the help of two novel schemes, and 2) a generation phase where the fine-tuned models are leveraged to create new samples for performance lifting. This paradigm opens up a new way of designing finetuning scheme to better serve DA in an easy-toimplement manner, and can be easily extended to other desired tasks. We evaluate our proposal on two public classification datasets and demonstrate its effectiveness with remarkable gains.",
        "id":252819362
      },
      {
        "title":"Span-Level Model for Relation Extraction",
        "text":"Relation Extraction is the task of identifying entity mention spans in raw text and then identifying relations between pairs of the entity mentions. Recent approaches for this spanlevel task have been token-level models which have inherent limitations. They cannot easily define and implement span-level features, cannot model overlapping entity mentions and have cascading errors due to the use of sequential decoding. To address these concerns, we present a model which directly models all possible spans and performs joint entity mention detection and relation extraction. We report a new state-of-the-art performance of 62.83 F 1 (prev best was 60.49) on the ACE2005 dataset.",
        "id":196187587
      },
      {
        "title":"Fine-grained Human Evaluation of Transformer and Recurrent Approaches to Neural Machine Translation for English-to-Chinese",
        "text":"This research presents a fine-grained human evaluation to compare the Transformer and recurrent approaches to neural machine translation (MT), on the translation direction English-to-Chinese. To this end, we develop an error taxonomy compliant with the Multidimensional Quality Metrics (MQM) framework that is customised to the relevant phenomena of this translation direction. We then conduct an error annotation using this customised error taxonomy on the output of state-of-theart recurrent-and Transformer-based MT systems on a subset of WMT2019's news test set. The resulting annotation shows that, compared to the best recurrent system, the best Transformer system results in a 31% reduction of the total number of errors and it produced significantly less errors in 10 out of 22 error categories. We also note that two of the systems evaluated do not produce any error for a category that was relevant for this translation direction prior to the advent of NMT systems: Chinese classifiers.",
        "id":219687371
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_acl",
    "question":"Which studies should I look into that have explored the task of factuality in abstractive summarization and proposed ways to detect hallucinated content in generated text?",
    "positive_ctxs":[
      {
        "title":"Detecting Hallucinated Content in Conditional Neural Sequence Generation",
        "text":"Neural sequence models can generate highly fluent sentences, but recent studies have also shown that they are also prone to hallucinate additional content not supported by the input. These variety of fluent but wrong outputs are particularly problematic, as it will not be possible for users to tell they are being presented incorrect content. To detect these errors, we propose a task to predict whether each token in the output sequence is hallucinated (not contained in the input) and collect new manually annotated evaluation sets for this task. We also introduce a method for learning to detect hallucinations using pretrained language models fine tuned on synthetic data that includes automatically inserted hallucinations Experiments on machine translation (MT) and abstractive summarization demonstrate that our proposed approach consistently outperforms strong baselines on all benchmark datasets. We further demonstrate how to use the token-level hallucination labels to define a fine-grained loss over the target sequence in low-resource MT and achieve significant improvements over strong baseline methods.We also apply our method to word-level quality estimation for MT and show its effectiveness in both supervised and unsupervised settings 1 .",
        "id":226254579
      },
      {
        "title":"Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics",
        "text":"Modern summarization models generate highly fluent but often factually unreliable outputs. This motivated a surge of metrics attempting to measure the factuality of automatically generated summaries. Due to the lack of common benchmarks, these metrics cannot be compared. Moreover, all these methods treat factuality as a binary concept and fail to provide deeper insights on the kinds of inconsistencies made by different systems. To address these limitations, we devise a typology of factual errors and use it to collect human annotations of generated summaries from state-of-the-art summarization systems for the CNN\/DM and XSum datasets. Through these annotations we identify the proportion of different categories of factual errors in various summarization models and benchmark factuality metrics, showing their correlation with human judgement as well as their specific strengths and weaknesses.",
        "id":233407441
      },
      {
        "title":"A Simple Recipe towards Reducing Hallucination in Neural Surface Realisation",
        "text":"Recent neural language generation systems often hallucinate contents (i.e., producing irrelevant or contradicted facts), especially when trained on loosely corresponding pairs of the input structure and text. To mitigate this issue, we propose to integrate a language understanding module for data refinement with selftraining iterations to effectively induce strong equivalence between the input data and the paired text. Experiments on the E2E challenge dataset show that our proposed framework can reduce more than 50% relative unaligned noise from the original data-text pairs. A vanilla sequence-to-sequence neural NLG model trained on the refined data has improved on content correctness compared with the current state-of-the-art ensemble generator. * Contribution during internship at Microsoft.MRName Rating Price Golden Palace 5 out of 5 Cheap Reference: Golden Palace is a restaurant specializing in breakfast in the low price range.",
        "id":196183567
      },
      {
        "title":"On Faithfulness and Factuality in Abstractive Summarization",
        "text":"It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.",
        "id":218487034
      },
      {
        "title":"Evaluating the Factual Consistency of Abstractive Text Summarization",
        "text":"The most common metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and generated summaries. Training data is generated by applying a series of rule-based transformations to the sentences of source documents. The factual consistency model is then trained jointly for three tasks: 1) predict whether each summary sentence is factually consistent or not, 2) in either case, extract a span in the source document to support this consistency prediction, 3) for each summary sentence that is deemed inconsistent, extract the inconsistent span from it. Transferring this model to summaries generated by several neural models reveals that this highly scalable approach outperforms previous models, including those trained with strong supervision using datasets from related domains, such as natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency. We also release a manually annotated dataset for factual consistency verification, code for training data generation, and trained model weights at https:\/\/github.com\/salesforce\/factCC.",
        "id":204976362
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"We present the contribution of the Unbabel team to the WMT 2019 Shared Task on Quality Estimation. We participated on the word, sentence, and document-level tracks, encompassing 3 language pairs: English-German, English-Russian, and English-French. Our submissions build upon the recent OpenKiwi framework: we combine linear, neural, and predictor-estimator systems with new transfer learning approaches using BERT and XLM pre-trained models. We compare systems individually and propose new ensemble techniques for word and sentence-level predictions. We also propose a simple technique for converting word labels into document-level predictions. Overall, our submitted systems achieve the best results on all tracks and language pairs by a considerable margin.",
        "id":203316451
      },
      {
        "title":"Semi-automated typical error annotation for learner English essays: integrating frameworks",
        "text":"This paper proposes integration of three open source utilities: brat web annotation tool, Freeling suite of linguistic analyzers and Aspell spellchecker. We demonstrate how their combination can be used to preannotate texts in a learner corpus of English essays with potential errors and ease human annotators' work.Spellchecker alerts and morphological analyzer tagging probabilities are used to detect students' possible errors of most typical sorts. F-measure for the developed pre-annotation framework with regard to human annotation is 0.57, which already makes the system a substantial help to human annotators, but at the same time leaves room for further improvement.",
        "id":6666295
      },
      {
        "title":"Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision",
        "text":"Harnessing the statistical power of neural networks to perform language understanding and symbolic reasoning is difficult, when it requires executing efficient discrete operations against a large knowledge-base. In this work, we introduce a Neural Symbolic Machine (NSM), which contains (a) a neural \"programmer\", i.e., a sequence-to-sequence model that maps language utterances to programs and utilizes a key-variable memory to handle compositionality (b) a symbolic \"computer\", i.e., a Lisp interpreter that performs program execution, and helps find good programs by pruning the search space. We apply REINFORCE to directly optimize the task reward of this structured prediction problem. To train with weak supervision and improve the stability of REINFORCE we augment it with an iterative maximum-likelihood training process. NSM outperforms the state-of-theart on the WEBQUESTIONSSP dataset when trained from question-answer pairs only, without requiring any feature engineering or domain-specific knowledge.",
        "id":2742513
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_acl",
    "question":"Which work should I explore to understand the techniques that expand the scope of open infomation extraction beyond verbs, to include various parts of speech such as nouns and adjectives?",
    "positive_ctxs":[
      {
        "title":"Open Language Learning for Information Extraction",
        "text":"Open Information Extraction (IE) systems extract relational tuples from text, without requiring a pre-specified vocabulary, by identifying relation phrases and associated arguments in arbitrary sentences. However, stateof-the-art Open IE systems such as REVERB and WOE share two important weaknesses -(1) they extract only relations that are mediated by verbs, and (2) they ignore context, thus extracting tuples that are not asserted as factual. This paper presents OLLIE, a substantially improved Open IE system that addresses both these limitations. First, OLLIE achieves high yield by extracting relations mediated by nouns, adjectives, and more. Second, a context-analysis step increases precision by including contextual information from the sentence in the extractions. OLLIE obtains 2.7 times the area under precision-yield curve (AUC) compared to REVERB and 1.9 times the AUC of WOE parse .",
        "id":74065
      }
    ],
    "negative_ctxs":[
      {
        "title":"An Open Dataset and Model for Language Identification",
        "text":"Language identification (LID) is a fundamental step in many natural language processing pipelines. However, current LID systems are far from perfect, particularly on lower-resource languages. We present a LID model which achieves a macro-average F1 score of 0.93 and a false positive rate of 0.033% across 201 languages, outperforming previous work. We achieve this by training on a curated dataset of monolingual data, the reliability of which we ensure by auditing a sample from each source and each language manually. We make both the model and the dataset available to the research community. Finally, we carry out detailed analysis into our model's performance, both in comparison to existing open models and by language class. eventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).",
        "id":258841338
      },
      {
        "title":"When Truth Matters -Addressing Pragmatic Categories in Natural Language Inference (NLI) by Large Language Models (LLMs)",
        "text":"In this paper, we focus on the ability of large language models (LLMs) to accommodate different pragmatic sentence types, such as questions, commands, as well as sentence fragments for natural language inference (NLI). On the commonly used notion of logical inference, nothing can be inferred from a question, a command, or an incomprehensible sentence fragment. We find MNLI, arguably the most important NLI dataset, and hence models fine-tuned on this dataset, insensitive to this fact. Using a symbolic semantic parser, we develop and make publicly available, fine-tuning datasets designed specifically to address this issue, with promising results. We also make a first exploration of ChatGPT's concept of entailment.Stanley Peters. 1979. A truth-conditional formulation ofKarttunen's account of presupposition.",
        "id":260063190
      },
      {
        "title":"Automatic Stochastic Tagging of Natural Language Texts",
        "text":"Five language and tagset independent stochastic taggers, handling morphological and contextual information, are presented and tested in corpora of seven European languages (Dutch, English,  French, German, Greek, Italian and Spanish), using two sets of grammatical tags; a small set containing the eleven main grammatical classes and a large set of grammatical categories common to all languages. The unknown words are tagged using an experimentally proven stochastic hypothesis that links the stochastic behavior of the unknown words with that of the less probable known words. A fully automatic training and tagging program has been implemented on an IBM PC-compatible 80386-based computer. Measurements of error rate, time response, and memory requirements have shown that the taggers\" performance is satisfactory, even though a small training text is available. The error rate is improved when new texts are used to update the stochastic model parameters.",
        "id":7791476
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"*Could you suggest a dataset with legally or ethically contentious content, and labels for acceptable and non-acceptable questions.",
    "positive_ctxs":[
      {
        "title":"SQUARE: A Large-Scale Dataset of Sensitive Questions and Acceptable Responses Created Through Human-Machine Collaboration",
        "text":"The potential social harms that large language models pose, such as generating offensive content and reinforcing biases, are steeply rising. Existing works focus on coping with this concern while interacting with ill-intentioned users, such as those who explicitly make hate speech or elicit harmful responses. However, discussions on sensitive issues can become toxic even if the users are well-intentioned. For safer models in such scenarios, we present the Sensitive Questions and Acceptable Response (SQUARE) dataset, a large-scale Korean dataset of 49k sensitive questions with 42k acceptable and 46k non-acceptable responses. The dataset was constructed leveraging HyperCLOVA in a human-in-the-loop manner based on real news headlines. Experiments show that acceptable response generation significantly improves for HyperCLOVA and GPT-3, demonstrating the efficacy of this dataset.ReferencesSanghwan Bae, Donghyun Kwak, Sungdong Kim, Donghoon Ham, Soyoung Kang, Sang-Woo Lee, and Woomyoung Park. 2022. Building a role specified open-domain dialogue system leveraging largescale language models. In . 2022. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In",
        "id":258960423
      }
    ],
    "negative_ctxs":[
      {
        "title":"Exploitation of Co-reference in Distributional Semantics",
        "text":"The aim of distributional semantics is to model the similarity of the meaning of words via the words they occur with. Thereby, it relies on the distributional hypothesis implying that similar words have similar contexts. Deducing meaning from the distribution of words is interesting as it can be done automatically on large amounts of freely available raw text. It is because of this convenience that most current state-of-the-art-models of distributional semantics operate on raw text, although there have been successful attempts to integrate other kinds of-e.g., syntactic-information to improve distributional semantic models. In contrast, less attention has been paid to semantic information in the research community. One reason for this is that the extraction of semantic information from raw text is a complex, elaborate matter and in great parts not yet satisfyingly solved. Recently, however, there have been successful attempts to integrate a certain kind of semantic information, i.e., co-reference. Two basically different kinds of information contributed by co-reference with respect to the distribution of words will be identified. We will then focus on one of these and examine its general potential to improve distributional semantic models as well as certain more specific hypotheses.",
        "id":23699712
      },
      {
        "title":"Sentiment Analysis and Topic Modeling for Public Perceptions of Air Travel: COVID Issues and Policy Amendments",
        "text":"Among many industries, air travel is impacted by the COVID pandemic. Airlines and airports rely on public sector information to enforce guidelines for ensuring health and safety of travelers. Such guidelines can be policy amendments or laws during the pandemic. In response to the inception of COVID preventive policies, travelers have exercised freedom of expression via the avenue of online reviews. This avenue facilitates voicing public concern while anonymizing \/ concealing user identity as needed. It is important to assess opinions on policy amendments to ensure transparency and openness, while also preserving confidentiality and ethics. Hence, this study leverages data science to analyze, with identity protection, the online reviews of airlines and airports since 2017, considering impacts of COVID issues and relevant policy amendments since 2020. Supervised learning with VADER sentiment analysis is deployed to predict changes in opinion from 2017 to date. Unsupervised learning with LDA topic modeling is employed to discover air travelers' major areas of concern before and after the pandemic. This study reveals that COVID policies have worsened public perceptions of air travel and aroused notable new concerns, affecting economics, environment and health.",
        "id":252624562
      },
      {
        "title":"",
        "text":"",
        "id":237010917
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Are there any papers on training video-language models with contrastive approahes and evaluation on temporal localization tasks?",
    "positive_ctxs":[
      {
        "title":"Contrastive Video-Language Learning with Fine-grained Frame Sampling",
        "text":"Despite recent progress in video and language representation learning, the weak or sparse correspondence between the two modalities remains a bottleneck in the area. Most videolanguage models are trained via pair-level loss to predict whether a pair of video and text is aligned. However, even in paired video-text segments, only a subset of the frames are semantically relevant to the corresponding text, with the remainder representing noise; where the ratio of noisy frames is higher for longer videos. We propose FineCo (Fine-grained Contrastive Loss for Frame Sampling), an approach to better learn video and language representations with a fine-grained contrastive objective operating on video frames. It helps distil a video by selecting the frames that are semantically equivalent to the text, improving cross-modal correspondence. Building on the well established VideoCLIP model as a starting point, FineCo achieves state-of-the-art performance on YouCookII, a text-video retrieval benchmark with long videos. FineCo also achieves competitive results on text-video retrieval (MSR-VTT), and video question answering datasets (MSR-VTT QA and MSR-VTT MC) with shorter videos.",
        "id":252815403
      }
    ],
    "negative_ctxs":[
      {
        "title":"Cross-Lingual Classification of Topics in Political Texts",
        "text":"In this paper, we propose an approach for cross-lingual topical coding of sentences from electoral manifestos of political parties in different languages. To this end, we exploit continuous semantic text representations and induce a joint multilingual semantic vector spaces to enable supervised learning using manually-coded sentences across different languages. Our experimental results show that classifiers trained on multilingual data yield performance boosts over monolingual topic classification.",
        "id":27628985
      },
      {
        "title":"Two Tools for Creating and Visualizing Sub-sentential Alignments of Parallel Text",
        "text":"We present two web-based, interactive tools for creating and visualizing sub-sentential alignments of parallel text. Yawat is a tool to support distributed, manual word-and phrase-alignment of parallel text through an intuitive, web-based interface. Kwipc is an interface for displaying words or bilingual word pairs in parallel, word-aligned context.A key element of the tools presented here is the interactive visualization: alignment information is shown only for one pair of aligned words or phrases at a time. This allows users to explore the alignment space interactively without being overwhelmed by the amount of information available.",
        "id":8106720
      },
      {
        "title":"Identifying Emotion Labels from Psychiatric Social Texts Using Independent Component Analysis",
        "text":"Accessing the web has been an efficient and effective means to acquire self-help knowledge when suffering from depressive problems. Many mental health websites have developed community-based services such as web forums and blogs for Internet users to share their depressive problems with other users and health professionals. Other users or health professionals can then make recommendations in response to these problems. Such communications produce a large number of documents called psychiatric social texts containing rich emotion labels representing different depressive problems. Automatically identify such emotion labels can make online psychiatric services more effective. This study proposes a framework combining latent semantic analysis (LSA) and independent component analysis (ICA) to extract concept-level features for emotion label identification. LSA is used to discover latent concepts that do not frequently occur in psychiatric social texts, and ICA is used to extract independent components by minimizing the term dependence among the concepts. By combining LSA and ICA, more useful latent concepts can be discovered for different emotion labels, and the dependence between them can also be minimized. The discriminant power of classifiers can thus be improved by training them on the independent components with minimized term overlap. Experimental results show that the use of conceptlevel features yielded better performance than the use of word-level features. Additionally, combining LSA and ICA improved the performance of using each LSA and ICA alone. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http:\/\/creativecommons.org\/licenses\/by\/4.0\/",
        "id":14685831
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Are there any recent papers investigating the use of expert and anti-expert models together to guide text generation and mitigate toxic output?",
    "positive_ctxs":[
      {
        "title":"DEXPERTS: Decoding-Time Controlled Text Generation with Experts and Anti-Experts",
        "text":"Despite recent advances in natural language generation, it remains challenging to control attributes of generated text. We propose DEX-PERTS: Decoding-time Experts, a decodingtime method for controlled text generation that combines a pretrained language model with \"expert\" LMs and\/or \"anti-expert\" LMs in a product of experts. Intuitively, under the ensemble, tokens only get high probability if they are considered likely by the experts and unlikely by the anti-experts. We apply DEXPERTS to language detoxification and sentiment-controlled generation, where we outperform existing controllable generation methods on both automatic and human evaluations. Moreover, because DEXPERTS operates only on the output of the pretrained LM, it is effective with (anti-)experts of smaller size, including when operating on GPT-3. Our work highlights the promise of tuning small LMs on text with (un)desirable attributes for efficient decoding-time steering.",
        "id":235313967
      }
    ],
    "negative_ctxs":[
      {
        "title":"Assessing Monotonicity Reasoning in Dutch through Natural Language Inference",
        "text":"In this paper we investigate monotonicity reasoning in Dutch, through a novel Natural Language Inference dataset. Monotonicity reasoning shows to be highly challenging for Transformer-based language models in English and here, we corroborate those findings using a parallel Dutch dataset, obtained by translating the Monotonicity Entailment Dataset of Yanaka et al.(2019). After fine-tuning two Dutch language models BERTje and RobBERT on the Dutch NLI dataset SICK-NL, we find that performance severely drops on the monotonicity reasoning dataset, indicating poor generalization capacity of the models. We provide a detailed analysis of the test results by means of the linguistic annotations in the dataset. We find that models struggle with downward entailing contexts, and argue that this is due to a poor understanding of negation. Additionally, we find that the choice of monotonicity context affects model performance on conjunction and disjunction. We hope that this new resource paves the way for further research in generalization of neural reasoning models in Dutch, and contributes to the development of better language technology for Natural Language Inference, specifically for Dutch.",
        "id":258378229
      },
      {
        "title":"Sentence and Clause Level Emotion Annotation, Detection, and Classification in a Multi-Genre Corpus",
        "text":"Predicting emotion categories (e.g. anger, joy, sadness) expressed by a sentence is challenging due to inherent multi-label smaller pieces such as phrases and clauses. To date, emotion has been studied in single genre, while models of human behaviors or situational awareness in the event of disasters require emotion modeling in multi-genres. In this paper, we expand and unify existing annotated data in different genres (emotional blog post, news title, and movie reviews) using an inventory of 8 emotions from Plutchik's Wheel of Emotions tags. We develop systems for automatically detecting and classifying emotions in text, in different textual genres and granularity levels, namely, sentence and clause levels in a supervised setting. We explore the effectiveness of clause annotation in sentence-level emotion detection and classification (EDC). To our knowledge, our EDC system is the first to target the clause level; further we provide emotion annotation for movie reviews dataset for the first time.",
        "id":21707078
      },
      {
        "title":"",
        "text":"",
        "id":218973847
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Are there any research papers investigating the improvement of radiology report summarization through the application of graph neural networks in conjunction with biomedical entity extraction?",
    "positive_ctxs":[
      {
        "title":"Word Graph Guided Summarization for Radiology Findings",
        "text":"Radiology reports play a critical role in communicating medical findings to physicians. In each report, the impression section summarizes essential radiology findings. In clinical practice, writing impression is highly demanded yet time-consuming and prone to errors for radiologists. Therefore, automatic impression generation has emerged as an attractive research direction to facilitate such clinical practice. Existing studies mainly focused on introducing salient word information to the general text summarization framework to guide the selection of the key content in radiology findings. However, for this task, a model needs not only capture the important words in findings but also accurately describe their relations so as to generate highquality impressions. In this paper, we propose a novel method for automatic impression generation, where a word graph is constructed from the findings to record the critical words and their relations, then a Word Graph guided Summarization model (WGSUM) is designed to generate impressions with the help of the word graph. Experimental results on two datasets, OPENI and MIMIC-CXR, confirm the validity and effectiveness of our proposed approach, where the state-of-the-art results are achieved on both datasets. Further experiments are also conducted to analyze the impact of different graph designs to the performance of our method. 1",
        "id":236477655
      }
    ],
    "negative_ctxs":[
      {
        "title":"Knowledge Portals",
        "text":"Knowledge portals provide views onto domainspecific information on the World Wide Web, thus facilitating their users to find relevant, domainspecific information. The construction of intelligent access and the provisioning of information to knowledge portals, however, remained an ad hoc task requiring extensive manual editing and maintenance by the knowledge portal providers. In order to diminish these efforts we use ontologies as a conceptual backbone for providing, accessing and structuring information in a comprehensive approach for building and maintaining knowledge portals. We have built several experimental and one commercial knowledge portal for knowledge management tasks such as skill management and corporate history analysis that show how our approach is used in practice. This practice, however, has exhibited a number bottlenecks, many of which could be avoided or at least diminished by Human Language Technology. We have used HLT in order to reduce the costs of ontology engineering and in order to narrow the gap between finding knowledge in texts and providing it to the portal.",
        "id":35749924
      },
      {
        "title":"Unsupervised Multi-Domain Adaptation with Feature Embeddings",
        "text":"Representation learning is the dominant technique for unsupervised domain adaptation, but existing approaches have two major weaknesses. First, they often require the specification of \"pivot features\" that generalize across domains, which are selected by taskspecific heuristics. We show that a novel but simple feature embedding approach provides better performance, by exploiting the feature template structure common in NLP problems.",
        "id":399358
      },
      {
        "title":"Navigating through Dense Annotation Spaces",
        "text":"Pattern matching, or querying, over annotations is a general purpose paradigm for inspecting, navigating, mining, and transforming annotation repositories-the common representation basis for modern pipelined text processing frameworks. Configurability of such frameworks and expressiveness of feature structure-based annotation schemes account for the 'high density' of some such annotation repositories. This particular characteristic makes challenging the design of a pattern matching engine, capable of interpreting (or imposing) flat patterns over an arbitrarily dense annotation lattice. We present an approach where a finite state device carries out the application of (compiled) grammars over what is, in effect, a linearized 'projection' of a unique route through the lattice; a route derived by a mix of static pattern (grammar) analysis and interpretation of navigational directives within the extended grammar formalism. Our approach achieves a mix of finite state scanning and lattice traversal for expressive and efficient pattern matching in dense annotations stores.",
        "id":15001324
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Are there any studies investigating example-based approaches to predict user intent in few-shot learning contexts?",
    "positive_ctxs":[
      {
        "title":"CONVFIT: Conversational Fine-Tuning of Pretrained Language Models",
        "text":"Transformer-based language models (LMs) pretrained on large text collections are proven to store a wealth of semantic knowledge. However, 1) they are not effective as sentence encoders when used off-the-shelf, and 2) thus typically lag behind conversationally pretrained (e.g., via response selection) encoders on conversational tasks such as intent detection (ID). In this work, we propose CON-VFIT, a simple and efficient two-stage procedure which turns any pretrained LM into a universal conversational encoder (after Stage 1 CONVFIT-ing) and task-specialised sentence encoder (after Stage 2). We demonstrate that 1) full-blown conversational pretraining is not required, and that LMs can be quickly transformed into effective conversational encoders with much smaller amounts of unannotated data; 2) pretrained LMs can be fine-tuned into task-specialised sentence encoders, optimised for the fine-grained semantics of a particular task. Consequently, such specialised sentence encoders allow for treating ID as a simple semantic similarity task based on interpretable nearest neighbours retrieval. We validate the robustness and versatility of the CON-VFIT framework with such similarity-based inference on the standard ID evaluation sets: CONVFIT-ed LMs achieve state-of-the-art ID performance across the board, with particular gains in the most challenging, few-shot setups. ConvFiT: Stage 1 (Behavioral) fine-tuning on Reddit data Stage 2 loss (xi, xj) = (senti, sentj)",
        "id":237581476
      },
      {
        "title":"Example-Driven Intent Prediction with Observers",
        "text":"A key challenge of dialog systems research is to effectively and efficiently adapt to new domains. A scalable paradigm for adaptation necessitates the development of generalizable models that perform well in few-shot settings. In this paper, we focus on the intent classification problem which aims to identify user intents given utterances addressed to the dialog system. We propose two approaches for improving the generalizability of utterance classification models: (1) observers and (2) example-driven training. Prior work has shown that BERT-like models tend to attribute a significant amount of attention to the [CLS]   token, which we hypothesize results in diluted representations. Observers are tokens that are not attended to, and are an alternative to the [CLS] token as a semantic representation of utterances. Example-driven training learns to classify utterances by comparing to examples, thereby using the underlying encoder as a sentence similarity model. These methods are complementary; improving the representation through observers allows the example-driven model to better measure sentence similarities. When combined, the proposed methods attain state-of-the-art results on three intent prediction datasets (BANKING77, CLINC150,  HWU64)  in both the full data and few-shot (10 examples per intent) settings. Furthermore, we demonstrate that the proposed approach can transfer to new intents and across datasets without any additional training.",
        "id":224725731
      }
    ],
    "negative_ctxs":[
      {
        "title":"RECASTING GRADIENT-BASED META-LEARNING AS HIERARCHICAL BAYES",
        "text":"Meta-learning allows an intelligent agent to leverage prior learning episodes as a basis for quickly improving performance on a novel task. Bayesian hierarchical modeling provides a theoretical framework for formalizing meta-learning as inference for a set of parameters that are shared across tasks. Here, we reformulate the model-agnostic meta-learning algorithm (MAML) of Finn et al.(2017)as a method for probabilistic inference in a hierarchical Bayesian model. In contrast to prior methods for meta-learning via hierarchical Bayes, MAML is naturally applicable to complex function approximators through its use of a scalable gradient descent procedure for posterior inference. Furthermore, the identification of MAML as hierarchical Bayes provides a way to understand the algorithm's operation as a meta-learning procedure, as well as an opportunity to make use of computational strategies for efficient inference. We use this opportunity to propose an improvement to the MAML algorithm that makes use of techniques from approximate inference and curvature estimation.",
        "id":3484654
      },
      {
        "title":"Dependency parser demo",
        "text":"",
        "id":30964362
      },
      {
        "title":"SYMMETRIC PROJECTION IN JAPANESE AND ENGLISH: A MODIFICATION OF STABLER'S PARSING\/GENERATION MODEL FOR MINIMALIST GRAMMAR",
        "text":"ABSTRCATThe essence of standard X-bar theory is that structure building is asymmetric in the sense that a complex structure inherits properties from only one of its constituents. There are some structures, however, that are best analyzed as reflecting the properties of all their constituents. This kind of symmetric projection should in principle be allowed within the minimalist program if the union of the features of all the constituents contains no incompatible features. This claim is supported by the fact that Japanese wh-phrases marked with ka can function as indefinites as well as interrogatives. Under the assumptions that a wh-phrase with ka has the same internal structure regardless of its interpretations and that ka has no category feature, merging a wh-phrase with ka is a case of symmetric projection. The properties of both ka and its sister wh-phrase interact with those of the predicate taking the ka-phrase as its argument or adjunct, which ensures that an appropriate interpretation will be picked up from the two possible interpretations of the ka-phrase.",
        "id":14076419
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Are there any studies investigating sentiment analysis through text-to-graph conversion models that incorporate contextual embeddings?",
    "positive_ctxs":[
      {
        "title":"Direct parsing to sentiment graphs",
        "text":"This paper demonstrates how a graph-based semantic parser can be applied to the task of structured sentiment analysis, directly predicting sentiment graphs from text. We advance the state of the art on 4 out of 5 standard benchmark sets. We release the source code, models and predictions. 1",
        "id":247628083
      }
    ],
    "negative_ctxs":[
      {
        "title":"Prototypical Verbalizer for Prompt-based Few-shot Tuning",
        "text":"Prompt-based tuning for pre-trained language models (PLMs) has shown its effectiveness in few-shot learning. Typically, prompt-based tuning wraps the input text into a cloze question. To make predictions, the model maps the output words to labels via a verbalizer, which is either manually designed or automatically built. However, manual verbalizers heavily depend on domain-specific prior knowledge and human efforts, while finding appropriate label words automatically still remains challenging. In this work, we propose the prototypical verbalizer (ProtoVerb) which is built directly from training data. Specifically, Pro-toVerb learns prototype vectors as verbalizers by contrastive learning. In this way, the prototypes summarize training instances and are able to enclose rich class-level semantics. We conduct experiments on both topic classification and entity typing tasks, and the results demonstrate that ProtoVerb significantly outperforms current automatic verbalizers, especially when training data is extremely scarce. More surprisingly, ProtoVerb consistently boosts promptbased tuning even on untuned PLMs, indicating an elegant non-tuning way to utilize PLMs. Our codes are avaliable at https: \/\/github.com\/thunlp\/OpenPrompt.",
        "id":247594302
      },
      {
        "title":"Methods and Tools for Corpus Lexicography",
        "text":"",
        "id":3247663
      },
      {
        "title":"First Broadcast News Transcription System for Khmer Language",
        "text":"In this paper we present an overview on the development of a large vocabulary continuous speech recognition (LVCSR) system for Khmer, the official language of Cambodia, spoken by more than 15 million people. As an under-resourced language, develop a LVCSR system for Khmer is a challenging task. We describe our methodologies for quick language data collection and processing for language modeling and acoustic modeling. For language modeling, we investigate the use of word and sub-word as basic modeling unit in order to see the potential of sub-word units in the case of unsegmented language like Khmer. Grapheme-based acoustic modeling is used to quickly build our Khmer language acoustic model. Furthermore, the approaches and tools used for the development of our system are documented and made publicly available on the web. We hope this will contribute to accelerate the development of LVCSR system for a new language, especially for under-resource languages of developing countries where resources and expertise are limited.",
        "id":8911750
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Are there any studies on incorporating external commonsense knowledge into conversational models to enhance emotional support?",
    "positive_ctxs":[
      {
        "title":"MISC: A MIxed Strategy-Aware Model Integrating COMET for Emotional Support Conversation",
        "text":"Applying existing methods to emotional support conversation-which provides valuable assistance to people who are in need-has two major limitations: (a) they generally employ a conversation-level emotion label, which is too coarse-grained to capture user's instant mental state; (b) most of them focus on expressing empathy in the response(s) rather than gradually reducing user's distress. To address the problems, we propose a novel model MISC, which firstly infers the user's fine-grained emotional status, and then responds skillfully using a mixture of strategy. Experimental results on the benchmark dataset demonstrate the effectiveness of our method and reveal the benefits of fine-grained emotion understanding as well as mixed-up strategy modeling. Our code and data could be found in https: \/\/github.com\/morecry\/MISC.",
        "id":247748640
      }
    ],
    "negative_ctxs":[
      {
        "title":"GENERALIZED TENSOR MODELS FOR RECURRENT NEURAL NETWORKS",
        "text":"Recurrent Neural Networks (RNNs) are very successful at solving challenging problems with sequential data. However, this observed efficiency is not yet entirely explained by theory. It is known that a certain class of multiplicative RNNs enjoys the property of depth efficiency -a shallow network of exponentially large width is necessary to realize the same score function as computed by such an RNN. Such networks, however, are not very often applied to real life tasks. In this work, we attempt to reduce the gap between theory and practice by extending the theoretical analysis to RNNs which employ various nonlinearities, such as Rectified Linear Unit (ReLU), and show that they also benefit from properties of universality and depth efficiency. Our theoretical results are verified by a series of extensive computational experiments.",
        "id":59413817
      },
      {
        "title":"LIFELONG LEARNING WITH DYNAMICALLY EXPAND- ABLE NETWORKS",
        "text":"We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting\/duplicating units and timestamping them. We validate DEN on multiple public datasets in lifelong learning scenarios on multiple public datasets, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch model with substantially fewer number of parameters.",
        "id":3693512
      },
      {
        "title":"",
        "text":"",
        "id":233364425
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Are there studies examining how well question answering systems perform on queries that cannot be directly recalled from their training data?",
    "positive_ctxs":[
      {
        "title":"Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets",
        "text":"Ideally Open-Domain Question Answering models should exhibit a number of competencies, ranging from simply memorizing questions seen at training time, to answering novel question formulations with answers seen during training, to generalizing to completely novel questions with novel answers. However, single aggregated test set scores do not show the full picture of what capabilities models truly have. In this work, we perform a detailed study of the test sets of three popular open-domain benchmark datasets with respect to these competencies. We find that 60-70% of test-time answers are also present somewhere in the training sets. We also find that 30% of test-set questions have a near-duplicate paraphrase in their corresponding training sets. Using these findings, we evaluate a variety of popular open-domain models to obtain greater insight into what extent they can actually generalize, and what drives their overall performance. We find that all models perform dramatically worse on questions that cannot be memorized from training sets, with a mean absolute performance difference of 63% between repeated and non-repeated data. Finally we show that simple nearest-neighbor models outperform a BART closed-book QA model, further highlighting the role that training set memorization plays in these benchmarks.",
        "id":221005781
      }
    ],
    "negative_ctxs":[
      {
        "title":"A Learning-based Sampling Approach to Extractive Summarization",
        "text":"In this paper we present a novel resampling model for extractive meeting summarization. With resampling based on the output of a baseline classifier, our method outperforms previous research in the field. Further, we compare an existing resampling technique with our model. We report on an extensive series of experiments on a large meeting corpus which leads to classification improvement in weighted precision and f-score.",
        "id":14263284
      },
      {
        "title":"Cutting Recursive Autoencoder Trees",
        "text":"Deep Learning models enjoy considerable success in Natural Language Processing. While deep architectures produce useful representations that lead to improvements in various tasks, they are often difficult to interpret. This makes the analysis of learned structures particularly difficult. In this paper, we rely on empirical tests to see whether a particular structure makes sense. We present an analysis of the Semi-Supervised Recursive Autoencoder, a well-known model that produces structural representations of text. We show that for certain tasks, the structure of the autoencoder can be significantly reduced without loss of classification accuracy and we evaluate the produced structures using human judgment.",
        "id":7788178
      },
      {
        "title":"Ensembles of Classifiers for Cleaning Web Parallel Corpora and Translation Memories",
        "text":"The last years witnessed an increasing interest in the automatic methods for spotting false translation units in translation memories. This problem presents a great interest to industry as there are many translation memories that contain errors. A closely related line of research deals with identifying sentences that do not align in the parallel corpora mined from the web. The task of spotting false translations is modeled as a binary classification problem. It is known that in certain conditions the ensembles of classifiers improve over the performance of the individual members. In this paper we benchmark the most popular ensemble of classifiers: Majority Voting, Bagging, Stacking and Ada Boost at the task of spotting false translation units for translation memories and parallel web corpora. We want to know if for this specific problem any ensemble technique improves the performance of the individual classifiers and if there is a difference between the data in translation memories and parallel web corpora with respect to this task.",
        "id":150847
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Are there studies that investigate debiasing language models automatically using prompting?",
    "positive_ctxs":[
      {
        "title":"Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts",
        "text":"Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task. In this paper, we propose an automatic method to mitigate the biases in pretrained language models. Different from previous debiasing work that uses external corpora to finetune the pretrained models, we instead directly probe the biases encoded in pretrained models through prompts. Specifically, we propose a variant of the beam search method to automatically search for biased prompts such that the cloze-style completions are the most different with respect to different demographic groups. Given the identified biased prompts, we then propose a distribution alignment loss to mitigate the biases. Experiment results on standard datasets and metrics show that our proposed Auto-Debias approach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT. Moreover, the improvement in fairness does not decrease the language models' understanding abilities, as shown using the GLUE benchmark.",
        "id":248780440
      }
    ],
    "negative_ctxs":[
      {
        "title":"MULTI-CLASS CLASSIFICATION WITHOUT MULTI- CLASS LABELS",
        "text":"This work presents a new strategy for multi-class classification that requires no class-specific labels, but instead leverages pairwise similarity between examples, which is a weaker form of annotation. The proposed method, meta classification learning, optimizes a binary classifier for pairwise similarity prediction and through this process learns a multi-class classifier as a submodule. We formulate this approach, present a probabilistic graphical model for it, and derive a surprisingly simple loss function that can be used to learn neural network-based models. We then demonstrate that this same framework generalizes to the supervised, unsupervised cross-task, and semi-supervised settings. Our method is evaluated against state of the art in all three learning paradigms and shows a superior or comparable accuracy, providing evidence that learning multi-class classification without multi-class labels is a viable learning option.",
        "id":57375742
      },
      {
        "title":"",
        "text":"",
        "id":235097504
      },
      {
        "title":"NEURAL GRAPH EVOLUTION: TOWARDS EFFICIENT AUTOMATIC ROBOT DESIGN",
        "text":"Despite the recent successes in robotic locomotion control, the design of robots, i.e., the design of their body structure, still heavily relies on human engineering. Automatic robot design has been a long studied subject, however, progress has been slow due to large combinatorial search space and the difficulty to efficiently evaluate the candidate structures. Note that one needs to both, search over many possible body structures, and choose among them based on how the robot with that structure performs in an environment. The latter means training an optimal controller given a candidate structure, which in itself is costly to obtain. In this paper, we propose Neural Graph Evolution (NGE), which performs evolutionary search in graph space, by iteratively evolving graph structures using simple mutation primitives. Key to our approach is to parameterize the control policies with graph neural networks, which allows us to transfer skills from previously evaluated designs during the graph search. This significantly reduces evaluation cost of new candidates and makes the search process orders of magnitude more efficient than that of past work. In addition, NGE applies Graph Mutation with Uncertainty (GM-UC) by incorporating model uncertainty, which reduces the search space by balancing exploration and exploitation. We show that NGE significantly outperforms previous methods in terms of convergence rate and final performance. As shown in experiments, NGE is the first algorithm that can automatically discover kinematically preferred robotic graph structures, such as a fish with two symmetric flat side-fins and a tail, or a cheetah with athletic front and back legs. NGE is extremely efficient, it finds plausible robotic structures within a day on a single 64 CPU-core Amazon EC2 machine. The code and project webpage are released 1 . * Two authors contribute equally.",
        "id":88503668
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Can you give me a paper that does self-supervised contrastive learning of sentence embeddings by sampling in-batch negatives?",
    "positive_ctxs":[
      {
        "title":"SimCSE: Simple Contrastive Learning of Sentence Embeddings",
        "text":"This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using \"entailment\" pairs as positives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2% and 2.2% improvement compared to previous best results. We also show-both theoretically and empirically-that contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available. 1",
        "id":233296292
      }
    ],
    "negative_ctxs":[
      {
        "title":"Spanish HPSG Treebank based on the AnCora Corpus",
        "text":"This paper describes a corpus of HPSG annotated trees for Spanish that contains morphosyntactic information, annotations for semantic roles, clitic pronouns and relative clauses. The corpus is based on the Spanish AnCora corpus, which contains trees for 17,000 sentences comprising half a million words, and it has CFG style annotations. The corpus is stored in two different formats: An XML dialect that is the direct serialization of the typed feature structure trees, and an HTML format that is suitable for visualizing the trees in a browser.",
        "id":21691538
      },
      {
        "title":"",
        "text":"",
        "id":207905114
      },
      {
        "title":"h-DETACH: MODIFYING THE LSTM GRADIENT TO- WARDS BETTER OPTIMIZATION",
        "text":"Recurrent neural networks are known for their notorious exploding and vanishing gradient problem (EVGP). This problem becomes more evident in tasks where the information needed to correctly solve them exist over long time scales, because EVGP prevents important gradient components from being back-propagated adequately over a large number of steps. We introduce a simple stochastic algorithm (h-detach) that is specific to LSTM optimization and targeted towards addressing this problem. Specifically, we show that when the LSTM weights are large, the gradient components through the linear path (cell state) in the LSTM computational graph get suppressed. Based on the hypothesis that these components carry information about long term dependencies (which we show empirically), their suppression can prevent LSTMs from capturing them. Our algorithm prevents gradients flowing through this path from getting suppressed, thus allowing the LSTM to capture such dependencies better. We show significant convergence and generalization improvements using our algorithm on various benchmark datasets.",
        "id":52937611
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Can you recommend a dialogue summarization dataset mined from broadcast interviews on the TV or radio?",
    "positive_ctxs":[
      {
        "title":"MEDIASUM: A Large-scale Media Interview Dataset for Dialogue Summarization",
        "text":"This paper introduces MEDIASUM 1 , a largescale media interview dataset consisting of 463.6K transcripts with abstractive summaries. To create this dataset, we collect interview transcripts from NPR and CNN and employ the overview and topic descriptions as summaries. Compared with existing public corpora for dialogue summarization, our dataset is an order of magnitude larger and contains complex multi-party conversations from multiple domains. We conduct statistical analysis to demonstrate the unique positional bias exhibited in the transcripts of televised and radioed interviews. We also show that MEDIASUM can be used in transfer learning to improve a model's performance on other dialogue summarization tasks. * Equal contribution 1 https:\/\/github.com\/zcgzcgzcg1\/ MediaSum\/",
        "id":232185439
      }
    ],
    "negative_ctxs":[
      {
        "title":"Does a Computational Linguist have to be a Linguist?",
        "text":"Invited Speaker AbstractEarly computational linguists supplied much of theoretical basis that the ALPAC report said was needed for research on the practical problem of machine translation. The result of their efforts turned out to be more fundamental in that it provided a general theoretical basis for the study of language use as a process, giving rise eventually to constraint-based grammatical formalisms for syntax, finite-state approaches to morphology and phonology, and a host of models how speakers might assemble sentences, and hearers take them apart. Recently, an entirely new enterprise, based on machine learning and big data, has sprung on the scene and challenged the ALPAC committee's finding that linguistic processing must have a firm basis in linguistic theory. In this talk, I will show that the long-term development of linguistic processing requires linguistic theory, sophisticated statistical manipulation of big data, and a third component which is not linguistic at all. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http:\/\/creativecommons.org\/licenses\/by\/4.0\/ 2030",
        "id":43656672
      },
      {
        "title":"État de l'art : l'influence du domaine sur la classification de l'opinion Dis-moi de quoi tu parles, je te dirai ce que tu penses",
        "text":"L'intérêt pour la fouille d'opinion s'est développé en même temps que se sont répandus les blogs, forums et autres plate-formes où les internautes peuvent librement exprimer leur opinion. La très grande quantité de données disponibles oblige à avoir recours à des traitements automatiques de fouille d'opinion. Cependant, la manière dont les gens expriment leur avis change selon ce dont ils parlent. Les distributions des mots utilisés sont différentes d'un domaine à l'autre. Aussi, il est très difficile d'obtenir un classifieur d'opinion fonctionnant sur tous les domaines. De plus, on ne peut appliquer sans adaptation sur un domaine cible un classifieur entraîné sur un domaine source différent. L'objet de cet article est de recenser les moyens de résoudre ce problème difficile.ABSTRACTState of the Art : Influence of Domain on Opinion ClassificationThe interest in opinion mining has grown concurrently with blogs, forums, and others platforms where the internauts can freely write about their opinion on every topic. As the amounts of available data are increasingly huge, the use of automatic methods for opinion mining becomes imperative. However, sentiment is expressed differently in different domains : words distributions can indeed differ significantly. An effective global opinion classifier is therefore hard to develop. Moreover, a classifier trained on a source domain can't be used without adaptation on a target domain. This article aims to describe the state-of-the-art methods used to solve this difficult task. MOTS-CLÉS : État de l'art, Fouille d'opinion, Multi-domaines, Cross-domaines.",
        "id":45292586
      },
      {
        "title":"Phrase-Based Translation Model for Question Retrieval in Community Question Answer Archives",
        "text":"Community-based question answer (Q&A) has become an important issue due to the popularity of Q&A archives on the web. This paper is concerned with the problem of question retrieval. Question retrieval in Q&A archives aims to find historical questions that are semantically equivalent or relevant to the queried questions. In this paper, we propose a novel phrase-based translation model for question retrieval. Compared to the traditional word-based translation models, the phrasebased translation model is more effective because it captures contextual information in modeling the translation of phrases as a whole, rather than translating single words in isolation. Experiments conducted on real Q&A data demonstrate that our proposed phrasebased translation model significantly outperforms the state-of-the-art word-based translation model.",
        "id":2726891
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Can you recommend research that uses an LLM to generate better prompts\/tempates given task input\/output?",
    "positive_ctxs":[
      {
        "title":"Making Pre-trained Language Models Better Few-shot Learners",
        "text":"The recent GPT-3 model(Brown et al., 2020)achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF-better few-shot fine-tuning of language models 1 -a suite of simple and complementary techniques for finetuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning. 2 * The first two authors contributed equally. 1 Alternatively, language models' best friends forever. 2 Our implementation is publicly available at https:\/\/ github.com\/princeton-nlp\/LM-BFF. Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Association for Computational Linguistics (ACL). Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Association for Computational Linguistics (ACL).",
        "id":229923710
      }
    ],
    "negative_ctxs":[
      {
        "title":"Fast and Accurate Unlexicalized Parsing via Structural Annotations",
        "text":"We suggest a new annotation scheme for unlexicalized PCFGs that is inspired by formal language theory and only depends on the structure of the parse trees. We evaluate this scheme on the TüBa-D\/Z treebank w.r.t. several metrics and show that it improves both parsing accuracy and parsing speed considerably. We also show that our strategy can be fruitfully combined with known ones like parent annotation to achieve accuracies of over 90% labeled F 1 and leaf-ancestor score. Despite increasing the size of the grammar, our annotation allows for parsing more than twice as fast as the PCFG baseline.",
        "id":15708792
      },
      {
        "title":"",
        "text":"",
        "id":187484830
      },
      {
        "title":"Data-Driven Sentence Simplification: Survey and Benchmark",
        "text":"Sentence Simplification (SS) aims to modify a sentence in order to make it easier to read and understand. In order to do so, several rewriting transformations can be performed such as replacement, reordering, and splitting. Executing these transformations while keeping sentences grammatical, preserving their main idea, and generating simpler output, is a challenging and still far from solved problem. In this article, we survey research on SS, focusing on approaches that attempt to learn how to simplify using corpora of aligned original-simplified sentence pairs in English, which is the dominant paradigm nowadays. We also include a benchmark of different approaches on common data sets so as to compare them and highlight their strengths and limitations. We expect that this survey will serve as a starting point for researchers interested in the task and help spark new ideas for future developments.",
        "id":209536754
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Can you show me a paper that built a large structured knowledge base from wikipedia, that can then be used for entity linking and ranking tasks?",
    "positive_ctxs":[
      {
        "title":"KILT: a Benchmark for Knowledge Intensive Language Tasks",
        "text":"Challenging problems such as open-domain question answering, fact checking, slot filling and entity linking require access to large, external knowledge sources. While some models do well on individual tasks, developing general models is difficult as each task might require computationally expensive indexing of custom knowledge sources, in addition to dedicated infrastructure. To catalyze research on models that condition on specific information in large textual resources, we present a benchmark for knowledge-intensive language tasks (KILT). All tasks in KILT are grounded in the same snapshot of Wikipedia, reducing engineering turnaround through the reuse of components, as well as accelerating research into task-agnostic memory architectures. We test both task-specific and general baselines, evaluating downstream performance in addition to the ability of the models to provide provenance. We find that a shared dense vector index coupled with a seq2seq model is a strong baseline, outperforming more tailor-made approaches for fact checking, open-domain question answering and dialogue, and yielding competitive results on entity linking and slot filling, by generating disambiguated text. KILT data and code are available at https:\/\/github.com\/ facebookresearch\/KILT.",
        "id":221507798
      }
    ],
    "negative_ctxs":[
      {
        "title":"Identifying the Most Dominant Event in a News Article by Mining Event Coreference Relations",
        "text":"Identifying the most dominant and central event of a document, which governs and connects other foreground and background events in the document, is useful for many applications, such as text summarization, storyline generation and text segmentation. We observed that the central event of a document usually has many coreferential event mentions that are scattered throughout the document for enabling a smooth transition of subtopics. Our empirical experiments, using gold event coreference relations, have shown that the central event of a document can be well identified by mining properties of event coreference chains. But the performance drops when switching to system predicted event coreference relations. In addition, we found that the central event can be more accurately identified by further considering the number of sub-events as well as the realis status of an event.",
        "id":4902473
      },
      {
        "title":"Addressing Limitations of Encoder-Decoder Based Approach to Text-to-SQL",
        "text":"Most attempts on Text-to-SQL task using encoder-decoder approach show a big problem of dramatic decline in performance for new databases. Models trained on Spider dataset, despite achieving 75% accuracy on Spider development or test sets, show a huge decline below 20% accuracy for databases not in Spider. We present a system that combines automated training-data augmentation and ensemble technique. We achieve double-digit percentage improvement for databases that are not part of the Spider corpus.",
        "id":252819055
      },
      {
        "title":"Fast Statistical Parsing of Noun Phrases for Document Indexing",
        "text":"Information Retrieval (IR) is an important application area of Natural Language Processing (NLP) where one encounters the genuine challenge of processing large quantities of unrestricted natural language text. While much effort has been made to apply NLP techniques to IR, very few NLP techniques have been evaluated on a document collection larger than several megabytes. Many NLP techniques are simply not efficient enough, and not robust enough, to handle a large amount of text. This paper proposes a new probabilistic model for noun phrase parsing, and reports on the application of such a parsing technique to enhance document indexing. The effectiveness of using syntactic phrases provided by the parser to supplement single words for indexing is evaluated with a 250 megabytes document collection. The experiment's results show that supplementing single words with syntactic phrases for indexing consistently and significantly improves retrieval performance.",
        "id":3262098
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Can you suggest research that deals with the multiple input sources in clinical text processing by combining several recurrent modules which are each responsible for a single source of information?",
    "positive_ctxs":[
      {
        "title":"",
        "text":"",
        "id":235097235
      }
    ],
    "negative_ctxs":[
      {
        "title":"Suggestion Mining from Opinionated Text",
        "text":"In addition to the positive and negative sentiments expressed by speakers, opinions on the web also convey suggestions. Such text comprise of advice, recommendations and tips on a variety of points of interest. We propose that suggestions can be extracted from the available opinionated text and put to several use cases. The problem has been identified only recently as a viable task, and there is a lot of scope for research in the direction of problem definition, datasets, and methods. From an abstract view, standard algorithms for tasks like sentence classification and keyphrase extraction appear to be usable for suggestion mining. However, initial experiments reveal that there is a need for new methods, or variations in the existing ones for addressing the problem specific challenges. We present a research proposal which divides the problem into three main research questions; we walk through them, presenting our analysis, results, and future directions.",
        "id":14195818
      },
      {
        "title":"YNU-HPCC at SemEval-2021 Task 6: Combining ALBERT and Text-CNN for Persuasion Detection in Texts and Images",
        "text":"In recent years, memes combining image and text have been widely used in social media, and memes are one of the most popular types of content used in online disinformation campaigns. In this paper, our study on the detection of persuasion techniques in texts and images in SemEval-2021 Task 6 is summarized. For propaganda technology detection in text, we propose a combination model of both AL-BERT and Text-CNN for text classification, as well as a BERT-based multi-task sequence labeling model for propaganda technology coverage span detection. For the meme classification task involved in text understanding and visual feature extraction, we designed a parallel channel model divided into text and image channels. Our method 1 achieved a good performance on subtasks 1 and 3. The micro F 1scores of 0.492, 0.091, and 0.446 achieved on the test sets of the three subtasks ranked 12th, 7th, and 11th, respectively, and all are higher than the baseline model.",
        "id":236460072
      },
      {
        "title":"Back up your Stance: Recognizing Arguments in Online Discussions",
        "text":"In online discussions, users often back up their stance with arguments. Their arguments are often vague, implicit, and poorly worded, yet they provide valuable insights into reasons underpinning users' opinions. In this paper, we make a first step towards argument-based opinion mining from online discussions and introduce a new task of argument recognition. We match usercreated comments to a set of predefined topic-based arguments, which can be either attacked or supported in the comment. We present a manually-annotated corpus for argument recognition in online discussions. We describe a supervised model based on comment-argument similarity and entailment features. Depending on problem formulation, model performance ranges from 70.5% to 81.8% F1-score, and decreases only marginally when applied to an unseen topic.",
        "id":15983978
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you direct me to research that evaluates few-shot slot tagging model performance by averaging micro-F1 scores across different test episodes?",
    "positive_ctxs":[
      {
        "title":"Inverse is Better! Fast and Accurate Prompt for Few-shot Slot Tagging",
        "text":"Prompting methods recently achieve impressive success in few-shot learning. These methods modify input samples with prompt sentence pieces, and decode label tokens to map samples to corresponding labels. However, such a paradigm is very inefficient for the task of slot tagging. Since slot tagging samples are multiple consecutive words in a sentence, the prompting methods have to enumerate all n-grams token spans to find all the possible slots, which greatly slows down the prediction. To tackle this, we introduce an inverse paradigm for prompting. Different from the classic prompts mapping tokens to labels, we reversely predict slot values given slot types. Such inverse prompting only requires a oneturn prediction for each slot type and greatly speeds up the prediction. Besides, we propose a novel Iterative Prediction Strategy, from which the model learns to refine predictions by considering the relations between different slot types. We find, somewhat surprisingly, the proposed method not only predicts faster but also significantly improves the effect (improve over 6.1 F1-scores on 10-shot setting) and achieves new state-of-the-art performance.",
        "id":247939641
      }
    ],
    "negative_ctxs":[
      {
        "title":"An Analysis of Active Learning Strategies for Sequence Labeling Tasks",
        "text":"Active learning is well-suited to many problems in natural language processing, where unlabeled data may be abundant but annotation is slow and expensive. This paper aims to shed light on the best active learning approaches for sequence labeling tasks such as information extraction and document segmentation. We survey previously used query selection strategies for sequence models, and propose several novel algorithms to address their shortcomings. We also conduct a large-scale empirical comparison using multiple corpora, which demonstrates that our proposed methods advance the state of the art.",
        "id":8197231
      },
      {
        "title":"GPLSI-IXA: Using Semantic Classes to Acquire Monosemous Training Examples from Domain Texts",
        "text":"This paper summarizes our participation in task #17 of SemEval-2 (All-words WSD on a specific domain) using a supervised class-based Word Sense Disambiguation system. Basically, we use Support Vector Machines (SVM) as learning algorithm and a set of simple features to build three different models. Each model considers a different training corpus: Sem-Cor (SC), examples from monosemous words extracted automatically from background data (BG), and both SC and BG (SCBG). Our system explodes the monosemous words appearing as members of a particular WordNet semantic class to automatically acquire class-based annotated examples from the domain text. We use the class-based examples gathered from the domain corpus to adapt our traditional system trained on SemCor. The evaluation reveal that the best results are achieved training with SemCor and the background examples from monosemous words, obtaining results above the first sense baseline and the fifth best position in the competition rank.",
        "id":17053517
      },
      {
        "title":"Hate-Speech and Offensive Language Detection in Roman Urdu",
        "text":"The task of automatic hate-speech and offensive language detection in social media content is of utmost importance due to its implications in unprejudiced society concerning race, gender, or religion. Existing research in this area, however, is mainly focused on the English language, limiting the applicability to particular demographics. Despite its prevalence, Roman Urdu (RU) lacks language resources, annotated datasets, and language models for this task. In this study, we: (1) Present a lexicon of hateful words in RU, (2) Develop an annotated dataset called RUHSOLD consisting of 10, 012 tweets in RU with both coarse-grained and fine-grained labels of hate-speech and offensive language, (3) Explore the feasibility of transfer learning of five existing embedding models to RU, (4) Propose a novel deep learning architecture called CNN-gram for hatespeech and offensive language detection and compare its performance with seven current baseline approaches on RUHSOLD dataset, and (5) Train domain-specific embeddings on more than 4.7 million tweets and make them publicly available. We conclude that transfer learning is more beneficial as compared to training embedding from scratch and that the proposed model exhibits greater robustness as compared to the baselines.",
        "id":226262272
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you direct me to studies investigating the enhancement of bi-encoder text matching performance through the application of knowledge distillation methods?",
    "positive_ctxs":[
      {
        "title":"VIRT: Improving Representation-based Text Matching via Virtual Interaction",
        "text":"Text matching is a fundamental research problem in natural language understanding. Interaction-based approaches treat the text pair as a single sequence and encode it through cross encoders, while representation-based models encode the text pair independently with siamese or dual encoders. Interactionbased models require dense computations and thus are impractical in real-world applications. Representation-based models have become the mainstream paradigm for efficient text matching. However, these models suffer from severe performance degradation due to the lack of interactions between the pair of texts. To remedy this, we propose a Virtual InteRacTion mechanism (VIRT) for improving representation-based text matching while maintaining its efficiency. In particular, we introduce an interactive knowledge distillation module that is only applied during training. It enables deep interaction between texts by effectively transferring knowledge from the interaction-based model. A light interaction strategy is designed to fully leverage the learned interactive knowledge. Experimental results on six text matching benchmarks demonstrate the superior performance of our method over several state-of-the-art representationbased models. We further show that VIRT can be integrated into existing methods as plugins to lift their performances.",
        "id":256461055
      },
      {
        "title":"VIRT: Improving Representation-based Text Matching via Virtual Interaction",
        "text":"Text matching is a fundamental research problem in natural language understanding. Interaction-based approaches treat the text pair as a single sequence and encode it through cross encoders, while representation-based models encode the text pair independently with siamese or dual encoders. Interactionbased models require dense computations and thus are impractical in real-world applications. Representation-based models have become the mainstream paradigm for efficient text matching. However, these models suffer from severe performance degradation due to the lack of interactions between the pair of texts. To remedy this, we propose a Virtual InteRacTion mechanism (VIRT) for improving representation-based text matching while maintaining its efficiency. In particular, we introduce an interactive knowledge distillation module that is only applied during training. It enables deep interaction between texts by effectively transferring knowledge from the interaction-based model. A light interaction strategy is designed to fully leverage the learned interactive knowledge. Experimental results on six text matching benchmarks demonstrate the superior performance of our method over several state-of-the-art representationbased models. We further show that VIRT can be integrated into existing methods as plugins to lift their performances.",
        "id":244954670
      }
    ],
    "negative_ctxs":[
      {
        "title":"Missing Counter-Evidence Renders NLP Fact-Checking Unrealistic for Misinformation",
        "text":"Misinformation emerges in times of uncertainty when credible information is limited. This is challenging for NLP-based fact-checking as it relies on counter-evidence, which may not yet be available. Despite increasing interest in automatic fact-checking, it is still unclear if automated approaches can realistically refute harmful real-world misinformation. Here, we contrast and compare NLP fact-checking with how professional fact-checkers combat misinformation in the absence of counter-evidence. In our analysis, we show that, by design, existing NLP task definitions for fact-checking cannot refute misinformation as professional fact-checkers do for the majority of claims. We then define two requirements that the evidence in datasets must fulfill for realistic factchecking: It must be (1) sufficient to refute the claim and (2) not leaked from existing fact-checking articles. We survey existing factchecking datasets and find that all of them fail to satisfy both criteria. Finally, we perform experiments to demonstrate that models trained on a large-scale fact-checking dataset rely on leaked evidence, which makes them unsuitable in real-world scenarios. Taken together, we show that current NLP fact-checking cannot realistically combat real-world misinformation because it depends on unrealistic assumptions about counter-evidence in the data 1 .",
        "id":253107194
      },
      {
        "title":"Cross-media Cross-genre Information Ranking Multi-media Information Networks",
        "text":"Current web technology has brought us a scenario that information about a certain topic is widely dispersed in data from different domains and data modalities, such as texts and images from news and social media. Automatic extraction of the most informative and important multimedia summary (e.g. a ranked list of inter-connected texts and images) from massive amounts of cross-media and cross-genre data can significantly save users' time and effort that is consumed in browsing. In this paper, we propose a novel method to address this new task based on automatically constructed Multi-media Information Networks (MiNets) by incorporating cross-genre knowledge and inferring implicit similarity across texts and images. The facts from MiNets are exploited in a novel random walk-based algorithm to iteratively propagate ranking scores across multiple data modalities. Experimental results demonstrated the effectiveness of our MiNets-based approach and the power of cross-media cross-genre inference.",
        "id":11029275
      },
      {
        "title":"Approximate Searching for Distributional Similarity",
        "text":"Distributional similarity requires large volumes of data to accurately represent infrequent words. However, the nearestneighbour approach to finding synonyms suffers from poor scalability. The Spatial Approximation Sample Hierarchy (SASH), proposed by Houle (2003b), is a data structure for approximate nearestneighbour queries that balances the efficiency\/approximation trade-off. We have intergrated this into an existing distributional similarity system, tripling efficiency with a minor accuracy penalty.",
        "id":158338
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you point me to research that tackles the issue of disambiguating word senses in infrequent and zero-shot scenarios?",
    "positive_ctxs":[
      {
        "title":"Rare and Zero-shot Word Sense Disambiguation using Z-Reweighting",
        "text":"Word sense disambiguation (WSD) is a crucial problem in the natural language processing (NLP) community. Current methods achieve decent performance by utilizing supervised learning and large pre-trained language models. However, the imbalanced training dataset leads to poor performance on rare senses and zero-shot senses. There are more training instances and senses for words with top frequency ranks than those with low frequency ranks in the training dataset. We investigate the statistical relation between word frequency rank and word sense number distribution. Based on the relation, we propose a Z-reweighting method on the word level to adjust the training on the imbalanced dataset. The experiments show that the Z-reweighting strategy achieves performance gain on the standard English all words WSD benchmark. Moreover, the strategy can help models generalize better on rare and zero-shot senses.",
        "id":248779910
      }
    ],
    "negative_ctxs":[
      {
        "title":"Building A Chinese WordNet Via Class-Based Translation Model",
        "text":"Semantic lexicons are indispensable to research in lexical semantics and word sense disambiguation (WSD). For the study of WSD for English text, researchers have been using different kinds of lexicographic resources, including machine readable dictionaries (MRDs), machine readable thesauri, and bilingual corpora. In recent years, WordNet has become the most widely used resource for the study of WSD and lexical semantics in general. This paper describes the Class-Based Translation Model and its application in assigning translations to nominal senses in WordNet in order to build a prototype Chinese WordNet. Experiments and evaluations show that the proposed approach can potentially be adopted to speed up the construction of WordNet for Chinese and other languages.",
        "id":18521734
      },
      {
        "title":"Covid or not Covid? Topic Shift in Information Cascades on Twitter",
        "text":"Social media have become a valuable source of information. However, its power to shape public opinion can be dangerous, especially in the case of misinformation. The existing studies on misinformation detection hypothesise that the initial message is fake. In contrast, we focus on information distortion occurring in cascades as the initial message is quoted or receives a reply. We show a significant topic shift in information cascades on Twitter during the Covid-19 pandemic providing valuable insights for the automatic analysis of information distortion.This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:\/\/ creativecommons.org\/licenses\/by\/4.0\/.",
        "id":227231347
      },
      {
        "title":"OpenUE: An Open Toolkit of Universal Extraction from Text",
        "text":"Natural language processing covers a wide variety of tasks with token-level or sentencelevel understandings. In this paper, we provide a simple insight that most tasks can be represented in a single universal extraction format. We introduce a prototype model and provide an open-source and extensible toolkit called OpenUE for various extraction tasks. OpenUE allows developers to train custom models to extract information from the text and supports quick model validation for researchers. Besides, OpenUE provides various functional modules to maintain sufficient modularity and extensibility. Except for the toolkit, we also deploy an online demo 1 with restful APIs to support real-time extraction without training and deploying. Additionally, the online system can extract information in various tasks, including relational triple extraction, slot & intent detection, event extraction, and so on. We release the source code, datasets, and pretrained models to promote future researches in",
        "id":226283775
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a contemporary research paper that has advanced natural language watermarking quality through algorithmic methods?",
    "positive_ctxs":[
      {
        "title":"Robust Multi-bit Natural Language Watermarking through Invariant Features",
        "text":"Recent years have witnessed a proliferation of valuable original natural language contents found in subscription-based media outlets, web novel platforms, and outputs of large language models. However, these contents are susceptible to illegal piracy and potential misuse without proper security measures. This calls for a secure watermarking system to guarantee copyright protection through leakage tracing or ownership identification. To effectively combat piracy and protect copyrights, a multi-bit watermarking framework should be able to embed adequate bits of information and extract the watermarks in a robust manner despite possible corruption. In this work, we explore ways to advance both payload and robustness by following a well-known proposition from image watermarking and identify features in natural language that are invariant to minor corruption. Through a systematic analysis of the possible sources of errors, we further propose a corruption-resistant infill model. Our full method improves upon the previous work on robustness by +16.8% point on average on four datasets, three corruption types, and two corruption ratios. 1",
        "id":259129912
      }
    ],
    "negative_ctxs":[
      {
        "title":"Relation Inference in Lexical Networks ... with Refinements",
        "text":"Improving lexical network's quality is an important issue in the creation process of these language resources. This can be done by automatically inferring new relations from already existing ones with the purpose of (1) densifying the relations to cover the eventual lack of information and (2) detecting errors. In this paper, we devise such an approach applied to the JeuxDeMots lexical network, which is a freely available lexical and semantic resource for French. We first present the principles behind the lexical network construction with crowdsourcing and games with a purpose and illustrated them with JeuxDeMots (JDM). Then, we present the outline of an elicitation engine based on an inference engine using schemes like deduction, induction and abduction which will be referenced and briefly presented and we will especially highlight the new scheme (Relation Inference Scheme with Refinements) added to our system. An experiment showing the relevance of this scheme is then presented.",
        "id":1699492
      },
      {
        "title":"An Improved Crowdsourcing Based Evaluation Technique for Word Embedding Methods",
        "text":"In this proposal track paper, we have presented a crowdsourcing-based word embedding evaluation technique that will be more reliable and linguistically justified. The method is designed for intrinsic evaluation and extends the approach proposed in(Schnabel et al., 2015). Our improved evaluation technique captures word relatedness based on the word context.",
        "id":515075
      },
      {
        "title":"Neural Generation Meets Real People: Building a Social, Informative Open-Domain Dialogue Agent",
        "text":"We present Chirpy Cardinal, an open-domain social chatbot. Aiming to be both informative and conversational, our bot chats with users in an authentic, emotionally intelligent way. By integrating controlled neural generation with scaffolded, hand-written dialogue, we let both the user and bot take turns driving the conversation, producing an engaging and socially fluent experience. Deployed in the fourth iteration of the Alexa Prize Socialbot Grand Challenge, Chirpy Cardinal handled thousands of conversations per day, placing second out of nine bots with an average user rating of 3.58\/5.",
        "id":251040192
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a paper that builds a writing assistant with autocomplete capabilities conditioned on user intent?",
    "positive_ctxs":[
      {
        "title":"IGA: An Intent-Guided Authoring Assistant",
        "text":"While large-scale pretrained language models have significantly improved writing assistance functionalities such as autocomplete, more complex and controllable writing assistants have yet to be explored. We leverage advances in language modeling to build an interactive writing assistant that generates and rephrases text according to fine-grained author specifications. Users provide input to our Intent-Guided Assistant (IGA) in the form of text interspersed with tags that correspond to specific rhetorical directives (e.g., adding description or contrast, or rephrasing a particular sentence). We fine-tune a language model on a dataset heuristically-labeled with author intent, which allows IGA to fill in these tags with generated text that users can subsequently edit to their liking. A series of automatic and crowdsourced evaluations confirm the quality of IGA's generated outputs, while a smallscale user study demonstrates author preference for IGA over baseline methods in a creative writing task. We release our dataset, code, and demo to spur further research into AI-assisted writing.",
        "id":233231544
      }
    ],
    "negative_ctxs":[
      {
        "title":"Automatic Chinese Abbreviation Generation Using Conditional Random Field",
        "text":"This paper presents a new method for automatically generating abbreviations for Chinese organization names. Abbreviations are commonly used in spoken Chinese, especially for organization names. The generation of Chinese abbreviation is much more complex than English abbreviations, most of which are acronyms and truncations. The abbreviation generation process is formulated as a character tagging problem and the conditional random field (CRF) is used as the tagging model. A carefully selected group of features is used in the CRF model. After generating a list of abbreviation candidates using the CRF, a length model is incorporated to re-rank the candidates. Finally the full-name and abbreviation co-occurrence information from a web search engine is utilized to further improve the performance. We achieved top-10 coverage of 88.3% by the proposed method.",
        "id":860754
      },
      {
        "title":"Joint Prediction of Morphosyntactic Categories for Fine-Grained Arabic Part-of-Speech Tagging Exploiting Tag Dictionary Information",
        "text":"Part-of-speech (POS) tagging for morphologically rich languages such as Arabic is a challenging problem because of their enormous tag sets. One reason for this is that in the tagging scheme for such languages, a complete POS tag is formed by combining tags from multiple tag sets defined for each morphosyntactic category. Previous approaches in Arabic POS tagging applied one model for each morphosyntactic tagging task, without utilizing shared information between the tasks. In this paper, we propose an approach that utilizes this information by jointly modeling multiple morphosyntactic tagging tasks with a multi-task learning framework. We also propose a method of incorporating tag dictionary information into our neural models by combining word representations with representations of the sets of possible tags. Our experiments showed that the joint model with tag dictionary information results in an accuracy of 91.38% on the Penn Arabic Treebank data set, with an absolute improvement of 2.11% over the current state-of-the-art tagger. 1",
        "id":20260724
      },
      {
        "title":"Speaker Naming in Movies",
        "text":"We propose a new model for speaker naming in movies that leverages visual, textual, and acoustic modalities in an unified optimization framework. To evaluate the performance of our model, we introduce a new dataset consisting of six episodes of the Big Bang Theory TV show and eighteen full movies covering different genres. Our experiments show that our multimodal model significantly outperforms several competitive baselines on the average weighted F-score metric. To demonstrate the effectiveness of our framework, we design an end-to-end memory network model that leverages our speaker naming model and achieves state-of-the-art results on the subtitles task of the MovieQA 2017 Challenge.",
        "id":44084674
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that does data-augmentation for biomedical named-entity recognition by replacing entities in the original sentence with entities of the same type from semantically similar sentences?",
    "positive_ctxs":[
      {
        "title":"Simple Semantic-based Data Augmentation for Named Entity Recognition in Biomedical Texts",
        "text":"Data augmentation is important in addressing data sparsity and low resources in NLP. Unlike data augmentation for other tasks such as sentence-level and sentence-pair ones, data augmentation for named entity recognition (NER) requires preserving the semantic of entities. To that end, in this paper we propose a simple semantic-based data augmentation method for biomedical NER. Our method leverages semantic information from pre-trained language models for both entity-level and sentence-level. Experimental results on two datasets: i2b2-2010 (English) and VietBioNER (Vietnamese) showed that the proposed method could improve NER performance.",
        "id":248780437
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":237099287
      },
      {
        "title":"Constructing a Culinary Interview Dialogue Corpus with Video Conferencing Tool",
        "text":"Interview is an efficient way to elicit knowledge from experts of different domains. In this paper, we introduce CIDC, an interview dialogue corpus in the culinary domain in which interviewers play an active role to elicit culinary knowledge from the cooking expert. The corpus consists of 308 interview dialogues (each about 13 minutes in length), which add up to a total of 64,000 utterances. We use a video conferencing tool for data collection, which allows us to obtain the facial expressions of the interlocutors as well as the screen-sharing contents. To understand the impact of the interlocutors' skill level, we divide the experts into \"professionals\" and \"enthusiasts\" and the interviewers into \"skilled interviewers\" and \"unskilled interviewers.\" For quantitative analysis, we report the statistics and the results of the post-interview questionnaire. We also conduct qualitative analysis on the collected interview dialogues and summarize the salient patterns of how interviewers elicit knowledge from the experts. The corpus serves the purpose to facilitate future research on the knowledge elicitation mechanism in interview dialogues.",
        "id":251436397
      },
      {
        "title":"Too Many Questions? What Can We Do? : Multiple Question Span Detection",
        "text":"When a human interacts with an information retrieval chat bot, he\/she can ask multiple questions at the same time. Current question answering systems can't handle this scenario effectively. In this paper we propose an approach to identify question spans in a given utterance, by posing this as a sequence labeling problem. The model is trained and evaluated over 4 different freely available datasets. To get a comprehensive coverage of the compound question scenarios, we also synthesize a dataset based on the natural question combination patterns. We exhibit improvement in the performance of the DrQA system when it encounters compound questions which suggests that this approach is vital for real-time human-chatbot interaction.",
        "id":202594324
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that examines how cross project code summarization evaluation methodologies compare to time-segmented eval methodology.",
    "positive_ctxs":[
      {
        "title":"Impact of Evaluation Methodologies on Code Summarization",
        "text":"There has been a growing interest in developing machine learning (ML) models for code summarization tasks, e.g., comment generation and method naming. Despite substantial increase in the effectiveness of ML models, the evaluation methodologies, i.e., the way people split datasets into training, validation, and test sets, were not well studied. Specifically, no prior work on code summarization considered the timestamps of code and comments during evaluation. This may lead to evaluations that are inconsistent with the intended use cases. In this paper, we introduce the time-segmented evaluation methodology, which is novel to the code summarization research community, and compare it with the mixed-project and cross-project methodologies that have been commonly used. Each methodology can be mapped to some use cases, and the time-segmented methodology should be adopted in the evaluation of ML models for code summarization. To assess the impact of methodologies, we collect a dataset of (code, comment) pairs with timestamps to train and evaluate several recent ML models for code summarization. Our experiments show that different methodologies lead to conflicting evaluation results. We invite the community to expand the set of methodologies used in evaluations.",
        "id":247958464
      }
    ],
    "negative_ctxs":[
      {
        "title":"Published as a conference paper at ICLR 2021 UNDERSTANDING OVERPARAMETERIZATION IN GENERATIVE ADVERSARIAL NETWORKS",
        "text":"A broad class of unsupervised deep learning methods such as Generative Adversarial Networks (GANs) involve training of overparameterized models where the number of parameters of the model exceeds a certain threshold. Indeed, most successful GANs used in practice are trained using overparameterized generator and discriminator networks, both in terms of depth and width. A large body of work in supervised learning have shown the importance of model overparameterization in the convergence of the gradient descent (GD) to globally optimal solutions. In contrast, the unsupervised setting and GANs in particular involve non-convex concave mini-max optimization problems that are often trained using Gradient Descent\/Ascent (GDA). The role and benefits of model overparameterization in the convergence of GDA to a global saddle point in non-convex concave problems is far less understood. In this work, we present a comprehensive analysis of the importance of model overparameterization in GANs both theoretically and empirically. We theoretically show that in an overparameterized GAN model with a 1-layer neural network generator and a linear discriminator, GDA converges to a global saddle point of the underlying non-convex concave min-max problem. To the best of our knowledge, this is the first result for global convergence of GDA in such settings. Our theory is based on a more general result that holds for a broader class of nonlinear generators and discriminators that obey certain assumptions (including deeper generators and random feature discriminators). Our theory utilizes and builds upon a novel connection with the convergence analysis of linear timevarying dynamical systems which may have broader implications for understanding the convergence behavior of GDA for non-convex concave problems involving overparameterized models. We also empirically study the role of model overparameterization in GANs using several large-scale experiments on CIFAR-10 and Celeb-A datasets. Our experiments show that overparameterization improves the quality of generated samples across various model architectures and datasets. Remarkably, we observe that overparameterization leads to faster and more stable convergence behavior of GDA across the board.",
        "id":232360541
      },
      {
        "title":"Annotating for Hate Speech: The MaNeCo Corpus and Some Input from Critical Discourse Analysis",
        "text":"This paper presents a novel scheme for the annotation of hate speech in corpora of Web 2.0 commentary. The proposed scheme is motivated by the critical analysis of posts made in reaction to news reports on the Mediterranean migration crisis and LGBTIQ+ matters in Malta, which was conducted under the auspices of the EU-funded C.O.N.T.A.C.T. project. Based on the realization that hate speech is not a clear-cut category to begin with, appears to belong to a continuum of discriminatory discourse and is often realized through the use of indirect linguistic means, it is argued that annotation schemes for its detection should refrain from directly including the label 'hate speech,' as different annotators might have different thresholds as to what constitutes hate speech and what not. In view of this, we suggest a multi-layer annotation scheme, which is pilot-tested against a binary ±hate speech classification and appears to yield higher inter-annotator agreement. Motivating the postulation of our scheme, we then present the MaNeCo corpus on which it will eventually be used; a substantial corpus of on-line newspaper comments spanning 10 years.",
        "id":218973764
      },
      {
        "title":"Trust Evaluation Mechanisms for Wikipedia",
        "text":"Wikipedia is the well-nigh successful and most popular free encyclopedia developed by many editors in collaborative manner. It provides multitude of opportunities for online large scale knowledge sharing between virtual communities by letting the viewer to create and edit articles directly in the web browser. Information on Wikipedia is expanding largely, but the increase in quantity is not proportional to quality of the content. The cursory observer of Wikipedia may not be able to differentiate between the good and the bad quality of the content. Despite the success of Wikipedia, trust on Wikipedia content is still questioned because of its open editing model. In this paper primarily the challenges for trust evaluation mechanisms, caused by the significant characteristics of Wikipedia's knowledge base are discussed. Existing Wikipedia trust evaluation models are comprehensively surveyed and key issues related to these are highlighted. Finally based on this study new dimensions for effective trust evaluation mechanisms are proposed, which are aimed to setup clear goals for future research in this area.36",
        "id":1136774
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that examines how incorporating external commonsense knowledge into conversational agents can better interpret user emotions and refine their response formulation techniques?",
    "positive_ctxs":[
      {
        "title":"MISC: A MIxed Strategy-Aware Model Integrating COMET for Emotional Support Conversation",
        "text":"Applying existing methods to emotional support conversation-which provides valuable assistance to people who are in need-has two major limitations: (a) they generally employ a conversation-level emotion label, which is too coarse-grained to capture user's instant mental state; (b) most of them focus on expressing empathy in the response(s) rather than gradually reducing user's distress. To address the problems, we propose a novel model MISC, which firstly infers the user's fine-grained emotional status, and then responds skillfully using a mixture of strategy. Experimental results on the benchmark dataset demonstrate the effectiveness of our method and reveal the benefits of fine-grained emotion understanding as well as mixed-up strategy modeling. Our code and data could be found in https: \/\/github.com\/morecry\/MISC.",
        "id":247748640
      }
    ],
    "negative_ctxs":[
      {
        "title":"Making Computers Laugh: Investigations in Automatic Humor Recognition",
        "text":"Humor is one of the most interesting and puzzling aspects of human behavior. Despite the attention it has received in fields such as philosophy, linguistics, and psychology, there have been only few attempts to create computational models for humor recognition or generation. In this paper, we bring empirical evidence that computational approaches can be successfully applied to the task of humor recognition. Through experiments performed on very large data sets, we show that automatic classification techniques can be effectively used to distinguish between humorous and non-humorous texts, with significant improvements observed over apriori known baselines.",
        "id":5766640
      },
      {
        "title":"Extended HMM and Ranking models for Chinese Spelling Correction",
        "text":"Spelling correction has been studied for many decades, which can be classified into two categories: (1) regular text spelling correction, (2) query spelling correction. Although the two tasks share many common techniques, they have different concerns. This paper presents our work on the CLP-2014 bake-off. The task focuses on spelling checking on foreigner Chinese essays. Compared to online search query spelling checking task, more complicated techniques can be applied for better performance. Therefore, we proposed a unified framework for Chinese essays spelling correction based on extended HMM and ranker-based models, together with a rule-based model for further polishing. Our system showed better performance on the test dataset.",
        "id":13253132
      },
      {
        "title":"Published as a conference paper at ICLR 2023 CERTIFIED TRAINING: SMALL BOXES ARE ALL YOU NEED",
        "text":"To obtain, deterministic guarantees of adversarial robustness, specialized training methods are used. We propose, SABR, a novel such certified training method, based on the key insight that propagating interval bounds for a small but carefully selected subset of the adversarial input region is sufficient to approximate the worst-case loss over the whole region while significantly reducing approximation errors. We show in an extensive empirical evaluation that SABR outperforms existing certified defenses in terms of both standard and certifiable accuracies across perturbation magnitudes and datasets, pointing to a new class of certified training methods promising to alleviate the robustness-accuracy trade-off.",
        "id":252780995
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that examines the intricacies of few-shot relation extraction challenges and introduces an approach integrating both global and local attributes alongside external descriptions of relations?",
    "positive_ctxs":[
      {
        "title":"Exploring Task Difficulty for Few-Shot Relation Extraction",
        "text":"Few-shot relation extraction (FSRE) focuses on recognizing novel relations by learning with merely a handful of annotated instances. Meta-learning has been widely adopted for such a task, which trains on randomly generated few-shot tasks to learn generic data representations. Despite impressive results achieved, existing models still perform suboptimally when handling hard FSRE tasks, where the relations are fine-grained and similar to each other. We argue this is largely because existing models do not distinguish hard tasks from easy ones in the learning process. In this paper, we introduce a novel approach based on contrastive learning that learns better representations by exploiting relation label information. We further design a method that allows the model to adaptively learn how to focus on hard tasks. Experiments on two standard datasets demonstrate the effectiveness of our method.",
        "id":237492043
      }
    ],
    "negative_ctxs":[
      {
        "title":"Using semantic roles to improve summaries",
        "text":"This paper describes preliminary analysis on the influence of the semantic roles in summary generation. The proposed method involves three steps: first, the named entities in the original text are identified using a named entity recognizer; secondly, the sentences are parsed and semantic roles are extracted; thirdly, selection of the sentences containing specific semantic roles for the most relevant entities in text. Although the method is language independent, in order to check its viability, we tested the proposed approach for Romanian summaries.",
        "id":68799
      },
      {
        "title":"Identifying relevant common sense information in knowledge graphs",
        "text":"Knowledge graphs are often used to store common sense information that is useful for various tasks. However, the extraction of contextuallyrelevant knowledge is an unsolved problem, and current approaches are relatively simple.Here we introduce a triple selection method based on a ranking model and find that it improves question answering accuracy over existing methods. We additionally investigate methods to ensure that extracted triples form a connected graph. Graph connectivity is important for model interpretability, as paths are frequently used as explanations for the reasoning that connects question and answer.",
        "id":248780238
      },
      {
        "title":"",
        "text":"",
        "id":227230866
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that explores a pre-trained multilingual text-to-text transformer applicable to text summarization tasks?",
    "positive_ctxs":[
      {
        "title":"mT5: A massively multilingual pre-trained text-to-text transformer",
        "text":"The recent \"Text-to-Text Transfer Transformer\" (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We describe the design and modified training of mT5 and demonstrate its stateof-the-art performance on many multilingual benchmarks. All of the code and model checkpoints used in this work are publicly available. 1 * Equal Contribution.Please direct correspondence to",
        "id":225040574
      }
    ],
    "negative_ctxs":[
      {
        "title":"MultiSubs: A Large-scale Multimodal and Multilingual Dataset",
        "text":"This paper introduces a large-scale multimodal and multilingual dataset that aims to facilitate research on grounding words to images in their contextual usage in language. The dataset consists of images selected to unambiguously illustrate concepts expressed in sentences from movie subtitles. The dataset is a valuable resource as (i) the images are aligned to text fragments rather than whole sentences; (ii) multiple images are possible for a text fragment and a sentence; (iii) the sentences are free-form and real-world like; (iv) the parallel texts are multilingual. We set up a fill-in-the-blank game for humans to evaluate the quality of the automatic image selection process of our dataset. We show the utility of the dataset on two automatic tasks: (i) fill-in-the blank; (ii) lexical translation. Results of the human evaluation and automatic models demonstrate that images can be a useful complement to the textual context. The dataset will benefit research on visual grounding of words especially in the context of free-form sentences, and can be obtained from https:\/\/doi.org\/10.5281\/zenodo.5034604 under a Creative Commons licence.",
        "id":232092534
      },
      {
        "title":"A Unified Approach in Speech-to-Speech Translation: Integrating Features of Speech Recognition and Machine Translation",
        "text":"Based upon a statistically trained speech translation system, in this study, we try to combine distinctive features derived from the two modules: speech recognition and statistical machine translation, in a loglinear model. The translation hypotheses are then rescored and translation performance is improved. The standard translation evaluation metrics, including BLEU, NIST, multiple reference word error rate and its position independent counterpart, were optimized to solve the weights of the features in the log-linear model. The experimental results have shown significant improvement over the baseline IBM model 4 in all automatic translation evaluation metrics. The largest was for BLEU, by 7.9% absolute.",
        "id":7150694
      },
      {
        "title":"The Role of Prosody and Speech Register in Word Segmentation: A Computational Modelling Perspective",
        "text":"This study explores the role of speech register and prosody for the task of word segmentation. Since these two factors are thought to play an important role in early language acquisition, we aim to quantify their contribution for this task. We study a Japanese corpus containing both infant-and adult-directed speech and we apply four different word segmentation models, with and without knowledge of prosodic boundaries. The results showed that the difference between registers is smaller than previously reported and that prosodic boundary information helps more adult-than infant-directed speech.",
        "id":29767064
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that explores employing variational autoencoders to standardize open knowledge graphs?",
    "positive_ctxs":[
      {
        "title":"Open Knowledge Graphs Canonicalization using Variational Autoencoders",
        "text":"Noun phrases and Relation phrases in open knowledge graphs are not canonicalized, leading to an explosion of redundant and ambiguous subject-relation-object triples. Existing approaches to solve this problem take a two-step approach. First, they generate embedding representations for both noun and relation phrases, then a clustering algorithm is used to group them using the embeddings as features. In this work, we propose Canonicalizing Using Variational Autoencoders (CUVA) 1 , a joint model to learn both embeddings and cluster assignments in an end-to-end approach, which leads to a better vector representation for the noun and relation phrases. Our evaluation over multiple benchmarks shows that CUVA outperforms the existing state-of-the-art approaches. Moreover, we introduce CANONICNELL, a novel dataset to evaluate entity canonicalization systems.",
        "id":238198752
      }
    ],
    "negative_ctxs":[
      {
        "title":"Evaluation methodologies in Automatic Question Generation",
        "text":"In the last few years Automatic Question Generation (AQG) has attracted increasing interest. In this paper we survey the evaluation methodologies used in AQG. Based on a sample of 37 papers, our research shows that the systems' development has not been accompanied by similar developments in the methodologies used for the systems' evaluation. Indeed, in the papers we examine here, we find a wide variety of both intrinsic and extrinsic evaluation methodologies. Such diverse evaluation practices make it difficult to reliably compare the quality of different generation systems. Our study suggests that, given the rapidly increasing level of research in the area, a common framework is urgently needed to compare the performance of AQG systems and NLG systems more generally.1 http:\/\/aclweb.org\/anthology\/",
        "id":53235639
      },
      {
        "title":"Constraint Grammar parsing with left and right sequential finite transducers",
        "text":"We propose an approach to parsing Constraint Grammars using finite-state transducers and report on a compiler that converts Constraint Grammar rules into transducer representations. The resulting transducers are further optimized by conversion to left and right sequential transducers. Using the method, we show that we can improve on the worstcase asymptotic bound of Constraint Grammar parsing from cubic to quadratic in the length of input sentences.",
        "id":15217163
      },
      {
        "title":"Learning to Paraphrase for Question Answering",
        "text":"Question answering (QA) systems are sensitive to the many different ways natural language expresses the same information need. In this paper we turn to paraphrases as a means of capturing this knowledge and present a general framework which learns felicitous paraphrases for various QA tasks. Our method is trained end-toend using question-answer pairs as a supervision signal. A question and its paraphrases serve as input to a neural scoring model which assigns higher weights to linguistic expressions most likely to yield correct answers. We evaluate our approach on QA over Freebase and answer sentence selection. Experimental results on three datasets show that our framework consistently improves performance, achieving competitive results despite the use of simple QA models.",
        "id":1282002
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that explores how language models are not robust to the surface form editing when testing commonsense knowledge?",
    "positive_ctxs":[
      {
        "title":"Did the Cat Drink the Coffee? Challenging Transformers with Generalized Event Knowledge",
        "text":"Prior research has explored the ability of computational models to predict a word semantic fit with a given predicate. While much work has been devoted to modeling the typicality relation between verbs and arguments in isolation, in this paper we take a broader perspective by assessing whether and to what extent computational approaches have access to the information about the typicality of entire events and situations described in language (Generalized Event Knowledge).",
        "id":236318140
      }
    ],
    "negative_ctxs":[
      {
        "title":"Learning Logical Structures of Paragraphs in Legal Articles",
        "text":"This paper presents a new task, learning logical structures of paragraphs in legal articles, which is studied in research on Legal Engineering (Katayama, 2007). The goals of this task are recognizing logical parts of law sentences in a paragraph, and then grouping related logical parts into some logical structures of formulas, which describe logical relations between logical parts. We present a two-phase framework to learn logical structures of paragraphs in legal articles. In the first phase, we model the problem of recognizing logical parts in law sentences as a multi-layer sequence learning problem, and present a CRF-based model to recognize them. In the second phase, we propose a graph-based method to group logical parts into logical structures. We consider the problem of finding a subset of complete sub-graphs in a weighted-edge complete graph, where each node corresponds to a logical part, and a complete sub-graph corresponds to a logical structure. We also present an integer linear programming formulation for this optimization problem. Our models achieve 74.37% in recognizing logical parts, 79.59% in recognizing logical structures, and 55.73% in the whole task on the Japanese National Pension Law corpus.",
        "id":17666550
      },
      {
        "title":"OpenNMT System Description for WNMT 2018: 800 words\/sec on a single-core CPU",
        "text":"We present a system description of the OpenNMT Neural Machine Translation entry for the WNMT 2018 evaluation. In this work, we developed a heavily optimized NMT inference model targeting a high-performance CPU system. The final system uses a combination of four techniques, all of them leading to significant speed-ups in combination: (a) sequence distillation, (b) architecture modifications, (c) pre-computation, particularly of vocabulary, and (d) CPU targeted quantization. This work achieves the fastest performance of the shared task, and led to the development of new features that have been integrated to OpenNMT and made available to the community.",
        "id":51793903
      },
      {
        "title":"",
        "text":"",
        "id":241583236
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that explores mitigating bias in natural language understanding via example reweighting?",
    "positive_ctxs":[
      {
        "title":"End-to-End Self-Debiasing Framework for Robust NLU Training",
        "text":"Existing Natural Language Understanding (NLU) models have been shown to incorporate dataset biases leading to strong performance on in-distribution (ID) test sets but poor performance on out-of-distribution (OOD) ones. We introduce a simple yet effective debiasing framework whereby the shallow representations of the main model are used to derive a bias model and both models are trained simultaneously. We demonstrate on three well studied NLU tasks that despite its simplicity, our method leads to competitive OOD results. It significantly outperforms other debiasing approaches on two tasks, while still delivering high in-distribution performance.",
        "id":236477370
      }
    ],
    "negative_ctxs":[
      {
        "title":"CUE Vectors: Modular Training of Language Models Conditioned on Diverse Contextual Signals",
        "text":"We propose a framework to modularize the training of neural language models that use diverse forms of sentence-external context (including metadata) by eliminating the need to jointly train sentence-external and withinsentence encoders. Our approach, contextual universal embeddings (CUE), trains LMs on one set of context, such as date and author, and adapts to novel metadata types, such as article title, or previous sentence. The model consists of a pretrained neural sentence LM, a BERT-based context encoder, and a masked transformer decoder that estimates LM probabilities using sentence-internal and sentenceexternal information. When context or metadata are unavailable, our model learns to combine contextual and sentence-internal information using noisy oracle unigram embeddings as a proxy. Real contextual information can be introduced later and used to adapt a small number of parameters that map contextual data into the decoder's embedding space. We validate the CUE framework on a NYTimes text corpus with multiple metadata types, for which the LM perplexity can be lowered from 36.6 to 27.4 by conditioning on context. Bootstrapping a contextual LM with only a subset of the context\/metadata during training retains 85% of the achievable gain. Training the model initially with proxy context retains 67% of the perplexity gain after adapting to real context. Furthermore, we can swap one type of pretrained sentence LM for another without retraining the context encoders, by only adapting the decoder model. Overall, we obtain a modular framework that allows incremental, scalable training of context-enhanced LMs.",
        "id":247476288
      },
      {
        "title":"Word Semantic Similarity for Morphologically Rich Languages",
        "text":"In this work, we investigate the role of morphology on the performance of semantic similarity for morphologically rich languages, such as German and Greek. The challenge in processing languages with richer morphology than English, lies in reducing estimation error while addressing the semantic distortion introduced by a stemmer or a lemmatiser. For this purpose, we propose a methodology for selective stemming, based on a semantic distortion metric. The proposed algorithm is tested on the task of similarity estimation between words using two types of corpus-based similarity metrics: co-occurrence-based and context-based. The performance on morphologically rich languages is boosted by stemming with the context-based metric, unlike English, where the best results are obtained by the co-occurrence-based metric. A key finding is that the estimation error reduction is different when a word is used as a feature, rather than when it is used as a target word.",
        "id":10678830
      },
      {
        "title":"Linguistically Light Lexical Extensions for Ontologies",
        "text":"An increasing number of enterprises are beginning to include semantic web ontologies into their Information Extraction (IE) and Text Analytics (TA) applications. This can be challenging for a TA group wishing to avail of semantic web ontologies due to the manual effort of retargeting and tailoring language resources within the TA system to a new domain to meet customer needs. A lightweight lexical layer within an ontology offers a solution to this problem. Furthermore, the identification of class instances within unstructured text for either the purposes of ontology population or semantic annotation are usually limited to term mentions of proper noun, personal noun or fixed key phrases within Text Analytics or Ontology Based Information Extraction (OBIE) applications. These systems do not generalise to cope with compound nominal classes of multi word expressions. LEON, a set of Lexical Extensions for Ontologies offers a solution to this problem. We describe LEON, which encodes light linguistic features of lexical entries for concepts within an ontology, as well as a lightweight lexical analyzer which complies the LEON metadata into efficient an dictionary format to drive large scale identification and semantic annotation of concepts mentioned in text.",
        "id":11003403
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that explores strategies for improving multi-label text classification by incorporating information about label distribution directly into the loss function?",
    "positive_ctxs":[
      {
        "title":"Balancing Methods for Multi-label Text Classification with Long-Tailed Class Distribution",
        "text":"Multi-label text classification is a challenging task because it requires capturing label dependencies. It becomes even more challenging when class distribution is long-tailed. Resampling and re-weighting are common approaches used for addressing the class imbalance problem, however, they are not effective when there is label dependency besides class imbalance because they result in oversampling of common labels. Here, we introduce the application of balancing loss functions for multilabel text classification. We perform experiments on a general domain dataset with 90 labels (Reuters-21578) and a domain-specific dataset from PubMed with 18211 labels. We find that a distribution-balanced loss function, which inherently addresses both the class imbalance and label linkage problems, outperforms commonly used loss functions. Distribution balancing methods have been successfully used in the image recognition field. Here, we show their effectiveness in natural language processing. Source code is available at https:\/\/github.com\/blessu\/ BalancedLossNLP.",
        "id":237485334
      }
    ],
    "negative_ctxs":[
      {
        "title":"An Entity-Mention Model for Coreference Resolution with Inductive Logic Programming",
        "text":"The traditional mention-pair model for coreference resolution cannot capture information beyond mention pairs for both learning and testing. To deal with this problem, we present an expressive entity-mention model that performs coreference resolution at an entity level. The model adopts the Inductive Logic Programming (ILP) algorithm, which provides a relational way to organize different knowledge of entities and mentions. The solution can explicitly express relations between an entity and the contained mentions, and automatically learn first-order rules important for coreference decision. The evaluation on the ACE data set shows that the ILP based entity-mention model is effective for the coreference resolution task.",
        "id":126675
      },
      {
        "title":"RedwoodNLP at SemEval-2021 Task 7: Ensembled Pretrained and Lightweight Models for Humor Detection",
        "text":"An understanding of humor is an essential component of human-facing NLP systems. In this paper, we investigate several methods for detecting humor in short statements as part of Semeval-2021 Shared Task 7. For Task 1a, we apply an ensemble of fine-tuned pre-trained language models; for Tasks 1b, 1c, and 2a, we investigate various tree-based and linear machine learning models. Our final system achieves an F1-score of 0.9571 (ranked 24 \/ 58) on Task 1a, an RMSE of 0.5580 (ranked 18 \/ 50) on Task 1b, an F1-score of 0.5024 (ranked 26 \/ 36) on Task 1c, and an RMSE of 0.7229 (ranked 45 \/ 48) on Task 2a.",
        "id":236459807
      },
      {
        "title":"Determining the Syntactic Structure of Medical Terms in Clinical Notes",
        "text":"This paper demonstrates a method for determining the syntactic structure of medical terms. We use a model-fitting method based on the Log Likelihood Ratio to classify three-word medical terms as right or left-branching. We validate this method by computing the agreement between the classification produced by the method and manually annotated classifications. The results show an agreement of 75% -83%. This method may be used effectively to enable a wide range of applications that depend on the semantic interpretation of medical terms including automatic mapping of terms to standardized vocabularies and induction of terminologies from unstructured medical text.",
        "id":6543751
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that explores the difficulties in creating shared multilingual vocabularies, especially focusing on the problem of over segmentation in low-resource languages?",
    "positive_ctxs":[
      {
        "title":"Multi-view Subword Regularization",
        "text":"Multilingual pretrained representations generally rely on subword segmentation algorithms to create a shared multilingual vocabulary. However, standard heuristic algorithms often lead to sub-optimal segmentation, especially for languages with limited amounts of data. In this paper, we take two major steps towards alleviating this problem. First, we demonstrate empirically that applying existing subword regularization methods(Kudo, 2018;Provilkov et al., 2020)during fine-tuning of pre-trained multilingual representations improves the effectiveness of cross-lingual transfer. Second, to take full advantage of different possible input segmentations, we propose Multi-view Subword Regularization (MVR), a method that enforces the consistency between predictions of using inputs tokenized by the standard and probabilistic segmentations. Results on the XTREME multilingual benchmark  show that MVR brings consistent improvements of up to 2.5 points over using standard segmentation algorithms. 1",
        "id":232233194
      }
    ],
    "negative_ctxs":[
      {
        "title":"Constraints and Type Hierarchies for Korean Serial Verb Constructions -An Analytic Study within the HPSG Framework -*",
        "text":"This paper provides a fine-grained analysis of Korean serial verb constructions within the HPSG framework, and covers major descriptive characteristics of the phenomena. This paper discusses constraints on serial verb constructions in terms of four aspects; transitivity, argument structure, semantic properties, and complementizers. As a result, 17 constraints have been built, which support the type hierarchies for Korean serial verb constructions. This paper also presents a sample derivation on the basis of on the constraints and the type hierarchies.",
        "id":7578809
      },
      {
        "title":"The Effect of Pitch Accenting on Pronoun Referent Resolution",
        "text":"By strictest interpretation, theories of both centering and intonational meaning fail to predict the existence of pitch accented pronominals. Yet they occur felicitously in spoken discourse. To explain this, I emphasize the dual functions served by pitch accents, as markers of both propositional (semantic\/pragmatic) and attentional salience. This distinction underlies my proposals about the attentional consequences of pitch accents when applied to pronominals, in particular, that while most pitch accents may weaken or reinforce a cospecifier's status as the center of attention, a contrastively stressed pronominal may force a shift, even when contraindicated by textual features.",
        "id":177600
      },
      {
        "title":"Figure Eight at SemEval-2019 Task 3: Ensemble of Transfer Learning Methods for Contextual Emotion Detection",
        "text":"This paper describes our transfer learningbased approach to contextual emotion detection as part of SemEval-2019 Task 3. We experiment with transfer learning using pretrained language models (ULMFiT, OpenAI GPT, and BERT) and fine-tune them on this task. We also train a deep learning model from scratch using pre-trained word embeddings and BiLSTM architecture with attention mechanism. The ensembled model achieves competitive result, ranking ninth out of 165 teams. The result reveals that ULMFiT performs best due to its superior fine-tuning techniques. We propose improvements for future work.",
        "id":184483109
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that explores the improvement of Chinese sequence labeling with BERT through the incorporation of lexical data via a character-to-word bilinear attention approach?",
    "positive_ctxs":[
      {
        "title":"Lexicon Enhanced Chinese Sequence Labelling Using BERT Adapter",
        "text":"Lexicon information and pre-trained models, such as BERT, have been combined to explore Chinese sequence labelling tasks due to their respective strength. However, existing methods solely fuse lexicon features via a shallow and random initialized sequence layer and do not integrate them into the bottom layers of BERT. In this paper, we propose Lexicon Enhanced BERT (LEBERT) for Chinese sequence labelling, which integrates external lexicon knowledge into BERT layers directly by a Lexicon Adapter layer. Compared with the existing methods, our model facilitates deep lexicon knowledge fusion at the lower layers of BERT. Experiments on ten Chinese datasets of three tasks including Named Entity Recognition, Word Segmentation, and Part-of-Speech Tagging, show that LEBERT achieves the stateof-the-art results.",
        "id":234741719
      }
    ],
    "negative_ctxs":[
      {
        "title":"Maximum Entropy Based Lexical Reordering Model for Hierarchical Phrase-based Machine Translation",
        "text":"The hierarchical phrase-based (HPB) model on the basis of a synchronous context-free grammar (SCFG) is prominent in solving global reorderings. However, the HPB model is inadequate to supervise the reordering process so that sometimes positions of different lexicons are switched due to the incorrect SCFG rules. In this paper, we consider the order of two lexicons as a classification problem and propose a novel lexical reordering model based on a maximum entropy classifier. Our model employs the word alignment and translation during the decoding process. Experimental results on the Chinese-to-English task showed that our method outperformed the baseline system in BLEU score significantly. Moreover, the translation results further proved the effectiveness of our approach. Keywords: hierarchical phrase-based model, reordering, maximum entropy model weishengyongpin[hygiene article] , tebieshi[particular] niaobu[diaper] ( yinger[infant] 、 shijin[incontinent] chengren[adult] ) 、 nvxing[feminine] weishegnyongpin [hygiene article] , BL Hygiene products, in particular (baby diapers, incontinent adult), feminine hygiene products, MEL Hygiene products, particular diapers (baby, incontinent adult), feminine hygiene products, RF Sanitary articles, in particular diapers (infant, incontinent adult), feminine hygiene products, Src2 kepeizhi[configurable] canshu[parameter] 375 ( liru[such as] shujv[data] leixing[category] , shujv[data] dingxiang[orientation], he[and] shujv[data] texing[characteristic] ) , BL configurable parameters 375 characteristics (e. g., data type, orientation and data), MEL configurable parameters 375 (e. g., data type, data orientation and data characteristics), RF configurable parameters 375 (such as data categories, data orientations and data characteristics), Src3 canzhao[refer] tu[figure] 7 , yonghu[user] dui[to] caidan[menu] xuanxiang[option] 607 de['s] xuanze[selection] ( tu[figure] 6 )BL Referring now to Figure 7, the user selects to menu options 607 (Figure 6) MEL Referring to FIG.7, user selection of the menu options 607 (Figure 6) RF Referring to FIG. 7, the user's selection of menu option 607 (FIG.6)",
        "id":6770074
      },
      {
        "title":"",
        "text":"",
        "id":196002081
      },
      {
        "title":"GCDT: A Chinese RST Treebank for Multigenre and Multilingual Discourse Parsing",
        "text":"A lack of large-scale human-annotated data has hampered the hierarchical discourse parsing of Chinese. In this paper, we present GCDT, the largest hierarchical discourse treebank for Mandarin Chinese in the framework of Rhetorical Structure Theory (RST). GCDT covers over 60K tokens across five genres of freely available text, using the same relation inventory as contemporary RST treebanks for English. We also report on this dataset's parsing experiments, including state-of-the-art (SOTA) scores for Chinese RST parsing and RST parsing on the English GUM dataset, using cross-lingual training in Chinese and English with multilingual embeddings.",
        "id":252992779
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that initializes embeddings in multilingual transformer for subwords common with original vocabulary with original embeddings?",
    "positive_ctxs":[
      {
        "title":"Lifting the Curse of Multilinguality by Pre-training Modular Transformers",
        "text":"Multilingual pre-trained models are known to suffer from the curse of multilinguality, which causes per-language performance to drop as they cover more languages. We address this issue by introducing language-specific modules, which allows us to grow the total capacity of the model, while keeping the total number of trainable parameters per language constant. In contrast with prior work that learns languagespecific components post-hoc, we pre-train the modules of our Cross-lingual Modular (X-MOD) models from the start. Our experiments on natural language inference, named entity recognition and question answering show that our approach not only mitigates the negative interference between languages, but also enables positive transfer, resulting in improved monolingual and cross-lingual performance. Furthermore, our approach enables adding languages post-hoc with no measurable drop in performance, no longer limiting the model usage to the set of pre-trained languages.",
        "id":248721770
      }
    ],
    "negative_ctxs":[
      {
        "title":"A Gold Standard Corpus of Early Modern German",
        "text":"This paper describes an annotated gold standard sample corpus of Early Modern German containing over 50,000 tokens of text manually annotated with POS tags, lemmas, and normalised spelling variants. The corpus is the first resource of its kind for this variant of German, and represents an ideal test bed for evaluating and adapting existing NLP tools on historical data. We describe the corpus format, annotation levels, and challenges, providing an example of the requirements and needs of smaller humanities-based corpus projects.",
        "id":12087109
      },
      {
        "title":"An Ensemble of Grapheme and Phoneme for Machine Transliteration",
        "text":"Machine transliteration is an automatic method to generate characters or words in one alphabetical system for the corresponding characters in another alphabetical system. There has been increasing concern on machine transliteration as an assistant of machine translation and information retrieval. Three machine transliteration models, including \"grapheme-based model\", \"phonemebased model\", and \"hybrid model\", have been proposed. However, there are few works trying to make use of correspondence between source grapheme and phoneme, although the correspondence plays an important role in machine transliteration. Furthermore there are few works, which dynamically handle source grapheme and phoneme. In this paper, we propose a new transliteration model based on an ensemble of grapheme and phoneme. Our model makes use of the correspondence and dynamically uses source grapheme and phoneme. Our method shows better performance than the previous works about 15~23% in English-to-Korean transliteration and about 15~43% in English-to-Japanese transliteration.",
        "id":17161141
      },
      {
        "title":"UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS",
        "text":"In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks -demonstrating their applicability as general image representations.arXiv:1511.06434v2 [cs.LG] 7 Jan 2016Under review as a conference paper at ICLR 2016• We show that the generators have interesting vector arithmetic properties allowing for easy manipulation of many semantic qualities of generated samples.",
        "id":11758569
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that investigates employing graph neural networks to produce replies within multi-party conversational contexts?",
    "positive_ctxs":[
      {
        "title":"HETERMPC: A Heterogeneous Graph Neural Network for Response Generation in Multi-Party Conversations",
        "text":"Recently, various response generation models for two-party conversations have achieved impressive improvements, but less effort has been paid to multi-party conversations (MPCs) which are more practical and complicated. Compared with a two-party conversation where a dialogue context is a sequence of utterances, building a response generation model for MPCs is more challenging, since there exist complicated context structures and the generated responses heavily rely on both interlocutors (i.e., speaker and addressee) and history utterances. To address these challenges, we present HeterMPC, a heterogeneous graph-based neural network for response generation in MPCs which models the semantics of utterances and interlocutors simultaneously with two types of nodes in a graph. Besides, we also design six types of meta relations with node-edge-typedependent parameters to characterize the heterogeneous interactions within the graph. Through multi-hop updating, HeterMPC can adequately utilize the structural knowledge of conversations for response generation. Experimental results on the Ubuntu Internet Relay Chat (IRC) channel benchmark show that HeterMPC outperforms various baseline models for response generation in MPCs.",
        "id":247476252
      }
    ],
    "negative_ctxs":[
      {
        "title":"4See (Weinreich, 1964), (Fillmore, 1971), (Cruse, 1993), (Pustejovsky, 1995) for interesting accounts on homography\/polysemy. See (Bouillon et al., 1992), (Busa, 1996) for accounts on nominals",
        "text":"1In this paper, we address representation issues of Chinese nominals. In particular, we look at lexical rules as a conceptual tool to link forms with the same semantics as is the case between nominalisations and the forms they are derived from. We also address Chinese compounds, illustrating how to recover implicit semantic relations in nominal compounds. Finally, we show how to translate Chinese nontinals within a knowledge-based framework.",
        "id":17290927
      },
      {
        "title":"Vision and Language Integration: Moving beyond Objects",
        "text":"The last years have seen an explosion of work on the integration of vision and language data. New tasks like Image Captioning and Visual Questions Answering have been proposed and impressive results have been achieved. There is now a shared desire to gain an in-depth understanding of the strengths and weaknesses of those models. To this end, several datasets have been proposed to try and challenge the state-of-the-art. Those datasets, however, mostly focus on the interpretation of objects (as denoted by nouns in the corresponding captions). In this paper, we reuse a previously proposed methodology to evaluate the ability of current systems to move beyond objects and deal with attributes (as denoted by adjectives), actions (verbs), manner (adverbs) and spatial relations (prepositions). We show that the coarse representations given by current approaches are not informative enough to interpret attributes or actions, whilst spatial relations somewhat fare better, but only in attention models.1 The data will be made available at: https:\/\/foilunitn.github.io\/.",
        "id":1970133
      },
      {
        "title":"The TechQA Dataset",
        "text":"We introduce TECHQA, a domain-adaptation question answering dataset for the technical support domain. The TECHQA corpus highlights two real-world issues from the automated customer support domain. First, it contains actual questions posed by users on a technical forum, rather than questions generated specifically for a competition or a task. Second, it has a real-world size -600 training, 310 dev, and 490 evaluation question\/answer pairs -thus reflecting the cost of creating large labeled datasets with actual data. Hence, TECHQA is meant to stimulate research in domain adaptation rather than as a resource to build QA systems from scratch. TECHQA was obtained by crawling the IB-MDeveloper and DeveloperWorks forums for questions with accepted answers provided in an IBM Technote-a technical document that addresses a specific technical issue. We also release a collection of the 801,998 Technotes available on the web as of April 4, 2019 as a companion resource that can be used to learn representations of the IT domain language.",
        "id":207847581
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that investigates employing prefix vectors for conditional natural language generation?",
    "positive_ctxs":[
      {
        "title":"Controllable Natural Language Generation with Contrastive Prefixes",
        "text":"To guide the generation of large pretrained language models (LM), previous work has focused on directly fine-tuning the language model or utilizing an attribute discriminator. In this work, we propose a novel lightweight framework for controllable GPT2 (Radford et al., 2019) generation, which utilizes a set of small attribute-specific vectors, called prefixes (Li and Liang, 2021), to steer natural language generation. Different from Li and Liang (2021), where each prefix is trained independently, we take the relationship among prefixes into consideration and train multiple prefixes simultaneously, as illustrated inFigure 1. We propose a novel supervised method and also an unsupervised method to train the prefixes for single-aspect control while the combination of these two methods can achieve multi-aspect control. Experimental results on both singleaspect and multi-aspect control show that our methods can guide generation towards the desired attributes while keeping high linguistic quality.",
        "id":247158838
      }
    ],
    "negative_ctxs":[
      {
        "title":"An Investigation of Representation and Allocation Harms in Contrastive Learning",
        "text":"The effect of underrepresentation on the performance of minority groups is known to be a serious problem in supervised learning settings; however, it has been underexplored so far in the context of self-supervised learning (SSL).In this paper, we demonstrate that contrastive learning (CL), a popular variant of SSL, tends to collapse representations of minority groups with certain majority groups.We refer to this phenomenon as representation harm and demonstrate it on image and text datasets using the corresponding popular CL methods.Furthermore, our causal mediation analysis of allocation harm on a downstream classification task reveals that representation harm is partly responsible for it, thus emphasizing the importance of studying and mitigating representation harm.Finally, we provide a theoretical explanation for representation harm using a stochastic block model that leads to a representational neural collapse in a contrastive learning setting.",
        "id":263608898
      },
      {
        "title":"What's in a gene name? Automated refinement of gene name dictionaries",
        "text":"Many approaches for named entity recognition rely on dictionaries gathered from curated databases (such as Entrez Gene for gene names.) Strategies for matching entries in a dictionary against arbitrary text use either inexact string matching that allows for known deviations, dictionaries enriched according to some observed rules, or a combination of both. Such refined dictionaries cover potential structural, lexical, orthographical, or morphological variations. In this paper, we present an approach to automatically analyze dictionaries to discover how names are composed and which variations typically occur. This knowledge can be constructed by looking at single entries (names and synonyms for one gene), and then be transferred to entries that show similar patterns in one or more synonyms. For instance, knowledge about words that are frequently missing in (or added to) a name (\"antigen\", \"protein\", \"human\") could automatically be extracted from dictionaries. This paper should be seen as a vision paper, though we implemented most of the ideas presented and show results for the task of gene name recognition. The automatically extracted name composition rules can easily be included in existing approaches, and provide valuable insights into the biomedical sub-language.",
        "id":11328858
      },
      {
        "title":"The NiuTrans Machine Translation Systems for WMT20",
        "text":"This paper describes NiuTrans neural machine translation systems of the WMT20 news translation tasks.We participated in Japanese↔English, English→Chinese, Inuktitut→English and Tamil→English total five tasks and rank first in Japanese↔English both sides. We mainly utilized iterative backtranslation, different depth and widen model architectures, iterative knowledge distillation and iterative fine-tuning. And we find that adequately widened and deepened the model simultaneously, the performance will significantly improve. Also, iterative fine-tuning strategy we implemented is effective during adapting domain. For Inuktitut→English and Tamil→English tasks, we built multilingual models separately and employed pretraining word embedding to obtain better performance.",
        "id":261826438
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that investigates enhancing prompt engineering techniques for generative models using meta-learning strategies?",
    "positive_ctxs":[
      {
        "title":"MetaPrompting: Learning to Learn Better Prompts",
        "text":"Prompting method is regarded as one of the crucial progress for few-shot nature language processing. Recent research on prompting moves from discrete tokens based \"hard prompts\" to continuous \"soft prompts\", which employ learnable vectors as pseudo prompt tokens and achieve better performance. Though showing promising prospects, these softprompting methods are observed to rely heavily on good initialization to take effect. Unfortunately, obtaining a perfect initialization for soft prompts requires understanding of inner language models working and elaborate design, which is no easy task and has to restart from scratch for each new task. To remedy this, we propose a generalized soft prompting method called MetaPrompting, which adopts the well-recognized modelagnostic meta-learning algorithm to automatically find better prompt initialization that facilitates fast adaptation to new prompting tasks. Extensive experiments show MetaPrompting tackles soft prompt initialization problem and brings significant improvement on four different datasets (over 7 points improvement in accuracy for 1-shot setting), achieving new stateof-the-art performance.",
        "id":252519660
      }
    ],
    "negative_ctxs":[
      {
        "title":"An Ontology-Based Approach for Key Phrase Extraction",
        "text":"Automatic key phrase extraction is fundamental to the success of many recent digital library applications and semantic information retrieval techniques and a difficult and essential problem in Vietnamese natural language processing (NLP). In this work, we propose a novel method for key phrase extracting of Vietnamese text that exploits the Vietnamese Wikipedia as an ontology and exploits specific characteristics of the Vietnamese language for the key phrase selection stage. We also explore NLP techniques that we propose for the analysis of Vietnamese texts, focusing on the advanced candidate phrases recognition phase as well as part-of-speech (POS) tagging. Finally, we review the results of several experiments that have examined the impacts of strategies chosen for Vietnamese key phrase extracting.",
        "id":219306007
      },
      {
        "title":"Detecting Figurative Word Occurrences Using Recurrent Neural Networks",
        "text":"The paper addresses the detection of figurative usage of words in English text. The chosen method was to use neural nets fed by pre-trained word embeddings. The obtained results show that simple solutions, based on word embeddings only, are comparable to complex solutions, using additional information as a result of taggers or a psycholinguistic database. This approach can be easily applied to other languages, even less-studied, for which we only have raw texts available.",
        "id":52001146
      },
      {
        "title":"Book Reviews Genesis: An Authorship Study",
        "text":"",
        "id":18962876
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that investigates enhancing token alignment in speech processing by employing inverse document frequency (idf)?",
    "positive_ctxs":[
      {
        "title":"SPLAT: Speech-Language Joint Pre-Training for Spoken Language Understanding",
        "text":"Spoken language understanding (SLU) requires a model to analyze input acoustic signal to understand its linguistic content and make predictions. To boost the models' performance, various pre-training methods have been proposed to learn rich representations from large-scale unannotated speech and text. However, the inherent disparities between the two modalities necessitate a mutual analysis. In this paper, we propose a novel semisupervised learning framework, SPLAT, to jointly pre-train the speech and language modules. Besides conducting a self-supervised masked language modeling task on the two individual modules using unpaired speech and text, SPLAT aligns representations from the two modules in a shared latent space using a small amount of paired speech and text. Thus, during fine-tuning, the speech module alone can produce representations carrying both acoustic information and contextual semantic knowledge of an input acoustic signal. Experimental results verify the effectiveness of our approach on various SLU tasks. For example, SPLAT improves the previous stateof-the-art performance on the Spoken SQuAD dataset by more than 10%.",
        "id":235097520
      }
    ],
    "negative_ctxs":[
      {
        "title":"Are BLEU and Meaning Representation in Opposition?",
        "text":"One of possible ways of obtaining continuous-space sentence representations is by training neural machine translation (NMT) systems. The recent attention mechanism however removes the single point in the neural network from which the source sentence representation can be extracted. We propose several variations of the attentive NMT architecture bringing this meeting point back. Empirical evaluation suggests that the better the translation quality, the worse the learned sentence representations serve in a wide range of classification and similarity tasks. Rico Sennrich et al. 2017. Nematus: a toolkit for neural machine translation. In EACL. Xing Shi, Inkit Padhi, and Kevin Knight. 2016. Does string-based neural MT learn source syntax? In EMNLP. Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. In NIPS.",
        "id":21717477
      },
      {
        "title":"Syntactic Annotations for the Google Books Ngram Corpus",
        "text":"We present a new edition of the Google Books Ngram Corpus, which describes how often words and phrases were used over a period of five centuries, in eight languages; it reflects 6% of all books ever published. This new edition introduces syntactic annotations: words are tagged with their part-of-speech, and headmodifier relationships are recorded. The annotations are produced automatically with statistical models that are specifically adapted to historical text. The corpus will facilitate the study of linguistic trends, especially those related to the evolution of syntax.",
        "id":17707301
      },
      {
        "title":"Towards a richer wordnet representation of properties",
        "text":"This paper discusses how information on properties in a currently developed Danish thesaurus can be transferred to the Danish wordnet, DanNet, and in this way enrich the wordnet with the highly relevant links between properties and their external arguments (i.e. tasty -food). In spite of the fact that the thesaurus is still under development (two thirds still to be compiled) we perform an automatic transfer of relations from the thesaurus to the wordnet which shows promising results. In all, 2,362 property relations are automatically transferred to DanNet and 2% of the transferred material is manually validated. The pilot validation indicates that approx. 90 % of the transferred relations are correctly assigned whereas around 10% are either erroneous or just not very informative, a fact which, however, can partly be explained by the incompleteness of the material at its current stage. As a further consequence, the experiment has led to a richer specification of the editor guidelines to be used in the last compilation phase of the thesaurus.",
        "id":14693730
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that investigates graph-based modeling of interactivity among various modalities over time within non-aligned multimodal sequential datasets?",
    "positive_ctxs":[
      {
        "title":"MTAG: Modal-Temporal Attention Graph for Unaligned Human Multimodal Language Sequences",
        "text":"Human communication is multimodal in nature; it is through multiple modalities such as language, voice, and facial expressions, that opinions and emotions are expressed. Data in this domain exhibits complex multi-relational and temporal interactions. Learning from this data is a fundamentally challenging research problem. In this paper, we propose Modal-Temporal Attention Graph (MTAG). MTAG is an interpretable graph-based neural model that provides a suitable framework for analyzing multimodal sequential data. We first introduce a procedure to convert unaligned multimodal sequence data into a graph with heterogeneous nodes and edges that captures the rich interactions across modalities and through time. Then, a novel graph fusion operation, called MTAG fusion, along with a dynamic pruning and read-out technique, is designed to efficiently process this modal-temporal graph and capture various interactions. By learning to focus only on the important interactions within the graph, MTAG achieves state-ofthe-art performance on multimodal sentiment analysis and emotion recognition benchmarks, while utilizing significantly fewer model parameters. 1",
        "id":235097361
      }
    ],
    "negative_ctxs":[
      {
        "title":"The European Thesaurus on International Relations and Area Studies - A Multilingual Resource for Indexing, Retrieval, and Translation",
        "text":"The multilingual European Thesaurus on International Relations and Area Studies (European Thesaurus) is a special subject thesaurus for the field of international affairs. It is intended for use in libraries and documentation centres of academic institutions and international organizations. The European Thesaurus was established in a collaborative project involving a number of leading European research institutes on international politics. It integrates the controlled terminologies of several existing thesauri. The European Thesaurus comprises about 8,200 terms and proper names from the 24 subject areas covered by the thesaurus. Because of its multilinguality, the European Thesaurus can not only be used for indexing, retrieval and terminological reference, but serves also as a translation tool for the languages represented. The establishment of cross-concordances to related thesauri extends the range of application of the European Thesaurus even further. They enable the treatment of semantic heterogeneity within subject gateways. The European Thesaurus is available both in a seven-lingual printversion as well as in an eight-lingual online-version. To reflect the changes in terminology the European Thesaurus is regularly being amended and modified. Further languages are going to be included.",
        "id":23412271
      },
      {
        "title":"",
        "text":"",
        "id":232021801
      },
      {
        "title":"Source-side Dependency Tree Reordering Models with Subtree Movements and Constraints",
        "text":"We propose a novel source-side dependency tree reordering model for statistical machine translation, in which subtree movements and constraints are represented as reordering events associated with the widely used lexicalized reordering models. This model allows us to not only efficiently capture the statistical distribution of the subtree-to-subtree transitions in training data, but also utilize it directly at the decoding time to guide the search process. Using subtree movements and constraints as features in a log-linear model, we are able to help the reordering models make better selections. It also allows the subtle importance of monolingual syntactic movements to be learned alongside other reordering features. We show improvements in translation quality in English→Spanish and English→Iraqi translation tasks.",
        "id":13716132
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that investigates guiding abstractive summarization through assessing sentence informativeness?",
    "positive_ctxs":[
      {
        "title":"Salience Allocation as Guidance for Abstractive Summarization",
        "text":"Abstractive summarization models typically learn to capture the salient information from scratch implicitly. Recent literature adds extractive summaries as guidance for abstractive summarization models to provide hints of salient content and achieves better performance. However, extractive summaries as guidance could be over strict, leading to information loss or noisy signals. Furthermore, it cannot easily adapt to documents with various abstractiveness. As the number and allocation of salience content pieces vary, it is hard to find a fixed threshold deciding which content should be included in the guidance. In this paper, we propose a novel summarization approach with a flexible and reliable salience guidance, namely SEASON (SaliencE Allocation as Guidance for Abstractive SummarizatiON). SEASON utilizes the allocation of salience expectation to guide abstractive summarization and adapts well to articles in different abstractiveness. Automatic and human evaluations on two benchmark datasets show that the proposed method is effective and reliable. Empirical results on more than one million news articles demonstrate a natural fifteen-fifty salience split for news article sentences, providing a useful insight for composing news articles. 1 * Work done during Fei Wang's internship at Tencent AI Lab Seattle. The first two authors contributed equally. 1 Code and model weights are available at https:\/\/ github.com\/tencent-ailab\/season.",
        "id":253098395
      }
    ],
    "negative_ctxs":[
      {
        "title":"Structural and Topical Dimensions in Multi-Task Patent Translation",
        "text":"Patent translation is a complex problem due to the highly specialized technical vocabulary and the peculiar textual structure of patent documents. In this paper we analyze patents along the orthogonal dimensions of topic and textual structure. We view different patent classes and different patent text sections such as title, abstract, and claims, as separate translation tasks, and investigate the influence of such tasks on machine translation performance. We study multitask learning techniques that exploit commonalities between tasks by mixtures of translation models or by multi-task metaparameter tuning. We find small but significant gains over task-specific training by techniques that model commonalities through shared parameters. A by-product of our work is a parallel patent corpus of 23 million German-English sentence pairs.",
        "id":785716
      },
      {
        "title":"Human Language Technology Elements in a Knowledge Organisation System - The VID project",
        "text":"This paper describes how Human Language Technologies and linguistic resources are used to support the construction of components of a knowledge organisation system. In particular we focus on methodologies and resources for building a corpus-based domain ontology and extracting relevant metadata information for text chunks from domain-specific corpora.",
        "id":15248731
      },
      {
        "title":"",
        "text":"",
        "id":208246040
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that investigates how a subset with clean, annotated datasets improve denoising methods?",
    "positive_ctxs":[
      {
        "title":"Semi-Supervised Data Programming with Subset Selection",
        "text":"The paradigm of data programming, which uses weak supervision in the form of rules\/labelling functions, and semi-supervised learning, which augments small amounts of labelled data with a large unlabelled dataset, have shown great promise in several text classification scenarios. In this work, we argue that by not using any labelled data, data programming based approaches can yield sub-optimal performances, particularly when the labelling functions are noisy. The first contribution of this work is an introduction of a framework, SPEAR which is a semi-supervised data programming paradigm that learns a joint model that effectively uses the rules\/labelling functions along with semi-supervised loss functions on the feature space. Next, we also study SPEAR-SS which additionally does subset selection on top of the joint semi-supervised data programming objective and selects a set of examples that can be used as the labelled set by SPEAR. The goal of SPEAR-SS is to ensure that the labelled data can complement the labelling functions, thereby benefiting from both data-programming as well as appropriately selected data for human labelling. We demonstrate that by effectively combining semi-supervision, data-programming, and subset selection paradigms, we significantly outperform the current state-of-the-art on seven publicly available datasets. 1",
        "id":235421599
      }
    ],
    "negative_ctxs":[
      {
        "title":"AutoLoss: Learning Discrete Schedules for Alternate Optimization",
        "text":"Many machine learning problems involve iteratively and alternately optimizing different task objectives with respect to different sets of parameters. Appropriately scheduling the optimization of a task objective or a set of parameters is usually crucial to the quality of convergence. In this paper, we present AutoLoss, a meta-learning framework that automatically learns and determines the optimization schedule. AutoLoss provides a generic way to represent and learn the discrete optimization schedule from metadata, allows for a dynamic and data-driven schedule in ML problems that involve alternating updates of different parameters or from different loss objectives. We apply AutoLoss on four ML tasks: d-ary quadratic regression, classification using a multi-layer perceptron (MLP), image generation using GANs, and multi-task neural machine translation (NMT). We show that the AutoLoss controller is able to capture the distribution of better optimization schedules that result in higher quality of convergence on all four tasks. The trained AutoLoss controller is generalizable -it can guide and improve the learning of a new task model with different specifications, or on different datasets.",
        "id":52926194
      },
      {
        "title":"Effective Tag Set Selection in Chinese Word Segmentation via Conditional Random Field Modeling",
        "text":"This paper is concerned with Chinese word segmentation, which is regarded as a character based tagging problem under conditional random field framework. It is different in our method that we consider both feature template selection and tag set selection, instead of feature template focused only method in existing work. Thus, there comes an empirical comparison study of performance among different tag sets in this paper. We show that there is a significant performance difference as different tag sets are selected. Based on the proposed method, our system gives the state-of-the-art performance.",
        "id":18371469
      },
      {
        "title":"Perturbation CheckLists for Evaluating NLG Evaluation Metrics",
        "text":"Natural Language Generation (NLG) evaluation is a multifaceted task requiring assessment of multiple desirable criteria, e.g., fluency, coherency, coverage, relevance, adequacy, overall quality, etc. Across existing datasets for 6 NLG tasks, we observe that the human evaluation scores on these multiple criteria are often not correlated. For example, there is a very low correlation between human scores on fluency and data coverage for the task of structured data to text generation. This suggests that the current recipe of proposing new automatic evaluation metrics for NLG by showing that they correlate well with scores assigned by humans for a single criteria (overall quality) alone is inadequate. Indeed, our extensive study involving 25 automatic evaluation metrics across 6 different tasks and 18 different evaluation criteria shows that there is no single metric which correlates well with human scores on all desirable criteria, for most NLG tasks. Given this situation, we propose CheckLists for better design and evaluation of automatic metrics. We design templates which target a specific criteria (e.g., coverage) and perturb the output such that the quality gets affected only along this specific criteria (e.g., the coverage drops). We show that existing evaluation metrics are not robust against even such simple perturbations and disagree with scores assigned by humans to the perturbed output. The proposed templates thus allow for a fine-grained assessment of automatic evaluation metrics exposing their limitations and will facilitate better design, analysis and evaluation of such metrics. 1Original sentence: Cameron is the director of Titanic Change names:Kate is the director of Titanic (incorrect)Task Criteria Machine TranslationAdequacy: The generated translation should adequately represent all the information present in the reference.Question GenerationRelevance: Is the question related to the source material they are based upon. Answerability: Is the generated question answerable given the context. Informativeness: The summary should convey the key points of the text. Non-redundancy: The summary should not repeat any points, and ideally have maximal information coverage within the limited text length.Abstractive SummarizationReferential clarity: Any intra-sentence or cross-sentence references in the summary should be unambiguous and within the scope of the summary. Focus: The summary needs to have a focus and all the sentences need to contain information related to this focal point. Structure and Coherence: The summary should be a well-organized and coherent body of informationDialogue GenerationMaking sense: Does the bot say things that don't make sense? Engagingness: Is the dialogue agent enjoyable to talk to? Interestingness: Did you find the bot interesting to talk to? Inquisitivenes: Does the bot ask a good amount of questions? Listening: Does the bot pay attention to what you say? Avoiding Repetition: Does the bot repeat itself? (either within or across utterances) Humanness: Is the conversation with a person or a bot?",
        "id":237492016
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that investigates how contrastive learning enhances sentence-level embeddings in natural language processing, especially for subsequent applications?",
    "positive_ctxs":[
      {
        "title":"Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders",
        "text":"Previous work has indicated that pretrained Masked Language Models (MLMs) are not effective as universal lexical and sentence encoders off-the-shelf, i.e., without further taskspecific fine-tuning on NLI, sentence similarity, or paraphrasing tasks using annotated task data. In this work, we demonstrate that it is possible to turn MLMs into effective lexical and sentence encoders even without any additional data, relying simply on self-supervision. We propose an extremely simple, fast, and effective contrastive learning technique, termed Mirror-BERT, which converts MLMs (e.g., BERT and RoBERTa) into such encoders in 20-30 seconds with no access to additional external knowledge. Mirror-BERT relies on identical and slightly modified string pairs as positive (i.e., synonymous) fine-tuning examples, and aims to maximise their similarity during \"identity fine-tuning\". We report huge gains over off-the-shelf MLMs with Mirror-BERT both in lexical-level and in sentencelevel tasks, across different domains and different languages. Notably, in sentence similarity (STS) and question-answer entailment (QNLI) tasks, our self-supervised Mirror-BERT model even matches the performance of the Sentence-BERT models from prior work which rely on annotated task data. Finally, we delve deeper into the inner workings of MLMs, and suggest some evidence on why this simple Mirror-BERT fine-tuning approach can yield effective universal lexical and sentence encoders.",
        "id":233289620
      }
    ],
    "negative_ctxs":[
      {
        "title":"Augmenting Physical Models with Deep Networks for Complex Dynamics Forecasting",
        "text":"Forecasting complex dynamical phenomena in settings where only partial knowledge of their dynamics is available is a prevalent problem across various scientific fields. While purely data-driven approaches are arguably insufficient in this context, standard physical modeling based approaches tend to be over-simplistic, inducing nonnegligible errors. In this work, we introduce the APHYNITY framework, a principled approach for augmenting incomplete physical dynamics described by differential equations with deep data-driven models. It consists in decomposing the dynamics into two components: a physical component accounting for the dynamics for which we have some prior knowledge, and a data-driven component accounting for errors of the physical model. The learning problem is carefully formulated such that the physical model explains as much of the data as possible, while the data-driven component only describes information that cannot be captured by the physical model, no more, no less. This not only provides the existence and uniqueness for this decomposition, but also ensures interpretability and benefits generalization. Experiments made on three important use cases, each representative of a different family of phenomena, i.e. reactiondiffusion equations, wave equations and the non-linear damped pendulum, show that APHYNITY can efficiently leverage approximate physical models to accurately forecast the evolution of the system and correctly identify relevant physical parameters. Code is available at https:\/\/github.com\/yuan-yin\/APHYNITY. arXiv:2010.04456v6 [stat.ML] 10 May 2022 APHYNITY",
        "id":222272443
      },
      {
        "title":"",
        "text":"",
        "id":221373826
      },
      {
        "title":"Un lexique pondéré des noms d'événements en français",
        "text":"Cet article décrit une étude sur l'annotation automatique des noms d'événements dans les textes en français. Plusieurs lexiques existants sont utilisés, ainsi que des règles syntaxiques d'extraction, et un lexique composé de façon automatique, permettant de fournir une valeur sur le niveau d'ambiguïté du mot en tant qu'événement. Cette nouvelle information permettrait d'aider à la désambiguïsation des noms d'événements en contexte 1 .Abstract. This article describes a study on automatic extraction of event nominals in French texts. Some existing lexicons are used, as well as some syntactic extraction rules, and a new, automatically built lexicon is presented. This lexicon gives a value concerning the level of ambiguity of each word as an event.Mots-clés : extraction d'information, événements nominaux, lexiques.",
        "id":57969899
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that investigates how integrating model quantization with knowledge distillation?",
    "positive_ctxs":[
      {
        "title":"BinaryBERT: Pushing the Limit of BERT Quantization",
        "text":"The rapid development of large pre-trained language models has greatly increased the demand for model compression techniques, among which quantization is a popular solution. In this paper, we propose Binary-BERT, which pushes BERT quantization to the limit by weight binarization. We find that a binary BERT is hard to be trained directly than a ternary counterpart due to its complex and irregular loss landscape. Therefore, we propose ternary weight splitting, which initializes BinaryBERT by equivalently splitting from a half-sized ternary network. The binary model thus inherits the good performance of the ternary one, and can be further enhanced by fine-tuning the new architecture after splitting. Empirical results show that our Binary-BERT has only a slight performance drop compared with the full-precision model while being 24× smaller, achieving the state-of-the-art compression results on the GLUE and SQuAD benchmarks. (a) Full-precision Model. (b) Ternary Model. (c) Binary Model. (d) All Together.",
        "id":229923538
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":219305397
      },
      {
        "title":"Sentence Ordering with Manifold-based Classification in Multi-Document Summarization",
        "text":"In this paper, we propose a sentence ordering algorithm using a semi-supervised sentence classification and historical ordering strategy. The classification is based on the manifold structure underlying sentences, addressing the problem of limited labeled data. The historical ordering helps to ensure topic continuity and avoid topic bias. Experiments demonstrate that the method is effective.",
        "id":583930
      },
      {
        "title":"Reduce Catastrophic Forgetting of Dense Retrieval Training with Teleportation Negatives",
        "text":"In this paper, we investigate the instability in the standard dense retrieval training, which iterates between model training and hard negative selection using the being-trained model. We show the catastrophic forgetting phenomena behind the training instability, where models learn and forget different negative groups during training iterations. We then propose ANCE-Tele, which accumulates momentum negatives from past iterations and approximates future iterations using lookahead negatives, as \"teleportations\" along the time axis to smooth the learning process. On web search and OpenQA, ANCE-Tele outperforms previous state-of-theart systems of similar size, eliminates the dependency on sparse retrieval negatives, and is competitive among systems using significantly more (50x) parameters. Our analysis demonstrates that teleportation negatives reduce catastrophic forgetting and improve convergence speed for dense retrieval training. The source code of this paper is available at https: \/\/github.com\/OpenMatch\/ANCE-Tele.",
        "id":253237545
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that investigates incorporating a fact memory component into neural networks to improve language modeling activities without requiring retraining or fine-tuning?",
    "positive_ctxs":[
      {
        "title":"Adaptable and Interpretable Neural Memory Over Symbolic Knowledge",
        "text":"Past research has demonstrated that large neural language models (LMs) encode surprising amounts of factual information: however, augmenting or modifying this information requires modifying a corpus and retraining, which is computationally expensive. To address this problem, we develop a neural LM that includes an interpretable neuro-symbolic KB in the form of a \"fact memory\". Each element of the fact memory is formed from a triple of vectors, where each vector corresponds to a KB entity or relation. Our LM improves performance on knowledge-intensive question-answering tasks, sometimes dramatically, including a 27 point increase in one setting of WebQuestionsSP over a state-of-the-art open-book model, despite using 5% of the parameters. Most interestingly, we demonstrate that the model can be modified, without any re-training, by updating the fact memory.",
        "id":235097242
      }
    ],
    "negative_ctxs":[
      {
        "title":"MixKMeans: Clustering Question-Answer Archives",
        "text":"Community-driven Question Answering (CQA) systems that crowdsource experiential information in the form of questions and answers and have accumulated valuable reusable knowledge.Clustering of QA datasets from CQA systems provides a means of organizing the content to ease tasks such as manual curation and tagging. In this paper, we present a clustering method that exploits the two-part question-answer structure in QA datasets to improve clustering quality. Our method, MixKMeans, composes question and answer space similarities in a way that the space on which the match is higher is allowed to dominate. This construction is motivated by our observation that semantic similarity between question-answer data (QAs) could get localized in either space. We empirically evaluate our method on a variety of real-world labeled datasets. Our results indicate that our method significantly outperforms stateof-the-art clustering methods for the task of clustering question-answer archives.",
        "id":16906583
      },
      {
        "title":"DATA TYPES IN COMPUTATIONAL PHONOLOGY",
        "text":"This paper exanfines certain aspects of phonological structure from the viewpoint of ahstract data types, Our imnlediate goal is to find a format for l)honological representation which will be reasonably f,'fithful to the concerns of theoreti: cal phonology while I)eing rigorous enough to a(Irail a computational interl)retation. The longer term goal is to incorporate such representations into all appropriate general framework for llatnral language processing, i 1 4 9",
        "id":6803114
      },
      {
        "title":"Senti-LSSVM: Sentiment-Oriented Multi-Relation Extraction with Latent Structural SVM",
        "text":"Extracting instances of sentiment-oriented relations from user-generated web documents is important for online marketing analysis. Unlike previous work, we formulate this extraction task as a structured prediction problem and design the corresponding inference as an integer linear program. Our latent structural SVM based model can learn from training corpora that do not contain explicit annotations of sentiment-bearing expressions, and it can simultaneously recognize instances of both binary (polarity) and ternary (comparative) relations with regard to entity mentions of interest. The empirical evaluation shows that our approach significantly outperforms stateof-the-art systems across domains (cameras and movies) and across genres (reviews and forum posts). The gold standard corpus that we built will also be a valuable resource for the community.",
        "id":2825268
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that investigates knowledge transfer from structured databases to unstructured data sources for improving sophisticated question-answering systems?",
    "positive_ctxs":[
      {
        "title":"Knowledge Transfer between Structured and Unstructured Sources for Complex Question Answering",
        "text":"Multi-hop question answering (QA) combines multiple pieces of evidence to search for the correct answer. Reasoning over a text corpus (TextQA) and\/or a knowledge base (KBQA) has been extensively studied and led to distinct system architectures. However, knowledge transfer between such two QA systems has been under-explored. Research questions like what knowledge is transferred or whether the transferred knowledge can help answer over one source using another one, are yet to be answered. In this paper, therefore, we study the knowledge transfer of multi-hop reasoning between structured and unstructured sources. We first propose a unified QA framework named SIMULTQA to enable knowledge transfer and bridge the distinct supervisions from KB and text sources. Then, we conduct extensive analyses to explore how knowledge is transferred by leveraging the pre-training and fine-tuning paradigm. We focus on the low-resource finetuning to show that pre-training SIMULTQA on one source can substantially improve its performance on the other source. More fine-grained analyses on transfer behaviors reveal the types of transferred knowledge and transfer patterns. We conclude with insights into how to construct better QA datasets and systems to exploit knowledge transfer for future work.",
        "id":250390946
      }
    ],
    "negative_ctxs":[
      {
        "title":"Rapid deployment of a new METIS language pair: Catalan-English",
        "text":"We show here the viability of a rapid deployment of a new language pair within the METIS architecture. Contrarily to other SMT or EBMT systems, the METIS architecture allows us to forgo parallel texts, which for many language pairs, such as Catalan-English are hard to obtain. In this experiment, we have successfully built a Catalan-English prototype by simply plugging a POS tagger for Catalan and a bilingual Catalan-English dictionary to the English generation part of the system already developed for other language pairs.",
        "id":14468016
      },
      {
        "title":"",
        "text":"",
        "id":235097514
      },
      {
        "title":"A Constraint-based Representation Scheme of Collocational Structures",
        "text":"t OTS -Trans 10 -3512 JK Utrecht (NL) CLMT Essex University -Wivenhoe Park -C04 3SQ Colchester (UK)",
        "id":7155034
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that investigates representing entities in knowledge graphs with intricate geometric shapes, emphasizing probabilistic analysis and uncertainty modeling?",
    "positive_ctxs":[
      {
        "title":"Probabilistic Box Embeddings for Uncertain Knowledge Graph Reasoning",
        "text":"Knowledge bases often consist of facts which are harvested from a variety of sources, many of which are noisy and some of which conflict, resulting in a level of uncertainty for each triple. Knowledge bases are also often incomplete, prompting the use of embedding methods to generalize from known facts, however existing embedding methods only model triple-level uncertainty and reasoning results lack global consistency. To address these shortcomings, we propose BEUrRE , a novel uncertain knowledge graph embedding method with calibrated probabilistic semantics. BEUrRE models each entity as a box (i.e. axis-aligned hyperrectangle), and relations between two entities as affine transforms on the head and tail entity boxes. The geometry of the boxes allows for efficient calculation of intersections and volumes, endowing the model with calibrated probabilistic semantics and facilitating the incorporation of relational constraints. Extensive experiments on two benchmark datasets show that BEUrRE consistently outperforms baselines on confidence prediction and fact ranking due to it's probabilistic calibration and ability to capture high-order dependencies among facts.",
        "id":233210621
      }
    ],
    "negative_ctxs":[
      {
        "title":"ON-TRAC Consortium End-to-End Speech Translation Systems for the IWSLT 2019 Shared Task",
        "text":"This paper describes the ON-TRAC Consortium translation systems developed for the end-to-end model task of IWSLT Evaluation 2019 for the English→ Portuguese language pair. ON-TRAC Consortium is composed of researchers from three French academic laboratories: LIA (Avignon Université), LIG (Université Grenoble Alpes), and LIUM (Le Mans Université). A single end-to-end model built as a neural encoder-decoder architecture with attention mechanism was used for two primary submissions corresponding to the two EN-PT evaluations sets: (1) TED (MuST-C) and (2) How2. In this paper, we notably investigate impact of pooling heterogeneous corpora for training, impact of target tokenization (characters or BPEs), impact of speech input segmentation and we also compare our best end-to-end model (BLEU of 26.91 on MuST-C and 43.82 on How2 validation sets) to a pipeline (ASR+MT) approach.",
        "id":204961369
      },
      {
        "title":"",
        "text":"",
        "id":232021799
      },
      {
        "title":"SmartNotes: Implicit Labeling of Meeting Data through User Note-Taking and Browsing",
        "text":"We have implemented SmartNotes, a system that automatically acquires labeled meeting data as users take notes during meetings and browse the notes afterwards. Such data can enable meeting understanding components such as topic and action item detectors to automatically improve their performance over a sequence of meetings. The SmartNotes system consists of a laptop based note taking application, and a web based note retrieval system. We shall demonstrate the functionalities of this system, and will also demonstrate the labeled data obtained during typical meetings and browsing sessions.",
        "id":2969941
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that investigates text generation from tabular data, considering elements such as titles, column headings, and cell content, while also integrating numerical reasoning?",
    "positive_ctxs":[
      {
        "title":"Towards Table-to-Text Generation with Numerical Reasoning",
        "text":"Recent neural text generation models have shown significant improvement in generating descriptive text from structured data such as table formats. One of the remaining important challenges is generating more analytical descriptions that can be inferred from facts in a data source. The use of a template-based generator and a pointer-generator is among the potential alternatives for table-to-text generators. In this paper, we propose a framework consisting of a pre-trained model and a copy mechanism. The pre-trained models are fine-tuned to produce fluent text that is enriched with numerical reasoning. However, it still lacks fidelity to the table contents. The copy mechanism is incorporated in the fine-tuning step by using general placeholders to avoid producing hallucinated phrases that are not supported by a table while preserving high fluency. In summary, our contributions are (1) a new dataset for numerical table-to-text generation using pairs of a table and a paragraph of a table description with richer inference from scientific papers, and (2) a table-to-text generation framework enriched with numerical reasoning.",
        "id":236460018
      }
    ],
    "negative_ctxs":[
      {
        "title":"Introduction to the Bio-Entity Recognition Task at JNLPBA",
        "text":"We describe here the JNLPBA shared task of bio-entity recognition using an extended version of the GENIA version 3 named entity corpus of MEDLINE abstracts. We provide background information on the task and present a general discussion of the approaches taken by participating systems.",
        "id":7985741
      },
      {
        "title":"Spoken Language Systems -Technical Challenges for Speech and Natural Language Processing",
        "text":"Speech is the most natural means of communication among humans. It is also believed that spoken language processing will play a major role in establishing a universal interface between humans and machines. Most of the existing spoken language systems are rather primitive. For example, speech synthesizers for reading unrestrict text of any language is only producing machine-sounding speech.Automatic speech recognizers are capable of recognizing spoken language from a selective population doing a highly restricted task. In this talk, we present some examples of spoken language translation and dialogue systems and examine the capabilities and limitations of current spoken language technologies. We also discuss technical challenges for language researchers to help realize the vision of natural human-machine communication to allow humans to converse with machines in any language to access information and solve problems.",
        "id":19366665
      },
      {
        "title":"DeepPavlov Dream: Platform for Building Generative AI Assistants",
        "text":"An open-source DeepPavlov Dream Platform is specifically tailored for development of complex dialog systems like Generative AI Assistants. The stack prioritizes efficiency, modularity, scalability, and extensibility with the goal to make it easier to develop complex dialog systems from scratch. It supports modular approach to implementation of conversational agents enabling their development through the choice of NLP components and conversational skills from a rich library organized into the distributions of ready-for-use multi-skill AI assistant systems. In DeepPavlov Dream, multiskill Generative AI Assistant consists of NLP components that extract features from user utterances, conversational skills that generate or retrieve a response, skill and response selectors that facilitate choice of relevant skills and the best response, as well as a conversational orchestrator that enables creation of multi-skill Generative AI Assistants scalable up to industrial grade AI assistants. The platform allows to integrate large language models into dialog pipeline, customize with prompt engineering, handle multiple prompts during the same dialog session and create simple multimodal assistants.",
        "id":259370765
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that investigates the implementation of sparsity within attention mechanisms to enhance the performance of models processing extremely lengthy documents?",
    "positive_ctxs":[
      {
        "title":"READTWICE: Reading Very Large Documents with Memories",
        "text":"Knowledge-intensive tasks such as question answering often require assimilating information from different sections of large inputs such as books or article collections. We propose READTWICE 1 , a simple and effective technique that combines several strengths of prior approaches to model long-range dependencies with Transformers. The main idea is to read text in small segments, in parallel, summarizing each segment into a memory table to be used in a second read of the text. We show that the method outperforms models of comparable size on several question answering (QA) datasets and sets a new state of the art on the challenging NarrativeQA task, with questions about entire books.",
        "id":234334398
      }
    ],
    "negative_ctxs":[
      {
        "title":"Augmenting WordNet-like lexical resources with distributional evidence. An application-oriented perspective\"",
        "text":"The paper deals with the issue of how and to what extent WordNet-like resources provide the necessary information for an assessment of semantic similarity which is useful for practical applications. The general point is made that taxonomical information should be complemented with distributional evidence. The claim is substantiated through experimental a~t8 and an illustration of a word sense disambiguation system (SENSE) capable of using contextually-relevant semantic similarity.",
        "id":6039355
      },
      {
        "title":"FBK-irst : A Multi-Phase Kernel Based Approach for Drug-Drug Interaction Detection and Classification that Exploits Linguistic Information",
        "text":"This paper presents the multi-phase relation extraction (RE) approach which was used for the DDI Extraction task of SemEval 2013. As a preliminary step, the proposed approach indirectly (and automatically) exploits the scope of negation cues and the semantic roles of involved entities for reducing the skewness in the training data as well as discarding possible negative instances from the test data. Then, a state-of-the-art hybrid kernel is used to train a classifier which is later applied on the instances of the test data not filtered out by the previous step. The official results of the task show that our approach yields an F-score of 0.80 for DDI detection and an F-score of 0.65 for DDI detection and classification. Our system obtained significantly higher results than all the other participating teams in this shared task and has been ranked 1st.",
        "id":18430390
      },
      {
        "title":"Movie Review Classification Based on a Multiple Classifier *",
        "text":"In this paper, we propose a method to classify movie review documents into positive or negative opinions. There are several approaches to classify documents. The previous studies, however, used only a single classifier for the classification task. We describe a multiple classifier for the review document classification task. The method consists of three classifiers based on SVMs, ME and score calculation. We apply two voting methods and SVMs to the integration process of single classifiers. The integrated methods improved the accuracy as compared with the three single classifiers. The experimental results show the effectiveness of our method.",
        "id":13949747
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that investigates the improvement of knowledge representation learning by incorporating global context into graph attention networks?",
    "positive_ctxs":[
      {
        "title":"",
        "text":"",
        "id":236477489
      }
    ],
    "negative_ctxs":[
      {
        "title":"Application Adaptive Electronic Dictionary with Intelligent Interface",
        "text":"The paper presents an electronic dictionary that can be adapted to the needs of different NLP applications. It suggests some ways to save on software customisation and acquisition effort through an intelligent developer interface. The emphasis is made on the flexibility of data representation, handling and access speed.",
        "id":8140941
      },
      {
        "title":"Published as a conference paper at ICLR 2023 LEARNING HIERARCHICAL PROTEIN REPRESENTA- TIONS VIA COMPLETE 3D GRAPH NETWORKS",
        "text":"We consider representation learning for proteins with 3D structures. We build 3D graphs based on protein structures and develop graph networks to learn their representations. Depending on the levels of details that we wish to capture, protein representations can be computed at different levels, e.g., the amino acid, backbone, or all-atom levels. Importantly, there exist hierarchical relations among different levels. In this work, we propose to develop a novel hierarchical graph network, known as ProNet, to capture the relations. Our ProNet is very flexible and can be used to compute protein representations at different levels of granularity. By treating each amino acid as a node in graph modeling as well as harnessing the inherent hierarchies, our ProNet is more effective and efficient than existing methods. We also show that, given a base 3D graph network that is complete, our ProNet representations are also complete at all levels. Experimental results show that ProNet outperforms recent methods on most datasets. In addition, results indicate that different downstream tasks may require representations at different levels. Our code is publicly available as part of the DIG library",
        "id":257364758
      },
      {
        "title":"Argument Structure and Unaccusativity in the Constraint-based Lexicon",
        "text":"This paper addresses the issue of Split Intransitivity (si) and Unaccusative Mismatches (uMs), proposing a constraint-based approach to si and ums within a recent framework of Head-driven Phrase Structure Grammar. I argue against the widely accepted dichotomous distinction of intransitive verbs, which has been advanced by the Unaccusative Hypothesis[Perlmutter (1978)]. I then propose a quadripartitive distinction of intransitive verbs on the basis of the distribution of subject argument in the semantically motivated argument structure, and show that this quadripartitive distinction allows a better understanding of si and ums. The main idea of this proposal will be summarized as the Quadripartitive Split Intransitivity Hypothesis (Qsm).",
        "id":17043198
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that investigates the use of trainable prompts to enhance the parameter optimization process in machine learning models?",
    "positive_ctxs":[
      {
        "title":"The Power of Scale for Parameter-Efficient Prompt Tuning",
        "text":"In this work, we explore \"prompt tuning,\" a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient \"prompt ensembling.\" We release code and model checkpoints to reproduce our experiments. 1ReferencesRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,  Danilo Giampiccolo, Bernardo Magnini, and Idan  Szpektor. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognising textual entailment, volume 6, pages 6-4. Venice. . 2019a. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
        "id":233296808
      }
    ],
    "negative_ctxs":[
      {
        "title":"Detecting Interesting Event Sequences for Sports Reporting",
        "text":"Hand-crafted approaches to content determination are expensive to port to new domains. Machine-learned approaches, on the other hand, tend to be limited to relatively simple selection of items from data sets. We observe that in time series domains, textual descriptions often aggregate a series of events into a compact description. We present a simple technique for automatically determining sequences of events that are worth reporting, and evaluate its effectiveness.",
        "id":528679
      },
      {
        "title":"Employing Phonetic Speech Recognition for Language and Dialect Specific Search",
        "text":"We discuss the notion of language and dialect-specific search in the context of audio indexing. A system is described where users can find dialect or language-specific pronunciations of Afghan placenames in Dari and Pashto. We explore the efficacy of a phonetic speech recognition system employed in this task.This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings are footer added by the organizers. License details: http:\/\/creativecommons.org\/licenses\/by\/4.0\/",
        "id":14249050
      },
      {
        "title":"COMPACT PART-BASED IMAGE REPRESENTATIONS",
        "text":"Learning compact, interpretable image representations is a very natural task which has not been solved satisfactorily even for simple classes of binary images. In this paper, we review various ways of composing parts (or experts) for binary data and argue that competitive forms of interaction are best suited to learn lowdimensional representations. We propose a new rule which discourages parts from learning similar structures and which penalizes opposing expert opinions strongly so that abstaining from voting becomes more attractive. Using a process of oversimplification and correction we show in experiments that very intuitive models can be obtained.",
        "id":16053260
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend a study that uses feedback-driven decoding for producing mathematical proofs using language models?",
    "positive_ctxs":[
      {
        "title":"Generating Natural Language Proofs with Verifier-Guided Search",
        "text":"Reasoning over natural language is a challenging problem in NLP. In this work, we focus on proof generation: Given a hypothesis and a set of supporting facts, the model generates a proof tree indicating how to derive the hypothesis from supporting facts. Compared to generating the entire proof in one shot, stepwise generation can better exploit the compositionality and generalize to longer proofs but has achieved limited success on real-world data. Existing stepwise methods struggle to generate proof steps that are both logically valid and relevant to the hypothesis. Instead, they tend to hallucinate invalid steps given the hypothesis. In this paper, we present a novel stepwise method, NLProofS (Natural Language Proof Search), which learns to generate relevant steps conditioning on the hypothesis. At the core of our approach, we train an independent verifier to check the validity of the proof steps to prevent hallucination. Instead of generating steps greedily, we search for proofs maximizing a global proof score judged by the verifier. NL-ProofS achieves state-of-the-art performance on EntailmentBank and RuleTaker. Specifically, it improves the correctness of predicted proofs from 27.7% to 33.3% in the distractor setting of EntailmentBank, demonstrating the effectiveness of NLProofS in generating challenging human-authored proofs. 1",
        "id":249062748
      }
    ],
    "negative_ctxs":[
      {
        "title":"Learning beyond datasets: Knowledge Graph Augmented Neural Networks for Natural language Processing",
        "text":"Machine Learning has been the quintessential solution for many AI problems, but learning models are heavily dependent on specific training data. Some learning models can be incorporated with prior knowledge using a Bayesian setup, but these learning models do not have the ability to access any organized world knowledge on demand. In this work, we propose to enhance learning models with world knowledge in the form of Knowledge Graph (KG) fact triples for Natural Language Processing (NLP) tasks. Our aim is to develop a deep learning model that can extract relevant prior support facts from knowledge graphs depending on the task using attention mechanism. We introduce a convolutionbased model for learning representations of knowledge graph entity and relation clusters in order to reduce the attention space. We show that the proposed method is highly scalable to the amount of prior information that has to be processed and can be applied to any generic NLP task. Using this method we show significant improvement in performance for text classification with 20Newsgroups (News20) & DBPedia datasets, and natural language inference with Stanford Natural Language Inference (SNLI) dataset. We also demonstrate that a deep learning model can be trained with substantially less amount of labeled training data, when it has access to organized world knowledge in the form of a knowledge base.",
        "id":3361768
      },
      {
        "title":"Document-level Neural MT: A Systematic Comparison",
        "text":"In this paper we provide a systematic comparison of existing and new documentlevel neural machine translation solutions. As part of this comparison, we introduce and evaluate a document-level variant of the recently proposed Star Transformer architecture. In addition to using the traditional metric BLEU, we report the accuracy of the models in handling anaphoric pronoun translation as well as coherence and cohesion using contrastive test sets. Finally, we report the results of human evaluation in terms of Multidimensional Quality Metrics (MQM) and analyse the correlation of the results obtained by the automatic metrics with human judgments.",
        "id":221097205
      },
      {
        "title":"Published as a conference paper at ICLR 2020 LEARNING HIERARCHICAL DISCRETE LINGUISTIC UNITS FROM VISUALLY-GROUNDED SPEECH",
        "text":"In this paper, we present a method for learning discrete linguistic units by incorporating vector quantization layers into neural models of visually grounded speech. We show that our method is capable of capturing both word-level and sub-word units, depending on how it is configured. What differentiates this paper from prior work on speech unit learning is the choice of training objective. Rather than using a reconstruction-based loss, we use a discriminative, multimodal grounding objective which forces the learned units to be useful for semantic image retrieval. We evaluate the sub-word units on the ZeroSpeech 2019 challenge, achieving a 27.3% reduction in ABX error rate over the top-performing submission, while keeping the bitrate approximately the same. We also present experiments demonstrating the noise robustness of these units. Finally, we show that a model with multiple quantizers can simultaneously learn phone-like detectors at a lower layer and word-like detectors at a higher layer. We show that these detectors are highly accurate, discovering 279 words with an F1 score of greater than 0.5.",
        "id":208202182
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend articles that explore the role of late interaction in dense retrieval systems and its influence on the performance of information retrieval?",
    "positive_ctxs":[
      {
        "title":"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction",
        "text":"Neural information retrieval (IR) has greatly advanced search and other knowledgeintensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6-10×.",
        "id":244799249
      }
    ],
    "negative_ctxs":[
      {
        "title":"Published as a conference paper at ICLR 2022 DENOISING LIKELIHOOD SCORE MATCHING FOR CONDITIONAL SCORE-BASED DATA GENERATION",
        "text":"Many existing conditional score-based data generation methods utilize Bayes' theorem to decompose the gradients of a log posterior density into a mixture of scores. These methods facilitate the training procedure of conditional score models, as a mixture of scores can be separately estimated using a score model and a classifier. However, our analysis indicates that the training objectives for the classifier in these methods may lead to a serious score mismatch issue, which corresponds to the situation that the estimated scores deviate from the true ones. Such an issue causes the samples to be misled by the deviated scores during the diffusion process, resulting in a degraded sampling quality. To resolve it, we formulate a novel training objective, called Denoising Likelihood Score Matching (DLSM) loss, for the classifier to match the gradients of the true log likelihood density. Our experimental evidence shows that the proposed method outperforms the previous methods on both Cifar-10 and Cifar-100 benchmarks noticeably in terms of several key evaluation metrics. We thus conclude that, by adopting DLSM, the conditional scores can be accurately modeled, and the effect of the score mismatch issue is alleviated. * Work done during an internship at Mediatek Inc.",
        "id":247763065
      },
      {
        "title":"Generalizing Case Frames Using a Thesaurus and the MDL Principle",
        "text":"A new method for automatically acquiring case frame patterns from large corpora is proposed. In particular, the problem of generalizing values of a case frame slot for a verb is viewed as that of estimating a conditional probability distribution over a partition of words, and a new generalization method based on the Minimum Description Length (MDL) principle is proposed. In order to assist with efficiency, the proposed method makes use of an existing thesaurus and restricts its attention to those partitions that are present as \"cuts\" in the thesaurus tree, thus reducing the generalization problem to that of estimating a \"tree cut model\" of the thesaurus tree. An efficient algorithm is given, which provably obtains the optimal tree cut model for the given frequency data of a case slot, in the sense of MDL. Case frame patterns obtained by the method were used to resolve PP-attachment ambiguity. Experimental results indicate that the proposed method improves upon or is at least comparable with existing methods. . In this paper, we propose a new generalization method, belonging to the first of these two categories, which is both theoretically well-motivated and computationally efficient. Specifically, we formalize the problem of generalizing values of a case frame slot for a given verb as that of estimating a conditional probability distribution over a partition of words, and propose a new generalization method based on the Minimum Description Length principle (MDL): a principle of data compression and statistical estimation from information theory. 1 In order to assist with efficiency, our method makes use of an existing thesaurus and restricts its attention on those partitions that are present as \"cuts\" in the thesaurus tree, thus reducing the generalization problem to that of estimating a \"tree cut model\" of the thesaurus tree. We then give an efficient algorithm that provably obtains the optimal tree cut model for the given frequency data of a case slot, in the sense of MDL. In order to test the effectiveness of our method, we conducted PP-attachment disambiguation experiments using the case frame patterns obtained by our method. Our experimental results indicate that the proposed method improves upon or is at least comparable to existing methods.The remainder of this paper is organized as follows: In Section 2, we formalize the problem of generalizing values of a case frame slot as that of estimating a conditional distribution. In Section 3, we describe our MDL-based generalization method. In Section 4, we present our experimental results. We then give some concluding remarks in Section 5.",
        "id":8497895
      },
      {
        "title":"Using the verifiability of details as a test of deception: A conceptual framework for the automation of the verifiability approach",
        "text":"The Verifiability Approach (VA) is a promising new approach for deception detection. It extends existing verbal credibility assessment tools by asking interviewees to provide statements rich in verifiable detail. Details that i) have been experienced with an identifiable person, ii) have been witnessed by an identifiable person, or iii) have been recorded through technology, are labelled as verifiable. With only minimal modifications of information-gathering interviews this approach has yielded remarkable classification accuracies. Currently, the VA relies on extensive manual annotation by human coders. Aiming to extend the VA's applicability, we present a work in progress on automated VA scoring. We provide a conceptual outline of two automation approaches: one being based on the Linguistic Inquiry and Word Count software and the other on rule-based shallow parsing and named entity recognition. Differences between both approaches and possible future steps for an automated VA are discussed.",
        "id":12108058
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research articles that explore the application of contrastive learning methods to improve sentence embedding efficacy in natural language processing?",
    "positive_ctxs":[
      {
        "title":"ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer",
        "text":"Learning high-quality sentence representations benefits a wide range of natural language processing tasks. Though BERT-based pretrained language models achieve high performance on many downstream tasks, the native derived sentence representations are proved to be collapsed and thus produce a poor performance on the semantic textual similarity (STS) tasks. In this paper, we present ConSERT, a Contrastive Framework for Self-Supervised SEntence Representation Transfer, that adopts contrastive learning to fine-tune BERT in an unsupervised and effective way. By making use of unlabeled texts, ConSERT solves the collapse issue of BERT-derived sentence representations and make them more applicable for downstream tasks. Experiments on STS datasets demonstrate that ConSERT achieves an 8% relative improvement over the previous state-of-the-art, even comparable to the supervised SBERT-NLI. And when further incorporating NLI supervision, we achieve new stateof-the-art performance on STS tasks. Moreover, ConSERT obtains comparable results with only 1000 samples available, showing its robustness in data scarcity scenarios.",
        "id":235187266
      }
    ],
    "negative_ctxs":[
      {
        "title":"Detection of Japanese Homophone Errors by a Decision List Including a Written Word as a Default Evidence",
        "text":"In this paper, we propose a practical method to detect Japanese homophone errors in Japanese texts.It is very important to detect homophone errors in Japanese revision systems because Japanese texts suffer from homophone errors frequently. In order to detect homophone errors, we have only to solve the homophone problem. We can use the decision list to do it because the homophone problem is equivalent to the word sense disambiguation problem. However, the homophone problem is different from the word sense disambiguation problem because the former can use the written word but the latter cannot. In this paper, we incorporate the written word into the original decision list by obtaining the identifying strength of the written word. The improved decision list can raise the F-measure of error detection.",
        "id":1467552
      },
      {
        "title":"Unsupervised Discovery of Implicit Gender Bias",
        "text":"Despite their prevalence in society, social biases are difficult to define and identify, primarily because human judgements in this domain can be unreliable. Therefore, we take an unsupervised approach to identifying gender bias at a comment or sentence level, and present a model that can surface text likely to contain bias. The main challenge in this approach is forcing the model to focus on signs of implicit bias, rather than other artifacts in the data. Thus, the core of our methodology relies on reducing the influence of confounds through propensity score matching and adversarial learning. Our analysis shows how biased comments directed towards female politicians contain mixed criticisms and references to their spouses, while comments directed towards other female public figures focus on appearance and sexualization. Ultimately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements.",
        "id":215814487
      },
      {
        "title":"Reducing Inference Time of Biomedical NER Tasks using Multi-Task Learning",
        "text":"Recently, fine-tuned transformer-based models (e.g., PubMedBERT, BioBERT) have shown the state-of-the-art performance of several BioNLP tasks, such as Named Entity Recognition (NER). However, transformer-based models are complex, have millions of parameters, and are relatively slow during inference. In this paper, we address the time complexity limitations of the BioNLP transformer models. In particular, we propose a Multi-Task Learning based framework for jointly learning three different biomedical NER tasks. Our experiments show a reduction in inference time by a factor of three without any reduction in prediction accuracy.",
        "id":257767707
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research papers that explore applying knowledge distillation to information retrieval, specifically those that concentrate on methods using in-batch negatives",
    "positive_ctxs":[
      {
        "title":"In-Batch Negatives for Knowledge Distillation with Tightly-Coupled Teachers for Dense Retrieval",
        "text":"We present an efficient training approach to text retrieval with dense representations that applies knowledge distillation using the Col-BERT late-interaction ranking model. Specifically, we propose to transfer the knowledge from a bi-encoder teacher to a student by distilling knowledge from ColBERT's expressive MaxSim operator into a simple dot product. The advantage of the bi-encoder teacherstudent setup is that we can efficiently add inbatch negatives during knowledge distillation, enabling richer interactions between teacher and student models. In addition, using Col-BERT as the teacher reduces training cost compared to a full cross-encoder. Experiments on the MS MARCO passage and document ranking tasks and data from the TREC 2019 Deep Learning Track demonstrate that our approach helps models learn robust representations for dense retrieval effectively and efficiently.",
        "id":235720578
      }
    ],
    "negative_ctxs":[
      {
        "title":"Fast Interleaved Bidirectional Sequence Generation",
        "text":"Independence assumptions during sequence generation can speed up inference, but parallel generation of highly inter-dependent tokens comes at a cost in quality. Instead of assuming independence between neighbouring tokens (semi-autoregressive decoding, SA), we take inspiration from bidirectional sequence generation and introduce a decoder that generates target words from the left-to-right and right-toleft directions simultaneously. We show that we can easily convert a standard architecture for unidirectional decoding into a bidirectional decoder by simply interleaving the two directions and adapting the word positions and selfattention masks. Our interleaved bidirectional decoder (IBDecoder) retains the model simplicity and training efficiency of the standard Transformer, and on five machine translation tasks and two document summarization tasks, achieves a decoding speedup of ∼2× compared to autoregressive decoding with comparable quality. Notably, it outperforms left-toright SA because the independence assumptions in IBDecoder are more felicitous. To achieve even higher speedups, we explore hybrid models where we either simultaneously predict multiple neighbouring tokens per direction, or perform multi-directional decoding by partitioning the target sequence. These methods achieve speedups to 4×-11× across different tasks at the cost of <1 BLEU or <0.5 ROUGE (on average). 1",
        "id":225075957
      },
      {
        "title":"Machine Translation Decoding beyond Beam Search",
        "text":"Beam search is the go-to method for decoding auto-regressive machine translation models. While it yields consistent improvements in terms of BLEU, it is only concerned with finding outputs with high model likelihood, and is thus agnostic to whatever end metric or score practitioners care about. Our aim is to establish whether beam search can be replaced by a more powerful metricdriven search technique. To this end, we explore numerous decoding algorithms, including some which rely on a value function parameterised by a neural network, and report results on a variety of metrics. Notably, we introduce a Monte-Carlo Tree Search (MCTS) based method and showcase its competitiveness. We provide a blueprint for how to use MCTS fruitfully in language applications, which opens promising future directions. We find that which algorithm is best heavily depends on the characteristics of the goal metric; we believe that our extensive experiments and analysis will inform further research in this area.",
        "id":233210339
      },
      {
        "title":"Published as a conference paper at ICLR 2022 ON THE CONNECTION BETWEEN LOCAL ATTENTION AND DYNAMIC DEPTH-WISE CONVOLUTION",
        "text":"Vision Transformer (ViT) attains state-of-the-art performance in visual recognition, and the variant, Local Vision Transformer, makes further improvements. The major component in Local Vision Transformer, local attention, performs the attention separately over small local windows. We rephrase local attention as a channel-wise locally-connected layer and analyze it from two network regularization manners, sparse connectivity and weight sharing, as well as dynamic weight computation. We point out that local attention resembles depth-wise convolution and its dynamic variants in sparse connectivity: there is no connection across channels, and each position is connected to the positions within a small local window. The main differences lie in (i) weight sharing -depth-wise convolution shares connection weights (kernel weights) across spatial positions and attention shares the connection weights across channels, and (ii) dynamic weight computation manners -local attention is based on dot-products between pairwise positions in the local window, and dynamic convolution is based on linear projections conducted on the center representation or the globally pooled representation. The connection between local attention and dynamic depth-wise convolution is empirically verified by the ablation study about weight sharing and dynamic weight computation in Local Vision Transformer and (dynamic) depth-wise convolution based network, namely (dynamic) DWNet. We empirically observe that the depth-wise convolution based DWNet and its dynamic variants with lower computation complexity perform on-par with or slightly better than Swin Transformer, an instance of Local Vision Transformer, for ImageNet classification, COCO object detection and ADE semantic segmentation.Published as a conference paper at ICLR 2022 sharing: the connection weights are shared across channels or within each group of channels. (iii) Dynamic weight: the connection weights are dynamically predicted according to each image instance.We connect local attention with depth-wise convolution(Chollet, 2017;Howard et al., 2017)and its dynamic variants that are also a channel-wise spatially-locally connected layer with optional dynamic connection weights. They are similar in sparse connectivity. The main differences lie in (i) weight sharing -depth-wise convolution shares connection weights (kernel weights) across spatial positions and attention shares the connection weights across channels, and (ii) dynamic weight computation manners -local attention is based on dot-products between pairwise positions in the local window, and dynamic convolution is based on linear projections conducted on the center representation or the globally pooled representation.We further present the empirical verification for the connection. We take the recently-developed Local Vision Transformer, Swin Transformer (Liu et al., 2021b), as an example, and study the empirical performance of local attention and (dynamic) depth-wise convolution in the same training settings as Swin Transformer. We replace the local attention layer with the (dynamic) depth-wise convolution layer, keeping the overall structure unchanged. The constructed model is named DWNet.The results show that the (dynamic) depth-wise convolution-based DWNet achieves comparable or slightly higher performance for ImageNet classification and two downstream tasks, COCO object detection and ADE semantic segmentation, and (dynamic) DWNet takes lower computation complexity. The ablation studies imply that weight sharing and dynamic weight improves the model capability. Specifically, (i) for Swin Transformer, weight sharing across channels is beneficial mainly for reducing the parameter (attention weight) complexity, and the attention-based dynamic weight scheme is advantageous in learning instance-specific weights and block-translation equivalent representations; (ii) for depth-wise convolution, weight sharing across positions is beneficial for reducing the parameter complexity as well as learning translation equivalent representations, and the linear projection-based dynamic weight scheme learns instance-specific weights.",
        "id":247450510
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research papers that investigate employing Transformer-based architectures for completing knowledge graphs?",
    "positive_ctxs":[
      {
        "title":"MLMLM: Link Prediction with Mean Likelihood Masked Language Model",
        "text":"Knowledge Bases (KBs) are easy to query, verifiable, and interpretable. They however scale with man-hours and high-quality data. Masked Language Models (MLMs), such as BERT, scale with computing power as well as unstructured raw text data. The knowledge contained within those models is however not directly interpretable. We propose to perform link prediction with MLMs to address both the KBs scalability issues and the MLMs interpretability issues. To do that we introduce MLMLM, Mean Likelihood Masked Language Model, an approach comparing the mean likelihood of generating the different entities to perform link prediction in a tractable manner. We obtain State of the Art (SotA) results on the WN18RR dataset and the best nonentity-embedding based results on the FB15k-237 dataset. We also obtain convincing results on link prediction on previously unseen entities, making MLMLM a suitable approach to introducing new entities to a KB.",
        "id":221703752
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":237367026
      },
      {
        "title":"Describing Spatial Relationships between Objects in Images in English and French",
        "text":"The context for the work we report here is the automatic description of spatial relationships between pairs of objects in images. We investigate the task of selecting prepositions for such spatial relationships. We describe the two datasets of object pairs and prepositions we have created for English and French, and report results for predicting prepositions for object pairs in both of these languages, using two methods: (a) an existing approach which manually fixes the mapping from geometrical features to prepositions, and (b) a Naive Bayes classifier trained on the English and French datasets. For the latter we use features based on object class labels and geometrical measurements of object bounding boxes. We evaluate the automatically generated prepositions on unseen data in terms of accuracy against the human-selected prepositions.",
        "id":9560571
      },
      {
        "title":"Catching the Red Priest: Using Historical Editions of Encyclopaedia Britannica to Track the Evolution of Reputations",
        "text":"In this paper, we investigate the feasibility of using the chronology of changes in historical editions of Encyclopaedia Britannica (EB) to track the changes in the landscape of cultural knowledge, and specifically, the rise and fall in reputations of historical figures. We describe the dataprocessing pipeline we developed in order to identify the matching articles about historical figures in Wikipedia, the current electronic edition of Encyclopaedia Britannica (edition 15), and several digitized historical editions, namely, editions 3, 9, 11. We evaluate our results on the tasks of article segmentation and cross-edition matching using a manually annotated subset of 1000 articles from each edition. As a case study for the validity of discovered trends, we use the Wikipedia category of 18th century classical composers. We demonstrate that our data-driven method allows us to identify cases where a historical figure's reputation experiences a drastic fall or a dramatic recovery which would allow scholars to further investigate previously overlooked instances of such change.",
        "id":14714125
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that analyses prompt tuning as a method to improve the generalizability of pre-trained models while avoiding catastrophic forgetting?",
    "positive_ctxs":[
      {
        "title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation",
        "text":"Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were \"virtual tokens\". We apply prefix-tuning to GPT-2 for table-totext generation and to BART for summarization. We show that by modifying only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. 2020. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. Amodei. 2020. Language models are few-shot learners.",
        "id":230433941
      },
      {
        "title":"The Power of Scale for Parameter-Efficient Prompt Tuning",
        "text":"In this work, we explore \"prompt tuning,\" a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient \"prompt ensembling.\" We release code and model checkpoints to reproduce our experiments. 1ReferencesRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,  Danilo Giampiccolo, Bernardo Magnini, and Idan  Szpektor. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognising textual entailment, volume 6, pages 6-4. Venice. . 2019a. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
        "id":233296808
      }
    ],
    "negative_ctxs":[
      {
        "title":"Recherche cross-modale pour répondre à des questions visuelles",
        "text":"Répondre à des questions visuelles à propos d'entités nommées (KVQAE) est une tâche difficile qui demande de rechercher des informations dans une base de connaissances multimodale.Nous étudions ici comment traiter cette tâche avec une recherche cross-modale et sa combinaison avec une recherche mono-modale, en se focalisant sur le modèle CLIP, un modèle multimodal entraîné sur des images appareillées à leur légende textuelle.Nos résultats démontrent la supériorité de la recherche cross-modale, mais aussi la complémentarité des deux, qui peuvent être combinées facilement.Nous étudions également différentes manières d'ajuster CLIP et trouvons que l'optimisation cross-modale est la meilleure solution, étant en adéquation avec son pré-entraînement.Notre méthode surpasse les approches précédentes, tout en étant plus simple et moins coûteuse.Ces gains de performance sont étudiés intrinsèquement selon la pertinence des résultats de la recherche et extrinsèquement selon l'exactitude de la réponse extraite par un module externe.Nous discutons des différences entre ces métriques et de ses implications pour l'évaluation de la KVQAE.",
        "id":264038819
      },
      {
        "title":"Investigation Into Using the Unicode Standard for Primitives of Unified Han Characters",
        "text":"The Unicode standard identifies and provides representation of the vast majority of known characters used in today's writing systems. Many of these characters belong to the unified Han series, which encapsulates characters from writing systems used in languages such as Chinese, Japanese and Korean languages. These pictographic characters are often made up of smaller primitives, either other characters or more simplified pictography. This paper presents research findings of how the Unicode standard currently represents the primitives used in 4134 of the most common Han characters.PACLIC 28! 131 character was somewhat similar in meaning across most data sets.For each entry, the primitives were then defined and described relative to their position. Character positions were broken up into four main directions: top (t), bottom (b), left (l), right (r), to describe where primitives belong visually within a parent character.",
        "id":20149764
      },
      {
        "title":"Published as a conference paper at ICLR 2023 SQA3D: SITUATED QUESTION ANSWERING IN 3D SCENES",
        "text":"DescriptionSitting at the edge of the bed and facing the couch. Question q : Can I go straight to the coffee table in front of me? Scene context : 3D scan, egocentric video, birdeye view (BEV) picture, etc. Answer : No Location (optional): t t+1ABSTRACT We propose a new task to benchmark scene understanding of embodied agents: Situated Question Answering in 3D Scenes (SQA3D). Given a scene context (e.g., 3D scan), SQA3D requires the tested agent to first understand its situation (position, orientation, etc.) in the 3D scene as described by text, then reason about its surrounding environment and answer a question under that situation. Based upon 650 scenes from ScanNet, we provide a dataset centered around 6.8k unique situations, along with 20.4k descriptions and 33.4k diverse reasoning questions for these situations. These questions examine a wide spectrum of reasoning capabilities for an intelligent agent, ranging from spatial relation comprehension to commonsense understanding, navigation, and multi-hop reasoning. SQA3D imposes a significant challenge to current multi-modal especially 3D reasoning models. We evaluate various state-of-the-art approaches and find that the best one only achieves an overall score of 47.20%, while amateur human participants can reach 90.06%. We believe SQA3D could facilitate future embodied AI research with stronger situation understanding and reasoning capabilities. Code and data are released at sqa3d.github.io.",
        "id":252907411
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that assesses how well language learning models, such as ChatGPT, perform in creating reading comprehension tasks for educational software?",
    "positive_ctxs":[
      {
        "title":"Evaluating Reading Comprehension Exercises Generated by LLMs: A Showcase of ChatGPT in Education Applications",
        "text":"The recent advancement of pre-trained Large Language Models (LLMs), such as OpenAI's ChatGPT, has led to transformative changes across fields. For example, developing intelligent systems in the educational sector that leverage the linguistic capabilities of LLMs demonstrates a visible potential. Though researchers have recently explored how Chat-GPT could possibly assist in student learning, few studies have applied these techniques to real-world classroom settings involving teachers and students. In this study, we implement a reading comprehension exercise generation system that provides high-quality and personalized reading materials for middle school English learners in China. Extensive evaluations of the generated reading passages and corresponding exercise questions, conducted both automatically and manually, demonstrate that the system-generated materials are suitable for students and even surpass the quality of existing human-written ones. By incorporating first-hand feedback and suggestions from experienced educators, this study serves as a meaningful pioneering application of ChatGPT, shedding light on the future design and implementation of LLM-based systems in the educational context.",
        "id":259376566
      }
    ],
    "negative_ctxs":[
      {
        "title":"Representing and Querying Multi-dimensional Markup for Question Answering",
        "text":"This paper describes our approach to representing and querying multi-dimensional, possibly overlapping text annotations, as used in our question answering (QA) system. We use a system extending XQuery, the W3C-standard XML query language, with new axes that allow one to jump easily between different annotations of the same data. The new axes are formulated in terms of (partial) overlap and containment. All annotations are made using stand-off XML in a single document, which can be efficiently queried using the XQuery extension. The system is scalable to gigabytes of XML annotations. We show examples of the system in QA scenarios.",
        "id":12407409
      },
      {
        "title":"Robust and Controllable Object-Centric Learning through Energy-based Models ROBUST AND CONTROLLABLE OBJECT-CENTRIC LEARNING THROUGH ENERGY-BASED MODELS",
        "text":"Humans are remarkably good at understanding and reasoning about complex visual scenes. The capability to decompose low-level observations into discrete objects allows us to build a grounded abstract representation and identify the compositional structure of the world. Accordingly, it is a crucial step for machine learning models to be capable of inferring objects and their properties from visual scenes without explicit supervision. However, existing works on objectcentric representation learning either rely on tailor-made neural network modules or strong probabilistic assumptions in the underlying generative and inference processes. In this work, we present EGO, a conceptually simple and general approach to learning object-centric representations through an energy-based model. By forming a permutation-invariant energy function using vanilla attention blocks readily available in Transformers, we can infer object-centric latent variables via gradient-based MCMC methods where permutation equivariance is automatically guaranteed. We show that EGO can be easily integrated into existing architectures and can effectively extract high-quality object-centric representations, leading to better segmentation accuracy and competitive downstream task performance. Further, empirical evaluations show that EGO's learned representations are robust against distribution shift. Finally, we demonstrate the effectiveness of EGO in systematic compositional generalization, by re-composing learned energy functions for novel scene generation and manipulation.arXiv:2210.05519v1 [cs.LG] 11 Oct 2022Robust and Controllable Object-Centric Learning through Energy-based Models K V Z1 Z2Figure 1: Architecture of EGO-Attention, the variant of EGO used in experiments. x is the input image and z i are the object-centric representations. In each block, EGO attends to the latent variables to refines the hidden scene representation using cross-attention mechanism between x and z i , to measure the consistency between image input and latent representation. minimal hand-designed inductive biases. In a similar spirit, we ask whether we can learn objectcentric representations with minimal human assumptions and task-specific architectures.Contributions In this work, we introduce EGO (EnerGy-based Object-centric learning), a conceptually simple yet effective approach to learning object-centric representations without the need for specially-tailored neural network architectures or excessive generative modeling (typically parametric) assumptions. Based on the Energy-based Model (EBM) framework, we propose to learn an energy function that takes as input a visual scene and a set of object-centric latent variables and outputs a scalar value that measures the consistency between the observation and the latent representation (Section 2). We minimally assume permutation invariance among objects and embed this assumption into the energy function by leveraging the vanilla attention mechanisms from the Transformer (Vaswani et al., 2017) architecture (Section 2.1). In essence, our method makes models act as segmentation annotators, aiming to iteratively improve their annotations by minimizing our energy function. We use gradient-based Markov chain Monte Carlo (MCMC) sampling to efficiently sample latent variables from the EBM distribution, which automatically yields a permutation-equivariant update rule for the latent variables (Section 2.2). This stochastic inference procedure also addresses the inherent uncertainty in learning object-centric representations; models can learn to represent scenes containing multiple objects and potential occlusions in a probabilistic and multi-modal manner. We demonstrate the effectiveness of our approach on a variety of unsupervised object discovery tasks and show both qualitatively and qualitatively that our model can learn to decompose complex scenes into highly accurate and interpretable objects, outperforming state-of-the-art methods on segmentation performance (Section 4.1). We also show that we can reuse the learned energy functions for controllable scene generation and manipulation, which enables systematic compositional generalization to novel scenes (Section 4.2). Finally, we demonstrate the robustness of our model to various distribution shifts and hyperparameter settings (Section 4.3).",
        "id":252815945
      },
      {
        "title":"Learning to Ignore: Long Document Coreference with Bounded Memory Neural Networks",
        "text":"Long document coreference resolution remains a challenging task due to the large memory and runtime requirements of current models. Recent work doing incremental coreference resolution using just the global representation of entities shows practical benefits but requires keeping all entities in memory, which can be impractical for long documents. We argue that keeping all entities in memory is unnecessary, and we propose a memoryaugmented neural network that tracks only a small bounded number of entities at a time, thus guaranteeing a linear runtime in length of document. We show that (a) the model remains competitive with models with high memory and computational requirements on OntoNotes and LitBank, and (b) the model learns an efficient memory management strategy easily outperforming a rule-based strategy.",
        "id":222140769
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that assesses how well large language models, such as GPT-3, perform at coreference resolution when tested in a few-shot learning context?",
    "positive_ctxs":[
      {
        "title":"Large Language Models are Few-Shot Clinical Information Extractors",
        "text":"A long-running goal of the clinical NLP community is the extraction of important variables trapped in clinical notes. However, roadblocks have included dataset shift from the general domain and a lack of public clinical corpora and annotations. In this work, we show that large language models, such as InstructGPT (Ouyang et al., 2022), perform well at zero-and few-shot information extraction from clinical text despite not being trained specifically for the clinical domain. Whereas text classification and generation performance have already been studied extensively in such models, here we additionally demonstrate how to leverage them to tackle a diverse set of NLP tasks which require more structured outputs, including span identification, token-level sequence classification, and relation extraction. Further, due to the dearth of available data to evaluate these systems, we introduce new datasets for benchmarking fewshot clinical information extraction based on a manual re-annotation of the CASI dataset (Moon et al., 2014) for new tasks 1 . On the clinical extraction tasks we studied, the GPT-3 systems significantly outperform existing zeroand few-shot baselines. . 2021. Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare (HEALTH), 3(1):1-23.A study of machine-learning-based approaches to extract clinical entities and their assertions from discharge summaries. improves prompt-based learning for large language models. arXiv preprint arXiv:2202.00828.Hunter Lang, Aravindan Vijayaraghavan, and David Sontag. 2022b. Training subset selection for weak supervision. arXiv preprint arXiv:2206.02914. . 2021. Assessing the impact of automated suggestions on decision making: Domain experts mediate model errors but take less initiative. . 2019b. Entity-relation extraction as multi-turn question answering. arXiv preprint arXiv:1905.05529. . 2022a. Qaner: Prompting question answering models for fewshot named entity recognition. arXiv preprint arXiv:2203.01543. Raffel. 2022b. Few-shot parameter-efficient finetuning is better and cheaper than in-context learning. arXiv preprint arXiv:2205.05638.",
        "id":249062918
      }
    ],
    "negative_ctxs":[
      {
        "title":"A PARADIGM-BASED MORPHOLOGICAL ANALYZER",
        "text":"",
        "id":34504645
      },
      {
        "title":"Automatic Detection of Machine Generated Text: A Critical Survey",
        "text":"Text generative models (TGMs) excel in producing text that matches the style of human language reasonably well. Such TGMs can be misused by adversaries, e.g., by automatically generating fake news and fake product reviews that can look authentic and fool humans. Detectors that can distinguish text generated by TGM from human written text play a vital role in mitigating such misuse of TGMs. Recently, there has been a flurry of works from both natural language processing (NLP) and machine learning (ML) communities to build accurate detectors for English. Despite the importance of this problem, there is currently no work that surveys this fast-growing literature and introduces newcomers to important research challenges. In this work, we fill this void by providing a critical survey and review of this literature to facilitate a comprehensive understanding of this problem. We conduct an in-depth error analysis of the state-of-the-art detector and discuss research directions to guide future work in this exciting area.",
        "id":226237099
      },
      {
        "title":"Graph Enhanced Dual Attention Network for Document-Level Relation Extraction",
        "text":"Document-level relation extraction requires inter-sentence reasoning capabilities to capture local and global contextual information for multiple relational facts. To improve inter-sentence reasoning, we propose to characterize the complex interaction between sentences and potential relation instances via a Graph Enhanced Dual Attention network (GEDA). In GEDA, sentence representation generated by the sentence-to-relation (S2R) attention is refined and synthesized by a Heterogeneous Graph Convolutional Network before being fed into the relation-to-sentence (R2S) attention . We further design a simple yet effective regularizer based on the natural duality of the S2R and R2S attention, whose weights are also supervised by the supporting evidence of relation instances during training. An extensive set of experiments on an existing large-scale dataset show that our model achieves competitive performance, especially for the inter-sentence relation extraction, while the neural predictions can also be interpretable and easily observed. * Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:\/\/ creativecommons.org\/licenses\/by\/4.0\/.",
        "id":227230619
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that assesses techniques to mitigate intersectional biases within Transformer-based models?",
    "positive_ctxs":[
      {
        "title":"Evaluating Debiasing Techniques for Intersectional Biases",
        "text":"Bias is pervasive in NLP models, motivating the development of automatic debiasing techniques. Evaluation of NLP debiasing methods has largely been limited to binary attributes in isolation, e.g., debiasing with respect to binary gender or race, however many corpora involve multiple such attributes, possibly with higher cardinality. In this paper we argue that a truly fair model must consider 'gerrymandering' groups which comprise not only single attributes, but also intersectional groups. We evaluate a form of bias-constrained model which is new to NLP, as well an extension of the iterative nullspace projection technique which can handle multiple protected attributes.",
        "id":237593027
      }
    ],
    "negative_ctxs":[
      {
        "title":"Supervised Grapheme-to-Phoneme Conversion of Orthographic Schwas in Hindi and Punjabi",
        "text":"Hindi grapheme-to-phoneme (G2P) conversion is mostly trivial, with one exception: whether a schwa represented in the orthography is pronounced or unpronounced (deleted). Previous work has attempted to predict schwa deletion in a rule-based fashion using prosodic or phonetic analysis. We present the first statistical schwa deletion classifier for Hindi, which relies solely on the orthography as the input and outperforms previous approaches. We trained our model on a newly-compiled pronunciation lexicon extracted from various online dictionaries. Our best Hindi model achieves state of the art performance, and also achieves good performance on a closely related language, Punjabi, without modification.",
        "id":216056409
      },
      {
        "title":"Coreference information guides human expectations during natural reading",
        "text":"Models of human sentence processing effort tend to focus on costs associated with retrieving structures and discourse referents from memory (memory-based) and\/or on costs associated with anticipating upcoming words and structures based on contextual cues (expectation-based)(Levy, 2008). Although evidence suggests that expectation and memory may play separable roles in language comprehension (Levy et al., 2013), theories of coreference processing have largely focused on memory: how comprehenders identify likely referents of linguistic expressions. In this study, we hypothesize that coreference tracking also informs human expectations about upcoming words, and we test this hypothesis by evaluating the degree to which incremental surprisal measures generated by a novel coreference-aware semantic parser explain human response times in a naturalistic self-paced reading experiment. Results indicate (1) that coreference information indeed guides human expectations and (2) that coreference effects on memory retrieval may exist independently of coreference effects on expectations. Together, these findings suggest that the language processing system exploits coreference information both to retrieve referents from memory and to anticipate upcoming material. This work is licensed under a Creative Commons Attribution 4.0 International License.License details:",
        "id":227231063
      },
      {
        "title":"",
        "text":"",
        "id":227230906
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that employs a relaxed l0 regularization for structured pruning to downsize language models with transformer architectures?",
    "positive_ctxs":[
      {
        "title":"Structured Pruning Learns Compact and Accurate Models",
        "text":"The growing size of neural language models has led to increased attention in model compression. The two predominant approaches are pruning, which gradually removes weights from a pre-trained model, and distillation, which trains a smaller compact model to match a larger one. Pruning methods can significantly reduce the model size but hardly achieve large speedups as distillation. However, distillation methods require large amounts of unlabeled data and are expensive to train. In this work, we propose a task-specific structured pruning method CoFi 1 (Coarse-and Fine-grained Pruning), which delivers highly parallelizable subnetworks and matches the distillation methods in both accuracy and latency, without resorting to any unlabeled data. Our key insight is to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads and hidden units) modules, which controls the pruning decision of each parameter with masks of different granularity. We also devise a layerwise distillation strategy to transfer knowledge from unpruned to pruned models during optimization. Our experiments on GLUE and SQuAD datasets show that CoFi yields models with over 10× speedups with a small accuracy drop, showing its effectiveness and efficiency compared to previous pruning and distillation approaches. 2",
        "id":247922354
      }
    ],
    "negative_ctxs":[
      {
        "title":"Two\/Too Simple Adaptations of Word2Vec for Syntax Problems",
        "text":"We present two simple modifications to the models in the popular Word2Vec tool, in order to generate embeddings more suited to tasks involving syntax. The main issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing using our proposed models.",
        "id":14800090
      },
      {
        "title":"Identification of Fertile Translations in Medical Comparable Corpora : a Morpho-Compositional Approach",
        "text":"This paper defines a method for lexicon in the biomedical domain from comparable corpora. The method is based on compositional translation and exploits morpheme-level translation equivalences. It can generate translations for a large variety of morphologically constructed words and can also generate 'fertile' translations. We show that fertile translations increase the overall quality of the extracted lexicon for English to French translation.",
        "id":13983070
      },
      {
        "title":"A Binarized Neural Network Joint Model for Machine Translation",
        "text":"The neural network joint model (NNJM), which augments the neural network language model (NNLM) with an m-word source context window, has achieved large gains in machine translation accuracy, but also has problems with high normalization cost when using large vocabularies. Training the NNJM with noise-contrastive estimation (NCE), instead of standard maximum likelihood estimation (MLE), can reduce computation cost. In this paper, we propose an alternative to NCE, the binarized NNJM (BNNJM), which learns a binary classifier that takes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks.",
        "id":16588388
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that evaluates the performance decline in various language models, like BLOOM, under 4-bit integer columnar weight-only quantization?",
    "positive_ctxs":[
      {
        "title":"GLM: General Language Model Pretraining with Autoregressive Blank Infilling",
        "text":"There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5).However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation.We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge.GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks.Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks.On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25× parameters of BERT Large , demonstrating its generalizability to different downstream tasks. 1 * The first two authors contributed equally.† Corresponding authors.2 Unconditional generation refers to generating text as a language model without finetuning, while conditional generation refers to sequence-to-sequence tasks.",
        "id":247519241
      }
    ],
    "negative_ctxs":[
      {
        "title":"SSMT: A Machine Translation Evaluation View to Paragraph-to-Sentence Semantic Similarity",
        "text":"This paper presents the system SSMT measuring the semantic similarity between a paragraph and a sentence submitted to the SemEval 2014 task3: Cross-level Semantic Similarity. The special difficulty of this task is the length disparity between the two semantic comparison texts. We adapt several machine translation evaluation metrics for features to cope with this difficulty, then train a regression model for the semantic similarity prediction. This system is straightforward in intuition and easy in implementation. Our best run gets 0.808 in Pearson correlation. METEORderived features are the most effective ones in our experiment.",
        "id":17865660
      },
      {
        "title":"TIGHT RATES IN SUPERVISED OUTLIER TRANSFER LEARNING",
        "text":"A critical barrier to learning an accurate decision rule for outlier detection is the scarcity of outlier data.As such, practitioners often turn to the use of similar but imperfect outlier data from which they might transfer information to the target outlier detection task.Despite the recent empirical success of transfer learning approaches in outlier detection, a fundamental understanding of when and how knowledge can be transferred from a source to a target outlier detection task remains elusive.In this work, we adopt the traditional framework of Neyman-Pearson classification-which formalizes supervised outlier detection-with the added assumption that one has access to some related but imperfect outlier data.Our main results are as follows:• We first determine the information-theoretic limits of the problem under a measure of discrepancy that extends some existing notions from traditional balanced classification; interestingly, unlike in balanced classification, seemingly very dissimilar sources can provide much information about a target, thus resulting in fast transfer.• We then show that, in principle, these information-theoretic limits are achievable by adaptive procedures, i.e., procedures with no a priori information on the discrepancy between source and target outlier distributions.",
        "id":263828899
      },
      {
        "title":"",
        "text":"",
        "id":218974526
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that examines how an annotator's individual attributes, like their gender, ethnicity, and political views, influence their judgment of content deemed offensive?",
    "positive_ctxs":[
      {
        "title":"Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection",
        "text":"Warning: this paper discusses and contains content that is offensive or upsetting.The perceived toxicity of language can vary based on someone's identity and beliefs, but this variation is often ignored when collecting toxic language datasets, resulting in dataset and model biases. We seek to understand the who, why, and what behind biases in toxicity annotations. In two online studies with demographically and politically diverse participants, we investigate the effect of annotator identities (who) and beliefs (why), drawing from social psychology research about hate speech, free speech, racist beliefs, political leaning, and more. We disentangle what is annotated as toxic by considering posts with three characteristics: anti-Black language, African American English (AAE) dialect, and vulgarity. Our results show strong associations between annotator identity and beliefs and their ratings of toxicity. Notably, more conservative annotators and those who scored highly on our scale for racist beliefs were less likely to rate anti-Black language as toxic, but more likely to rate AAE as toxic. We additionally present a case study illustrating how a popular toxicity detection system's ratings inherently reflect only specific beliefs and perspectives. Our findings call for contextualizing toxicity labels in social variables, which raises immense implications for toxic language annotation and detection.Text Categories",
        "id":244117167
      },
      {
        "title":"On Releasing Annotator-Level Labels and Information in Datasets",
        "text":"A common practice in building NLP datasets, especially using crowd-sourced annotations, involves obtaining multiple annotator judgements on the same data instances, which are then flattened to produce a single \"ground truth\" label or score, through majority voting, averaging, or adjudication. While these approaches may be appropriate in certain annotation tasks, such aggregations overlook the socially constructed nature of human perceptions that annotations for relatively more subjective tasks are meant to capture. In particular, systematic disagreements between annotators owing to their socio-cultural backgrounds and\/or lived experiences are often obfuscated through such aggregations. In this paper, we empirically demonstrate that label aggregation may introduce representational biases of individual and group perspectives. Based on this finding, we propose a set of recommendations for increased utility and transparency of datasets for downstream use cases.",
        "id":238634705
      }
    ],
    "negative_ctxs":[
      {
        "title":"Présentation du logiciel Antidote RX",
        "text":"Antidote RX est la sixième édition d'Antidote, un logiciel d'aide à la rédaction développé et commercialisé par la société Druide informatique. Antidote RX comporte un correcteur grammatical avancé, dix dictionnaires de consultation et dix guides linguistiques. Il fonctionne sous les systèmes d'exploitation Windows, Mac OS X et Linux.",
        "id":232021489
      },
      {
        "title":"Towards Relation Extraction from Speech",
        "text":"Relation extraction has focused on extracting semantic relationships between entities from the unstructured written textual data. However, with the vast and rapidly increasing amounts of spoken data, relation extraction from speech is an important but under-explored problem. In this paper, we propose a new information extraction task, speech relation extraction (SpeechRE). To facilitate further research, we construct the first synthetic training datasets, as well as the first human-spoken test set with native English speakers. We establish strong baseline performance for SpeechRE via two approaches. The pipeline approach connects a pretrained ASR module with a text-based relation extraction module. The end-to-end approach employs a cross-modal encoder-decoder architecture. Our comprehensive experiments reveal the relative strengths and weaknesses of these approaches, and shed light on important future directions in SpeechRE research. We share the source code and datasets on https:\/\/github.com\/ wutong8023\/SpeechRE.",
        "id":252918407
      },
      {
        "title":"Improving Collocation Extraction for High Frequency Words",
        "text":"The purpose of this paper is to introduce an alternative word association measure aimed at addressing the under-extraction collocations that contain high frequency words. While measures such as MI provide the important contribution of filtering out sheer high frequency of words in the detection of collocations in large corpora, one side effect of this filtering is that it becomes correspondingly difficult for such measures to detect true collocations involving high frequency words. As an alternative, we propose normalizing the MI measure by dividing the frequency of a candidate lexeme by the number of senses of that lexeme. We premise this alternative approach on the one sense per collocation assumption ofYarowsky (1992;1995). Ten verb-noun collocations involving three high frequency verbs (make, take, run) are used to compare the extraction results of traditional MI and the proposed normalized MI. Results show the ranking of these high-frequency verbs as candidate collocates with the target focal nouns is raised by normalizing MI as proposed. Side effects of these improved rankings are discussed, such as increase in false positives resulting from higher recall. It is found that overall rank precision remains quite stable even with the increased recall of normalized MI.",
        "id":41849556
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that examines how decoding strategies like top-k impact hallucinatory in generated text?",
    "positive_ctxs":[
      {
        "title":"Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding",
        "text":"Dialogue systems powered by large pretrained language models exhibit an innate ability to deliver fluent and natural-sounding responses. Despite their impressive performance, these models are fitful and can often generate factually incorrect statements impeding their widespread adoption. In this paper, we focus on the task of improving faithfulness and reducing hallucination of neural dialogue systems to known facts supplied by a Knowledge Graph (KG). We propose NEU-RAL PATH HUNTER which follows a generatethen-refine strategy whereby a generated response is amended using the KG. NEURAL PATH HUNTER leverages a separate tokenlevel fact critic to identify plausible sources of hallucination followed by a refinement stage that retrieves correct entities by crafting a query signal that is propagated over a k-hop subgraph. We empirically validate our proposed approach on the OpenDialKG dataset (Moon et al., 2019) against a suite of metrics and report a relative improvement of faithfulness over dialogue responses by 20.35% based on FeQA (Durmus et al., 2020). The code is available at https:\/\/github.com\/ nouhadziri\/Neural-Path-Hunter.",
        "id":233296059
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":1833004
      },
      {
        "title":"A Computational Approach to Deciphering Unknown Scripts",
        "text":"We propose and evaluate computational techniques for deciphering unknown scripts. We focus on the case in which an unfamiliar script encodes a known language. The decipherment of a brief document or inscription is driven by data about the spoken language. We consider which scripts are easy or hard to decipher, how much data is required, and whether the techniques are robust against language change over time.",
        "id":12106333
      },
      {
        "title":"Hierarchy Identification for Automatically Generating Table-of-Contents",
        "text":"A table-of-contents (TOC) provides a quick reference to a document's content and structure. We present the first study on identifying the hierarchical structure for automatically generating a TOC using only textual features instead of structural hints e.g. from HTML-tags. We create two new datasets to evaluate our approaches for hierarchy identification. We find that our algorithm performs on a level that is sufficient for a fully automated system. For documents without given segment titles, we extend our work by automatically generating segment titles.We make the datasets and our experimental framework publicly available in order to foster future research in TOC generation.",
        "id":15256469
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that examines how multihead attention networks discern word interrelations in news content for detecting political perspectives?",
    "positive_ctxs":[
      {
        "title":"Using Social and Linguistic Information to Adapt Pretrained Representations for Political Perspective Identification",
        "text":"Understanding the political perspective shaping the way events are discussed in the media is increasingly important due to the dramatic change in news distribution. With the advance in text classification models, the performance of political perspective detection is also improving rapidly. However, current deep learning based text models often require a large amount of supervised data for training, which can be very expensive to obtain for this task. Meanwhile, models pre-trained on the general source and task (e.g. BERT) lack the ability to focus on bias-related text span. In this paper, we propose a novel framework that pretrains the text model using signals from the rich social and linguistic context that is readily available, including entity mentions, news sharing, and frame indicators. The pre-trained models benefit from tasks related to bias detection and therefore are easier to train with the bias labels. We demonstrate the effectiveness of our proposed framework by experiments on two news bias datasets. The models with pre-training achieve significant improvement in performance and are capable of identifying the text span for bias better.The two articles discuss the presentation of John 1 https:\/\/en.wikipedia.org\/wiki\/2021_ storming_of_the_United_States_Capitol",
        "id":236478070
      }
    ],
    "negative_ctxs":[
      {
        "title":"Improved Statistical Machine Translation for Resource-Poor Languages Using Related Resource-Rich Languages",
        "text":"We propose a novel language-independent approach for improving statistical machine translation for resource-poor languages by exploiting their similarity to resource-rich ones. More precisely, we improve the translation from a resourcepoor source language X 1 into a resourcerich language Y given a bi-text containing a limited number of parallel sentences for X 1 -Y and a larger bi-text for X 2 -Y for some resource-rich language X 2 that is closely related to X 1 . The evaluation for Indonesian→English (using Malay) and Spanish→English (using Portuguese and pretending Spanish is resource-poor) shows an absolute gain of up to 1.35 and 3.37 Bleu points, respectively, which is an improvement over the rivaling approaches, while using much less additional data.",
        "id":62762739
      },
      {
        "title":"Natural Language Models for Predicting Programming Comments",
        "text":"Statistical language models have successfully been used to describe and analyze natural language documents. Recent work applying language models to programming languages is focused on the task of predicting code, while mainly ignoring the prediction of programmer comments. In this work, we predict comments from JAVA source files of open source projects, using topic models and n-grams, and we analyze the performance of the models given varying amounts of background data on the project being predicted. We evaluate models on their comment-completion capability in a setting similar to codecompletion tools built into standard code editors, and show that using a comment completion tool can save up to 47% of the comment typing.",
        "id":6706547
      },
      {
        "title":"Fully Connected Neural Network with Advance Preprocessor to Identify Aggression over Facebook and Twitter",
        "text":"Aggression Identification and Hate Speech detection had become an essential part of cyberharassment and cyberbullying and an automatic aggression identification can lead to the interception of such trolling. Following the same idealization, vista.ue team participated in the workshop which included a shared task on 'Aggression Identification'.A dataset of 15,000 aggression-annotated Facebook Posts and Comments written in Hindi (in both Roman and Devanagari script) and English languages were made available and different classification models were designed. This paper presents a model that outperforms Facebook FastText (Joulin et al., 2016a) and deep learning models over this dataset. Especially, the English developed system, when used to classify Twitter text, outperforms all the shared task submitted systems.Related WorkMachine Learning and Deep Learning approaches are been used in a multitude of problems and the text classification is one of them. Many researchers and the companies are working on text classification to get meaningful and relevant information out of text corpora. Next, research published from 2011 to 2018 over aggression, hate speech, offensive, and abusive language identification is presented.Schmidt and Wiegand (2017) present \"A Survey on Hate Speech Detection using Natural Language Processing\". Mainly, the authors empathize on features for hate speech detection, namely bag of word including unigram, bigram and trigram word representations and also character level n-gram features. This work is licensed under a Creative Commons Attribution 4.0 International License.License details:",
        "id":53540549
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that examines how optimized continuous prompts perform against discrete prompts in relational tasks?",
    "positive_ctxs":[
      {
        "title":"Factual Probing Is [MASK]: Learning vs. Learning to Recall",
        "text":"Petroni et al. (2019)demonstrated that it is possible to retrieve world facts from a pretrained language model by expressing them as cloze-style prompts and interpret the model's prediction accuracy as a lower bound on the amount of factual information it encodes. Subsequent work has attempted to tighten the estimate by searching for better prompts, using a disjoint set of facts as training data. In this work, we make two complementary contributions to better understand these factual probing techniques. First, we propose OPTIPROMPT, a novel and efficient method which directly optimizes in continuous embedding space. We find this simple method is able to predict an additional 6.4% of facts in the LAMA benchmark. Second, we raise a more important question: Can we really interpret these probing results as a lower bound? Is it possible that these prompt-search methods learn from the training data too? We find, somewhat surprisingly, that the training data used by these methods contains certain regularities of the underlying fact distribution, and all the existing prompt methods, including ours, are able to exploit them for better fact prediction. We conduct a set of control experiments to disentangle \"learning\" from \"learning to recall\", providing a more detailed picture of what different prompts can reveal about pre-trained language models. 1",
        "id":233210199
      }
    ],
    "negative_ctxs":[
      {
        "title":"PARADOCS : l'entremetteur de documents parallèles indépendant de la langue",
        "text":"Les corpus parallèles sont la pierre angulaire de plusieurs technologies de traduction automatique et des efforts conséquents sont régulièrement portés afin d'en réunir de nouveaux. L'expérience montre que la stratégie visant à réduire l'intervention manuelle dans cet exercice n'est jamais la même d'un corpus à l'autre. Ce constat nous a amené à développer PARADOCS, un entremetteur de documents parallèles qui utilise les entités numériques des documents afin de les apparier. Un classificateur est entraîné à décider des documents parallèles et un moteur de recherche d'information est utilisé afin de réduire l'espace de recherche des paires de documents parallèles. Nous montrons l'efficacité de PARADOCS sur de nombreuses tâches avec de nombreuses paires de langues.ABSTRACT. Parallel corpora are the bread and butter of a number of machine translation technologies. Therefore, important efforts are regularly spent in acquiring new ones. This task often involves a rather cumbersome manual inspection and it is rather difficult to set up a strategy that fits all the needs. We thus developed PARADOCS, a system aiming at doing this automatically. Our solution exploits numerical entities in documents in order to pair them. A classifier trained to recognize parallel text coupled to an information retrieval engine controlling the search space of candidate pairs are the main components of our approach. We tested PARADOCS on a number of tasks involving numerous pairs of languages and report good results. MOTS-CLÉS : corpus parallèles, recherche d'information, traduction automatique.",
        "id":42870841
      },
      {
        "title":"Évaluation automatique de la satisfaction client à partir de conversations de type \"chat\" par réseaux de neurones récurrents avec mécanisme d'attention",
        "text":"Cet article présente des méthodes permettant l'évaluation de la satisfaction client à partir de très vastes corpus de conversation de type \"chat\" entre des clients et des opérateurs. Extraire des connaissances dans ce contexte demeure un défi pour les méthodes de traitement automatique des langues de par la dimension interactive et les propriétés de ce nouveau type de langage à l'intersection du langage écrit et parlé. Nous présentons une étude utilisant des réponses à des sondages utilisateurs comme supervision faible permettant de prédire la satisfaction des usagers d'un service en ligne d'assistance technique et commerciale.ABSTRACTCustomer satisfaction prediction with attention-based RNNs from a chat contact center corpus This paper presents methods to perform knowledge extraction from very large databases of WEB chat conversations between operators and clients in customer contact centers. Extracting knowledge from chat corpus is a challenging research issue. Simply applying traditional text mining tools is clearly sub-optimal as it takes into account neither the interaction dimension nor the particular nature of this language which shares properties of both spoken and written language. We present a method predicting users satisfaction in a chat-based service trained on answers from users to satisfaction surveys. Question Alias J'ai été accompagné(e) et j'ai eu les explications pour faire par moi-même Accompagnement J'ai été écouté(e) et ma demande a été prise en charge Ecoute J'ai été bien conseillé(e) Conseil La solution proposée par Orange me convient Solution Suite à votre contact avec le Service Clients, recommanderiez-vous Orange à vos proches ? RecommanderSi certaines questions portent directement sur l'interaction en elle même (\"Accompagnement\",",
        "id":150982715
      },
      {
        "title":"Classifying Taxonomic Relations between Pairs of Wikipedia Articles",
        "text":"Natural language generation systems rely on taxonomic thesauri for tasks such as lexical choice and aggregation. WordNet is one such taxonomy, but it is limited in size. Motivated by the needs of a generation system in the scientific literature domain, we present a method for building a taxonomic thesaurus from Wikipedia articles, where each article represents a potential concept in the taxonomy. We propose framing the problem of creating a taxonomy as a classification task of the potential relations between individual Wikipedia article pairs, and show that a supervised algorithm can achieve high precision in this task with very little training data.",
        "id":12591323
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that examines how syntactic configurations affect aspect-level sentiment analysis when employing a pretrained model such as RoBERTa?",
    "positive_ctxs":[
      {
        "title":"Does syntax matter? A strong baseline for Aspect-based Sentiment Analysis with RoBERTa",
        "text":"Aspect-Based Sentiment Analysis (ABSA), aiming at predicting the polarities for aspects, is a fine-grained task in the field of sentiment analysis. Previous work showed syntactic information, e.g. dependency trees, can effectively improve the ABSA performance. Recently, pre-trained models (PTMs) also have shown their effectiveness on ABSA. Therefore, the question naturally arises whether PTMs contain sufficient syntactic information for ABSA so that we can obtain a good ABSA model only based on PTMs. In this paper, we firstly compare the induced trees from PTMs and the dependency parsing trees on several popular models for the ABSA task, showing that the induced tree from finetuned RoBERTa (FT-RoBERTa) outperforms the parser-provided tree. The further analysis experiments reveal that the FT-RoBERTa Induced Tree is more sentiment-word-oriented and could benefit the ABSA task. The experiments also show that the pure RoBERTa-based model can outperform or approximate to the previous SOTA performances on six datasets across four languages since it implicitly incorporates the task-oriented syntactic information.",
        "id":233209848
      }
    ],
    "negative_ctxs":[
      {
        "title":"Image-Mediated Learning for Zero-Shot Cross-Lingual Document Retrieval",
        "text":"We propose an image-mediated learning approach for cross-lingual document retrieval where no or only a few parallel corpora are available. Using the images in image-text documents of each language as the hub, we derive a common semantic subspace bridging two languages by means of generalized canonical correlation analysis. For the purpose of evaluation, we create and release a new document dataset consisting of three types of data (English text, Japanese text, and images). Our approach substantially enhances retrieval accuracy in zero-shot and few-shot scenarios where text-to-text examples are scarce.",
        "id":14768436
      },
      {
        "title":"The RWTH Aachen University Filtering System for the WMT 2018 Parallel Corpus Filtering Task",
        "text":"This paper describes the submission of RWTH Aachen University for the De→En parallel corpus filtering task of the EMNLP 2018 Third Conference on Machine Translation (WMT 2018). We use several rule-based, heuristic methods to preselect sentence pairs. These sentence pairs are scored with count-based and neural systems as language and translation models. In addition to single sentence-pair scoring, we further implement a simple redundancy removing heuristic. Our best performing corpus filtering system relies on recurrent neural language models and translation models based on the transformer architecture. A model trained on 10M randomly sampled tokens reaches a performance of 9.2% BLEU on newstest2018. Using our filtering and ranking techniques we achieve 34.8% BLEU.",
        "id":53246405
      },
      {
        "title":"Morphological Tagging to Resolve Morphological Ambiguities",
        "text":"The issue of this paper is to present the advantages of a morphological tagging of English in order to resolve morphological ambiguities. Such a way of tagging seems to be more efficient because it allows an intention description of morphological forms compared with the extensive collection of usual dictionaries. This method has already been experimented on French and has given promising results. It is very relevant since it allows both to bring hidden morphological rules to light which are very useful especially for foreign learners and take lexical creativity into account. Moreover, this morphological tagging was conceived in relation to the subsequent disambiguation which is mainly based on local grammars. The purpose is to create a morphological analyser being easily adaptable and modifiable and avoiding the usual errors of the ordinary morphological taggers linked to dictionaries.",
        "id":9323576
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that examines the challenges faced by pre-trained language models in learning inferential commonsense knowledge when the context is sparse?",
    "positive_ctxs":[
      {
        "title":"Are Rotten Apples Edible? Challenging Commonsense Inference Ability with Exceptions",
        "text":"Previous studies have argued that pre-trained language models encode commonsense relational knowledge (e.g. that apples are edible). However, simultaneous work has revealed that such models are often insensitive to context, even ignoring overt contextual cues such as negations. In this paper, we investigate whether masked language models (the BERT family) can move beyond naive associative biases (e.g., apple → edible) when the context warrants (e.g. ranking inedible higher when presented with the information that the apple is rotten). We introduce the WINOVENTI procedure, which adversarially exploits generic associations in masked language models to create model-specific Winograd-style entailment schemas. Using our constructed WINOVENTI challenges set of over 2, 000 schemas, we show that language models in the BERT family experience a steep drop in performance on prompts that require them to pick answers which require reasoning about context (e.g., from 89.8% to 18.4% for BERT LARGE ). We present evidence that language models exhibit different associative biases, suggesting a need for future work in developing and analyzing frameworks similar to WINOVENTI that are tuned to model-specific weaknesses.",
        "id":236478307
      }
    ],
    "negative_ctxs":[
      {
        "title":"FST Morphology for the Endangered Skolt Sami Language",
        "text":"We present advances in the development of a FST-based morphological analyzer and generator for Skolt Sami. Like other minority Uralic languages, Skolt Sami exhibits a rich morphology, on the one hand, and there is little golden standard material for it, on the other. This makes NLP approaches for its study difficult without a solid morphological analysis. The language is severely endangered and the work presented in this paper forms a part of a greater whole in its revitalization efforts. Furthermore, we intersperse our description with facilitation and description practices not well documented in the infrastructure. Currently, the analyzer covers over 30,000 Skolt Sami words in 148 inflectional paradigms and over 12 derivational forms.",
        "id":215736958
      },
      {
        "title":"",
        "text":"",
        "id":201638186
      },
      {
        "title":"",
        "text":"",
        "id":236145058
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that examines the effect of example sequencing on machine learning model efficacy in few-shot learning scenarios?",
    "positive_ctxs":[
      {
        "title":"Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
        "text":"When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, finetuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are \"fantastic\" and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true fewshot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13% relative improvement for GPTfamily models across eleven different established text classification tasks.",
        "id":233296494
      }
    ],
    "negative_ctxs":[
      {
        "title":"LEARNING TO RESOLVE BRIDGING REFERENCES",
        "text":"We use machine learning techniques to find the best combination of local focus and lexical distance features for identifying the anchor of mereological bridging references. We find that using first mention, utterance distance, and lexical distance computed using either Google or WordNet results in an accuracy significantly higher than obtained in previous experiments.",
        "id":14194
      },
      {
        "title":"Modeling Pronunciation Variation for Bi-Lingual Mandarin\/Taiwanese Speech Recognition",
        "text":"In this paper, a bi-lingual large vocaburary speech recognition experiment based on the idea of modeling pronunciation variations is described. The two languages under study are Mandarin Chinese and Taiwanese (Min-nan). These two languages are basically mutually unintelligible, and they have many words with the same Chinese characters and the same meanings, although they are pronounced differently. Observing the bi-lingual corpus, we found five types of pronunciation variations for Chinese characters. A one-pass, three-layer recognizer was developed that includes a combination of bi-lingual acoustic models, an integrated pronunciation model, and a tree-structure based searching net. The recognizer's performance was evaluated under three different pronunciation models. The results showed that the character error rate with integrated pronunciation models was better than that with pronunciation models, using either the knowledge-based or the data-driven approach. The relative frequency ratio was also used as a measure to choose the best number of pronunciation variations for each Chinese character. Finally, the best character error rates in Mandarin and Taiwanese testing sets were found to be 16.2% and 15.0%, respectively, when the average number of pronunciations for one Chinese character was 3.9.",
        "id":5434385
      },
      {
        "title":"Not Just Iconic: Emoji Interpretation is Shaped by Use",
        "text":"Where do the meaning of emoji come from? Though it is often assumed that emoji are fully iconic, with meanings derived from their visual forms, we argue that this is only one component of their meaning. We surveyed users and non-users of the Chinese social media platform WeChat for their interpretations of emoji specific to WeChat. We find that some emoji show significant differences in their interpretations between users and non-users, as well as how familiar a person is with the specific emoji's use. We argue that this reflects a more complex process for building the meaning of emoji on a platform than pure iconicity.",
        "id":260063134
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that explores how the loss of spatial information impacts the effectiveness of global features in visual tasks?",
    "positive_ctxs":[
      {
        "title":"ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension",
        "text":"Training a referring expression comprehension (ReC) model for a new visual domain requires collecting referring expressions, and potentially corresponding bounding boxes, for images in the domain. While large-scale pre-trained models are useful for image classification across domains, it remains unclear if they can be applied in a zero-shot manner to more complex tasks like ReC. We present ReCLIP, a simple but strong zero-shot baseline that repurposes CLIP, a state-of-the-art large-scale model, for ReC. Motivated by the close connection between ReC and CLIP's contrastive pre-training objective, the first component of ReCLIP is a region-scoring method that isolates object proposals via cropping and blurring, and passes them to CLIP. However, through controlled experiments on a synthetic dataset, we find that CLIP is largely incapable of performing spatial reasoning off-the-shelf. Thus, the second component of ReCLIP is a spatial relation resolver that handles several types of spatial relations. We reduce the gap between zero-shot baselines from prior work and supervised models by as much as 29% on RefCOCOg, and on RefGTA (video game imagery), ReCLIP's relative improvement over supervised ReC models trained on real images is 8%. * This work was done while Sanjay, Will, and Matt were affiliated with AI2. (a) RefCOCO+ (Yu et al., 2016) (b) RefGTA (Tanaka et al., 2019) . 2017. Bottom-up and top-down attention for image captioning and vqa. ArXiv, abs\/1707.07998.",
        "id":248118561
      }
    ],
    "negative_ctxs":[
      {
        "title":"Impact of Politically Biased Data on Hate Speech Classification",
        "text":"One challenge that social media platforms are facing nowadays is hate speech. Hence, automatic hate speech detection has been increasingly researched in recent years -in particular with the rise of deep learning. A problem of these models is their vulnerability to undesirable bias in training data. We investigate the impact of political bias on hate speech classification by constructing three politicallybiased data sets (left-wing, right-wing, politically neutral) and compare the performance of classifiers trained on them. We show that (1) political bias negatively impairs the performance of hate speech classifiers and (2) an explainable machine learning model can help to visualize such bias within the training data. The results show that political bias in training data has an impact on hate speech classification and can become a serious issue.",
        "id":226284001
      },
      {
        "title":"A Language-Independent Unsupervised Model for Morphological Segmentation",
        "text":"Morphological segmentation has been shown to be beneficial to a range of NLP tasks such as machine translation, speech recognition, speech synthesis and information retrieval. Recently, a number of approaches to unsupervised morphological segmentation have been proposed. This paper describes an algorithm that draws from previous approaches and combines them into a simple model for morphological segmentation that outperforms other approaches on English and German, and also yields good results on agglutinative languages such as Finnish and Turkish. We also propose a method for detecting variation within stems in an unsupervised fashion. The segmentation quality reached with the new algorithm is good enough to improve grapheme-to-phoneme conversion.",
        "id":6645042
      },
      {
        "title":"Inter-rater Agreement Measures and the Refinement of Metrics in the PLATO MT Evaluation Paradigm",
        "text":"The PLATO machine translation (MT) evaluation (MTE) research program has as a goal the systematic development of a predictive relationship between discrete, welldefined MTE metrics and the specific information processing tasks that can be reliably performed with output. Traditional measures of quality, informed by the International Standards for Language Engineering (ISLE), namely, clarity, coherence, morphology, syntax, general and domain-specific lexical robustness, and named-entity translation, as well as a DARPAinspired measure of adequacy are its core. For robust validation, indispensable for refinement of tests and guidelines, we measure inter-rater reliability on the assessments. Here we report on our results, focusing on the PLATO Clarity and Coherence assessments, and we discuss our method for iteratively refining both the linguistic metrics and the guidelines for applying them within the PLATO evaluation paradigm. Finally, we discuss reasons why kappa might not be the best measure of interrater agreement for our purposes, and suggest directions for future investigation.",
        "id":2528404
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that explores identifying excess and insufficient translations in evaluating machine translation, especially regarding resolving label discrepancies in cases of omitted content?",
    "positive_ctxs":[
      {
        "title":"Detecting Over-and Undertranslations with Contrastive Conditioning",
        "text":"Omission and addition of content is a typical issue in neural machine translation. We propose a method for detecting such phenomena with off-the-shelf translation models. Using contrastive conditioning, we compare the likelihood of a full sequence under a translation model to the likelihood of its parts, given the corresponding source or target sequence. This allows to pinpoint superfluous words in the translation and untranslated words in the source even in the absence of a reference translation. The accuracy of our method is comparable to a supervised method that requires a custom quality estimation model.",
        "id":247223093
      }
    ],
    "negative_ctxs":[
      {
        "title":"CREAD: Combined Resolution of Ellipses and Anaphora in Dialogues",
        "text":"Anaphora and ellipses are two common phenomena in dialogues. Without resolving referring expressions and information omission, dialogue systems may fail to generate consistent and coherent responses. Traditionally, anaphora is resolved by coreference resolution and ellipses by query rewrite. In this work, we propose a novel joint learning framework of modeling coreference resolution and query rewriting for complex, multi-turn dialogue understanding. Given an ongoing dialogue between a user and a dialogue assistant, for the user query, our joint learning model first predicts coreference links between the query and the dialogue context, and then generates a selfcontained rewritten user query. To evaluate our model, we annotate a dialogue based coreference resolution dataset, MuDoCo, with rewritten queries. Results show that the performance of query rewrite can be substantially boosted (+2.3% F1) with the aid of coreference modeling. Furthermore, our joint model outperforms the state-of-the-art coreference resolution model (+2% F1) on this dataset. Daniel M Bikel, Vittorio Castelli, Radu Florian, and  Ding-jung Han. 2009. Entity linking and slot filling through statistical processing and inference rules. In TAC.Anders Björkelund and Jonas Kuhn. 2014. Learning structured perceptrons for coreference resolution with latent antecedents and non-local features. In",
        "id":234790024
      },
      {
        "title":"Published as a conference paper at ICLR 2023 GRAPH NEURAL NETWORK-INSPIRED KERNELS FOR GAUSSIAN PROCESSES IN SEMI-SUPERVISED LEARN- ING",
        "text":"Gaussian processes (GPs) are an attractive class of machine learning models because of their simplicity and flexibility as building blocks of more complex Bayesian models. Meanwhile, graph neural networks (GNNs) emerged recently as a promising class of models for graph-structured data in semi-supervised learning and beyond. Their competitive performance is often attributed to a proper capturing of the graph inductive bias. In this work, we introduce this inductive bias into GPs to improve their predictive performance for graph-structured data. We show that a prominent example of GNNs, the graph convolutional network, is equivalent to some GP when its layers are infinitely wide; and we analyze the kernel universality and the limiting behavior in depth. We further present a programmable procedure to compose covariance kernels inspired by this equivalence and derive example kernels corresponding to several interesting members of the GNN family. We also propose a computationally efficient approximation of the covariance matrix for scalable posterior inference with large-scale data. We demonstrate that these graph-based kernels lead to competitive classification and regression performance, as well as advantages in computation time, compared with the respective GNNs. Published as a conference paper at ICLR 2023 can be recursively computed if the weights (and biases) in each layer are iid Gaussian. Similar results for other architectures, such as convolution layers and residual connections, were subsequently established in the literature (Novak et al., 2019; Garriga-Alonso et al., 2019).One focus of this work is to establish a similar relationship between GNNs and the limiting GPs. We will derive the covariance kernel that incorporates the graph inductive bias as GNNs do. We start with one of the most widely studied GNNs, the graph convolutional network (GCN)(Kipf & Welling, 2017), and analyze the kernel universality as well as the limiting behavior when the depth also tends to infinity. We then derive covariance kernels from other GNNs by using a programmable procedure that corresponds every building block of a neural network to a kernel operation.Meanwhile, we design efficient computational procedures for posterior inference (i.e., regression and classification). GPs are notoriously difficult to scale because of the cubic complexity with respect to the number of training data. Benchmark graph datasets used by the GNN literature may contain thousands or even millions of labeled nodes(Hu et al., 2020b). The semi-supervised setting worsens the scenario, as the covariance matrix needs to be (recursively) evaluated in full because of the graph convolution operation. We propose a Nyström-like scheme to perform low-rank approximations and apply the approximation recursively on each layer, to yield a low-rank kernel matrix.",
        "id":256827686
      },
      {
        "title":"LEARNING AUDIO-VISUAL SPEECH REPRESENTATION BY MASKED MULTIMODAL CLUSTER PREDICTION",
        "text":"Video recordings of speech contain correlated audio and visual information, providing a strong signal for speech representation learning from the speaker's lip movements and the produced sound. We introduce Audio-Visual Hidden Unit BERT (AV-HuBERT), a self-supervised representation learning framework for audio-visual speech, which masks multi-stream video input and predicts automatically discovered and iteratively refined multimodal hidden units. AV-HuBERT learns powerful audio-visual speech representation benefiting both lip-reading and automatic speech recognition. On the largest public lip-reading benchmark LRS3 (433 hours), AV-HuBERT achieves 32.5% WER with only 30 hours of labeled data, outperforming the former state-of-the-art approach (33.6%) trained with a thousand times more transcribed video data (31K hours) (Makino et al., 2019). The lip-reading WER is further reduced to 26.9% when using all 433 hours of labeled data from LRS3 and combined with self-training. Using our audio-visual representation on the same benchmark for audio-only speech recognition leads to a 40% relative WER reduction over the state-of-the-art performance (1.3% vs 2.3%). Our code and models are available at https:\/\/github.com\/ facebookresearch\/av_hubert * Work done at Meta AI Published as a conference paper at ICLR 2022 including but not limited to keyword spotting in sign language (Albanie et al., 2020), speech enhancement (Xu et al., 2020) and talking face generation(Chen et al., 2018).In this paper, we present Audio-Visual Hidden Unit BERT (AV-HuBERT), a multimodal selfsupervised speech representation learning framework. It encodes masked audio and image sequences into audio-visual features via a hybrid ResNet-transformer architecture to predict the predetermined sequence of discrete cluster assignments. The target cluster assignments are initially generated from signal processing-based acoustic features (e.g., MFCC) and iteratively refined using the features learned by the audio-visual encoder via k-means clustering. AV-HuBERT simultaneously captures linguistic and phonetic information for unmasked regions from both the lipmovement and audio streams into its latent representations, then encodes their long-range temporal relationships to solve the masked-prediction task.The contextualized representations learned by AV-HuBERT show excellent transferability to the lipreading task, where only the visual modality is available. Pre-training on audio and visual input streams led to substantially better results than only visual input. In the low-resource setup using only 30 hours of labeled data from LRS3 (Afouras et al., 2018b), our model achieves a lip-reading WER of 32.5%, outperforming the previous state-of-the-art model (33.6%) trained on 31,000 hours of transcribed videos (Makino et al., 2019). Using the complete 433 hours from LRS3 further reduces WER to 28.6%. We further show AV-HuBERT and self-training are complementary to each other: combining both sets a new lip-reading WER record of 26.9%. In addition, we show that the multimodal clusters derived from AV-HuBERT can be used to pre-train a HuBERT model for audio-based speech recognition, outperforming the previous state-of-the-art model (2.3%) and the unimodal HuBERT pre-trained on audio clusters (1.5%) by a large margin (1.3%).",
        "id":245769552
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that has introduced a dual-phase method for sentence paraphrasing?",
    "positive_ctxs":[
      {
        "title":"Keep the Primary, Rewrite the Secondary: A Two-Stage Approach for Paraphrase Generation",
        "text":"Paraphrase generation is an important and challenging NLG problem.In this work, we propose a new Identification-then-Aggregation (IA) framework to tackle this task. In the identification step, the input tokens are sorted into two groups by a novel Primary\/Secondary Identification (PSI) algorithm. In the aggregation step, these groups are separately encoded, before being aggregated by a custom designed decoder, which autoregressively generates the paraphrased sentence. In extensive experiments on two benchmark datasets, we demonstrate that our model outperforms previous studies by a notable margin. We also show that the proposed approach can generate paraphrases in an interpretable and controllable way.",
        "id":236477430
      }
    ],
    "negative_ctxs":[
      {
        "title":"DIALMED: A Dataset for Dialogue-based Medication Recommendation",
        "text":"Medication recommendation is a crucial task for intelligent healthcare systems.",
        "id":247447718
      },
      {
        "title":"Developing Meeting Support Technologies: From Data to Demonstration (and Beyond)",
        "text":"In 2004, the AMI Consortium set out to collect a multimodal meeting corpus that would give us all the raw material we needed to demonstrate a whole range of meeting support technologies, most of which we knew we hadn't thought of yet. In this keynote, I will talk about how we designed the corpus to grow an interdisciplinary community that would collectively understand not just the technologies but how groups work, and then I will describe some of the novel applications we have built using the data and are currently showing to industrial end users.",
        "id":24044885
      },
      {
        "title":"",
        "text":"",
        "id":226262380
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that improves knowledge base generation using non-differentiable evaluation metrics like BLEU, METEOR, and chrF++?",
    "positive_ctxs":[
      {
        "title":"ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models",
        "text":"Automatic construction of relevant Knowledge Bases (KBs) from text, and generation of semantically meaningful text from KBs are both long-standing goals in Machine Learning. In this paper, we present ReGen, a bidirectional generation of text and graph leveraging Reinforcement Learning (RL) to improve performance. Graph linearization enables us to re-frame both tasks as a sequence to sequence generation problem regardless of the generative direction, which in turn allows the use of Reinforcement Learning for sequence training where the model itself is employed as its own critic leading to Self-Critical Sequence Training (SCST). We present an extensive investigation demonstrating that the use of RL via SCST benefits graph and text generation on WebNLG+ 2020 and TEKGEN datasets.Our system provides state-of-the-art results on WebNLG+ 2020 by significantly improving upon published results from the WebNLG 2020+ Challenge for both text-to-graph and graph-to-text generation tasks. More details in https:\/\/github.com\/IBM\/regen.",
        "id":237353321
      }
    ],
    "negative_ctxs":[
      {
        "title":"Task-based MT Evaluation: Tackling Software, Experimental Design, & Statistical Models",
        "text":"Even with recent, renewed attention to MT evaluation-due in part to n-gram-based metrics(Papineni et al., 2001;Doddington, 2002)and the extensive, online catalogue of MT metrics on the ISLE project(Hovy et al., 2001, few reports involving task-based metrics have surfaced. This paper presents our work on three parts of taskbased MT evaluation: (i) software to track and record users' task performance via a browser, run from a desktop computer or remotely over the web, (ii) factorial experimental design with replicate observations to compare the MT engines, based on the accuracy of users' task responses, and (iii) the use of chi-squared and generalized linear models (GLMs) to permit finer-grained data analyses. We report on the experimental results of a six-way document categorization task, used for the evaluation of three Korean-English MT engines. The statistical models of the probabilities of correct responses yield an ordering of the MT engines, with one engine having a statistically significant lead over the other two. Future research will involve testing user performance on linguistically more complex tasks, as well as extending our initial GLMs with the documents' Bleu scores as variables, to test the scores as independent predictors of task results.",
        "id":26565480
      },
      {
        "title":"Published as a conference paper at ICLR 2021 NEURAL THOMPSON SAMPLING",
        "text":"Thompson Sampling (TS) is one of the most effective algorithms for solving contextual multi-armed bandit problems. In this paper, we propose a new algorithm, called Neural Thompson Sampling, which adapts deep neural networks for both exploration and exploitation. At the core of our algorithm is a novel posterior distribution of the reward, where its mean is the neural network approximator, and its variance is built upon the neural tangent features of the corresponding neural network. We prove that, provided the underlying reward function is bounded, the proposed algorithm is guaranteed to achieve a cumulative regret of O(T 1\/2 ), which matches the regret of other contextual bandit algorithms in terms of total round number T . Experimental comparisons with other benchmark bandit algorithms on various data sets corroborate our theory. arXiv:2010.00827v2 [cs.LG] 30 Dec 2021",
        "id":222124941
      },
      {
        "title":"TIB-VA at SemEval-2022 Task 5: A Multimodal Architecture for the Detection and Classification of Misogynous Memes",
        "text":"The detection of offensive, hateful content on social media is a challenging problem that affects many online users on a daily basis. Hateful content is often used to target a group of people based on ethnicity, gender, religion and other factors. The hate or contempt toward women has been increasing on social platforms. Misogynous content detection is especially challenging when textual and visual modalities are combined to form a single context, e.g., an overlay text embedded on top of an image, also known as meme. In this paper, we present a multimodal architecture that combines textual and visual features to detect misogynous memes. The proposed architecture is evaluated in the SemEval-2022 Task 5: MAMI -Multimedia Automatic Misogyny Identification challenge under the team name TIB-VA. We obtained the best result in the Task-B where the challenge is to classify whether a given document is misogynous and further identify the following sub-classes: shaming, stereotype, objectification, and violence.",
        "id":248157364
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that introduces a metric for assessing Text-to-Image synthesis, emphasizing the semantic congruence between the text and the produced image rather than solely the visual quality?",
    "positive_ctxs":[
      {
        "title":"CLIPScore: A Reference-free Evaluation Metric for Image Captioning",
        "text":"Image captioning has conventionally relied on reference-based automatic evaluations, where machine captions are compared against captions written by humans. This is in contrast to the reference-free manner in which humans assess caption quality.In this paper, we report the surprising empirical finding that CLIP (Radford et al., 2021), a cross-modal model pretrained on 400M im-age+caption pairs from the web, can be used for robust automatic evaluation of image captioning without the need for references. Experiments spanning several corpora demonstrate that our new reference-free metric, CLIPScore, achieves the highest correlation with human judgements, outperforming existing reference-based metrics like CIDEr and SPICE. Information gain experiments demonstrate that CLIPScore, with its tight focus on image-text compatibility, is complementary to existing reference-based metrics that emphasize text-text similarities. Thus, we also present a reference-augmented version, RefCLIPScore, which achieves even higher correlation. Beyond literal description tasks, several case studies reveal domains where CLIPScore performs well (clip-art images, alt-text rating), but also where it is relatively weaker in comparison to reference-based metrics, e.g., news captions that require richer contextual knowledge.",
        "id":233296711
      }
    ],
    "negative_ctxs":[
      {
        "title":"Summ-it++: an enriched version of the Summ-it corpus",
        "text":"This paper presents Summ-it++, an enriched version the Summ-it corpus. In this new version, the corpus has received new semantic layers, named entity categories and relations between named entities, adding to the previous coreference annotation. In addition, we change the original Summ-it format to SemEval.",
        "id":9775710
      },
      {
        "title":"Improving Summaries by Revising Them",
        "text":"This paper describes a program which revises a draft text by aggregating together descriptions of discourse entities, in addition to deleting extraneous information. In contrast to knowledgerich sentence aggregation approaches explored in the past, this approach exploits statistical parsing and robust coreference detection. In an evaluation involving revision of topic-related summaries using informativeness measures from the TIPSTER SUMMAC evaluation, the results show gains in informativeness without compromising readability.",
        "id":2601442
      },
      {
        "title":"",
        "text":"",
        "id":207911293
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that investigates applying conventional data augmentation methods such as removing words and truncating sequences to contrastive learning in NLP applications?",
    "positive_ctxs":[
      {
        "title":"Bootstrapped Unsupervised Sentence Representation Learning",
        "text":"As high-quality labeled data is scarce, unsupervised sentence representation learning has attracted much attention. In this paper, we propose a new framework with a two-branch Siamese Network which maximizes the similarity between two augmented views of each sentence. Specifically, given one augmented view of the input sentence, the online network branch is trained by predicting the representation yielded by the target network of the same sentence under another augmented view. Meanwhile, the target network branch is bootstrapped with a moving average of the online network. The proposed method significantly outperforms other state-of-the-art unsupervised methods on semantic textual similarity (STS) and classification tasks. It can be adopted as a post-training procedure to boost the performance of the supervised methods. We further extend our method for learning multilingual sentence representations and demonstrate its effectiveness on cross-lingual STS tasks. Our code is available at https: \/\/github.com\/yanzhangnlp\/BSL. . 2020. Don't stop pretraining:Adapt language models to domains and tasks. In Proc. of ACL.Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. 2020. Momentum contrast for unsupervised visual representation learning.",
        "id":236460287
      }
    ],
    "negative_ctxs":[
      {
        "title":"Combining Parameter-efficient Modules for Task-level Generalisation",
        "text":"A modular design encourages neural models to disentangle and recombine different facets of knowledge to generalise more systematically to new tasks. In this work, we assume that each task is associated with a subset of latent skills from an (arbitrary size) inventory. In turn, each skill corresponds to a parameter-efficient (sparse \/ low-rank) model adapter. By jointly learning adapters and a routing function that allocates skills to each task, the full network is instantiated as the average of the parameters of active skills. We propose several inductive biases that encourage re-usage and composition of the skills, including variable-size skill allocation and a dual-speed learning rate. We evaluate our latent-skill model in two main settings: 1) multitask reinforcement learning for instruction following on 8 levels of the BabyAI platform; and 2) few-shot fine-tuning of language models on 160 NLP tasks of the CrossFit benchmark. We find that the modular design of our network enhances sample efficiency in reinforcement learning and few-shot generalisation in supervised learning, compared to a series of baselines. These include models where parameters are fully shared, task-specific, or conditionally generated (HyperFormer), as well as sparse mixture-of-experts (Task-MoE).",
        "id":258378176
      },
      {
        "title":"Bridging the Gap in Multilingual Semantic Role Labeling: a Language-Agnostic Approach",
        "text":"Recent research indicates that taking advantage of complex syntactic features leads to favorable results in Semantic Role Labeling. Nonetheless, an analysis of the latest state-of-the-art multilingual systems reveals the difficulty of bridging the wide gap in performance between highresource (e.g., English) and low-resource (e.g., German) settings. To overcome this issue, we propose a fully language-agnostic model that does away with morphological and syntactic features to achieve robustness across languages. Our approach outperforms the state of the art in all the languages of the CoNLL-2009 benchmark dataset, especially whenever a scarce amount of training data is available. Our objective is not to reject approaches that rely on syntax, rather to set a strong and consistent language-independent baseline for future innovations in Semantic Role Labeling. We release our model code and checkpoints at https:\/\/github.com\/SapienzaNLP\/multi-srl.",
        "id":227231163
      },
      {
        "title":"Similarity-weighted Construction of Contextualized Commonsense Knowledge Graphs for Knowledge-intense Argumentation Tasks",
        "text":"Arguments often do not make explicit how a conclusion follows from its premises. To compensate for this lack, we enrich arguments with structured background knowledge to support knowledge-intense argumentation tasks. We present a new unsupervised method for constructing Contextualized Commonsense Knowledge Graphs (CCKGs) that selects contextually relevant knowledge from large knowledge graphs (KGs) efficiently and at high quality. Our work goes beyond context-insensitive knowledge extraction heuristics by computing semantic similarity between KG triplets and textual arguments. Using these triplet similarities as weights, we extract contextualized knowledge paths that connect a conclusion to its premise, while maximizing similarity to the argument. We combine multiple paths into a CCKG that we optionally prune to reduce noise and raise precision. Intrinsic evaluation of the quality of our graphs shows that our method is effective for (re)constructing human explanation graphs. Manual evaluations in a large-scale knowledge selection setup confirm high recall and precision of implicit CSK in the CCKGs. Finally, we demonstrate the effectiveness of CCKGs in a knowledge-insensitive argument quality rating task, outperforming strong baselines and rivaling a GPT-3 based system. 1",
        "id":258685674
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that investigates generative modeling approaches for event extraction, specifically focusing on leveraging large pre-trained language models to reduce the complexity of template engineering?",
    "positive_ctxs":[
      {
        "title":"TEXT2EVENT: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction",
        "text":"Event extraction is challenging due to the complex structure of event records and the semantic gap between text and event. Traditional methods usually extract event records by decomposing the complex structure prediction task into multiple subtasks. In this paper, we propose TEXT2EVENT, a sequence-to-structure generation paradigm that can directly extract events from the text in an end-to-end manner. Specifically, we design a sequence-to-structure network for unified event extraction, a constrained decoding algorithm for event knowledge injection during inference, and a curriculum learning algorithm for efficient model learning. Experimental results show that, by uniformly modeling all tasks in a single model and universally predicting different labels, our method can achieve competitive performance using only record-level annotations in both supervised learning and transfer learning settings.",
        "id":235458429
      }
    ],
    "negative_ctxs":[
      {
        "title":"Question Answering through Transfer Learning from Large Fine-grained Supervision Data",
        "text":"We show that the task of question answering (QA) can significantly benefit from the transfer learning of models trained on a different large, fine-grained QA dataset. We achieve the state of the art in two well-studied QA datasets, WikiQA and SemEval-2016 (Task 3A), through a basic transfer learning technique from SQuAD. For WikiQA, our model outperforms the previous best model by more than 8%. We demonstrate that finer supervision provides better guidance for learning lexical and syntactic information than coarser supervision, through quantitative results and visual analysis. We also show that a similar transfer learning procedure achieves the state of the art on an entailment task. . 2016. Kelp at semeval-2016 task 3: Learning semantic relations between questions and answers. SemEval 16:1116-1123.",
        "id":7928230
      },
      {
        "title":"IN SEARCH OF THE REAL INDUCTIVE BIAS: ON THE ROLE OF IMPLICIT REGULARIZATION IN DEEP LEARNING",
        "text":"We present experiments demonstrating that some other form of capacity control, different from network size, plays a central role in learning multi-layer feedforward networks. We argue, partially through analogy to matrix factorization, that this is an inductive bias that can help shed light on deep learning.",
        "id":6021932
      },
      {
        "title":"Classical Chinese Sentence Segmentation",
        "text":"Sentence segmentation is a fundamental issue in Classical Chinese language processing. To facilitate reading and processing of the raw Classical Chinese data, we propose a statistical method to split unstructured Classical Chinese text into smaller pieces such as sentences and clauses. The segmenter based on the conditional random field (CRF) model is tested under different tagging schemes and various features including n-gram, jump, word class, and phonetic information. We evaluated our method on four datasets from several eras (i.e., from the 5th century BCE to the 19th century). Our CRF segmenter achieves an F-score of 83.34% and can be applied on a variety of data from different eras.",
        "id":3874199
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that investigates how to use contrastive learning for improving logical reasoning over text?",
    "positive_ctxs":[
      {
        "title":"MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning",
        "text":"Logical reasoning is of vital importance to natural language understanding. Previous studies either employ graph-based models to incorporate prior knowledge about logical relations, or introduce symbolic logic into neural models through data augmentation. These methods, however, heavily depend on annotated training data, and thus suffer from overfitting and poor generalization problems due to the dataset sparsity. To address these two problems, in this paper, we propose MERIt, a MEta-path guided contrastive learning method for logical ReasonIng of text, to perform selfsupervised pre-training on abundant unlabeled text data. Two novel strategies serve as indispensable components of our method. In particular, a strategy based on meta-path is devised to discover the logical structure in natural texts, followed by a counterfactual data augmentation strategy to eliminate the information shortcut induced by pre-training. The experimental results on two challenging logical reasoning benchmarks, i.e., ReClor and LogiQA, demonstrate that our method outperforms the SOTA baselines with significant improvements. 1 * Corresponding author: Yangyang Guo and Liqiang Nie. 1 Our code and pre-trained models are available at https: \/\/github.com\/SparkJiao\/MERIt. 2  We refer the term logical reasoning to the task itself in the remaining of this paper.",
        "id":247187518
      }
    ],
    "negative_ctxs":[
      {
        "title":"Introducing QuBERT: A Large Monolingual Corpus and BERT Model for Southern Quechua",
        "text":"The lack of resources for languages in the Americas has proven to be a problem for the creation of digital systems such as machine translation, search engines, chat bots, and more. The scarceness of digital resources for a language causes a higher impact on populations where the language is spoken by millions of people. We introduce the first official large combined corpus for deep learning of an indigenous South American low-resource language spoken by millions called Quechua. Specifically, our curated corpus is created from text gathered from the southern region of Peru where a dialect of Quechua is spoken that has not traditionally been used for digital systems as a target dialect in the past. In order to make our work repeatable by others, we also offer a public, pre-trained, BERT model called Qu-BERT which is the largest linguistic model ever trained for any Quechua type, not just the southern region dialect. We furthermore test our corpus and its corresponding BERT model on two major tasks: (1) named-entity recognition (NER) and (2) part-of-speech (POS) tagging by using state-of-the-art techniques where we achieve results comparable to other work on higher-resource languages. In this article, we describe the methodology, challenges, and results from the creation of QuBERT which is on on par with other state-of-the-art multilingual models for natural language processing achieving between 71 and 74% F1 score on NER and 84-87% on POS tasks. . 2020. Multilingual denoising pretraining for neural machine translation. Trans. Assoc.",
        "id":250391068
      },
      {
        "title":"Sense Extension Functions in Lexical Semantics",
        "text":"A b stra ctRepresenting polysemy in an economical way is an issue of major impor tance within lexical semantics. Polysemy is found both within single lexi cal entries, and systematically in some lexical classes with common semantic properties. Prepositions in various languages are generally considered highly polysemic in an unpredictable way. The latter participate in what can be called systematic polysemy. This work is highly inspired by work as different as Pustejovsky [Pus91], Copestake and Briscoe [CB95], and Lakoff [Lak94].1 will sketch a framework or the fundamentals of a formalism in which important polysemic properties can be described. The interpretational se mantics is built as typed lambda-calculus. This choice is not essential to the formalism, which might be extended to situation-theoretical notation and in terpretation. Currently, situation-theoretical issues are not discussed within the framework.It is briefly outlined how the lexical semantics as construed in this paper can be implemented in a typed feature structure formalism compatible to HPSG [PS94]. Accounts of various aspects of prepositional semantics are given in this formalism, with special emphasis on the Danish preposition rued.",
        "id":18156564
      },
      {
        "title":"Joint Learning for Coreference Resolution with Markov Logic",
        "text":"Pairwise coreference resolution models must merge pairwise coreference decisions to generate final outputs. Traditional merging methods adopt different strategies such as the bestfirst method and enforcing the transitivity constraint, but most of these methods are used independently of the pairwise learning methods as an isolated inference procedure at the end. We propose a joint learning model which combines pairwise classification and mention clustering with Markov logic. Experimental results show that our joint learning system outperforms independent learning systems. Our system gives a better performance than all the learning-based systems from the CoNLL-2011 shared task on the same dataset. Compared with the best system from CoNLL-2011, which employs a rule-based method, our system shows competitive performance.",
        "id":17786494
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that investigates how to use data augmentation for improving logical reasoning over text?",
    "positive_ctxs":[
      {
        "title":"MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning",
        "text":"Logical reasoning is of vital importance to natural language understanding. Previous studies either employ graph-based models to incorporate prior knowledge about logical relations, or introduce symbolic logic into neural models through data augmentation. These methods, however, heavily depend on annotated training data, and thus suffer from overfitting and poor generalization problems due to the dataset sparsity. To address these two problems, in this paper, we propose MERIt, a MEta-path guided contrastive learning method for logical ReasonIng of text, to perform selfsupervised pre-training on abundant unlabeled text data. Two novel strategies serve as indispensable components of our method. In particular, a strategy based on meta-path is devised to discover the logical structure in natural texts, followed by a counterfactual data augmentation strategy to eliminate the information shortcut induced by pre-training. The experimental results on two challenging logical reasoning benchmarks, i.e., ReClor and LogiQA, demonstrate that our method outperforms the SOTA baselines with significant improvements. 1 * Corresponding author: Yangyang Guo and Liqiang Nie. 1 Our code and pre-trained models are available at https: \/\/github.com\/SparkJiao\/MERIt. 2  We refer the term logical reasoning to the task itself in the remaining of this paper.",
        "id":247187518
      },
      {
        "title":"Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text",
        "text":"Logical reasoning of text requires identifying critical logical structures in the text and performing inference over them. Existing methods for logical reasoning mainly focus on contextual semantics of text while struggling to explicitly model the logical inference process. In this paper, we not only put forward a logic-driven context extension framework but also propose a logic-driven data augmentation algorithm. The former follows a three-step reasoning paradigm, and each step is respectively to extract logical expressions as elementary reasoning units, symbolically infer the implicit expressions following equivalence laws and extend the context to validate the options. The latter augments literally similar but logically different instances and incorporates contrastive learning to better capture logical information, especially logical negative and conditional relationships. We conduct experiments on two benchmark datasets, ReClor and LogiQA. The results show that our method achieves state-of-the-art performance on both datasets, and even surpasses human performance on the ReClor dataset. 1",
        "id":234335834
      }
    ],
    "negative_ctxs":[
      {
        "title":"Regular Expression Guided Entity Mention Mining from Noisy Web Data",
        "text":"Many important entity types in web documents, such as dates, times, email addresses, and course numbers, follow or closely resemble patterns that can be described by Regular Expressions (REs). Due to a vast diversity of web documents and ways in which they are being generated, even seemingly straightforward tasks such as identifying mentions of date in a document become very challenging. It is reasonable to claim that it is impossible to create a RE that is capable of identifying such entities from web documents with perfect precision and recall. Rather than abandoning REs as a go-to approach for entity detection, this paper explores ways to combine the expressive power of REs, ability of deep learning to learn from large data, and human-in-the loop approach into a new integrated framework for entity identification from web data. The framework starts by creating or collecting the existing REs for a particular type of an entity. Those REs are then used over a large document corpus to collect weak labels for the entity mentions and a neural network is trained to predict those RE-generated weak labels. Finally, a human expert is asked to label a small set of documents and the neural network is fine tuned on those documents. The experimental evaluation on several entity identification problems shows that the proposed framework achieves impressive accuracy, while requiring very modest human effort.",
        "id":53083786
      },
      {
        "title":"HR@JUST team at SemEval-2020 Task 4: The impact of RoBERTa transformer for evaluation common sense understanding",
        "text":"This paper describes the results of our team HR@JUST participation at SemEval-2020 Task 4 -Commonsense Validation and Explanation (ComVE) for the POST evaluation period. The provided task consists of three sub-tasks, we participate in task A. We considered a state-of-the-art approach for solving this task by performing RoBERTa model with no Next Sentences Prediction (NSP), dynamic masking, larger training data, and larger batch size. The achieved results show that we got the 11th rank on the final test set leaderboard with an accuracy of 91.3%.This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:\/\/ creativecommons.org\/licenses\/by\/4.0\/.",
        "id":227230568
      },
      {
        "title":"Diachronic word embeddings and semantic shifts: a survey",
        "text":"Recent years have witnessed a surge of publications aimed at tracing temporal changes in lexical semantics using distributional methods, particularly prediction-based word embedding models. However, this vein of research lacks the cohesion, common terminology and shared practices of more established areas of natural language processing. In this paper, we survey the current state of academic research related to diachronic word embeddings and semantic shifts detection. We start with discussing the notion of semantic shifts, and then continue with an overview of the existing methods for tracing such time-related shifts with word embedding models. We propose several axes along which these methods can be compared, and outline the main challenges before this emerging subfield of NLP, as well as prospects and possible applications.This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:\/\/ creativecommons.org\/licenses\/by\/4.0\/",
        "id":47019063
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that investigates merging speech and text modalities in a unified representation space for processing spoken language through encoder-decoder models?",
    "positive_ctxs":[
      {
        "title":"SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing",
        "text":"Motivated by the success of T5 (Text-To-Text Transfer Transformer) in pre-trained natural language processing models, we propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech\/text representation learning. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech\/text) pre\/post-nets. After preprocessing the input speech\/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech\/text modality based on the output of the decoder. Leveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text. To align the textual and speech information into this unified semantic space, we propose a cross-modal vector quantization approach that randomly mixes up speech\/text states with latent units as the interface between encoder and decoder. Extensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification. We release our code and model at https:\/\/github.com\/microsoft\/ SpeechT5.",
        "id":238856828
      }
    ],
    "negative_ctxs":[
      {
        "title":"MIT-MEDG at SemEval-2018 Task 7: Semantic Relation Classification via Convolution Neural Network",
        "text":"SemEval 2018 Task 7 tasked participants to build a system to classify two entities within a sentence into one of the 6 possible relation types. We tested 3 classes of models: Linear classifiers, Long Short-Term Memory (LSTM) models, and Convolutional Neural Network (CNN) models. Ultimately, the CNN model class proved most performant, so we specialized to this model for our final submissions.We improved performance beyond a vanilla CNN by including a variant of negative sampling, using custom word embeddings learned over a corpus of ACL articles, training over corpora of both tasks 1.1 and 1.2, using reversed feature, using part of context words beyond the entity pairs and using ensemble methods to improve our final predictions. We also tested attention based pooling, upsampling, and data augmentation, but none improved performance. Our model achieved rank 6 out of 28 (macro-averaged F1-score: 72.7) in subtask 1.1, and rank 4 out of 20 (macro F1: 80.6) in subtask 1.2.",
        "id":44072041
      },
      {
        "title":"Evaluation Method for Determining Groups of Users Who Find MT \"Useful\"",
        "text":"This paper describes an evaluation experiment designed to determine groups of subjects who prefer reading MT outputs to reading the original text. Our approach can be applied to any language pairs, but we will explain the methodology by taking English to Japanese translation as an example. In the case of E-J MT, it can be assumed that main users are Japanese and that most of them have some knowledge of English. It is often the case, in the case of E-J MT systems, that those people who are comfortable with reading English do not find E-J MT outputs useful, and in many cases, they would rather prefer reading the original English text. On the other hand, E-J MT outputs prove to be useful to those who find it hard to read the original English texts. We have used the reading comprehension part of the Test Of English for International Communication (TOEIC) to determine the threshold English ability level, dividing these two user groups.",
        "id":7939877
      },
      {
        "title":"A Telecom-Domain Online Customer Service Assistant Based on Question Answering with Word Embedding and Intent Classification",
        "text":"In the paper, we propose an information retrieval based (IR-based) Question Answering (QA) system to assist online customer service staffs respond users in the telecom domain. When user asks a question, the system retrieves a set of relevant answers and ranks them. Moreover, our system uses a novel reranker to enhance the ranking result of information retrieval. It employs the word2vec model to represent the sentences as vectors. It also uses a sub-category feature, predicted by the knearest neighbor algorithm. Finally, the system returns the top five candidate answers, making online staffs find answers much more efficiently.",
        "id":37505914
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that investigates methods for fine-tuning generative language models with a focus on parameter efficiency to reduce computational demands?",
    "positive_ctxs":[
      {
        "title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation",
        "text":"Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were \"virtual tokens\". We apply prefix-tuning to GPT-2 for table-totext generation and to BART for summarization. We show that by modifying only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. 2020. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. Amodei. 2020. Language models are few-shot learners.",
        "id":230433941
      }
    ],
    "negative_ctxs":[
      {
        "title":"Representing Clinical Notes for Adverse Drug Event Detection",
        "text":"Electronic health records have emerged as a promising source of information for pharmacovigilance. Adverse drug events are, however, known to be heavily underreported, which makes it important to develop capabilities to detect such information automatically in clinical text.While machine learning offers possible solutions, it remains unclear how best to represent clinical notes in a manner conducive to learning high-performing predictive models. Here, 42 representations are explored in an empirical investigation using 27 real, clinical datasets, indicating that combining local and global (distributed) representations of words and named entities yields higher accuracy than using either in isolation. Subsequent analyses highlight the relative importance of various named entity classes for predicting adverse drug events.",
        "id":6304413
      },
      {
        "title":"SpeechRecorder -a Universal Platform Independent Multi-Channel Audio Recording Software",
        "text":"SpeechRecorder is a platform independent audio recording software for speech corpus recordings. It is implemented in Java in a clean object-oriented design and adheres to established technology standards and document interchange formats. SpeechRecorder allows Unicode text and multimedia prompts, it supports audio recordings via more than two channels, and it features multiple configurable screens. Recording sessions are defined by recording scripts written in XML. The recording scripts can be executed manually by the experimenter, or automatically for unsupervised recordings; progress through the script can be sequential or randomized. SpeechRecorder is based on URLs to access local and network resources and thus allows recordings via the WWW.",
        "id":15486073
      },
      {
        "title":"",
        "text":"",
        "id":248780088
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that investigates techniques for creating counterfactual examples to enhance question-answering systems, such as training a T5 model augmented with retrieval?",
    "positive_ctxs":[
      {
        "title":"Retrieval-guided Counterfactual Generation for QA",
        "text":"Deep NLP models have been shown to be brittle to input perturbations. Recent work has shown that data augmentation using counterfactuals -i.e. minimally perturbed inputscan help ameliorate this weakness. We focus on the task of creating counterfactuals for question answering, which presents unique challenges related to world knowledge, semantic diversity, and answerability. To address these challenges, we develop a Retrieve-Generate-Filter (RGF) technique to create counterfactual evaluation and training data with minimal human supervision. Using an open-domain QA framework and question generation model trained on original task data, we create counterfactuals that are fluent, semantically diverse, and automatically labeled. Data augmentation with RGF counterfactuals improves performance on out-of-domain and challenging evaluation sets over and above existing methods, in both the reading comprehension and open-domain QA settings. Moreover, we find that RGF data leads to significant improvements to robustness to local perturbations. 1",
        "id":238856938
      }
    ],
    "negative_ctxs":[
      {
        "title":"Modelling the Reduplicating Lushootseed Morphology with an FST and LSTM",
        "text":"In this paper, we present an FST based approach for conducting morphological analysis, lemmatization and generation of Lushootseed words. Furthermore, we use the FST to generate training data for an LSTM based neural model and train this model to do morphological analysis. The neural model reaches a 71.9% accuracy on the test data. Furthermore, we discuss reduplication types in the Lushootseed language forms. The approach involves the use of both attested instances of reduplication and bare stems for applying a variety of reduplications to, as it is unclear just how much variation can be attributed to the individual speakers and authors of the source materials. That is, there may be areal factors that can be aligned with certain types of reduplication and their frequencies.",
        "id":259833791
      },
      {
        "title":"SURF: SEMI-SUPERVISED REWARD LEARNING WITH DATA AUGMENTATION FOR FEEDBACK-EFFICIENT PREFERENCE-BASED REINFORCEMENT LEARNING",
        "text":"Preference-based reinforcement learning (RL) has shown potential for teaching agents to perform the target tasks without a costly, pre-defined reward function by learning the reward with a supervisor's preference between the two agent behaviors. However, preference-based learning often requires a large amount of human feedback, making it difficult to apply this approach to various applications. This data-efficiency problem, on the other hand, has been typically addressed by using unlabeled samples or data augmentation techniques in the context of supervised learning. Motivated by the recent success of these approaches, we present SURF, a semi-supervised reward learning framework that utilizes a large amount of unlabeled samples with data augmentation. In order to leverage unlabeled samples for reward learning, we infer pseudo-labels of the unlabeled samples based on the confidence of the preference predictor. To further improve the label-efficiency of reward learning, we introduce a new data augmentation that temporally crops consecutive subsequences from the original behaviors. Our experiments demonstrate that our approach significantly improves the feedback-efficiency of the state-ofthe-art preference-based method on a variety of locomotion and robotic manipulation tasks.",
        "id":247613305
      },
      {
        "title":"",
        "text":"",
        "id":219302491
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that investigates the enhancement of neural passage retrieval through the application of dual encoders and the generation of synthetic questions?",
    "positive_ctxs":[
      {
        "title":"Zero-shot Neural Passage Retrieval via Domain-targeted Synthetic Question Generation",
        "text":"A major obstacle to the wide-spread adoption of neural retrieval models is that they require large supervised training sets to surpass traditional term-based techniques, which are constructed from raw corpora. In this paper, we propose an approach to zero-shot learning for passage retrieval that uses synthetic question generation to close this gap. The question generation system is trained on general domain data, but is applied to documents in the targeted domain. This allows us to create arbitrarily large, yet noisy, question-passage relevance pairs that are domain specific. Furthermore, when this is coupled with a simple hybrid termneural model, first-stage retrieval performance can be improved further. Empirically, we show that this is an effective strategy for building neural passage retrieval models in the absence of large training corpora. Depending on the domain, this technique can even approach the accuracy of supervised models.",
        "id":231704318
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":219732655
      },
      {
        "title":"Parsing with the Shortest Derivation",
        "text":"Common wisdom has it that tile bias of stochastic grammars in favor of shorter deriwttions of a sentence is hamfful and should be redressed. We show that the common wisdom is wrong for stochastic grammars that use elementary trees instead o1' conlext-l'ree rules, such as Stochastic Tree-Substitution Grammars used by Data-Oriented Parsing models. For such grammars a non-probabilistic metric based on tile shortest derivation outperforms a probabilistic metric on the ATIS and OVIS corpora, while it obtains competitive results on the Wall Street Journal (WSJ) corpus. This paper also contains the first publislmd experiments with DOP on the WSJ.",
        "id":1051919
      },
      {
        "title":"DIDEC: The Dutch Image Description and Eye-tracking Corpus",
        "text":"We present a corpus of spoken Dutch image descriptions, paired with two sets of eye-tracking data: free viewing, where participants look at images without any particular purpose, and description viewing, where we track eye movements while participants produce spoken descriptions of the images they are viewing. This paper describes the data collection procedure and the corpus itself, and provides an initial analysis of self-corrections in image descriptions. We also present two studies showing the potential of this data. Though these studies mainly serve as an example, we do find two interesting results: (1) the eye-tracking data for the description viewing task is more coherent than for the free-viewing task; (2) variation in image descriptions (also called image specificity; Jas and Parikh, 2015) is only moderately correlated across different languages. Our corpus can be used to gain a deeper understanding of the image description task, particularly how visual attention is correlated with the image description process.Title and Abstract in DutchDIDEC: Een corpus van afbeeldingen met Nederlandstalige beschrijvingen en eye-tracking data Wij presenteren DIDEC, een corpus van foto's met gesproken Nederlandse beschrijvingen en twee verschillende soorten eye-tracking data: ofwel verzameld tijdens het beschrijven van de afbeeldingen, ofwel verzameld terwijl de proefpersonen alleen maar keken naar de afbeeldingen (zonder ze te hoeven beschrijven). Dit artikel beschrijft de dataverzameling, alsook een eerste analyse van de zelf-correcties in de beschrijvingen. Daarnaast beschrijven we twee voorbeeldstudies om aan te geven wat er mogelijk is met DIDEC. Deze studies geven twee interessante resultaten: (1) de eye-tracking data verzameld tijdens het beschrijven van de afbeeldingen is eenduidiger dan de data verzameld tijdens het bekijken van de afbeeldingen; (2) de variatie in de beschrijvingen voor iedere afbeelding (ook wel afbeeldingsspecificiteit genoemd; Jas and Parikh, 2015) is slechts matig gecorreleerd tussen verschillende talen (Duits, Nederlands, Engels). Ons corpus kan gebruikt worden om beter te begrijpen hoe mensen afbeeldingen beschrijven, en in het bijzonder wat de rol is van visuele aandacht op het beschrijvingsproces. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:\/\/ creativecommons.org\/licenses\/by\/4.0\/ 1. Free viewing: eye-tracking data collected without any concurrent task. 2. Description viewing: eye-tracking data collected simultaneously with the spoken descriptions.",
        "id":52009933
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that investigates the impact of randomly removing words from sentences as a data augmentation strategy to mitigate overfitting in NLP models?",
    "positive_ctxs":[
      {
        "title":"DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings",
        "text":"We propose DiffCSE, an unsupervised contrastive learning framework for learning sentence embeddings. DiffCSE learns sentence embeddings that are sensitive to the difference between the original sentence and an edited sentence, where the edited sentence is obtained by stochastically masking out the original sentence and then sampling from a masked language model. We show that DiffSCE is an instance of equivariant contrastive learning (Dangovski et al., 2021), which generalizes contrastive learning and learns representations that are insensitive to certain types of augmentations and sensitive to other \"harmful\" types of augmentations. Our experiments show that DiffCSE achieves state-of-the-art results among unsupervised sentence representation learning methods, outperforming unsupervised SimCSE 1 by 2.3 absolute points on semantic textual similarity tasks. 2",
        "id":248299679
      }
    ],
    "negative_ctxs":[
      {
        "title":"Evaluation of Online Dialogue Policy Learning Techniques",
        "text":"The number of applied Dialogue Systems is ever increasing in several service providing and other applications as a way to efficiently and inexpensively serve large numbers of customers. A DS that employs some form of adaptation to the environment and its users is called an Adaptive Dialogue System (ADS). A significant part of the research community has lately focused on ADS and many existing or novel techniques are being applied to this problem. One of the most promising techniques is Reinforcement Learning (RL) and especially online RL. This paper focuses on online RL techniques used to achieve adaptation in Dialogue Management and provides an evaluation of various such methods in an effort to aid the designers of ADS in deciding which method to use. To the best of our knowledge there is no other work to compare online RL techniques on the dialogue management problem.",
        "id":18538074
      },
      {
        "title":"Optimal k-arization of Synchronous Tree-Adjoining Grammar",
        "text":"SynchronousTree-Adjoining Grammar (STAG) is a promising formalism for syntaxaware machine translation and simultaneous computation of natural-language syntax and semantics. Current research in both of these areas is actively pursuing its incorporation. However, STAG parsing is known to be NP-hard due to the potential for intertwined correspondences between the linked nonterminal symbols in the elementary structures. Given a particular grammar, the polynomial degree of efficient STAG parsing algorithms depends directly on the rank of the grammar: the maximum number of correspondences that appear within a single elementary structure. In this paper we present a compile-time algorithm for transforming a STAG into a strongly-equivalent STAG that optimally minimizes the rank, k, across the grammar. The algorithm performs in O(|G| + |Y | · L 3 G ) time where L G is the maximum number of links in any single synchronous tree pair in the grammar and Y is the set of synchronous tree pairs of G.",
        "id":218606769
      },
      {
        "title":"A Data-Driven, Factorization Parser for CCG Dependency Structures",
        "text":"This paper is concerned with building CCG-grounded, semantics-oriented deep dependency structures with a data-driven, factorization model. Three types of factorization together with different higherorder features are designed to capture different syntacto-semantic properties of functor-argument dependencies. Integrating heterogeneous factorizations results in intractability in decoding. We propose a principled method to obtain optimal graphs based on dual decomposition. Our parser obtains an unlabeled f-score of 93.23 on the CCGBank data, resulting in an error reduction of 6.5% over the best published result. which yields a significant improvement over the best published result in the literature. Our implementation is available at http:\/\/www.icst. pku.edu.cn\/lcwm\/grass.",
        "id":9531504
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that investigates the influence of cognitive biases on human interpretation of AI-generated explanations, specifically within the realm of explainable natural language processing?",
    "positive_ctxs":[
      {
        "title":"On the Interaction of Belief Bias and Explanations",
        "text":"A myriad of explainability methods have been proposed in recent years, but there is little consensus on how to evaluate them. While automatic metrics allow for quick benchmarking, it isn't clear how such metrics reflect human interaction with explanations. Human evaluation is of paramount importance, but previous protocols fail to account for belief biases affecting human performance, which may lead to misleading conclusions. We provide an overview of belief bias, its role in human evaluation, and ideas for NLP practitioners on how to account for it. For two experimental paradigms, we present a case study of gradientbased explainability introducing simple ways to account for humans' prior beliefs: models of varying quality and adversarial examples. We show that conclusions about the highest performing methods change when introducing such controls, pointing to the importance of accounting for belief bias in evaluation.",
        "id":235669650
      }
    ],
    "negative_ctxs":[
      {
        "title":"The Synaesthetic and Metaphorical Uses of wei 'taste' in Chinese Buddhist Texts",
        "text":"This paper investigates the non-gustatory uses of the gustatory word wei 'taste' in Chinese Buddhist texts, in particular, in the Āgamas. The non-gustatory uses of wei 'taste' basically fall into two categories: the synaesthetic category and the metaphorical category. The former features the use of wei 'taste' as an umbrella sensory term which can collocate with all the other sensory words, whereas the latter shows that wei 'taste' can modify abstract and sublime Buddhist terms, such as fa 'dhamma' and jietuo 'enlightenment', for the sake of concretization. These two categories of uses have one sense in common: the sense of \"pleasure and joy\", which can be interpreted in both mundane and supra-mundane levels, depending on the context. Moreover, we find that the versatile uses of wei 'taste' are most likely to be influenced by its equivalent in the Pāli Buddhist texts. This finding sheds light on the history of Chinese language development, specifically, how Chinese language has been influenced by Buddhist text translation.",
        "id":18449613
      },
      {
        "title":"Morpho-Syntactic Analysis for Reordering in Statistical Machine Translation",
        "text":"In the framework of statistical machine translation (SMT), correspondences between the words in the source and the target language are learned from bilingual corpora on the basis of so-called alignment models. Among other things these are meant to capture the differences in word order in different languages. In this paper we show that SMT can take advantage of the explicit introduction of some linguistic knowledge about the sentence structure in the languages under consideration. In contrast to previous publications dealing with the incorporation of morphological and syntactic information into SMT, we focus on two aspects of reordering for the language pair German and English, namely question inversion and detachable German verb prefixes. The results of systematic experiments are reported and demonstrate the applicability of the approach to both translation directions on a German-English corpus.",
        "id":13158483
      },
      {
        "title":"Parsing Incomplete Sentences",
        "text":"An efficient context-free parsing algorithm is presented that can parse sentences with unknown parts of unknown length. It pa'oduees in finite form all possible parses (often infinite in number) that could account for the missing parts. The algorithm is a variation oa the construction due to Earley. ltowever, its presentation is such that it can readily be adapted to any chart parsing schema (topdown, bottom-up, etc...).",
        "id":3052195
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that investigates using autoencoder architectures to generate e-commerce product descriptions by integrating product titles, features, and supplementary descriptions crafted by marketers?",
    "positive_ctxs":[
      {
        "title":"Interactive Latent Knowledge Selection for E-commerce Product Copywriting Generation",
        "text":"As the multi-modal e-commerce is thriving, high-quality advertising product copywriting has gain more attentions, which plays a crucial role in the e-commerce recommender, advertising and even search platforms. The advertising product copywriting is able to enhance the user experience by highlighting the product's characteristics with textual descriptions and thus to improve the likelihood of user click and purchase. Automatically generating product copywriting has attracted noticeable interests from both academic and industrial communities, where existing solutions merely make use of a product's title and attribute information to generate its corresponding description. However, in addition to the product title and attributes, we observe that there are various auxiliary descriptions created by the shoppers or marketers in the ecommerce platforms (namely human knowledge), which contains valuable information for product copywriting generation, yet always accompanying lots of noises. In this work, we propose a novel solution to automatically generating product copywriting that involves all the title, attributes and denoised auxiliary knowledge. To be specific, we design an end-to-end generation framework equipped with two variational autoencoders that works interactively to select informative human knowledge and generate diverse copywriting. Experiments on real-world e-commerce product copywriting datasets demonstrate that our proposed method outperforms various baselines with regard to both automatic and human evaluation metrics.",
        "id":248780467
      }
    ],
    "negative_ctxs":[
      {
        "title":"Parallel corpora for the Galician language: building and processing of the CLUVI (Linguistic Corpus of the University of Vigo)",
        "text":"In this paper, we present the methodology developed by the SLI (Computational Linguistics Group of the University of Vigo) for the building and processing of the CLUVI Corpus, showing the TMX-based XML specification designed to encode both morphosyntactic features and translation alignments in parallel corpora, and the solutions adopted for making the CLUVI parallel corpora freely available over the WWW",
        "id":18658160
      },
      {
        "title":"TWEETQA: A Social Media Focused Question Answering Dataset",
        "text":"With social media becoming increasingly popular on which lots of news and real-time events are reported, developing automated question answering systems is critical to the effectiveness of many applications that rely on realtime knowledge. While previous datasets have concentrated on question answering (QA) for formal text like news and Wikipedia, we present the first large-scale dataset for QA over social media data. To ensure that the tweets we collected are useful, we only gather tweets used by journalists to write news articles. We then ask human annotators to write questions and answers upon these tweets. Unlike other QA datasets like SQuAD in which the answers are extractive, we allow the answers to be abstractive. We show that two recently proposed neural models that perform well on formal texts are limited in their performance when applied to our dataset. In addition, even the finetuned BERT model is still lagging behind human performance with a large margin. Our results thus point to the need of improved QA systems targeting social media text. 1",
        "id":196174735
      },
      {
        "title":"Reranking Models in Fine-grained Opinion Analysis",
        "text":"We describe the implementation of reranking models for fine-grained opinion analysis -marking up opinion expressions and extracting opinion holders. The reranking approach makes it possible to model complex relations between multiple opinions in a sentence, allowing us to represent how opinions interact through the syntactic and semantic structure. We carried out evaluations on the MPQA corpus, and the experiments showed significant improvements over a conventional system that only uses local information: for both tasks, our system saw recall boosts of over 10 points.",
        "id":15851196
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend research that proposed enhancing the RoBERTa model for event extraction by adding a Bi-LSTM and a CRF layer?",
    "positive_ctxs":[
      {
        "title":"",
        "text":"",
        "id":236486171
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":207999578
      },
      {
        "title":"Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal Aided Diagnosis of Dementia Type through Computer-Based Analysis of Spontaneous Speech",
        "text":"This pilot study evaluates the ability of machined learned algorithms to assist with the differential diagnosis of dementia subtypes based on brief (< 10 min) spontaneous speech samples. We analyzed 1 recordings of a brief spontaneous speech sample from 48 participants from 5 different groups: 4 types of dementia plus healthy controls. Recordings were analyzed using a speech recognition system optimized for speakerindependent spontaneous speech. Lexical and acoustic features were automatically extracted. The resulting feature profiles were used as input to a machine learning system that was trained to identify the diagnosis assigned to each research participant. Between groups lexical and acoustic differences features were detected in accordance with expectations from prior research literature suggesting that classifications were based on features consistent with human-observed symptomatology. Machine learning algorithms were able to identify participants' diagnostic group with accuracy comparable to existing diagnostic methods in use today. Results suggest this clinical speech analytic approach offers promise as an additional, objective and easily obtained source of diagnostic information for clinicians.",
        "id":14523848
      },
      {
        "title":"Evaluation of Unsupervised Information Extraction",
        "text":"Unsupervised methods gain more and more attention nowadays in information extraction area, which allows to design more open extraction systems. In the domain of unsupervised information extraction, clustering methods are of particular importance. However, evaluating the results of clustering remains difficult at a large scale, especially in the absence of a reliable reference. On the basis of our experiments on unsupervised relation extraction, we first discuss in this article how to evaluate clustering quality without a reference by relying on internal measures. Then we propose a method, supported by a dedicated annotation tool, for building a set of reference clusters of relations from a corpus. Moreover, we apply it to our experimental framework and illustrate in this way how to build a significant reference for unsupervised relation extraction, more precisely made of 80 clusters gathering more than 4,000 relation instances, in a short time. Finally, we present how such reference is exploited for the evaluation of clustering with external measures and analyze the results of the application of these measures to the clusters of relations produced by our unsupervised relation extraction system.",
        "id":17024886
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend scholarly articles that investigate the practice of refining language models through the exclusive modification of bias parameters in their linear components?",
    "positive_ctxs":[
      {
        "title":"BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
        "text":"We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods. Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.",
        "id":231672601
      }
    ],
    "negative_ctxs":[
      {
        "title":"DSCORER: A Fast Evaluation Metric for Discourse Representation Structure Parsing",
        "text":"Discourse representation structures (DRSs) are scoped semantic representations for texts of arbitrary length. Evaluation of the accuracy of predicted DRSs plays a key role in developing semantic parsers and improving their performance. DRSs are typically visualized as nested boxes, in a way that is not straightforward to process automatically. COUNTER, an evaluation algorithm for DRSs, transforms them to clauses and measures clause overlap by searching for variable mappings between two DRSs. Unfortunately, COUNTER is computationally costly (with respect to memory and CPU time) and does not scale with longer texts. We introduce DSCORER, an efficient new metric which converts box-style DRSs to graphs and then measures the overlap of n-grams in the graphs. Experiments show that DSCORER computes accuracy scores that correlate with scores from COUNTER at a fraction of the time.",
        "id":219963636
      },
      {
        "title":"Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies",
        "text":"We present the Uppsala system for the CoNLL 2018 Shared Task on universal dependency parsing. Our system is a pipeline consisting of three components: the first performs joint word and sentence segmentation; the second predicts part-ofspeech tags and morphological features; the third predicts dependency trees from words and tags. Instead of training a single parsing model for each treebank, we trained models with multiple treebanks for one language or closely related languages, greatly reducing the number of models. On the official test run, we ranked 7th of 27 teams for the LAS and MLAS metrics. Our system obtained the best scores overall for word segmentation, universal POS tagging, and morphological features.",
        "id":52175678
      },
      {
        "title":"An Open-Source Finite State Morphological Transducer for Modern Standard Arabic",
        "text":"We develop an open-source large-scale finitestate morphological processing toolkit (Ara-ComLex) for Modern Standard Arabic (MSA) distributed under the GPLv3 license. 1 The morphological transducer is based on a lexical database specifically constructed for this purpose. In contrast to previous resources, the database is tuned to MSA, eliminating lexical entries no longer attested in contemporary use. The database is built using a corpus of 1,089,111,204 words, a pre-annotation tool, machine learning techniques, and knowledgebased pattern matching to automatically acquire lexical knowledge. Our morphological transducer is evaluated and compared to LDC's SAMA (Standard Arabic Morphological Analyser).",
        "id":17781445
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend studies on hierarchical modeling of user interests for tailoring news recommendation systems?",
    "positive_ctxs":[
      {
        "title":"HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation",
        "text":"User interest modeling is critical for personalized news recommendation. Existing news recommendation methods usually learn a single user embedding for each user from their previous behaviors to represent their overall interest. However, user interest is usually diverse and multi-grained, which is difficult to be accurately modeled by a single user embedding. In this paper, we propose a news recommendation method with hierarchical user interest modeling, named HieRec. Instead of a single user embedding, in our method each user is represented in a hierarchical interest tree to better capture their diverse and multi-grained interest in news. We use a three-level hierarchy to represent 1) overall user interest; 2) user interest in coarse-grained topics like sports; and 3) user interest in fine-grained topics like football. Moreover, we propose a hierarchical user interest matching framework to match candidate news with different levels of user interest for more accurate user interest targeting. Extensive experiments on two real-world datasets validate our method can effectively improve the performance of user modeling for personalized news recommendation.",
        "id":235368202
      }
    ],
    "negative_ctxs":[
      {
        "title":"SLEDGE-Z: A Zero-Shot Baseline for COVID-19 Literature Search",
        "text":"With worldwide concerns surrounding the Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2), there is a rapidly growing body of scientific literature on the virus. Clinicians, researchers, and policymakers need to be able to search these articles effectively. In this work, we present a zeroshot ranking algorithm that adapts to COVIDrelated scientific literature. Our approach filters training data from another collection down to medical-related queries, uses a neural reranking model pre-trained on scientific text (SciBERT), and filters the target document collection. This approach ranks top among zeroshot methods on the TREC COVID Round 1 leaderboard, and exhibits a P@5 of 0.80 and an nDCG@10 of 0.68 when evaluated on both Round 1 and 2 judgments. Despite not relying on TREC-COVID data, our method outperforms models that do. As one of the first search methods to thoroughly evaluate",
        "id":222310503
      },
      {
        "title":"Weak Connectivity in (Un)bounded Dependency Constructions *",
        "text":"This paper argues that various kinds of displaced structures in English should be licensed by a more explicitly formulated type of rule schema in order to deal with what is called weak connectivity in English. This paper claims that the filler and the gap site cannot maintain the total identity of features but a partial overlap since the two positions need to obey the structural forces that come from occupying respective positions. One such case is the missing object construction where the subject fillers and the object gaps are to observe requirements that are imposed on the respective positions. Others include passive constructions and topicalized structures. In this paper, it is argued that the feature discrepancy comes from the different syntactic positions in which the fillers are assumed to be located before and after displacement. In order to capture this type of mismatch, syntactically relevant features are handled separately from the semantically motivated features in order to deal with the syntactically imposed requirements.",
        "id":2760209
      },
      {
        "title":"Predicting Fine-Tuning Performance with Probing",
        "text":"Large NLP models have recently shown impressive performance in language understanding tasks, typically evaluated by their finetuned performance. Alternatively, probing has received increasing attention as being a lightweight method for interpreting the intrinsic mechanisms of large NLP models. In probing, post-hoc classifiers are trained on \"out-ofdomain\" datasets that diagnose specific abilities. While probing the language models has led to insightful findings, they appear disjointed from the development of models. This paper explores the utility of probing deep NLP models to extract a proxy signal widely used in model development -the fine-tuning performance. We find that it is possible to use the accuracies of only three probing tests to predict the fine-tuning performance with errors 40% -80% smaller than baselines. We further discuss possible avenues where probing can empower the development of deep NLP models.",
        "id":249121168
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend studies that concentrate on analyzing and constructing models for discourse organization in conversations involving multiple turns and parties, aimed at separating dialogues?",
    "positive_ctxs":[
      {
        "title":"Structural Characterization for Dialogue Disentanglement",
        "text":"Tangled multi-party dialogue contexts lead to challenges for dialogue reading comprehension, where multiple dialogue threads flow simultaneously within a common dialogue record, increasing difficulties in understanding the dialogue history for both human and machine. Previous studies mainly focus on utterance encoding methods with carefully designed features but pay inadequate attention to characteristic features of the structure of dialogues. We specially take structure factors into account and design a novel model for dialogue disentangling. Based on the fact that dialogues are constructed on successive participation and interactions between speakers, we model structural information of dialogues in two aspects: 1)speaker property that indicates whom a message is from, and 2) reference dependency that shows whom a message may refer to. The proposed method achieves new state-of-the-art on the Ubuntu IRC benchmark dataset and contributes to dialogue-related comprehension.",
        "id":247451284
      }
    ],
    "negative_ctxs":[
      {
        "title":"Exploring Diversity in Back Translation for Low-Resource Machine Translation",
        "text":"Back translation is one of the most widely used methods for improving the performance of neural machine translation systems. Recent research has sought to enhance the effectiveness of this method by increasing the 'diversity' of the generated translations. We argue that the definitions and metrics used to quantify 'diversity' in previous work have been insufficient. This work puts forward a more nuanced framework for understanding diversity in training data, splitting it into lexical diversity and syntactic diversity. We present novel metrics for measuring these different aspects of diversity and carry out empirical analysis into the effect of these types of diversity on final neural machine translation model performance for low-resource English↔Turkish and mid-resource English↔Icelandic. Our findings show that generating back translation using nucleus sampling results in higher final model performance, and that this method of generation has high levels of both lexical and syntactic diversity. We also find evidence that lexical diversity is more important than syntactic for back translation performance.",
        "id":249240504
      },
      {
        "title":"Enhancing Zero-shot and Few-shot Stance Detection with Commonsense Knowledge Graph",
        "text":"In this paper, we consider a realistic scenario on stance detection with more application potential, i.e., zero-shot and few-shot stance detection, which identifies stances for a wide range of topics with no or very few training examples. Conventional data-driven approaches are not applicable to the above zero-shot and few-shot scenarios. For human beings, commonsense knowledge is a crucial element of understanding and reasoning. In the absence of annotated data and cryptic expression of users' stance, we believe that introducing commonsense relational knowledge as support for reasoning can further improve the generalization and reasoning ability of the model in the zero-shot and few-shot scenarios. Specifically, we introduce a commonsense knowledge enhanced model to exploit both the structurallevel and semantic-level information of the relational knowledge. Extensive experiments demonstrate that our model outperforms the state-of-the-art methods on zero-shot and fewshot stance detection task.",
        "id":236478383
      },
      {
        "title":"CLIoS: Cross-lingual Induction of Speech Recognition Grammars",
        "text":"We present an approach for the cross-lingual induction of speech recognition grammars that separates the task of translation from the task of grammar generation. The source speech recognition grammar is used to generate phrases, which are translated by a common translation service. The target recognition grammar is induced by using the production rules of the source language, manually translated sentences and a statistical word alignment tool. The coverage of the resulting grammars (for Spanish and Japanese) is evaluated on two corpora and compared quantitatively and qualitatively to a grammar induced with unsupervised monolingual grammar induction.",
        "id":1661650
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend studies that investigate fine-tuning pre-trained language models using weakly supervised learning, especially those employing techniques like contrastive regularization or self-training?",
    "positive_ctxs":[
      {
        "title":"Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach",
        "text":"Fine-tuned pre-trained language models (LMs) have achieved enormous success in many natural language processing (NLP) tasks, but they still require excessive labeled data in the finetuning stage. We study the problem of finetuning pre-trained LMs using only weak supervision, without any labeled data. This problem is challenging because the high capacity of LMs makes them prone to overfitting the noisy labels generated by weak supervision.To address this problem, we develop a contrastive self-training framework, COSINE, to enable fine-tuning LMs with weak supervision. Underpinned by contrastive regularization and confidence-based reweighting, our framework gradually improves model fitting while effectively suppressing error propagation. Experiments on sequence, token, and sentence pair classification tasks show that our model outperforms the strongest baseline by large margins and achieves competitive performance with fully-supervised fine-tuning methods. Our implementation is available on https:\/\/ github.com\/yueyu1030\/COSINE.",
        "id":222377768
      }
    ],
    "negative_ctxs":[
      {
        "title":"Understanding Information Graphics: A Discourse-Level Problem",
        "text":"Keywords: graphics, understanding, discourse, plan-based modelsInformation graphics that appear in newspapers and magazines generally have a message that the viewer is intended to recognize. This paper argues that understanding such information graphics is a discourse-level problem. In particular, it requires assimilating information from multiple knowledge sources to recognize the intended message of the graphic, just as recognizing intention in text does. Moreover, when an article is composed of text and graphics, the intended message of the information graphic (its discourse intention) must be integrated into the discourse structure of the surrounding text and contributes to the overall discourse intention of the article. This paper describes how we extend plan-based techniques that have been used for understanding traditional discourse to the understanding of information graphics. This work is part of a project to develop an interactive natural language system that provides sight-impaired users with access to information graphics.",
        "id":17291778
      },
      {
        "title":"#Irony or #Sarcasm- A Quantitative and Qualitative Study Based on Twitter",
        "text":"Current study is with the aim to identify similarities and distinctions between irony and sarcasm by adopting quantitative sentiment analysis as well as qualitative content analysis. The result of quantitative sentiment analysis shows that sarcastic tweets are used with more positive tweets than ironic tweets. The result of content analysis corresponds to the result of quantitative sentiment analysis in identifying the aggressiveness of sarcasm. On the other hand, from content analysis it shows that irony owns two senses. The first sense of irony is equal to aggressive sarcasm with speaker awareness. Thus, tweets of first sense of irony may attack a specific target, and the speaker may tag his\/her tweet irony because the tweet itself is ironic. These tweets though tagged as irony are in fact sarcastic tweets. Different from this, the tweets of second sense of irony is tagged to classify an event to be ironic. However, from the distribution in sentiment analysis and examples in content analysis, irony seems to be more broadly used in its second sense.",
        "id":7967443
      },
      {
        "title":"A Novel Disambiguation Method For Unification-Based Grammars Using Probabilistic Context-Free Approximations",
        "text":"We present a novel disambiguation method for unification-based grammars (UBGs). In contrast to other methods, our approach obviates the need for probability models on the UBG side in that it shifts the responsibility to simpler context-free models, indirectly obtained from the UBG. Our approach has three advantages: (i) training can be effectively done in practice, (ii) parsing and disambiguation of context-free readings requires only cubic time, and (iii) involved probability distributions are mathematically clean. In an experiment for a mid-size UBG, we show that our novel approach is feasible. Using unsupervised training, we achieve 88% accuracy on an exact-match task.",
        "id":7470225
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you recommend studies that tackle the issue of popularity bias within news recommendation engines and offer techniques to distinguish between user interests and the popularity of news items?",
    "positive_ctxs":[
      {
        "title":"PP-Rec: News Recommendation with Personalized User Interest and Time-aware News Popularity",
        "text":"Personalized news recommendation methods are widely used in online news services. These methods usually recommend news based on the matching between news content and user interest inferred from historical behaviors. However, these methods usually have difficulties in making accurate recommendations to cold-start users, and tend to recommend similar news with those users have read. In general, popular news usually contain important information and can attract users with different interests. Besides, they are usually diverse in content and topic. Thus, in this paper we propose to incorporate news popularity information to alleviate the cold-start and diversity problems for personalized news recommendation. In our method, the ranking score for recommending a candidate news to a target user is the combination of a personalized matching score and a news popularity score. The former is used to capture the personalized user interest in news. The latter is used to measure timeaware popularity of candidate news, which is predicted based on news content, recency, and real-time CTR using a unified framework. Besides, we propose a popularity-aware user encoder to eliminate the popularity bias in user behaviors for accurate interest modeling. Experiments on two real-world datasets show our method can effectively improve the accuracy and diversity for news recommendation.",
        "id":235294032
      }
    ],
    "negative_ctxs":[
      {
        "title":"Knowledge Enhanced Fine-Tuning for Better Handling Unseen Entities in Dialogue Generation",
        "text":"Although pre-training models have achieved great success in dialogue generation, their performance drops dramatically when the input contains an entity that does not appear in pretraining and fine-tuning datasets (unseen entity). To address this issue, existing methods leverage an external knowledge base to generate appropriate responses. In real-world scenario, the entity may not be included by the knowledge base or suffer from the precision of knowledge retrieval. To deal with this problem, instead of introducing knowledge base as the input, we force the model to learn a better semantic representation by predicting the information in the knowledge base, only based on the input context. Specifically, with the help of a knowledge base, we introduce two auxiliary training objectives: 1) Interpret Masked Word, which conjectures the meaning of the masked entity given the context; 2) Hypernym Generation, which predicts the hypernym of the entity based on the context. Experiment results on two dialogue corpus verify the effectiveness of our methods under both knowledge available and unavailable settings.",
        "id":237490367
      },
      {
        "title":"Sharing Copies of Synthetic Clinical Corpora without Physical Distribution - A Case Study to Get Around IPRs and Privacy Constraints Featuring the German JSYNCC Corpus",
        "text":"The legal culture in the European Union imposes almost unsurmountable hurdles to exploit copyright protected language data (in terms of intellectual property rights (IPRs) of media contents) and privacy protected medical health data (in terms of the notion of informational self-determination) as language resources for the NLP community. These juridical constraints have seriously hampered progress in resource-greedy NLP research, in particular for non-English languages in the clinical domain. In order to get around these restrictions, we introduce a novel approach for the creation and re-use of clinical corpora which is based on a two-step workflow. First, we substitute authentic clinical documents by synthetic ones, i.e., made-up reports and case studies written by medical professionals for educational purposes and published in medical e-textbooks. We thus eliminate patients' privacy concerns since no real, concrete individuals are addressed in such narratives. In a second step, we replace physical corpus distribution by sharing software for trustful re-construction of corpus copies. This is achieved by an end-to-end tool suite which extracts well-specified text fragments from e-books and assembles, on demand, identical copies of the same text corpus we defined at our lab at any other site where this software is executed. Thus, we avoid IPR violations since no physical corpus (raw text data) is distributed. As an illustrative case study which is easily portable to other languages we present JSYNCC, the largest and, even more importantly, first publicly available, corpus of German clinical language.",
        "id":21701790
      },
      {
        "title":"Automatic Assessment of Conceptual Text Complexity Using Knowledge Graphs",
        "text":"Complexity of texts is usually assessed only at the lexical and syntactic levels. Although it is known that conceptual complexity plays a significant role in text understanding, no attempts have been made at assessing it automatically. We propose to automatically estimate the conceptual complexity of texts by exploiting a number of graph-based measures on a large knowledge base. By using a high-quality language learners corpus for English, we show that graph-based measures of individual text concepts, as well as the way they relate to each other in the knowledge graph, have a high discriminative power when distinguishing between two versions of the same text. Furthermore, when used as features in a binary classification task aiming to choose the simpler of two versions of the same text, our measures achieve high performance even in a default setup.This work is licensed under a Creative Commons Attribution 4.0 International License.License details:",
        "id":52013038
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you recommend studies which explore how to optimally select demonstrations for few-shot in-context learning?",
    "positive_ctxs":[
      {
        "title":"What Makes Good In-Context Examples for GPT-3?",
        "text":"GPT-3 has attracted lots of attention due to its superior performance across a wide range of NLP tasks, especially with its in-context learning abilities. Despite its success, we found that the empirical results of GPT-3 depend heavily on the choice of in-context examples. In this work, we investigate whether there are more effective strategies for judiciously selecting incontext examples (relative to random sampling) that better leverage GPT-3's in-context learning capabilities. Inspired by the recent success of leveraging a retrieval module to augment neural networks, we propose to retrieve examples that are semantically-similar to a test query sample to formulate its corresponding prompt. Intuitively, the examples selected with such a strategy may serve as more informative inputs to unleash GPT-3's power of text generation. We evaluate the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random selection baseline. Moreover, it is observed that the sentence encoders finetuned on task-related datasets yield even more helpful retrieval results. Notably, significant gains are observed on tasks such as table-totext generation (44.3% on the ToTTo dataset) and open-domain question answering (45.5% on the NQ dataset).",
        "id":231632658
      }
    ],
    "negative_ctxs":[
      {
        "title":"COLLECTION AND ANALYSIS OF DATA FROM REAL USERS: IMPLICATIONS FOR SPEECH RECOGNITION\/UNDERSTANDING SYSTEMS",
        "text":"Performance estimates given for speech recognition\/understanding systems are typically based on the assumption that users will behave in ways similar to the observed behavior of laboratory volunteers. This includes the acoustic\/phonetic characteristics of the spcech they produce as well as their willingness and ability to constrain their input to the device according to instructions. Since speech recognition devices often do not perform as well in the field as they do in the laboratory, analyses of real user behavior have been undertaken. The results of several field trials suggest that real user compliance with instructions is dramatically affected by the particular details of the prompts supplied to the user. A significant amount of real user speech data has been collected during these trials (34,000 uuerances, 29 hours of data). These speech databases are described along with the results of an experiment comparing the performance of a speech recognition system on real user vs. laboratory speech.",
        "id":2170463
      },
      {
        "title":"",
        "text":"",
        "id":219307696
      },
      {
        "title":"Overview of the Third Workshop on Scholarly Document Processing",
        "text":"With the ever-increasing pace of research and high volume of scholarly communication, scholars face a daunting task. Not only must they keep up with the growing literature in their own and related fields, scholars increasingly also need to rebut pseudo-science and disinformation. These needs have motivated an increasing focus on computational methods for enhancing search, summarization, and analysis of scholarly documents. However, the various strands of research on scholarly document processing remain fragmented. To reach out to the broader NLP and AI\/ML community, pool distributed efforts in this area, and enable shared access to published research, we held the 3 rd Workshop on Scholarly Document Processing (SDP) at COLING as a hybrid event (https:\/\/sdproc.org\/2022\/).",
        "id":252819487
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest a dataset containing diverse, intricate natural language queries that necessitate multi-step reasoning, comparing attributes, and performing set operations for answering questions from a knowledge base, without depending on entity linking?",
    "positive_ctxs":[
      {
        "title":"KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base",
        "text":"Complex question answering over knowledge base (Complex KBQA) is challenging because it requires various compositional reasoning capabilities, such as multi-hop inference, attribute comparison, set operation. Existing benchmarks have some shortcomings that limit the development of Complex KBQA: 1) they only provide QA pairs without explicit reasoning processes; 2) questions are poor in diversity or scale. To this end, we introduce KQA Pro, a dataset for Complex KBQA including 120K diverse natural language questions. We introduce a compositional and interpretable programming language KoPL to represent the reasoning process of complex questions. For each question, we provide the corresponding KoPL program and SPARQL query, so that KQA Pro serves for both KBQA and semantic parsing tasks. Experimental results show that SOTA KBQA methods cannot achieve promising results on KQA Pro as on current datasets, which suggests that KQA Pro is challenging and Complex KBQA requires further research efforts. We also treat KQA Pro as a diagnostic dataset for testing multiple reasoning skills, conduct a thorough evaluation of existing models and discuss further directions for Complex KBQA. Our codes and datasets can be obtained from https:\/\/github.com\/shijx12\/ KQAPro_Baselines.",
        "id":247362971
      }
    ],
    "negative_ctxs":[
      {
        "title":"Published as a conference paper at ICLR 2023 VOXURF: VOXEL-BASED EFFICIENT AND ACCURATE NEURAL SURFACE RECONSTRUCTION",
        "text":"Neural surface reconstruction aims to reconstruct accurate 3D surfaces based on multi-view images. Previous methods based on neural volume rendering mostly train a fully implicit model with MLPs, which typically require hours of training for a single scene. Recent efforts explore the explicit volumetric representation to accelerate the optimization via memorizing significant information with learnable voxel grids. However, existing voxel-based methods often struggle in reconstructing fine-grained geometry, even when combined with an SDF-based volume rendering scheme. We reveal that this is because 1) the voxel grids tend to break the color-geometry dependency that facilitates fine-geometry learning, and 2) the under-constrained voxel grids lack spatial coherence and are vulnerable to local minima. In this work, we present Voxurf, a voxel-based surface reconstruction approach that is both efficient and accurate. Voxurf addresses the aforementioned issues via several key designs, including 1) a two-stage training procedure that attains a coherent coarse shape and recovers fine details successively, 2) a dual color network that maintains color-geometry dependency, and 3) a hierarchical geometry feature to encourage information propagation across voxels. Extensive experiments show that Voxurf achieves high efficiency and high quality at the same time. On the DTU benchmark, Voxurf achieves higher reconstruction quality with a 20x training speedup compared to previous fully implicit methods.Corresponding authors. Our code is available at https:\/\/github.com\/wutong16\/Voxurf.",
        "id":251881568
      },
      {
        "title":"Improving Translation through Contextual Information",
        "text":"This paper proposes a two-layered model of dialogue structure for task-oriented dialogues that processes contextual information and disambiguates speech acts. The final goal is to improve translation quality in a speech-to-speech translation system.",
        "id":17409270
      },
      {
        "title":"",
        "text":"",
        "id":237099295
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest a dataset for question-answering frameworks utilizing temporal knowledge graphs with broad coverage?",
    "positive_ctxs":[
      {
        "title":"Question Answering Over Temporal Knowledge Graphs",
        "text":"Temporal Knowledge Graphs (Temporal KGs) extend regular Knowledge Graphs by providing temporal scopes (e.g., start and end times) on each edge in the KG. While Question Answering over KG (KGQA) has received some attention from the research community, QA over Temporal KGs (Temporal KGQA) is a relatively unexplored area. Lack of broadcoverage datasets has been another factor limiting progress in this area. We address this challenge by presenting CRONQUESTIONS, the largest known Temporal KGQA dataset, clearly stratified into buckets of structural complexity. CRONQUESTIONS expands the only known previous dataset by a factor of 340×. We find that various state-of-the-art KGQA methods fall far short of the desired performance on this new dataset. In response, we also propose CRONKGQA, a transformerbased solution that exploits recent advances in Temporal KG embeddings, and achieves performance superior to all baselines, with an increase of 120% in accuracy over the next best performing method. Through extensive experiments, we give detailed insights into the workings of CRONKGQA, as well as situations where significant further improvements appear possible. In addition to the dataset, we have released our code as well.",
        "id":235313508
      }
    ],
    "negative_ctxs":[
      {
        "title":"Using Syntactic and Semantic Context to Explore Psychodemographic Differences in Self-reference",
        "text":"Psychological analysis of language has repeatedly shown that an individual's rate of mentioning 1st person singular pronouns predicts a wealth of important demographic and psychological factors. However, these analyses are performed out of context -syntactic and semantic -which may change the magnitude or even direction of such relationships. In this paper, we put \"pronouns in their context\", exploring the relationship between self-reference and age, gender, and depression depending on syntactic position and verbal governor. We find that pronouns are overall more predictive when taking dependency relations and verb semantic categories into account, and, the direction of the relationship can change depending on the semantic class of the verbal governor.",
        "id":15154458
      },
      {
        "title":"PEPDS: A Polite and Empathetic Persuasive Dialogue System for Charity Donation",
        "text":"Persuasive conversations for a social cause often require influencing other person's attitude or intention that may fail even with compelling arguments. The use of emotions and different types of polite tones as needed with facts may enhance the persuasiveness of a message. To incorporate these two aspects, we propose a polite, empathetic persuasive dialogue system (PEPDS). First, in a Reinforcement Learning (RL) setting, a Maximum Likelihood Estimation loss based model is finetuned by designing an efficient reward function consisting of five different sub rewards viz. Persuasion, Emotion, Politeness-Strategy Consistency, Dialogue-Coherence and Nonrepetitiveness. Then, to generate empathetic utterances for non-empathetic ones, an Empathetic transfer model is built upon the RL finetuned model. Due to the unavailability of an appropriate dataset, by utilizing the PERSUASION-FORGOOD dataset, we create two datasets, viz. EPP4G and ETP4G. EPP4G is used to train three transformer-based classification models as per persuasiveness, emotion and politenessstrategy to achieve respective reward feedbacks. The ETP4G dataset is used to train an empathetic transfer model. Our experimental results demonstrate that PEPDS increases the rate of persuasive responses with emotion and politeness acknowledgement compared to the current state-of-the-art dialogue models, while also enhancing the dialogue's engagement and maintaining the linguistic quality 1 .",
        "id":252818930
      },
      {
        "title":"Fine-Grained Temporal Relation Extraction",
        "text":"We present a novel semantic framework for modeling temporal relations and event durations that maps pairs of events to realvalued scales for the purpose of constructing document-level event timelines. We use this framework to construct the largest temporal relations dataset to date, covering the entirety of the Universal Dependencies English Web Treebank. We use this dataset to train models for jointly predicting fine-grained temporal relations and event durations. We report strong results on our data and show the efficacy of a transfer-learning approach for predicting standard, categorical TimeML relations.",
        "id":59599681
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest a research article that explores generative methods for extracting information, specifically one that covers the generation of surface forms, labeling of entities, and classification of entity types?",
    "positive_ctxs":[
      {
        "title":"GenIE: Generative Information Extraction",
        "text":"Structured and grounded representation of text is typically formalized by closed information extraction, the problem of extracting an exhaustive set of (subject, relation, object) triplets that are consistent with a predefined set of entities and relations from a knowledge base schema. Most existing works are pipelines prone to error accumulation, and all approaches are only applicable to unrealistically small numbers of entities and relations. We introduce GenIE (generative information extraction), the first end-to-end autoregressive formulation of closed information extraction. GenIE naturally exploits the language knowledge from the pre-trained transformer by autoregressively generating relations and entities in textual form. Thanks to a new bi-level constrained generation strategy, only triplets consistent with the predefined knowledge base schema are produced. Our experiments show that GenIE is state-of-the-art on closed information extraction, generalizes from fewer training data points than baselines, and scales to a previously unmanageable number of entities and relations. With this work, closed information extraction becomes practical in realistic scenarios, providing new opportunities for downstream tasks. Finally, this work paves the way towards a unified end-to-end approach to the core tasks of information extraction.",
        "id":245144839
      }
    ],
    "negative_ctxs":[
      {
        "title":"Published as a conference paper at ICLR 2023 FLIP: A PROVABLE DEFENSE FRAMEWORK FOR BACKDOOR MITIGATION IN FEDERATED LEARNING",
        "text":"Federated Learning (FL) is a distributed learning paradigm that enables different parties to train a model together for high quality and strong privacy protection. In this scenario, individual participants may get compromised and perform backdoor attacks by poisoning the data (or gradients). Existing work on robust aggregation and certified FL robustness does not study how hardening benign clients can affect the global model (and the malicious clients). In this work, we theoretically analyze the connection among cross-entropy loss, attack success rate, and clean accuracy in this setting. Moreover, we propose a trigger reverse engineering based defense and show that our method can achieve robustness improvement with guarantee (i.e., reducing the attack success rate) without affecting benign accuracy. We conduct comprehensive experiments across different datasets and attack settings. Our results on nine competing SOTA defense methods show the empirical superiority of our method on both single-shot and continuous FL backdoor attacks. Code is available at https:\/\/github.com\/KaiyuanZh\/FLIP.",
        "id":253098063
      },
      {
        "title":"Why adversarial training can hurt robust accuracy",
        "text":"Machine learning classifiers with high test accuracy often perform poorly under adversarial attacks. It is commonly believed that adversarial training alleviates this issue. In this paper, we demonstrate that, surprisingly, the opposite may be true -Even though adversarial training helps when enough data is available, it may hurt robust generalization in the small sample size regime. We first prove this phenomenon for a high-dimensional linear classification setting with noiseless observations. Our proof provides explanatory insights that may also transfer to feature learning models. Further, we observe in experiments on standard image datasets that the same behavior occurs for perceptible attacks that effectively reduce class information such as mask attacks and object corruptions.",
        "id":247244739
      },
      {
        "title":"Discovering Latent Concepts and Exploiting Ontological Features for Semantic Text Search",
        "text":"Named entities and WordNet words are important in defining the content of a text in which they occur. Named entities have ontological features, namely, their aliases, classes, and identifiers. WordNet words also have ontological features, namely, their synonyms, hypernyms, hyponyms, and senses. Those features of concepts may be hidden from their textual appearance. Besides, there are related concepts that do not appear in a query, but can bring out the meaning of the query if they are added. The traditional constrained spreading activation algorithms use all relations of a node in the network that will add unsuitable information into the query. Meanwhile, we only use relations represented in the query. We propose an ontology-based generalized Vector Space Model to semantic text search. It discovers relevant latent concepts in a query by relation constrained spreading activation. Besides, to represent a word having more than one possible direct sense, it combines the most specific common hypernym of the remaining undisambiguated multi-senses with the form of the word. Experiments on a benchmark dataset in terms of the MAP measure for the retrieval performance show that our model is 41.9% and 29.3% better than the purely keyword-based model and the traditional constrained spreading activation model, respectively.",
        "id":3001852
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest a study examining how transformer models utilize feed-forward neural networks (FFNs) to encode factual information?",
    "positive_ctxs":[
      {
        "title":"Transformer Feed-Forward Layers Are Key-Value Memories",
        "text":"Feed-forward layers constitute two-thirds of a transformer model's parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformerbased language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys' input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model's layers via residual connections to produce the final output distribution.",
        "id":229923720
      }
    ],
    "negative_ctxs":[
      {
        "title":"A Multi-party Multi-modal Dataset for Focus of Visual Attention in Human-human and Human-robot Interaction",
        "text":"This papers describes a data collection setup and a newly recorded dataset. The main purpose of this dataset is to explore patterns in the focus of visual attention of humans under three different conditions -two humans involved in task-based interaction with a robot; same two humans involved in task-based interaction where the robot is replaced by a third human, and a free three-party human interaction. The dataset contains two parts -6 sessions with duration of approximately 3 hours and 9 sessions with duration of approximately 4.5 hours. Both parts of the dataset are rich in modalities and recorded data streams -they include the streams of three Kinect v2 devices (color, depth, infrared, body and face data), three high quality audio streams, three high resolution GoPro video streams, touch data for the task-based interactions and the system state of the robot. In addition, the second part of the dataset introduces the data streams from three Tobii Pro Glasses 2 eye trackers. The language of all interactions is English and all data streams are spatially and temporally aligned.",
        "id":39290532
      },
      {
        "title":"Two-Neighbor Orientation Model with Cross-Boundary Global Contexts",
        "text":"Long distance reordering remains one of the greatest challenges in statistical machine translation research as the key contextual information may well be beyond the confine of translation units. In this paper, we propose Two-Neighbor Orientation (TNO) model that jointly models the orientation decisions between anchors and two neighboring multi-unit chunks which may cross phrase or rule boundaries. We explicitly model the longest span of such chunks, referred to as Maximal Orientation Span, to serve as a global parameter that constrains underlying local decisions. We integrate our proposed model into a state-of-the-art string-to-dependency translation system and demonstrate the efficacy of our proposal in a large-scale Chinese-to-English translation task. On NIST MT08 set, our most advanced model brings around +2.0 BLEU and -1.0 TER improvement.",
        "id":5665391
      },
      {
        "title":"Anchor Text Extraction for Academic Search",
        "text":"*",
        "id":3176893
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you suggest a study that evaluates cross-encoder BERT rankers?",
    "positive_ctxs":[
      {
        "title":"Pretrained Transformers for Text Ranking: BERT and Beyond",
        "text":"The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query for a particular task. Although the most common formulation of text ranking is search, instances of the task can also be found in many text processing applications. This tutorial provides an overview of text ranking with neural network architectures known as transformers, of which BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019) is the best-known example. These models produce high quality results across many domains, tasks, and settings.This tutorial, which is based on the preprint (Lin et al., 2020a) of a forthcoming book to be published by Morgan and & Claypool under the Synthesis Lectures on Human Language Technologies series, provides an overview of existing work as a single point of entry for practitioners who wish to deploy transformers for text ranking in real-world applications and researchers who wish to pursue work in this area. We cover a wide range of techniques, grouped into two categories: transformer models that perform reranking in multi-stage ranking architectures and learned dense representations that perform ranking directly. . 2019. Cross-domain modeling of sentence-level evidence for document retrieval. In Amodei. 2020. Language models are few-shot learners. arXiv:2005.14165.",
        "id":222310837
      }
    ],
    "negative_ctxs":[
      {
        "title":"Lexical Chains as Document Features",
        "text":"Document clustering and classification is usually done by representing the documents using a bag of words scheme. This scheme ignores many of the linguistic and semantic features contained in text documents. We propose here an alternative representation for documents using Lexical Chains. We compare the performance of the new representation against the old one on a clustering task. We show that Lexical Chain based features give better results than the Bag of Words based features, while achieving almost 30% reduction in the dimensionality of the feature vectors resulting in faster execution of the algorithms.",
        "id":1894295
      },
      {
        "title":"Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet",
        "text":"Assigning a positive or negative score to a word out of context (i.e. a word's prior polarity) is a challenging task for sentiment analysis. In the literature, various approaches based on SentiWordNet have been proposed. In this paper, we compare the most often used techniques together with newly proposed ones and incorporate all of them in a learning framework to see whether blending them can further improve the estimation of prior polarity scores. Using two different versions of Sen-tiWordNet and testing regression and classification models across tasks and datasets, our learning approach consistently outperforms the single metrics, providing a new state-ofthe-art approach in computing words' prior polarity for sentiment analysis. We conclude our investigation showing interesting biases in calculated prior polarity scores when word Part of Speech and annotator gender are considered.",
        "id":2329174
      },
      {
        "title":"",
        "text":"",
        "id":232021615
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you suggest a study that examines how well contrastive learning performs in unimodal representation learning, specifically for sentence embeddings?",
    "positive_ctxs":[
      {
        "title":"SimCSE: Simple Contrastive Learning of Sentence Embeddings",
        "text":"This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using \"entailment\" pairs as positives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2% and 2.2% improvement compared to previous best results. We also show-both theoretically and empirically-that contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available. 1",
        "id":233296292
      }
    ],
    "negative_ctxs":[
      {
        "title":"Sketching the Dependency Relations of Words in Chinese",
        "text":"We proposes a language resource by automatically sketching grammatical relations of words based on dependency parses from untagged texts. The advantage of word sketch based on parsed corpora is, compared to Sketch Engine (Kilgarriff, Rychly,   Smrz, & Tugwell, 2004), to provide more details about the different usage of each word such as various types of modification, which is also important in language pedagogy. Although some language resources of other languages have attempted to sketch words based on parsed data, in Chinese we have not seen a resource for dependency sketch of words in customized texts. Therefore, we propose such a resource and evaluate with Chinese Sketch Engine (Huang et al., 2005)  in terms of corresponding thesaurus function.",
        "id":1563651
      },
      {
        "title":"Information and Computation (PACLIC 30) Seoul, Republic of Korea",
        "text":"Large amounts of biomedical corpora have emerged from different sources, including scientific literature, lab notes, patents and electronic health records. Most of the efforts in biomedical text mining have focused on the extraction and linkage of specific facts, such as molecular interactions, links between genes and diseases, or patients' symptoms. Such facts are rarely contextualised using the associated scientific or professional methodology (e.g. what methods were used to detect particular interaction, or to diagnose a particular disease). However, methods are the vital, but often neglected, under-pinning of science and practice. Given enough data, the ability to extract methodological knowledge would allow us to \"infer\" common (and possibly best) practice for a given task, and thus indeed learn from vast amount of text. This is obviously a complex task that involves identification, representation and linking of steps in associated methods, requiring a series of NLP methods such as temporal information extraction and discourse analysis. In this talk we will explore finding out what methods are being used to do what experiment from the literature, or to infer what clinical pathways patients have followed, based on the notes in their electronic health records. We will illustrate some of the work in the context of bioinformatics (e.g. recovering a general view of the methods described in the literature) and clinical practice (e.g. reconstruction of patient journeys). We will also discuss how feasible this task is given the known issues with the lack of reported details needed for understanding and reproducibility of associated methods (i.e. how much of a method is indeed present in the literate or clinical records).",
        "id":203965
      },
      {
        "title":"Pre-processing Closed Captions for Machine Translation",
        "text":"We describe an approach to Machine Translation of transcribed speech, as found in closed captions. We discuss how the colloquial nature and input format peculiarities of closed captions are dealt with in a pre-processing pipeline that prepares the input for effective processing by a core MT system. In particular, we describe components for proper name recognition and input segmentation. We evaluate the contribution of such modules to the system performance. The described methods have been implemented on an MT system for translating English closed captions to Spanish and Portuguese.",
        "id":6552619
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest a study that explores a cohesive pre-training method for code representation learning across different modalities?",
    "positive_ctxs":[
      {
        "title":"UniXcoder: Unified Cross-Modal Pre-training for Code Representation",
        "text":"Pre-trained models for programming languages have recently demonstrated great success on code intelligence. To support both code-related understanding and generation tasks, recent works attempt to pre-train unified encoder-decoder models. However, such encoder-decoder framework is sub-optimal for auto-regressive tasks, especially code completion that requires a decoder-only manner for efficient inference. In this paper, we present UniXcoder, a unified cross-modal pre-trained model for programming language. The model utilizes mask attention matrices with prefix adapters to control the behavior of the model and leverages cross-modal contents like AST and code comment to enhance code representation. To encode AST that is represented as a tree in parallel, we propose a one-to-one mapping method to transform AST in a sequence structure that retains all structural information from the tree. Furthermore, we propose to utilize multi-modal contents to learn representation of code fragment with contrastive learning, and then align representations among programming languages using a cross-modal generation task. We evaluate UniXcoder on five code-related tasks over nine datasets. To further evaluate the performance of code fragment representation, we also construct a dataset for a new task, called zero-shot code-to-code search. Results show that our model achieves state-of-the-art performance on most tasks and analysis reveals that comment and AST can both enhance UniXcoder.",
        "id":247315559
      }
    ],
    "negative_ctxs":[
      {
        "title":"Cross-discourse Development of Supervised Sentiment Analysis in the Clinical Domain",
        "text":"Current approaches to sentiment analysis assume that the sole discourse function of sentiment-bearing texts is expressivity. However, the persuasive discourse function also utilises expressive language. In this work, we present the results of training supervised classifiers on a new corpus of clinical texts that contain documents with an expressive discourse function, and we test the learned models on a subset of the same corpus containing persuasive texts. The results of this indicate that despite the difference in discourse function, the learned models perform favourably.",
        "id":2540417
      },
      {
        "title":"",
        "text":"",
        "id":14597336
      },
      {
        "title":"NEAR-OPTIMAL REPRESENTATION LEARNING FOR HIERARCHICAL REINFORCEMENT LEARNING",
        "text":"We study the problem of representation learning in goal-conditioned hierarchical reinforcement learning. In such hierarchical structures, a higher-level controller solves tasks by iteratively communicating goals which a lower-level policy is trained to reach. Accordingly, the choice of representation -the mapping of observation space to goal space -is crucial. To study this problem, we develop a notion of sub-optimality of a representation, defined in terms of expected reward of the optimal hierarchical policy using this representation. We derive expressions which bound the sub-optimality and show how these expressions can be translated to representation learning objectives which may be optimized in practice. Results on a number of difficult continuous-control tasks show that our approach to representation learning yields qualitatively better representations as well as quantitatively better hierarchical policies, compared to existing methods. 1",
        "id":52909341
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest a study that explores a compression method that merges product quantization with integer quantization for token-level retrieval?",
    "positive_ctxs":[
      {
        "title":"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction",
        "text":"Neural information retrieval (IR) has greatly advanced search and other knowledgeintensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6-10×.",
        "id":244799249
      }
    ],
    "negative_ctxs":[
      {
        "title":"Discovering Relations among Named Entities by Detecting Community Structure",
        "text":"This paper proposes a networked data mining method for relations discovery from large corpus. The key idea is representing the named entities pairs and their contexts as the network structure and detecting the communities from the network. Then each community relates to a relation the named entities pairs in the same community have the same relation. Finally, we labeled the relations. Our experiment using the corpus of People's Daily reveals not only that the relations among named entities could be detected with high precision, but also that appropriate labels could be automatically provided for the relations.",
        "id":14409919
      },
      {
        "title":"Prompting for explanations improves Adversarial NLI. Is this true? {Yes} it is {true} because {it weakens superficial cues}",
        "text":"Explanation prompts ask language models to not only assign a label to a given input, such as entailment or contradiction in natural language inference (NLI) tasks, but also to generate a free-text explanation that supports this label. While explanation prompts originally introduced aiming to improve model interpretability, here we show that they also improve robustness to superficial cues. Compared to prompting for labels only, explanation prompting shows stronger performance on adversarial NLI benchmarks, outperforming the state of the art on ANLI, Counterfactually-Augmented NLI, and SNLI-Hard datasets. Analysis suggests that the increase in robustness is due to a reduction in the association strength between single tokens and labels, i.e., explanation prompting weakens superficial cues. More specifically, we find that single tokens that are highly predictive of the correct answer in the label-only setting become uninformative when the model also has to generate explanations. . 2021. Multitask prompted training enables zero-shot task generalization.",
        "id":258378340
      },
      {
        "title":"Large-Scale Corpus-Driven PCFG Approximation of an HPSG",
        "text":"We present a novel corpus-driven approach towards grammar approximation for a linguistically deep Head-driven Phrase Structure Grammar. With an unlexicalized probabilistic context-free grammar obtained by Maximum Likelihood Estimate on a largescale automatically annotated corpus, we are able to achieve parsing accuracy higher than the original HPSG-based model. Different ways of enriching the annotations carried by the approximating PCFG are proposed and compared. Comparison to the state-of-the-art latent-variable PCFG shows that our approach is more suitable for the grammar approximation task where training data can be acquired automatically. The best approximating PCFG achieved ParsEval F 1 accuracy of 84.13%. The high robustness of the PCFG suggests it is a viable way of achieving full coverage parsing with the hand-written deep linguistic grammars.",
        "id":12190874
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you suggest a study that explores data annotation paradigms that help assuage concerns in lack of annotator expertise when using crowdsourcing?",
    "positive_ctxs":[
      {
        "title":"MULTIVERS: Improving scientific claim verification with weak supervision and full-document context",
        "text":"The scientific claim verification task requires an NLP system to label scientific documents which SUPPORT or REFUTE an input claim, and to select evidentiary sentences (or rationales) justifying each predicted label. In this work, we present MULTIVERS, which predicts a fact-checking label and identifies rationales in a multitask fashion based on a shared encoding of the claim and full document context. This approach accomplishes two key modeling goals. First, it ensures that all relevant contextual information is incorporated into each labeling decision. Second, it enables the model to learn from instances annotated with a document-level fact-checking label, but lacking sentence-level rationales. This allows MULTIVERS to perform weakly-supervised domain adaptation by training on scientific documents labeled using high-precision heuristics. Our approach outperforms two competitive baselines on three scientific claim verification datasets, with particularly strong performance in zero \/ few-shot domain adaptation experiments. Our code and data are available at https:\/\/github.com\/ dwadden\/multivers. . 2021. Abstract, Rationale, Stance: A Joint Model for Scientific Claim Verification. In EMNLP.",
        "id":245130931
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":229365710
      },
      {
        "title":"使用關聯法則為主之語言模型於擷取 長距離中文文字關聯性 Association Rule Based Language Models for Discovering Long Distance Dependency in Chinese",
        "text":"2) 在這邊 n-gram 模型 P(d|c)扮演的是可能性量測的角色，透過語言模型機率計算 可以獲得 P(d|c) 的值，而假設所有類別出現的機率是均等，只要能使 P(d|c)最 佳化的類別語言模型，即為此文件 d 為最有可能對應之類別。 2.2 n-gram 模型之建立 語言模型主要的功能是在評估一段文句出現的機率，假設有一文句 S 其長度 為 T 並且是由一段詞序列 W 1 W 2 W 3 …W T 所組成，則 S 出現的機率可以寫成",
        "id":33418460
      },
      {
        "title":"Détermination et pondération des raffinements d'un terme à partir de son arbre des usages nommés",
        "text":"Grâce à la participation d'un grand nombre de personnes via des jeux accessibles sur le web, nous avons construit un réseau lexical évolutif de grande taille pour le Français. A partir de cette ressource, nous avons abordé la question de la détermination des sens d'usage d'un terme, puis après avoir introduit la notion de similarité entre ces différents usages, nous avons pu obtenir pour un terme son arbre des usages : la racine regroupe tous les usages du terme et une descente dans l'arbre correspond à un raffinement de ces usages. Le nommage des différents noeuds est effectué lors d'une descente en largeur. En simplifiant l'arbre des usages nommés, nous déterminons les différents sens d'un terme, sens que nous introduisons dans le réseau lexical en tant que noeuds de raffinement du terme considéré. Nous terminons par une évaluation empirique des résultats obtenus.Abstract Thanks to the participation of a large number of persons via web-based games, a largesized evolutionary lexical network is available for French. With this resource, we approached the question of the determination of the word usages of a term, and after introducing the notion of similarity between these various word usages, we were able to build for a term its word usage tree: the root groups together all possible usages of this term and a search in the tree corresponds to a refinement of these word usages. The labelling of the various nodes is made during a width-first search. From its labelled word usage tree, we obtain the different meanings of a term, which can be inserted in the lexical network as refinement nodes for this term. Lastly, we present an evaluation of the results we obtain.Mots-clés : réseau lexical, arbre des usages nommés d'un terme, pondération des sens d'un terme Keywords: lexical network, tree of labelled word usages for a term, weighting of the meanings",
        "id":232021942
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest a study that explores employing a beta distribution to sample span sizes within unsupervised learning frameworks for text representation?",
    "positive_ctxs":[
      {
        "title":"DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations",
        "text":"Sentence embeddings are an important component of many natural language processing (NLP) systems. Like word embeddings, sentence embeddings are typically learned on large text corpora and then transferred to various downstream tasks, such as clustering and retrieval. Unlike word embeddings, the highest performing solutions for learning sentence embeddings require labelled data, limiting their usefulness to languages and domains where labelled data is abundant. In this paper, we present DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations. Inspired by recent advances in deep metric learning (DML), we carefully design a self-supervised objective for learning universal sentence embeddings that does not require labelled training data. When used to extend the pretraining of transformer-based language models, our approach closes the performance gap between unsupervised and supervised pretraining for universal sentence encoders. Importantly, our experiments suggest that the quality of the learned embeddings scale with both the number of trainable parameters and the amount of unlabelled training data. Our code and pretrained models are publicly available and can be easily adapted to new domains or used to embed unseen text. 1",
        "id":219530980
      }
    ],
    "negative_ctxs":[
      {
        "title":"Towards Modeling Social and Content Dynamics in Discussion Forums",
        "text":"Extended AbstractRecent years have witnessed the transformation of the World Wide Web from an information-gathering and processing tool into an interactive communication medium in the form of online discussion forums, chat-rooms, blogs, and so on. There is strong evidence suggesting that social networks facilitate new ways to interact with information in such media. Understanding the mechanisms and the patterns of such interactions can be important for many applications. Currently, there is not much work that adequately models interaction between social networks and information content. From the perspective of social network analysis, most existing work is concerned with understanding static topological properties of social networks represented by such forums. For instance, Park and Maurer (2009) applied node clustering to identify consensus and consensus facilitators, while Kang et al.(2009)uses discussion thread co-participation relations to identify (static) groups in discussions. On discussion content analysis research side, there have been approaches for classifying messages with respect to dialogue roles(Carvalho and Cohen, 2005;Ravi and Kim, 2007), but they often ignore the role and the impact of underlying social interactions.Thus, the current static network and content analysis approaches provide limited support for• Capturing dynamics of social interactions: the sequence of communication or who is responding to whom is important in understanding the nature of interactions. • Relating social interactions to content analysis: the content can give hint on the nature of the interaction and vice versa (e.g., users with more social interactions are more likely to have common interests).To address the above issues, one needs to go beyond the static analysis approach, and develop dynamical models that will explicitly account for the interplay between the content of communication (topics) and the structure of communications (social networks). Such framework and corresponding algorithmic base will allow us to infer \"polarizing\" topics discussed in forums, identify evolving communities of interests, and examine the link between social and content dynamics.",
        "id":18663038
      },
      {
        "title":"Reducing the Size of the Representation for the uDOP-Estimate",
        "text":"The unsupervised Data Oriented Parsing (uDOP) approach has been repeatedly reported to achieve state of the art performance in experiments on parsing of different corpora. At the same time the approach is demanding both in computation time and memory. This paper describes an approach which decreases these demands. First the problem is translated into the generation of probabilistic bottom up tree automata (pBTA). Then it is explained how solving two standard problems for these automata results in a reduction in the size of the grammar. The reduction of the grammar size by using efficient algorithms for pBTAs is the main contribution of this paper. Experiments suggest that this leads to a reduction in grammar size by a factor of 2. This paper also suggests some extensions of the original uDOP algorithm that are made possible or aided by the use of tree automata.A Short Discussion of uDOPThe unsupervised Data Oriented Parsing (uDOP) approach (Bod",
        "id":16956934
      },
      {
        "title":"Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints",
        "text":"In most practical settings and theoretical analysis, one assumes that a model can be trained until convergence. However, the growing complexity of machine learning datasets and models may violate such assumptions. Moreover, current approaches for hyper-parameter tuning and neural architecture search tend to be limited by practical resource constraints. Therefore, we introduce a formal setting for studying training under the non-asymptotic, resourceconstrained regime, i.e. budgeted training. We analyze the following problem: \"given a dataset, algorithm, and resource budget, what is the best achievable performance?\" We focus on the number of optimization iterations as the representative resource. Under such a setting, we show that it is critical to adjust the learning rate schedule according to the given budget. Among budget-aware learning schedules, we find simple linear decay to be both robust and high-performing. We support our claim through extensive experiments with state-of-the-art models on ImageNet (image classification), Cityscapes (semantic segmentation), MS COCO (object detection and instance segmentation), and Kinetics (video classification). We also analyze our results and find that the key to a good schedule is budgeted convergence, a phenomenon whereby the gradient vanishes at the end of each allowed budget. We also revisit existing approaches for fast convergence, and show that budget-aware learning schedules readily outperform such approaches under (the practical but under-explored) budgeted setting.",
        "id":152282636
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you suggest a study that explores improved training methods for dense passage retrieval within open-domain question answering systems?",
    "positive_ctxs":[
      {
        "title":"RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering",
        "text":"In open-domain question answering, dense passage retrieval has become a new paradigm to retrieve relevant passages for finding answers. Typically, the dual-encoder architecture is adopted to learn dense representations of questions and passages for semantic matching. However, it is difficult to effectively train a dual-encoder due to the challenges including the discrepancy between training and inference, the existence of unlabeled positives and limited training data. To address these challenges, we propose an optimized training approach, called RocketQA, to improving dense passage retrieval. We make three major technical contributions in RocketQA, namely crossbatch negatives, denoised hard negatives and data augmentation. The experiment results show that RocketQA significantly outperforms previous state-of-the-art models on both MS-MARCO and Natural Questions. We also conduct extensive experiments to examine the effectiveness of the three strategies in RocketQA. Besides, we demonstrate that the performance of end-to-end QA can be improved based on our RocketQA retriever 1 .",
        "id":231815627
      }
    ],
    "negative_ctxs":[
      {
        "title":"Clustering Words with the MDL Principle",
        "text":"We address the probhml of automaticMly constructing a thesaurus by clustering words based on corpus data. We view this problem as that of estimating a joint distribution over the (:artesian product of a partition of a set of nouns and a partition of a set of verbs, and propose a learning a.lgorithm based on the Mininmm Description Length (MDL) Principle for such estimation. We empirically compared the performance of our method based on the MDL Principle against the Maximum Likelihood Estimator in word clustering, and found that the former outperforms the latter. ~¢Ve also evaluated the method by conducting pp-attachment disambiguation experiments using an automaticMly constructed thesaurus. Our experimental results indicate that such a thesaurus can be used to improve accuracy in disambiguation.",
        "id":148
      },
      {
        "title":"THE ROLE OF METAPHORS IN DESCRIPTIONS OF EMOTIONS",
        "text":"Why do we use metaphors? For nearly 2000 years, the most generally accepted answer was that people only use metaphors for rhetorical purposes. Metaphorical language was thought to be merely ornamental --the seasoning of language, exploited for effect by poets and politicians 1, as compared with the cold factual language of the scientist. This view, however, is now no longer accepted (see, for example,Gentner, 1982;Boyd, 1979). It is now assumed, at least by psychologists and linguists, that metaphors, and their close cousins, analogies, are important tools of cognition and communication, providing us with unfamiliar ways of conceptualizing familiar things, and familiar ways of conceptualizing unfamiliar things(Lakoff & Johnson, 1980;Ortony, 1979;Vosniadou & Ortony, in preparation). Yet, what is still assumed, rather than demonstrated, is that nonliteral uses of language are sometimes necessary for accomplishing such goals, rather than merely convenient or elegant ways of doing so. In this paper we present a sort of empirical existence proof that there are some things whose descriptions appear to invoke much more use of metaphorical language than others. This, while not establishing the necessity of metaphors, certainly is a first step.In theory, there are at least three communicative functions that metaphor might serve(Ortony 1975). First, they might allow one to express that which is difficult or impossible to express if one is restricted to literal uses of language. Evidence for this \"inexpressibility\" claim would constitute encouraging support for the necessity-of-metaphors view. A second possible function of metaphors is that they may constitute a particularly compact means of communication. Although conscious experience is continuous in form, the linguistic system we use to talk about it is comprised of discrete elements (lexical items). Unlike more literal forms of language, metaphor may enable us to convey a great deal of information in a succinct manner by obviating the need to isolate the predicates to be expressed into their corresponding lexical representations. Finally, metaphors may help capture the vividness of phenomenal experience. If metaphors convey chunks of information rather than discrete units, they can paint a richer and more detailed picture of our subjective experience than might be expressed by literal language. This we call the \"'vividness\" claim.In this paper we shall concentrate on the first and last of these possible functions. In order to do so, we need to examine a discourse domain for which a prima facie case can be made for supposing that literal language will often be inadequate and which lends itself to variations in vividness. There doubtless are many such domains. The one that we selected was that of internal states, in particular, emotional states. The literature on the linguistic expression of emotions suggests a relatively high incidence of figurative language use (e.g.,Davitz, 1969), providing pragmatic reasons for believing that the context of (linguistic) emotional expression may be a profitable one within which to study metaphor production. Emotional states seemed wellsuited for our purposes because they tend to have an elusive, transient quality that is difficult to describe using literal language, although, of course, they can usually be labeled using literal language. Thus, while it might be easy for a person to label an emotional state as, for example, \"fear,\" it is difficult to provide a literal description of the quality of some particular experience of fear. Furthermore, because emotions vary in intensity, one might expect differential levels of vividness. There seem to be two possible ways in which people might try to communicate the quality of an emotional state. First, a speaker might use literal language to describe the events that triggered the emotional state and hope that the hearer correctly infers how he or she felt. For example, a person might describe the details of being mugged, hoping that a listener would recognize the emotional experience as the type one would have if one were attacked by a mugger. In such a case, the literal description would not describe the quality Of lWinston Churchill once renmrked: \"How infinite is the debt owed to metaphors by politicians who want to speak strongly but are not sure what they are going to say\"!",
        "id":3903723
      },
      {
        "title":"The Classical Language Toolkit: An NLP Framework for Pre-Modern Languages",
        "text":"This paper announces version 1.0 of the Classical Language Toolkit (CLTK), an NLP framework for pre-modern languages. The vast majority of NLP, its algorithms and software, is created with assumptions particular to living languages, thus neglecting certain important characteristics of largely non-spoken historical languages. Further, scholars of pre-modern languages often have different goals than those of living-language researchers. To fill this void, the CLTK adapts ideas from several leading NLP frameworks to create a novel software architecture that satisfies the unique needs of premodern languages and their researchers. Its centerpiece is a modular processing pipeline that balances the competing demands of algorithmic diversity with pre-configured defaults. The CLTK currently provides pipelines, including models, for almost 20 languages.",
        "id":237732031
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you suggest a study that explores the idea of using prompts to fine-tune language models, potentially providing an alternative viewpoint to standard optimization and regularization methods?",
    "positive_ctxs":[
      {
        "title":"Making Pre-trained Language Models Better Few-shot Learners",
        "text":"The recent GPT-3 model(Brown et al., 2020)achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF-better few-shot fine-tuning of language models 1 -a suite of simple and complementary techniques for finetuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning. 2 * The first two authors contributed equally. 1 Alternatively, language models' best friends forever. 2 Our implementation is publicly available at https:\/\/ github.com\/princeton-nlp\/LM-BFF. Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Association for Computational Linguistics (ACL). Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Association for Computational Linguistics (ACL).",
        "id":229923710
      }
    ],
    "negative_ctxs":[
      {
        "title":"When does deep multi-task learning work for loosely related document classification tasks? When does deep multi-task learning work for loosely related document classification tasks?",
        "text":"This work aims to contribute to our understanding of when multi-task learning through parameter sharing in deep neural networks leads to improvements over single-task learning. We focus on the setting of learning from loosely related tasks, for which no theoretical guarantees exist. We therefore approach the question empirically, studying which properties of datasets and single-task learning characteristics correlate with improvements from multi-task learning. We are the first to study this in a text classification setting and across more than 500 different task pairs.",
        "id":53581688
      },
      {
        "title":"Event Schema Induction with a Probabilistic Entity-Driven Model",
        "text":"Event schema induction is the task of learning high-level representations of complex events (e.g., a bombing) and their entity roles (e.g., perpetrator and victim) from unlabeled text. Event schemas have important connections to early NLP research on frames and scripts, as well as modern applications like template extraction. Recent research suggests event schemas can be learned from raw text. Inspired by a pipelined learner based on named entity coreference, this paper presents the first generative model for schema induction that integrates coreference chains into learning. Our generative model is conceptually simpler than the pipelined approach and requires far less training data. It also provides an interesting contrast with a recent HMM-based model. We evaluate on a common dataset for template schema extraction. Our generative model matches the pipeline's performance, and outperforms the HMM by 7 F1 points (20%).",
        "id":6341459
      },
      {
        "title":"Temporal Ranking for Fresh Information Retrieval",
        "text":"In business, the retrieval of up-to-date, or fresh, information is very important. It is difficult for conventional search engines based on a centralized architecture to retrieve fresh information, because they take a long time to collect documents via Web robots. In contrast to a centralized architecture, a search engine based on a distributed architecture does not need to collect documents, because each site makes an index independently. As a result, distributed search engines can be used to retrieve fresh information. However, fast indexing alone is not enough to retrieve fresh information, as support for temporal information based retrieval is also required. In this paper, we describe temporal information retrieval in distributed search engines. In particular, we propose an implementation of temporal ranking.",
        "id":7209972
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you suggest a study that explores the use of multi-modal pre-training techniques to improve the comprehension of documents with a high visual content?",
    "positive_ctxs":[
      {
        "title":"LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding",
        "text":"Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned\/digital-born documents. We propose LayoutLMv2 architecture with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. Specifically, with a two-stream multi-modal Transformer encoder, LayoutLMv2 uses not only the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks, which make it better capture the cross-modality interaction in the pre-training stage. Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture so that the model can fully understand the relative positional relationship among different text blocks. Experiment results show that LayoutLMv2 outperforms LayoutLM by a large margin and achieves new state-ofthe-art results on a wide variety of downstream visually-rich document understanding tasks, including FUNSD (0.7895 → 0.8420), CORD (0.9493 → 0.9601), SROIE (0.9524 → 0.9781), Kleister-NDA (0.8340 → 0.8520), RVL-CDIP (0.9443 → 0.9564), and DocVQA (0.7295 → 0.8672). We made our model and code publicly available at https:\/\/aka.ms \/layoutlmv2.",
        "id":229923949
      }
    ],
    "negative_ctxs":[
      {
        "title":"Multiview Identifiers Enhanced Generative Retrieval",
        "text":"Instead of simply matching a query to preexisting passages, generative retrieval generates identifier strings of passages as the retrieval target. At a cost, the identifier must be distinctive enough to represent a passage. Current approaches use either a numeric ID or a text piece (such as a title or substrings) as the identifier. However, these identifiers cannot cover a passage's content well. As such, we are motivated to propose a new type of identifier, synthetic identifiers, that are generated based on the content of a passage and could integrate contextualized information that text pieces lack. Furthermore, we simultaneously consider multiview identifiers, including synthetic identifiers, titles, and substrings. These views of identifiers complement each other and facilitate the holistic ranking of passages from multiple perspectives. We conduct a series of experiments on three public datasets, and the results indicate that our proposed approach performs the best in generative retrieval, demonstrating its effectiveness and robustness. The code is released at https:\/\/github.com\/liyongqi67\/MINDER.",
        "id":258947148
      },
      {
        "title":"Subgradient Descent Learns Orthogonal Dictionaries",
        "text":"This paper concerns dictionary learning, i.e., sparse coding, a fundamental representation learning problem. We show that a subgradient descent algorithm, with random initialization, can provably recover orthogonal dictionaries on a natural nonsmooth, nonconvex 1 minimization formulation of the problem, under mild statistical assumptions on the data. This is in contrast to previous provable methods that require either expensive computation or delicate initialization schemes. Our analysis develops several tools for characterizing landscapes of nonsmooth functions, which might be of independent interest for provable training of deep networks with nonsmooth activations (e.g., ReLU), among numerous other applications. Preliminary experiments corroborate our analysis and show that our algorithm works well empirically in recovering orthogonal dictionaries.",
        "id":53022741
      },
      {
        "title":"Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints",
        "text":"Language is increasingly being used to define rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora.In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5% and 40.5% for multilabel classification and visual semantic role labeling, respectively.",
        "id":1389483
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest a study that investigates the incorporation of human intervention in generating adversarial examples to attack conversational agents, with the aim of improving classifier efficacy",
    "positive_ctxs":[
      {
        "title":"Recipes for building an open-domain chatbot",
        "text":"Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent persona. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.",
        "id":216562425
      }
    ],
    "negative_ctxs":[
      {
        "title":"Designing an Evaluation Framework for Spoken Term Detection and Spoken Document Retrieval at the NTCIR-9 SpokenDoc Task",
        "text":"We describe the evaluation framework for spoken document retrieval for the IR for the Spoken Documents Task, conducted in the ninth NTCIR Workshop. The two parts of this task were a spoken term detection (STD) subtask and an ad hoc spoken document retrieval subtask (SDR). Both subtasks target search terms, passages and documents included in academic and simulated lectures of the Corpus of Spontaneous Japanese. Seven teams participated in the STD subtask and five in the SDR subtask. The results obtained through the evaluation in the workshop are discussed.",
        "id":65373
      },
      {
        "title":"Concept Disambiguation for Improved Subject Access Using Multiple Knowledge Sources",
        "text":"We address the problem of mining text for relevant image metadata. Our work is situated in the art and architecture domain, where highly specialized technical vocabulary presents challenges for NLP techniques. To extract high quality metadata, the problem of word sense disambiguation must be addressed in order to avoid leading the searcher to the wrong image as a result of ambiguous -and thus faulty -metadata. In this paper, we present a disambiguation algorithm that attempts to select the correct sense of nouns in textual descriptions of art objects, with respect to a rich domain-specific thesaurus, the Art and Architecture Thesaurus (AAT). We performed a series of intrinsic evaluations using a data set of 600 subject terms extracted from an online National Gallery of Art (NGA) collection of images and text. Our results showed that the use of external knowledge sources shows an improvement over a baseline.",
        "id":15313711
      },
      {
        "title":"Synthesising Personality with Neural Speech Synthesis",
        "text":"Matching the personality of conversational001 agents to the personality of the user can signifi-002 cantly improve the user experience, with many 003 successful examples in text-based chatbots.It 004 is also important for a voice-based system to 005 be able to alter the personality of the speech 006 as perceived by the users.In this pilot study, 007 fifteen voices were rated using Big Five per-008 sonality traits.Five content-neutral sentences 009 were chosen for the listening tests.The audio 010 data, together with two rated traits (Extrover-011 sion and Agreeableness), were used to train a 012 neural speech synthesiser based on one male 013 and one female voices.The effect of altering 014 the personality trait features was evaluated by a 015 second listening test.Both perceived extrover-016 sion and agreeableness in the synthetic voices 017 were affected significantly.The controllable 018 range was limited due to a lack of variance in 019 the source audio data.The perceived person-020 ality traits correlated with each other and with 021 the naturalness of the speech.022 1 Introduction 023 The law of attraction in human-robot interaction 024 means users prefer social robots with similar per-025 sonality traits to themselves (Park et al., 2012).026 Previous work has shown that it is possible to de-027 sign a text-based chatbot with a pre-defined per-028 sonality (Ahmad et al., 2020; Ruane et al., 2021), 029 and matching the personality of the agent to the 030 personality of the user can significantly improve 031 the user experience (Smestad and Volden, 2019; 032 * This author is currently affiliated with Cambridge University Press & Assessment.Research was conducted while studying at The University of Edinburgh.† This author is currently affiliated with Sanas.ai.Research was conducted while working at CereProc Ltd. text, which can be matching or mismatching, and 2) 323 adaptive personality based on the personality of the 324 user, as such adaptation is shown possible in text-325 based chatbots (Fernau et al., 2022).A multi-turn 326 conversational set-up can be used to experiment 327 the consistency of synthetised personality.The per-328 ception and impact of synthesised personality in 329 different cultural context can also be explored in 330 various user studies.331 5 Acknowledgements 332",
        "id":263609421
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest a study that proposes high-parameter efficeint fine-tuning techinque that only trains the bias terms?",
    "positive_ctxs":[
      {
        "title":"BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
        "text":"We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods. Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.",
        "id":231672601
      }
    ],
    "negative_ctxs":[
      {
        "title":"FUNCTIONAL STRUCTURES FOR PARSING DEPENDENCY CONSTRAINT5",
        "text":"",
        "id":9669733
      },
      {
        "title":"SemLink 2: Chasing Lexical Resources",
        "text":"The SemLink resource provides mappings between a variety of lexical semantic ontologies, each with their strengths and weaknesses. To take advantage of these differences, the ability to move between resources is essential. This work describes advances made to improve the usability of the SemLink resource: the automatic addition of new instances and mappings, manual corrections, sense-based vectors and collocation information, and architecture built to automatically update the resource when versions of the underlying resources change. These updates improve coverage, provide new tools to leverage the capabilities of these resources, and facilitate seamless updates, ensuring the consistency and applicability of these mappings in the future. 1",
        "id":243864612
      },
      {
        "title":"Phrase Structure Trees Bear More Fruit than You Would Have Thought 1",
        "text":"In this paper we will present several results concerning phrase structure trees. These results show that phrase structure trees, when viewed in certain ways, have much more descriptive power than one would have thought. We have given a brief account of local constraints on structural descriptions and an intuitive proof of a theorem about local constraints. We have compared the local constraints approach to some aspects of Gazdar's framework and that of Peters and Ritchie and of Karttunen. We have also presented some results on skeletons (phrase structure trees without labels) which show that phrase structure trees, even when deprived of the labels, retain in a certain sense all the structural information. This result has implications for grammatical inference procedures.",
        "id":1552091
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest a thorough comparative analysis or review of the performance of different pretrained transformer architectures, such as BERT, in text ranking applications?",
    "positive_ctxs":[
      {
        "title":"Pretrained Transformers for Text Ranking: BERT and Beyond",
        "text":"The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query for a particular task. Although the most common formulation of text ranking is search, instances of the task can also be found in many text processing applications. This tutorial provides an overview of text ranking with neural network architectures known as transformers, of which BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019) is the best-known example. These models produce high quality results across many domains, tasks, and settings.This tutorial, which is based on the preprint (Lin et al., 2020a) of a forthcoming book to be published by Morgan and & Claypool under the Synthesis Lectures on Human Language Technologies series, provides an overview of existing work as a single point of entry for practitioners who wish to deploy transformers for text ranking in real-world applications and researchers who wish to pursue work in this area. We cover a wide range of techniques, grouped into two categories: transformer models that perform reranking in multi-stage ranking architectures and learned dense representations that perform ranking directly. . 2019. Cross-domain modeling of sentence-level evidence for document retrieval. In Amodei. 2020. Language models are few-shot learners. arXiv:2005.14165.",
        "id":222310837
      }
    ],
    "negative_ctxs":[
      {
        "title":"PLAN-BASED RELAXED REWARD SHAPING FOR GOAL-DIRECTED TASKS",
        "text":"In high-dimensional state spaces, the usefulness of Reinforcement Learning (RL) is limited by the problem of exploration. This issue has been addressed using potential-based reward shaping (PB-RS) previously. In the present work, we introduce Final-Volume-Preserving Reward Shaping (FV-RS). FV-RS relaxes the strict optimality guarantees of PB-RS to a guarantee of preserved long-term behavior. Being less restrictive, FV-RS allows for reward shaping functions that are even better suited for improving the sample efficiency of RL algorithms. In particular, we consider settings in which the agent has access to an approximate plan. Here, we use examples of simulated robotic manipulation tasks to demonstrate that plan-based FV-RS can indeed significantly improve the sample efficiency of RL over plan-based PB-RS.INTRODUCTIONReinforcement Learning (RL) provides a general framework for autonomous agents to learn complex behavior, adapt to changing environments, and generalize to unseen tasks and environments with little human interference or engineering effort. However, RL in high-dimensional state spaces generally suffers from a difficult exploration problem, making learning prohibitively slow and sample-inefficient for many real-world tasks with sparse rewards.A possible strategy to increase the sample efficiency of RL algorithms is reward shaping (Mataric,  1994; Randløv & Alstrøm, 1998), in particular potential-based reward shaping (PB-RS) (Ng et al.,  1999). Reward shaping provides a dense reward signal to the RL agent, enabling it to converge faster to the optimal policy. In robotics tasks, approximate domain knowledge is often available and can be used by a planning algorithm to generate approximate plans. Here, the resulting plan can be provided to the RL agent using plan-based reward shaping (Grzes & Kudenko, 2008; Brys et al.,  2015). Thus, plan-based reward shaping offers a natural way to combine the efficiency of planning with the flexibility of RL. We analyze the use of plan-based reward shaping for RL. The key novelty is that we theoretically introduce Final-Volume-Preserving Reward Shaping (FV-RS), a superset of PB-RS. Intuitively speaking, FV-RS allows for shaping rewards that convey the information encoded in the shaping reward in a more direct way than PB-RS, since the value of following a policy is not only determined by the shaping reward at the end of the trajectory, but can also depend on all intermediate states.While FV-RS inevitably relaxes the optimality guarantees provided by PB-RS, we show in the experiments that FV-RS can significantly improve sample efficiency beyond PB-RS, e.g. allowing RL agents to learn simulated 10-dimensional continuous robotic manipulation tasks after ca. 300 rollout episodes. We argue that the strict notion of optimality in PB-RS is not necessary in many robotics applications, while on the other hand relaxing PB-RS to FV-RS facilitates speeding up the learning process. Using FV-RS could be a better trade-off between optimality and sample efficiency in many domains. The contributions of this work are:• We introduce FV-RS as a new class of reward shaping for RL methods in general.arXiv:2107.06661v1 [cs.AI] 14 Jul 2021Published as a conference paper at ICLR 2021• We propose to specifically use FV-RS for plan-based reward shaping.• We show that compared to no RS and plan-based PB-RS, plan-based FV-RS significantly increases the sample efficiency in several robotic manipulation tasks.",
        "id":235614336
      },
      {
        "title":"Understanding the Performance of Statistical MT Systems: A Linear Regression Framework",
        "text":"We present a framework for the analysis of Machine Translation performance. We use multivariate linear models to determine the impact of a wide range of features on translation performance. Our assumption is that variables that most contribute to predict translation performance are the key to understand the differences between good and bad translations. During training, we learn the regression parameters that better predict translation quality using a wide range of input features based on the translation model and the first-best translation hypotheses. We use a linear regression with regularization. Our results indicate that with regularized linear regression, we can achieve higher levels of correlation between our predicted values and the actual values of the quality metrics. Our analysis shows that the performance for in-domain data is largely dependent on the characteristics of the translation model. On the other hand, out-of domain data can benefit from better reordering strategies.",
        "id":14409460
      },
      {
        "title":"On the Role of Explicit Morphological Feature Representation in Syntactic Dependency Parsing for German",
        "text":"We investigate the question whether an explicit feature representation for morphological features is necessary when parsing German with a fully lexicalized, statistical dependency parser. We use two morphosyntactic phenomena of German to show that while lexicalization does indeed suffice to a large extent when recovering the internal structure of noun phrases, an accurate explicit representation can support the correct selection of its grammatical function.",
        "id":876880
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest a triplet-formatted structured dataset suitable for training table-to-text generation models?",
    "positive_ctxs":[
      {
        "title":"DART: Open-Domain Structured Data Record to Text Generation",
        "text":"We present DART, an open domain structuredDAta-Record-to-Text generation dataset with over 82k instances (DARTs). Data-to-text annotations can be a costly process, especially when dealing with tables which are the major source of structured data and contain nontrivial structures. To this end, we propose a procedure of extracting semantic triples from tables that encodes their structures by exploiting the semantic dependencies among table headers and the table title. Our dataset construction framework effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https:\/\/github. com\/Yale-LILY\/dart. . 2020c. Few-shot nlg with pre-trained language model. In ACL.Dorottya Demszky, Kelvin Guu, and Percy Liang. 2018. Transforming question answering datasets into natural language inference datasets. arXiv preprint arXiv:1809.02922.",
        "id":220364230
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"We present an English lexical database which is fuller, more accurate and more consistent than any other. We believe this to be so because the project has been well-planned, with a 12-month intensive planning phase prior to the lexicography beginning; well-resourced, employing a team of fifteen highly experienced lexicographers for a thirty-month main phase; it has had access to the latest corpus and dictionary-editing technology; it has not been constrained to meet any goals other than an accurate description of the language; and it has been led by a team with singular experience in delivering high-quality and innovative resources. The lexicon will be complete in Summer 2010 and will be available for NLP groups, on terms designed to encourage its research use.",
        "id":17969549
      },
      {
        "title":"Modeling and Characterizing Social Media Topics Using the Gamma Distribution",
        "text":"We present a novel technique to identify emerging or important topics mentioned on social media. A sudden increase in related posts can indicate an occurrence of an external event. Assuming that the sequence of posts is a homogeneous Poisson process, this sudden change can be modeled using the Gamma distribution. Our Gamma curve fitter is used to return a set of emerging topics. We demonstrate our algorithm on Twitter data and evaluate empirically using the Reuters News Archive and manual inspection. Our experimental results show that our algorithm provides a good picture of the emerging topics discussed on Twitter.",
        "id":15657766
      },
      {
        "title":"Semantic Relations Established by Specialized Processes Expressed by Nouns and Verbs: Identification in a Corpus by means of Syntactico-semantic Annotation",
        "text":"This article presents the methodology and results of the analysis of terms referring to processes expressed by verbs or nouns in a corpus of specialized texts dealing with ceramics. Both noun and verb terms are explored in context in order to identify and represent the semantic roles held by their participants (arguments and circumstants), and therefore explore some of the relations established by these terms. We present a methodology for the identification of related terms that take part in the development of specialized processes and the annotation of the semantic roles expressed in these contexts. The analysis has allowed us to identify participants in the process, some of which were already present in our previous work, but also some new ones. This method is useful in the distinction of different meanings of the same verb. Contexts in which processes are expressed by verbs have proved to be very informative, even if they are less frequent in the corpus. This work is viewed as a first step in the implementation -in ontologies -of conceptual relations in which activities are involved.",
        "id":9090544
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest an article that leverages the spatial information available in documents for multi-modal LMs by using a spatially-aware attention mechanism?",
    "positive_ctxs":[
      {
        "title":"LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding",
        "text":"Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned\/digital-born documents. We propose LayoutLMv2 architecture with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. Specifically, with a two-stream multi-modal Transformer encoder, LayoutLMv2 uses not only the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks, which make it better capture the cross-modality interaction in the pre-training stage. Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture so that the model can fully understand the relative positional relationship among different text blocks. Experiment results show that LayoutLMv2 outperforms LayoutLM by a large margin and achieves new state-ofthe-art results on a wide variety of downstream visually-rich document understanding tasks, including FUNSD (0.7895 → 0.8420), CORD (0.9493 → 0.9601), SROIE (0.9524 → 0.9781), Kleister-NDA (0.8340 → 0.8520), RVL-CDIP (0.9443 → 0.9564), and DocVQA (0.7295 → 0.8672). We made our model and code publicly available at https:\/\/aka.ms \/layoutlmv2.",
        "id":229923949
      }
    ],
    "negative_ctxs":[
      {
        "title":"Integrating NLP Tools in a Distributed Environment: A Case Study Chaining a Tagger with a Dependency Parser",
        "text":"The present paper tackles the issue of PoS tag conversion within the framework of a distributed web service platform for the automatic creation of language resources. PoS tagging is now considered a \"solved problem\"; yet, because of the differences in the tagsets, interchange of the various PoS taggers available is still hampered. In this paper we describe the implementation of a PoS-tagged-corpus converter, which is needed for chaining together in a workflow the FreeLing PoS tagger for Italian and the DESR dependency parser, given that these two tools have been developed independently. The conversion problems experienced during the implementation, related to the properties of the different tagsets and of tagset conversion in general, are discussed together with the solutions adopted. Finally, the converter is evaluated by assessing the impact of conversion on the performance of the dependency parser by comparing with the outcome of the native pipeline. From this we learn that in most cases parsing errors are due to actual tagging errors, and not to conversion itself. Besides, information on accuracy loss is an important feature in a distributed environment of (NLP) services, where users need to decide which services best suit their needs.",
        "id":15300153
      },
      {
        "title":"Leveraging Web Based Evidence Gathering for Drug Information Identification from Tweets",
        "text":"In this paper, we have explored web-based evidence gathering and different linguistic features to automatically extract drug names from tweets and further classify such tweets into Adverse Drug Events or not. We have evaluated our proposed models with the dataset as released by the SMM4H workshop shared Task-1 and Task-3 respectively. Our evaluation results shows that the proposed model achieved good results, with Precision, Recall and F-scores of 78.5%, 88% and 82.9% respectively for Task1 and 33.2%, 54.7% and 41.3% for Task3.",
        "id":53622046
      },
      {
        "title":"Memory Consolidation for Contextual Spoken Language Understanding with Dialogue Logistic Inference",
        "text":"Dialogue contexts are proven helpful in the spoken language understanding (SLU) system and they are typically encoded with explicit memory representations. However, most of the previous models learn the context memory with only one objective to maximizing the SLU performance, leaving the context memory under-exploited. In this paper, we propose a new dialogue logistic inference (DLI) task to consolidate the context memory jointly with SLU in the multi-task framework. DLI is defined as sorting a shuffled dialogue session into its original logical order and shares the same memory encoder and retrieval mechanism as the SLU model. Our experimental results show that various popular contextual SLU models can benefit from our approach, and improvements are quite impressive, especially in slot filling.",
        "id":174799129
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest datasets that can benchmark LLM performance in achieving conversational continuity and recall over long multi-session conversations?",
    "positive_ctxs":[
      {
        "title":"Beyond Goldfish Memory * : Long-Term Open-Domain Conversation",
        "text":"Despite recent improvements in open-domain dialogue models, state-of-the-art models are trained and evaluated on short conversations with little context. In contrast, the long-term conversation setting has hardly been studied. In this work we collect and release a humanhuman dataset consisting of multiple chat sessions whereby the speaking partners learn about each other's interests and discuss the things they have learnt from past sessions. We show how existing models trained on existing datasets perform poorly in this long-term conversation setting in both automatic and human evaluations, and we study long-context models that can perform much better. In particular, we find retrieval-augmented methods and methods with an ability to summarize and recall previous conversations outperform the standard encoder-decoder architectures currently considered state-of-the-art. * We use this term colloquially, seeAgranoff et al. (1965)for evidence of goldfish long-term memory.",
        "id":236034497
      }
    ],
    "negative_ctxs":[
      {
        "title":"Clean Data for Training Statistical MT: The Case of MT Contamination",
        "text":"Users of Statistical Machine Translation (SMT) sometimes turn to the Web to obtain data to train their systems. One problem with this approach is the potential for \"MT contamination\": when large amounts of parallel data are collected automatically, there is a risk that a nonnegligible portion consists of machine-translated text. Theoretically, using this kind of data to train SMT systems is likely to reinforce the errors committed by other systems, or even by an earlier versions of the same system. In this paper, we study the effect of MT-contaminated training data on SMT quality, by performing controlled simulations under a wide range of conditions. Our experiments highlight situations in which MT contamination can be harmful, and assess the potential of decontamination techniques.",
        "id":11770707
      },
      {
        "title":"Compasses, Magnets, Water Microscopes Annotation and Analysis of Terminology in a Diachronic Corpus of Scientific Texts",
        "text":"The specialised lexicon belongs to the most prominent attributes of specialised writing: Terms function as semantically dense encodings of specialised concepts, which, in the absence of terms, would require lengthy explanations and descriptions. In this paper, we argue that terms are the result of diachronic processes on both the semantic and the morpho-syntactic level. Very little is known about these processes. We therefore present a corpus annotation project aiming at revealing how terms are coined and how they evolve to fit their function as semantically and morpho-syntactically dense encodings of specialised knowledge. The scope of this paper is two-fold: Firstly, we outline our methodology for annotating terminology in a diachronic corpus of scientific publications. Moreover, we provide a detailed analysis of our annotation results and suggest methods for improving the accuracy of annotations in a setting as difficult as ours. Secondly, we present results of a pilot study based on the annotated terms. The results suggest that terms in older texts are linguistically relatively simple units that are hard to distinguish from the lexicon of general language. We believe that this supports our hypothesis that terminology undergoes diachronic processes of densification and specialisation.",
        "id":32319993
      },
      {
        "title":"Social Proof: The Impact of Author Traits on Influence Detection",
        "text":"It has been claimed that people are more likely to be influenced by those who are similar to them than those who are not. In this paper, we test this hypothesis by measuring the impact of author traits on the detection of influence. The traits we explore are age, gender, religion, and political party. We create a single classifier to detect the author traits of each individual. We then use the personal traits predicted by this classifier to predict the influence of contributors in a Wikipedia Talk Page corpus. Our research shows that the influencer tends to have the same traits as the majority of people in the conversation. Furthermore, we show that this is more pronounced when considering the personal traits most relevant to the conversation. Our research thus provides evidence for the theory of social proof.27",
        "id":10698681
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you suggest papers that tackle conversational search by using retrieval and conversational response ranking?",
    "positive_ctxs":[
      {
        "title":"Fine-grained Post-training for Improving Retrieval-based Dialogue Systems",
        "text":"Retrieval-based dialogue systems display an outstanding performance when pre-trained language models are used, which includes bidirectional encoder representations from transformers (BERT). During the multi-turn response selection, BERT focuses on training the relationship between the context with multiple utterances and the response. However, this method of training is insufficient when considering the relations between each utterance in the context. This leads to a problem of not completely understanding the context flow that is required to select a response. To address this issue, we propose a new fine-grained post-training method that reflects the characteristics of the multi-turn dialogue. Specifically, the model learns the utterance level interactions by training every short context-response pair in a dialogue session. Furthermore, by using a new training objective, the utterance relevance classification, the model understands the semantic relevance and coherence between the dialogue utterances. Experimental results show that our model achieves new state-of-the-art with significant margins on three benchmark datasets. This suggests that the fine-grained post-training method is highly effective for the response selection task. 1",
        "id":235097662
      }
    ],
    "negative_ctxs":[
      {
        "title":"A Unified Lexical Processing Framework Based on the Margin Infused Relaxed Algorithm. A Case Study on the Romanian Language",
        "text":"General natural language processing and text-to-speech applications require certain (lexical level) processing steps in order to solve some frequent tasks such as lemmatization, syllabification, lexical stress prediction and phonetic transcription. These steps usually require knowledge of the word's lexical composition (derivative morphology, inflectional affixes, etc.). For known words all applications use lexicons, but there are always out-of-vocabulary (OOV) words that impede the performance of NLP and speech synthesis applications. In such cases, either rule based or data-driven techniques are used to automatically process these OOV words and generate the desired results. In this paper we describe how the above mentioned tasks can be achieved using a Perceptron with the Margin Infused Relaxed Algorithm (MIRA) and sequence labeling.",
        "id":18383661
      },
      {
        "title":"",
        "text":"",
        "id":227230352
      },
      {
        "title":"Integrating a Phrase-based SMT Model and a Bilingual Lexicon for Human in Semi-Automatic Acquisition of Technical Term Translation Lexicon",
        "text":"This paper presents an attempt at developing a technique of acquiring translation pairs of technical terms with sufficiently high precision from parallel patent documents. The approach taken in the proposed technique is based on integrating the phrase translation table of a state-of-the-art statistical phrasebased machine translation model, and compositional translation generation based on an existing bilingual lexicon for human use. Our evaluation results clearly show that the agreement between the two individual techniques definitely contribute to improving precision of translation candidates. We then apply the Support Vector Machines (SVMs) to the task of automatically validating translation candidates in the phrase translation table. Experimental evaluation results again show that the SVMs based approach to translation candidates validation can contribute to improving the precision of translation candidates in the phrase translation table.",
        "id":7184744
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research on detecting common errors like additions and omissions in machine translation?",
    "positive_ctxs":[
      {
        "title":"Detecting Over-and Undertranslations with Contrastive Conditioning",
        "text":"Omission and addition of content is a typical issue in neural machine translation. We propose a method for detecting such phenomena with off-the-shelf translation models. Using contrastive conditioning, we compare the likelihood of a full sequence under a translation model to the likelihood of its parts, given the corresponding source or target sequence. This allows to pinpoint superfluous words in the translation and untranslated words in the source even in the absence of a reference translation. The accuracy of our method is comparable to a supervised method that requires a custom quality estimation model.",
        "id":247223093
      }
    ],
    "negative_ctxs":[
      {
        "title":"INTEGRATING WORD BOUNDARY IDENTIFICATION WITH SENTENCE UNDERSTANDING",
        "text":"Chinese sentences are written with no special delimiters such as space to indicate word boundaries. Existing Chinese NLP systems therefore employ preprocessors to segment sentences into words. Contrary to the conventional wisdom of separating this issue from the task of sentence understanding, we propose an integrated model that performs word boundary identification in lockstep with sentence understanding. In this approach, there is no distinction between rules for word boundary identification and rules for sentence understanding. These two functions are combined. Word boundary ambiguities are detected, especially the fallacious ones, when they block the primary task of discovering the inter-relationships among the various constituents of a sentence, which essentially is the essence of the understanding process. In this approach, statistical information is also incorporated, providing the system a quick and fairly reliable starting ground to carry out the primary task of relationship-building.",
        "id":7993951
      },
      {
        "title":"Shapes of Emotions: Multimodal Emotion Recognition in Conversations via Emotion Shifts",
        "text":"Emotion Recognition in Conversations (ERC)is an important and active research area. Recent work has shown the benefits of using multiple modalities (e.g., text, audio, and video) for the ERC task. In a conversation, participants tend to maintain a particular emotional state unless some stimuli evokes a change. There is a continuous ebb and flow of emotions in a conversation. Inspired by this observation, we propose a multimodal ERC model and augment it with an emotion-shift component that improves performance. The proposed emotion-shift component is modular and can be added to any existing multimodal ERC model (with a few modifications). We experiment with different variants of the model, and results show that the inclusion of emotion shift signal helps the model to outperform existing models for ERC on MOSEI and IEMOCAP datasets.",
        "id":244896303
      },
      {
        "title":"NTT Statistical Machine Translation System for IWSLT 2008",
        "text":"The NTT Statistical Machine Translation System consists of two primary components: a statistical machine translation decoder and a reranker. The decoder generates kbest translation canditates using a hierarchical phrase-based translation based on synchronous context-free grammar. The decoder employs a linear feature combination among several real-valued scores on translation and language models. The reranker reorders the k-best translation candidates using Ranking SVMs with a large number of sparse features. This paper describes the two components and presents the results for the evaluation campaign of IWSLT 2008.",
        "id":5794284
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that assesses if language models use extended contextual information, with experiments on the book dataset from Project Gutenberg?",
    "positive_ctxs":[
      {
        "title":"Do Long-Range Language Models Actually Use Long-Range Context?",
        "text":"Language models are generally trained on short, truncated input sequences, which limits their ability to use discourse-level information present in long-range context to improve their predictions. Recent efforts to improve the efficiency of self-attention have led to a proliferation of long-range Transformer language models, which can process much longer sequences than models of the past. However, the ways in which such models take advantage of the longrange context remain unclear. In this paper, we perform a fine-grained analysis of two longrange Transformer language models (including the Routing Transformer, which achieves state-of-the-art perplexity on the PG-19 longsequence LM benchmark dataset) that accept input sequences of up to 8K tokens. Our results reveal that providing long-range context (i.e., beyond the previous 2K tokens) to these models only improves their predictions on a small set of tokens (e.g., those that can be copied from the distant context) and does not help at all for sentence-level prediction tasks. Finally, we discover that PG-19 contains a variety of different document types and domains, and that long-range context helps most for literary novels (as opposed to textbooks or magazines).",
        "id":237572264
      }
    ],
    "negative_ctxs":[
      {
        "title":"Indirectly Determined Comparison and Difference: The Case of Japanese",
        "text":"When making comparisons,",
        "id":234487216
      },
      {
        "title":"DECOMPOSITION OF JAPANESE SENTENCES INTO NORMAL FORMS BASED ON HUMAN LINGUISTIC PROCESS",
        "text":"A diversity and a flexibility of language expression forms are awkward problems for the machine processing of language, such as translation, indexing and question-answering. This paper presents a method of decomposing Japanese sentences appearing in the Patent Documents on \"Pulse network\", into normal forms.First, the linguistic information is analysed and classified based on the human linguistic process. Then, predicate functions, phrase functions and operators are introduced as the normal forms. Finally, the decomposing procedure and some experimental results are shown.492-",
        "id":13036650
      },
      {
        "title":"Conceptual text representation for multi-lingual generation and translation",
        "text":"This paper presents some ideas and preliminary results o f a project aim ed a t automatic generation and translation o f text horn conceptual (interlingual) representations. In the first part we give some arguments for treating translation and generation as closely coupled processes and motivate the need for conceptual text representations to aid these tasks. In the second part we describe an implemented method fw multi-lingual gene ration o f sentences.",
        "id":11198970
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that concentrates on pinpointing sentence components that carry hateful expressions, potentially aiding in sentence-level standardization for content moderation?",
    "positive_ctxs":[
      {
        "title":"SemEval-2021 Task 5: Toxic Spans Detection",
        "text":"The Toxic Spans Detection task of SemEval-2021 required participants to predict the spans of toxic posts that were responsible for the toxic label of the posts. The task could be addressed as supervised sequence labeling, using training data with gold toxic spans provided by the organisers. It could also be treated as rationale extraction, using classifiers trained on potentially larger external datasets of posts manually annotated as toxic or not, without toxic span annotations. For the supervised sequence labeling approach and evaluation purposes, posts previously labeled as toxic were crowd-annotated for toxic spans. Participants submitted their predicted spans for a held-out test set, and were scored using character-based F1. This overview summarises the work of the 36 teams that provided system descriptions.",
        "id":236460230
      }
    ],
    "negative_ctxs":[
      {
        "title":"The aNALoGuE Challenge: Non Aligned Language GEneration",
        "text":"We propose a shared task based on recent advances in learning to generate natural language from meaning representations using semantically unaligned data. The aNALoGuE challenge aims to evaluate and compare recent corpus-based methods with respect to their scalability to data size and target complexity, as well as to assess predictive quality of automatic evaluation metrics.",
        "id":40317724
      },
      {
        "title":"Boosting Radiology Report Generation by Infusing Comparison Prior",
        "text":"Recent transformer-based models have made significant strides in generating radiology reports from chest X-ray images. However, a prominent challenge remains: these models often lack prior knowledge, resulting in the generation of synthetic reports that mistakenly reference non-existent prior exams. This discrepancy can be attributed to a knowledge gap between radiologists and the generation models. While radiologists possess patientspecific prior information, the models solely receive X-ray images at a specific time point. To tackle this issue, we propose a novel approach that leverages a rule-based labeler to extract comparison prior information from radiology reports. This extracted comparison prior is then seamlessly integrated into stateof-the-art transformer-based models, enabling them to produce more realistic and comprehensive reports. Our method is evaluated on English report datasets, such as IU X-ray and MIMIC-CXR. The results demonstrate that our approach surpasses baseline models in terms of natural language generation metrics. Notably, our model generates reports that are free from false references to non-existent prior exams, setting it apart from previous models. By addressing this limitation, our approach represents a significant step towards bridging the gap between radiologists and generation models in the domain of medical report generation.",
        "id":258557717
      },
      {
        "title":"A SPEECH-FIRST MODEL FOR REPAIR DETECTION AND CORRECTION",
        "text":"Interpreting fully natural speech is an important goal for spoken language understanding systems. However, while corpus studies have shown that about 10% of spontaneous utterances contain self-corrections, or RE-PAIRS, little is known about the extent to which cues in the speech signal may facilitate repair processing. We identify several cues based on acoustic and prosodic analysis of repairs in a corpus of spontaneous speech, and propose methods for exploiting these cues to detect and correct repairs. We test our acoustic-prosodic cues with other lexical cues to repair identification and find that precision rates of 89-93% and recall of 78-83% can be achieved, depending upon the cues employed, from a prosodically labeled corpus.Recently,  and  have proposed a two-stage method for processing repairs. In the first stage, lexical pattern",
        "id":62267053
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that examines a system for multimodal annotation intended to assist people in analyzing dialogues within video content?",
    "positive_ctxs":[
      {
        "title":"MONAH: Multi-Modal Narratives for Humans to analyze conversations",
        "text":"In conversational analyses, humans manually weave multimodal information into the transcripts, which is significantly time-consuming. We introduce a system that automatically expands the verbatim transcripts of videorecorded conversations using multimodal data streams. This system uses a set of preprocessing rules to weave multimodal annotations into the verbatim transcripts and promote interpretability. Our feature engineering contributions are two-fold: firstly, we identify the range of multimodal features relevant to detect rapport-building; secondly, we expand the range of multimodal annotations and show that the expansion leads to statistically significant improvements in detecting rapport-building.",
        "id":231639161
      }
    ],
    "negative_ctxs":[
      {
        "title":"Preparing SNACS for Subjects and Objects",
        "text":"Research on adpositions and possessives in multiple languages has led to a small inventory of general-purpose meaning classes that disambiguate tokens. Importantly, that work has argued for a principled separation of the semantic role in a scene from the function coded by morphosyntax. Here, we ask whether this approach can be generalized beyond adpositions and possessives to cover all scene participants-including subjects and objectsdirectly, without reference to a frame lexicon. We present new guidelines for English and the results of an interannotator agreement study.",
        "id":201708265
      },
      {
        "title":"Cross-lingual NIL Entity Clustering for Low-resource Languages",
        "text":"Clustering unlinkable entity mentions across documents in multiple languages (crosslingual NIL Clustering) is an important task as part of Entity Discovery and Linking (EDL). This task has been largely neglected by the EDL community because it is challenging to outperform simple edit distance or other heuristics based baselines. We propose a novel approach based on encoding the orthographic similarity of the mentions using a Recurrent Neural Network (RNN) architecture. Our model adapts a training procedure from the one-shot facial recognition literature in order to achieve this. We also perform several exploratory probing tasks on our name encodings in order to determine what specific types of information are likely to be encoded by our model. Experiments show our approach provides up to a 6.6% absolute CEAFm F-Score improvement over state-of-the-art methods and successfully captures phonological relations across languages.",
        "id":199556521
      },
      {
        "title":"Learning Probabilistic Subcategorization Preference by Identifying Case Dependencies and Optimal Noun Class Generalization Level*",
        "text":"This paper proposes a novel method of learning probabilistic subcategorization preference. In the method, for the purpose of coping with the ambiguities of case dependencies and noun class generalization of argument\/adjunct nouns, we introduce a data structure which represents a tuple of independent partial subcategorization frames. Each collocation of a verb and argument\/adjunct nouns is assumed to be generated from one of the possible tuples of independent partial subcategorization frames. Parameters of subcategorization preference are then estimated so as to maximize the subcategorization preference function for each collocation of a verb and argument\/adjunct nouns in the training corpus. We also describe the results of the experiments on learning probabilistic subcategorization preference from the EDR Japanese bracketed corpus, as well as those on evaluating the performance of subcategorization preference.",
        "id":14308271
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that examines how coreference resolution affects dialogue summarization quality?",
    "positive_ctxs":[
      {
        "title":"Coreference-Aware Dialogue Summarization",
        "text":"Summarizing conversations via neural approaches has been gaining research traction lately, yet it is still challenging to obtain practical solutions. Examples of such challenges include unstructured information exchange in dialogues, informal interactions between speakers, and dynamic role changes of speakers as the dialogue evolves. Many of such challenges result in complex coreference links. Therefore, in this work, we investigate different approaches to explicitly incorporate coreference information in neural abstractive dialogue summarization models to tackle the aforementioned challenges. Experimental results show that the proposed approaches achieve state-of-the-art performance, implying it is useful to utilize coreference information in dialogue summarization. Evaluation results on factual correctness suggest such coreferenceaware models are better at tracing the information flow among interlocutors and associating accurate status\/actions with the corresponding interlocutors and person mentions.",
        "id":235446920
      }
    ],
    "negative_ctxs":[
      {
        "title":"An Unsupervised Speaker Clustering Technique based on SOM and I-vectors for Speech Recognition Systems",
        "text":"In this paper, we introduce an enhancement for speech recognition systems using an unsupervised speaker clustering technique. The proposed technique is mainly based on I-vectors and Self-Organizing Map Neural Network (SOM). The input to the proposed algorithm is a set of speech utterances. For each utterance, we extract 100-dimensional I-vector and then SOM is used to group the utterances to different speakers. In our experiments, we compared our technique with Normalized Cross Likelihood ratio Clustering (NCLR). Results show that the proposed technique reduces the speaker error rate in comparison with NCLR. Finally, we have experimented the effect of speaker clustering on Speaker Adaptive Training (SAT) in a speech recognition system implemented to test the performance of the proposed technique. It was noted that the proposed technique reduced the WER over clustering speakers with NCLR.",
        "id":16370881
      },
      {
        "title":"Understanding Abuse: A Typology of Abusive Language Detection Subtasks",
        "text":"As the body of research on abusive language detection and analysis grows, there is a need for critical consideration of the relationships between different subtasks that have been grouped under this label. Based on work on hate speech, cyberbullying, and online abuse we propose a typology that captures central similarities and differences between subtasks and we discuss its implications for data annotation and feature construction. We emphasize the practical actions that can be taken by researchers to best approach their abusive language detection subtask of interest.",
        "id":8821211
      },
      {
        "title":"DRSM: DE-RANDOMIZED SMOOTHING ON MALWARE CLASSIFIER PROVIDING CERTIFIED ROBUSTNESS",
        "text":"Machine Learning (ML) models have been utilized for malware detection for over two decades.Consequently, this ignited an ongoing arms race between malware authors and antivirus systems, compelling researchers to propose defenses for malware-detection models against evasion attacks.However, most if not all existing defenses against evasion attacks suffer from sizable performance degradation and\/or can defend against only specific attacks, which makes them less practical in real-world settings.In this work, we develop a certified defense, DRSM (De-Randomized Smoothed MalConv), by redesigning the de-randomized smoothing technique for the domain of malware detection.Specifically, we propose a window ablation scheme to provably limit the impact of adversarial bytes while maximally preserving local structures of the executables.After showing how DRSM is theoretically robust against attacks with contiguous adversarial bytes, we verify its performance and certified robustness experimentally, where we observe only marginal accuracy drops as the cost of robustness.To our knowledge, we are the first to offer certified robustness in the realm of static detection of malware executables.More surprisingly, through evaluating DRSM against 9 empirical attacks of different types, we observe that the proposed defense is empirically robust to some extent against a diverse set of attacks, some of which even fall out of the scope of its original threat model.In addition, we collected 15.5K recent benign raw executables from diverse sources, which will be made public as a dataset called PACE (Publicly Accessible Collection(s) of Executables) to alleviate the scarcity of publicly available benign datasets for studying malware detection and provide future research with more representative data of the time.",
        "id":257687205
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that examines how prompt tuning can be used for domain transfer?",
    "positive_ctxs":[
      {
        "title":"The Power of Scale for Parameter-Efficient Prompt Tuning",
        "text":"In this work, we explore \"prompt tuning,\" a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient \"prompt ensembling.\" We release code and model checkpoints to reproduce our experiments. 1ReferencesRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,  Danilo Giampiccolo, Bernardo Magnini, and Idan  Szpektor. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognising textual entailment, volume 6, pages 6-4. Venice. . 2019a. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
        "id":233296808
      }
    ],
    "negative_ctxs":[
      {
        "title":"Learning a Scanning Understanding for \"Real-world\" Library Categorization",
        "text":"This paper describes, compares, and evaluates three different approaches for learning a semantic classification of library titles: 1) syntactically condensed titles, 2) complete titles, and 3) titles without insignificant words are used for learning the classification in connectionist recurrent plausibility networks. In particular, we demonstrate in this paper that automatically derived feature representations and recurrent plausibility networks can scale up to several thousand library titles and reach almost perfect classification accuracy (>98%) compared to a real-world library classification.",
        "id":8925228
      },
      {
        "title":"Expectations of Word Sense in Parallel Corpora",
        "text":"Given a parallel corpus, if two distinct words in language A, a 1 and a 2 , are aligned to the same word b 1 in language B, then this might signal that b 1 is polysemous, or it might signal a 1 and a 2 are synonyms. Both assumptions with successful work have been put forward in the literature. We investigate these assumptions, along with other questions of word sense, by looking at sampled parallel sentences containing tokens of the same type in English, asking how often they mean the same thing when they are: 1. aligned to the same foreign type; and 2. aligned to different foreign types. Results for French-English and Chinese-English parallel corpora show similar behavior: Synonymy is only very weakly the more prevalent scenario, where both cases regularly occur.",
        "id":467937
      },
      {
        "title":"",
        "text":"",
        "id":220444939
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that examines how prompt tuning efficacy in language model training is affected by scaling?",
    "positive_ctxs":[
      {
        "title":"The Power of Scale for Parameter-Efficient Prompt Tuning",
        "text":"In this work, we explore \"prompt tuning,\" a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient \"prompt ensembling.\" We release code and model checkpoints to reproduce our experiments. 1ReferencesRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,  Danilo Giampiccolo, Bernardo Magnini, and Idan  Szpektor. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognising textual entailment, volume 6, pages 6-4. Venice. . 2019a. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
        "id":233296808
      }
    ],
    "negative_ctxs":[
      {
        "title":"Online Learning of Interpretable Word Embeddings",
        "text":"Word embeddings encode semantic meanings of words into low-dimension word vectors. In most word embeddings, one cannot interpret the meanings of specific dimensions of those word vectors. Nonnegative matrix factorization (NMF) has been proposed to learn interpretable word embeddings via non-negative constraints. However, NMF methods suffer from scale and memory issue because they have to maintain a global matrix for learning. To alleviate this challenge, we propose online learning of interpretable word embeddings from streaming text data. Experiments show that our model consistently outperforms the state-of-the-art word embedding methods in both representation ability and interpretability. The source code of this paper can be obtained from http: \/\/github.com\/skTim\/OIWE.",
        "id":10705753
      },
      {
        "title":"Briefly Noted Computers and Writing: State of the Art Computers and Writing: Issues and Im- plementations Mike Sharples (editor)",
        "text":"",
        "id":10137532
      },
      {
        "title":"Ò ÜÔ Ö Ñ ÒØ Ð ÓÑÔ Ö ×ÓÒ Ó Ø ÎÓØ È Ö ÔØÖÓÒ Ò ËÙÔÔÓÖØ Î ØÓÖ Å Ò × Ò Â Ô Ò × Ò ÐÝ× × Ì × × Å Ò Ù Ë ×× ÒÓ ÓÓ Â Ô Ò ÓÖÔÓÖ Ø ÓÒ ¹½¼¹½ ÊÓÔÔÓÒ ¸Å Ò ØÓ¹ Ù¸ÌÓ ÝÓ ½¼ ¹ ½ ¾ Â Ô Ò Ñ× ×× ÒÓ Ý ÓÓ¹ ÓÖÔº Ô ×ØÖ Ø Ï Ü Ñ Ò Ú Ö ÓÙ× ×Ô Ø× Ó Ø ÚÓØ Ô Ö ÔØÖÓÒ Ò ×ÙÔÔÓÖØ Ú ØÓÖ Ñ Ò × Ò Ð ×× ¢ Ø ÓÒ Ø × × Ò AEÄÈ Ö Ø Ö Ø Ò Ö Ò ¹ Ò Ø × ×º Ì × ×Ô Ø× Ò ÐÙ ØÖ Ò Ò Ø Ñ ¸ ÙÖ Ý Ò Ð ÖÒ Ò ÙÖÚ ×º Ï Ù× Â Ô Ò × Ô Ò Ò Ý Ô Ö× Ò × Ñ Ò Ø × ÓÖ ÜÔ Ö Ñ ÒØ×¸ Ò Â Ô Ò × ÛÓÖ × Ñ ÒØ Ø ÓÒ Ò ÙÒ× Ø×Ù´ × Ô Ö × Ò Â Ô Ò × µ ÙÒ Ò × ÙÜ Ð ÖÝ Ø × ×º ÁÒ ÓÙÖ ÜÔ Ö Ñ ÒØ× Û Ú Ó × ÖÚ Ø Ø Ø ÚÓØ Ô Ö ÔØÖÓÒ × ÓÑÔ Ö Ð ØÓ ËÎÅ Ò Ø ÖÑ× Ó ÙÖ Ý Ò ¸ Ò Ø ÓÒ¸ × ØÓ Ð ÖÒ Ò Ø Ñ Ò ÔÖ Ø ÓÒ ×Ô Ø ÚÓØ Ô Ö Ô¹ ØÖÓÒ × ÓÒ× Ö ÐÝ ØØ Ö Ø Ò ËÎÅº ½ ÁÒØÖÓ Ù Ø ÓÒ ËÙÔÔÓÖØ Ú ØÓÖ Ñ Ò ×´ËÎÅµ´Î ÔÒ ¸½ µ Ú Ò × ÓÛÒ ØÓ Ø Ú ÓÖ Ñ ÒÝ Ò ØÙÖ Ð Ð Ò Ù ÔÖÓ ×× Ò ´AEÄÈµ Ø × ×´ º º¸´ÃÙ Ó Ò Å Ø×ÙÑÓØÓ¸¾¼¼½ ÃÙ Ó Ò Å Ø×ÙÑÓØÓ¸¾¼¼¾µµº ÀÓÛ Ú Ö¸Ø Ö Ö ×Ø ÐÐ ×ÓÑ ÔÖ Ø Ð ¢ ÙÐØ × Û Ò Û ÔÔÐÝ ËÎÅØÓ AEÄÈØ × ×º Ì Û Ò ×× Ó ËÎÅ × Ø Ø Ø Ý Ö ÒÓØ ×Ý ØÓ ÑÔÐ Ñ ÒØ Ò Ø Ö Ð ÖÒ Ò ÔÖÓ ×× × ×ÐÓÛ¸ ×Ô ÐÐÝ Û Ø ÔÓÐÝÒÓÑ Ð ÖÒ Ð×ó Ö ÙÒ Ò Ë Ô Ö ¸½ µ ÔÖÓÔÓ× Ø ÚÓØ Ô Ö ÔØÖÓÒ¸Û × Ò ÑÔÖÓÚ Ú Ö× ÓÒ Ó È Ö¹ ÔØÖÓÒ´ÊÓ× Ò Ð ØØ¸½ µ¸ Ò Ø Ý Ú Ø ÓÖ Ø ¹ Ð Ò ÐÝ× × Ò Ú ÔÖÓÚ ÓÓ Ô Ö ÓÖÑ Ò ÓÖ Ø Ò ¹ÛÖ ØØ Ò Ø Ö Ó Ò Ø ÓÒº ÐØ ÓÙ ÓÐÐ Ò× Ò × ÓÐÐ Ù × Ù× Ø ÚÓØ Ô Ö ÔØÖÓÒ ÓÖ Ö Ò ¹ Ò Ò Ú Ö ÓÙ× AEÄÈ Ø × ×´ ÓÐÐ Ò×¸¾¼¼¾ ÓÐÐ Ò× Ò Ù Ý¸¾¼¼¾ ÓÐÐ Ò×¸¾¼¼¾ µ Ò Ó Ø Ò ÑÔÖ ×¹ × Ú Ö ×ÙÐØ×¸Ø Ù× × Ð ×× ¢ Ö × Ò ÒÓØ ×Ù ¢¹ ÒØÐÝ Ü Ñ Ò º ÁÒ Ô ÖØ ÙÐ Ö¸ Ø Û ÓÙÐ Ò ÒØ Ö¹ ×Ø Ò ÕÙ ×Ø ÓÒ Û Ø Ö ÓÖ ÒÓØ Ø ÚÓØ Ô Ö ÔØÖÓÒ × ÓÑÔ Ö Ð ØÓ ËÎÅ ÓÖ AEÄÈ Ø × × Ø Ø Ö ÓÖ¹ Ñ Ð Þ × Ð ×× ¢ Ø ÓÒ ÓÒ º ÁÒ Ø × Ô Ô Ö Û Ó Ù× ÓÒ ÓÑÔ Ö ×ÓÒ Ó ËÎÅ Ò Ø ÚÓØ Ô Ö ÔØÖÓÒ ØÓ ÒÚ ×Ø Ø Ø Ù× ÙÐÒ ×× Ó Ø ÚÓØ Ô Ö ÔØÖÓÒ Ò AEÄÈ Ø × ×º Ï Û ÓÙÐ Ð Ø Ó ÒÓÛ Ø ×ØÖ Ò Ø Ò Û Ò ×× Ó Ø ÚÓØ Ô Ö¹ ÔØÖÓÒº Ï ÓÓ× Ø Ö Ø × × ÓÖ Ø × ÔÙÖÔÓ× º Ì × Ø × × Ö Â Ô Ò × ÛÓÖ × Ñ ÒØ Ø ÓÒ¸ ÙÒ¹ × Ø×Ù´ × Ô Ö × Ò Â Ô Ò × µ ÙÒ Ò ¸ Ò ¹ Ô Ò Ò Ý Ô Ö× Ò º ÜÔ Ö Ñ ÒØ× Ò Ø Ø Ø ËÎÅ Ò Ø ÚÓØ Ô Ö¹ ÔØÖÓÒ Ö ÕÙ ÐÐÝ ÓÓ ÓÖ Ø Ø Ö Ø × × Ò Ø ÖÑ× Ó ÙÖ Ýº ÀÓÛ Ú Ö¸Ø ÚÓØ Ô Ö ÔØÖÓÒ × ×Ù¹ Ô Ö ÓÖ ØÓ ËÎÅ Ò Ø ÖÑ× Ó Ð ÖÒ Ò Ø Ñ ¸ÔÖ Ø ÓÒ Ø Ñ ¸ Ò Ñ ÑÓÖÝ ÓÓØÔÖ ÒØº ¾ Ì ÎÓØ È Ö ÔØÖÓÒ ÓÐÐÓÛ Ò ´ Ö ÙÒ Ò Ë Ô Ö ¸½ µ¸Û × ÓÛ Ø ØÖ Ò Ò Ò ÔÖ Ø ÓÒ Ð ÓÖ Ø Ñ Ó Ø ÚÓØ Ô Ö ÔØÖÓÒ Ò ÙÖ ½º Ì ÚÓØ Ô Ö ÔØÖÓÒ × Û ÐÐ × ËÎÅ Ò Ù× ÖÒ Ð ÙÒ Ø ÓÒº Ï × Ó Û Ò ÙÖ ¾ Ø Ð ÓÖ Ø Ñ Ó Ø ÚÓØ Ô Ö ÔØÖÓÒ Û Ø ÖÒ Ð ÙÒ Ø ÓÒº Ì × Ð ÓÖ Ø Ñ × Ñ× ØÓ Ö ¹ ÕÙ Ö Ç´ ¾ µ ÖÒ Ð Ð ÙÐ Ø ÓÒ×º ÀÓÛ Ú Ö¸Û Ò ÚÓ Ø Ñ Ý Ø Ò Ú ÒØ Ó Ø Ö ÙÖÖ Ò",
        "text":"",
        "id":5267403
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that examines how strangers exchange information during discussions, specifically concentrating on the kinds of information typically shared during first encounters?",
    "positive_ctxs":[
      {
        "title":"Beyond Goldfish Memory * : Long-Term Open-Domain Conversation",
        "text":"Despite recent improvements in open-domain dialogue models, state-of-the-art models are trained and evaluated on short conversations with little context. In contrast, the long-term conversation setting has hardly been studied. In this work we collect and release a humanhuman dataset consisting of multiple chat sessions whereby the speaking partners learn about each other's interests and discuss the things they have learnt from past sessions. We show how existing models trained on existing datasets perform poorly in this long-term conversation setting in both automatic and human evaluations, and we study long-context models that can perform much better. In particular, we find retrieval-augmented methods and methods with an ability to summarize and recall previous conversations outperform the standard encoder-decoder architectures currently considered state-of-the-art. * We use this term colloquially, seeAgranoff et al. (1965)for evidence of goldfish long-term memory.",
        "id":236034497
      }
    ],
    "negative_ctxs":[
      {
        "title":"Interactive Corpus Annotation",
        "text":"We present an easy-to-use graphical tool for syntactic corpus annotation. This tool, Annotate, interacts with a part-of-speech tagger and a parser running in the background. The parser incrementally suggests single phrases bottom-up based on cascaded Markov models. A human annotator confirms or rejects the parser's suggestions. This semi-automatic process facilitates a very rapid and efficient annotation.",
        "id":14465818
      },
      {
        "title":"",
        "text":"",
        "id":226283868
      },
      {
        "title":"",
        "text":"",
        "id":221353376
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that examines how the order of in-context examples influences the efficacy of in-context learning in language models?",
    "positive_ctxs":[
      {
        "title":"Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
        "text":"When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, finetuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are \"fantastic\" and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true fewshot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13% relative improvement for GPTfamily models across eleven different established text classification tasks.",
        "id":233296494
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":198159868
      },
      {
        "title":"",
        "text":"",
        "id":204824163
      },
      {
        "title":"Randomized Language Models via Perfect Hash Functions",
        "text":"We propose a succinct randomized language model which employs a perfect hash function to encode fingerprints of n-grams and their associated probabilities, backoff weights, or other parameters. The scheme can represent any standard n-gram model and is easily combined with existing model reduction techniques such as entropy-pruning. We demonstrate the space-savings of the scheme via machine translation experiments within a distributed language modeling framework.",
        "id":7942435
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that examines how well language models work with creole languages, particularly in relation to their effectiveness with Nigerian Pidgin, given its close linguistic relationship with English?",
    "positive_ctxs":[
      {
        "title":"On Language Models for Creoles",
        "text":"Creole languages such as Nigerian Pidgin English and Haitian Creole are under-resourced and largely ignored in the NLP literature. Creoles typically result from the fusion of a foreign language with multiple local languages, and what grammatical and lexical features are transferred to the creole is a complex process (Sessarego, 2020). While creoles are generally stable, the prominence of some features may be much stronger with certain demographics or in some linguistic situations(Winford, 1999;Patrick, 1999). This paper makes several contributions: We collect existing corpora and release models for Haitian Creole, Nigerian Pidgin English, and Singaporean Colloquial English. We evaluate these models on intrinsic and extrinsic tasks. Motivated by the above literature, we compare standard language models with distributionally robust ones and find that, somewhat surprisingly, the standard language models are superior to the distributionally robust ones. We investigate whether this is an effect of overparameterization or relative distributional stability, and find that the difference persists in the absence of over-parameterization, and that drift is limited, confirming the relative stability of creole languages.",
        "id":237490383
      }
    ],
    "negative_ctxs":[
      {
        "title":"NL Domain Explanations in Knowledge Based MAT",
        "text":"This paper discusses an innovative approach to knowledge based Machine Aided Translation (MAT) where the translator is supported by an user-friendly environment providing linguistic and domain knowledge explanations. Our project aims at integration of a Knowledge Base (KB) in a MAT system and studies the integration principles as well as the internal interface between language and knowledge. The paper presents some related work, rel~)rts the solutions applied in our project and tries to generaiize our evaluation of the selected MAT approach.",
        "id":774544
      },
      {
        "title":"An Open Web Platform for Rule-Based Speech-to-Sign Translation",
        "text":"We present an open web platform for developing, compiling, and running rulebased speech to sign language translation applications. Speech recognition is performed using the Nuance Recognizer 10.2 toolkit, and signed output, including both manual and non-manual components, is rendered using the JASigning avatar system. The platform is designed to make the component technologies readily accessible to sign language experts who are not necessarily computer scientists. Translation grammars are written in a version of Synchronous Context-Free Grammar adapted to the peculiarities of sign language. All processing is carried out on a remote server, with content uploaded and accessed through a web interface. Initial experiences show that simple translation grammars can be implemented on a time-scale of a few hours to a few days and produce signed output readily comprehensible to Deaf informants. Overall, the platform drastically lowers the barrier to entry for researchers interested in building applications that generate high-quality signed language.",
        "id":15635164
      },
      {
        "title":"A machine learning approach to the automatic evaluation of machine translation",
        "text":"We present a machine learning approach to evaluating the wellformedness of output of a machine translation system, using classifiers that learn to distinguish human reference translations from machine translations. This approach can be used to evaluate an MT system, tracking improvements over time; to aid in the kind of failure analysis that can help guide system development; and to select among alternative output strings. The method presented is fully automated and independent of source language, target language and domain.",
        "id":890333
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that examines how well prompt tuning using soft embeddings works for utilizing pretrained language models in downstream applications with minimal finetuning?",
    "positive_ctxs":[
      {
        "title":"The Power of Scale for Parameter-Efficient Prompt Tuning",
        "text":"In this work, we explore \"prompt tuning,\" a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient \"prompt ensembling.\" We release code and model checkpoints to reproduce our experiments. 1ReferencesRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,  Danilo Giampiccolo, Bernardo Magnini, and Idan  Szpektor. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognising textual entailment, volume 6, pages 6-4. Venice. . 2019a. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
        "id":233296808
      }
    ],
    "negative_ctxs":[
      {
        "title":"Paraphrase Acquisition from Image Captions",
        "text":"We propose to use image captions from the Web as a previously underutilized resource for paraphrases (i.e., texts with the same \"message\") and to create and analyze a corresponding dataset. When an image is reused on the Web, an original caption is often assigned. We hypothesize that different captions for the same image naturally form a set of mutual paraphrases. To demonstrate the suitability of this idea, we analyze captions in the English Wikipedia, where editors frequently relabel the same image for different articles. The paper introduces the underlying mining technology, the resulting Wikipedia-IPC dataset, and compares known paraphrase corpora with respect to their syntactic and semantic paraphrase similarity to our new resource. In this context, we introduce characteristic maps along the two similarity dimensions to identify the style of paraphrases coming from different sources. An annotation study demonstrates the high reliability of the algorithmically determined characteristic maps.",
        "id":256274522
      },
      {
        "title":"PurePos 2.0: a hybrid tool for morphological disambiguation",
        "text":"We present PurePos, an open-source HMM-based automatic morphological annotation tool. PurePos can perform tagging and lemmatization at the same time, it is very fast to train, with the possibility of easy integration of symbolic rulebased components into the annotation process that can be used to boost the accuracy of the tool. The hybrid approach implemented in PurePos is especially beneficial in the case of rich morphology, highly detailed annotation schemes and if a small amount of training data is available. Evaluation of the tool was on a Hungarian corpus revealed that its hybrid components significantly improve overall annotation accuracy.",
        "id":15597981
      },
      {
        "title":"",
        "text":"",
        "id":207988685
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that examines how well structured pruning techniques perform in developing both small and precise models in natural language processing?",
    "positive_ctxs":[
      {
        "title":"Structured Pruning Learns Compact and Accurate Models",
        "text":"The growing size of neural language models has led to increased attention in model compression. The two predominant approaches are pruning, which gradually removes weights from a pre-trained model, and distillation, which trains a smaller compact model to match a larger one. Pruning methods can significantly reduce the model size but hardly achieve large speedups as distillation. However, distillation methods require large amounts of unlabeled data and are expensive to train. In this work, we propose a task-specific structured pruning method CoFi 1 (Coarse-and Fine-grained Pruning), which delivers highly parallelizable subnetworks and matches the distillation methods in both accuracy and latency, without resorting to any unlabeled data. Our key insight is to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads and hidden units) modules, which controls the pruning decision of each parameter with masks of different granularity. We also devise a layerwise distillation strategy to transfer knowledge from unpruned to pruned models during optimization. Our experiments on GLUE and SQuAD datasets show that CoFi yields models with over 10× speedups with a small accuracy drop, showing its effectiveness and efficiency compared to previous pruning and distillation approaches. 2",
        "id":247922354
      }
    ],
    "negative_ctxs":[
      {
        "title":"A Framework to Generate High-Quality Datapoints for Multiple Novel Intent Detection",
        "text":"Systems like Voice-command based conversational agents are characterized by a predefined set of skills or intents to perform user specified tasks. In the course of time, newer intents may emerge requiring retraining. However, the newer intents may not be explicitly announced and need to be inferred dynamically. Thus, there are two important tasks at hand (a). identifying emerging new intents, (b). annotating data of the new intents so that the underlying classifier can be retrained efficiently. The tasks become specially challenging when a large number of new intents emerge simultaneously and there is a limited budget of manual annotation. In this paper, we propose MNID (Multiple Novel Intent Detection) which is a cluster based framework to detect multiple novel intents with budgeted human annotation cost. Empirical results on various benchmark datasets (of different sizes) demonstrate that MNID, by intelligently using the budget for annotation, outperforms the baseline methods in terms of accuracy and F1score.",
        "id":248513007
      },
      {
        "title":"Analysis of Language Change in Collaborative Instruction Following",
        "text":"We analyze language change over time in a collaborative, goal-oriented instructional task, where utility-maximizing participants form conventions and increase their expertise. Prior work studied such scenarios mostly in the context of reference games, and consistently found that language complexity is reduced along multiple dimensions, such as utterance length, as conventions are formed. In contrast, we find that, given the ability to increase instruction utility, instructors increase language complexity along these previously studied dimensions to better collaborate with increasingly skilled instruction followers.",
        "id":237453540
      },
      {
        "title":"DIFFERENTIALLY PRIVATE ADAPTIVE OPTIMIZATION WITH DELAYED PRECONDITIONERS",
        "text":"Privacy noise may negate the benefits of using adaptive optimizers in differentially private model training. Prior works typically address this issue by using auxiliary information (e.g., public data) to boost the effectiveness of adaptive optimization. In this work, we explore techniques to estimate and efficiently adapt to gradient geometry in private adaptive optimization without auxiliary data. Motivated by the observation that adaptive methods can tolerate stale preconditioners, we propose differentially private adaptive training with delayed preconditioners (DP 2 ), a simple method that constructs delayed but less noisy preconditioners to better realize the benefits of adaptivity. Theoretically, we provide convergence guarantees for our method for both convex and non-convex problems, and analyze trade-offs between delay and privacy noise reduction. Empirically, we explore DP 2 across several realworld datasets, demonstrating that it can improve convergence speed by as much as 4× relative to non-adaptive baselines and match the performance of state-of-the-art optimization methods that require auxiliary data.arXiv:2212.00309v2 [cs.LG] 7 Jun 2023John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization.",
        "id":254125312
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that examines the application of specialized architecture in pre-trained language models to enhance text-to-SQL tasks?",
    "positive_ctxs":[
      {
        "title":"Structure-Grounded Pretraining for Text-to-SQL",
        "text":"Learning to capture text-table alignment is essential for tasks like text-to-SQL. A model needs to correctly recognize natural language references to columns and values and to ground them in the given database schema. In this paper, we present a novel weakly supervised Structure-Grounded pretraining framework (STRUG) for text-to-SQL that can effectively learn to capture text-table alignment based on a parallel text-table corpus. We identify a set of novel pretraining tasks: column grounding, value grounding and columnvalue mapping, and leverage them to pretrain a text-table encoder. Additionally, to evaluate different methods under more realistic text-table alignment settings, we create a new evaluation set Spider-Realistic based on Spider dev set with explicit mentions of column names removed, and adopt eight existing textto-SQL datasets for cross-database evaluation. STRUG brings significant improvement over BERT LARGE in all settings. Compared with existing pretraining methods such as GRAPPA, STRUG achieves similar performance on Spider, and outperforms all baselines on more realistic sets. All the code and data used in this work is public available at https:\/\/aka.ms\/ strug.",
        "id":225066679
      }
    ],
    "negative_ctxs":[
      {
        "title":"Chinese Named Entity Recognition with a Multi-Phase Model",
        "text":"Chinese named entity recognition is one of the difficult and challenging tasks of NLP. In this paper, we present a Chinese named entity recognition system using a multi-phase model. First, we segment the text with a character-level CRF model. Then we apply three word-level CRF models to the labeling person names, location names and organization names in the segmentation results, respectively. Our systems participated in the NER tests on open and closed tracks of Microsoft Research (MSRA). The actual evaluation results show that our system performs well on both the open tracks and closed tracks.",
        "id":11514373
      },
      {
        "title":"Stochastically Evaluating the Validity of Partial Parse Trees in Incremental Parsing",
        "text":"This paper proposes a method for evaluating the validity of partial parse trees constructed in incremental parsing. Our method is based on stochastic incremental parsing, and it incrementally evaluates the validity for each partial parse tree on a wordby-word basis. In our method, incremental parser returns partial parse trees at the point where the validity for the partial parse tree becomes greater than a threshold. Our technique is effective for improving the accuracy of incremental parsing.1.[t] X , where t is a terminal symbol and X is a nonterminal symbol.",
        "id":15337818
      },
      {
        "title":"Incremental Parsing in Bounded Memory",
        "text":"This tutorial will describe the use of a factored probabilistic sequence model for parsing speech and text using a bounded store of three to four incomplete constituents over time, in line with recent estimates of human shortterm working memory capacity. This formulation uses a grammar transform to minimize memory usage during parsing. Incremental operations on incomplete constituents in this transformed representation then define an extended domain of locality similar to those defined in mildly context-sensitive grammar formalisms, which can similarly be used to process long-distance and crossed-and-nested dependencies.Incremental Parsing in Bounded Memory",
        "id":15386832
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that examines the challenges faced by neural networks in discerning causation from correlation?",
    "positive_ctxs":[
      {
        "title":"CausaLM: Causal Model Explanation Through Counterfactual Language Models under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics",
        "text":"Understanding predictions made by deep neural networks is notoriously difficult, but also crucial to their dissemination. As all machine learning-based methods, they are as good as their training data, and can also capture unwanted biases. While there are tools that can help understand whether such biases exist, they do not distinguish between correlation and causation, and might be ill-suited for text-based models and for reasoning about high-level language concepts. A key problem of estimating the causal effect of a concept of interest on a given model is that this estimation requires the generation of counterfactual examples, which is challenging with existing generation technology. To bridge that gap, we propose CausaLM, a framework for producing causal model explanations using counterfactual language representation models. Our approach is based on fine-tuning of deep contextualized embedding models with auxiliary adversarial tasks derived from the causal graph of the problem. Concretely, we show that by carefully choosing Submission Volume 47, Number 2 auxiliary adversarial pre-training tasks, language representation models such as BERT can effectively learn a counterfactual representation for a given concept of interest, and be used to estimate its true causal effect on model performance. A byproduct of our method is a language representation model that is unaffected by the tested concept, which can be useful in mitigating unwanted bias ingrained in the data. 1",
        "id":218901061
      }
    ],
    "negative_ctxs":[
      {
        "title":"Named Entity Recognition with Bidirectional LSTM-CNNs",
        "text":"Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word-and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons from public sources, we establish new states of the art with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information.",
        "id":6300165
      },
      {
        "title":"Planning for Intentions with Rhetorical Relations",
        "text":"A viable model of interactive discourse must explain how interlocutors share conversational control to construct the discourse, and recognize the contributions of others to it. The position that I take in this paper is from a narrower perspective, one in which there is a primary speaker who controls and directs the discourse, but that does so while trying to accommodate the informational needs of someone who primarily listens, and who believes what the speaker has to say. From this perspective, I will argue that speakers realize their intentions by conveying rhetorical relations. With the primary speaker's intent understood, I maintain that the primary listener's contribution prompts the speaker to express his intent with rhetorical relations that satisfy the listener's informational needs. Both the intentions of the primary speaker, and the primary listener's feedback are interpreted in the context of their shared beliefs about the domain, their shared beliefs about the discourse that has been constructed so far, and their common, language-based knowledge of possible extensions to it.Researchers have argued for several years that communicative intent not only exists, its recognition by a listener is essential in order for communication to occur [Aus62, Gri69, GS86]. More recently, Mann and Thompson have proposed that the juxtaposition of propositional content communicates one or more rhetorical relations [MT87]. Their theory, Rhetorical Structure Theory (RST), explains coherent text structure in terms of a set of rhetorical relations that exist between units of text. The model that I use for planning discourse about domain plans in interactive settings was developed under the assumption that intentions and rhetorical relations exist, and that the relations that a speaker chooses to express are appropriate to conveying her underlying intentions [Hal93]. These assumptions are consistent with views that have been expressed by others [Kib93, KK93, Lim93, Mai93, MP93,Tra93].The following dialogue illustrates a relationship between intentions and rhetorical relations that my model captures: Jack: Jill:I'm going shopping at Wegman's (1) These checks must go in the mail today.(2) There is a mailbox to the right as you go into Wegman's.(3)Jill's remarks convey her intention to have Jack mail some letters, however her request is [,ol,  explicit in either line (2) or (3). In addition to the assertional content of each sentence, when ,Jill follows line (2) with line (3), she conveys to Jack that a relation holds between these pieces of information. This relation is that the information in line (3) allows the situation in line (2) to be dealt with effectively. In terms of Mann and Thompson's rhetorical relations, the content of line(3)is enablement for line (2). It is Jill's conveyance of this relation that communicates her intention, thereby making her request. Furthermore, Jill believes that Jack knows how different rhetorical relations relate to discourse intentions. This assumption is as basic as her assumption that Jack will know the words that she uses.23",
        "id":7742606
      },
      {
        "title":"MIE: A Medical Information Extractor towards Medical Dialogues",
        "text":"Electronic Medical Records (EMRs) have become key components of modern medical care systems. Despite the merits of EMRs, many doctors suffer from writing them, which is time-consuming and tedious. We believe that automatically converting medical dialogues to EMRs can greatly reduce the burdens of doctors, and extracting information from medical dialogues is an essential step. To this end, we annotate online medical consultation dialogues in a window-sliding style, which is much easier than the sequential labeling annotation. We then propose a Medical Information Extractor (MIE) towards medical dialogues. MIE is able to extract mentioned symptoms, surgeries, tests, other information and their corresponding status. To tackle the particular challenges of the task, MIE uses a deep matching architecture, taking dialogue turn-interaction into account. The experimental results demonstrate MIE is a promising solution to extract medical information from doctor-patient dialogues. 1",
        "id":220047186
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that examines the difficulties in employing weakly labeled datasets for named entity recognition and offers techniques to mitigate the associated data noise?",
    "positive_ctxs":[
      {
        "title":"Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data",
        "text":"Weak supervision has shown promising results in many natural language processing tasks, such as Named Entity Recognition (NER). Existing work mainly focuses on learning deep NER models only with weak supervision, i.e., without any human annotation, and shows that by merely using weakly labeled data, one can achieve good performance, though still underperforms fully supervised NER with manually\/strongly labeled data. In this paper, we consider a more practical scenario, where we have both a small amount of strongly labeled data and a large amount of weakly labeled data. Unfortunately, we observe that weakly labeled data does not necessarily improve, or even deteriorate the model performance (due to the extensive noise in the weak labels) when we train deep NER models over a simple or weighted combination of the strongly labeled and weakly labeled data. To address this issue, we propose a new multi-stage computational framework -NEEDLE with three essential ingredients: (1) weak label completion, (2) noise-aware loss function, and (3) final finetuning over the strongly labeled data. Through experiments on E-commerce query NER and Biomedical NER, we demonstrate that NEE-DLE can effectively suppress the noise of the weak labels and outperforms existing methods. In particular, we achieve new SOTA F1-scores on 3 Biomedical NER datasets: BC5CDRchem 93.74, BC5CDR-disease 90.69, NCBIdisease 92.28.Gamal Crichton, Sampo Pyysalo, Billy Chiu, and AnnaKorhonen. 2017. A neural network multi-task learning approach to biomedical named entity recognition. BMC bioinformatics, 18(1):368.",
        "id":235446386
      }
    ],
    "negative_ctxs":[
      {
        "title":"Building Graph Representations of Deep Vector Embeddings",
        "text":"Patterns stored within pre-trained deep neural networks compose large and powerful descriptive languages that can be used for many different purposes. Typically, deep network representations are implemented within vector embedding spaces, which enables the use of traditional machine learning algorithms on top of them. In this short paper we propose the construction of a graph embedding space instead, introducing a methodology to transform the knowledge coded within a deep convolutional network into a topological space (i.e., a network). We outline how such graph can hold data instances, data features, relations between instances and features, and relations among features. Finally, we introduce some preliminary experiments to illustrate how the resultant graph embedding space can be exploited through graph analytics algorithms.",
        "id":5213431
      },
      {
        "title":"Multi 2 OIE: Multilingual Open Information Extraction Based on Multi-Head Attention with BERT",
        "text":"In this paper, we propose Multi 2 OIE, which performs open information extraction (open IE) by combining BERT (Devlin et al., 2019)   with multi-head attention blocks(Vaswani et al., 2017). Our model is a sequence-labeling system with an efficient and effective argument extraction method. We use a query, key, and value setting inspired by the Multimodal Transformer (Tsai et al., 2019) to replace the previously used bidirectional long short-term memory architecture with multihead attention. Multi 2 OIE outperforms existing sequence-labeling systems with high computational efficiency on two benchmark evaluation datasets, Re-OIE2016 and CaRB. Additionally, we apply the proposed method to multilingual open IE using multilingual BERT. Experimental results on new benchmark datasets introduced for two languages (Spanish and Portuguese) demonstrate that our model outperforms other multilingual systems without training data for the target languages.",
        "id":221761515
      },
      {
        "title":"Text Simplification as Tree Labeling",
        "text":"We present a new, structured approach to text simplification using conditional random fields over top-down traversals of dependency graphs that jointly predicts possible compressions and paraphrases. Our model reaches readability scores comparable to word-based compression approaches across a range of metrics and human judgements while maintaining more of the important information.",
        "id":16305240
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that examines the effects of starting language models with weights from pretrained nondiffusion models on the convergence behavior of diffusion losses?",
    "positive_ctxs":[
      {
        "title":"SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control",
        "text":"Despite the growing success of diffusion models in continuous-valued domains (e.g., images), similar efforts for discrete domains such as text have yet to match the performance of autoregressive language models. In this work, we present SSD-LM-a diffusion-based language model with two key design choices. First, SSD-LM is semi-autoregressive, iteratively generating blocks of text, allowing for flexible output length at decoding time while enabling local bidirectional context updates. Second, it is simplex-based, performing diffusion on the natural vocabulary space rather than a learned latent space, allowing us to incorporate classifier guidance and modular control using offthe-shelf classifiers without any adaptation. We evaluate SSD-LM on unconstrained text generation benchmarks, and show that it matches or outperforms strong autoregressive GPT-2 models across standard quality and diversity metrics, while vastly outperforming diffusionbased baselines. On controlled text generation, SSD-LM also outperforms competitive baselines, with an extra advantage in modularity. 1 J. Atchison and S.M. Shen. 1980. Logistic-normal distributions:Some properties and uses. Biometrika, 67(2):261-272.",
        "id":253237701
      }
    ],
    "negative_ctxs":[
      {
        "title":"Plug and Play Autoencoders for Conditional Text Generation",
        "text":"Text autoencoders are commonly used for conditional generation tasks such as style transfer. We propose methods which are plug and play, where any pretrained autoencoder can be used, and only require learning a mapping within the autoencoder's embedding space, training embedding-to-embedding (Emb2Emb). This reduces the need for labeled training data for the task and makes the training procedure more efficient. Crucial to the success of this method is a loss term for keeping the mapped embedding on the manifold of the autoencoder and a mapping which is trained to navigate the manifold by learning offset vectors. Evaluations on style transfer tasks both with and without sequence-to-sequence supervision show that our method performs better than or comparable to strong baselines while being up to four times faster.",
        "id":222178257
      },
      {
        "title":"The overview of the SST speech corpus of Japanese learner English and evaluation through the experiment on automatic detection of learners' errors",
        "text":"This paper introduces an overview of the speech corpus of Japanese learner English compiled by National Institute of Information and Communications Technology by showing its data collection procedure and annotation schemes including error tagging. We have collected 1,200 interviews for three years. One of the most unique features of this corpus is that it contains rich information on learners' errors. We have performed error tagging for learners' grammatical and lexical errors with originally-designed error tagset. We also evaluated the corpus through the experiment on automatic detection of learners' errors by using error tag information in the corpus. We did this by using a machine learning model, Maximum Entropy (ME) model. Since we had obtained the limited amount of error-tagged data, we needed to make some efforts to enlarge training data. We added the correct sentences and artificially-made errors to the training data, and found that it improved accuracy. We are planning to make this corpus publicly available in the spring of 2004, so that teachers and researchers in many fields can use the data for their own interests, such as second language acquisition research, syllabus and material design, or the development of computerized pedagogical tools, by combining it with NLP technology.",
        "id":9016696
      },
      {
        "title":"SWEAT: Scoring Polarization of Topics across Different Corpora",
        "text":"Understanding differences of viewpoints across corpora is a fundamental task for computational social sciences. In this paper, we propose the Sliced Word Embedding Association Test (SWEAT), a novel statistical measure to compute the relative polarization of a topical wordset across two distributional representations. To this end, SWEAT uses two additional wordsets, deemed to have opposite valence, to represent two different poles. We validate our approach and illustrate a case study to show the usefulness of the introduced measure.",
        "id":237513556
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that explores a pre-trained encoder-decoder architecture aimed at comprehending and generating code, potentially beneficial for enhancing automated code repair systems?",
    "positive_ctxs":[
      {
        "title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation",
        "text":"Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https: \/\/github.com\/salesforce\/CodeT5.",
        "id":237386541
      }
    ],
    "negative_ctxs":[
      {
        "title":"Story Assembly in the R 2 aft Dyslexia Fluency Tutor",
        "text":"To overcome their substantial barriers to fluent reading, students with dyslexia need to be enticed to read more, and to read texts with carefully controlled lexical content. We describe and show examples from a prototype of the new R 2 aft story assembly engine, which generates an interactive text that has A) variable plot and B) lexical content which is individualized by decoding pattern.",
        "id":7232815
      },
      {
        "title":"PREDICTING AND MANAGING SPOKEN DISFLUENCIES DURING HUMAN-COMPUTER INTERACTION*",
        "text":"This research characterizes the spontaneous spoken disfluencies typical of human-computer interaction, and presents a predictive model accounting for their occurrence. Data were collected during three empirical studies in which people spoke or wrote to a highly interactive simulated system. The studies involved within-subject factorial designs in which input modality and presentation format were varied. Spoken disfluency rates during human-computer interaction were documented to be substantially lower than rates typically observed during comparable human-human speech. Two separate factors, both associated with increased planning demands, were statistically related to increased speech disfluency rates: (1) length of utterance, and (2) lack of structure in the presentation format. Regression techniques revealed that a linear model based simply on utterance length accounts for over 77% of the variability in spoken disfluencies. Therefore, design techniques capable of channeling users' speech into briefer sentences potentially could eliminate most spoken disfluencies. In addition, the degree of structure in the presentation format was manipulated in a manner that successfully elimluated 60 to 70% of all disfluent speech. The long-term goal of this research is to provide empirical guidance for the design of robust spoken language technology.",
        "id":19037207
      },
      {
        "title":"Learning Long-term Visual Dynamics with Region Proposal Interaction Networks",
        "text":"Learning long-term dynamics models is the key to understanding physical common sense. Most existing approaches on learning dynamics from visual input sidestep long-term predictions by resorting to rapid re-planning with short-term models. This not only requires such models to be super accurate but also limits them only to tasks where an agent can continuously obtain feedback and take action at each step until completion. In this paper, we aim to leverage the ideas from success stories in visual recognition tasks to build object representations that can capture inter-object and object-environment interactions over a long range. To this end, we propose Region Proposal Interaction Networks (RPIN), which reason about each object's trajectory in a latent region-proposal feature space. Thanks to the simple yet effective object representation, our approach outperforms prior methods by a significant margin both in terms of prediction quality and their ability to plan for downstream tasks, and also generalize well to novel environments. Our code is available at https:\/\/github.com\/HaozhiQi\/RPIN. Preprint. Under review.",
        "id":220968856
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that explores employing independently sampled dropout masks to generate positive pairs in contrastive learning for sentence embeddings?",
    "positive_ctxs":[
      {
        "title":"SimCSE: Simple Contrastive Learning of Sentence Embeddings",
        "text":"This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using \"entailment\" pairs as positives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2% and 2.2% improvement compared to previous best results. We also show-both theoretically and empirically-that contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available. 1",
        "id":233296292
      }
    ],
    "negative_ctxs":[
      {
        "title":"The DCU Discourse Parser for Connective, Argument Identification and Explicit Sense Classification",
        "text":"This paper describes our submission to the CoNLL-2015 shared task on discourse parsing. We factor the pipeline into subcomponents which are then used to form the final sequential architecture. Focusing on achieving good performance when inferring explicit discourse relations, we apply maximum entropy and recurrent neural networks to different sub-tasks such as connective identification, argument extraction, and sense classification. The our final system achieves 16.51%, 12.73% and 11.15% overall F1 scores on the dev, WSJ and blind test sets, respectively.",
        "id":16489279
      },
      {
        "title":"RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms",
        "text":"Pre-trained language models (PTLMs) have achieved impressive performance on commonsense inference benchmarks, but their ability to employ commonsense to make robust inferences, which is crucial for effective communications with humans, is debated. In the pursuit of advancing fluid human-AI communication, we propose a new challenge, RICA: Robust Inference using Commonsense Axioms, that evaluates robust commonsense inference despite textual perturbations. To generate data for this challenge, we develop a systematic and scalable procedure using commonsense knowledge bases and probe PTLMs across two different evaluation settings. Extensive experiments on our generated probe sets with more than 10k statements show that PTLMs perform no better than random guessing on the zero-shot setting, are heavily impacted by statistical biases, and are not robust to perturbation attacks. We also find that fine-tuning on similar statements offer limited gains, as PTLMs still fail to generalize to unseen inferences. Our new large-scale benchmark exposes a significant gap between PTLMs and human-level language understanding and offers a new challenge for PTLMs to demonstrate commonsense. 1 . 2020. Evaluating commonsense in pretrained language models. AAAI.",
        "id":221865854
      },
      {
        "title":"SCIR-QA at SemEval-2017 Task 3: CNN Model Based on Similar and Dissimilar Information between Keywords for Question Similarity",
        "text":"We describe a method of calculating the similarity between questions in community QA. Questions in cQA are usually very long and there are a lot of useless information about calculating the similarity between questions. Therefore, we implement a CNN model based on similar and dissimilar information on questions keywords. We extract the keywords of questions, and then model the similar and dissimilar information between the keywords, and use the CNN model to calculate the similarity.",
        "id":26478127
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that explores generating synthetic labels for sentences within a vast text collection to pre-train models for few-shot learning applications?",
    "positive_ctxs":[
      {
        "title":"PPT: Pre-trained Prompt Tuning for Few-shot Learning",
        "text":"Prompts for pre-trained language models (PLMs) have shown remarkable performance by bridging the gap between pre-training tasks and various downstream tasks. Among these methods, prompt tuning, which freezes PLMs and only tunes soft prompts, provides an efficient and effective solution for adapting largescale PLMs to downstream tasks. However, prompt tuning is yet to be fully explored. In our pilot experiments, we find that prompt tuning performs comparably with conventional full-model tuning when downstream data are sufficient, whereas it is much worse under fewshot learning settings, which may hinder the application of prompt tuning. We attribute this low performance to the manner of initializing soft prompts. Therefore, in this work, we propose to pre-train prompts by adding soft prompts into the pre-training stage to obtain a better initialization. We name this Pretrained Prompt Tuning framework \"PPT\". To ensure the generalization of PPT, we formulate similar classification tasks into a unified task form and pre-train soft prompts for this unified task. Extensive experiments show that tuning pre-trained prompts for downstream tasks can reach or even outperform full-model fine-tuning under both full-data and few-shot settings. Our approach is effective and efficient for using large-scale PLMs in practice. The code is publicly available at https:\/\/ github.com\/thu-coai\/PPT.",
        "id":237452236
      }
    ],
    "negative_ctxs":[
      {
        "title":"Finding Relevant Concepts for Unknown Terms Using a Web-based Approach",
        "text":"Previous research on automatic thesaurus construction most focused on extracting relevant terms for each term of concern from a small-scale and domain-specific corpus. This study emphasizes on utilizing the Web as the rich and dynamic corpus source for term association estimation. In addition to extracting relevant terms, we are interested in finding concept-level information for each term of concern. For a single term, our idea is that to send it into Web search engines to retrieve its relevant documents and we propose a Greedy-EMbased document clustering algorithm to cluster them and determine an appropriate number of relevant concepts for the term. Then the keywords with the highest weighted log likelihood ratio in each cluster are treated as the label(s) of the associated concept cluster for the term of concern. With some initial experiments, the proposed approach has been shown its potential in finding relevant concepts for unknown terms.",
        "id":18028927
      },
      {
        "title":"Learning to Generate Word-and Phrase-Embeddings for Efficient Phrase-Based Neural Machine Translation",
        "text":"Neural machine translation (NMT) often fails in one-to-many translation, e.g., in the translation of multi-word expressions, compounds, and collocations. To improve the translation of phrases, phrase-based NMT systems have been proposed; these typically combine wordbased NMT with external phrase dictionaries or with phrase tables from phrase-based statistical MT systems. These solutions introduce a significant overhead of additional resources and computational costs. In this paper, we introduce a phrase-based NMT model built upon continuous-output NMT, in which the decoder generates embeddings of words or phrases. The model uses a fertility module, which guides the decoder to generate embeddings of sequences of varying lengths. We show that our model learns to translate phrases better, performing on par with state of the art phrase-based NMT. Since our model does not resort to softmax computation over a huge vocabulary of phrases, its training time is about 112x faster than the baseline.",
        "id":207914581
      },
      {
        "title":"Entity Contrastive Learning in a Large-Scale Virtual Assistant System",
        "text":"Conversational agents are typically made up of domain (DC) and intent classifiers (IC) that identify the general subject an utterance belongs to and the specific action a user wishes to achieve. In addition, named entity recognition (NER) performs per token labeling to identify specific entities of interest in a spoken utterance. We investigate improving joint IC and NER models using entity contrastive learning that attempts to cluster similar entities together in a learned representation space. We compare a full virtual assistant system trained using entity contrastive learning to a baseline system that does not use contrastive learning. We present both offline results, using retrospective test sets, as well as online results from an A\/B test that compared the two systems. In both the offline and online settings, entity contrastive training improved overall performance against baseline systems. Furthermore, we provide a detailed analysis of learned entity embeddings, including both qualitative analysis via dimensionalityreduced visualizations and quantitative analysis by computing alignment and uniformity metrics. We show that entity contrastive learning improves alignment metrics and produces wellformed embedding clusters in representation space.",
        "id":259370605
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that explores the drawbacks of dense retrieval systems especially with large-scale indices?",
    "positive_ctxs":[
      {
        "title":"The Curse of Dense Low-Dimensional Information Retrieval for Large Index Sizes",
        "text":"Information Retrieval using dense lowdimensional representations recently became popular and showed out-performance to traditional sparse-representations like BM25. However, no previous work investigated how dense representations perform with large index sizes. We show theoretically and empirically that the performance for dense representations decreases quicker than sparse representations for increasing index sizes. In extreme cases, this can even lead to a tipping point where at a certain index size sparse representations outperform dense representations. We show that this behavior is tightly connected to the number of dimensions of the representations: The lower the dimension, the higher the chance for false positives, i.e. returning irrelevant documents.",
        "id":229680000
      }
    ],
    "negative_ctxs":[
      {
        "title":"Published as a conference paper at ICLR 2021 DC3: A LEARNING METHOD FOR OPTIMIZATION WITH HARD CONSTRAINTS",
        "text":"Large optimization problems with hard constraints arise in many settings, yet classical solvers are often prohibitively slow, motivating the use of deep networks as cheap \"approximate solvers.\" Unfortunately, naive deep learning approaches typically cannot enforce the hard constraints of such problems, leading to infeasible solutions. In this work, we present Deep Constraint Completion and Correction (DC3), an algorithm to address this challenge. Specifically, this method enforces feasibility via a differentiable procedure, which implicitly completes partial solutions to satisfy equality constraints and unrolls gradient-based corrections to satisfy inequality constraints. We demonstrate the effectiveness of DC3 in both synthetic optimization tasks and the real-world setting of AC optimal power flow, where hard constraints encode the physics of the electrical grid. In both cases, DC3 achieves near-optimal objective values while preserving feasibility. * These authors contributed equally.Published as a conference paper at ICLR 2021 • AC optimal power flow. We show how the general DC3 framework can be used to optimize power flows on the electrical grid. This difficult non-convex optimization task must be solved at scale and is especially critical for renewable energy adoption. Our results greatly improve upon the performance of general-purpose deep learning methods on this task.RELATED WORKOur approach is situated within the broader literature on fast optimization methods, and draws inspiration from literature on implicit layers and on incorporating constraints into neural networks. We briefly describe each of these areas and their relationship to the present work.Fast optimization methods. Many classical optimization methods have been proposed to improve the practical efficiency of solving optimization problems. These include general techniques such as constraint and variable elimination (i.e., the removal of non-active constraints or redundant variables, respectively), as well as problem-specific techniques (e.g., KKT factorization techniques in the case of convex quadratic programs)(Nocedal & Wright, 2006). Our present work builds upon aspects of this literature, applying concepts from variable elimination to reduce the number of degrees of freedom associated with the optimization problems we wish to solve.In addition to the classical optimization literature, there has been a large body of literature in deep learning that has sought to approximate or speed up optimization models. As described in reviews on topics such as combinatorial optimization (Bengio et al., 2020) and optimal power flow (Hasan et al., 2020), ML methods to speed up optimization models have thus far taken two main approaches. The first class of approaches, akin to work on surrogate modeling(Koziel & Leifsson, 2013), has involved training machine learning models to map directly from optimization inputs to full solutions. However, such approaches have often struggled to produce solutions that are both feasible and (near-)optimal. The second class of approaches has instead focused on employing machine learning approaches alongside or in the loop of optimization models, e.g., to learn warm-start points (see, e.g., Baker (2019) and Dong et al.(2020)) or to enable constraint elimination techniques by predicting active constraints (see, e.g., Misra et al.(2018)). We view our work as part of the former set of approaches, but drawing important inspiration from the latter: that employing structural knowledge about the optimization model is paramount to achieving both feasibility and optimality.Constraints in neural networks. While deep learning is often thought of as wholly unconstrained, in reality, it is quite common to incorporate (simple) constraints within deep learning procedures. For instance, softmax layers encode simplex constraints, sigmoids instantiate upper and lower bounds, ReLUs encode projections onto the positive orthant, and convolutional layers enforce translational equivariance (an idea taken further in general group-equivariant networks (Cohen & Welling, 2016)). Recent work has also focused on embedding specialized kinds of constraints into neural networks, such as conservation of energy (see, e.g., Greydanus et al.(2019)and Beucler et al.(2019)), and homogeneous linear inequality constraints(Frerix et al., 2020). However, while these represent common \"special cases,\" there has to date been little work on building more general hard constraints into deep learning models.",
        "id":233394003
      },
      {
        "title":"Towards Detecting Harmful Agendas in News Articles Warning: This paper contains examples of online text containing strong views and offensive language",
        "text":"Manipulated news online is a growing problem which necessitates the use of automated systems to curtail its spread. We argue that while misinformation and disinformation detection have been studied, there has been a lack of investment in the important open challenge of detecting harmful agendas in news articles; identifying harmful agendas is critical to flag news campaigns with the greatest potential for real world harm. Moreover, due to real concerns around censorship, harmful agenda detectors must be interpretable to be effective. In this work, we propose this new task and release a dataset, NEWSAGENDAS, of annotated news articles for agenda identification. We show how interpretable systems can be effective on this task and demonstrate that they can perform comparably to black-box models.",
        "id":260063114
      },
      {
        "title":"Steps towards Semantically Annotated Language Resources",
        "text":"The use of textual resources such as text corpora, tree banks, large-scale lexica etc., has become a widely accepted commitment in the field of computational linguistics. However the scope of the annotations proposed has been unbalanced towards the 'surface' level. Only recently corpora with a deeper level of annotations have started to emerge. In this paper we describe a machine learning approach aimed at learning transformational rules that would allow the (partial) generation of semantic annotations starting from syntactic annotations generated by a parser (or created manually).",
        "id":18467426
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that explores the idea of training soft prompts rather than identifying fixed ones within the realm of prompt tuning?",
    "positive_ctxs":[
      {
        "title":"Factual Probing Is [MASK]: Learning vs. Learning to Recall",
        "text":"Petroni et al. (2019)demonstrated that it is possible to retrieve world facts from a pretrained language model by expressing them as cloze-style prompts and interpret the model's prediction accuracy as a lower bound on the amount of factual information it encodes. Subsequent work has attempted to tighten the estimate by searching for better prompts, using a disjoint set of facts as training data. In this work, we make two complementary contributions to better understand these factual probing techniques. First, we propose OPTIPROMPT, a novel and efficient method which directly optimizes in continuous embedding space. We find this simple method is able to predict an additional 6.4% of facts in the LAMA benchmark. Second, we raise a more important question: Can we really interpret these probing results as a lower bound? Is it possible that these prompt-search methods learn from the training data too? We find, somewhat surprisingly, that the training data used by these methods contains certain regularities of the underlying fact distribution, and all the existing prompt methods, including ours, are able to exploit them for better fact prediction. We conduct a set of control experiments to disentangle \"learning\" from \"learning to recall\", providing a more detailed picture of what different prompts can reveal about pre-trained language models. 1",
        "id":233210199
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"We take the four following steps to extract collocations made of combinations of 2, 3, 4 words and\/or part of speech, respectively. First, we use \"Smadja's Xtract\" to extract the co-occurrence combinations of words and\/or part of speech of varying distance by computing means and variances. Second, we evaluate the significances of collocation candidates by 2 metrics: mutual information and t-test value. At last, we compare the head words of tagged word sense corpus made by Academic Sinica with the collocation candidates. If in the same distance, the head words of collocation candidates match the ones made by Academic Sinica, we say they are collocations. In addition, we apply the collocation information produced from this research to word sense disambiguation. It reaches application rate of 20.07% and precision rate of 90.83%.T",
        "id":44446387
      },
      {
        "title":"Dependency Extraction for Knowledge-based Domain Classification",
        "text":"Question classification is an important part in Question Answering. It refers to classifying a given question into a category. This paper presents a learning based question classifier. The previous works in this field have used UIUC questions dataset for the classification purpose. In contrast to this, we use the Web-Questions dataset to build the classifier. The dataset consists of questions with the links to the Freebase pages on which the answers will be found. To extract the exact answer of a question from a Freebase page, it is very essential to know the domain of the answer as it narrows down the number of possible answer candidates. Proposed classifier will be very helpful in extracting answers from the Freebase. Classifier uses the questions' features to classify a question into the domain of the answer, given the link to the freebase page on which the answer can be found.",
        "id":2080760
      },
      {
        "title":"Few-Shot Text Classification with Triplet Networks, Data Augmentation, and Curriculum Learning",
        "text":"Few-shot text classification is a fundamental NLP task in which a model aims to classify text into a large number of categories, given only a few training examples per category. This paper explores data augmentation-a technique particularly suitable for training with limited data-for this few-shot, highlymulticlass text classification setting. On four diverse text classification tasks, we find that common data augmentation techniques can improve the performance of triplet networks by up to 3.0% on average.To further boost performance, we present a simple training strategy called curriculum data augmentation, which leverages curriculum learning by first training on only original examples and then introducing augmented data as training progresses. We explore a twostage and a gradual schedule, and find that, compared with standard single-stage training, curriculum data augmentation trains faster, improves performance, and remains robust to high amounts of noising from augmentation.",
        "id":232233485
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that explores the improvement of Recurrent Neural Network Transducers (RNN-T) through the integration of cross-attention mechanisms for use in simultaneous translation?",
    "positive_ctxs":[
      {
        "title":"Cross Attention Augmented Transducer Networks for Simultaneous Translation",
        "text":"This paper proposes a novel architecture, Cross Attention Augmented Transducer (CAAT), for simultaneous translation. The framework aims to jointly optimize the policy and translation models. To effectively consider all possible READ-WRITE simultaneous translation action paths, we adapt the online automatic speech recognition (ASR) model, RNN-T, but remove the strong monotonic constraint, which is critical for the translation task to consider reordering. To make CAAT work, we introduce a novel latency loss whose expectation can be optimized by a forward-backward algorithm. We implement CAAT with Transformer while the general CAAT architecture can also be implemented with other attention-based encoder-decoder frameworks. Experiments on both speech-to-text (S2T) and text-to-text (T2T) simultaneous translation tasks show that CAAT achieves significantly better latency-quality trade-offs compared to the state-of-the-art simultaneous translation approaches. 1",
        "id":243865395
      }
    ],
    "negative_ctxs":[
      {
        "title":"Position Offset Label Prediction for Grammatical Error Correction",
        "text":"We introduce a novel position offset label prediction subtask to the encoder-decoder architecture for grammatical error correction (GEC) task. To keep the meaning of the input sentence unchanged, only a few words should be inserted or deleted during correction, and most of tokens in the erroneous sentence appear in the paired correct sentence with limited position movement. Inspired by this observation, we design an auxiliary task to predict position offset label (POL) of tokens, which is naturally capable of integrating different correction editing operations into a unified framework. Based on the predicted POL, we further propose a new copy mechanism (P-copy) to replace the vanilla copy module. Experimental results on Chinese, English and Japanese datasets demonstrate that our proposed POL-Pc framework obviously improves the performance of baseline models. Moreover, our model yields consistent performance gain over various data augmentation methods. Especially, after incorporating synthetic data, our model achieves a 38.95 F 0.5 score on Chinese GEC dataset, which outperforms the previous state-of-the-art by a wide margin of 1.98 points.",
        "id":252818994
      },
      {
        "title":"Challenges in Urdu Text Tokenization and Sentence Boundary Disambiguation",
        "text":"Urdu is morphologically rich language with different nature of its characters. Urdu text tokenization and sentence boundary disambiguation is difficult as compared to the language like English. Major hurdle for tokenization is improper use of space between words, where as absence of case discrimination makes the sentence boundary detection a difficult task. In this paper some issues regarding both of these language processing tasks have been identified.",
        "id":14464094
      },
      {
        "title":"Resolving Implicit References in Instructional Texts",
        "text":"The usage of (co-)referring expressions in discourse contributes to the coherence of a text.However, text comprehension can be difficult when referring expressions are non-verbalized and have to be resolved in the discourse context. In this paper, we propose a novel dataset of such implicit references, which we automatically derive from insertions of references in collaboratively edited how-to guides. Our dataset consists of 6,014 instances, making it one of the largest datasets of implicit references and a useful starting point to investigate misunderstandings caused by underspecified language. We test different methods for resolving implicit references in our dataset based on the Generative Pre-trained Transformer model (GPT) and compare them to heuristic baselines. Our experiments indicate that GPT can accurately resolve the majority of implicit references in our data. Finally, we investigate remaining errors and examine human preferences regarding different resolutions of an implicit reference given the discourse context.",
        "id":241583375
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that includes an online community dataset used to examine machine learning models for hate speech identification?",
    "positive_ctxs":[
      {
        "title":"Hate Towards the Political Opponent: A Twitter Corpus Study of the 2020 US Elections on the Basis of Offensive Speech and Stance Detection",
        "text":"The 2020 US Elections have been, more than ever before, characterized by social media campaigns and mutual accusations. We investigate in this paper if this manifests also in online communication of the supporters of the candidates Biden and Trump, by uttering hateful and offensive communication. We formulate an annotation task, in which we join the tasks of hateful\/offensive speech detection and stance detection, and annotate 3000 Tweets from the campaign period, if they express a particular stance towards a candidate. Next to the established classes of favorable and against, we add mixed and neutral stances and also annotate if a candidate is mentioned without an opinion expression. Further, we annotate if the tweet is written in an offensive style. This enables us to analyze if supporters of Joe Biden and the Democratic Party communicate differently than supporters of Donald Trump and the Republican Party. A BERT baseline classifier shows that the detection if somebody is a supporter of a candidate can be performed with high quality (.89 F 1 for Trump and .91 F 1 for Biden), while the detection that somebody expresses to be against a candidate is more challenging (.79 F 1 and .64 F 1 , respectively). The automatic detection of hate\/offensive speech remains challenging (with .53 F 1 ). Our corpus is publicly available and constitutes a novel resource for computational modelling of offensive language under consideration of stances.",
        "id":232092599
      }
    ],
    "negative_ctxs":[
      {
        "title":"PortageLive: Delivering Machine Translation Technology via Virtualization",
        "text":"We describe and report initial results on using virtual machines as a vehicle to deploy machine translation technology to the marketplace. Virtual machines can bridge the gap between the computing infrastructure typically used in research environments and commodity PCs typical of office environments. A key component is the compact representation of the underlying databases and models in tightly packed tries, which allows us to run state-ofthe art translation technology on regular office PCs.",
        "id":35789919
      },
      {
        "title":"Propagation of emotions, arousal and polarity in WordNet using Heterogeneous Structured Synset Embeddings",
        "text":"In this paper we present a novel method for emotive propagation in a wordnet based on a large emotive seed. We introduce a sense-level emotive lexicon annotated with polarity, arousal and emotions. The data were annotated as a part of a large study involving over 20,000 participants. A total of 30,000 lexical units in Polish WordNet were described with metadata, each unit received about 50 annotations concerning polarity, arousal and 8 basic emotions, marked on a multilevel scale. We present a preliminary approach to propagating emotive metadata to unlabeled lexical units based on the distribution of manual annotations using logistic regression and description of mixed synset embeddings based on our Heterogeneous Structured Synset Embeddings.",
        "id":232021736
      },
      {
        "title":"",
        "text":"",
        "id":208128503
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that investigates a clustering-based efficient attention mechanism within Transformer models?",
    "positive_ctxs":[
      {
        "title":"Efficient Content-Based Sparse Attention with Routing Transformers",
        "text":"Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n 1.5 d) from O(n 2 d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits\/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow. 1 53",
        "id":212718077
      }
    ],
    "negative_ctxs":[
      {
        "title":"Identification of Drug-Related Medical Conditions in Social Media",
        "text":"Monitoring social media has been shown to be an interesting approach for the early detection of drug adverse effects. In this paper, we describe a system which extracts medical entities in French drug reviews written by users. We focus on the identification of medical conditions, which is based on the concept of post-coordination: we first extract minimal medical-related entities (pain, stomach) then we combine them to identify complex ones (It was the worst [pain I ever felt in my stomach]). These two steps are respectively performed by two classifiers, the first being based on Conditional Random Fields and the second one on Support Vector Machines. The overall results of the minimal entity classifier are the following: P=0.926; R=0.849; F1=0.886. A thourough analysis of the feature set shows that, when combined with word lemmas, clusters generated by word2vec are the most valuable features. When trained on the output of the first classifier, the second classifier's performances are the following: p=0.683;r=0.956;f1=0.797. The addition of post-processing rules did not add any significant global improvement but was found to modify the precision\/recall ratio.",
        "id":26521766
      },
      {
        "title":"Gradient Origin Networks",
        "text":"This paper proposes a new type of implicit generative model that is able to quickly learn a latent representation without an explicit encoder. This is achieved with an implicit neural network that takes as inputs points in the coordinate space alongside a latent vector initialised with zeros. The gradients of the data fitting loss with respect to this zero vector are jointly optimised to act as latent points that capture the data manifold. The results show similar characteristics to autoencoders, but with fewer parameters and the advantages of implicit representation networks. * Authors contributed equally.",
        "id":220364453
      },
      {
        "title":"Sympathy Begins with a Smile, Intelligence Begins with a Word: Use of Multimodal Features in Spoken Human-Robot Interaction",
        "text":"Recognition of social signals, from human facial expressions or prosody of speech, is a popular research topic in human-robot interaction studies. There is also a long line of research in the spoken dialogue community that investigates user satisfaction in relation to dialogue characteristics. However, very little research relates a combination of multimodal social signals and language features detected during spoken face-to-face human-robot interaction to the resulting user perception of a robot. In this paper we show how different emotional facial expressions of human users, in combination with prosodic characteristics of human speech and features of human-robot dialogue, correlate with users' impressions of the robot after a conversation. We find that happiness in the user's recognised facial expression strongly correlates with likeability of a robot, while dialogue-related features (such as number of human turns or number of sentences per robot utterance) correlate with perceiving a robot as intelligent. In addition, we show that facial expression, emotional features, and prosody are better predictors of human ratings related to perceived robot likeability and anthropomorphism, while linguistic and non-linguistic features more often predict perceived robot intelligence and interpretability. As such, these characteristics may in future be used as an online reward signal for in-situ Reinforcement Learningbased adaptive human-robot dialogue systems. Figure 1: Left: a live view of experimental setup showing a participant interacting with Pepper. Right: a diagram of experimental setup showing the participant (green) and the robot (white) positioned face to face. The scene was recorded by cameras (triangles C) from the robot's perspective focusing on the face of the participant and from the side, showing the whole scene. The experimenter (red) was seated behind a divider.",
        "id":10586665
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that investigates across multiple languages how dictionary definitions and word embedding models compare?",
    "positive_ctxs":[
      {
        "title":"Semeval-2022 Task 1: CODWOE -Comparing Dictionaries and Word Embeddings",
        "text":"Word embeddings have advanced the state of the art in NLP across numerous tasks. Understanding the contents of dense neural representations is of utmost interest to the computational semantics community. We propose to focus on relating these opaque word vectors with human-readable definitions, as found in dictionaries. This problem naturally divides into two subtasks: converting definitions into embeddings, and converting embeddings into definitions. This task was conducted in a multilingual setting, using comparable sets of embeddings trained homogeneously.",
        "id":249151979
      }
    ],
    "negative_ctxs":[
      {
        "title":"UPF-taln: SemEval 2015 Tasks 10 and 11 Sentiment Analysis of Literal and Figurative Language in Twitter *",
        "text":"In this paper, we describe the approach used by the UPF-taln team for tasks 10 and 11 of SemEval 2015 that respectively focused on \"Sentiment Analysis in Twitter\" and \"Sentiment Analysis of Figurative Language in Twitter\". Our approach achieved satisfactory results in the figurative language analysis task, obtaining the second best result. In task 10, our approach obtained acceptable performances. We experimented with both wordbased features and domain-independent intrinsic word features. We exploited two machine learning methods: the supervised algorithm Support Vector Machines for task 10, and Random-Sub-Space with M5P as base algorithm for task 11.",
        "id":18492222
      },
      {
        "title":"New Functions of FrameSQL for Multilingual FrameNets",
        "text":"The Berkeley FrameNet Project (BFN) is making an English lexical database called FrameNet, which describes syntactic and semantic properties of an English lexicon extracted from large electronic text corpora(Baker et al., 1998). Other projects dealing with Spanish, German and Japanese follow a similar approach and annotate large corpora. FrameSQL is a web-based application developed by the author, and it allows the user to search the BFN database in a variety of ways (Sato, 2003). FrameSQL shows a clear view of the headword's grammar and combinatorial properties offered by the FrameNet database. FrameSQL has been developing and new functions were implemented for processing the Spanish FrameNet data(Subirats and Sato, 2004). FrameSQL is also in the process of incorporating the data of the Japanese FrameNet Project(Ohara et al., 2003)and that of the Saarbrücken Lexical Semantics Acquisition Project(Erk et al., 2003)into the database and will offer the same user-interface for searching these lexical data. This paper describes new functions of FrameSQL, showing how FrameSQL deals with the lexical data of English, Spanish, Japanese and German seamlessly.",
        "id":2456041
      },
      {
        "title":"DONNELLAN'S DISTINCTION AND A COMPUTATIONAL MODEL OF REFERENCE",
        "text":"In this paper, I describe how Donnellan's distinction between referential and attributive uses of definite descriptions should be represented in a computational model of reference. After briefly discussing the significance of Donnellan's distinction, I reinterpret it as being three-tiered, relating to object representation, referring intentions, and choice of rehrring expression. I then present a cognitive model of referring, the components of which correspond to this analysis, and discuss the interaction that takes place among those components. Finally, the implementation of this model, now in progress, is described.",
        "id":6515223
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that investigates applying combinatorial optimization techniques in unsupervised entity matching?",
    "positive_ctxs":[
      {
        "title":"From Alignment to Assignment: Frustratingly Simple Unsupervised Entity Alignment",
        "text":"Cross-lingual entity alignment (EA) aims to find the equivalent entities between crosslingual KGs (Knowledge Graphs), which is a crucial step for integrating KGs. Recently, many GNN-based EA methods are proposed and show decent performance improvements on several public datasets. However, existing GNN-based EA methods inevitably inherit poor interpretability and low efficiency from neural networks. Motivated by the isomorphic assumption of GNN-based methods, we successfully transform the cross-lingual EA problem into an assignment problem. Based on this re-definition, we propose a frustratingly Simple but Effective Unsupervised entity alignment method (SEU) without neural networks. Extensive experiments have been conducted to show that our proposed unsupervised approach even beats advanced supervised methods across all public datasets while having high efficiency, interpretability, and stability.",
        "id":237420821
      }
    ],
    "negative_ctxs":[
      {
        "title":"The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 80-85, PREFER: Using a Graph-Based Approach to Generate Paraphrases for Language Learning",
        "text":"Paraphrasing is an important aspect of language competence; however, EFL learners have long had difficulty paraphrasing in their writing owing to their limited language proficiency. Therefore, automatic paraphrase suggestion systems can be useful for writers. In this paper, we present PREFER 1 , a paraphrase reference tool for helping language learners improve their writing skills. In this paper, we attempt to transform the paraphrase generation problem into a graphical problem in which the phrases are treated as nodes and translation similarities as edges. We adopt the PageRank algorithm to rank and filter the paraphrases generated by the pivot-based paraphrase generation method. We manually evaluate the performance of our method and assess the effectiveness of PREFER in language learning. The results show that our method successfully preserves both the semantic meaning and syntactic structure of the query phrase. Moreover, the students' writing performance improve most with the assistance of PREFER.",
        "id":1326531
      },
      {
        "title":"Box Embeddings: An open-source library for representation learning using geometric structures",
        "text":"A major factor contributing to the success of modern representation learning is the ease of performing various vector operations. Recently, objects with geometric structures (eg. distributions, complex or hyperbolic vectors, or regions such as cones, disks, or boxes) have been explored for their alternative inductive biases and additional representational capacities.",
        "id":237485374
      },
      {
        "title":"",
        "text":"",
        "id":218973754
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that investigates aspect-based sentiment analysis across different languages, incorporating methods such as code-switching of aspect terms?",
    "positive_ctxs":[
      {
        "title":"Cross-lingual Aspect-based Sentiment Analysis with Aspect Term Code-Switching *",
        "text":"Many efforts have been made in solving the Aspect-based sentiment analysis (ABSA) task. While most existing studies focus on English texts, handling ABSA in resource-poor languages remains a challenging problem. In this paper, we consider the unsupervised crosslingual transfer for the ABSA task, where only labeled data in the source language is available and we aim at transferring its knowledge to the target language having no labeled data. To this end, we propose an alignment-free label projection method to obtain high-quality pseudolabeled data of the target language with the help of the translation system, which could preserve more accurate task-specific knowledge in the target language. For better utilizing the source and translated data, as well as enhancing the cross-lingual alignment, we design an aspect code-switching mechanism to augment the training data with code-switched bilingual sentences. To further investigate the importance of language-specific knowledge in solving the ABSA problem, we distill the above model on the unlabeled target language data which improves the performance to the same level of the supervised method.",
        "id":243865296
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":208264552
      },
      {
        "title":"A GRADIENT FLOW FRAMEWORK FOR ANALYZING NETWORK PRUNING",
        "text":"Recent network pruning methods focus on pruning models early-on in training. To estimate the impact of removing a parameter, these methods use importance measures that were originally designed to prune trained models. Despite lacking justification for their use early-on in training, such measures result in surprisingly low accuracy loss. To better explain this behavior, we develop a general framework that uses gradient flow to unify state-of-the-art importance measures through the norm of model parameters. We use this framework to determine the relationship between pruning measures and evolution of model parameters, establishing several results related to pruning models early-on in training: (i) magnitude-based pruning removes parameters that contribute least to reduction in loss, resulting in models that converge faster than magnitude-agnostic methods; (ii) loss-preservation based pruning preserves first-order model evolution dynamics and is therefore appropriate for pruning minimally trained models; and (iii) gradient-norm based pruning affects second-order model evolution dynamics, such that increasing gradient norm via pruning can produce poorly performing models. We validate our claims on several VGG-13, MobileNet-V1, and ResNet-56 models trained on CIFAR-10\/CIFAR-100. 1 gradient flow refers to gradient descent with infinitesimal learning rate; see Equation 6 for a short primer.Published as a conference paper at ICLR 2021 what properties make a parameter dispensable according to a particular importance measure. Our findings follow. (i) Magnitude-based pruning measures remove parameters that contribute least to reduction in loss. This enables magnitude-based pruned models to achieve faster convergence than magnitude-agnostic measures. (ii) Loss-preservation based measures remove parameters with the least tendency to change, thus preserving first-order model evolution dynamics. This shows that loss-preservation is appropriate for pruning models early-on in training as well. (iii) Gradient-norm based pruning is linearly related to second-order model evolution dynamics. Increasing gradient norm via pruning for even slightly trained models can permanently damage earlier layers, producing poorly performing architectures. This behavior is a result of aggressively pruning filters that maximally increase model loss. We validate our claims on several VGG-13, MobileNet-V1, and ResNet-56 models trained on CIFAR-10 and CIFAR-100.",
        "id":221879071
      },
      {
        "title":"Some Notes on p(e)re-Reduplication in Bulgarian and Ukrainian: A Corpus-based Study",
        "text":"We present a comparative study of p(e)re-",
        "id":252995552
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that investigates efficient finetuning methods that only trains very very few parameters in language models?",
    "positive_ctxs":[
      {
        "title":"BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
        "text":"We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods. Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.",
        "id":231672601
      }
    ],
    "negative_ctxs":[
      {
        "title":"應用混淆音矩陣之中英文音譯詞組自動抽取 郭金喜",
        "text":"",
        "id":5757808
      },
      {
        "title":"MWEs as Non-propositional Content Indicators",
        "text":"We report that a proper employment of MWEs concerned enables us to put forth a tractable framework, which is based on a multiple nesting of semantic operations, for the processing of non-inferential, Nonpropositional Contents (NPCs) of natural Japanese sentences. Our framework is characterized by its broad syntactic and semantic coverage, enabling us to deal with multiply composite modalities and their semantic\/pragmatic similarity. Also, the relationship between indirect(Searle, 1975)and direct speech, and equations peculiar to modal logic and its family(Mally, 1926;Prior, 1967)are treated in the similarity paradigm.",
        "id":17759492
      },
      {
        "title":"Dynamic Online Conversation Recommendation",
        "text":"Trending topics in social media content evolve over time, and it is therefore crucial to understand social media users and their interpersonal communications in a dynamic manner. In this research we study dynamic online conversation recommendation, to help users engage in conversations that satisfy their evolving interests. Different from works in conversation recommendation which assume static user interests, our model captures the temporal aspects of user interests. Moreover, our model can cater for cold start problem where conversations are new and unseen in training. We propose a neural architecture to analyze changes of user interactions and interests over time, whose result is used to predict which discussions the users are likely to enter. We conduct experiments on large-scale collections of Reddit conversations. Results on three subreddits show that our model significantly outperforms state-of-the-art models based on static assumption of user interests. We further evaluate performance in cold start, and observe consistently better performance by our model when considering various degrees of sparsity of user's chatting history and conversation contexts. Lastly, our analysis also confirms the change of user interests. This further justify the advantage and efficacy of our model.",
        "id":219720103
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that investigates employing graph attention techniques for integrating multiple modalities for identifying emotions?",
    "positive_ctxs":[
      {
        "title":"COGMEN: COntextualized GNN based Multimodal Emotion recognitioN",
        "text":"Emotions are an inherent part of human interactions, and consequently, it is imperative to develop AI systems that understand and recognize human emotions. During a conversation involving various people, a person's emotions are influenced by the other speaker's utterances and their own emotional state over the utterances. In this paper, we propose COntextualized Graph Neural Network based Multimodal Emotion recognitioN (COGMEN) system that leverages local information (i.e., inter\/intra dependency between speakers) and global information (context). The proposed model uses Graph Neural Network (GNN) based architecture to model the complex dependencies (local and global information) in a conversation. Our model gives state-of-theart (SOTA) results on IEMOCAP and MOSEI datasets, and detailed ablation experiments show the importance of modeling information at both levels.",
        "id":248524998
      }
    ],
    "negative_ctxs":[
      {
        "title":"Analyse des inférences pour la fouille d'opinion en chinois",
        "text":"La fouille d'opinion est une activité essentielle pour la veille économique, facilitée par les réseaux sociaux et forums dédiés. L'analyse repose généralement sur des lexiques de sentiments. Pourtant, certaines opinions sont exprimées au moyen d'inférences. Dans cet article, nous proposons une classification des inférences utilisées en chinois dans des commentaires touristiques, à des fins de fouille d'opinion, selon trois niveaux d'analyse (réalisation sémantique, modalité de réalisation, et mode de production). Nous démontrons l'intérêt d'analyser les différents types d'inférence pour déterminer la polarité des opinions exprimées en corpus. Nous présentons également de premiers résultats fondés sur des plongements lexicaux.ABSTRACTAnalysis of Inferences in Chinese for Opinion MiningOpinion mining is an essential activity for economic watch, made easier by social networks and ad hoc forums. The analysis generally relies on lexicon of sentiments. Nevertheless, some opinions are expressed through inferences. In this paper, we propose a classification of inferences used in Chinese in tourist comments, for an opinion mining task, based on three levels of analysis (semantic realization, modality of realization and production mode). We proved the interest to analyze the distinct types of inferences to identify the polarity of opinions expressed in corpora. We also present some results based on word embeddings. MOTS-CLÉS : Inférences, fouille d'opinion, polarité.",
        "id":220835075
      },
      {
        "title":"Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations",
        "text":"Modern conversational AI systems support natural language understanding for a wide variety of capabilities. While a majority of these tasks can be accomplished using a simple and flat representation of intents and slots, more sophisticated capabilities require complex hierarchical representations supported by semantic parsing. State-of-the-art semantic parsers are trained using supervised learning with data labeled according to a hierarchical schema which might be costly to obtain or not readily available for a new domain. In this work, we explore the possibility of generating synthetic data for neural semantic parsing using a pretrained denoising sequence-to-sequence model (i.e., BART). Specifically, we first extract masked templates from the existing labeled utterances, and then fine-tune BART to generate synthetic utterances conditioning on the extracted templates. Finally, we use an auxiliary parser (AP) to filter the generated utterances. The AP guarantees the quality of the generated data. We show the potential of our approach when evaluating on the Facebook TOP dataset 1 for navigation domain.",
        "id":226246180
      },
      {
        "title":"Natural Questions: A Benchmark for Question Answering Research",
        "text":"We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long\/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.453",
        "id":86611921
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that investigates enhancing zero-shot question answering with prompts from language models integrated with knowledge graph data?",
    "positive_ctxs":[
      {
        "title":"Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering",
        "text":"Large Language Models (LLMs) are capable of performing zero-shot closed-book question answering tasks, based on their internal knowledge stored in parameters during pre-training. However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive. To this end, we propose to augment the knowledge directly in the input of LLMs. Specifically, we first retrieve the relevant facts to the input question from the knowledge graph based on semantic similarities between the question and its associated facts. After that, we prepend the retrieved facts to the input question in the form of the prompt, which is then forwarded to LLMs to generate the answer. Our framework, Knowledge-Augmented language model PromptING (KAP-ING), requires no model training, thus completely zero-shot. We validate the performance of our KAPING framework on the knowledge graph question answering task, that aims to answer the user's question based on facts over a knowledge graph, on which ours outperforms relevant zero-shot baselines by up to 48% in average, across multiple LLMs of various sizes.",
        "id":259095910
      },
      {
        "title":"Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering",
        "text":"Large Language Models (LLMs) are capable of performing zero-shot closed-book question answering tasks, based on their internal knowledge stored in parameters during pre-training. However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive. To this end, we propose to augment the knowledge directly in the input of LLMs. Specifically, we first retrieve the relevant facts to the input question from the knowledge graph based on semantic similarities between the question and its associated facts. After that, we prepend the retrieved facts to the input question in the form of the prompt, which is then forwarded to LLMs to generate the answer. Our framework, Knowledge-Augmented language model PromptING (KAP-ING), requires no model training, thus completely zero-shot. We validate the performance of our KAPING framework on the knowledge graph question answering task, that aims to answer the user's question based on facts over a knowledge graph, on which ours outperforms relevant zero-shot baselines by up to 48% in average, across multiple LLMs of various sizes.",
        "id":260063238
      }
    ],
    "negative_ctxs":[
      {
        "title":"Automatic Acronym Acquisition and Term Variation Management within Domain-Specific Texts *",
        "text":"In this paper we present a framework for the effective management of terms and their variants that are automatically acquired from domain-specific texts. In our approach, the term variant recognition is incorporated in the automatic term retrieval process by taking into account orthographical, morphological, syntactic, lexico-semantic and pragmatic term variations. In particular, we address acronyms as a common way of introducing term variants in scientific papers. We describe a method for the automatic acquisition of newly introduced acronyms and the mapping to their 'meanings', i.e. the corresponding terms. The proposed three-step procedure is based on morpho-syntactic constraints that are commonly used in acronym definitions. First, acronym definitions containing an acronym and the corresponding term are retrieved. These two elements are matched in the second step by performing morphological analysis of words and combining forms constituting the term. The problems of acronym variation and acronym ambiguity are addressed in the third step by establishing classes of term variants that correspond to specific concepts. We present the results of the acronym acquisition in the domain of molecular biology: the precision of the method ranged from 94% to 99% depending on the size of the corpus used for evaluation, whilst the recall was 73%. * This research is a part of the BioPATH research project coordinated by LION BioScience (http:\/\/www.lionbioscience.com) and funded by German Ministry of Research.",
        "id":9818857
      },
      {
        "title":"A Knowledge-Graph-Based Intrinsic Test for Benchmarking Medical Concept Embeddings and Pretrained Language Models",
        "text":"Using language models created from large data sources has improved the performance of several deep learning-based architectures, obtaining state-of-the-art results in several NLP extrinsic tasks. However, little research is related to creating intrinsic tests that allow us to compare the quality of different language models when obtaining contextualized embeddings. This gap increases even more when working on specific domains in languages other than English. This paper proposes a novel graph-based intrinsic test that allows us to measure the quality of different language models in clinical and biomedical domains in Spanish. Our results show that our intrinsic test performs better for clinical and biomedical language models than a general one. Also, it correlates with better outcomes for a NER task using a probing model over contextualized embeddings. We hope our work will help the clinical NLP research community to evaluate and compare new language models in other languages and find the most suitable models for solving downstream tasks.",
        "id":256461352
      },
      {
        "title":"Making Sense of Sound: Unsupervised Topic Segmentation over Acoustic Input",
        "text":"We address the task of unsupervised topic segmentation of speech data operating over raw acoustic information. In contrast to existing algorithms for topic segmentation of speech, our approach does not require input transcripts. Our method predicts topic changes by analyzing the distribution of reoccurring acoustic patterns in the speech signal corresponding to a single speaker. The algorithm robustly handles noise inherent in acoustic matching by intelligently aggregating information about the similarity profile from multiple local comparisons. Our experiments show that audio-based segmentation compares favorably with transcriptbased segmentation computed over noisy transcripts. These results demonstrate the desirability of our method for applications where a speech recognizer is not available, or its output has a high word error rate.",
        "id":6996024
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that investigates expanding keyword collections through embedding similarity in weakly-supervised document categorization?",
    "positive_ctxs":[
      {
        "title":"X-Class: Text Classification with Extremely Weak Supervision",
        "text":"In this paper, we explore text classification with extremely weak supervision, i.e., only relying on the surface text of class names. This is a more challenging setting than the seed-driven weak supervision, which allows a few seed words per class. We opt to attack this problem from a representation learning perspective-ideal document representations should lead to nearly the same results between clustering and the desired classification. In particular, one can classify the same corpus differently (e.g., based on topics and locations), so document representations should be adaptive to the given class names. We propose a novel framework X-Class to realize the adaptive representations. Specifically, we first estimate class representations by incrementally adding the most similar word to each class until inconsistency arises. Following a tailored mixture of class attention mechanisms, we obtain the document representation via a weighted average of contextualized word representations. With the prior of each document assigned to its nearest class, we then cluster and align the documents to classes. Finally, we pick the most confident documents from each cluster to train a text classifier. Extensive experiments demonstrate that X-Class can rival and even outperform seed-driven weakly supervised methods on 7 benchmark datasets.",
        "id":225067563
      }
    ],
    "negative_ctxs":[
      {
        "title":"Impact de l'apprentissage multi-labels actif appliqué aux transformers",
        "text":"L'Apprentissage Actif (AA) est largement utilisé en apprentissage automatique afin de réduire l'effort d'annotation.Bien que la plupart des travaux d'AA soient antérieurs aux transformers, le succès récent de ces architectures a conduit la communauté à revisiter l'AA dans le contexte des modèles de langues pré-entraînés.De plus, le mécanisme de fine-tuning, où seules quelques données annotées sont utilisées pour entraîner le modèle sur une nouvelle tâche, est parfaitement en accord avec l'objectif de l'AA.Nous proposons d'étudier l'impact de l'AA dans le contexte des transformers pour la tâche de classification multi-labels.Or la plupart des stratégies AA, lorsqu'elles sont appliquées à ces modèles, conduisent à des temps de calcul excessifs, ce qui empêche leurs utilisations au cours d'une interaction homme-machine en temps réel.Afin de pallier ce problème, nous utilisons des stratégies d'AA basées sur l'incertitude.L'article compare six stratégies d'AA basées sur l'incertitude dans le contexte des transformers et montre que si deux stratégies améliorent invariablement les performances, les autres ne surpassent pas l'échantillonnage aléatoire.L'étude montre également que les stratégies performantes ont tendance à sélectionner des ensembles d'instances plus diversifiées pour l'annotation.",
        "id":264038768
      },
      {
        "title":"User-friendly Automatic Transcription of Low-resource Languages: Plugging ESPnet into Elpis",
        "text":"This paper reports on progress integrating the speech recognition toolkit ESPnet into Elpis, a web front-end originally designed to provide access to the Kaldi automatic speech recognition toolkit. The goal of this work is to make end-to-end speech recognition models available to language workers via a user-friendly graphical interface. Encouraging results are reported on (i) development of an ESPnet recipe for use in Elpis, with preliminary results on data sets previously used for training acoustic models with the Persephone toolkit along with a new data set that had not previously been used in speech recognition, and (ii) incorporating ESPnet into Elpis along with UI enhancements and a CUDA-supported Dockerfile.",
        "id":230577657
      },
      {
        "title":"RANGE CONCATENATION GRAMMARS",
        "text":"In this paper we present Range Concatenation Grammars, a syntactic formalism which possesses many attractive features among which we underline here, power and closure properties. For example, Range Concatenation Grammars are more powerful than Linear Context-Free Rewriting Systems though this power is not reached to the detriment of efficiency since its sentences can always be parsed in polynomial time. Range Concatenation Languages are closed both under intersection and complementation and these closure properties may allow to consider novel ways to describe some linguistic processings. We also present a parsing algorithm which is the basis of our current prototype implementation.",
        "id":120038201
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that investigates graph-based methods for predicting connections in knowledge graphs, focusing on n-ary relational facts rather than just simple triple structures?",
    "positive_ctxs":[
      {
        "title":"Link Prediction on N-ary Relational Facts: A Graph-based Approach",
        "text":"Link prediction on knowledge graphs (KGs) is a key research topic. Previous work mainly focused on binary relations, paying less attention to higher-arity relations although they are ubiquitous in real-world KGs. This paper considers link prediction upon n-ary relational facts and proposes a graph-based approach to this task.The key to our approach is to represent the nary structure of a fact as a small heterogeneous graph, and model this graph with edge-biased fully-connected attention. The fully-connected attention captures universal inter-vertex interactions, while with edge-aware attentive biases to particularly encode the graph structure and its heterogeneity. In this fashion, our approach fully models global and local dependencies in each n-ary fact, and hence can more effectively capture associations therein. Extensive evaluation verifies the effectiveness and superiority of our approach. It performs substantially and consistently better than current state-of-the-art across a variety of n-ary relational benchmarks. Our code is publicly available. 1",
        "id":234763248
      }
    ],
    "negative_ctxs":[
      {
        "title":"Correcting ESL Errors Using Phrasal SMT Techniques",
        "text":"This paper presents a pilot study of the use of phrasal Statistical Machine Translation (SMT) techniques to identify and correct writing errors made by learners of English as a Second Language (ESL). Using examples of mass noun errors found in the Chinese Learner Error Corpus (CLEC) to guide creation of an engineered training set, we show that application of the SMT paradigm can capture errors not well addressed by widely-used proofing tools designed for native speakers. Our system was able to correct 61.81% of mistakes in a set of naturallyoccurring examples of mass noun errors found on the World Wide Web, suggesting that efforts to collect alignable corpora of pre-and post-editing ESL writing samples offer can enable the development of SMT-based writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of ESL learners.",
        "id":757808
      },
      {
        "title":"Hantology-A Linguistic Resource for Chinese Language Processing and Studying",
        "text":"Hantology, a character-based Chinese language resource is created to provide an infrastructure for language processing and research on the writing system. Unlike alphabetic or syllabic writing systems, the ideographic writing system of Chinese poses both a challenge and an opportunity. The challenge is that a totally different resources structure must be created to represent and process speaker's conventionalization of the language. The rare opportunity is that the structure itself is enriched with conceptual classification and can be utilized for ontology building. We describe the contents and possible applications of Hantology in this paper. The applications of Hantology include: (1) an account for the diachronic development of Chinese lexica (2) character-based language processing, (3) a study of conceptual structure differences in Chinese and English, and (4) comparisons of different ideographic writing systems.",
        "id":12446797
      },
      {
        "title":"Evaluation of Computational Linguistic Techniques for Identifying Significant Topics for Browsing Applications",
        "text":"AbstractEvaluation of natural language processing tools and systems must focus on two complementary aspects: first, evaluation of the accuracy of the output, and second, evaluation of the functionality of the output as embedded in an application. This paper presents evaluations of two aspects of LinkIT, a tool for noun phrase identification linking, sorting and filtering. LinkIT[Evans 1998] uses a head sorting method[Wacholder 1998] to organize and rank simplex noun phrases (SNPs). LinkIT is to identify significant topics in domainindependent documents. The first evaluation, reported in D.K.Evans et al. 2000 compares the output of the Noun Phrase finder in LinkIT to two other systems. Issues of establishing a gold standard and criteria for matching are discussed. The second evaluation directly concerns the construction of the browsing application. We present results from Wacholder et al. 2000 on a qualitative evaluation which compares three shallow processing methods for extracting index terms, i.e., terms that can be used to model the content of documents. We analyze both quality and coverage. We discuss how experimental results such as these guide the building of an effective browsing applications.",
        "id":17461076
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that investigates how cross-attention mechanisms improve the interplay between token and syntax data within automated program repair systems?",
    "positive_ctxs":[
      {
        "title":"",
        "text":"",
        "id":236478113
      }
    ],
    "negative_ctxs":[
      {
        "title":"Published as a conference paper at ICLR 2021 ONE NETWORK FITS ALL? MODULAR VERSUS MONOLITHIC TASK FORMULATIONS IN NEURAL NETWORKS",
        "text":"Can deep learning solve multiple tasks simultaneously, even when they are unrelated and very different? We investigate how the representations of the underlying tasks affect the ability of a single neural network to learn them jointly. We present theoretical and empirical findings that a single neural network is capable of simultaneously learning multiple tasks from a combined data set, for a variety of methods for representing tasks-for example, when the distinct tasks are encoded by well-separated clusters or decision trees over certain task-code attributes. More concretely, we present a novel analysis that shows that families of simple programming-like constructs for the codes encoding the tasks are learnable by two-layer neural networks with standard training. We study more generally how the complexity of learning such combined tasks grows with the complexity of the task codes; we find that combining many tasks may incur a sample complexity penalty, even though the individual tasks are easy to learn. We provide empirical support for the usefulness of the learning bounds by training networks on clusters, decision trees, and SQL-style aggregation. Uszkoreit. One model to learn them all. arXiv preprint arXiv:1706.05137, 2017.Michael Kearns. Efficient noise-tolerant learning from statistical queries. . Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. and practical bert models for sequence labeling. arXiv preprint arXiv:1909.00100, 2019.Gregory Valiant. Finding correlations in subquadratic time, with applications to learning parities and the closest pair problem. survey on multi-task learning. arXiv preprint arXiv:1707.08114, 2017.",
        "id":232404824
      },
      {
        "title":"Tamil NER -Coping with Real Time Challenges",
        "text":"This paper describes various challenges encountered while developing an automatic Named Entity Recognition (NER) using Conditional Random Fields (CRFs) for Tamil. We also discuss how we have overcome some of these challenges. Though most of the challenges in NER discussed here are common to many Indian languages, in this work the focus is on Tamil, a South Indian language belonging to Dravidian language family. The corpus used in this work is the web data. The web data consisted of news paper articles, articles on blog sites and other online web portals.",
        "id":16456944
      },
      {
        "title":"Grammars for Local and Long Dependencies",
        "text":"Polarized dependency (PD-)  grammars are proposed as a means of efficient treatment of discontinuous constructions. PD-grammars describe two kinds of dependencies : local, explicitly derived by the rules, and long, implicitly specified by negative and positive valencies of words. If in a PD-grammar the number of non-saturated valencies in derived structures is bounded by a constant, then it is weakly equivalent to a cf-grammar and has a ¢ ¡ ¤ £ ¦ ¥ § time parsing algorithm. It happens that such bounded PD-grammars are strong enough to express such phenomena as unbounded raising, extraction and extraposition.© and a word dependent on © be dominated by © In first dependency grammars(Gaifman, 1961)and in some more recent proposals: link grammars (Sleator and Temperly, 1993), projective dependency grammars(Lombardo and Lesmo, 1996)the projectivity is implied by definition. In some other theories, e.g. in word grammar(Hudson, 1984), it is used as one of the axioms defining acceptable surface structures. In presence of this property, D-trees are in a sense equivalent to phrase structures with head selection 1 . It is for this reason that D-trees determined by grammars ofRobinson (Robinson, 1970), categorial grammars(Bar-Hillel et al., 1960), classical Lambek calculus(Lambek, 1958), and some other formalisms are projective. Projectivity affects the complexity of parsing : as a rule, it allows dynamic programming technics which lead to polynomial time algorithms (cf.",
        "id":10350260
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that investigates how hierarchical structures within transformers enhance task-oriented dialogue systems?",
    "positive_ctxs":[
      {
        "title":"Hierarchical Transformer for Task Oriented Dialog Systems",
        "text":"Generative models for dialog systems have gained much interest because of the recent success of RNN and Transformer based models in tasks like question answering and summarization. Although the task of dialog response generation is generally seen as a sequence to sequence (Seq2Seq) problem, researchers in the past have found it challenging to train dialog systems using the standard Seq2Seq models. Therefore, to help the model learn meaningful utterance and conversation level features, Sordoni et al. (2015b); Serban et al.(2016)proposed Hierarchical RNN architecture, which was later adopted by several other RNN based dialog systems. With the transformer-based models dominating the seq2seq problems lately, the natural question to ask is the applicability of the notion of hierarchy in transformer based dialog systems. In this paper, we propose a generalized framework for Hierarchical Transformer Encoders and show how a standard transformer can be morphed into any hierarchical encoder, including HRED and HIBERT like models, by using specially designed attention masks and positional encodings. We demonstrate that Hierarchical Encoding helps achieve better natural language understanding of the contexts in transformer-based models for task-oriented dialog systems through a wide range of experiments. The code and data for all experiments in this paper has been open-sourced 1 2 .",
        "id":226965200
      }
    ],
    "negative_ctxs":[
      {
        "title":"Differentiable Euler Characteristic Transforms for Shape Classification",
        "text":"The Euler Characteristic Transform (ECT) has proven to be a powerful representation, combining geometrical and topological characteristics of shapes and graphs.However, the ECT was hitherto unable to learn task-specific representations.We overcome this issue and develop a novel computational layer that enables learning the ECT in an end-to-end fashion.Our method DECT is fast and computationally efficient, while exhibiting performance on a par with more complex models in both graph and point cloud classification tasks.Moreover, we show that this seemingly unexpressive statistic still provides the same topological expressivity as more complex topological deep learning layers provide.",
        "id":263835059
      },
      {
        "title":"Discrimination between Similar Languages, Varieties and Dialects using CNN-and LSTM-based Deep Neural Networks",
        "text":"In this paper, we describe a system (CGLI) for discriminating similar languages, varieties and dialects using convolutional neural networks (CNNs) and long short-term memory (LSTM) neural networks. We have participated in the Arabic dialect identification sub-task of DSL 2016 shared task for distinguishing different Arabic language texts under closed submission track. Our proposed approach is language independent and works for discriminating any given set of languages, varieties and dialects. We have obtained 43.29% weighted-F1 accuracy in this subtask using CNN approach using default network parameters.",
        "id":3116257
      },
      {
        "title":"Dependency Based Chinese Sentence Realization",
        "text":"This paper describes log-linear models for a general-purpose sentence realizer based on dependency structures. Unlike traditional realizers using grammar rules, our method realizes sentences by linearizing dependency relations directly in two steps. First, the relative order between head and each dependent is determined by their dependency relation. Then the best linearizations compatible with the relative order are selected by log-linear models. The log-linear models incorporate three types of feature functions, including dependency relations, surface words and headwords. Our approach to sentence realization provides simplicity, efficiency and competitive accuracy. Trained on 8,975 dependency structures of a Chinese Dependency Treebank, the realizer achieves a BLEU score of 0.8874.",
        "id":438829
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that investigates how many evidence sentences are needed for document-level RE?",
    "positive_ctxs":[
      {
        "title":"Three Sentences Are All You Need: Local Path Enhanced Document Relation Extraction",
        "text":"Document-level Relation Extraction (RE) is a more challenging task than sentence RE as it often requires reasoning over multiple sentences. Yet, human annotators usually use a small number of sentences to identify the relationship between a given entity pair. In this paper, we present an embarrassingly simple but effective method to heuristically select evidence sentences for document-level RE, which can be easily combined with BiLSTM to achieve good performance on benchmark datasets, even better than fancy graph neural network based methods. We have released our code at https:\/\/github.com\/AndrewZhe\/Three-Sentences-Are-All-You-Need.",
        "id":235313346
      }
    ],
    "negative_ctxs":[
      {
        "title":"Sociolinguistics for Computational Social Science",
        "text":"In recent years, a major growth area in applied natural language processing has been the application of automated techniques to massive datasets in order to answer questions about society, and by extension people. Sociolinguistics, which combines anthropology, statistics and linguistics (e.g.Labov 1994Labov , 2001, studies linguistic data in order to answer key questions about the relationship of language and society. Sociolinguists focus on frequency and patterns in linguistic usage, correlations, strength of factors and significance, which together reveal information about the sex, age, education and occupation of speakers\/writers but also their history, culture, place of residence, social relationships and affiliations. The findings arising from this type research offer important insights into the nature of human organizations at the global, national or community level. They also reveal connections and interactions, the convergence and divergence of groups, historical associations and developing trends.",
        "id":37495699
      },
      {
        "title":"Introducing the CURLICAT Corpora: Seven-language Domain Specific Annotated Corpora from Curated Sources",
        "text":"This article presents the current outcomes of the CURLICAT CEF Telecom project, which aims to collect and deeply annotate a set of large corpora from selected domains. The CURLICAT corpus includes 7 monolingual corpora (Bulgarian, Croatian, Hungarian, Polish, Romanian, Slovak and Slovenian) containing selected samples from respective national corpora. These corpora are automatically tokenized, lemmatized and morphologically analysed and the named entities annotated. The annotations are uniformly provided for each language specific corpus while the common metadata schema is harmonised across the languages. Additionally, the corpora are annotated for IATE terms in all languages. The file format is CoNLL-U Plus format, containing the ten columns specific to the CoNLL-U format and three extra columns specific to our corpora as defined by Varádi et al. (2020). The CURLICAT corpora represent a rich and valuable source not just for training NMT models, but also for further studies and developments in machine learning, cross-lingual terminological data extraction and classification.",
        "id":250150837
      },
      {
        "title":"Disentangled Learning of Stance and Aspect Topics for Vaccine Attitude Detection in Social Media",
        "text":"Building models to detect vaccine attitudes on social media is challenging because of the composite, often intricate aspects involved, and the limited availability of annotated data. Existing approaches have relied heavily on supervised training that requires abundant annotations and pre-defined aspect categories. Instead, with the aim of leveraging the large amount of unannotated data now available on vaccination, we propose a novel semi-supervised approach for vaccine attitude detection, called VADET. A variational autoencoding architecture based on language models is employed to learn from unlabelled data the topical information of the domain. Then, the model is fine-tuned with a few manually annotated examples of user attitudes. We validate the effectiveness of VADET on our annotated data and also on an existing vaccination corpus annotated with opinions on vaccines. Our results show that VADET is able to learn disentangled stance and aspect topics, and outperforms existing aspect-based sentiment analysis models on both stance detection and tweet clustering. Our source code and dataset are available at http:\/\/github. com\/somethingx1202\/VADet.",
        "id":248562688
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that investigates how neural language models' forecasts correlate with human linguistic processing, especially in terms of syntactic surprisal?",
    "positive_ctxs":[
      {
        "title":"Syntactic Surprisal From Neural Models Predicts, But Underestimates, Human Processing Difficulty From Syntactic Ambiguities",
        "text":"Humans exhibit garden path effects: When reading sentences that are temporarily structurally ambiguous, they slow down when the structure is disambiguated in favor of the less preferred alternative. Surprisal theory(Hale, 2001;Levy, 2008), a prominent explanation of this finding, proposes that these slowdowns are due to the unpredictability of each of the words that occur in these sentences. Challenging this hypothesis, van Schijndel and Linzen (2021) find that estimates of the cost of word predictability derived from language models severely underestimate the magnitude of human garden path effects. In this work, we consider whether this underestimation is due to the fact that humans weight syntactic factors in their predictions more highly than language models do. We propose a method for estimating syntactic predictability from a language model, allowing us to weigh the cost of lexical and syntactic predictability independently. We find that treating syntactic predictability independently from lexical predictability indeed results in larger estimates of garden path. At the same time, even when syntactic predictability is independently weighted, surprisal still greatly underestimate the magnitude of human garden path effects. Our results support the hypothesis that predictability is not the only factor responsible for the processing cost associated with garden path sentences.",
        "id":253098758
      }
    ],
    "negative_ctxs":[
      {
        "title":"Baseline Models for Pronoun Prediction and Pronoun-Aware Translation",
        "text":"This paper presents baseline models for the cross-lingual pronoun prediction task and the pronoun-focused translation task at Dis-coMT 2015. We present simple yet effective classifiers for the former and discuss the impact of various contextual features on the prediction performance. In the translation task we rely on the document-level decoder Docent and a cross-sentence target language-model over selected words based on the parts-of-speech of the aligned source language words.",
        "id":14151586
      },
      {
        "title":"Embedding Learning Through Multilingual Concept Induction",
        "text":"We present a new method for estimating vector space representations of words: embedding learning by concept induction. We test this method on a highly parallel corpus and learn semantic representations of words in 1259 different languages in a single common space. An extensive experimental evaluation on crosslingual word similarity and sentiment analysis indicates that concept-based multilingual embedding learning performs better than previous approaches.",
        "id":46885888
      },
      {
        "title":"One Year of Contender: What Have We Learned about Assessing and Tuning Industrial Spoken Dialog Systems?",
        "text":"A lot. Since inception of Contender, a machine learning method tailored for computerassisted decision making in industrial spoken dialog systems, it was rolled out in over 200 instances throughout our applications processing nearly 40 million calls. The net effect of this data-driven method is a significantly increased system performance gaining about 100,000 additional automated calls every month.",
        "id":15978451
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that investigates how undetectable backdoor attacks are in NLP models?",
    "positive_ctxs":[
      {
        "title":"Rethinking Stealthiness of Backdoor Attack against NLP Models",
        "text":"Recent researches have shown that large natural language processing (NLP) models are vulnerable to a kind of security threat called the Backdoor Attack. Backdoor attacked models can achieve good performance on clean test sets but perform badly on those input sentences injected with designed trigger words. In this work, we point out a potential problem of current backdoor attacking research: its evaluation ignores the stealthiness of backdoor attacks, and most of existing backdoor attacking methods are not stealthy either to system deployers or to system users. To address this issue, we first propose two additional stealthiness-based metrics to make the backdoor attacking evaluation more credible. We further propose a novel word-based backdoor attacking method based on negative data augmentation and modifying word embeddings, making an important step towards achieving stealthy backdoor attacking. Experiments on sentiment analysis and toxic detection tasks show that our method is much stealthier while maintaining pretty good attacking performance. Our code is available at https:\/\/github.com\/lancopku\/SOS.",
        "id":236459933
      }
    ],
    "negative_ctxs":[
      {
        "title":"A Valency Dictionary Architecture for Machine Translation",
        "text":"This research is aimed at developing a valency dictionary architecture to comprehensively list the full range of alternations associated with a given predicate sense, both efficiently and robustly. The architecture is designed to incorporate all information available in current on-line resources, as well as additional features such as argument status, grammatical relations, and an augmented case-role representation. Words are divided into senses, which are distinguished on semantic grounds, depending on the core lexical meaning of the verb. Each sense may have one or more alternations, thus keeping the number of senses manageable, while allowing for systematic variation in the lexical realization. Individual syntactic case frames are indexed back to the basic semantic argument component of the given predicate sense.",
        "id":14993809
      },
      {
        "title":"The Shared Corpora Working Group Report",
        "text":"We seek to identify a limited amount of representative corpora, suitable for annotation by the computational linguistics annotation community. Our hope is that a wide variety of annotation will be undertaken on the same corpora, which would facilitate: (1) the comparison of annotation schemes; (2) the merging of information represented by various annotation schemes; (3) the emergence of NLP systems that use information in multiple annotation schemes; and (4) the adoption of various types of best practice in corpus annotation. Such best practices would include: (a) clearer demarcation of phenomena being annotated; (b) the use of particular test corpora to determine whether a particular annotation task can feasibly achieve good agreement scores; (c) The use of underlying models for representing annotation content that facilitate merging, comparison, and analysis; and (d) To the extent possible, the use of common annotation categories or a mapping among categories for the same phenomenon used by different annotation groups.This study will focus on the problem of identifying such corpora as well as the suitability of two candidate corpora: the Open portion of the American National Corpus(Ide and Macleod, 2001;Ide and Suderman, 2004)and the \"Controversial\" portions of the WikipediaXML corpus .",
        "id":16694202
      },
      {
        "title":"Neural Attention Model for Classification of Sentences that Support Promoting\/Suppressing Relationship",
        "text":"Evidences that support a claim \"a subject phrase promotes or suppresses a value\" help in making a rational decision. We aim to construct a model that can classify if a particular evidence supports a claim of a promoting\/suppressing relationship given an arbitrary subject-value pair. In this paper, we propose a recurrent neural network (RNN) with an attention model to classify such evidences. We incorporated a word embedding technique in an attention model such that our method generalizes for never-encountered subjects and value phrases. Benchmarks showed that the method outperforms conventional methods in evidence classification tasks.",
        "id":5703887
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that investigates improving retrieval-based conversational systems using by combining masked language modeling and relevance classification objectives?",
    "positive_ctxs":[
      {
        "title":"Fine-grained Post-training for Improving Retrieval-based Dialogue Systems",
        "text":"Retrieval-based dialogue systems display an outstanding performance when pre-trained language models are used, which includes bidirectional encoder representations from transformers (BERT). During the multi-turn response selection, BERT focuses on training the relationship between the context with multiple utterances and the response. However, this method of training is insufficient when considering the relations between each utterance in the context. This leads to a problem of not completely understanding the context flow that is required to select a response. To address this issue, we propose a new fine-grained post-training method that reflects the characteristics of the multi-turn dialogue. Specifically, the model learns the utterance level interactions by training every short context-response pair in a dialogue session. Furthermore, by using a new training objective, the utterance relevance classification, the model understands the semantic relevance and coherence between the dialogue utterances. Experimental results show that our model achieves new state-of-the-art with significant margins on three benchmark datasets. This suggests that the fine-grained post-training method is highly effective for the response selection task. 1",
        "id":235097662
      }
    ],
    "negative_ctxs":[
      {
        "title":"A HYBRID APPROACH TO ADAPTIVE STATISTICAL LANGUAGE MODELING",
        "text":"We desert'be our latest attempt at adaptive language modeling. At the heart of our approach is a Maximum Entropy (ME) model which inc.orlxnates many knowledge sources in a consistent manner. The other components are a selective unigram cache, a conditional bigram cache, and a conventionalstatic trigram. We describe the knowledge sources used to build such a model with ARPA's official WSJ corpus, and report on perplexity and word error rate results obtained with it. Then, three different adaptation paradigms are discussed, and an additional experiment, based on AP wire data, is used to compare them.",
        "id":70704
      },
      {
        "title":"Entropy Reduction correlates with temporal lobe activity",
        "text":"Using the Entropy Reduction incremental complexity metric, we relate high gamma power signals from the brains of epileptic patients to incremental stages of syntactic analysis in English and French. We find that signals recorded intracranially from the anterior Inferior Temporal Sulcus (aITS) and the posterior Inferior Temporal Gyrus (pITG) correlate with wordby-word Entropy Reduction values derived from phrase structure grammars for those languages. In the anterior region, this correlation persists even in combination with surprisal co-predictors from PCFG and ngram models. The result confirms the idea that the brain's temporal lobe houses a parsing function, one whose incremental processing difficulty profile reflects changes in grammatical uncertainty.",
        "id":3915049
      },
      {
        "title":"Published as a conference paper at ICLR 2023 NO REASON FOR NO SUPERVISION: IMPROVED GENERALIZATION IN SUPERVISED MODELS",
        "text":"We consider the problem of training a deep neural network on a given classification task, e.g., ImageNet-1K (IN1K), so that it excels at both the training task as well as at other (future) transfer tasks. These two seemingly contradictory properties impose a trade-off between improving the model's generalization and maintaining its performance on the original task. Models trained with self-supervised learning tend to generalize better than their supervised counterparts for transfer learning; yet, they still lag behind supervised models on IN1K. In this paper, we propose a supervised learning setup that leverages the best of both worlds. We extensively analyze supervised training using multi-scale crops for data augmentation and an expendable projector head, and reveal that the design of the projector allows us to control the trade-off between performance on the training task and transferability. We further replace the last layer of class weights with class prototypes computed on the fly using a memory bank and derive two models: t-ReX that achieves a new state of the art for transfer learning and outperforms top methods such as DINO and PAWS on IN1K, and t-ReX* that matches the highly optimized RSB-A1 model on IN1K while performing better on transfer tasks. Code and pretrained models: https:\/\/europe.naverlabs.com\/t-rex",
        "id":257482840
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that investigates the use of past dialogues for enhancing query expansion in conversational search systems?",
    "positive_ctxs":[
      {
        "title":"Open-Domain Question Answering Goes Conversational via Question Rewriting",
        "text":"We introduce a new dataset for Question Rewriting in Conversational Context (QReCC), which contains 14K conversations with 80K question-answer pairs. The task in QReCC is to find answers to conversational questions within a collection of 10M web pages (split into 54M passages). Answers to questions in the same conversation may be distributed across several web pages. QReCC provides annotations that allow us to train and evaluate individual subtasks of question rewriting, passage retrieval and reading comprehension required for the end-to-end conversational question answering (QA) task. We report the effectiveness of a strong baseline approach that combines the state-of-the-art model for question rewriting, and competitive models for open-domain QA. Our results set the first baseline for the QReCC dataset with F1 of 19.10, compared to the human upper bound of 75.45, indicating the difficulty of the setup and a large room for improvement.Baseline model answerThe most common tools used were daggers and spear points, used for hunting, and hand axesAnswer F119.05",
        "id":222290679
      }
    ],
    "negative_ctxs":[
      {
        "title":"Reviewing Natural Language Processing Research",
        "text":"The reviewing procedure has been identified as one of the major issues in the current situation of the NLP field. While it is implicitly assumed that junior researcher learn reviewing during their PhD project, this might not always be the case. Additionally, with the growing NLP community and the efforts in the context of widening the NLP community, researchers joining the field might not have the opportunity to practise reviewing. This tutorial fills in this gap by providing an opportunity to learn the basics of reviewing. Also more experienced researchers might find this tutorial interesting to revise their reviewing procedure.",
        "id":233365118
      },
      {
        "title":"AUTOMATIC CONSTRUCTION OF CLEAN BROAD-COVERAGE TRANSLATION LEXICONS",
        "text":"Word-level translational equivalences can be extracted from parallel texts by surprisingly simple statistical techniques. However, these techniques are easily fooled by indirect associations | pairs of unrelated words whose statistical properties resemble those of mutual translations. Indirect associations pollute the resulting translation lexicons, drastically reducing their precision. This paper presents an iterative lexicon cleaning method. On each iteration, most of the remaining incorrect lexicon entries are ltered out, without signi cant degradation in recall. This lexicon cleaning technique can produce translation lexicons with recall and precision both exceeding 90%, as well as dictionary-sized translation lexicons that are over 99% correct.",
        "id":1649762
      },
      {
        "title":"MilaNLP at WASSA 2021: Does BERT Feel Sad When You Cry?",
        "text":"The paper describes the MilaNLP team's submission (Bocconi University, Milan) in the WASSA 2021 Shared Task on Empathy Detection and Emotion Classification. We focus on Track 2 -Emotion Classification -which consists of predicting the emotion of reactions to English news stories at the essay-level. We test different models based on multi-task and multi-input frameworks. The goal was to better exploit all the correlated information given in the data set. We find, though, that empathy as an auxiliary task in multi-task learning and demographic attributes as additional input provide worse performance with respect to singletask learning. While the result is competitive in terms of the competition, our results suggest that emotion and empathy are not related tasks -at least for the purpose of prediction.",
        "id":233365252
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that investigates training BERT-based classifiers with Wikipedia data for zero-shot text classification in open domains?",
    "positive_ctxs":[
      {
        "title":"Towards Open-Domain Topic Classification",
        "text":"We introduce an open-domain topic classification system that accepts user-defined taxonomy in real time. Users will be able to classify a text snippet with respect to any candidate labels they want, and get instant response from our web interface. To obtain such flexibility, we build the backend model in a zero-shot way. By training on a new dataset constructed from Wikipedia, our label-aware text classifier can effectively utilize implicit knowledge in the pretrained language model to handle labels it has never seen before. We evaluate our model across four datasets from various domains with different label sets. Experiments show that the model significantly improves over existing zero-shot baselines in open-domain scenarios, and performs competitively with weaklysupervised models trained on in-domain data. 12",
        "id":250390995
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":160586750
      },
      {
        "title":"How to evaluate necessary cooperative systems of terminology building?",
        "text":"Terminology building cannot be considered as a full automated process but rather as a cooperative task between terminological tools and terminologists. Identifying terms in a technical domain is a matter of word usage and expert agreement. We point out the problem of the evaluation of such tools: their quality and their contribution to the terminology building is difficult to estimate and cannot be fully evaluated with usual precision and recall measures. We aim at evaluating more globally their technical aspects and their usability. We give a non-exhaustive list of the features of such evaluation. Then, we apply them on four terminological systems.",
        "id":10477942
      },
      {
        "title":"Fine-grained Post-training for Improving Retrieval-based Dialogue Systems",
        "text":"Retrieval-based dialogue systems display an outstanding performance when pre-trained language models are used, which includes bidirectional encoder representations from transformers (BERT). During the multi-turn response selection, BERT focuses on training the relationship between the context with multiple utterances and the response. However, this method of training is insufficient when considering the relations between each utterance in the context. This leads to a problem of not completely understanding the context flow that is required to select a response. To address this issue, we propose a new fine-grained post-training method that reflects the characteristics of the multi-turn dialogue. Specifically, the model learns the utterance level interactions by training every short context-response pair in a dialogue session. Furthermore, by using a new training objective, the utterance relevance classification, the model understands the semantic relevance and coherence between the dialogue utterances. Experimental results show that our model achieves new state-of-the-art with significant margins on three benchmark datasets. This suggests that the fine-grained post-training method is highly effective for the response selection task. 1",
        "id":235097662
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that offers an in-depth examination of the shortcomings associated with pretraining evaluation measures such as BERTScore, particularly regarding their alignment with human evaluative assessments?",
    "positive_ctxs":[
      {
        "title":"A Fine-Grained Analysis of BERTScore",
        "text":"BERTScore(Zhang et al., 2020), a recently proposed automatic metric for machine translation quality, uses BERT (Devlin et al., 2019), a large pre-trained language model to evaluate candidate translations with respect to a gold translation. Taking advantage of BERT's semantic and syntactic abilities, BERTScore seeks to avoid the flaws of earlier approaches like BLEU, instead scoring candidate translations based on their semantic similarity to the gold sentence. However, BERT is not infallible; while its performance on NLP tasks set a new state of the art in general, studies of specific syntactic and semantic phenomena have shown where BERT's performance deviates from that of humans more generally.This naturally raises the questions we address in this paper: what are the strengths and weaknesses of BERTScore? Do they relate to known weaknesses on the part of BERT? We find that while BERTScore can detect when a candidate differs from a reference in important content words, it is less sensitive to smaller errors, especially if the candidate is lexically or stylistically similar to the reference.",
        "id":245855878
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":222141011
      },
      {
        "title":"Thai Grapheme-Based Speech Recognition",
        "text":"In this paper we present the results for building a grapheme-based speech recognition system for Thai. We experiment with different settings for the initial context independent system, different number of acoustic models and different contexts for the speech unit. In addition, we investigate the potential of an enhanced tree clustering method as a way of sharing parameters across models. We compare our system with two phoneme-based systems; one that uses a hand-crafted dictionary and another that uses an automatically generated dictionary. Experiment results show that the grapheme-based system with enhanced tree clustering outperforms the phoneme-based system using an automatically generated dictionary, and has comparable results to the phoneme-based system with the handcrafted dictionary.",
        "id":5153552
      },
      {
        "title":"Learning to Identify Metaphors from a Corpus of Proverbs",
        "text":"In this paper, we experiment with a resource consisting of metaphorically annotated proverbs on the task of word-level metaphor recognition. We observe that existing feature sets do not perform well on this data. We design a novel set of features to better capture the peculiar nature of proverbs and we demonstrate that these new features are significantly more effective on the metaphorically dense proverb data.",
        "id":14555476
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that shows multilingual language models can understand plural\/singular verb agreement across multiple languages?",
    "positive_ctxs":[
      {
        "title":"Data-driven Cross-lingual Syntax: An Agreement Study with Massively Multilingual Models",
        "text":"Massively multilingual models such as mBERT and XLM-R are increasingly valued in Natural Language Processing research and applications, due to their ability to tackle the uneven distribution of resources available for different languages. The models' ability to process multiple languages relying on a shared set of parameters raises the question of whether the grammatical knowledge they extracted during pre-training can be considered as a data-driven cross-lingual grammar. The present work studies the inner workings of mBERT and XLM-R in order to test the cross-lingual consistency of the individual neural units that respond to a precise syntactic phenomenon, that is, number agreement, in five languages (English, German, French, Hebrew, Russian). We found that there is a significant overlap in the latent dimensions that encode agreement across the languages we considered. This overlap is larger (a) for long-vis-à-vis shortdistance agreement and (b) when considering XLM-R as compared to mBERT, and peaks in the intermediate layers of the network. We further show that a small set of syntax-sensitive neurons can capture agreement violations across languages; however, their contribution is not decisive in agreement processing.",
        "id":255894238
      }
    ],
    "negative_ctxs":[
      {
        "title":"Cross-Lingual Morphological Tagging for Low-Resource Languages",
        "text":"Morphologically rich languages often lack the annotated linguistic resources required to develop accurate natural language processing tools. We propose models suitable for training morphological taggers with rich tagsets for low-resource languages without using direct supervision. Our approach extends existing approaches of projecting part-of-speech tags across languages, using bitext to infer constraints on the possible tags for a given word type or token. We propose a tagging model using Wsabie, a discriminative embeddingbased model with rank-based learning. In our evaluation on 11 languages, on average this model performs on par with a baseline weakly-supervised HMM, while being more scalable. Multilingual experiments show that the method performs best when projecting between related language pairs. Despite the inherently lossy projection, we show that the morphological tags predicted by our models improve the downstream performance of a parser by +0.6 LAS on average.",
        "id":565312
      },
      {
        "title":"",
        "text":"",
        "id":232021826
      },
      {
        "title":"",
        "text":"",
        "id":219303177
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that trains language models specifically on mental health-related social media data and the model is helpful for identifying mental health issues?",
    "positive_ctxs":[
      {
        "title":"MentalBERT: Publicly Available Pretrained Language Models for Mental Healthcare",
        "text":"Mental health is a critical issue in modern society, and mental disorders could sometimes turn to suicidal ideation without adequate treatment. Early detection of mental disorders and suicidal ideation from social content provides a potential way for effective social intervention. Recent advances in pretrained contextualized language representations have promoted the development of several domain-specific pretrained models and facilitated several downstream applications. However, there are no existing pretrained language models for mental healthcare. This paper trains and releases two pretrained masked language models, i.e., MentalBERT and MentalRoBERTa, to benefit machine learning for the mental healthcare research community. Besides, we evaluate our trained domain-specific models and several variants of pretrained language models on several mental disorder detection benchmarks and demonstrate that language representations pretrained in the target domain improve the performance of mental health detection tasks.",
        "id":240288892
      }
    ],
    "negative_ctxs":[
      {
        "title":"Constructing a Model of Dialog",
        "text":"In the present paper communicative cycle",
        "id":12405045
      },
      {
        "title":"Designing a Symbolic Intermediate Representation for Neural Surface Realization",
        "text":"Generated output from neural NLG systems often contain errors such as hallucination, repetition or contradiction. This work focuses on designing a symbolic intermediate representation to be used in multi-stage neural generation with the intention of reducing the frequency of failed outputs. We show that surface realization from this intermediate representation is of high quality and when the full system is applied to the E2E dataset it outperforms the winner of the E2E challenge. Furthermore, by breaking out the surface realization step from typically end-to-end neural systems, we also provide a framework for non-neural content selection and planning systems to potentially take advantage of semi-supervised pretraining of neural surface realization models.",
        "id":166228140
      },
      {
        "title":"",
        "text":"",
        "id":46941014
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you suggest research that tries to interpret how bi-directional RNNs manage to carry out Named Entity Recognition (NER) tasks?",
    "positive_ctxs":[
      {
        "title":"Interpretability Analysis for Named Entity Recognition to Understand System Predictions and How They Can Improve under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics",
        "text":"Named entity recognition systems achieve remarkable performance on domains such as English news. It is natural to ask: What are these models actually learning to achieve this? Are they merely memorizing the names themselves? Or are they capable of interpreting the text and inferring the correct entity type from the linguistic context? We examine these questions by contrasting the performance of several variants of architectures for named entity recognition, with some provided only representations of the context as features. We experiment with GloVebased BiLSTM-CRF as well as BERT. We find that context does influence predictions, but the main factor driving high performance is learning the named tokens themselves. Furthermore, we find that BERT is not always better at recognizing predictive contexts compared to a BiLSTM-CRF model. We enlist human annotators to evaluate the feasibility of inferring entity types from context alone and find that humans are also mostly unable to infer entity types for the majority of examples on which the context-only system made errors. However, there is room for improvement: A system should be able to recognize any named entity in a predictive context correctly Submission Volume 47, Number 1 and our experiments indicate that current systems may be improved by such capability. Our human study also revealed that systems and humans do not always learn the same contextual clues, and context-only systems are sometimes correct even when humans fail to recognize the entity type from the context. Finally, we find that one issue contributing to model errors is the use of \"entangled\" representations that encode both contextual and local token information into a single vector, which can obscure clues. Our results suggest that designing models that explicitly operate over representations of local inputs and context, respectively, may in some cases improve performance. In light of these and related findings, we highlight directions for future work.",
        "id":215548904
      }
    ],
    "negative_ctxs":[
      {
        "title":"\"",
        "text":"",
        "id":15360530
      },
      {
        "title":"Learning Lexicon Models from Search Logs for Query Expansion",
        "text":"This paper explores log-based query expansion (QE) models for Web search. Three lexicon models are proposed to bridge the lexical gap between Web documents and user queries. These models are trained on pairs of user queries and titles of clicked documents. Evaluations on a real world data set show that the lexicon models, integrated into a ranker-based QE system, not only significantly improve the document retrieval performance but also outperform two state-of-the-art log-based QE methods.",
        "id":15849128
      },
      {
        "title":"",
        "text":"",
        "id":13963354
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Could you suggest some work that develops multimodal models with contrastive learning approaches?",
    "positive_ctxs":[
      {
        "title":"UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning",
        "text":"Existed pre-training methods either focus on single-modal tasks or multi-modal tasks, and cannot effectively adapt to each other. They can only utilize single-modal data (i.e., text or image) or limited multi-modal data (i.e., image-text pairs). In this work, we propose a UNIfied-MOdal pre-training architecture, namely UNIMO, which can effectively adapt to both single-modal and multi-modal understanding and generation tasks. Large scale of free text corpus and image collections are utilized to improve the capability of visual and textual understanding, and crossmodal contrastive learning (CMCL) is leveraged to align the textual and visual information into a unified semantic space, over a corpus of image-text pairs augmented with related images and texts. With the help of rich non-paired single-modal data, our model is able to learn more generalizable representations, by allowing textual knowledge and visual knowledge to enhance each other in the unified semantic space. The experimental results show that UNIMO greatly improves the performance of several singlemodal and multi-modal downstream tasks. Our code and pre-trained models are public at https:\/\/github.com\/PaddlePaddle\/ Research\/tree\/master\/NLP\/UNIMO.",
        "id":229924402
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":219306605
      },
      {
        "title":"Propa-L: a Semantic Filtering Service from a Lexical Network Created using Games With A Purpose",
        "text":"This article presents Propa-L, a freely accessible Web service that allows to semantically filter a lexical network. The language resources behind the service are dynamic and created through Games With A Purpose. We show an example of application of this service: the generation of a list of keywords for parental filtering on the Web, but many others can be envisaged. Moreover, the propagation algorithm we present here can be applied to any lexical network, in any language.",
        "id":43612512
      },
      {
        "title":"Substring-based Transliteration with Conditional Random Fields",
        "text":"Motivated by phrase-based translation research, we present a transliteration system where characters are grouped into substrings to be mapped atomically into the target language. We show how this substring representation can be incorporated into a Conditional Random Field model that uses local context and phonemic information.",
        "id":167261
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Could you suggest studies focused on emotion-classification within group dialogues?",
    "positive_ctxs":[
      {
        "title":"MuCDN: Mutual Conversational Detachment Network for Emotion Recognition in Multi-Party Conversations",
        "text":"As an emerging research topic in natural language processing community, emotion recognition in multi-party conversations has attained increasing interest. Previous approaches that focus either on dyadic or multi-party scenarios exert much effort to cope with the challenge of emotional dynamics and achieve appealing results. However, since emotional interactions among speakers are often more complicated within the entangled multi-party conversations, these works are limited in capturing effective emotional clues in conversational context. In this work, we propose Mutual Conversational Detachment Network (MuCDN) to clearly understand the conversational context by separating conversations into detached threads. Specifically, two detachment ways are devised to perform context and speaker-specific modeling within detached threads and they are bridged through a mutual module. Experimental results on two datasets show that our model achieves better performance over the baseline models.",
        "id":252819095
      }
    ],
    "negative_ctxs":[
      {
        "title":"A Subcategorization Acquisition System for French Verbs",
        "text":"This paper presents a system capable of automatically acquiring subcategorization frames (SCFs) for French verbs from the analysis of large corpora. We applied the system to a large newspaper corpus (consisting of 10 years of the French newspaper 'Le Monde') and acquired subcategorization information for 3267 verbs. The system learned 286 SCF types for these verbs. From the analysis of 25 representative verbs, we obtained 0.82 precision, 0.59 recall and 0.69 F-measure. These results are comparable with those reported in recent related work.",
        "id":13989926
      },
      {
        "title":"Continually Detection, Rapidly React: Unseen Rumors Detection based on Continual Prompt-Tuning",
        "text":"Since open social platforms allow for a large and continuous flow of unverified information, rumors can emerge unexpectedly and spread quickly. However, existing rumor detection (RD) models often assume the same training and testing distributions and can not",
        "id":247596973
      },
      {
        "title":"An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training",
        "text":"Pre-training large language models has become a standard in the natural language processing community. Such models are pretrained on generic data (e.g. BookCorpus and English Wikipedia) and often fine-tuned on tasks in the same domain. However, in order to achieve state-of-the-art performance on out of domain tasks such as clinical named entity recognition and relation extraction, additional in domain pre-training is required. In practice, staged multi-domain pre-training presents performance deterioration in the form of catastrophic forgetting (CF) when evaluated on a generic benchmark such as GLUE. In this paper we conduct an empirical investigation into known methods to mitigate CF. We find that elastic weight consolidation provides best overall scores yielding only a 0.33% drop in performance across seven generic tasks while remaining competitive in bio-medical tasks. Furthermore, we explore gradient and latent clustering based data selection techniques to improve coverage when using elastic weight consolidation and experience replay methods.",
        "id":222125077
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Has any research explored using other off-the-shelf summarization techniques to improve neural abstractive summarization?",
    "positive_ctxs":[
      {
        "title":"GSum: A General Framework for Guided Neural Abstractive Summarization",
        "text":"Neural abstractive summarization models are flexible and can produce coherent summaries, but they are sometimes unfaithful and can be difficult to control. While previous studies attempt to provide different types of guidance to control the output and increase faithfulness, it is not clear how these strategies compare and contrast to each other. In this paper, we propose a general and extensible guided summarization framework (GSum) that can effectively take different kinds of external guidance as input, and we perform experiments across several different varieties. Experiments demonstrate that this model is effective, achieving state-of-the-art performance according to ROUGE on 4 popular summarization datasets when using highlighted sentences as guidance. In addition, we show that our guided model can generate more faithful summaries and demonstrate how different types of guidance generate qualitatively different summaries, lending a degree of controllability to the learned models. 1",
        "id":223953416
      }
    ],
    "negative_ctxs":[
      {
        "title":"Modular and Efficient Top-Down Parsing for Ambiguous Left-Recursive Grammars",
        "text":"In functional and logic programming, parsers can be built as modular executable specifications of grammars, using parser combinators and definite clause grammars respectively. These techniques are based on top-down backtracking search. Commonly used implementations are inefficient for ambiguous languages, cannot accommodate left-recursive grammars, and require exponential space to represent parse trees for highly ambiguous input. Memoization is known to improve efficiency, and work by other researchers has had some success in accommodating left recursion. This paper combines aspects of previous approaches and presents a method by which parsers can be built as modular and efficient executable specifications of ambiguous grammars containing unconstrained left recursion.",
        "id":18749678
      },
      {
        "title":"A Pipeline Japanese Entity Linking System with Embedding Features",
        "text":"Entity linking (EL) is the task of connecting mentions in texts to entities in a large-scale knowledge base such as Wikipedia. In this paper, we present a pipeline system for Japanese EL which consists of two standard components, namely candidate generation and candidate ranking. We investigate several techniques for each component, using a recently developed Japanese EL corpus. For candidate generation, we find that a concept dictionary using anchor texts of Wikipedia is more effective than methods based on surface similarity. For candidate ranking, we verify that a set of features used in English EL is effective in Japanese EL as well. In addition, by using a corpus that links Japanese mentions to Japanese Wikipedia entries, we are able to get rich context information from Japanese Wikipedia articles and benefit mention disambiguation. It was not directly possible with previous EL corpora, which associate mentions to English Wikipedia entities. We take this advantage by exploring several embedding models that encode context information of Wikipedia entities, and show that they improve candidate ranking. As a whole, our system achieves 82.27% accuracy, significantly outperforming previous work.",
        "id":12207182
      },
      {
        "title":"Learning Bilingual Lexicons from Monolingual Corpora",
        "text":"We present a method for learning bilingual translation lexicons from monolingual corpora. Word types in each language are characterized by purely monolingual features, such as context counts and orthographic substrings. Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings. We show that high-precision lexicons can be learned in a variety of language pairs and from a range of corpus types.",
        "id":7185434
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Has any research tried to mitigate overfitting in weakly-supervised settings by introducing an adversarial framework where the influence of the labeling function is a hyperparameter for the feature representation?",
    "positive_ctxs":[
      {
        "title":"KnowMAN: Weakly Supervised Multinomial Adversarial Networks",
        "text":"The absence of labeled data for training neural models is often addressed by leveraging knowledge about the specific task, resulting in heuristic but noisy labels. The knowledge is captured in labeling functions, which detect certain regularities or patterns in the training samples and annotate corresponding labels for training. This process of weakly supervised training may result in an over-reliance on the signals captured by the labeling functions and hinder models to exploit other signals or to generalize well. We propose KnowMAN, an adversarial scheme that enables to control influence of signals associated with specific labeling functions. KnowMAN forces the network to learn representations that are invariant to those signals and to pick up other signals that are more generally associated with an output label. KnowMAN strongly improves results compared to direct weakly supervised learning with a pre-trained transformer language model and a feature-based baseline.",
        "id":237532398
      }
    ],
    "negative_ctxs":[
      {
        "title":"Using Structured Content Plans for Fine-grained Syntactic Control in Pretrained Language Model Generation",
        "text":"Large pretrained language models offer powerful generation capabilities, but cannot be reliably controlled at a sub-sentential level. We propose to make such fine-grained control possible in pretrained LMs by generating text directly from a semantic representation, Abstract Meaning Representation (AMR), which is augmented at the node level with syntactic control tags. We experiment with English-language generation of three modes of syntax relevant to the framing of a sentence -verb voice, verb tense, and realization of human entities -and demonstrate that they can be reliably controlled, even in settings that diverge drastically from the training distribution. These syntactic aspects contribute to how information is framed in text, something that is important for applications such as summarization which aim to highlight salient information.",
        "id":252586876
      },
      {
        "title":"Measure Country-Level Socio-Economic Indicators with Streaming News: An Empirical Study",
        "text":"Socio-economic conditions are difficult to measure. For example, the U.S. Bureau of Labor Statistics needs to conduct large-scale household surveys regularly to track the unemployment rate, an indicator widely used by economists and policy makers. We argue that events reported in streaming news can be used as \"micro-sensors\" for measuring socio-economic conditions. Similar to collecting surveys and then counting answers, it is possible to measure a socio-economic indicator by counting related events. In this paper, we propose Event-Centric Indicator Measure (ECIM), a novel approach to measure socioeconomic indicators with events. We empirically demonstrate strong correlations between ECIM values to several representative indicators in socio-economic research.",
        "id":202786163
      },
      {
        "title":"The Ubiqus English-Inuktitut System for WMT20",
        "text":"This paper describes Ubiqus' submission to the WMT20 English-Inuktitut shared news translation task. Our main system, and only submission, is based on a multilingual approach, jointly training a Transformer model on several agglutinative languages.The English-Inuktitut translation task is challenging at every step, from data selection, preparation and tokenization to quality evaluation down the line. Difficulties emerge both because of the peculiarities of the Inuktitut language as well as the low-resource context.",
        "id":227013461
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Has any study explored the zero-shot extraction of persona characteristics within conversational dialogues?",
    "positive_ctxs":[
      {
        "title":"PAED: Zero-Shot Persona Attribute Extraction in Dialogues",
        "text":"Persona attribute extraction is critical for personalized human-computer interaction. Dialogue is an important medium that communicates and delivers persona information. Although there is a public dataset for triplet-based persona attribute extraction from conversations, its automatically generated labels present many issues, including unspecific relations and inconsistent annotations. We fix such issues by leveraging more reliable text-label matching criteria to generate high-quality data for persona attribute extraction. We also propose a contrastive learning-and generation-based model with a novel hard negative sampling strategy for generalized zero-shot persona attribute extraction. We benchmark our model with stateof-the-art baselines on our dataset and a public dataset, showing outstanding accuracy gains. Our sampling strategy also exceeds others by a large margin in persona attribute extraction.",
        "id":258971765
      }
    ],
    "negative_ctxs":[
      {
        "title":"Association Norms of German Noun Compounds",
        "text":"This paper introduces association norms of German noun compounds as a lexical-semantic resource for cognitive and computational linguistics research on compositionality. Based on an existing database of German noun compounds, we collected human associations to the compounds and their constituents within a web experiment. The current study describes the collection process and a part-of-speech analysis of the association resource. In addition, we demonstrate that the associations provide insight into the semantic properties of the compounds, and perform a case study that predicts the degree of compositionality of the experiment compound nouns, as relying on the norms. Applying a comparatively simple measure of association overlap, we reach a Spearman rank correlation coefficient of rs = 0.5228, p < .000001, when comparing our predictions with human judgements.",
        "id":17986977
      },
      {
        "title":"Extrinsic Corpus Evaluation with a Collocation Dictionary Task",
        "text":"The NLP researcher or application-builder often wonders \"what corpus should I use, or should I build one of my own? If I build one of my own, how will I know if I have done a good job?\" Currently there is very little help available for them. They are in need of a framework for evaluating corpora. We develop such a framework, in relation to corpora which aim for good coverage of 'general language'. The task we set is automatic creation of a publication-quality collocations dictionary. For a sample of 100 headwords of Czech and 100 of English, we identify a gold standard dataset of (ideally) all the collocations that should appear for these headwords in such a dictionary. The datasets are being made available alongside this paper. We then use them to determine precision and recall for a range of corpora, with a range of parameters.",
        "id":5716928
      },
      {
        "title":"Multimodal Dialogue State Tracking",
        "text":"Designed for tracking user goals in dialogues, a dialogue state tracker is an essential component in a dialogue system. However, the research of dialogue state tracking has largely been limited to unimodality, in which slots and slot values are limited by knowledge domains (e.g. restaurant domain with slots of restaurant name and price range) and are defined by specific database schema. In this paper, we propose to extend the definition of dialogue state tracking to multimodality. Specifically, we introduce a novel dialogue state tracking task to track the information of visual objects that are mentioned in video-grounded dialogues. Each new dialogue utterance may introduce a new video segment, new visual objects, or new object attributes and a state tracker is required to update these information slots accordingly. We created a new synthetic benchmark and designed a novel baseline, Video-Dialogue Transformer Network (VDTN), for this task. VDTN combines both object-level features and segment-level features and learns contextual dependencies between videos and dialogues to generate multimodal dialogue states. We optimized VDTN for a state generation task as well as a self-supervised video understanding task which recovers video segment or object representations. Finally, we trained VDTN to use the decoded states in a response prediction task. Together with comprehensive ablation and qualitative analysis, we discovered interesting insights towards building more capable multimodal dialogue systems.",
        "id":249712518
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Has there been any research that uses multiple models to learn the preferences of individual annotators, and then ensembles these models to obtain majority-vote preference scores while also having an uncertainty measure?",
    "positive_ctxs":[
      {
        "title":"Dealing with Disagreements: Looking Beyond the Majority Vote in Subjective Annotations",
        "text":"Majority voting and averaging are common approaches used to resolve annotator disagreements and derive single ground truth labels from multiple annotations. However, annotators may systematically disagree with one another, often reflecting their individual biases and values, especially in the case of subjective tasks such as detecting affect, aggression, and hate speech. Annotator disagreements may capture important nuances in such tasks that are often ignored while aggregating annotations to a single ground truth. In order to address this, we investigate the efficacy of multi-annotator models. In particular, our multi-task based approach treats predicting each annotators' judgements as separate subtasks, while sharing a common learned representation of the task. We show that this approach yields same or better performance than aggregating labels in the data prior to training across seven different binary classification tasks. Our approach also provides a way to estimate uncertainty in predictions, which we demonstrate better correlate with annotation disagreements than traditional methods. Being able to model uncertainty is especially useful in deployment scenarios where knowing when not to make a prediction is important.92",
        "id":238634750
      }
    ],
    "negative_ctxs":[
      {
        "title":"Automatic Generation of Information State Update Dialogue Systems that Dynamically Create Voice XML, as Demonstrated on the iPhone",
        "text":"We demonstrate DUDE 1 (Dialogue and Understanding Development Environment), a prototype development environment that automatically generates dialogue systems from business-user resources and databases. These generated spoken dialogue systems (SDS) are then deployed on an industry standard Voice XML platform. Specifically, the deployed system works by dynamically generating context-sensitive Voice XML pages. The dialogue move of each page is determined in real time by the dialogue manager, which is an Information State Update engine. Firstly, we will demonstrate the development environment which includes automatic generation of speech recognition grammars for robust interpretation of spontaneous speech, and uses the application database to generate lexical entries and grammar rules. A simple graphical interface allows users (i.e. developers) to easily and quickly create and the modify SDS without the need for expensive service providers. Secondly, we will demonstrate the deployed system which enables participants to call up and speak to the SDS recently created. We will also show a pre-built application running on the iPhone and Google Android phone for searching for places such as restaurants, hotels and museums.",
        "id":672449
      },
      {
        "title":"Developing a Fine-Grained Corpus for a Less-resourced Language: the case of Kurdish *",
        "text":"Kurdish is a less-resourced language consisting of different dialects written in various scripts. Approximately 30 million people in different countries speak the language. The lack of corpora is one of the main obstacles in Kurdish language processing. In this paper, we present KTC-the Kurdish Textbooks Corpus, which is composed of 31 K-12 textbooks in Sorani dialect. The corpus is normalized and categorized into 12 educational subjects containing 693,800 tokens (110,297 types). Our resource is publicly available for non-commercial use under the CC BY-NC-SA 4.0 license 1 .",
        "id":200031907
      },
      {
        "title":"Context-Sensitive Electronic Dictionaries",
        "text":"This paper introduces a context-sensitive electronic dictionary that provides translations for any piece of text displayed on a computer screen, without requiring user interaction. This is achieved through a process of three phases: text acquisition from the screen, morpho-syntactic analysis of the context of the selected word, and the dictionary lookup. As with other similar tools available, this program usually works with dictionaries adapted from one or more printed dictionaries. To implement context sensitive features, however, traditional dictionary entries need to be restructured. By splitting up entries into smaller pieces and indexing them in a special way, the program is able to display a restricted set of information that is relevant to the context. Based on the information in the dictionaries, the program is able to recognize-even discontinuous-multiword expressions on the screen. The program has three major features which we believe make it unique for the time being, and which the development focused on: linguistic flexibility (stemming, morphological analysis and shallow parsing), open architecture (three major architectural blocks, all replaceable along public documented APIs), and flexible user interface (replaceable dictionaries, direct user feedback). In this paper, we assess the functional requirements of a context-sensitive dictionary as a start; then we explain the program's three phases of operation, focusing on the implementation of the lexicons and the context-sensitive features. We conclude the paper by comparing our tool to other similar publicly available products, and summarize plans for future development.",
        "id":11852941
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Has there been any work that improves the work on integrated gradients by leveraging interpolation strategies to improve gradient accuracies?",
    "positive_ctxs":[
      {
        "title":"Discretized Integrated Gradients for Explaining Language Models",
        "text":"As a prominent attribution-based explanation algorithm, Integrated Gradients (IG) is widely adopted due to its desirable explanation axioms and the ease of gradient computation. It measures feature importance by averaging the model's output gradient interpolated along a straight-line path in the input data space. However, such straight-line interpolated points are not representative of text data due to the inherent discreteness of the word embedding space. This questions the faithfulness of the gradients computed at the interpolated points and consequently, the quality of the generated explanations. Here we propose Discretized Integrated Gradients (DIG), which allows effective attribution along non-linear interpolation paths. We develop two interpolation strategies for the discrete word embedding space that generates interpolation points that lie close to actual words in the embedding space, yielding more faithful gradient computation. We demonstrate the effectiveness of DIG over IG through experimental and human evaluations on multiple sentiment classification datasets. We provide the source code of DIG to encourage reproducible research 1 . . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
        "id":237363853
      }
    ],
    "negative_ctxs":[
      {
        "title":"Rethinking representations: A log-bilinear model of phonotactics",
        "text":"Models of phonotactics include subsegmental representations in order to generalize to unattested sequences.These representations can be encoded in at least two ways: as discrete, phonetically-based features, or as continuous, distribution-based representations induced from the statistical patterning of sounds.Because phonological theory typically assumes that representations are discrete, past work has reduced continuous representations to discrete ones, which eliminates potentially relevant information.In this paper we present a model of phonotactics that can use continuous representations directly, and show that this approach yields competitive performance on modeling experimental judgments of English sonority sequencing.The proposed model broadens the space of possible phonotactic models by removing requirements for discrete features, and is a step towards an integrated picture of phonotactic learning based on distributional statistics and continuous representations.",
        "id":259325958
      },
      {
        "title":"SEMI-PARAMETRIC INDUCING POINT NETWORKS AND NEURAL PROCESSES",
        "text":"We introduce semi-parametric inducing point networks (SPIN), a general-purpose architecture that can query the training set at inference time in a compute-efficient manner. Semi-parametric architectures are typically more compact than parametric models, but their computational complexity is often quadratic. In contrast, SPIN attains linear complexity via a cross-attention mechanism between datapoints inspired by inducing point methods. Querying large training sets can be particularly useful in meta-learning, as it unlocks additional training signal, but often exceeds the scaling limits of existing models. We use SPIN as the basis of the Inducing Point Neural Process, a probabilistic model which supports large contexts in metalearning and achieves high accuracy where existing models fail. In our experiments, SPIN reduces memory requirements, improves accuracy across a range of metalearning tasks, and improves state-of-the-art performance on an important practical problem, genotype imputation.",
        "id":257834209
      },
      {
        "title":"Improving Text Normalization via Unsupervised Model and Discriminative Reranking",
        "text":"Various models have been developed for normalizing informal text. In this paper, we propose two methods to improve normalization performance. First is an unsupervised approach that automatically identifies pairs of a non-standard token and proper word from a large unlabeled corpus. We use semantic similarity based on continuous word vector representation, together with other surface similarity measurement. Second we propose a reranking strategy to combine the results from different systems. This allows us to incorporate information that is hard to model in individual systems as well as consider multiple systems to generate a final rank for a test case. Both word-and sentence-level optimization schemes are explored in this study. We evaluate our approach on data sets used in prior studies, and demonstrate that our proposed methods perform better than the state-of-the-art systems.",
        "id":9342522
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Have any new metrics been developed to assess the factual alignment of machine-generated summaries with their original source texts?",
    "positive_ctxs":[
      {
        "title":"QuestEval: Summarization Asks for Fact-based Evaluation",
        "text":"Summarization evaluation remains an open research problem: current metrics such as ROUGE are known to be limited and to correlate poorly with human judgments. To alleviate this issue, recent work has proposed evaluation metrics which rely on question answering models to assess whether a summary contains all the relevant information in its source document. Though promising, the proposed approaches have so far failed to correlate better than ROUGE with human judgments.In this paper, we extend previous approaches and propose a unified framework, named QuestEval.In contrast to established metrics such as ROUGE or BERTScore, QuestEval does not require any groundtruth reference. Nonetheless, QuestEval substantially improves the correlation with human judgments over four evaluation dimensions (consistency, coherence, fluency, and relevance), as shown in the extensive experiments we report.",
        "id":233219059
      },
      {
        "title":"SUMMAC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization",
        "text":"In the summarization domain, a key requirement for summaries is to be factually consistent with the input document. Previous work has found that natural language inference (NLI) models do not perform competitively when applied to inconsistency detection. In this work, we revisit the use of NLI for inconsistency detection, finding that past work suffered from a mismatch in input granularity between NLI datasets (sentence-level), and inconsistency detection (document level). We provide a highly effective and light-weight method called SUMMAC CONV that enables NLI models to be successfully used for this task by segmenting documents into sentence units and aggregating scores between pairs of sentences. We furthermore introduce a new benchmark called SUMMAC (Summary Consistency) which consists of six large inconsistency detection datasets. On this dataset, SUMMAC Conv obtains state-of-the-art results with a balanced accuracy of 74.4%, a 5% improvement compared with prior work. *",
        "id":244345901
      }
    ],
    "negative_ctxs":[
      {
        "title":"CityU-DAC: Disambiguating Sentiment-Ambiguous Adjectives within Context",
        "text":"This paper describes our system participating in task 18 of SemEval-2010, i.e. disambiguating Sentiment-Ambiguous Adjectives (SAAs). To disambiguating SAAs, we compare the machine learning-based and lexiconbased methods in our submissions: 1) Maximum entropy is used to train classifiers based on the annotated Chinese data from the NTCIR opinion analysis tasks, and the clause-level and sentence-level classifiers are compared; 2) For the lexicon-based method, we first classify the adjectives into two classes: intensifiers (i.e. adjectives intensifying the intensity of context) and suppressors (i.e. adjectives decreasing the intensity of context), and then use the polarity of context to get the SAAs' contextual polarity based on a sentiment lexicon. The results show that the performance of maximum entropy is not quite high due to little training data; on the other hand, the lexicon-based method could improve the precision by considering the polarity of context.",
        "id":7860647
      },
      {
        "title":"Selecting Text Features for Gene Name Classification: from Documents to Terms",
        "text":"In this paper we discuss the performance of a text-based classification approach by comparing different types of features. We consider the automatic classification of gene names from the molecular biology literature, by using a support-vector machine method. Classification features range from words, lemmas and stems, to automatically extracted terms. Also, simple co-occurrences of genes within documents are considered. The preliminary experiments performed on a set of 3,000 S. cerevisiae gene names and 53,000 Medline abstracts have shown that using domain-specific terms can improve the performance compared to the standard bag-of-words approach, in particular for genes classified with higher confidence, and for under-represented classes.",
        "id":2613800
      },
      {
        "title":"",
        "text":"",
        "id":218973740
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Have any papers tried to address the background-shift problem in named entity recognition by identifying non-entity type tokens belonging to old entity types through knowledge distillation from an old model?",
    "positive_ctxs":[
      {
        "title":"Distilling Causal Effect from Miscellaneous Other-Class for Continual Named Entity Recognition",
        "text":"Continual Learning for Named Entity Recognition (CL-NER) aims to learn a growing number of entity types over time from a stream of data. However, simply learning Other-Class in the same way as new entity types amplifies the catastrophic forgetting and leads to a substantial performance drop. The main cause behind this is that Other-Class samples usually contain old entity types, and the old knowledge in these Other-Class samples is not preserved properly. Thanks to the causal inference, we identify that the forgetting is caused by the missing causal effect from the old data. To this end, we propose a unified causal framework to retrieve the causality from both new entity types and Other-Class. Furthermore, we apply curriculum learning to mitigate the impact of label noise and introduce a self-adaptive weight for balancing the causal effects between new entity types and Other-Class. Experimental results on three benchmark datasets show that our method outperforms the state-of-theart method by a large margin. Moreover, our method can be combined with the existing stateof-the-art methods to improve the performance in CL-NER. 1",
        "id":252780822
      }
    ],
    "negative_ctxs":[
      {
        "title":"Say the Right Thing Right: Ethics Issues in Natural Language Generation Systems",
        "text":"We discuss the ethical implications of Natural Language Generation systems. We use one particular system as a case study to identify and classify issues, and we provide an ethics checklist, in the hope that future system designers may benefit from conducting their own ethics reviews based on our checklist.",
        "id":15291488
      },
      {
        "title":"Temporal\/Locative WHs and Null-P Incorporation*",
        "text":"This paper is an investigation on the categorial status of locative and temporal WHs. We argue, based on empirical data, for the null P hypothesis proposed byHuang (1982), and against the proposal of Murasugi and Saito (1992) that when and where are sentential arguments. We suggest that the problem raised inMurasugi and Saito (1992)for the null P analysis can be solved by the null P incorporation hypothesis. We further address the issues related to the applications of the P incorporation.",
        "id":16650242
      },
      {
        "title":"Novel Feature Discovery for Task-Oriented Dialog Systems",
        "text":"A novel feature represents a cluster of semantically equivalent novel user requests e.g., requests to play a song on a service or reading user's messages. Detecting and supporting novel features is crucial towards wider adoption of dialog systems by end users. Intuitively, features are represented by a combination of intents, slot types and\/or their values. For example, while playing a song is a feature represented by a single intent (PlayMusic) only, playing a song on a service is another feature represented by the combination of PlayMusic intent and ServiceName slot type. Prior work on novelty detection limits the scope of features to those represented by novel single intents, leading to (1) giant clusters spanning several user-perceived fine-grained features belonging to the same intent, (2) incoherent interpretation of clusters from users' perspective (no direct connection to some user-perceived feature), and (3) missing those features spanning several intents. In this work, we introduce feature discovery as opposed to single intent discovery, which aims at discovering novel features spanning a combination of intents and slots, and present a technique for discovering novel features from user utterances. Experiments on two datasets demonstrate the effectiveness of our approach and consistently show its ability to detect novel features.",
        "id":258378330
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Have any recent publications explored the use of neural network methods, like transformer architectures, in creating novel readability metrics?",
    "positive_ctxs":[
      {
        "title":"Supervised and Unsupervised Neural Approaches to Text Readability Senja Pollak",
        "text":"We present a set of novel neural supervised and unsupervised approaches for determining the readability of documents. In the unsupervised setting, we leverage neural language models, whereas in the supervised setting, three different neural classification architectures are tested. We show that the proposed neural unsupervised approach is robust, transferable across languages, and allows adaptation to a specific readability task and data set. By systematic comparison of several neural architectures on a number of benchmark and new labeled readability data sets in two languages, this study also offers a comprehensive analysis of different neural approaches to readability classification. We expose their strengths and weaknesses, compare their performance to current state-of-the-art classification approaches to readability, which in most cases still rely on extensive feature engineering, and propose possibilities for improvements.",
        "id":198967754
      }
    ],
    "negative_ctxs":[
      {
        "title":"A New Representation Model for the Automatic Recognition and Translation of Arabic Named Entities with NooJ",
        "text":"Recognition and translation of named entities (NEs) are two current research topics with regard to the proliferation of electronic documents exchanged through the Internet. The need to assimilate these documents through NLP tools has become necessary and interesting. Moreover, the formal or semiformal modeling of these NEs may intervene in both processes of recognition and translation. Indeed, the modeling makes more reliable the constitution of linguistic resources, limits the impact of linguistic specificities and facilitates transformations from one representation to another. In this context, we propose an approach of recognition and translation based on a representation model of Arabic NEs and a set of transducers resolving morphological and syntactical phenomena.",
        "id":8821039
      },
      {
        "title":"UDeasy: a Tool for Querying Treebanks in CoNLL-U Format",
        "text":"Many tools are available to query a dependency treebank, but they require the users to know a query language. This paper presents UDeasy, an application whose main goal is to allow the users to easily query and extract patterns from a dependency treebank in CoNLL-U format. To do this, users are prompted in a series of dialogs to enter relevant information about syntactic nodes, their properties, relationship, and positions.",
        "id":252624768
      },
      {
        "title":"Adversarial Text Generation via Sequence Contrast Discrimination",
        "text":"In this paper, we propose a sequence contrast loss driven text generation framework, which learns the difference between real texts and generated texts and uses that difference. Specifically, our discriminator contains a discriminative sequence generator instead of a binary classifier, and measures the 'relative realism' of generated texts against real texts by making use of them simultaneously. Moreover, our generator uses discriminative sequences to directly improve itself, which not only replaces the gradient propagation process from the discriminator to the generator, but also avoids the time-consuming sampling process of estimating rewards in some previous methods. We conduct extensive experiments with various metrics, substantiating that our framework brings improvements in terms of training stability and the quality of generated texts.",
        "id":226284010
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Have any research efforts been made to gather dialogue data via crowdworkers to enhance conversational information retrieval systems?",
    "positive_ctxs":[
      {
        "title":"DuRecDial 2.0: A Bilingual Parallel Corpus for Conversational Recommendation",
        "text":"In this paper, we provide a bilingual parallel human-to-human recommendation dialog dataset (DuRecDial 2.0) to enable researchers to explore a challenging task of multilingual and cross-lingual conversational recommendation. The difference between DuRecDial 2.0 and existing conversational recommendation datasets is that the data item (Profile, Goal, Knowledge, Context, Response) in DuRecDial 2.0 is annotated in two languages, both English and Chinese, while other datasets are built with the setting of a single language. We collect 8.2k dialogs aligned across English and Chinese languages (16.5k dialogs and 255k utterances in total) that are annotated by crowdsourced workers with strict quality control procedure. We then build monolingual, multilingual, and cross-lingual conversational recommendation baselines on DuRecDial 2.0. Experiment results show that the use of additional English data can bring performance improvement for Chinese conversational recommendation, indicating the benefits of DuRecDial 2.0. Finally, this dataset provides a challenging testbed for future studies of monolingual, multilingual, and cross-lingual conversational recommendation.",
        "id":237571370
      }
    ],
    "negative_ctxs":[
      {
        "title":"The Bank of Swedish -a general background",
        "text":"",
        "id":6622478
      },
      {
        "title":"MT and Topic-Based Techniques to Enhance Speech Recognition Systems for Professional Translators",
        "text":"Our principle objective was to reduce the error rate of speech recognition systems used by professional translators. Our work concentrated on Spanish-to-English translation. In a baseline study we estimated the error rate o1' an off-the-shelf recognizer to he 9.98%. in this paper we describe two independent methods ot' improving speech recognizers: a machine translation (MT) method and a topic-based one. An evaluation of the MT method suggests that the vocabulary used for recognition cannot be completely restricted to the set of translations produced by the MT system and a more sophisticated constraint system must be used. An ewduation of the topic-based method showed significanl error rate reduction, to 5.07%.",
        "id":2496958
      },
      {
        "title":"Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods",
        "text":"We introduce a new benchmark, WinoBias, for coreference resolution focused on gender bias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing coreference benchmark datasets. Our dataset and code are available at http:\/\/winobias.org.",
        "id":4952494
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Have any research papers been published on models for representing sentences in under-resourced languages like Slovenian or Romanian?",
    "positive_ctxs":[
      {
        "title":"Language-agnostic BERT Sentence Embedding",
        "text":"While BERT is an effective method for learning monolingual sentence embeddings for semantic similarity and embedding based transfer learning (Reimers and Gurevych, 2019), BERT based cross-lingual sentence embeddings have yet to be explored. We systematically investigate methods for learning multilingual sentence embeddings by combining the best methods for learning monolingual and cross-lingual representations including: masked language modeling (MLM), translation language modeling (TLM) (Conneau and Lample, 2019), dual encoder translation ranking (Guo et al., 2018), and additive margin softmax(Yang et al., 2019a). We show that introducing a pre-trained multilingual language model dramatically reduces the amount of parallel training data required to achieve good performance by 80%. Composing the best of these methods produces a model that achieves 83.7% bi-text retrieval accuracy over 112 languages on Tatoeba, well above the 65.5% achieved by Artetxe and Schwenk (2019b), while still performing competitively on monolingual transfer learning benchmarks (Conneau and Kiela, 2018). Parallel data mined from CommonCrawl using our best model is shown to train competitive NMT models for en-zh and en-de. We publicly release our best multilingual sentence embedding model for 109+ languages at https:\/\/tfhub.dev\/ google\/LaBSE. * Equal contributions. † Work done while at Google.",
        "id":220347683
      }
    ],
    "negative_ctxs":[
      {
        "title":"Learning Disentangled Textual Representations via Statistical Measures of Similarity",
        "text":"When working with textual data, a natural application of disentangled representations is fair classification where the goal is to make predictions without being biased (or influenced) by sensitive attributes that may be present in the data (e.g., age, gender or race). Dominant approaches to disentangle a sensitive attribute from textual representations rely on learning simultaneously a penalization term that involves either an adversarial loss (e.g., a discriminator) or an information measure (e.g., mutual information). However, these methods require the training of a deep neural network with several parameter updates for each update of the representation model. As a matter of fact, the resulting nested optimization loop is both time consuming, adding complexity to the optimization dynamic, and requires a fine hyperparameter selection (e.g., learning rates, architecture). In this work, we introduce a family of regularizers for learning disentangled representations that do not require training. These regularizers are based on statistical measures of similarity between the conditional probability distributions with respect to the sensitive attributes. Our novel regularizers do not require additional training, are faster and do not involve additional tuning while achieving better results both when combined with pretrained and randomly initialized text encoders.",
        "id":248572355
      },
      {
        "title":"",
        "text":"",
        "id":233365334
      },
      {
        "title":"A Preliminary Study of Statistically Predictive Syntactic Complexity Features and Manual Simplifications in Basque",
        "text":"In this paper, we present a comparative analysis of statistically predictive syntactic features of complexity and the treatment of these features by humans when simplifying texts. To that end, we have used a list of the most five statistically predictive features obtained automatically and the Corpus of Basque Simplified Texts (CBST) to analyse how the syntactic phenomena in these features have been manually simplified. Our aim is to go beyond the descriptions of operations found in the corpus and relate the multidisciplinary findings to understand text complexity from different points of view. We also present some issues that can be important when analysing linguistic complexity.This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:\/\/ creativecommons.org\/licenses\/by\/4.0\/",
        "id":16856890
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Have any research papers collected feedback from real users who were using LLMs for scientific writing?",
    "positive_ctxs":[
      {
        "title":"Sparks: Inspiration for Science Writing using Language Models",
        "text":"Large-scale language models are rapidly improving, performing well on a variety of tasks with little to no customization. In this work we investigate how language models can support science writing, a challenging writing task that is both open-ended and highly constrained. We present a system for generating \"sparks\", sentences related to a scientific concept intended to inspire writers. We run a user study with 13 STEM graduate students and find three main use cases of sparks-inspiration, translation, and perspective-each of which correlates with a unique interaction pattern. We also find that while participants were more likely to select higher quality sparks, the overall quality of sparks seen by a given participant did not correlate with their satisfaction with the tool. 1",
        "id":239009871
      }
    ],
    "negative_ctxs":[
      {
        "title":"Identifying Bad Semantic Neighbors for Improving Distributional Thesauri",
        "text":"Distributional thesauri are now widely used in a large number of Natural Language Processing tasks. However, they are far from containing only interesting semantic relations. As a consequence, improving such thesaurus is an important issue that is mainly tackled indirectly through the improvement of semantic similarity measures. In this article, we propose a more direct approach focusing on the identification of the neighbors of a thesaurus entry that are not semantically linked to this entry. This identification relies on a discriminative classifier trained from unsupervised selected examples for building a distributional model of the entry in texts. Its bad neighbors are found by applying this classifier to a representative set of occurrences of each of these neighbors. We evaluate the interest of this method for a large set of English nouns with various frequencies.",
        "id":1669441
      },
      {
        "title":"Multi-view Models for Political Ideology Detection of News Articles",
        "text":"A news article's title, content and link structure often reveal its political ideology. However, most existing works on automatic political ideology detection only leverage textual cues. Drawing inspiration from recent advances in neural inference, we propose a novel attention based multi-view model to leverage cues from all of the above views to identify the ideology evinced by a news article. Our model draws on advances in representation learning in natural language processing and network science to capture cues from both textual content and the network structure of news articles. We empirically evaluate our model against a battery of baselines and show that our model outperforms state of the art by 10 percentage points F1 score.",
        "id":52183735
      },
      {
        "title":"Towards Convenient Bi-Directional Grammar Formalisms",
        "text":"P. Newmzrn",
        "id":5127127
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Have any research papers critically analyzed the performance speed of non-autoregressive translation models compared to autoregressive models",
    "positive_ctxs":[
      {
        "title":"Non-Autoregressive Machine Translation: It's Not as Fast as it Seems",
        "text":"Efficient machine translation models are commercially important as they can increase inference speeds, and reduce costs and carbon emissions. Recently, there has been much interest in non-autoregressive (NAR) models, which promise faster translation. In parallel to the research on NAR models, there have been successful attempts to create optimized autoregressive models as part of the WMT shared task on efficient translation. In this paper, we point out flaws in the evaluation methodology present in the literature on NAR models and we provide a fair comparison between a stateof-the-art NAR model and the autoregressive submissions to the shared task. We make the case for consistent evaluation of NAR models, and also for the importance of comparing NAR models with other widely used methods for improving efficiency. We run experiments with a connectionist-temporal-classification-based (CTC) NAR model implemented in C++ and compare it with AR models using wall clock times. Our results show that, although NAR models are faster on GPUs, with small batch sizes, they are almost always slower under more realistic usage conditions. We call for more realistic and extensive evaluation of NAR models in future work.",
        "id":248512647
      }
    ],
    "negative_ctxs":[
      {
        "title":"Can SMT and RBMT Improve each other's Performance?-An Experiment with English-Hindi Translation",
        "text":"Rule-based machine translation (RBMT) and Statistical machine translation (SMT) are two well-known approaches for translation which have their own benefits.System architecture of SMT often complements RBMT, and the vice-versa. In this paper, we propose an effective method of serial coupling where we attempt to build a hybrid model that exploits the benefits of both the architectures. The first part of coupling is used to obtain good lexical selection and robustness, second part is used to improve syntax and the final one is designed to combine other modules along with the best phrase reordering. Our experiments on a English-Hindi product domain dataset show the effectiveness of the proposed approach with improvement in BLEU score.",
        "id":2910089
      },
      {
        "title":"Conversational Robots: Building Blocks for Grounding Word Meaning",
        "text":"How can we build robots that engage in fluid spoken conversations with people, moving beyond canned responses to words and towards actually understanding? As a step towards addressing this question, we introduce a robotic architecture that provides a basis for grounding word meanings. The architecture provides perceptual, procedural, and affordance representations for grounding words. A perceptuallycoupled on-line simulator enables sensorymotor representations that can shift points of view. Held together, we show that this architecture provides a rich set of data structures and procedures that provide the foundations for grounding the meaning of certain classes of words. 1  We acknowledge that the words in this example, like most words, have numerous additional connotations that are not captured by the representations that we have suggested. For example, words such as touch, heavy and blue can be used metaphorically to refer to emotional actions and states. Things are not always physical perceivable objects, my usually indicates possession, and so forth. Barwise and Perry use the phrase \"efficiency of language\" to highlight the situation-dependent reusability of words and utterances(Barwise and Perry, 1983). However, for",
        "id":358056
      },
      {
        "title":"Représentation vectorielle de paires de verbes pour la prédiction de relations lexicales",
        "text":"Dans cet article, nous proposons un modèle de représentations vectorielles de paire de mots, obtenues à partir d'une adaptation du modèle Skip-gram de Word2vec. Ce modèle est utilisé pour générer des vecteurs de paires de verbes, entraînées sur le corpus de textes anglais Ukwac. Les vecteurs sont évalués sur les données ConceptNet & EACL, sur une tâche de classification de relations lexicales. Nous comparons les résultats obtenus avec les vecteurs paires à des modèles utilisant des vecteurs mots, et testons l'évaluation avec des verbes dans leur forme originale et dans leur forme lemmatisée. Enfin, nous présentons des expériences où ces vecteurs paires sont utilisés sur une tâche d'identification de relation discursive entre deux segments de texte. Nos résultats sur le corpus anglais Penn Discourse Treebank, démontrent l'importance de l'information verbale pour la tâche, et la complémentarité de ces vecteurs paires avec les connecteurs discursifs des relations.ABSTRACTVerb-pairs embeddings for discourse relation predictionThis paper proposes a model to obtain vector representations of pairs of words, obtained from an adaptation of the Word2vec Skip-gram Model. This model is used to generate embeddings for pairs of verbs, trained on the english corpus Ukwac. The pair-embeddings are then evaluated on a classification task, where the goal is to predict the lexical relation between the input pair of words. The scores obtained on this task with the pair-embeddings are compared with the scores obtained with individual word-embeddings and with pairs of lemmatized verbs. Finally, the pair-embeddings are used on the discourse relation prediction task, on the Penn Discourse Treebank dataset, revealing the relevance of verbs for this task, and the complementarity between the verbs and the discourse connective.MOTS-CLÉS : Vecteur mot, vecteur relation de verbes, analyse de texte, prédiction de relation du discours.KEYWORDS: Word embedding, relation embedding between verbs, text analysis, discourse relation prediction.Actes de la 6e conférence conjointe Journées d'Études sur la Parole (JEP, 33e édition), Traitement Automatique des Langues Naturelles (TALN, 27e édition), Rencontre des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (RÉCITAL, 22e édition) Nancy, France, 08-19 juin 2020 Volume 3 : Rencontre des Étudiants Chercheurs en Informatique pour le TAL, pages 179-192. hal : hal-02786199. Cette oeuvre est mise à disposition sous licence Attribution 4.0 International. objectif de construire des représentations de relations entre une paire de mots, et d'évaluer la qualité de ces représentations dans différentes tâches. Pour ce faire, nous présentons un modèle permettant d'entraîner des représentations vectorielles de relations entre des paires de mots.Les représentations vectorielles de mots sont très utilisées en traitement automatique des langues depuis les années 1990, l'idée étant que les mots sémantiquement proches se retrouvent proches dans l'espace vectoriel des mots. On retrouve souvent dans la littérature le terme word embedding ou \"plongement lexical\" pour qualifier ces représentations vectorielles. Ces représentations sont fondées sur l'hypothèse distributionnelle(Harris, 1954): \"les lexèmes possédant un contexte linguistique similaire ont un sens similaire.\". On peut par exemple obtenir des représentations vectorielles de mots en construisant une matrice de co-occurrences, en comptant le nombre d'occurrences de mots dans le contexte d'autres mots(Turney & Pantel, 2010). L'inconveniant de cette méthode étant néanmmoins la taille importante des vecteurs obtenus, et leurs composantes qui sont majoritairement nulles.",
        "id":221373817
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Have any research papers examined the efficacy of multilingual text-to-text transformers across various languages, particularly those less represented in pretraining corpora?",
    "positive_ctxs":[
      {
        "title":"mT5: A massively multilingual pre-trained text-to-text transformer",
        "text":"The recent \"Text-to-Text Transfer Transformer\" (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We describe the design and modified training of mT5 and demonstrate its stateof-the-art performance on many multilingual benchmarks. All of the code and model checkpoints used in this work are publicly available. 1 * Equal Contribution.Please direct correspondence to",
        "id":225040574
      }
    ],
    "negative_ctxs":[
      {
        "title":"Disambiguation of English Contractions for Machine Translation of TV Subtitles",
        "text":"This paper presents a disambiguation method for English apostrophe+s contractions. They occur frequently in subtitles and pose special difficulties for Machine Translation. We propose to disambiguate these contractions in a pre-processing step and show that this leads to improved translation quality.",
        "id":17096699
      },
      {
        "title":"Using Confidential Data for Domain Adaptation of Neural Machine Translation",
        "text":"We study the problem of domain adaptation in Neural Machine Translation (NMT) when domain-specific data cannot be shared due to confidentiality or copyright issues. As a first step, we propose to fragment data into phrase pairs and use a random sample to fine-tune a generic NMT model instead of the full sentences. Despite the loss of long segments for the sake of confidentiality protection, we find that NMT quality can considerably benefit from this adaptation, and that further gains can be obtained with a simple tagging technique.",
        "id":235097220
      },
      {
        "title":"A Pragmatic Chinese Word Segmentation System",
        "text":"This paper presents our work for participation in the Third International Chinese Word Segmentation Bakeoff. We apply several processing approaches according to the corresponding sub-tasks, which are exhibited in real natural language. In our system, Trigram model with smoothing algorithm is the core module in word segmentation, and Maximum Entropy model is the basic model in Named Entity Recognition task. The experiment indicates that this system achieves Fmeasure 96.8% in MSRA open test in the third SIGHAN-2006 bakeoff.",
        "id":12689722
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Have any research papers examined whether using language models for providing evidence in fact-checking systems risks propagating biases?",
    "positive_ctxs":[
      {
        "title":"Towards Few-Shot Fact-Checking via Perplexity",
        "text":"Few-shot learning has drawn researchers' attention to overcome the problem of data scarcity. Recently, large pre-trained language models have shown great performance in fewshot learning for various downstream tasks, such as question answering and machine translation. Nevertheless, little exploration has been made to achieve few-shot learning for the fact-checking task. However, fact-checking is an important problem, especially when the amount of information online is growing exponentially every day. In this paper, we propose a new way of utilizing the powerful transfer learning ability of a language model via a perplexity score. The most notable strength of our methodology lies in its capability in fewshot learning. With only two training samples, our methodology can already outperform the Major Class baseline by more than an absolute 10% on the F1-Macro metric across multiple datasets. Through experiments, we empirically verify the plausibility of the rather surprising usage of the perplexity score in the context of fact-checking and highlight the strength of our few-shot methodology by comparing it to strong fine-tuning-based baseline models. Moreover, we construct and publicly release two new fact-checking datasets related to",
        "id":232258000
      }
    ],
    "negative_ctxs":[
      {
        "title":"Sentence Completion Tests for Training and Assessment in a Computational Linguistics Curriculum",
        "text":"This paper presents a novel type of test, halfway between multiple-choice and free-form text, used for training and assessment in several courses in a Computational Linguistics curriculum. We will describe the principles of the test, the different ways in which it can be used by learners, and the tools developed for authoring. Use of this type of test is not limited to the field of Computational Linguistics. Wherever text heavy or even picture based topics are taught use of this type of test is possible.",
        "id":45598818
      },
      {
        "title":"Parsing and Subcategorization Data",
        "text":"In this paper, we compare the performance of a state-of-the-art statistical parser(Bikel, 2004)in parsing written and spoken language and in generating subcategorization cues from written and spoken language. Although Bikel's parser achieves a higher accuracy for parsing written language, it achieves a higher accuracy when extracting subcategorization cues from spoken language. Additionally, we explore the utility of punctuation in helping parsing and extraction of subcategorization cues. Our experiments show that punctuation is of little help in parsing spoken language and extracting subcategorization cues from spoken language. This indicates that there is no need to add punctuation in transcribing spoken corpora simply in order to help parsers.",
        "id":5436060
      },
      {
        "title":"MANDARIN LOANWORD PHONOLOGY AND OPTIMALITY THEORY: EVIDENCE FROM TRANSLITERATED AMERICAN STATE NAMES AND TYPHOON NAMES",
        "text":"This paper aims to examine how the consonant clusters and illicit codas are modified in Mandarin loanwords transliterated from English, and to argue that no rules need to be involved and that a purely constraint-based approach-within the framework of Optimality Theory-can explain the data.The data collected from transliterated American state names and state names display the onset-coda inconsistency in Mandarin loanwords. All the onset consonant clusters in the data are faithfully parsed into Mandarin syllables, with inserting vowels to shun consonant cluster. The coda clusters and illicit codas are generally parsed. However, the coda liquids may be parsed in some cases but unparsed in others. The preference of insertion in Mandarin loanwords can be explained by the interaction between the constraints--*COMPLEX, CODACON, MAX-I0, and DEMO. The distinctive behavior of coda liquids can be accounted for by the effect of MINwD.",
        "id":2523524
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Have any research papers explored methods to improve BERT's efficiency on long-text tasks, such as early exiting or self-distillation strategies?",
    "positive_ctxs":[
      {
        "title":"BERxiT: Early Exiting for BERT with Better Fine-Tuning and Extension to Regression",
        "text":"The slow speed of BERT has motivated much research on accelerating its inference, and the early exiting idea has been proposed to make trade-offs between model quality and efficiency. This paper aims to address two weaknesses of previous work: (1) existing fine-tuning strategies for early exiting models fail to take full advantage of BERT; (2) methods to make exiting decisions are limited to classification tasks. We propose a more advanced fine-tuning strategy and a learning-toexit module that extends early exiting to tasks other than classification. Experiments demonstrate improved early exiting for BERT, with better trade-offs obtained by the proposed finetuning strategy, successful application to regression tasks, and the possibility to combine it with other acceleration methods. Source code can be found at https:\/\/github.com\/ castorini\/berxit.",
        "id":233189542
      }
    ],
    "negative_ctxs":[
      {
        "title":"ForceReader: a BERT-based Interactive Machine Reading Comprehension Model with Attention Separation",
        "text":"The release of BERT revolutionized the development of NLP. Various BERT-based reading comprehension models have been proposed, thus updating the performance ranking of reading comprehension tasks. However, the above BERT-based models inherently employ BERT's combined input method, representing the input question and paragraph as a single packed sequence, without further modification for reading comprehension. This paper makes an in-depth analysis of this input method, proposes a problem of this approach. We call it attention deconcentration. Accordingly, this paper proposes ForceReader, a BERT-based interactive machine reading comprehension model. First, ForceReader proposes a novel solution called the Attention Separation Representation to respond to attention deconcentration. Moreover, starting from the logical nature of reading comprehension tasks, ForceReader adopts Multi-mode Reading, and Interactive Reasoning strategy. For the calculation of attention, ForceReader employs Conditional Background Attention to solve the lack of the overall context semantic after the separation of attention. As an integral model, ForceReader shows a significant improvement in reading comprehension tasks compared to BERT. Moreover, this paper makes detailed visual analyses of the attention and propose strategies accordingly. This may be another argument to the explanations of the attention.This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:\/\/ creativecommons.org\/licenses\/by\/4.0\/.",
        "id":227231086
      },
      {
        "title":"",
        "text":"",
        "id":227230617
      },
      {
        "title":"Natural-language Interactive Narratives in Imaginal Exposure Therapy for Obsessive-Compulsive Disorder",
        "text":"Obsessive-compulsive disorder (OCD) is an anxiety-based disorder that affects around 2.5% of the population. A common treatment for OCD is exposure therapy, where the patient repeatedly confronts a feared experience, which has the long-term effect of decreasing their anxiety. Some exposures consist of reading and writing stories about an imagined anxiety-provoking scenario. In this paper, we present a technology that enables patients to interactively contribute to exposure stories by supplying natural language input (typed or spoken) that advances a scenario. This interactivity could potentially increase the patient's sense of immersion in an exposure and contribute to its success. We introduce the NLP task behind processing inputs to predict new events in the scenario, and describe our initial approach. We then illustrate the future possibility of this work with an example of an exposure scenario authored with our application.",
        "id":30226677
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Have any research papers introduced a dedicated pre-training architecture designed to improve dense retrieval system efficacy?",
    "positive_ctxs":[
      {
        "title":"Condenser: a Pre-training Architecture for Dense Retrieval",
        "text":"Pre-trained Transformer language models (LM) have become go-to text representation encoders. Prior research fine-tunes deep LMs to encode text sequences such as sentences and passages into single dense vector representations for efficient text comparison and retrieval. However, dense encoders require a lot of data and sophisticated techniques to effectively train and suffer in low data situations. This paper finds a key reason is that standard LMs' internal attention structure is not ready-to-use for dense encoders, which needs to aggregate text information into the dense representation. We propose to pre-train towards dense encoder with a novel Transformer architecture, Condenser, where LM prediction CONditions on DENSE Representation. Our experiments show Condenser improves over standard LM by large margins on various text retrieval and similarity tasks. 1",
        "id":237581068
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"Feature analysis of Chinese characters plays a prominent role in \"character-based\" education. However, there is an urgent need for a text analysis system for processing the difficulty of composing components for characters, primarily based on Chinese learners' performance. To meet this need, the purpose of this research was to provide such a system by adapting a data-driven approach. Based on Chen et al.'s (2011) Chinese Orthography Database, this research has designed and developed an system: Character Difficulty -Research on Multi-features (CD-ROM). This system provides three functions: (1) analyzing a text and providing its difficulty regarding Chinese characters; (2) decomposing characters into components and calculating the frequency of components based on the analyzed text; and (3) affording component-deriving characters based on the analyzed text and downloadable images as teaching materials. With these functions highlighting multi-level features of characters, this system has the potential to benefit the fields of Chinese character instruction, Chinese orthographic learning, and Chinese natural language processing. 關鍵字：漢字難度、漢字特徵、字本位教學 、 漢字教學系統",
        "id":253628222
      },
      {
        "title":"Developing further speech recognition resources for Welsh",
        "text":"This paper reports on ongoing research into developing large-vocabulary continuous speech recognition (LVCSR) for the Welsh language. We address data design issues and the method for data collection using a purposely designed application for mobile devices. We also discuss the application of the data including the design and collection of a small speech corpus to cover the commands used to control a robotic arm in Welsh on a Raspberry Pi computer the licensing of the project and our hopes for the application of the project resources to other languages.",
        "id":62466051
      },
      {
        "title":"Building Better Open-Source Tools to Support Fairness in Automated Scoring",
        "text":"Automated scoring of written and spoken responses is an NLP application that can significantly impact lives especially when deployed as part of high-stakes tests such as the GRE® and the TOEFL®. Ethical considerations require that automated scoring algorithms treat all testtakers fairly. The educational measurement community has done significant research on fairness in assessments and automated scoring systems must incorporate their recommendations. The best way to do that is by making available automated, non-proprietary tools to NLP researchers that directly incorporate these recommendations and generate the analyses needed to help identify and resolve biases in their scoring systems. In this paper, we attempt to provide such a solution.",
        "id":8761461
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Have any research papers investigated human capacity to distinguish AI-generated text from human-authored text?",
    "positive_ctxs":[
      {
        "title":"All That's 'Human' Is Not Gold: Evaluating Human Evaluation of Generated Text",
        "text":"Human evaluations are typically considered the gold standard in natural language generation, but as models' fluency improves, how well can evaluators detect and judge machinegenerated text? We run a study assessing nonexperts' ability to distinguish between humanand machine-authored text (GPT2 and GPT3) in three domains (stories, news articles, and recipes). We find that, without training, evaluators distinguished between GPT3-and humanauthored text at random chance level. We explore three approaches for quickly training evaluators to better identify GPT3-authored text (detailed instructions, annotated examples, and paired examples) and find that while evaluators' accuracy improved up to 55%, it did not significantly improve across the three domains. Given the inconsistent results across text domains and the often contradictory reasons evaluators gave for their judgments, we examine the role untrained human evaluations play in NLG evaluation and provide recommendations to NLG researchers for improving human evaluations of text generated from state-of-the-art models.",
        "id":235694265
      }
    ],
    "negative_ctxs":[
      {
        "title":"HEGEL: Hypergraph Transformer for Long Document Summarization",
        "text":"Extractive summarization for long documents is challenging due to the extended structured input context. The long-distance sentence dependency hinders cross-sentence relations modeling, the critical step of extractive summarization. This paper proposes HEGEL, a hypergraph neural network for long document summarization by capturing high-order crosssentence relations. HEGEL updates and learns effective sentence representations with hypergraph transformer layers and fuses different types of sentence dependencies, including latent topics, keywords coreference, and section structure. We validate HEGEL by conducting extensive experiments on two benchmark datasets, and experimental results demonstrate the effectiveness and efficiency of HEGEL.",
        "id":252780923
      },
      {
        "title":"Semantics WORD SENSE ACQUISITION FOR MULTILINGUAL TEXT INTERPRETATION *",
        "text":"",
        "id":7711342
      },
      {
        "title":"ACQUIRING LEXICAL KNOWLEDGE FOR ANAPHORA RESOLUTION",
        "text":"The lack of adequate bases of commonsense or even lexical knowledge is perhaps the main obstacle to the development of highperformance, robust tools for semantic interpretation. It is also generally accepted that, notwithstanding the increasing availability in recent years of substantial hand-coded lexical resources such as WordNet and EuroWordNet, addressing the commonsense knowledge bottleneck will eventually require the development of effective techniques for acquiring such information automatically, e.g., from corpora. We discuss research aimed at improving the performance of anaphora resolution systems by acquiring the commonsense knowledge require to resolve the more complex cases of anaphora, such as bridging references. We focus in particular on the problem of acquiring information about part-of relations.",
        "id":6397234
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Have any research papers investigated the creation of datasets through model-generated data for annotators to identify hallucinations in the results, specifically for developing diagnostic evaluation datasets?",
    "positive_ctxs":[
      {
        "title":"Q 2 : Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering",
        "text":"Neural knowledge-grounded generative models for dialogue often produce content that is factually inconsistent with the knowledge they rely on, making them unreliable and limiting their applicability. Inspired by recent work on evaluating factual consistency in abstractive summarization, we propose an automatic evaluation metric for factual consistency in knowledge-grounded dialogue using automatic question generation and question answering. Our metric, denoted Q 2 , compares answer spans using natural language inference (NLI), instead of token-based matching as done in previous work. To foster proper evaluation, we curate a novel dataset of dialogue system outputs for the Wizard-of-Wikipedia dataset, manually annotated for factual consistency. We perform a thorough meta-evaluation of Q 2 against other metrics using this dataset and two others, where it consistently shows higher correlation with human judgements.",
        "id":233289483
      }
    ],
    "negative_ctxs":[
      {
        "title":"Annotating Educational Questions for Student Response Analysis",
        "text":"Questions play an important role in the educational domain, representing the main form of interaction between instructors and students. In this paper, we introduce the first taxonomy and annotated educational corpus of questions that aims to help with the analysis of student responses. The dataset can be employed in approaches that classify questions based on the expected answer types. This can be an important component in applications that require prior knowledge about the desired answer to a given question, such as educational and question answering systems. To demonstrate the applicability and the effectiveness of the data within approaches to classify questions based on expected answer types, we performed extensive experiments on our dataset using a neural network with word embeddings as features. The approach achieved a weighted F1-score of 0.511, overcoming the baseline by 12%. This demonstrates that our corpus can be effectively integrated in simple approaches that classify questions based on the response type.",
        "id":21730476
      },
      {
        "title":"Statistical Models for Unsupervised Prepositional Phrase Attachment",
        "text":"We present several unsupervised statistical models for the prepositional phrase attachment task that approach the accuracy of the best supervised methods for this task. Our unsupervised approach uses a heuristic based on attachment proximity and trains from raw text that is annotated with only part-of-speech tags and morphological base forms, as opposed to attachment information. It is therefore less resource-intensive and more portable than previous corpus-based algorithm proposed for this task.We present results for prepositional phrase attachment in both English and Spanish.",
        "id":2751975
      },
      {
        "title":"Speech Technology for Everyone: Automatic Speech Recognition for Non-Native English with Transfer Learning",
        "text":"To address the performance gap of English ASR models on L2 English speakers, we evaluate fine-tuning of pretrained wav2vec 2.0 models (Baevski et al., 2020;Xu et al., 2021)on L2-ARCTIC, a non-native English speech corpus(Zhao et al., 2018)under different training settings. We compare (a) models trained with a combination of diverse accents to ones trained with only specific accents and (b) results from different single-accent models. Our experiments demonstrate the promise of developing ASR models for non-native English speakers, even with small amounts of L2 training data and even without a language model. Our models also excel in the zero-shot setting where we train on multiple L2 datasets and test on a blind L2 test set.",
        "id":238259710
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Have any research papers suggested methods for summarizing arbitrary length documents without truncating the input, by using memory networks?",
    "positive_ctxs":[
      {
        "title":"Sliding Selector Network with Dynamic Memory for Extractive Summarization of Long Documents",
        "text":"Neural-based summarization models suffer from the length limitation of text encoder. Long documents have to been truncated before they are sent to the model, which results in huge loss of summary-relevant contents. To address this issue, we propose the sliding selector network with dynamic memory for extractive summarization of long-form documents, which employs a sliding window to extract summary sentences segment by segment. Moreover, we adopt memory mechanism to preserve and update the history information dynamically, allowing the semantic flow across different windows. Experimental results on two large-scale datasets that consist of scientific papers demonstrate that our model substantially outperforms previous state-of-the-art models. Besides, we perform qualitative and quantitative investigations on how our model works and where the performance gain comes from. 1",
        "id":235097475
      }
    ],
    "negative_ctxs":[
      {
        "title":"An Evaluation of Image-based Verb Prediction Models against Human Eye-tracking Data",
        "text":"Recent research in language and vision has developed models for predicting and disambiguating verbs from images. Here, we ask whether the predictions made by such models correspond to human intuitions about visual verbs. We show that the image regions a verb prediction model identifies as salient for a given verb correlate with the regions fixated by human observers performing a verb classification task.",
        "id":44177418
      },
      {
        "title":"Efficiently Computing Nash Equilibria in Adversarial Team Markov Games",
        "text":"Computing Nash equilibrium policies is a central problem in multi-agent reinforcement learning that has received extensive attention both in theory and in practice. However, in light of computational intractability barriers in general-sum games, provable guarantees have been thus far either limited to fully competitive or cooperative scenarios or impose strong assumptions that are difficult to meet in most practical applications.In this work, we depart from those prior results by investigating infinite-horizon adversarial team Markov games, a natural and well-motivated class of games in which a team of identically-interested players-in the absence of any explicit coordination or communication-is competing against an adversarial player. This setting allows for a unifying treatment of zero-sum Markov games and Markov potential games, and serves as a step to model more realistic strategic interactions that feature both competing and cooperative interests. Our main contribution is the first algorithm for computing stationary ǫ-approximate Nash equilibria in adversarial team Markov games with computational complexity that is polynomial in all the natural parameters of the game, as well as 1\/ǫ.The proposed algorithm is particularly natural and practical, and it is based on performing independent policy gradient steps for each player in the team, in tandem with best responses from the side of the adversary; in turn, the policy for the adversary is then obtained by solving a carefully constructed linear program. Our analysis leverages non-standard techniques to establish the KKT optimality conditions for a nonlinear program with nonconvex constraints, thereby leading to a natural interpretation of the induced Lagrange multipliers. Along the way, we significantly extend an important characterization of optimal policies in adversarial (normal-form) team games due to Von Stengel and Koller (GEB '97).",
        "id":251279905
      },
      {
        "title":"Published as a conference paper at ICLR 2020 NETWORK RANDOMIZATION: A SIMPLE TECHNIQUE FOR GENERALIZATION IN DEEP REINFORCEMENT LEARNING",
        "text":"Deep reinforcement learning (RL) agents often fail to generalize to unseen environments (yet semantically similar to trained agents), particularly when they are trained on high-dimensional state spaces, such as images. In this paper, we propose a simple technique to improve a generalization ability of deep RL agents by introducing a randomized (convolutional) neural network that randomly perturbs input observations. It enables trained agents to adapt to new domains by learning robust features invariant across varied and randomized environments. Furthermore, we consider an inference method based on the Monte Carlo approximation to reduce the variance induced by this randomization. We demonstrate the superiority of our method across 2D CoinRun, 3D DeepMind Lab exploration and 3D robotics control tasks: it significantly outperforms various regularization and data augmentation methods for the same purpose. Code is available at . Deep direct reinforcement learning for financial signal representation and trading. IEEE transactions on neural networks and learning systems, 28(3):653-664, 2016.Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. , et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In ICML, 2018.",
        "id":213597045
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Have any research papers suggested techniques for automatically choosing in-context examples?",
    "positive_ctxs":[
      {
        "title":"Active Example Selection for In-Context Learning",
        "text":"With a handful of demonstration examples, large-scale language models show strong capability to perform various tasks by in-context learning from these examples, without any finetuning. We demonstrate that in-context learning performance can be highly unstable across samples of examples, indicating the idiosyncrasies of how language models acquire information. We formulate example selection for in-context learning as a sequential decision problem, and propose a reinforcement learning algorithm for identifying generalizable policies to select demonstration examples. For GPT-2, our learned policies demonstrate strong abilities of generalizing to unseen tasks in training, with a 5.8% improvement on average. Examples selected from our learned policies can even achieve a small improvement on GPT-3 Ada. However, the improvement diminishes on larger GPT-3 models, suggesting emerging capabilities of large language models. . 2022. We-bGPT: Browser-assisted question-answering with human feedback.",
        "id":253420743
      }
    ],
    "negative_ctxs":[
      {
        "title":"Enjambment Detection in a Large Diachronic Corpus of Spanish Sonnets Enjambment Detection in a Large Diachronic Corpus of Spanish Sonnets",
        "text":"Enjambment takes place when a syntactic unit is broken up across two lines of poetry, giving rise to different stylistic effects. In Spanish literary studies, there are unclear points about the types of stylistic effects that can arise, and under which linguistic conditions. To systematically gather evidence about this, we developed a system to automatically identify enjambment (and its type) in Spanish. For evaluation, we manually annotated a reference corpus covering different periods. As a scholarly corpus to apply the tool, from public HTML sources we created a diachronic corpus covering four centuries of sonnets (3750 poems), and we analyzed the occurrence of enjambment across stanzaic boundaries in different periods. Besides, we found examples that highlight limitations in current definitions of enjambment.",
        "id":28722680
      },
      {
        "title":"Error Identification for Machine Translation with Metric Embedding and Attention",
        "text":"Quality Estimation (QE) for Machine Translation has been shown to reach relatively high accuracy in predicting sentence-level scores, relying on pretrained contextual embeddings and human-produced quality scores. However, the lack of explanations along with decisions made by end-to-end neural models makes the results difficult to interpret. Furthermore, word-level annotated datasets are rare due to the prohibitive effort required to perform this task, while they could provide interpretable signals in addition to sentence-level QE outputs. In this paper, we propose a novel QE architecture which tackles both the wordlevel data scarcity and the interpretability limitations of recent approaches. Sentence-level and word-level components are jointly pretrained through an attention mechanism based on synthetic data and a set of MT metrics embedded in a common space. Our approach is evaluated on the Eval4NLP 2021 shared task and our submissions reach the first position in all language pairs. The extraction of metricto-input attention weights show that different metrics focus on different parts of the source and target text, providing strong rationales in the decision-making process of the QE model.",
        "id":241583432
      },
      {
        "title":"Ask to Understand: Question Generation for Multi-hop Question Answering",
        "text":"Multi-hop Question Answering (QA) requires the machine to answer complex questions by finding scattering clues and reasoning from multiple documents. Graph Network (GN) and Question Decomposition (QD) are two common approaches at present. The former uses the \"black-box\" reasoning process to capture the potential relationship between entities and sentences, thus achieving good performance. At the same time, the latter provides a clear reasoning logical route by decomposing multi-hop questions into simple singlehop sub-questions. In this paper, we propose a novel method to complete multi-hop QA from the perspective of Question Generation (QG). Specifically, we carefully design an endto-end QG module on the basis of a classical QA module, which could help the model understand the context by asking inherently logical sub-questions, thus inheriting interpretability from the QD-based method and showing superior performance. Experiments on the HotpotQA dataset demonstrate that the effectiveness of our proposed QG module, human evaluation further clarifies its interpretability quantitatively, and thorough analysis shows that the QG module could generate better subquestions than QD methods in terms of fluency, consistency, and diversity.\"Educated mind first sign is good at asking questions.\" -Plekhanov G.V.",
        "id":247518809
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Have any research papers tried to create conversational agents with inner states represented by a knowledge graph that can be continually updated based on the agent’s environment?",
    "positive_ctxs":[
      {
        "title":"Towards Socially Intelligent Agents with Mental State Transition and Human Value",
        "text":"Building a socially intelligent agent involves many challenges. One of which is to track the agent's mental state transition and teach the agent to make decisions guided by its value like a human. Towards this end, we propose to incorporate mental state simulation and value modeling into dialogue agents. First, we build a hybrid mental state parser that extracts information from both the dialogue and event observations and maintains a graphical representation of the agent's mind; Meanwhile, the transformer-based value model learns human preferences from the human value dataset, VALUENET. Empirical results show that the proposed model attains state-of-the-art performance on the dialogue\/action\/emotion prediction task in the fantasy text-adventure game dataset, LIGHT. We also show example cases to demonstrate: (i) how the proposed mental state parser can assist the agent's decision by grounding on the context like locations and objects, and (ii) how the value model can help the agent make decisions based on its personal priorities.Alex Abella. 2009. Soldiers of reason: The RAND corporation and the rise of the American empire. , et al. 2021. Alexa conversations: An extensible data-driven approach for building task-oriented dialogue systems. arXiv preprint arXiv:2104.09088.",
        "id":252847559
      }
    ],
    "negative_ctxs":[
      {
        "title":"Paradigmatic Modifiability Statistics for the Extraction of Complex Multi-Word Terms",
        "text":"We here propose a new method which sets apart domain-specific terminology from common non-specific noun phrases. It is based on the observation that terminological multi-word groups reveal a considerably lesser degree of distributional variation than non-specific noun phrases. We define a measure for the observable amount of paradigmatic modifiability of terms and, subsequently, test it on bigram, trigram and quadgram noun phrases extracted from a 104-million-word biomedical text corpus. Using a community-wide curated biomedical terminology system as an evaluation gold standard, we show that our algorithm significantly outperforms a variety of standard term identification measures. We also provide empirical evidence that our methodolgy is essentially domain-and corpus-size-independent.",
        "id":15273358
      },
      {
        "title":"",
        "text":"",
        "id":208981197
      },
      {
        "title":"LEARNING NON-DETERMINISTIC REPRESENTATIONS WITH ENERGY-BASED ENSEMBLES",
        "text":"The goal of a generative model is to capture the distribution underlying the data, typically through latent variables. After training, these variables are often used as a new representation, more effective than the original features in a variety of learning tasks. However, the representations constructed by contemporary generative models are usually point-wise deterministic mappings from the original feature space. Thus, even with representations robust to class-specific transformations, statistically driven models trained on them would not be able to generalize when the labeled data is scarce. Inspired by the stochasticity of the synaptic connections in the brain, we introduce Energy-based Stochastic Ensembles. These ensembles can learn non-deterministic representations, i.e., mappings from the feature space to a family of distributions in the latent space. These mappings are encoded in a distribution over a (possibly infinite) collection of models. By conditionally sampling models from the ensemble, we obtain multiple representations for every input example and effectively augment the data. We propose an algorithm similar to contrastive divergence for training restricted Boltzmann stochastic ensembles. Finally, we demonstrate the concept of the stochastic representations on a synthetic dataset as well as test them in the one-shot learning scenario on MNIST.",
        "id":16521404
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Have any studies explored the creation of memory management systems in AI to improve sustained conversational capabilities and tackle the challenge of contextual retention over extended periods?",
    "positive_ctxs":[
      {
        "title":"Long Time No See! Open-Domain Conversation with Long-Term Persona Memory",
        "text":"Most of the open-domain dialogue models tend to perform poorly in the setting of long-term human-bot conversations. The possible reason is that they lack the capability of understanding and memorizing long-term dialogue history information. To address this issue, we present a novel task of Long-term Memory Conversation (LeMon) and then build a new dialogue dataset DuLeMon and a dialogue generation framework PLATO-LTM with a Long-Term Memory (LTM) mechanism. This LTM mechanism enables our system to accurately extract and continuously update long-term persona memory without requiring multiple-session dialogue datasets for model training. To our knowledge, this is the first attempt to conduct real-time dynamic management of persona information of both parties, including the user and the bot. Results on DuLeMon indicate that PLATO-LTM can significantly outperform baselines in terms of long-term dialogue consistency, leading to better dialogue engagingness 1 .",
        "id":247411350
      }
    ],
    "negative_ctxs":[
      {
        "title":"Contextualized Sparse Representations for Real-Time Open-Domain Question Answering",
        "text":"Open-domain question answering can be formulated as a phrase retrieval problem, in which we can expect huge scalability and speed benefit but often suffer from low accuracy due to the limitation of existing phrase representation models. In this paper, we aim to improve the quality of each phrase embedding by augmenting it with a contextualized sparse representation (SPARC). Unlike previous sparse vectors that are term-frequencybased (e.g., tf-idf) or directly learned (only few thousand dimensions), we leverage rectified self-attention to indirectly learn sparse vectors in n-gram vocabulary space. By augmenting the previous phrase retrieval model(Seo et al., 2019)with SPARC, we show 4%+ improvement in CuratedTREC and SQuAD-Open. Our CuratedTREC score is even better than the best known retrieve & read model with at least 45x faster inference speed. 1",
        "id":219058995
      },
      {
        "title":"End-to-End Neural Pipeline for Goal-Oriented Dialogue Systems using GPT-2",
        "text":"The goal-oriented dialogue system needs to be optimized for tracking the dialogue flow and carrying out an effective conversation under various situations to meet the user goal. The traditional approach to building such a dialogue system is to take a pipelined modular architecture, where its modules are optimized individually. However, such an optimization scheme does not necessarily yield an overall performance improvement of the whole system. On the other hand, end-to-end dialogue systems with monolithic neural architecture are often trained only with input-output utterances, without taking into account the entire annotations available in the corpus. This scheme makes it difficult for goal-oriented dialogues where the system needs to be integrated with external systems or to provide interpretable information about why the system generated a particular response. In this paper, we present an end-to-end neural architecture for dialogue systems that addresses both challenges above. Our dialogue system achieved the success rate of 68.32%, the language understanding score of 4.149, and the response appropriateness score of 4.287 in human evaluations, which ranked the system at the top position in the end-to-end multi-domain dialogue system task in the 8th dialogue systems technology challenge (DSTC8).",
        "id":219719687
      },
      {
        "title":"BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent Summarization",
        "text":"Most existing text summarization datasets are compiled from the news domain, where summaries have a flattened discourse structure. In such datasets, summary-worthy content often appears in the beginning of input articles. Moreover, large segments from input articles are present verbatim in their respective summaries. These issues impede the learning and evaluation of systems that can understand an article's global content structure as well as produce abstractive summaries with high compression ratio. In this work, we present a novel dataset, BIGPATENT, consisting of 1.3 million records of U.S. patent documents along with human written abstractive summaries. Compared to existing summarization datasets, BIGPATENT has the following properties: i) summaries contain a richer discourse structure with more recurring entities, ii) salient content is evenly distributed in the input, and iii) lesser and shorter extractive fragments are present in the summaries. Finally, we train and evaluate baselines and popular learning models on BIGPATENT to shed light on new challenges and motivate future directions for summarization research.1 BIGPATENT dataset is available to download online at evasharma.github.io\/bigpatent.",
        "id":182953211
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Have there been any advancements in language models that operate without tokenization and emphasize encoding at the character level, and what benefits might they have compared to conventional methods using subword tokenization?",
    "positive_ctxs":[
      {
        "title":"CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation",
        "text":"Pipelined NLP systems have largely been superseded by end-to-end neural modeling, yet nearly all commonly used models still require an explicit tokenization step. While recent tokenization approaches based on data-derived subword lexicons are less brittle than manually engineered tokenizers, these techniques are not equally suited to all languages, and the use of any fixed vocabulary may limit a model's ability to adapt. In this paper, we present CANINE, a neural encoder that operates directly on character sequences-without explicit tokenization or vocabulary-and a pre-training strategy that operates either directly on characters or optionally uses subwords as a soft inductive bias. To use its finer-grained input effectively and efficiently, CANINE combines downsampling, which reduces the input sequence length, with a deep transformer stack, which encodes context. CANINE outperforms a comparable mBERT model by 5.7 F1 on TYDI QA, a challenging multilingual benchmark, despite having fewer model parameters.73",
        "id":232185112
      }
    ],
    "negative_ctxs":[
      {
        "title":"The Language of Power and its Cultural Influence",
        "text":"In this paper, we investigate whether the social goals of an individual can be recognized through analysis of the social actions indicated by their use of language. Specifically, we focus on recognizing when someone is pursuing power within a web forum. Individuals pursue power in order to increase their control over the actions and goals of the group. We cast the problem as social conversational entailment where we determine if a dialogue entails a hypothesis which states a dialogue participant is in pursuit of power. In the social conversational entailment framework the hypothesis is decomposed into a series of social commitments which define series of actions and responses that are indicative of the hypothesis. The social commitments are modeled as social acts which are pragmatic speech acts. We identify nine culturally neutral psychologically-motivated social acts that can be detected in language and are indicative of whether an individual is pursuing power. Our best results using social conversational entailment achieve an overall F-measure of 79.7% for predicting pursuit of power for English speakers and 78.3% for Chinese speakers.",
        "id":15818826
      },
      {
        "title":"CMUQ@QALB-2014: An SMT-based System for Automatic Arabic Error Correction",
        "text":"In this paper, we describe the CMUQ system we submitted to The ANLP-QALB 2014 Shared Task on Automatic Text Correction for Arabic. Our system combines rule-based linguistic techniques with statistical language modeling techniques and machine translationbased methods. Our system outperforms the baseline and reaches an F-score of 65.42% on the test set of QALB corpus. This ranks us 3rd in the competition.",
        "id":17910054
      },
      {
        "title":"Multilingual Neural Machine Translation involving Indian Languages",
        "text":"Neural Machine Translations (NMT) models are capable of translating a single bilingual pair and require a new model for each new language pair. Multilingual Neural Machine Translation models are capable of translating multiple language pairs, even pairs which it hasn't seen before in training. Availability of parallel sentences is a known problem in machine translation. Multilingual NMT model leverages information from all the languages to improve itself and performs better. We propose a data augmentation technique that further improves this model profoundly. The technique helps achieve a jump of more than 15 points in BLEU score from the Multilingual NMT Model. A BLEU score of 36.2 was achieved for Sindhi-English translation, which is higher than any score on the leaderboard of the LoResMT SharedTask at MT Summit 2019, which provided the data for the experiments.",
        "id":219307271
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"How can I locate a dataset containing toxic sentence pairs alongside their non-toxic paraphrased counterparts for studying text detoxification?",
    "positive_ctxs":[
      {
        "title":"ParaDetox: Detoxification with Parallel Data",
        "text":"We present a novel pipeline for the collection of parallel data for the detoxification task. We collect non-toxic paraphrases for over 10,000 English toxic sentences. We also show that this pipeline can be used to distill a large existing corpus of paraphrases to get toxicneutral sentence pairs. We release two parallel corpora which can be used for the training of detoxification models. To the best of our knowledge, these are the first parallel datasets for this task. We describe our pipeline in detail to make it fast to set up for a new language or domain, thus contributing to faster and easier development of new parallel resources.We train several detoxification models on the collected data and compare them with several baselines and state-of-the-art unsupervised approaches. We conduct both automatic and manual evaluations. All models trained on parallel data outperform the state-of-the-art unsupervised models by a large margin. This suggests that our novel datasets can boost the performance of detoxification systems.",
        "id":248780527
      }
    ],
    "negative_ctxs":[
      {
        "title":"Duluth at SemEval-2016 Task 14 : Extending Gloss Overlaps to Enrich Semantic Taxonomies",
        "text":"This paper describes the Duluth systems that participated in Task 14 of SemEval 2016, Semantic Taxonomy Enrichment. There were three related systems in the formal evaluation which are discussed here, along with numerous post-evaluation runs. All of these systems identified synonyms between Word-Net and other dictionaries by measuring the gloss overlaps between them. These systems perform better than the random baseline and one post-evaluation variation was within a respectable margin of the median result attained by all participating systems.",
        "id":776002
      },
      {
        "title":"",
        "text":"",
        "id":226283765
      },
      {
        "title":"Hard Gate Knowledge Distillation - Leverage Calibration for a Robust and Reliable Language Model",
        "text":"In knowledge distillation, a student model is trained with supervisions from both knowledge from a teacher and observations drawn from a training data distribution. Knowledge of a teacher is considered a subject that holds interclass relations which send a meaningful supervision to a student; hence, much effort has been put to find such knowledge to be distilled. In this paper, we explore a question that has been given little attention: \"when to distill such knowledge.\" The question is answered in our work with the concept of model calibration; we view a teacher model not only as a source of knowledge but also as a gauge to detect miscalibration of a student. This simple and yet novel view leads to a hard gate knowledge distillation scheme that switches between learning from a teacher model and training data. We verify the gating mechanism in the context of natural language generation at both the token-level and the sentence-level. Empirical comparisons with strong baselines show that hard gate knowledge distillation not only improves model generalization, but also significantly lowers model calibration error.",
        "id":253097876
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"How can SQL-to-text be utilized to improve text-to-SQL parsing through data augmentation techniques?",
    "positive_ctxs":[
      {
        "title":"Data Augmentation with Hierarchical SQL-to-Question Generation for Cross-domain Text-to-SQL Parsing",
        "text":"Data augmentation has attracted a lot of research attention in the deep learning era for its ability in alleviating data sparseness. The lack of labeled data for unseen evaluation databases is exactly the major challenge for cross-domain text-to-SQL parsing. Previous works either require human intervention to guarantee the quality of generated data, or fail to handle complex SQL queries. This paper presents a simple yet effective data augmentation framework. First, given a database, we automatically produce a large number of SQL queries based on an abstract syntax tree grammar. For better distribution matching, we require that at least 80% of SQL patterns in the training data are covered by generated queries. Second, we propose a hierarchical SQL-to-question generation model to obtain high-quality natural language questions, which is the major contribution of this work. Finally, we design a simple sampling strategy that can greatly improve training efficiency given large amounts of generated data. Experiments on three cross-domain datasets, i.e., WikiSQL and Spider in English, and DuSQL in Chinese, show that our proposed data augmentation framework can consistently improve performance over strong baselines, and the hierarchical generation component is the key for the improvement.",
        "id":232104941
      }
    ],
    "negative_ctxs":[
      {
        "title":"Probabilistic Frame Induction",
        "text":"In natural-language discourse, related events tend to appear near each other to describe a larger scenario. Such structures can be formalized by the notion of a frame (a.k.a. template), which comprises a set of related events and prototypical participants and event transitions. Identifying frames is a prerequisite for information extraction and natural language generation, and is usually done manually. Methods for inducing frames have been proposed recently, but they typically use ad hoc procedures and are difficult to diagnose or extend. In this paper, we propose the first probabilistic approach to frame induction, which incorporates frames, events, and participants as latent topics and learns those frame and event transitions that best explain the text. The number of frame components is inferred by a novel application of a split-merge method from syntactic parsing. In end-to-end evaluations from text to induced frames and extracted facts, our method produces state-of-the-art results while substantially reducing engineering effort.",
        "id":278288
      },
      {
        "title":"",
        "text":"",
        "id":218974069
      },
      {
        "title":"A Noisy-Channel Approach to Question Answering",
        "text":"We introduce a probabilistic noisychannel model for question answering and we show how it can be exploited in the context of an end-to-end QA system. Our noisy-channel system outperforms a stateof-the-art rule-based QA system that uses similar resources. We also show that the model we propose is flexible enough to accommodate within one mathematical framework many QA-specific resources and techniques, which range from the exploitation of WordNet, structured, and semi-structured databases to reasoning, and paraphrasing.",
        "id":12305296
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"How can dense retrieval models for open-domain question answering be improved, specifically through the use of hard negative mining techniques?",
    "positive_ctxs":[
      {
        "title":"RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering",
        "text":"In open-domain question answering, dense passage retrieval has become a new paradigm to retrieve relevant passages for finding answers. Typically, the dual-encoder architecture is adopted to learn dense representations of questions and passages for semantic matching. However, it is difficult to effectively train a dual-encoder due to the challenges including the discrepancy between training and inference, the existence of unlabeled positives and limited training data. To address these challenges, we propose an optimized training approach, called RocketQA, to improving dense passage retrieval. We make three major technical contributions in RocketQA, namely crossbatch negatives, denoised hard negatives and data augmentation. The experiment results show that RocketQA significantly outperforms previous state-of-the-art models on both MS-MARCO and Natural Questions. We also conduct extensive experiments to examine the effectiveness of the three strategies in RocketQA. Besides, we demonstrate that the performance of end-to-end QA can be improved based on our RocketQA retriever 1 .",
        "id":231815627
      }
    ],
    "negative_ctxs":[
      {
        "title":"The Hunvec Framework For NN-CRF-based Sequential Tagging",
        "text":"In this work we present the open source hunvec framework for sequential tagging, built upon Theano and Pylearn2. The underlying statistical model, which connects linear CRF-s with neural networks, was used by Collobert and co-workers, and several other researchers. For demonstrating the flexibility of our tool, we describe a set of experiments on part-of-speech and named-entityrecognition tasks, using English and Hungarian datasets, where we modify both model and training parameters, and illustrate the usage of custom features. Model parameters we experiment with affect the vectorial word representations used by the model; we apply different word vector initializations, defined by Word2vec and GloVe embeddings and enrich the representation of words by vectors assigned trigram features. We extend training methods by using their regularized (l2 and dropout) version. When testing our framework on a Hungarian named entity corpus, we find that its performance reaches the best published results on this dataset, with no need for language-specific feature engineering. Our code is available at http:\/\/github.com\/zseder\/hunvec",
        "id":41531604
      },
      {
        "title":"Twitter Language Identification Of Similar Languages And Dialects Without Ground Truth",
        "text":"We present a new method to bootstrap filter Twitter language ID labels in our dataset for automatic language identification (LID). Our method combines geolocation, original Twitter LID labels, and Amazon Mechanical Turk to resolve missing and unreliable labels. We are the first to compare LID classification performance using the MIRA algorithm and langid.py. We show classifier performance on different versions of our dataset with high accuracy using only Twitter data, without ground truth, and very few training examples. We also show how Platt Scaling can be use to calibrate MIRA classifier output values into a probability distribution over candidate classes, making the output more intuitive. Our method allows for fine-grained distinctions between similar languages and dialects and allows us to rediscover the language composition of our Twitter dataset.",
        "id":6936764
      },
      {
        "title":"GUSUM: Graph-Based Unsupervised Summarization using Sentence Features Scoring and Sentence-BERT",
        "text":"Unsupervised extractive document summarization aims to extract salient sentences from a document without requiring a labelled corpus. In existing graph-based methods, vertex and edge weights are usually created by calculating sentence similarities. In this paper, we develop a Graph-Based Unsupervised Summarization(GUSUM) method for extractive text summarization based on the principle of including the most important sentences while excluding sentences with similar meanings in the summary. We modify traditional graph ranking algorithms with recent sentence embedding models and sentence features and modify how sentence centrality is computed. We first define the sentence feature scores represented at the vertices, indicating the importance of each sentence in the document. After this stage, we use Sentence-BERT for obtaining sentence embeddings to better capture the sentence meaning. In this way, we define the edges of a graph where semantic similarities are represented. Next we create an undirected graph that includes sentence significance and similarities between sentences. In the last stage, we determine the most important sentences in the document with the ranking method we suggested on the graph created. Experiments on CNN\/Daily Mail, New York Times, arXiv, and PubMed datasets show our approach achieves high performance on unsupervised graph-based summarization when evaluated both automatically and by humans.",
        "id":252819347
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"I know about prompt tuning, but have any works tried learning embeddings that are inputted to every transformer layer in a language model?",
    "positive_ctxs":[
      {
        "title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation",
        "text":"Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were \"virtual tokens\". We apply prefix-tuning to GPT-2 for table-totext generation and to BART for summarization. We show that by modifying only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. 2020. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. Amodei. 2020. Language models are few-shot learners.",
        "id":230433941
      }
    ],
    "negative_ctxs":[
      {
        "title":"UNSUPERVISED DISENTANGLEMENT WITH TENSOR PRODUCT REPRESENTATIONS ON THE TORUS",
        "text":"The current methods for learning representations with auto-encoders almost exclusively employ vectors as the latent representations. In this work, we propose to employ a tensor product structure for this purpose. This way, the obtained representations are naturally disentangled. In contrast to the conventional variations methods, which are targeted toward normally distributed features, the latent space in our representation is distributed uniformly over a set of unit circles. We argue that the torus structure of the latent space captures the generative factors effectively. We employ recent tools for measuring unsupervised disentanglement, and in an extensive set of experiments demonstrate the advantage of our method in terms of disentanglement, completeness, and informativeness. The code for our proposed method is available at https:\/\/github.com\/rotmanmi\/ Unsupervised-Disentanglement-Torus.",
        "id":246822414
      },
      {
        "title":"",
        "text":"",
        "id":237155074
      },
      {
        "title":"",
        "text":"",
        "id":6086996
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"In multi-hop question answering, is there a paper that explores \"per-hop\" retrieval evaluation that treats each hop of retrieval independently?",
    "positive_ctxs":[
      {
        "title":"Combining Lexical and Dense Retrieval for Computationally Efficient Multi-hop Question Answering",
        "text":"In simple open-domain question answering (QA), dense retrieval has become one of the standard approaches for retrieving the relevant passages to infer an answer. Recently, dense retrieval also achieved state-of-the-art results in multi-hop QA, where aggregating information from multiple pieces of information and reasoning over them is required. Despite their success, dense retrieval methods are computationally intensive, requiring multiple GPUs to train. In this work, we introduce a hybrid (lexical and dense) retrieval approach that is highly competitive with the state-of-the-art dense retrieval models, while requiring substantially less computational resources. Additionally, we provide an in-depth evaluation of dense retrieval methods on limited computational resource settings, something that is missing from the current literature.",
        "id":237592852
      }
    ],
    "negative_ctxs":[
      {
        "title":"Harvesting Re-usable High-level Rules for Expository Dialogue Generation",
        "text":"This paper proposes a method for extracting high-level rules for expository dialogue generation. The rules are extracted from dialogues that have been authored by expert dialogue writers. We examine the rules that can be extracted by this method, focusing on whether different dialogues and authors exhibit different dialogue styles.",
        "id":14269639
      },
      {
        "title":"",
        "text":"",
        "id":233365227
      },
      {
        "title":"Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks",
        "text":"There are two approaches for pairwise sentence scoring: Cross-encoders, which perform full-attention over the input pair, and Bi-encoders, which map each input independently to a dense vector space. While crossencoders often achieve higher performance, they are too slow for many practical use cases. Bi-encoders, on the other hand, require substantial training data and fine-tuning over the target task to achieve competitive performance. We present a simple yet efficient data augmentation strategy called Augmented SBERT, where we use the cross-encoder to label a larger set of input pairs to augment the training data for the bi-encoder. We show that, in this process, selecting the sentence pairs is non-trivial and crucial for the success of the method. We evaluate our approach on multiple tasks (in-domain) as well as on a domain adaptation task. Augmented SBERT achieves an improvement of up to 6 points for in-domain and of up to 37 points for domain adaptation tasks compared to the original bi-encoder performance. 1",
        "id":223957053
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Is it possible to adatp named entity recognition systems to learn new entities by combining knowledge distillation and synthetic data augmentation by using a two-stage training approach?",
    "positive_ctxs":[
      {
        "title":"Learn and Review: Enhancing Continual Named Entity Recognition via Reviewing Synthetic Samples",
        "text":"Traditional methods for named entity recognition (NER) classify mentions into a fixed set of pre-defined entity types. However, in many real-world scenarios, new entity types are incrementally involved. To investigate this problem, continual learning is introduced for NER. However, the existing method depends on the relevance between tasks and is prone to inter-type confusion. In this paper, we propose a novel two-stage framework Learn-and-Review (L&R) for continual NER under the type-incremental setting to alleviate the above issues. Specifically, for the learning stage, we distill the old knowledge from teacher to a student on the current dataset. For the reviewing stage, we first generate synthetic samples of old types to augment the dataset. Then, we further distill new knowledge from the above student and old knowledge from the teacher to get an enhanced student on the augmented dataset. This stage has the following advantages: (1) The synthetic samples mitigate the gap between the old and new task and thus enhance the further distillation; (2) Different types of entities are jointly seen during training which alleviates the inter-type confusion. Experimental results show that L&R outperforms the state-of-the-art method on CoNLL-03 and OntoNotes-5.0.",
        "id":248779939
      }
    ],
    "negative_ctxs":[
      {
        "title":"ISTIC's Triangular Machine Translation System for WMT' 2021",
        "text":"This paper describes the ISTIC's submission to the Triangular Machine Translation Task of Russian-to-Chinese machine translation for WMT' 2021. In order to fully utilize the provided corpora and promote the translation performance from Russian to Chinese, the pivot method is used in our system which pipelines the Russian-to-English translator and the English-to-Chinese translator to form a Russian-to-Chinese translator. Our system is based on the Transformer architecture and several effective strategies are adopted to improve the quality of translation, including corpus filtering, data pre-processing, system combination, model averaging, model ensemble and reranking.",
        "id":245855694
      },
      {
        "title":"ModelTalker Voice Recorder -An Interface System for Recording a Corpus of Speech for Synthesis",
        "text":"We will demonstrate the ModelTalker Voice Recorder (MT Voice Recorder) -an interface system that lets individuals record and bank a speech database for the creation of a synthetic voice. The system guides users through an automatic calibration process that sets pitch, amplitude, and silence. The system then prompts users with both visual (text-based) and auditory prompts. Each recording is",
        "id":543388
      },
      {
        "title":"Domain Adaptation for Parsing",
        "text":"We compare two different methods in domain adaptation applied to constituent parsing: parser combination and cotraining, each used to transfer information from the source domain of news to the target domain of natural dialogs, in a setting without annotated data. Both methods outperform the baselines and reach similar results. Parser combination profits most from the large amounts of training data combined with a robust probability model. Co-training, in contrast, relies on a small set of higher quality data.",
        "id":9510501
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Is there a benchmark designed to assess language models' social biases within question answering frameworks, specifically one encompassing diverse categories of social bias and offering instances of ambiguity and its resolution?",
    "positive_ctxs":[
      {
        "title":"BBQ: A Hand-Built Bias Benchmark for Question Answering",
        "text":"It is well documented that NLP models learn social biases, but little work has been done on how these biases manifest in model outputs for applied tasks like question answering (QA). We introduce the Bias Benchmark for QA (BBQ), a dataset of question sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts. Our task evaluates model responses at two levels: (i) given an under-informative context, we test how strongly responses refect social biases, and (ii) given an adequately informative context, we test whether the model's biases override a correct answer choice. We fnd that models often rely on stereotypes when the context is under-informative, meaning the model's outputs consistently reproduce harmful biases in this setting. Though models are more accurate when the context provides an informative answer, they still rely on stereotypes and average up to 3.4 percentage points higher accuracy when the correct answer aligns with a social bias than when it conficts, with this difference widening to over 5 points on examples targeting gender for most models tested.",
        "id":239010011
      }
    ],
    "negative_ctxs":[
      {
        "title":"Alignment-based Annotation of Proofreading Texts toward Professional Writing Assistance",
        "text":"This work aims at constructing a corpus to satisfy such requirements to support research towards professional writing assistance. Our corpus is a collection of scientific work written by non-native speakers that has been proofread by native English experts. A new annotation scheme, which is based on word-alignments, is then proposed that is used to capture all types of inarticulations and their corrections including both spelling\/grammatical error corrections and paraphrases made by proofreaders. The resulting corpus contains 3,485 pairs of original and revised sentences, of which, 2,516 pairs contain at least one articulation.",
        "id":16892737
      },
      {
        "title":"",
        "text":"",
        "id":219309690
      },
      {
        "title":"Translating a Language You Don't Know in the Chinese Room",
        "text":"In a corruption of John Searle's famous AI thought experiment, the Chinese Room (Searle, 1980), we twist its original intent by enabling humans to translate text, e.g. from Uyghur to English, even if they don't have any prior knowledge of the source language. Our enabling tool, which we call the Chinese Room, is equipped with the same resources made available to a machine translation engine. We find that our superior language model and world knowledge allows us to create perfectly fluent and nearly adequate translations, with human expertise required only for the target language. The Chinese Room tool can be used to rapidly create small corpora of parallel data when bilingual translators are not readily available, in particular for low-resource languages.",
        "id":51871755
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Is there a comprehensive dataset available for summarizing broad-spectrum conversational dialogues?",
    "positive_ctxs":[
      {
        "title":"DIALOGSUM: A Real-Life Scenario Dialogue Summarization Dataset",
        "text":"Proposal of large-scale datasets has facilitated research on deep neural models for news summarization. Deep learning can also be potentially useful for spoken dialogue summarization, which can benefit a range of reallife scenarios including customer service management and medication tracking. To this end, we propose DIALOGSUM, a large-scale labeled dialogue summarization dataset. We conduct empirical analysis on DIALOGSUM using state-of-the-art neural summarizers. Experimental results show unique challenges in dialogue summarization, such as spoken terms, special discourse structures, coreferences and ellipsis, pragmatics and social common sense, which require specific representation learning technologies to better deal with.",
        "id":234681504
      }
    ],
    "negative_ctxs":[
      {
        "title":"The Design and Construction of the Corpus of China English",
        "text":"The paper describes the design and construction of the Corpus of China English (CCE). With the emergence of China English as a developing variety in the family of the world Englishes, more and more research has been done to explore its use in China. In order to provide a reliable resource for researchers in the field of China English, CCE was built with due consideration given to its representativeness and authenticity. The general principles for the corpus were authentic, representative and manageable in size. It was composed of more than 13,962,102 tokens in 15,333 texts evenly divided between the following four genres: newspapers, magazines, fiction and academic writings. The texts cover a wide range of domains, such as news, financial, politics, environment, social, culture, technology, sports, education, philosophy, literary, etc. It will be a helpful resource for research on China English, computational linguistics, natural language processing and corpus linguistics. Moreover, it is also a valuable resource for English language teaching in the context of English as a lingua franca (ELF) in which the theories and practice of English language teaching shift from teaching the \"Standard\" English to English as a lingua franca. With the corpus evidence, English teaching textbooks and materials can be developed with due consideration given to China English.",
        "id":219300379
      },
      {
        "title":"SYNTACTIC PRIVILEGE",
        "text":"",
        "id":61930487
      },
      {
        "title":"Focused Evaluation for Image Description with Binary Forced-Choice Tasks",
        "text":"Current evaluation metrics for image description may be too coarse. We therefore propose a series of binary forced-choice tasks that each focus on a different aspect of the captions. We evaluate a number of different off-the-shelf image description systems. Our results indicate strengths and shortcomings of both generation and ranking based approaches.",
        "id":8676220
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Is there a dataset available for open-domain targeted sentiment analysis containing user reviews from platforms like Yelp and Amazon?",
    "positive_ctxs":[
      {
        "title":"YASO: A Targeted Sentiment Analysis Evaluation Dataset for Open-Domain Reviews",
        "text":"Current TSA evaluation in a cross-domain setup is restricted to the small set of review domains available in existing datasets. Such an evaluation is limited, and may not reflect true performance on sites like Amazon or Yelp that host diverse reviews from many domains. To address this gap, we present YASO -a new TSA evaluation dataset of open-domain user reviews. YASO contains 2215 English sentences from dozens of review domains, annotated with target terms and their sentiment. Our analysis verifies the reliability of these annotations, and explores the characteristics of the collected data. Benchmark results using five contemporary TSA systems show there is ample room for improvement on this challenging new dataset. YASO is available at github.com\/IBM\/yaso-tsa. 1 E.g. Yelp has more than 1,200 business categories here. 2  The name is an acronym of the data sources.",
        "id":237497416
      }
    ],
    "negative_ctxs":[
      {
        "title":"'Am I the Bad One'? Predicting the Moral Judgement of the Crowd Using Pre-trained Language Models",
        "text":"Natural language processing (NLP) has been shown to perform well in various tasks, such as answering questions, ascertaining natural language inference and anomaly detection. However, there are few NLP-related studies that touch upon the moral context conveyed in text. This paper studies whether state-of-the-art, pre-trained language models are capable of passing moral judgments on posts retrieved from a popular Reddit user board. Reddit is a social discussion website and forum where posts are promoted by users through a voting system. In this work, we construct a dataset that can be used for moral judgement tasks by collecting data from the AITA? (Am I the A*******?) subreddit. To model our task, we harnessed the power of pre-trained language models, including BERT, RoBERTa, RoBERTa-large, ALBERT and Longformer. We then fine-tuned these models and evaluated their ability to predict the correct verdict as judged by users for each post in the datasets. RoBERTa showed relative improvements across the three datasets, exhibiting a rate of 87% accuracy and a Matthews correlation coefficient (MCC) of 0.76, while the use of the Longformer model slightly improved the performance when used with longer sequences, achieving 87% accuracy and 0.77 MCC.",
        "id":252411574
      },
      {
        "title":"Published as a conference paper at ICLR 2020 SHIFTED AND SQUEEZED 8-BIT FLOATING POINT FOR- MAT FOR LOW-PRECISION TRAINING OF DEEP NEU- RAL NETWORKS",
        "text":"Training with larger number of parameters while keeping fast iterations is an increasingly adopted strategy and trend for developing better performing Deep Neural Network (DNN) models. This necessitates increased memory footprint and computational requirements for training. Here we introduce a novel methodology for training deep neural networks using 8-bit floating point (FP8) numbers. Reduced bit precision allows for a larger effective memory and increased computational speed. We name this method Shifted and Squeezed FP8 (S2FP8). We show that, unlike previous 8-bit precision training methods, the proposed method works out-of-the-box for representative models: ResNet-50, Transformer and NCF. The method can maintain model accuracy without requiring fine-tuning loss scaling parameters or keeping certain layers in single precision. We introduce two learnable statistics of the DNN tensors -shifted and squeezed factors that are used to optimally adjust the range of the tensors in 8-bits, thus minimizing the loss in information due to quantization. * Work performed during an internship at Intel † Equal contribution arXiv:2001.05674v1 [cs.LG]",
        "id":210699231
      },
      {
        "title":"Referring Expression Generation in time-constrained communication",
        "text":"In game-like applications and many others, an underlying Natural Language Generation system may have to express urgency or other dynamic aspects of a fast-evolving situation as text, which may be considerably different from text produced under so-called 'normal' circumstances (e.g., without time constrains). As a means to shed light on possible differences of this kind, this paper addresses the computational generation of natural language text in time-constrained communication by presenting two experiments that use the attribute selection task of definite descriptions (or Referring Expression Generation -REG) as a working example. In the first experiment, we describe a psycholinguistic study in which human participants are engaged in a time-constrained reference production task. This results in a corpus of time-constrained descriptions to be compared with 'normal' descriptions available from an existing (i.e., with no time constraint) REG corpus. In the second experiment, we discuss how a REG algorithm may be customised so as to produce time-constrained descriptions that resemble those produced by human speakers in similar situations. The proposed algorithm is then evaluated against the time-constrained descriptions produced by the human subjects in the first experiment, and it is shown to outperform standard approaches to REG in these conditions.",
        "id":21697022
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Is there a dataset containing question-answer pairs used in psychological counseling available for research?",
    "positive_ctxs":[
      {
        "title":"PsyQA: A Chinese Dataset for Generating Long Counseling Text for Mental Health Support",
        "text":"Great research interests have been attracted to devise AI services that are able to provide mental health support. However, the lack of corpora is a main obstacle to this research, particularly in Chinese language. In this paper, we propose PsyQA, a Chinese dataset of psychological health support in the form of question and answer pair. PsyQA is crawled from a Chinese mental health service platform, and contains 22K questions and 56K long and wellstructured answers. Based on the psychological counseling theories, we annotate a portion of answer texts with typical strategies for providing support, and further present in-depth analysis of both lexical features and strategy patterns in the counseling answers. We also evaluate the performance of generating counseling answers with the generative pretrained models. Results show that utilizing strategies enhances the fluency and helpfulness of generated answers, but there is still a large space for future research.",
        "id":235313818
      }
    ],
    "negative_ctxs":[
      {
        "title":"Evaluating the Impact of Using a Domain-specific Bilingual Lexicon on the Performance of a Hybrid Machine Translation Approach",
        "text":"This paper describes an Example-Based Machine Translation prototype and presents an evaluation of the impact of using a domainspecific vocabulary on its performance. This prototype is based on a hybrid approach which needs only monolingual texts in the target language and consists to combine translation candidates returned by a cross-language search engine with translation hypotheses provided by a finite-state transducer. The results of this combination are evaluated against a statistical language model of the target language in order to obtain the n-best translations. To measure the performance of this hybrid approach, we achieved several experiments using corpora on two domains from the European Parliament proceedings (Europarl) and the European Medicines Agency documents (Emea). The obtained results show that the proposed approach outperforms the state-of-the-art Statistical Machine Translation system Moses when texts to translate are related to the specialized domain.",
        "id":16539945
      },
      {
        "title":"The ACL Anthology Reference Corpus: A Reference Dataset for Bibliographic Research in Computational Linguistics",
        "text":"The ACL Anthology is a digital archive of conference and journal papers in natural language processing and computational linguistics. Its primary purpose is to serve as a reference repository of research results, but we believe that it can also be an object of study and a platform for research in its own right. We describe an enriched and standardized reference corpus derived from the ACL Anthology that can be used for research in scholarly document processing. This corpus, which we call the ACL Anthology Reference Corpus (ACL ARC), brings together the recent activities of a number of research groups around the world. Our goal is to make the corpus widely available, and to encourage other researchers to use it as a standard testbed for experiments in both bibliographic and bibliometric research.",
        "id":174500
      },
      {
        "title":"A Comparison of Approaches for Sentiment Classification on Lithuanian Internet Comments",
        "text":"Despite many methods that effectively solve sentiment classification task for such widely used languages as English, there is no clear answer which methods are the most suitable for the languages that are substantially different. In this paper we attempt to solve Internet comments sentiment classification task for Lithuanian, using two classification approachesknowledge-based and supervised machine learning. We explore an influence of sentiment word dictionaries based on the different parts-of-speech (adjectives, adverbs, nouns, and verbs) for knowledge-based method; different feature types (bag-ofwords, lemmas, word n-grams, character n-grams) for machine learning methods; and pre-processing techniques (emoticons replacement with sentiment words, diacritics replacement, etc.) for both approaches. Despite that supervised machine learning methods (Support Vector Machine and Naïve Bayes Multinomial) significantly outperform proposed knowledge-based method all obtained results are above baseline. The best accuracy 0.679 was achieved with Naïve Bayes Multinomial and token unigrams plus bigrams, when pre-processing involved diacritics replacement.",
        "id":1190575
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Is there a research paper that has developed a customer service conversation dataset aimed at forecasting customer intentions, taking into account the limitations imposed by agent protocols?",
    "positive_ctxs":[
      {
        "title":"Action-Based Conversations Dataset: A Corpus for Building More In-Depth Task-Oriented Dialogue Systems",
        "text":"Existing goal-oriented dialogue datasets focus mainly on identifying slots and values. However, customer support interactions in reality often involve agents following multi-step procedures derived from explicitly-defined company policies as well. To study customer service dialogue systems in more realistic settings, we introduce the Action-Based Conversations Dataset (ABCD), a fully-labeled dataset with over 10K human-to-human dialogues containing 55 distinct user intents requiring unique sequences of actions constrained by policies to achieve task success.We propose two additional dialog tasks, Action State Tracking and Cascading Dialogue Success, and establish a series of baselines involving large-scale, pre-trained language models on this dataset. Empirical results demonstrate that while more sophisticated networks outperform simpler models, a considerable gap (50.8% absolute accuracy) still exists to reach human-level performance on ABCD. 1",
        "id":233004708
      }
    ],
    "negative_ctxs":[
      {
        "title":"Toward Better Storylines with Sentence-Level Language Models",
        "text":"We propose a sentence-level language model which selects the next sentence in a story from a finite set of fluent alternatives. Since it does not need to model fluency, the sentence-level language model can focus on longer range dependencies, which are crucial for multisentence coherence. Rather than dealing with individual words, our method treats the story so far as a list of pre-trained sentence embeddings and predicts an embedding for the next sentence, which is more efficient than predicting word embeddings. Notably this allows us to consider a large number of candidates for the next sentence during training. We demonstrate the effectiveness of our approach with state-of-the-art accuracy on the unsupervised Story Cloze task and with promising results on larger-scale next sentence prediction tasks.",
        "id":218551245
      },
      {
        "title":"ReAugKD: Retrieval-Augmented Knowledge Distillation For Pre-trained Language Models",
        "text":"Knowledge Distillation (KD)(Hinton et al., 2015)is one of the most effective approaches for deploying large-scale pre-trained language models in low-latency environments by transferring the knowledge contained in the largescale models to smaller student models. Previous KD approaches use the soft labels and intermediate activations generated by the teacher to transfer knowledge to the student model parameters alone. In this paper, we show that having access to non-parametric memory in the form of a knowledge base with the teacher's soft labels and predictions can further enhance student capacity and improve generalization. To enable the student to retrieve from the knowledge base effectively, we propose a new Retrieval-augmented KD framework with a loss function that aligns the relational knowledge in teacher and student embedding spaces. We show through extensive experiments that our retrieval mechanism can achieve state-of-the-art performance for taskspecific knowledge distillation on the GLUE benchmark(Wang et al., 2018a).",
        "id":259370551
      },
      {
        "title":"Exploiting Multi-Word Units in History-Based Probabilistic Generation",
        "text":"We present a simple history-based model for sentence generation from LFG f-structures, which improves on the accuracy of previous models by breaking down PCFG independence assumptions so that more f-structure conditioning context is used in the prediction of grammar rule expansions. In addition, we present work on experiments with named entities and other multi-word units, showing a statistically significant improvement of generation accuracy. Tested on section 23 of the Penn Wall Street Journal Treebank, the techniques described in this paper improve BLEU scores from 66.52 to 68.82, and coverage from 98.18% to 99.96%.",
        "id":10339151
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Is there a specialized question answering dataset that concentrates on intricate tables within certain sectors, like the airline industry?",
    "positive_ctxs":[
      {
        "title":"AIT-QA: Question Answering Dataset over Complex Tables in the Airline Industry",
        "text":"",
        "id":235623770
      }
    ],
    "negative_ctxs":[
      {
        "title":"Vocabulary Decomposition for Estonian Open Vocabulary Speech Recognition",
        "text":"Speech recognition in many morphologically rich languages suffers from a very high out-of-vocabulary (OOV) ratio. Earlier work has shown that vocabulary decomposition methods can practically solve this problem for a subset of these languages. This paper compares various vocabulary decomposition approaches to open vocabulary speech recognition, using Estonian speech recognition as a benchmark. Comparisons are performed utilizing large models of 60000 lexical items and smaller vocabularies of 5000 items. A large vocabulary model based on a manually constructed morphological tagger is shown to give the lowest word error rate, while the unsupervised morphology discovery method Morfessor Baseline gives marginally weaker results. Only the Morfessor-based approach is shown to adequately scale to smaller vocabulary sizes.",
        "id":524673
      },
      {
        "title":"BioinformaticsUA: Concept Recognition in Clinical Narratives Using a Modular and Highly Efficient Text Processing Framework",
        "text":"Clinical texts, such as discharge summaries or test reports, contain a valuable amount of information that, if efficiently and effectively mined, could be used to infer new knowledge, possibly leading to better diagnosis and therapeutics. With this in mind, the SemEval-2014 Analysis of Clinical Text task aimed at assessing and improving current methods for identification and normalization of concepts occurring in clinical narrative. This paper describes our approach in this task, which was based on a fully modular architecture for text mining. We followed a pure dictionary-based approach, after performing error analysis to refine our dictionaries.We obtained an F-measure of 69.4% in the entity recognition task, achieving the second best precision over all submitted runs (81.3%), with above average recall (60.5%). In the normalization task, we achieved a strict accuracy of 53.1% and a relaxed accuracy of 87.0%.",
        "id":8675882
      },
      {
        "title":"QCS: A Tool for Querying, Clustering, and Summarizing Documents",
        "text":"The QCS information retrieval (IR) system is presented as a tool for querying, clustering, and summarizing document sets. QCS has been developed as a modular development framework, and thus facilitates the inclusion of new technologies targeting these three IR tasks. Details of the system architecture, the QCS interface, and preliminary results are presented.",
        "id":5517454
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Is there a study that investigates if large language models can assist with generating ideas pertinent to scientific concepts during the scientific writing ideation stage?",
    "positive_ctxs":[
      {
        "title":"Sparks: Inspiration for Science Writing using Language Models",
        "text":"Large-scale language models are rapidly improving, performing well on a variety of tasks with little to no customization. In this work we investigate how language models can support science writing, a challenging writing task that is both open-ended and highly constrained. We present a system for generating \"sparks\", sentences related to a scientific concept intended to inspire writers. We run a user study with 13 STEM graduate students and find three main use cases of sparks-inspiration, translation, and perspective-each of which correlates with a unique interaction pattern. We also find that while participants were more likely to select higher quality sparks, the overall quality of sparks seen by a given participant did not correlate with their satisfaction with the tool. 1",
        "id":239009871
      }
    ],
    "negative_ctxs":[
      {
        "title":"Fast Interleaved Bidirectional Sequence Generation",
        "text":"Independence assumptions during sequence generation can speed up inference, but parallel generation of highly inter-dependent tokens comes at a cost in quality. Instead of assuming independence between neighbouring tokens (semi-autoregressive decoding, SA), we take inspiration from bidirectional sequence generation and introduce a decoder that generates target words from the left-to-right and right-toleft directions simultaneously. We show that we can easily convert a standard architecture for unidirectional decoding into a bidirectional decoder by simply interleaving the two directions and adapting the word positions and selfattention masks. Our interleaved bidirectional decoder (IBDecoder) retains the model simplicity and training efficiency of the standard Transformer, and on five machine translation tasks and two document summarization tasks, achieves a decoding speedup of ∼2× compared to autoregressive decoding with comparable quality. Notably, it outperforms left-toright SA because the independence assumptions in IBDecoder are more felicitous. To achieve even higher speedups, we explore hybrid models where we either simultaneously predict multiple neighbouring tokens per direction, or perform multi-directional decoding by partitioning the target sequence. These methods achieve speedups to 4×-11× across different tasks at the cost of <1 BLEU or <0.5 ROUGE (on average). 1",
        "id":225075957
      },
      {
        "title":"Semantic Web based Machine Translation",
        "text":"This paper describes the experimental combination of traditional Natural Language Processing (NLP) technology with the Semantic Web building stack in order to extend the expert knowledge required for a Machine Translation (MT) task. Therefore, we first give a short introduction in the state of the art of MT and the Semantic Web and discuss the problem of disambiguation being one of the common challenges in MT which can only be solved using world knowledge during the disambiguation process. In the following, we construct a sample sentence which demonstrates the need for world knowledge and design a prototypical program as a successful solution for the outlined translation problem. We conclude with a critical view on the developed approach.",
        "id":10349161
      },
      {
        "title":"Automatic annotation of speculation in biomedical texts: new perspec- tives and large-scale evaluation",
        "text":"One emergent field in text mining tools applied to biological texts is the automatic detection of speculative sentences. In this paper, we test on a large scale BioExcom, a rule-based system which annotates and categorizes automatically speculative sentences (\"prior\" and \"new\"). This work enables us to highlight a more restrictive way to consider speculations, viewed as a source of knowledge, and to discuss the criteria used to determine if a sentence is speculative or not. By doing so, we demonstrate the efficiency of BioExcom to extract these types of speculations and we argue the importance of this tool for biologists, who are also interested in finding hypotheses.",
        "id":16956988
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Is there any paper that tried fine-tuning mBERT to enhance word-level alignment in a multilingual setting?",
    "positive_ctxs":[
      {
        "title":"Word Alignment by Fine-tuning Embeddings on Parallel Corpora",
        "text":"Word alignment over parallel corpora has a wide variety of applications, including learning translation lexicons, cross-lingual transfer of language processing tools, and automatic evaluation or analysis of translation outputs. The great majority of past work on word alignment has worked by performing unsupervised learning on parallel text. Recently, however, other work has demonstrated that pre-trained contextualized word embeddings derived from multilingually trained language models (LMs) prove an attractive alternative, achieving competitive results on the word alignment task even in the absence of explicit training on parallel data. In this paper, we examine methods to marry the two approaches: leveraging pre-trained LMs but finetuning them on parallel text with objectives designed to improve alignment quality, and proposing methods to effectively extract alignments from these fine-tuned models. We perform experiments on five language pairs and demonstrate that our model can consistently outperform previous state-of-the-art models of all varieties. In addition, we demonstrate that we are able to train multilingual word aligners that can obtain robust performance on different language pairs. Our aligner, AWE-SOME (Aligning Word Embedding Spaces Of Multilingual Encoders), with pre-trained models is available at https:\/\/github. com\/neulab\/awesome-align. Amodei. 2020. Language models are few-shot learners. arXiv preprint.",
        "id":231648372
      }
    ],
    "negative_ctxs":[
      {
        "title":"Thai Sentence Paraphrasing from the Lexical Resource",
        "text":"Paraphrase generation in any language has gained much attention and importance in the study of Natural Language Processing. Therefore, the focus of this paper is on Thai language paraphrase generation for the sentence level. Six sentence paraphrasing techniques for Thai are proposed and illustratively explained. In addition, the Thai-sentence Paraphrase Generation (TPG) system is designed using a lexical resource based system subsequently entitled the Thai Lexical Conceptual Structure with Thai Lexicalized Tree Adjoining Grammar (TLCS-TLTAG) Resource.",
        "id":12341085
      },
      {
        "title":"Constructing Open Cloze Tests Using Generation and Discrimination Capabilities of Transformers",
        "text":"This paper presents the first multi-objective transformer model for constructing open cloze tests that exploits generation and discrimination capabilities to improve performance. Our model is further enhanced by tweaking its loss function and applying a post-processing reranking algorithm that improves overall test structure. Experiments using automatic and human evaluation show that our approach can achieve up to 82% accuracy according to experts, outperforming previous work and baselines. We also release a collection of highquality open cloze tests along with sample system output and human annotations that can serve as a future benchmark.",
        "id":248218650
      },
      {
        "title":"Named Entity Recognition Based Automatic Generation of Research Highlights",
        "text":"A scientific paper is traditionally prefaced by an abstract that summarizes the paper. Recently, research highlights that focus on the main findings of the paper have emerged as a complementary summary in addition to an abstract. However, highlights are not yet as common as abstracts, and are absent in many papers. In this paper, we aim to automatically generate research highlights using different sections of a research paper as input. We investigate whether the use of named entity recognition on the input improves the quality of the generated highlights. In particular, we have used two deep learning-based models: the first is a pointer-generator network, and the second augments the first model with coverage mechanism. We then augment each of the above models with named entity recognition features. The proposed method can be used to produce highlights for papers with missing highlights. Our experiments show that adding named entity information improves the performance of the deep learning-based summarizers in terms of ROUGE, METEOR and BERTScore measures.",
        "id":252819161
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Is there any research that investigates how to use Transformer decoders to extract interactive shared representations from CNN networks on clinical notes?",
    "positive_ctxs":[
      {
        "title":"Automatic ICD Coding via Interactive Shared Representation Networks with Self-distillation Mechanism",
        "text":"The ICD coding task aims at assigning codes of the International Classification of Diseases in clinical notes. Since manual coding is very laborious and prone to errors, many methods have been proposed for the automatic ICD coding task. However, existing works either ignore the long-tail of code frequency or the noisy clinical notes. To address the above issues, we propose an Interactive Shared Representation Network with Self-Distillation mechanism. Specifically, an interactive shared representation network targets building connections among codes while modeling the cooccurrence, consequently alleviating the longtail problem. Moreover, to cope with the noisy text issue, we encourage the model to focus on the clinical note's noteworthy part and extract valuable information through a self-distillation learning mechanism. Experimental results on two MIMIC datasets demonstrate the effectiveness of our method.",
        "id":236459913
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":185879829
      },
      {
        "title":"",
        "text":"",
        "id":218974338
      },
      {
        "title":"ShadowGNN: Graph Projection Neural Network for Text-to-SQL Parser",
        "text":"Given a database schema, Text-to-SQL aims to translate a natural language question into the corresponding SQL query. Under the setup of cross-domain, traditional semantic parsing models struggle to adapt to unseen database schemas. To improve the model generalization capability for rare and unseen schemas, we propose a new architecture, ShadowGNN, which processes schemas at abstract and semantic levels. By ignoring names of semantic items in databases, abstract schemas are exploited in a well-designed graph projection neural network to obtain delexicalized representation of question and schema. Based on the domain-independent representations, a relation-aware transformer is utilized to further extract logical linking between question and schema. Finally, a SQL decoder with context-free grammar is applied. On the challenging Text-to-SQL benchmark Spider, empirical results show that ShadowGNN outperforms state-of-the-art models. When the annotated data is extremely limited (only 10% training set), ShadowGNN gets over absolute 5% performance gain, which shows its powerful generalization ability. Our implementation will be open-sourced at https:\/\/github. com\/WowCZ\/shadowgnn.",
        "id":233210172
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Is there research examining if multilingual pre-trained models utilize identical sets of neural units to encode morphosyntactic features in various languages?",
    "positive_ctxs":[
      {
        "title":"Same Neurons, Different Languages: Probing Morphosyntax in Multilingual Pre-trained Models",
        "text":"The success of multilingual pre-trained models is underpinned by their ability to learn representations shared by multiple languages even in absence of any explicit supervision. However, it remains unclear how these models learn to generalise across languages. In this work, we conjecture that multilingual pretrained models can derive language-universal abstractions about grammar. In particular, we investigate whether morphosyntactic information is encoded in the same subset of neurons in different languages. We conduct the first large-scale empirical study over 43 languages and 14 morphosyntactic categories with a state-of-the-art neuron-level probe. Our findings show that the cross-lingual overlap between neurons is significant, but its extent may vary across categories and depends on language proximity and pre-training data size.",
        "id":248512463
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"A proposal is made for the use of contextual information in the machine translation of Japanese and English. This paper describes the use of a Context Monitor to nlaintaill contextual infortmttion dyn,'unically and the ~tugmenlalion of appropriate features to ~t semantic network to enahh~ simple inference. The al}proach taken is that {}t\" \"best gucs~?' processing with the cont~extual information being hal~dled with semantic inf{~rmalion on ~ shallow level.IntroductionCurrent Machine qh'anslatiou (MT) systems proc,~ss input sentence by sentence. I[owever, experience wil.h English and Japanese has shown that some languages difl'er to such a degree that sentential translation yiehls poor results, l,eL us first compare the results of a conventional MT sysl.em with those we expect, t,o get for MT with context: t. J::q'lJf]~-._-i~¢{:¢\/,?)[ b v,-)--1\/~3-'.~E~gfi £\" 2)i L v, K 2. K-7: ~t-laJaS'd?,l~ ~a ~t~.I;~y b t:: ok-c g ];! ( 5'~koThis might be translated by a current, machine tl'anslation system as shown inFigure 11:It can clearly l)e seen that meaning in IHally seI\/tenees is obscured. Let us compare this with I.he resuits of a system using simple cont.exl.ual informal,ion ms shown inFigure2: This secol\/d translation is i-tlllch Ill(H'(? CO]lO['{~ll{. ;IH{I better preserves the meaning of the original se,lten{'o.An attempt has therefore I}een made to solve some of tile problems of translal.ing languages SllCIt sis Japanese and English using contextual information. Due to [.he consideral.ions of wanting to produce a high quality small-sized MT system, lhe approach taken is to use tile resources awdlahle in an exisl;ing MT system and to process the contextu;d i,l['orlmd.ion l There is obviously n great difference in results Imtweet, systems, hnl, l.hese translat.icms relweSent tyl}iCal {uHe, llted) r~.stdts fi'om a numher of systems, a) and I}) options,hq}end on the default settings of individual systems 1,0(] usinp; nn l,FC-LIike gramma.tieal formalism 2. ']'he current, dictionaries conl,~dn information to translal.e aboul; 300 words. There are 350 grammar rules which (:own' a wide range of sentence pal,terns.The context monil,or operates using information retrieved from file f-sLrueturc of ;t SOlli.eltCe al't(W analysis. This ilfforlnal.ion is then used during the transfer 2'['[le cn'igimd lm,gl.anl for l;;hgllnh-Spanlsh iranslallcm deveh,p,,d I,y (1, Amm'es [:\\m, wes'89]  has been widely adapted and eldal'ged Io .I-I'; & E-J Iranslallon.",
        "id":18991996
      },
      {
        "title":"From phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning",
        "text":"We present a model of visually-grounded language learning based on stacked gated recurrent neural networks which learns to predict visual features given an image description in the form of a sequence of phonemes. The learning task resembles that faced by human language learners who need to discover both structure and meaning from noisy and ambiguous data across modalities. We show that our model indeed learns to predict features of the visual context given phonetically transcribed image descriptions, and show that it represents linguistic information in a hierarchy of levels: lower layers in the stack are comparatively more sensitive to form, whereas higher layers are more sensitive to meaning.",
        "id":11631121
      },
      {
        "title":"Manipuri-English Bidirectional Statistical Machine Translation Systems using Morphology and Dependency Relations",
        "text":"The present work reports the development of Manipuri-English bidirectional statistical machine translation systems. In the English-Manipuri statistical machine translation system, the role of the suffixes and dependency relations on the source side and case markers on the target side are identified as important translation factors. A parallel corpus of 10350 sentences from news domain is used for training and the system is tested with 500 sentences. Using the proposed translation factors, the output of the translation quality is improved as indicated by baseline BLEU score of 13.045 and factored BLEU score of 16.873 respectively. Similarly, for the Manipuri English system, the role of case markers and POS tags information at the source side and suffixes and dependency relations at the target side are identified as useful translation factors. The case markers and suffixes are not only responsible to determine the word classes but also to determine the dependency relations. Using these translation factors, the output of the translation quality is improved as indicated by baseline BLEU score of 13.452 and factored BLEU score of 17.573 respectively. Further, the subjective evaluation indicates the improvement in the fluency and adequacy of both the factored SMT outputs over the respective baseline systems.",
        "id":13113510
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Is there research on a specialized language model designed to detect mental health issues on social media platforms?",
    "positive_ctxs":[
      {
        "title":"MentalBERT: Publicly Available Pretrained Language Models for Mental Healthcare",
        "text":"Mental health is a critical issue in modern society, and mental disorders could sometimes turn to suicidal ideation without adequate treatment. Early detection of mental disorders and suicidal ideation from social content provides a potential way for effective social intervention. Recent advances in pretrained contextualized language representations have promoted the development of several domain-specific pretrained models and facilitated several downstream applications. However, there are no existing pretrained language models for mental healthcare. This paper trains and releases two pretrained masked language models, i.e., MentalBERT and MentalRoBERTa, to benefit machine learning for the mental healthcare research community. Besides, we evaluate our trained domain-specific models and several variants of pretrained language models on several mental disorder detection benchmarks and demonstrate that language representations pretrained in the target domain improve the performance of mental health detection tasks.",
        "id":240288892
      }
    ],
    "negative_ctxs":[
      {
        "title":"An Evolutionary Algorithm for Automatic Summarization",
        "text":"This paper proposes a novel method to select sentences for automatic summarization based on an evolutionary algorithm. The algorithm explores candidate summaries space following an objective function computed over ngrams probability distributions of the candidate summary and the source documents. This method does not consider a summary as a stack of independent sentences but as a whole text, and makes use of advances in unsupervised summarization evaluation. We compare this sentence extraction method to one of the best existing methods which is based on integer linear programming, and show its efficiency on three different acknowledged corpora.",
        "id":3259134
      },
      {
        "title":"Language Identification in Code-Switched Text Using Conditional Random Fields and Babelnet",
        "text":"The paper outlines a supervised approach to language identification in code-switched data, framing this as a sequence labeling task where the label of each token is identified using a classifier based on Conditional Random Fields and trained on a range of different features, extracted both from the training data and by using information from Babelnet and Babelfy.The method was tested on the development dataset provided by organizers of the shared task on language identification in codeswitched data, obtaining tweet level monolingual, code-switched and weighted F1-scores of 94%, 85% and 91%, respectively, with a token level accuracy of 95.8%. When evaluated on the unseen test data, the system achieved 90%, 85% and 87.4% monolingual, code-switched and weighted tweet level F1scores, and a token level accuracy of 95.7%.",
        "id":10254810
      },
      {
        "title":"The Norwegian Dependency Treebank",
        "text":"The Norwegian Dependency Treebank is a new syntactic treebank for Norwegian Bokmål and Nynorsk with manual syntactic and morphological annotation, developed at the National Library of Norway in collaboration with the University of Oslo. It is the first publically available treebank for Norwegian. This paper presents the core principles behind the syntactic annotation and how these principles were employed in certain specific cases. We then present the selection of texts and distribution between genres, as well as the annotation process and an evaluation of the inter-annotator agreement. Finally, we present the first results of data-driven dependency parsing of Norwegian, contrasting four state-of-the-art dependency parsers trained on the treebank. The consistency and the parsability of this treebank is shown to be comparable to other large treebank initiatives.",
        "id":7770967
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Is there research that argues for transparency and open-access to the training data of LLMs and demontrates its importance with case studies of existing data?",
    "positive_ctxs":[
      {
        "title":"Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus",
        "text":"Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of Common Crawl. We begin by investigating where the data came from, and find a significant amount of text from unexpected sources like patents and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from machine translation systems) and evaluation examples from other benchmark NLP datasets. To understand the impact of the filters applied to create this dataset, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about minority individuals. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet.",
        "id":237568724
      }
    ],
    "negative_ctxs":[
      {
        "title":"FRAGMENTATION AND PART OF SPEECH DISAMBIGUATION l",
        "text":"That at least some syntax is necessary to support semantic processing is fairly obvious. To know exactly how much syntax is needed, however, and how and when to apply it, is still an open and crucial, albeit old, question. This paper discusses the solutions used in a semantic analyser of French called SABA, developed at the University of Liege, Belgium. Specifically, we shall argue in favor of the usefulness of two syntactic processes: fragmentation, which can he interleaved with semantic processing, and part-of-speech disambiguation, which can be performed as a preprocesslng step.",
        "id":17096521
      },
      {
        "title":"Learning the Countability of English Nouns from Corpus Data",
        "text":"This paper describes a method for learning the countability preferences of English nouns from raw text corpora. The method maps the corpus-attested lexico-syntactic properties of each noun onto a feature vector, and uses a suite of memory-based classifiers to predict membership in 4 countability classes. We were able to assign countability to English nouns with a precision of 94.6%.",
        "id":7506619
      },
      {
        "title":"",
        "text":"",
        "id":235097358
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Is there research that investigates embedding multi-bit data into watermarks to improve resilience to text corruption, particularly aimed at safeguarding keywords and syntactic elements from modification?",
    "positive_ctxs":[
      {
        "title":"Robust Multi-bit Natural Language Watermarking through Invariant Features",
        "text":"Recent years have witnessed a proliferation of valuable original natural language contents found in subscription-based media outlets, web novel platforms, and outputs of large language models. However, these contents are susceptible to illegal piracy and potential misuse without proper security measures. This calls for a secure watermarking system to guarantee copyright protection through leakage tracing or ownership identification. To effectively combat piracy and protect copyrights, a multi-bit watermarking framework should be able to embed adequate bits of information and extract the watermarks in a robust manner despite possible corruption. In this work, we explore ways to advance both payload and robustness by following a well-known proposition from image watermarking and identify features in natural language that are invariant to minor corruption. Through a systematic analysis of the possible sources of errors, we further propose a corruption-resistant infill model. Our full method improves upon the previous work on robustness by +16.8% point on average on four datasets, three corruption types, and two corruption ratios. 1",
        "id":259129912
      }
    ],
    "negative_ctxs":[
      {
        "title":"Semi-automatic Annotation of Chinese Word Structure",
        "text":"Chinese word structure annotation is potentially useful for many NLP tasks, especially for Chinese word segmentation. Li and Zhou (2012) have presented an annotation for word structures in the Penn Chinese Treebank. But they only consider words that have productive affixes, which covers 35% of word types in that corpus. In this paper, we propose a linguistically inspired annotation that covers various morphological derivations of Chinese in a more general way, such that almost all multiple-character words can be structurally analyzed. As manual annotation is expensive, we propose a semi-supervised approach to automatic annotation, which combines the maximum entropy learning and the EM iteration for the Gaussian mixture model. The proposed method has achieved an accuracy of 90% on the testing set.",
        "id":10002136
      },
      {
        "title":"Fostering the Next Generation of European Language Technology: Recent Developments -Emerging Initiatives -Challenges and Opportunities",
        "text":"META-NET is a European network of excellence, founded in 2010, that consists of 60 research centres in 34 European countries. One of the key visions and goals of META-NET is a truly multilingual Europe, which is substantially supported and realised through language technologies. In this article we provide an overview of recent developments around the multilingual Europe topic, we also describe recent and upcoming events as well as recent and upcoming strategy papers. Furthermore, we provide overviews of two new emerging initiatives, the CEF.AT and ELRC activity on the one hand and the Cracking the Language Barrier federation on the other. The paper closes with several suggested next steps in order to address the current challenges and to open up new opportunities.",
        "id":27942273
      },
      {
        "title":"A Hybrid Hierarchical Model for Multi-Document Summarization",
        "text":"Scoring sentences in documents given abstract summaries created by humans is important in extractive multi-document summarization. In this paper, we formulate extractive summarization as a two step learning problem building a generative model for pattern discovery and a regression model for inference. We calculate scores for sentences in document clusters based on their latent characteristics using a hierarchical topic model. Then, using these scores, we train a regression model based on the lexical and structural characteristics of the sentences, and use the model to score sentences of new documents to form a summary. Our system advances current state-of-the-art improving ROUGE scores by ∼7%. Generated summaries are less redundant and more coherent based upon manual quality evaluations.",
        "id":5833592
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Is there work on text classification that explores using BERT for the text and using GNN for the label hierarchy?",
    "positive_ctxs":[
      {
        "title":"Hierarchy-aware Label Semantics Matching Network for Hierarchical Text Classification",
        "text":"Hierarchical text classification is an important yet challenging task due to the complex structure of the label hierarchy. Existing methods ignore the semantic relationship between text and labels, so they cannot make full use of the hierarchical information. To this end, we formulate the text-label semantics relationship as a semantic matching problem and thus propose a hierarchy-aware label semantics matching network (HiMatch). First, we project text semantics and label semantics into a joint embedding space. We then introduce a joint embedding loss and a matching learning loss to model the matching relationship between the text semantics and the label semantics. Our model captures the text-label semantics matching relationship among coarse-grained labels and fine-grained labels in a hierarchy-aware manner. The experimental results on various benchmark datasets verify that our model achieves state-of-the-art results.",
        "id":236460056
      }
    ],
    "negative_ctxs":[
      {
        "title":"UD-Japanese BCCWJ: Universal Dependencies Annotation for the Balanced Corpus of Contemporary Written Japanese",
        "text":"In this paper, we describe a corpus UD Japanese-BCCWJ that was created by converting the Balanced Corpus of Contemporary Written Japanese (BCCWJ), a Japanese language corpus, to adhere to the UD annotation schema. The BCCWJ already assigns dependency information at the level of the bunsetsu (a Japanese syntactic unit comparable to the phrase). We developed a program to convert the BCCWJto UD based on this dependency structure, and this corpus is the result of completely automatic conversion using the program. UD Japanese-BCCWJ is the largestscale UD Japanese corpus and the secondlargest of all UD corpora, including 1,980 documents, 57,109 sentences, and 1,273k words across six distinct domains.",
        "id":53643217
      },
      {
        "title":"",
        "text":"",
        "id":218974227
      },
      {
        "title":"Unediting: Detecting Disfluencies Without Careful Transcripts",
        "text":"Speech transcripts often only capture semantic content, omitting disfluencies that can be useful for analyzing social dynamics of a discussion. This work describes steps in building a model that can recover a large fraction of locations where disfluencies were present, by transforming carefully annotated text to match the standard transcription style, introducing a two-stage model for handling different types of disfluencies, and applying semi-supervised learning. Experiments show improvement in disfluency detection on Supreme Court oral arguments, nearly 23% improvement in F1.",
        "id":849336
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"What approaches have been suggested to lower the computational demands of basic attention mechanisms in transformers, and is there a study that explores a memory-saving technique through selective key-value pairing for each query?",
    "positive_ctxs":[
      {
        "title":"Memory-efficient Transformers via Top-k Attention",
        "text":"Following the success of dot-product attention in Transformers, numerous approximations have been recently proposed to address its quadratic complexity with respect to the input length. While these variants are memory and compute efficient, it is not possible to directly use them with popular pre-trained language models trained using vanilla attention, without an expensive corrective pre-training stage. In this work, we propose a simple yet highly accurate approximation for vanilla attention. We process the queries in chunks, and for each query, compute the top-k scores with respect to the keys. Our approach offers several advantages: (a) its memory usage is linear in the input size, similar to linear attention variants, such as Performer and RFA (b) it is a drop-in replacement for vanilla attention that does not require any corrective pre-training, and (c) it can also lead to significant memory savings in the feed-forward layers after casting them into the familiar query-key-value framework. We evaluate the quality of top-k approximation for multi-head attention layers on the Long Range Arena Benchmark, and for feedforward layers of T5 and UnifiedQA on multiple QA datasets. We show our approach leads to accuracy that is nearly-identical to vanilla attention in multiple setups including training from scratch, fine-tuning, and zero-shot inference. * majority of work done while author was part of IBM AI Residency program. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150.",
        "id":235422257
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":8565686
      },
      {
        "title":"Shallow Convolutional Neural Network for Implicit Discourse Relation Recognition",
        "text":"Implicit discourse relation recognition remains a serious challenge due to the absence of discourse connectives. In this paper, we propose a Shallow Convolutional Neural Network (SCNN) for implicit discourse relation recognition, which contains only one hidden layer but is effective in relation recognition. The shallow structure alleviates the overfitting problem, while the convolution and nonlinear operations help preserve the recognition and generalization ability of our model. Experiments on the benchmark data set show that our model achieves comparable and even better performance when comparing against current state-of-the-art systems.",
        "id":12968123
      },
      {
        "title":"KYOTO: A System for Mining, Structuring, and Distributing Knowledge Across Languages and Cultures",
        "text":"We outline work performed within the framework of a current EC project. The goal is to construct a language-independent information system for a specific domain (environment\/ecology\/biodiversity) anchored in a language-independent ontology that is linked to wordnets in seven languages. For each language, information extraction and identification of lexicalized concepts with ontological entries is carried out by text miners (\"Kybots\"). The mapping of language-specific lexemes to the ontology allows for crosslinguistic identification and translation of equivalent terms. The infrastructure developed within this project enables long-range knowledge sharing and transfer across many languages and cultures, addressing the need for global and uniform transition of knowledge beyond the specific domains addressed here.",
        "id":3074735
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"What are some scholarly articles that explore scaling laws for parameter-efficient prompt tuning techniques for fine-tuning language models?",
    "positive_ctxs":[
      {
        "title":"The Power of Scale for Parameter-Efficient Prompt Tuning",
        "text":"In this work, we explore \"prompt tuning,\" a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient \"prompt ensembling.\" We release code and model checkpoints to reproduce our experiments. 1ReferencesRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,  Danilo Giampiccolo, Bernardo Magnini, and Idan  Szpektor. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognising textual entailment, volume 6, pages 6-4. Venice. . 2019a. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
        "id":233296808
      }
    ],
    "negative_ctxs":[
      {
        "title":"AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models",
        "text":"Pretrained language models (PLMs) are trained on massive corpora, but often need to specialize to specific domains. A parameter-efficient adaptation method suggests training an adapter for each domain on the task of language modeling. This leads to good in-domain scores but can be impractical for domain-or resourcerestricted settings. A solution is to use a relateddomain adapter for the novel domain at test time. In this paper, we introduce Adapter-Soup, an approach that performs weight-space averaging of adapters trained on different domains. Our approach is embarrassingly parallel: first, we train a set of domain-specific adapters; then, for each novel domain, we determine which adapters should be averaged at test time. We present extensive experiments showing that AdapterSoup consistently improves performance to new domains without extra training. We also explore weight averaging of adapters trained on the same domain with different hyper-parameters, and show that it preserves the performance of a PLM on new domains while obtaining strong in-domain results. We explore various approaches for choosing which adapters to combine, such as text clustering and semantic similarity. We find that using clustering leads to the most competitive results on novel domains.Domain AdapterAdapter Soupθ D k moyer. 2022. Branch-train-merge: Embarrassingly parallel training of expert language models. . 2019. Roberta: A robustly optimized bert pretraining approach.Michael Matena and Colin Raffel. 2021. Merging models with fisher-weighted averaging.",
        "id":256846453
      },
      {
        "title":"Mini But Mighty: Efficient Multilingual Pretraining with Linguistically-Informed Data Selection",
        "text":"With the prominence of large pretrained language models, low-resource languages are rarely modelled monolingually and become victims of the \"curse of multilinguality\" in massively multilingual models. Recently, Afri-BERTa showed that training transformer models from scratch on 1GB of data from many unrelated African languages outperforms massively multilingual models on downstream NLP tasks. Here we extend this direction, focusing on the use of related languages. We propose that training on smaller amounts of data but from related languages could match the performance of models trained on large, unrelated data. We test our hypothesis on the Niger-Congo family and its Bantu and Volta-Niger sub-families, pretraining models with data solely from Niger-Congo languages and finetuning on 4 downstream tasks: NER, partof-speech tagging, sentiment analysis and text classification. We find that models trained on genetically related languages achieve equal performance on downstream tasks in low-resource languages despite using less training data. We recommend selecting training data based on language-relatedness when pretraining language models for low-resource languages.",
        "id":258378192
      },
      {
        "title":"F-Score Driven Max Margin Neural Network for Named Entity Recognition in Chinese Social Media",
        "text":"We focus on named entity recognition (NER) for Chinese social media. With massive unlabeled text and quite limited labelled corpus, we propose a semisupervised learning model based on B-LSTM neural network. To take advantage of traditional methods in NER such as CRF, we combine transition probability with deep learning in our model. To bridge the gap between label accuracy and F-score of NER, we construct a model which can be directly trained on F-score. When considering the instability of Fscore driven method and meaningful information provided by label accuracy, we propose an integrated method to train on both F-score and label accuracy. Our integrated model yields substantial improvement over previous state-of-the-art result.",
        "id":14874026
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"What are some scholarly articles that explore the enhancement of dense retrieval in student models through the application of prediction distributions from teacher models?",
    "positive_ctxs":[
      {
        "title":"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction",
        "text":"Neural information retrieval (IR) has greatly advanced search and other knowledgeintensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6-10×.",
        "id":244799249
      }
    ],
    "negative_ctxs":[
      {
        "title":"Trans-disciplinary spoken language processing studies for scientific understanding of second language learner's characteristics",
        "text":"Language and speech proficiency is considered to be an important factor to identify human beings. Though traditional studies on language and speech can reveal many aspects of their characteristics, we have not yet had a complete view of human's language and speech ability. I believe that trans-disciplinary studies will enable us to have its scientific modeling. In this talk, I would like to introduce our research efforts on segmental duration control as an example of research towards computational human modeling. The computational modeling of segmental duration that we have been studying around three decades not only contributes to prosody control in speech synthesis technology but also gives an integrated view of individual timing characteristics studied in phonetic science. Together with duration control modeling, a series of perceptual studies on duration modifications needed for model evaluation have suggested us a unified view of scientific understanding on rhythm and timing. Through the introduction of our current efforts on the objective evaluation of 2nd language (L2) proficiency in speech timing control, we will see that these models and findings are useful for L2 learning and acquisition. To conclude my talk, I finally introduce research consortium called AESOP (Asian English Speech cOrpus Project) where researchers in different fields (speech science, informatics, phonetics, psychology and language education) have started to work together by collecting commonly sharable L2 language and speech data.5",
        "id":30820840
      },
      {
        "title":"",
        "text":"",
        "id":236779173
      },
      {
        "title":"A Practical of Memory-based Approach for Improving Accuracy of MT",
        "text":"Rule-Based Machine Translation (RBMT)[1] approach is a major approach in MT research. It needs linguistic knowledge to create appropriate rules of translation. However, we cannot completely add all linguistic rules to the system because adding new rules may cause a conflict with the old ones. So, we propose a memory based approach to improve the translation quality without modifying the existing linguistic rules. This paper analyses the translation problems and shows how this approach works.",
        "id":27010733
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"What are some studies that explore data-poisoning strategies that only require very few poisoned training examples?",
    "positive_ctxs":[
      {
        "title":"Concealed Data Poisoning Attacks on NLP Models",
        "text":"Adversarial attacks alter NLP model predictions by perturbing test-time inputs. However, it is much less understood whether, and how, predictions can be manipulated with small, concealed changes to the training data. In this work, we develop a new data poisoning attack that allows an adversary to control model predictions whenever a desired trigger phrase is present in the input. For instance, we insert 50 poison examples into a sentiment model's training set that causes the model to frequently predict Positive whenever the input contains \"James Bond\". Crucially, we craft these poison examples using a gradient-based procedure so that they do not mention the trigger phrase. We also apply our poison attack to language modeling (\"Apple iPhone\" triggers negative generations) and machine translation (\"iced coffee\" mistranslated as \"hot coffee\"). We conclude by proposing three defenses that can mitigate our attack at some cost in prediction accuracy or extra human annotation.",
        "id":233230124
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":226283895
      },
      {
        "title":"Writing in a Second Language with Machine Translation (WiLMa)",
        "text":"The WiLMa project aims to assess the effects of using machine translation (MT) tools on the writing processes of second language (L2) learners of varying proficiency. Particular attention is given to individual variation in learners' tool use.",
        "id":249204474
      },
      {
        "title":"Evaluating Transferability of BERT Models on Uralic Languages",
        "text":"Transformer-based language models such as BERT have outperformed previous models on a large number of English benchmarks, but their evaluation is often limited to English or a small number of well-resourced languages. In this work, we evaluate monolingual, multilingual, and randomly initialized language models from the BERT family on a variety of Uralic languages including Estonian, Finnish, Hungarian, Erzya, Moksha, Karelian, Livvi, Komi Permyak, Komi Zyrian, Northern Sámi, and Skolt Sámi. When monolingual models are available (currently only et, fi, hu), these perform better on their native language, but in general they transfer worse than multilingual models or models of genetically unrelated languages that share the same character set. Remarkably, straightforward transfer of high-resource models, even without special efforts toward hyperparameter optimization, yields what appear to be state of the art POS and NER tools for the minority Uralic languages where there is sufficient data for finetuning.",
        "id":237502635
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"What are the latest advancements in predicting suicidal tendencies using innovative feature extraction methods?",
    "positive_ctxs":[
      {
        "title":"PHASE: Learning Emotional Phase-aware Representations for Suicide Ideation Detection on Social Media",
        "text":"Recent psychological studies indicate that individuals exhibiting suicidal ideation increasingly turn to social media rather than mental health practitioners. Contextualizing the buildup of such ideation is critical for the identification of users at risk. In this work, we focus on identifying suicidal intent in tweets by augmenting linguistic models with emotional phases modeled from users' historical context. We propose PHASE, a time-and phase-aware framework that adaptively learns features from a user's historical emotional spectrum on Twitter for preliminary screening of suicidal risk. Building on clinical studies, PHASE learns phase-like progressions in users' historical Plutchik-wheel-based emotions to contextualize suicidal intent. While outperforming stateof-the-art methods, we show the utility of temporal and phase-based emotional contextual cues for suicide ideation detection. We further discuss practical and ethical considerations. 1 Amy Bruckman. 2002. Studying the amateur artist: A perspective on disguising data collected in human subjects research on the internet. Ethics and Information Technology, 4(3):217-231. Craig J. Bryan. 2020. Chapter 4 -the temporal dynamics of the wish to live and the wish to die among suicidal individuals. In Andrew C. Page and Werner G.K. Stritzke, editors, Alternatives to Suicide, pages 71 -88. Academic Press. Craig J Bryan and M David Rudd. 2006. Advances in the assessment of suicide risk. Journal of clinical psychology, 62(2):185-200. Craig J Bryan and M David Rudd. 2016. The importance of temporal dynamics in the transition from suicidal thought to behavior. Nock. 2017. Risk factors for suicidal thoughts and behaviors: a metaanalysis of 50 years of research. Psychological bulletin, 143(2):187. King wa Fu, Ka Y. Liu, and Paul S. F. Yip. 2007. Predictive validity of the chinese version of the adult suicidal ideation questionnaire: Psychometric properties and its short version. Psychological Assessment, 19(4):422-429.",
        "id":233189632
      }
    ],
    "negative_ctxs":[
      {
        "title":"A Language Modeling Approach to Predicting Reading Difficulty",
        "text":"We demonstrate a new research approach to the problem of predicting the reading difficulty of a text passage, by recasting readability in terms of statistical language modeling. We derive a measure based on an extension of multinomial naïve Bayes classification that combines multiple language models to estimate the most likely grade level for a given passage. The resulting classifier is not specific to any particular subject and can be trained with relatively little labeled data. We perform predictions for individual Web pages in English and compare our performance to widely-used semantic variables from traditional readability measures. We show that with minimal changes, the classifier may be retrained for use with French Web documents. For both English and French, the classifier maintains consistently good correlation with labeled grade level (0.63 to 0.79) across all test sets. Some traditional semantic variables such as type-token ratio gave the best performance on commercial calibrated test passages, while our language modeling approach gave better accuracy for Web documents and very short passages (less than 10 words).",
        "id":5206782
      },
      {
        "title":"Semi-Automatic Construction of Text-to-SQL Data for Domain Transfer",
        "text":"Strong and affordable in-domain data is a desirable asset when transferring trained semantic parsers to novel domains. As previous methods for semi-automatically constructing such data cannot handle the complexity of realistic SQL queries, we propose to construct SQL queries via context-dependent sampling, and introduce the concept of topic. Along with our SQL query construction method, we propose a novel pipeline of semi-automatic Textto-SQL dataset construction that covers the broad space of SQL queries. We show that the created dataset is comparable with expert annotation along multiple dimensions, and is capable of improving domain transfer performance for SOTA semantic parsers.",
        "id":236486090
      },
      {
        "title":"On the Sentence Embeddings from Pre-trained Language Models",
        "text":"Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. The code is available at https:\/\/github.com\/ bohanli\/BERT-flow. * The work was done when BL was an intern at ByteDance.",
        "id":226262227
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"What are the latest developments in conversational agents that integrate external knowledge sources and employ diverse tactics to offer emotional support during interactions?",
    "positive_ctxs":[
      {
        "title":"MISC: A MIxed Strategy-Aware Model Integrating COMET for Emotional Support Conversation",
        "text":"Applying existing methods to emotional support conversation-which provides valuable assistance to people who are in need-has two major limitations: (a) they generally employ a conversation-level emotion label, which is too coarse-grained to capture user's instant mental state; (b) most of them focus on expressing empathy in the response(s) rather than gradually reducing user's distress. To address the problems, we propose a novel model MISC, which firstly infers the user's fine-grained emotional status, and then responds skillfully using a mixture of strategy. Experimental results on the benchmark dataset demonstrate the effectiveness of our method and reveal the benefits of fine-grained emotion understanding as well as mixed-up strategy modeling. Our code and data could be found in https: \/\/github.com\/morecry\/MISC.",
        "id":247748640
      }
    ],
    "negative_ctxs":[
      {
        "title":"On the difficulty of a distributional semantics of spoken language",
        "text":"In the domain of unsupervised learning most work on speech has focused on discovering low-level constructs such as phoneme inventories or word-like units. In contrast, for written language, where there is a large body of work on unsupervised induction of semantic representations of words, whole sentences and longer texts. In this study we examine the challenges of adapting these approaches from written to spoken language. We conjecture that unsupervised learning of the semantics of spoken language becomes feasible if we abstract from the surface variability. We simulate this setting with a dataset of utterances spoken by a realistic but uniform synthetic voice. We evaluate two simple unsupervised models which, to varying degrees of success, learn semantic representations of speech fragments. Finally we present inconclusive results on human speech, and discuss the challenges inherent in learning distributional semantic representations on unrestricted natural spoken language.",
        "id":4314399
      },
      {
        "title":"The Anatomy of a Systemic Choice",
        "text":"Scope and Purpose 1Choice is one of the most prominent organizing concepts in systemic linguistics. Languages are described in terms of the choices available to the speaker and the relationships of those choices to each other and to the language produced. This paper addresses the problems of characterizing processes of choosing in a systemic framework and creating a corresponding notation. Focus on notation is necessary at this point since development of notation must to some extent precede development of corresponding content. Although these developments are part of an investigation of computer text production, their significance is not confined to that enterprise; they are as significant in linguistics as in computer science. This paper focuses on the perspective of choice as a speaker's action. We hope that by exploring the nature of systemic choices, several kinds of interests in language will be served:Interest in Grammar as Language Description-Describing choices can yield a richer understanding of the alternatives offered, and how one system of choices differs from another.",
        "id":9972666
      },
      {
        "title":"FastClass: A Time-Efficient Approach to Weakly-Supervised Text Classification",
        "text":"Weakly-supervised text classification aims to train a classifier using only class descriptions and unlabeled data. Recent research shows that keyword-driven methods can achieve state-ofthe-art performance on various tasks. However, these methods not only rely on carefullycrafted class descriptions to obtain classspecific keywords but also require substantial amount of unlabeled data and takes a long time to train. This paper proposes FastClass, an efficient weakly-supervised classification approach. It uses dense text representation to retrieve class-relevant documents from external unlabeled corpus and selects an optimal subset to train a classifier. Compared to keyworddriven methods, our approach is less reliant on initial class descriptions as it no longer needs to expand each class description into a set of class-specific keywords. Experiments on a wide range of classification tasks show that the proposed approach frequently outperforms keyword-driven models in terms of classification accuracy and often enjoys orders-ofmagnitude faster training speed. * Corresponding authors.",
        "id":254564126
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"What benchmarks have prior research utilized to assess models performing knowledge-rich language tasks?",
    "positive_ctxs":[
      {
        "title":"KILT: a Benchmark for Knowledge Intensive Language Tasks",
        "text":"Challenging problems such as open-domain question answering, fact checking, slot filling and entity linking require access to large, external knowledge sources. While some models do well on individual tasks, developing general models is difficult as each task might require computationally expensive indexing of custom knowledge sources, in addition to dedicated infrastructure. To catalyze research on models that condition on specific information in large textual resources, we present a benchmark for knowledge-intensive language tasks (KILT). All tasks in KILT are grounded in the same snapshot of Wikipedia, reducing engineering turnaround through the reuse of components, as well as accelerating research into task-agnostic memory architectures. We test both task-specific and general baselines, evaluating downstream performance in addition to the ability of the models to provide provenance. We find that a shared dense vector index coupled with a seq2seq model is a strong baseline, outperforming more tailor-made approaches for fact checking, open-domain question answering and dialogue, and yielding competitive results on entity linking and slot filling, by generating disambiguated text. KILT data and code are available at https:\/\/github.com\/ facebookresearch\/KILT.",
        "id":221507798
      }
    ],
    "negative_ctxs":[
      {
        "title":"Entity Relation Extraction as Dependency Parsing in Visually Rich Documents",
        "text":"Previous works on key information extraction from visually rich documents (VRDs) mainly focus on labeling the text within each bounding box (i.e., semantic entity), while the relations in-between are largely unexplored. In this paper, we adapt the popular dependency parsing model, the biaffine parser, to this entity relation extraction task. Being different from the original dependency parsing model which recognizes dependency relations between words, we identify relations between groups of words with layout information instead. We have compared different representations of the semantic entity, different VRD encoders, and different relation decoders. For the model training, we explore multi-task learning to combine entity labeling and relation extraction tasks; and for the evaluation, we conduct experiments on different datasets with filtering and augmentation. The results demonstrate that our proposed model achieves 65.96% F1 score on the FUNSD dataset. As for the realworld application, our model has been applied to the in-house customs data, achieving reliable performance in the production setting. * Corresponding author. The author's contributions were carried out while at Alibaba Group. His current affiliation is Vipshop (China) Co., Ltd.",
        "id":239024709
      },
      {
        "title":"",
        "text":"",
        "id":219306404
      },
      {
        "title":"ConvoKit: A Toolkit for the Analysis of Conversations",
        "text":"This paper describes the design and functionality of ConvoKit, an open-source toolkit for analyzing conversations and the social interactions embedded within. ConvoKit provides an unified framework for representing and manipulating conversational data, as well as a large and diverse collection of conversational datasets. By providing an intuitive interface for exploring and interacting with conversational data, this toolkit lowers the technical barriers for the broad adoption of computational methods for conversational analysis.",
        "id":218581419
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"What concerns or key points have been highlighted in scholarly articles about employing random divisions in machine learning datasets, especially with respect to contamination of the test set?",
    "positive_ctxs":[
      {
        "title":"We Need to Talk About Random Splits",
        "text":"Gorman and Bedrick (2019) argued for using random splits rather than standard splits in NLP experiments. We argue that random splits, like standard splits, lead to overly optimistic performance estimates. We can also split data in biased or adversarial ways, e.g., training on short sentences and evaluating on long ones. Biased sampling has been used in domain adaptation to simulate real-world drift; this is known as the covariate shift assumption. In NLP, however, even worst-case splits, maximizing bias, often under-estimate the error observed on new samples of in-domain data, i.e., the data that models should minimally generalize to at test time. This invalidates the covariate shift assumption. Instead of using multiple random splits, future benchmarks should ideally include multiple, independent test sets instead; if infeasible, we argue that multiple biased splits leads to more realistic performance estimates than multiple random splits.",
        "id":218487319
      }
    ],
    "negative_ctxs":[
      {
        "title":"End-to-End Non-Factoid Question Answering with an Interactive Visualization of Neural Attention Weights",
        "text":"Advanced attention mechanisms are an important part of successful neural network approaches for non-factoid answer selection because they allow the models to focus on few important segments within rather long answer texts. Analyzing attention mechanisms is thus crucial for understanding strengths and weaknesses of particular models. We present an extensible, highly modular service architecture that enables the transformation of neural network models for non-factoid answer selection into fully featured end-to-end question answering systems. The primary objective of our system is to enable researchers a way to interactively explore and compare attentionbased neural networks for answer selection. Our interactive user interface helps researchers to better understand the capabilities of the different approaches and can aid qualitative analyses. The source-code of our system is publicly available. 1",
        "id":20020443
      },
      {
        "title":"Why Is MBTI Personality Detection from Texts a Difficult Task?",
        "text":"Automatic detection of the four MBTI personality dimensions from texts has recently attracted noticeable attention from the natural language processing and computational linguistic communities. Despite the large collections of Twitter data for training, the best systems rarely even outperform the majority-class baseline. In this paper, we discuss the theoretical reasons for such low results and present the insights from an annotation study that further shed the light on this issue.",
        "id":233189643
      },
      {
        "title":"A Rate-Distortion view of human pragmatic reasoning",
        "text":"What computational principles underlie human pragmatic reasoning? A prominent approach to pragmatics is the Rational Speech Act (RSA) framework, which formulates pragmatic reasoning as probabilistic speakers and listeners recursively reasoning about each other. While RSA enjoys broad empirical support, it is not yet clear whether the dynamics of such recursive reasoning may be governed by a general optimization principle. Here, we present a novel analysis of the RSA framework that addresses this question. First, we show that RSA recursion implements an alternating maximization for optimizing a tradeoff between expected utility and communicative effort. On that basis, we study the dynamics of RSA recursion and disconfirm the conjecture that expected utility is guaranteed to improve with recursion depth. Second, we show that RSA can be grounded in Rate-Distortion theory, while maintaining a similar ability to account for human behavior and avoiding a bias of RSA toward random utterance production. This work furthers the mathematical understanding of RSA models, and suggests that general information-theoretic principles may give rise to human pragmatic reasoning.",
        "id":218628625
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"What difficulties do neural conversational models face, particularly concerning the decoder's ability to produce precise and fact-based replies?",
    "positive_ctxs":[
      {
        "title":"Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding",
        "text":"Dialogue systems powered by large pretrained language models exhibit an innate ability to deliver fluent and natural-sounding responses. Despite their impressive performance, these models are fitful and can often generate factually incorrect statements impeding their widespread adoption. In this paper, we focus on the task of improving faithfulness and reducing hallucination of neural dialogue systems to known facts supplied by a Knowledge Graph (KG). We propose NEU-RAL PATH HUNTER which follows a generatethen-refine strategy whereby a generated response is amended using the KG. NEURAL PATH HUNTER leverages a separate tokenlevel fact critic to identify plausible sources of hallucination followed by a refinement stage that retrieves correct entities by crafting a query signal that is propagated over a k-hop subgraph. We empirically validate our proposed approach on the OpenDialKG dataset (Moon et al., 2019) against a suite of metrics and report a relative improvement of faithfulness over dialogue responses by 20.35% based on FeQA (Durmus et al., 2020). The code is available at https:\/\/github.com\/ nouhadziri\/Neural-Path-Hunter.",
        "id":233296059
      }
    ],
    "negative_ctxs":[
      {
        "title":"Swag: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference",
        "text":"Given a partial description like \"she opened the hood of the car,\" humans can reason about the situation and anticipate what might come next (\"then, she examined the engine\"). In this paper, we introduce the task of grounded commonsense inference, unifying natural language inference and commonsense reasoning.We present Swag, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations. To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data. To account for the aggressive adversarial filtering, we use state-of-theart language models to massively oversample a diverse set of potential counterfactuals. Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88%), various competitive models struggle on our task. We provide comprehensive analysis that indicates significant opportunities for future research.",
        "id":52019251
      },
      {
        "title":"",
        "text":"",
        "id":208281781
      },
      {
        "title":"De-Conflated Semantic Representations",
        "text":"One major deficiency of most semantic representation techniques is that they usually model a word type as a single point in the semantic space, hence conflating all the meanings that the word can have. Addressing this issue by learning distinct representations for individual meanings of words has been the subject of several research studies in the past few years. However, the generated sense representations are either not linked to any sense inventory or are unreliable for infrequent word senses. We propose a technique that tackles these problems by de-conflating the representations of words based on the deep knowledge it derives from a semantic network. Our approach provides multiple advantages in comparison to the past work, including its high coverage and the ability to generate accurate representations even for infrequent word senses. We carry out evaluations on six datasets across two semantic similarity tasks and report state-of-the-art results on most of them.",
        "id":16173223
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"What literature is available on training semantic parsers with deep learning for knowledge base question answering systems, especially those employing tree-structured representations of queries?",
    "positive_ctxs":[
      {
        "title":"Leveraging Abstract Meaning Representation for Knowledge Base Question Answering",
        "text":"Knowledge base question answering (KBQA) is an important task in Natural Language Processing. Existing approaches face significant challenges including complex question understanding, necessity for reasoning, and lack of large end-to-end training datasets. In this work, we propose Neuro-Symbolic Question Answering (NSQA), a modular KBQA system, that leverages (1) Abstract Meaning Representation (AMR) parses for task-independent question understanding; (2) a simple yet effective graph transformation approach to convert AMR parses into candidate logical queries that are aligned to the KB; (3) a pipeline-based approach which integrates multiple, reusable modules that are trained specifically for their individual tasks (semantic parser, entity and relationship linkers, and neuro-symbolic reasoner) and do not require end-to-end training data. NSQA achieves state-of-the-art performance on two prominent KBQA datasets based on DBpedia (QALD-9 and LC-QuAD 1.0). Furthermore, our analysis emphasizes that AMR is a powerful tool for KBQA systems.",
        "id":235303644
      }
    ],
    "negative_ctxs":[
      {
        "title":"The SIGMORPHON 2016 Shared Task-Morphological Reinflection",
        "text":"The 2016 SIGMORPHON Shared Task was devoted to the problem of morphological reinflection. It introduced morphological datasets for 10 languages with diverse typological characteristics. The shared task drew submissions from 9 teams representing 11 institutions reflecting a variety of approaches to addressing supervised learning of reinflection. For the simplest task, inflection generation from lemmas, the best system averaged 95.56% exact-match accuracy across all languages, ranging from Maltese (88.99%) to Hungarian (99.30%). With the relatively large training datasets provided, recurrent neural network architectures consistently performed best-in fact, there was a significant margin between neural and non-neural approaches. The best neural approach, averaged over all tasks and languages, outperformed the best nonneural one by 13.76% absolute; on individual tasks and languages the gap in accuracy sometimes exceeded 60%. Overall, the results show a strong state of the art, and serve as encouragement for future shared tasks that explore morphological analysis and generation with varying degrees of supervision.",
        "id":18613906
      },
      {
        "title":"Does a Virtual Talking Face Generate Proper Multimodal Cues to Draw User's Attention to Points of Interest?",
        "text":"We present a series of experiments investigating face-to-face interaction between an Embodied Conversational Agent (ECA) and a human interlocutor. The ECA is embodied by a video realistic talking head with independent head and eye movements. For a beneficial application in face-to-face interaction, the ECA should be able to derive meaning from communicational gestures of a human interlocutor, and likewise to reproduce such gestures. Conveying its capability to interpret human behaviour, the system encourages the interlocutor to show appropriate natural activity. Therefore it is important that the ECA knows how to display what would correspond to mental states in humans. This allows to interpret the machine processes of the system in terms of human expressiveness and to assign them a corresponding meaning. Thus the system may maintain an interaction based on human patterns. During a first experiment we investigated the ability of our talking head to direct user attention with facial deictic cues(Raidt, Bailly et al. 2005). Users interact with the ECA during a simple card game offering different levels of help and guidance through facial deictic cues. We analyzed the users' performance and their perception of the quality of assistance given by the ECA. The experiment showed that users profit from its presence and its facial deictic cues. In the continuative series of experiments presented here, we investigated the effect of an enhancement of the multimodality of the deictic gestures by adding a spoken instruction.",
        "id":6040286
      },
      {
        "title":"Normalized Contrastive Learning for Text-Video Retrieval",
        "text":"Cross-modal contrastive learning has led the recent advances in multimodal retrieval with its simplicity and effectiveness. In this work, however, we reveal that cross-modal contrastive learning suffers from incorrect normalization of the sum retrieval probabilities of each text or video instance. Specifically, we show that many test instances are either overor under-represented during retrieval, significantly hurting the retrieval performance. To address this problem, we propose Normalized Contrastive Learning (NCL) which utilizes the Sinkhorn-Knopp algorithm to compute the instance-wise biases that properly normalize the sum retrieval probabilities of each instance so that every text and video instance is fairly represented during cross-modal retrieval. Empirical study shows that NCL brings consistent and significant gains in text-video retrieval on different model architectures, with new stateof-the-art multimodal retrieval metrics on the ActivityNet, MSVD, and MSR-VTT datasets without any architecture engineering.",
        "id":254974222
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"What methods exist for tailoring news suggestions that consider a user's preferences as well as the current popularity of news stories?",
    "positive_ctxs":[
      {
        "title":"PP-Rec: News Recommendation with Personalized User Interest and Time-aware News Popularity",
        "text":"Personalized news recommendation methods are widely used in online news services. These methods usually recommend news based on the matching between news content and user interest inferred from historical behaviors. However, these methods usually have difficulties in making accurate recommendations to cold-start users, and tend to recommend similar news with those users have read. In general, popular news usually contain important information and can attract users with different interests. Besides, they are usually diverse in content and topic. Thus, in this paper we propose to incorporate news popularity information to alleviate the cold-start and diversity problems for personalized news recommendation. In our method, the ranking score for recommending a candidate news to a target user is the combination of a personalized matching score and a news popularity score. The former is used to capture the personalized user interest in news. The latter is used to measure timeaware popularity of candidate news, which is predicted based on news content, recency, and real-time CTR using a unified framework. Besides, we propose a popularity-aware user encoder to eliminate the popularity bias in user behaviors for accurate interest modeling. Experiments on two real-world datasets show our method can effectively improve the accuracy and diversity for news recommendation.",
        "id":235294032
      }
    ],
    "negative_ctxs":[
      {
        "title":"Efficient incremental beam-search parsing with generative and discriminative models",
        "text":"Extended Abstract:This talk will present several issues related to incremental (left-to-right) beam-search parsing of natural language using generative or discriminative models, either individually or in combination. The first part of the talk will provide background in incremental top-down and (selective) left-corner beamsearch parsing algorithms, and in stochastic models for such derivation strategies. Next, the relative benefits and drawbacks of generative and discriminative models with respect to heuristic pruning and search will be discussed. A range of methods for using multiple models during incremental parsing will be detailed. Finally, we will discuss the potential for effective use of fast, finite-state processing, e.g. partof-speech tagging, to reduce the parsing search space without accuracy loss. POS-tagging is shown to improve efficiency by as much as 20-25 percent with the same accuracy, largely due to the treatment of unknown words. In contrast, an 'islands-of-certainty' approach, which quickly annotates labeled bracketing over low-ambiguity word sequences, is shown to provide little or no efficiency gain over the existing beam-search.The basic parsing approach that will be described in this talk is stochastic incremental top-down parsing, using a beam-search to prune the search space. Grammar induction occurs from an annotated treebank, and non-local features are extracted from each derivation to enrich the stochastic model. Left-corner grammar and tree transforms can be applied to the treebank or the induced grammar, either fully or selectively, to change the derivation order while retaining the same underlying parsing algorithm. This approach has been shown to be accurate, relatively efficient, and robust using both generative and discriminative models(Roark, 2001;Roark, 2004;Collins and Roark, 2004).",
        "id":18917446
      },
      {
        "title":"A convex relaxation for weakly supervised relation extractioń",
        "text":"A promising approach to relation extraction, called weak or distant supervision, exploits an existing database of facts as training data, by aligning it to an unlabeled collection of text documents. Using this approach, the task of relation extraction can easily be scaled to hundreds of different relationships. However, distant supervision leads to a challenging multiple instance, multiple label learning problem. Most of the proposed solutions to this problem are based on non-convex formulations, and are thus prone to local minima. In this article, we propose a new approach to the problem of weakly supervised relation extraction, based on discriminative clustering and leading to a convex formulation. We demonstrate that our approach outperforms state-of-the-art methods on the challenging dataset introduced byRiedel et al. (2010).",
        "id":1849410
      },
      {
        "title":"Published as a conference paper at ICLR 2021 ROBUST REINFORCEMENT LEARNING ON STATE OB- SERVATIONS WITH LEARNED OPTIMAL ADVERSARY",
        "text":"We study the robustness of reinforcement learning (RL) with adversarially perturbed state observations, which aligns with the setting of many adversarial attacks to deep reinforcement learning (DRL) and is also important for rolling out real-world RL agent under unpredictable sensing noise. With a fixed agent policy, we demonstrate that an optimal adversary to perturb state observations can be found, which is guaranteed to obtain the worst case agent reward. For DRL settings, this leads to a novel empirical adversarial attack to RL agents via a learned adversary that is much stronger than previous ones. To enhance the robustness of an agent, we propose a framework of alternating training with learned adversaries (ATLA), which trains an adversary online together with the agent using policy gradient following the optimal adversarial attack framework. Additionally, inspired by the analysis of state-adversarial Markov decision process (SA-MDP), we show that past states and actions (history) can be useful for learning a robust agent, and we empirically find a LSTM based policy can be more robust under adversaries. Empirical evaluations on a few continuous control environments show that ATLA achieves state-of-the-art performance under strong adversaries. Our code is available at https:\/\/github.com\/huanzhang12\/ATLA_robust_RL.arXiv:2101.08452v1 [cs.LG] 21 Jan 2021Published as a conference paper at ICLR 2021 (a) Path in unperturbed environment (found by policy iteration). Agent's reward = +1. Black arrows and numbers show actions and value function of the agent.(b) Path under the optimal adversary. Agent's reward = −∞. Red arrows and numbers show actions and value function of the optimal adversary (Section 3.1).(c) A robust POMDP policy solved by SARSOP(Kurniawati et al., 2008)under the same adversary. This policy is history dependent (Section 3.2).",
        "id":231662383
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"What papers discuss the effect of false negatives among hard negatives in dense retriever training?",
    "positive_ctxs":[
      {
        "title":"Debiased Contrastive Learning of Unsupervised Sentence Representations",
        "text":"Recently, contrastive learning has been shown to be effective in improving pre-trained language models (PLM) to derive high-quality sentence representations. It aims to pull close positive examples to enhance the alignment while push apart irrelevant negatives for the uniformity of the whole representation space. However, previous works mostly adopt in-batch negatives or sample from training data at random. Such a way may cause the sampling bias that improper negatives (e.g., false negatives and anisotropy representations) are used to learn sentence representations, which will hurt the uniformity of the representation space. To address it, we present a new framework DCLR (Debiased Contrastive Learning of unsupervised sentence Representations) to alleviate the influence of these improper negatives. In DCLR, we design an instance weighting method to punish false negatives and generate noise-based negatives to guarantee the uniformity of the representation space. Experiments on seven semantic textual similarity tasks show that our approach is more effective than competitive baselines. Our code and data are publicly available at the link: https: \/\/github.com\/RUCAIBox\/DCLR.",
        "id":248496439
      }
    ],
    "negative_ctxs":[
      {
        "title":"Friendly Topic Assistant for Transformer Based Abstractive Summarization",
        "text":"Abstractive document summarization is a comprehensive task including document understanding and summary generation, in which area Transformer-based models have achieved the state-of-the-art performance. Compared with Transformers, topic models are better at learning explicit document semantics, and hence could be integrated into Transformers to further boost their performance. To this end, we rearrange and explore the semantics learned by a topic model, and then propose a topic assistant (TA) including three modules. TA is compatible with various Transformerbased models and user-friendly since i) TA is a plug-and-play model that does not break any structure of the original Transformer network, making users easily fine-tune Transformer+TA based on a well pre-trained model; ii) TA only introduces a small number of extra parameters. Experimental results on three datasets demonstrate that TA is able to improve the performance of several Transformer-based models.",
        "id":226262313
      },
      {
        "title":"",
        "text":"",
        "id":164936286
      },
      {
        "title":"A Translation Model For Languages of Acceding Countries",
        "text":"The paper proposes a model for translation between syntactically similar languages of acceding countries. This model is based on the presupposition that the translation of related languages should exploit the relatedness by using as simple methods and tools as possible. In the first part the paper discusses the properties of some \"new\" languages, the second part describes a simple translation model which has already been tested on several pairs of syntactically similar languages..",
        "id":16165854
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"What papers explore replacing schema linking with human annotations to study the maximum potential benefit of schema linking for text-to-SQL tasks?",
    "positive_ctxs":[
      {
        "title":"Awakening Latent Grounding from Pretrained Language Models for Semantic Parsing",
        "text":"Recent years pretrained language models (PLMs) hit a success on several downstream tasks, showing their power on modeling language. To better understand and leverage what PLMs have learned, several techniques have emerged to explore syntactic structures entailed by PLMs. However, few efforts have been made to explore grounding capabilities of PLMs, which are also essential. In this paper, we highlight the ability of PLMs to discover which token should be grounded to which concept, if combined with our proposed erasingthen-awakening approach. Empirical studies on four datasets demonstrate that our approach can awaken latent grounding which is understandable to human experts, even if it is not exposed to such labels during training. More importantly, our approach shows great potential to benefit downstream semantic parsing models. Taking text-to-SQL as a case study, we successfully couple our approach with two off-the-shelf parsers, obtaining an absolute improvement of up to 9.8%. * Work done during an internship at Microsoft Research. The first three authors contributed equally.",
        "id":236478283
      }
    ],
    "negative_ctxs":[
      {
        "title":"Enhancing The RATP-DECODA Corpus With Linguistic Annotations For Performing A Large Range Of NLP Tasks",
        "text":"In this article, we present the RATP-DECODA Corpus which is composed by a set of 67 hours of speech from telephone conversations of a Customer Care Service (CCS). This corpus is already available on line at http:\/\/sldr.org\/sldr000847\/fr in its first version. However, many enhancements have been made in order to allow the development of automatic techniques to transcript conversations and to capture their meaning. These enhancements fall into two categories: firstly, we have increased the size of the corpus with manual transcriptions from a new operational day; secondly we have added new linguistic annotations to the whole corpus (either manually or through an automatic processing) in order to perform various linguistic tasks from syntactic and semantic parsing to dialog act tagging and dialog summarization.",
        "id":13300447
      },
      {
        "title":"Implicit Premise Generation with Discourse-aware Commonsense Knowledge Models",
        "text":"Enthymemes are defined as arguments where a premise or conclusion is left implicit. We tackle the task of generating the implicit premise in an enthymeme, which requires not only an understanding of the stated conclusion and premise, but also additional inferences that could depend on commonsense knowledge. The largest available dataset for enthymemes(Habernal et al., 2018)consists of 1.7k samples, which is not large enough to train a neural text generation model. To address this issue, we take advantage of a similar task and dataset: Abductive reasoning in narrative text(Bhagavatula et al., 2020). However, we show that simply using a state-of-the-art seq2seq model fine-tuned on this data might not generate meaningful implicit premises associated with the given enthymemes. We demonstrate that encoding discourse-aware commonsense during fine-tuning improves the quality of the generated implicit premises and outperforms all other baselines both in automatic and human evaluations on three different datasets.",
        "id":237491918
      },
      {
        "title":"LTH: Semantic Structure Extraction using Nonprojective Dependency Trees",
        "text":"We describe our contribution to the SemEval task on Frame-Semantic Structure Extraction. Unlike most previous systems described in literature, ours is based on dependency syntax. We also describe a fully automatic method to add words to the FrameNet lexical database, which gives an improvement in the recall of frame detection.",
        "id":7991041
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"What recent developments in transformer architecture aim to improve the multi-head self-attention mechanism for better transmission of unprocessed attention scores and more stable training?",
    "positive_ctxs":[
      {
        "title":"RealFormer: Transformer Likes Residual Attention",
        "text":"Transformer is the backbone of modern NLP models. In this paper, we propose Real-Former, a simple and generic technique to create Residual Attention Layer Transformer networks that significantly outperform the canonical Transformer and its variants (BERT, ETC, etc.) on a wide spectrum of tasks including Masked Language Modeling, GLUE, SQuAD, Neural Machine Translation, WikiHop, HotpotQA, Natural Questions, and OpenKP. We also observe empirically that RealFormer stabilizes training and leads to models with sparser attention. Source code and pre-trained checkpoints for RealFormer can be found at https",
        "id":229376913
      }
    ],
    "negative_ctxs":[
      {
        "title":"Published as a conference paper at ICLR 2016 UNSUPERVISED AND SEMI-SUPERVISED LEARNING WITH CATEGORICAL GENERATIVE ADVERSARIAL NETWORKS",
        "text":"In this paper we present a method for learning a discriminative classifier from unlabeled or partially labeled data. Our approach is based on an objective function that trades-off mutual information between observed examples and their predicted categorical class distribution, against robustness of the classifier to an adversarial generative model. The resulting algorithm can either be interpreted as a natural generalization of the generative adversarial networks (GAN) framework or as an extension of the regularized information maximization (RIM) framework to robust classification against an optimal adversary. We empirically evaluate our method -which we dub categorical generative adversarial networks (or CatGAN) -on synthetic data as well as on challenging image classification tasks, demonstrating the robustness of the learned classifiers. We further qualitatively assess the fidelity of samples generated by the adversarial generator that is learned alongside the discriminative classifier, and identify links between the CatGAN objective and discriminative clustering algorithms (such as RIM).",
        "id":6230637
      },
      {
        "title":"How Much Do Modifications to Transformer Language Models Affect Their Ability to Learn Linguistic Knowledge?",
        "text":"Recent progress in large pretrained language models (LMs) has led to a growth of analyses examining what kinds of linguistic knowledge are encoded by these models. Due to computational constraints, existing analyses are mostly conducted on publicly-released LM checkpoints, which makes it difficult to study how various factors during training affect the models' acquisition of linguistic knowledge. In this paper, we train a suite of small-scale Transformer LMs that differ from each other with respect to architectural decisions (e.g., self-attention configuration) or training objectives (e.g., multi-tasking, focal loss). We evaluate these LMs on BLiMP, a targeted evaluation benchmark of multiple English linguistic phenomena. Our experiments show that while none of these modifications yields significant improvements on aggregate, changes to the loss function result in promising improvements on several subcategories (e.g., detecting adjunct islands, correctly scoping negative polarity items). We hope our work offers useful insights for future research into designing Transformer LMs that more effectively learn linguistic knowledge.",
        "id":248368718
      },
      {
        "title":"Statistical syntactic parsing for Latvian",
        "text":"Syntactic parsing is an important technique in the natural language processing, yet Latvian is still lacking an efficient general coverage syntax parser. This paper reports on the first experiments on statistical syntactic parsing for Latvian -a highly inflective Indo-European language with a relatively free word order. We have induced a statistical parser from a small, non-balanced Latvian Treebank using the MaltParser toolkit and measured the unlabeled attachment score (UAS). As MaltParser is based on the dependency grammar approach, we have also developed a convertor from the hybrid dependency-based annotation model used in the Latvian Treebank to the pure dependency annotation model. We have obtained a promising 74.63% UAS in 10-fold cross-validation using only ~2500 sentences. The results revealed that best results can be achieved using non-projective stack parsing algorithm with lazy arc adding strategy, but comparably good results can be achieved using projective parsing algorithms combined with appropriate projectiviziation preprocessing.",
        "id":1076208
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"What recent research has been conducted on improving few-shot learning in pre-trained language models through the use of prompt-based fine tuning techniques?",
    "positive_ctxs":[
      {
        "title":"Making Pre-trained Language Models Better Few-shot Learners",
        "text":"The recent GPT-3 model(Brown et al., 2020)achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF-better few-shot fine-tuning of language models 1 -a suite of simple and complementary techniques for finetuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning. 2 * The first two authors contributed equally. 1 Alternatively, language models' best friends forever. 2 Our implementation is publicly available at https:\/\/ github.com\/princeton-nlp\/LM-BFF. Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Association for Computational Linguistics (ACL). Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Association for Computational Linguistics (ACL).",
        "id":229923710
      }
    ],
    "negative_ctxs":[
      {
        "title":"Using Aspect Extraction Approaches to Generate Review Summaries and User Profiles",
        "text":"Reviews of products or services on Internet marketplace websites contain a rich amount of information. Users often wish to survey reviews or review snippets from the perspective of a certain aspect, which has resulted in a large body of work on aspect identification and extraction from such corpora. In this work, we evaluate a newly-proposed neural model for aspect extraction on two practical tasks. The first is to extract canonical sentences of various aspects from reviews, and is judged by human evaluators against alternatives. A kmeans baseline does remarkably well in this setting. The second experiment focuses on the suitability of the recovered aspect distributions to represent users by the reviews they have written. Through a set of review reranking experiments, we find that aspect-based profiles can largely capture notions of user preferences, by showing that divergent users generate markedly different review rankings. 1 words that co-occur with each other get mapped to points close to each other in the embedding space(Harris, 1968;Schütze, 1998).2 www.airbnb.com",
        "id":13746285
      },
      {
        "title":"Faking it: Synthetic text-to-speech synthesis for under-resourced languages - Experimental design",
        "text":"Speech synthesis or text-to-speech (TTS) systems are currently available for a number of the world's major languages, but for thousands of the world's 'minor' languages no such technology is available. While awaiting the development of such technology, we would like to try the stop-gap solution of using an existing TTS system for a major language (the base language) to 'fake' TTS for a minor language (the target language). This paper describes the design for an experiment which involves finding a suitable base language for the Australian Aboriginal language Pitjantjajara as a target language, and evaluating its usability in the real-life situation of providing language technology support for speakers of the target language whose understanding of the local majority language is limited, for example in the scenario of going to the doctor.",
        "id":15668195
      },
      {
        "title":"Knowledge Mining and Discovery for Searching in Literary Texts",
        "text":"The article describes a query system on texts and literary material with advanced information retrieval tools suitable to retrieve the content of a text, either as material specifically organized with respect to linguistic, stylistic and rethoric features, and in its historical, social and cultural context. As a test bed we chose the Dante's characters of al di là. This method of investigation should help a scholar of a literary text to realize part of his interpretative intentions. For this purpose, we will adopt advanced methodologies in knowledge management and knowledge discovery to be applied to a rich representation of the tagged content of a text.",
        "id":9819513
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"What research articles should I consult to understand a method for quantitatively assessing how successful neuron interventions are at altering a model's predictions?",
    "positive_ctxs":[
      {
        "title":"Sparse Interventions in Language Models with Differentiable Masking",
        "text":"There has been a lot of interest in understanding what information is captured by hidden representations of language models (LMs). Typically, interpretation methods i) do not guarantee that the model actually uses the encoded information, and ii) do not discover small subsets of neurons responsible for a considered phenomenon. Inspired by causal mediation analysis, we propose a method that discovers within a neural LM a small subset of neurons responsible for a particular linguistic phenomenon, i.e., subsets causing a change in the corresponding token emission probabilities. We use a differentiable relaxation to approximately search through the combinatorial space. An L 0 regularization term ensures that the search converges to discrete and sparse solutions. We apply our method to analyze subject-verb number agreement and gender bias detection in LSTMs. We observe that it is fast and finds better solutions than the alternative (REINFORCE). Our experiments confirm that each of these phenomenons is mediated through a small subset of neurons that do not play any other discernible role.",
        "id":245123965
      }
    ],
    "negative_ctxs":[
      {
        "title":"COMPUTATIONAL ANALYSIS OF PREDICATIONAL STRUCTURES IN ENGLISH",
        "text":"The results of a computational analysis of all predications, finite and non-finite, in a one-million-word corpus of present-day American English (the \"Brown Corpus\") are presented. The analysis shows the nature of the syntactic differences among the various genres of writing represented in the data base, especially between informative prose and imaginative prose.The results also demonstrate that syntactic complexity, if defined as the number of predications per sentence, is not directly predictable from sentence length.The purpose of this paper is to present an outline of the procedures and the summary of the results of a computational analysis of the structure of predications in a large and representative sample of English texts.",
        "id":27120
      },
      {
        "title":"Large-scale Semantic Networks: Annotation and Evaluation",
        "text":"We introduce a large-scale semantic-network annotation effort based on the MutliNet formalism. Annotation is achieved via a process which incorporates several independent tools including a MultiNet graph editing tool, a semantic concept lexicon, a user-editable knowledge-base for semantic concepts, and a MultiNet parser. We present an evaluation metric for these semantic networks, allowing us to determine the quality of annotations in terms of inter-annotator agreement. We use this metric to report the agreement rates for a pilot annotation effort involving three annotators.",
        "id":9840979
      },
      {
        "title":"Acquiring Hyponymy Relations from Web Documents",
        "text":"This paper describes an automatic method for acquiring hyponymy relations from HTML documents on the WWW. Hyponymy relations can play a crucial role in various natural language processing systems. Most existing acquisition methods for hyponymy relations rely on particular linguistic patterns, such as \"NP such as NP\". Our method, however, does not use such linguistic patterns, and we expect that our procedure can be applied to a wide range of expressions for which existing methods cannot be used. Our acquisition algorithm uses clues such as itemization or listing in HTML documents and statistical measures such as document frequencies and verb-noun co-occurrences.",
        "id":5510520
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"What research exists comparing adapter-based tuning and full fine-tuning efficacy in limited data contexts?",
    "positive_ctxs":[
      {
        "title":"UNIPELT: A Unified Framework for Parameter-Efficient Language Model Tuning",
        "text":"Recent parameter-efficient language model tuning (PELT) methods manage to match the performance of fine-tuning with much fewer trainable parameters and perform especially well when training data is limited. However, different PELT methods may perform rather differently on the same task, making it nontrivial to select the most appropriate method for a specific task, especially considering the fast-growing number of new PELT methods and tasks. In light of model diversity and the difficulty of model selection, we propose a unified framework, UNIPELT, which incorporates different PELT methods as submodules and learns to activate the ones that best suit the current data or task setup via gating mechanism. On the GLUE benchmark, UNIPELT consistently achieves 1~4% gains compared to the best individual PELT method that it incorporates and outperforms fine-tuning under different setups. Moreover, UNIPELT generally surpasses the upper bound that takes the best performance of all its submodules used individually on each task, indicating that a mixture of multiple PELT methods may be inherently more effective than single methods.",
        "id":238857301
      },
      {
        "title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation",
        "text":"Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were \"virtual tokens\". We apply prefix-tuning to GPT-2 for table-totext generation and to BART for summarization. We show that by modifying only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. 2020. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. Amodei. 2020. Language models are few-shot learners.",
        "id":230433941
      }
    ],
    "negative_ctxs":[
      {
        "title":"American Journal of Computational LinguisticsMuIIilin~.ua1 Dictionary",
        "text":"This documerrt is a brief introdilct ion to Carnegie-Mellon University's il~teractive con~p~terized m~rltilitrg~ral dictionary. It describes the use of Rhis cfictionary both by translatots in the course of their vjork and by the i r r~r i n o l o g c s~s responsible for updating and maintaining it. This discussion i s placed in the context of the overall effort (known as the target Project)   to provide attls to translators. A final seclion presents the solution to the problem of representation bf term equivalence adopted in Target. F e r e~g n R r o a d c~s t Information S e r v l c~~ eemlner on Alda 10 T~R T I B~R~O~B , Wsihingfon, DC, Mayq 1970 The rfinf~furalron in dally uee by the Tranelnl~orr C~tlter r i CRrnea~s-Mellan Unrvere~ly involves r Lerr Siogler ADM-3 t~r m i n a l connected by a 300'baud d~al up !me \\ a e POP-10 run under the TOPS-I0 operaling aystsm by the Computer Sclencfi Department ~n d shared e~multaneouely by user8 worklnt on many d~ffstent ptojectr",
        "id":219301746
      },
      {
        "title":"Direct Embedding of Temporal Network Edges via Time-Decayed Line Graphs",
        "text":"Temporal networks model a variety of important phenomena involving timed interactions between entities. Existing methods for machine learning on temporal networks generally exhibit at least one of two limitations. First, time is assumed to be discretized, so if the time data is continuous, the user must determine the discretization and discard precise time information. Second, edge representations can only be calculated indirectly from the nodes, which may be suboptimal for tasks like edge classification. We present a simple method that avoids both shortcomings: construct the line graph of the network, which includes a node for each interaction, and weigh the edges of this graph based on the difference in time between interactions. From this derived graph, edge representations for the original network can be computed with efficient classical methods. The simplicity of this approach facilitates explicit theoretical analysis: we can constructively show the effectiveness of our method's representations for a natural synthetic model of temporal networks. Empirical results on real-world networks demonstrate our method's efficacy and efficiency on both edge classification and temporal link prediction.",
        "id":252682995
      },
      {
        "title":"",
        "text":"",
        "id":3035164
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"What research exists on employing generative models with latent variable to capture semantic dependencies in conversational systems?",
    "positive_ctxs":[
      {
        "title":"GTM: A Generative Triple-Wise Model for Conversational Question Generation",
        "text":"Generating some appealing questions in opendomain conversations is an effective way to improve human-machine interactions and lead the topic to a broader or deeper direction. To avoid dull or deviated questions, some researchers tried to utilize answer, the \"future\" information, to guide question generation. However, they separate a post-questionanswer (PQA) triple into two parts: postquestion (PQ) and question-answer (QA) pairs, which may hurt the overall coherence. Besides, the QA relationship is modeled as a one-to-one mapping that is not reasonable in open-domain conversations. To tackle these problems, we propose a generative triple-wise model with hierarchical variations for open-domain conversational question generation (CQG). Latent variables in three hierarchies are used to represent the shared background of a triple and one-to-many semantic mappings in both PQ and QA pairs. Experimental results on a largescale CQG dataset show that our method significantly improves the quality of questions in terms of fluency, coherence and diversity over competitive baselines.",
        "id":235358586
      }
    ],
    "negative_ctxs":[
      {
        "title":"Negative Concord and Restructuring in Palestinian Arabic: A Comparison of TAG and CCG Analyses",
        "text":"This paper discusses interactions between negative concord and restructuring\/clause union in Palestinian Arabic. Analyses formulated in Tree Adjoining Grammar and Combinatorial Categorial Grammar are compared, with the conclusion that a perspicuous analysis of the the intricacies of the data requires aspects of both formalisms; in particular, the TAG notion of the extended domain of locality and the CCG notion of flexible constituency.",
        "id":11275509
      },
      {
        "title":"WarwickDCS: From Phrase-Based to Target-Specific Sentiment Recognition",
        "text":"We present and evaluate several hybrid systems for sentiment identification for Twitter, both at the phrase and document (tweet) level. Our approach has been to use a novel combination of lexica, traditional NLP and deep learning features. We also analyse techniques based on syntactic parsing and tokenbased association to handle topic specific sentiment in subtask C. Our strategy has been to identify subphrases relevant to the designated topic\/target and assign sentiment according to our subtask A classifier. Our submitted subtask A classifier ranked fourth in the Se-mEval official results while our BASELINE and µPARSE classifiers for subtask C would have ranked second.",
        "id":4159041
      },
      {
        "title":"CMUQ@QALB-2014: An SMT-based System for Automatic Arabic Error Correction",
        "text":"In this paper, we describe the CMUQ system we submitted to The ANLP-QALB 2014 Shared Task on Automatic Text Correction for Arabic. Our system combines rule-based linguistic techniques with statistical language modeling techniques and machine translationbased methods. Our system outperforms the baseline and reaches an F-score of 65.42% on the test set of QALB corpus. This ranks us 3rd in the competition.",
        "id":17910054
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"What research exists on incorporating knowledge graphs into language models to improve their complex question-answering capabilities?",
    "positive_ctxs":[
      {
        "title":"Knowledge Graph-augmented Language Models for Complex Question Answering",
        "text":"Large language models have shown impressive abilities to reason over input text, however, they are prone to hallucinations. On the other hand, end-to-end knowledge graph question answering (KGQA) models output responses grounded in facts, but they still struggle with complex reasoning, such as comparison or ordinal questions. In this paper, we propose a new method for complex question answering where we combine a knowledge graph retriever based on an end-to-end KGQA model with a language model that reasons over the retrieved facts to return an answer. We observe that augmenting language model prompts with retrieved KG facts improves performance over using a language model alone by an average of 83%. In particular, we see improvements on complex questions requiring count, intersection, or multi-hop reasoning operations.",
        "id":259833781
      }
    ],
    "negative_ctxs":[
      {
        "title":"Bloom Library: Multimodal Datasets in 300+ Languages for a Variety of Downstream Tasks",
        "text":"We present Bloom Library, a linguistically diverse set of multimodal and multilingual datasets for language modeling, image captioning, visual storytelling, and speech synthesis\/recognition. These datasets represent either the most, or among the most, multilingual datasets for each of the included downstream tasks. In total, the initial release of the Bloom Library datasets covers 363 languages across 32 language families. We train downstream task models for various languages represented in the data, showing the viability of the data for future work in low-resource, multimodal NLP and establishing the first known baselines for these downstream tasks in certain languages (e.g., Bisu [bzi], with an estimated population of 700 users). Some of these first-of-their-kind baselines are comparable to state-of-the-art performance for higher-resourced languages. The Bloom Library datasets are released under Creative Commons licenses on the Hugging Face datasets hub to catalyze more linguistically diverse research in the included downstream tasks.",
        "id":253116724
      },
      {
        "title":"A New Version of the Składnica Treebank of Polish Harmonised with the Walenty Valency Dictionary",
        "text":"This paper reports on developments in the Składnica treebank of Polish which were possible due to the switch to the Walenty valency dictionary. The change required several modifications in the Świgra parser, such as implementing unlike coordination, semantically motivated phrases, and non-standard case values. A procedure to upgrade manually disambiguated trees of Składnica was required as well. Modifications introduced in the treebank included systematic changes of notation and resolving ambiguity between semantically motivated phrases. The procedure of confronting Składnica treebank with the trees generated with the new version of the Świgra parser using Walenty dictionary allowed us to check the consistency of all the resources. This resulted in several corrections introduced in both the treebank and the valence dictionary.",
        "id":21715487
      },
      {
        "title":"SIMPLE EMERGENT ACTION REPRESENTATIONS FROM MULTI-TASK POLICY TRAINING",
        "text":"The low-level sensory and motor signals in deep reinforcement learning, which exist in high-dimensional spaces such as image observations or motor torques, are inherently challenging to understand or utilize directly for downstream tasks. While sensory representations have been extensively studied, the representations of motor actions are still an area of active exploration. Our work reveals that a space containing meaningful action representations emerges when a multi-task policy network takes as inputs both states and task embeddings. Moderate constraints are added to improve its representation ability. Therefore, interpolated or composed embeddings can function as a high-level interface within this space, providing instructions to the agent for executing meaningful action sequences. Empirical results demonstrate that the proposed action representations are effective for intra-action interpolation and inter-action composition with limited or no additional learning. Furthermore, our approach exhibits superior task adaptation ability compared to strong baselines in Mujoco locomotion tasks. Our work sheds light on the promising direction of learning action representations for efficient, adaptable, and composable RL, forming the basis of abstract action planning and the understanding of motor signal space. Project page: https:\/\/sites. google.com\/view\/emergent-action-representation\/ * Denotes equal contributions.",
        "id":252967887
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"What research exists on leveraging syntactic roles and semantic interpretations for backdoor attacks on natural language processing systems?",
    "positive_ctxs":[
      {
        "title":"Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger",
        "text":"Backdoor attacks are a kind of insidious security threat against machine learning models. After being injected with a backdoor in training, the victim model will produce adversaryspecified outputs on the inputs embedded with predesigned triggers but behave properly on normal inputs during inference. As a sort of emergent attack, backdoor attacks in natural language processing (NLP) are investigated insufficiently. As far as we know, almost all existing textual backdoor attack methods insert additional contents into normal samples as triggers, which causes the trigger-embedded samples to be detected and the backdoor attacks to be blocked without much effort. In this paper, we propose to use the syntactic structure as the trigger in textual backdoor attacks. We conduct extensive experiments to demonstrate that the syntactic trigger-based attack method can achieve comparable attack performance (almost 100% success rate) to the insertionbased methods but possesses much higher invisibility and stronger resistance to defenses. These results also reveal the significant insidiousness and harmfulness of textual backdoor attacks. All the code and data of this paper can be obtained at https:\/\/github.com\/ thunlp\/HiddenKiller.",
        "id":235196099
      },
      {
        "title":"Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution",
        "text":"Recent studies show that neural natural language processing (NLP) models are vulnerable to backdoor attacks. Injected with backdoors, models perform normally on benign examples but produce attacker-specified predictions when the backdoor is activated, presenting serious security threats to real-world applications. Since existing textual backdoor attacks pay little attention to the invisibility of backdoors, they can be easily detected and blocked. In this work, we present invisible backdoors that are activated by a learnable combination of word substitution. We show that NLP models can be injected with backdoors that lead to a nearly 100% attack success rate, whereas being highly invisible to existing defense strategies and even human inspections. The results raise a serious alarm to the security of NLP models, which requires further research to be resolved. All the data and code of this paper are released at https: \/\/github.com\/thunlp\/BkdAtk-LWS.",
        "id":235417102
      }
    ],
    "negative_ctxs":[
      {
        "title":"Lifetime Achievement Award Natural Language Processing and Computational Linguistics",
        "text":"",
        "id":239212345
      },
      {
        "title":"Gender Representation in Open Source Speech Resources",
        "text":"With the rise of artificial intelligence (AI) and the growing use of deep-learning architectures, the question of ethics, transparency and fairness of AI systems has become a central concern within the research community. We address transparency and fairness in spoken language systems by proposing a study about gender representation in speech resources available through the Open Speech and Language Resource platform. We show that finding gender information in open source corpora is not straightforward and that gender balance depends on other corpus characteristics (elicited\/non elicited speech, low\/high resource language, speech task targeted). The paper ends with recommendations about metadata and gender information for researchers in order to assure better transparency of the speech systems built using such corpora.",
        "id":212747709
      },
      {
        "title":"",
        "text":"",
        "id":220445929
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"What research exists on the impact of scaling on prompt tuning efficiency in pre-trained language models?",
    "positive_ctxs":[
      {
        "title":"The Power of Scale for Parameter-Efficient Prompt Tuning",
        "text":"In this work, we explore \"prompt tuning,\" a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient \"prompt ensembling.\" We release code and model checkpoints to reproduce our experiments. 1ReferencesRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,  Danilo Giampiccolo, Bernardo Magnini, and Idan  Szpektor. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognising textual entailment, volume 6, pages 6-4. Venice. . 2019a. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
        "id":233296808
      }
    ],
    "negative_ctxs":[
      {
        "title":"Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers",
        "text":"ChatGPT is a large language model developed by OpenAI 1 . Despite its impressive performance across various tasks, no prior work has investigated its capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of ChatGPT on various benchmark biomedical tasks, such as relation extraction, document classification, question answering, and summarization. To the best of our knowledge, this is the first work that conducts an extensive evaluation of ChatGPT in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zeroshot ChatGPT even outperforms the state-ofthe-art fine-tuned generative transformer models, such as BioGPT and BioBART. This suggests that ChatGPT's pre-training on large text corpora makes it quite specialized even in the biomedical domain. Our findings demonstrate that ChatGPT has the potential to be a valuable tool for various tasks in the biomedical domain that lack large annotated data. . 2019. BART:Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461.",
        "id":259096053
      },
      {
        "title":"Vowel sequences in Old Japanese: from a corpus-based approach",
        "text":"This paper investigates vowel sequences in OldJapanese from a corpus-based approach by using data from the Oxford-NINJAL Corpus of Old Japanese (ONCOJ). Three conditions are taken into consideration to analyze the corpus data, and they are (a) within a phonological word, (b) through morphological process, and (c) by grammatical element. The results have shown that vowel sequences in Old Japanese are not random combinations of vowels, but they are phonologically constrained. In the first and second conditions, the vowel sequences tend to show vowel[-front] + vowel [-low]. In the third condition, the first vowel of the vowel sequence is -i-. In addition to frequency, this paper also discusses the types of words and compares types with frequencies.",
        "id":258463943
      },
      {
        "title":"Capturing Pragmatic Knowledge in Article Usage Prediction using LSTMs",
        "text":"We examine the potential of recurrent neural networks for handling pragmatic inferences involving complex contextual cues for the task of article usage prediction. We train and compare several variants of Long Short-Term Memory (LSTM) networks with an attention mechanism. Our model outperforms a previous state-of-the-art system, achieving up to 96.63% accuracy on the WSJ\/PTB corpus. In addition, we perform a series of analyses to understand the impact of various model choices. We find that the gain in performance can be attributed to the ability of LSTMs to pick up on contextual cues, both local and further away in distance, and that the model is able to solve cases involving reasoning about coreference and synonymy. We also show how the attention mechanism contributes to the interpretability of the model's effectiveness.",
        "id":8341728
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"What research exists on using reinforcement learning methods for event prediction in temporal knowledge graphs?",
    "positive_ctxs":[
      {
        "title":"TimeTraveler: Reinforcement Learning for Temporal Knowledge Graph Forecasting",
        "text":"Temporal knowledge graph (TKG) reasoning is a crucial task that has gained increasing research interest in recent years. Most existing methods focus on reasoning at past timestamps to complete the missing facts, and there are only a few works of reasoning on known TKGs to forecast future facts. Compared with the completion task, the forecasting task is more difficult and faces two main challenges:(1) how to effectively model the time information to handle future timestamps? (2) how to make inductive inference to handle previously unseen entities that emerge over time? To address these challenges, we propose the first reinforcement learning method for forecasting. Specifically, the agent travels on historical knowledge graph snapshots to search for the answer. Our method defines a relative time encoding function to capture the timespan information, and we design a novel time-shaped reward based on Dirichlet distribution to guide the model learning. Furthermore, we propose a novel representation method for unseen entities to improve the inductive inference ability of the model. We evaluate our method for this link prediction task at future timestamps. Extensive experiments on four benchmark datasets demonstrate substantial performance improvement meanwhile with higher explainability, less calculation, and fewer parameters when compared with existing stateof-the-art methods.",
        "id":237454564
      }
    ],
    "negative_ctxs":[
      {
        "title":"A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data",
        "text":"Understanding unstructured text is a major goal within natural language processing. Comprehension tests pose questions based on short text passages to evaluate such understanding. In this work, we investigate machine comprehension on the challenging MCTest benchmark. Partly because of its limited size, prior work on MCTest has focused mainly on engineering better features. We tackle the dataset with a neural approach, harnessing simple neural networks arranged in a parallel hierarchy. The parallel hierarchy enables our model to compare the passage, question, and answer from a variety of trainable perspectives, as opposed to using a manually designed, rigid feature set. Perspectives range from the word level to sentence fragments to sequences of sentences; the networks operate only on word-embedding representations of text. When trained with a methodology designed to help cope with limited training data, our Parallel-Hierarchical model sets a new state of the art for MCTest, outperforming previous feature-engineered approaches slightly and previous neural approaches by a significant margin (over 15% absolute).",
        "id":12834729
      },
      {
        "title":"Jigg: A Framework for an Easy Natural Language Processing Pipeline",
        "text":"We present Jigg, a Scala (or JVMbased) NLP annotation pipeline framework, which is easy to use and is extensible. Jigg supports a very simple interface similar to Stanford CoreNLP, the most successful NLP pipeline toolkit, but has more flexibility to adapt to new types of annotation. On this framework, system developers can easily integrate their downstream system into a NLP pipeline from a raw text by just preparing a wrapper of it.",
        "id":14941111
      },
      {
        "title":"Multiple Sclerosis Severity Classification From Clinical Text",
        "text":"Multiple Sclerosis (MS) is a chronic, inflammatory and degenerative neurological disease, which is monitored by a specialist using the Expanded Disability Status Scale (EDSS) and recorded in unstructured text in the form of a neurology consult note. An EDSS measurement contains an overall 'EDSS' score and several functional subscores. Typically, expert knowledge is required to interpret consult notes and generate these scores. Previous approaches used limited context length Word2Vec embeddings and keyword searches to predict scores given a consult note, but often failed when scores were not explicitly stated. In this work, we present MS-BERT, the first publicly available transformer model trained on real clinical data other than MIMIC. Next, we present MSBC, a classifier that applies MS-BERT to generate embeddings and predict EDSS and functional subscores. Lastly, we explore combining MSBC with other models through the use of Snorkel to generate scores for unlabelled consult notes. MSBC achieves state-of-the-art performance on all metrics and prediction tasks and outperforms the models generated from the Snorkel ensemble. We improve Macro-F1 by 0.12 (to 0.88) for predicting EDSS and on average by 0.29 (to 0.63) for predicting functional subscores over previous Word2Vec CNN and rule-based approaches.",
        "id":225103074
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"What research has been conducted on applying contrastive techniques to distinguish normal from abnormal imagery for the creation of radiology reports?",
    "positive_ctxs":[
      {
        "title":"Contrastive Attention for Automatic Chest X-ray Report Generation",
        "text":"Recently, chest X-ray report generation, which aims to automatically generate descriptions of given chest X-ray images, has received growing research interests. The key challenge of chest X-ray report generation is to accurately capture and describe the abnormal regions. In most cases, the normal regions dominate the entire chest X-ray image, and the corresponding descriptions of these normal regions dominate the final report. Due to such data bias, learning-based models may fail to attend to abnormal regions. In this work, to effectively capture and describe abnormal regions, we propose the Contrastive Attention (CA) model. Instead of solely focusing on the current input image, the CA model compares the current input image with normal images to distill the contrastive information. The acquired contrastive information can better represent the visual features of abnormal regions. According to the experiments on the public IU-X-ray and MIMIC-CXR datasets, incorporating our CA into several existing models can boost their performance across most metrics. In addition, according to the analysis, the CA model can help existing models better attend to the abnormal regions and provide more accurate descriptions which are crucial for an interpretable diagnosis. Specifically, we achieve the state-ofthe-art results on the two public datasets.",
        "id":235422047
      }
    ],
    "negative_ctxs":[
      {
        "title":"Personality Profiling of Fictional Characters using Sense-Level Links between Lexical Resources",
        "text":"This study focuses on personality prediction of protagonists in novels based on the Five-Factor Model of personality. We present and publish a novel collaboratively built dataset of fictional character personality and design our task as a text classification problem. We incorporate a range of semantic features, including WordNet and VerbNet sense-level information and word vector representations. We evaluate three machine learning models based on the speech, actions and predicatives of the main characters, and show that especially the lexical-semantic features significantly outperform the baselines. The most predictive features correspond to reported findings in personality psychology.",
        "id":10679333
      },
      {
        "title":"Identifying attack and support argumentative relations using deep learning",
        "text":"We propose a deep learning architecture to capture argumentative relations of attack and support from one piece of text to another, of the kind that naturally occur in a debate. The architecture uses two (unidirectional or bidirectional) Long Short-Term Memory networks and (trained or non-trained) word embeddings, and allows to considerably improve upon existing techniques that use syntactic features and supervised classifiers for the same form of (relation-based) argument mining.",
        "id":28193257
      },
      {
        "title":"Fast(er) Exact Decoding and Global Training for Transition-Based Dependency Parsing via a Minimal Feature Set",
        "text":"We first present a minimal feature set for transition-based dependency parsing, continuing a recent trend started by Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) of using bi-directional LSTM features. We plug our minimal feature set into the dynamic-programming framework of Huang and Sagae(2010)andKuhlmann et al. (2011)to produce the first implementation of worst-case Opn 3 q exact decoders for arc-hybrid and arceager transition systems. With our minimal features, we also present Opn 3 q global training methods. Finally, using ensembles including our new parsers, we achieve the best unlabeled attachment score reported (to our knowledge) on the Chinese Treebank and the \"second-best-in-class\" result on the English Penn Treebank.",
        "id":269533
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"What research has been conducted on creating neural network frameworks for parsing text into SQL?",
    "positive_ctxs":[
      {
        "title":"ShadowGNN: Graph Projection Neural Network for Text-to-SQL Parser",
        "text":"Given a database schema, Text-to-SQL aims to translate a natural language question into the corresponding SQL query. Under the setup of cross-domain, traditional semantic parsing models struggle to adapt to unseen database schemas. To improve the model generalization capability for rare and unseen schemas, we propose a new architecture, ShadowGNN, which processes schemas at abstract and semantic levels. By ignoring names of semantic items in databases, abstract schemas are exploited in a well-designed graph projection neural network to obtain delexicalized representation of question and schema. Based on the domain-independent representations, a relation-aware transformer is utilized to further extract logical linking between question and schema. Finally, a SQL decoder with context-free grammar is applied. On the challenging Text-to-SQL benchmark Spider, empirical results show that ShadowGNN outperforms state-of-the-art models. When the annotated data is extremely limited (only 10% training set), ShadowGNN gets over absolute 5% performance gain, which shows its powerful generalization ability. Our implementation will be open-sourced at https:\/\/github. com\/WowCZ\/shadowgnn.",
        "id":233210172
      }
    ],
    "negative_ctxs":[
      {
        "title":"Proceedings of the 2nd Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal Automated morphological analysis of clinical language samples",
        "text":"Quantitative analysis of clinical language samples is a powerful tool for assessing and screening developmental language impairments, but requires extensive manual transcription, annotation, and calculation, resulting in error-prone results and clinical underutilization. We describe a system that performs automated morphological analysis needed to calculate statistics such as the mean length of utterance in morphemes (MLUM), so that these statistics can be computed directly from orthographic transcripts. Estimates of MLUM computed by this system are closely comparable to those produced by manual annotation. Our system can be used in conjunction with other automated annotation techniques, such as maze detection. This work represents an important first step towards increased automation of language sample analysis, and towards attendant benefits of automation, including clinical greater utilization and reduced variability in care delivery.",
        "id":919698
      },
      {
        "title":"Qualia Structure and the Compositional Interpretation of Compounds",
        "text":"The analysis of nominal compound constructions has proven to be a recalcitrant problem for linguistic semantics and poses serious challenges for natural language processing systems. We argue for a compositional treatment of compound constructions which limits the need for listing of compounds in the lexicon. We argue that the development of a practical model of compound interpretation crucially depends on issues of lexicon design. The Generative Lexicon (Pustejovsky 1995) provides us with a model of the lexicon which couples sufficiently expressive lexical semantic representations with mechanisms which capture the relationship between those representations and their syntactic expression. In our approach, the qualia structures of the nouns in a compound provide relational structure enabling compositional interpretation of the modification of the head noun by the modifying noun. This brings compound interpretation under the same rubric as other forms of composition in natural language, including argument selection, adjectival modification, and type coercion(Pustejovsky (1991(Pustejovsky ( ,1995, Bouillon 1995). We examine data from both English and Italian and develop analyses for both languages which use phrase structure schemata to account for the connections between lexical semantic representation and syntactic expression. In addition to applications in natural language understanding, machine translation, and generation, the model of compound interpretation developed here can be applied to multi-lingual information extraction tasks.",
        "id":5170618
      },
      {
        "title":"Diagnosing meaning errors in short answers to reading comprehension questions",
        "text":"A common focus of systems in Intelligent Computer-Assisted Language Learning (ICALL) is to provide immediate feedback to language learners working on exercises. Most of this research has focused on providing feedback on the form of the learner input. Foreign language practice and second language acquisition research, on the other hand, emphasizes the importance of exercises that require the learner to manipulate meaning.",
        "id":865293
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"What research has been conducted on determining the ideal segment length for unsupervised keyphrase extraction?",
    "positive_ctxs":[
      {
        "title":"Unsupervised Keyphrase Extraction by Jointly Modeling Local and Global Context",
        "text":"Embedding based methods are widely used for unsupervised keyphrase extraction (UKE) tasks. Generally, these methods simply calculate similarities between phrase embeddings and document embedding, which is insufficient to capture different context for a more effective UKE model. In this paper, we propose a novel method for UKE, where local and global contexts are jointly modeled. From a global view, we calculate the similarity between a certain phrase and the whole document in the vector space as transitional embedding based models do. In terms of the local view, we first build a graph structure based on the document where phrases are regarded as vertices and the edges are similarities between vertices. Then, we proposed a new centrality computation method to capture local salient information based on the graph structure. Finally, we further combine the modeling of global and local context for ranking. We evaluate our models on three public benchmarks (Inspec, DUC 2001, SemEval 2010 and compare with existing state-of-the-art models. The results show that our model outperforms most models while generalizing better on input documents with different domains and length. Additional ablation study shows that both the local and global information is crucial for unsupervised keyphrase extraction tasks.",
        "id":237513596
      }
    ],
    "negative_ctxs":[
      {
        "title":"CROSS 2 STRA: Unpaired Cross-lingual Image Captioning with Cross-lingual Cross-modal Structure-pivoted Alignment",
        "text":"Unpaired cross-lingual image captioning has long suffered from irrelevancy and disfluency issues, due to the inconsistencies of the semantic scene and syntax attributes during transfer. In this work, we propose to address the above problems by incorporating the scene graph (SG) structures and the syntactic constituency (SC) trees. Our captioner contains the semantic structure-guided image-to-pivot captioning and the syntactic structure-guided pivot-to-target translation, two of which are joined via pivot language. We then take the SG and SC structures as pivoting, performing cross-modal semantic structure alignment and cross-lingual syntactic structure alignment learning. We further introduce cross-lingual&cross-modal back-translation training to fully align the captioning and translation stages. Experiments on English↔Chinese transfers show that our model shows great superiority in improving captioning relevancy and fluency.",
        "id":258833472
      },
      {
        "title":"The role of memory in superiority violation gradience",
        "text":"This paper examines how grammatical and memory constraints explain gradience in superiority violation acceptability. A computational model encoding both categories of constraints is compared to experimental evidence. By formalizing memory capacity as beam-search in the parser, the model predicts gradience evident in human data. To predict attachment behavior, the parser must be sensitive to the types of nominal intervenors that occur between a wh-filler and its head. The results suggest memory is more informative for modeling violation gradience patterns than grammatical constraints.",
        "id":716928
      },
      {
        "title":"Using the argumentative structure of scientific literature to improve information access",
        "text":"MEDLINE\/PubMed contains structured abstracts that can provide argumentative labels. Selection of abstract sentences based on the argumentative label has shown to improve the performance of information retrieval tasks. These abstracts make up less than one quarter of all the abstracts in MEDLINE\/PubMed, so it is worthwhile to learn how to automatically label the non-structured ones.",
        "id":9047303
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"What research has been conducted on enhancing conversational generation models using knowledge sourced from the internet?",
    "positive_ctxs":[
      {
        "title":"Internet-Augmented Dialogue Generation",
        "text":"The largest store of continually updating knowledge on our planet can be accessed via internet search. In this work we study giving access to this information to conversational agents. Large language models, even though they store an impressive amount of knowledge within their weights, are known to hallucinate facts when generating dialogue (Shuster et al., 2021); moreover, those facts are frozen in time at the point of model training. In contrast, we propose an approach that learns to generate an internet search query based on the context, and then conditions on the search results to finally generate a response, a method that can employ up-to-the-minute relevant information. We train and evaluate such models on a newly collected dataset of human-human conversations whereby one of the speakers is given access to internet search during knowledge-driven discussions in order to ground their responses. We find that search-query based access of the internet in conversation provides superior performance compared to existing approaches that either use no augmentation or FAISS-based retrieval (Lewis et al., 2020b). . 2020. Exploring the limits of transfer learning with a unified text-to-text transformer.",
        "id":236034557
      }
    ],
    "negative_ctxs":[
      {
        "title":"SemEval-2016 Task 7: Determining Sentiment Intensity of English and Arabic Phrases",
        "text":"We present a shared task on automatically determining sentiment intensity of a word or a phrase. The words and phrases are taken from three domains: general English, English Twitter, and Arabic Twitter. The phrases include those composed of negators, modals, and degree adverbs as well as phrases formed by words with opposing polarities. For each of the three domains, we assembled the datasets that include multi-word phrases and their constituent words, both manually annotated for real-valued sentiment intensity scores. The three datasets were presented as the test sets for three separate tasks (each focusing on a specific domain). Five teams submitted nine system outputs for the three tasks. All datasets created for this shared task are freely available to the research community.",
        "id":11994495
      },
      {
        "title":"",
        "text":"",
        "id":236999865
      },
      {
        "title":"Hate-CLIPper: Multimodal Hateful Meme Classification based on Cross-modal Interaction of CLIP Features",
        "text":"Hateful memes are a growing menace on social media. While the image and its corresponding text in a meme are related, they do not necessarily convey the same meaning when viewed individually. Hence, detecting hateful memes requires careful consideration of both visual and textual information. Multimodal pretraining can be beneficial for this task because it effectively captures the relationship between the image and the text by representing them in a similar feature space. Furthermore, it is essential to model the interactions between the image and text features through intermediate fusion. Most existing methods either employ multimodal pre-training or intermediate fusion, but not both. In this work, we propose the Hate-CLIPper architecture, which explicitly models the cross-modal interactions between the image and text representations obtained using Contrastive Language-Image Pre-training (CLIP) encoders via a feature interaction matrix (FIM). A simple classifier based on the FIM representation is able to achieve state-of-the-art performance on the Hateful Memes Challenge (HMC) dataset with an AUROC of 85.8, which even surpasses the human performance of 82.65. Experiments on other meme datasets such as Propaganda Memes and TamilMemes also demonstrate the generalizability of the proposed approach. Finally, we analyze the interpretability of the FIM representation and show that cross-modal interactions can indeed facilitate the learning of meaningful concepts. The code for this work is available at https:\/\/github. com\/gokulkarthik\/hateclipper.",
        "id":252846273
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"What research has been conducted on incorporating visual data into the text summarization process?",
    "positive_ctxs":[
      {
        "title":"Summary-Oriented Vision Modeling for Multimodal Abstractive Summarization",
        "text":"Multimodal abstractive summarization (MAS) aims to produce a concise summary given the multimodal data (text and vision). Existing studies mainly focus on how to effectively use the visual features from the perspective of an article, having achieved impressive success on the high-resource English dataset. However, less attention has been paid to the visual features from the perspective of the summary, which may limit the model performance, especially in the low-and zero-resource scenarios. In this paper, we propose to improve the summary quality through summary-oriented visual features. To this end, we devise two auxiliary tasks including vision to summary task and masked image modeling task. Together with the main summarization task, we optimize the MAS model via the training objectives of all these tasks. By these means, the MAS model can be enhanced by capturing the summaryoriented visual features, thereby yielding more accurate summaries. Experiments on 44 languages, covering mid-high-, low-, and zeroresource scenarios, verify the effectiveness and superiority of the proposed approach, which achieves state-of-the-art performance under all scenarios. Additionally, we will contribute a large-scale multilingual multimodal abstractive summarization (MM-Sum) dataset. 1",
        "id":254685691
      }
    ],
    "negative_ctxs":[
      {
        "title":"An Empirical Study for Generating Zero Pronoun in Korean based on Cost-based Centering Model",
        "text":"In Korean, in order to generate a coherent text, a redundantly prominent noun should be replaced by a non-zero pronoun or zero pronoun. Otherwise, the text becomes unnatural. Specifically, a redundant noun in Korean is frequently omitted while a redundant noun in English is replaced by a pronoun. This paper proposes a generation algorithm of the zero pronoun, using a Cost-based Centering Model which considers the inference cost. For an objective evaluation of our algorithm, we collected 87 texts from three genres, and manually recovered the omitted elements. Using the collected texts, we verify that our algorithm is well defined to explain the phenomenon of the zero pronoun in Korean. We also show that the proposed approach resolves both the overgeneration of the zero pronoun in Continue and its under-generation in other transitions in terms of Centering.",
        "id":13133422
      },
      {
        "title":"The European Thesaurus on International Relations and Area Studies - A Multilingual Resource for Indexing, Retrieval, and Translation",
        "text":"The multilingual European Thesaurus on International Relations and Area Studies (European Thesaurus) is a special subject thesaurus for the field of international affairs. It is intended for use in libraries and documentation centres of academic institutions and international organizations. The European Thesaurus was established in a collaborative project involving a number of leading European research institutes on international politics. It integrates the controlled terminologies of several existing thesauri. The European Thesaurus comprises about 8,200 terms and proper names from the 24 subject areas covered by the thesaurus. Because of its multilinguality, the European Thesaurus can not only be used for indexing, retrieval and terminological reference, but serves also as a translation tool for the languages represented. The establishment of cross-concordances to related thesauri extends the range of application of the European Thesaurus even further. They enable the treatment of semantic heterogeneity within subject gateways. The European Thesaurus is available both in a seven-lingual printversion as well as in an eight-lingual online-version. To reflect the changes in terminology the European Thesaurus is regularly being amended and modified. Further languages are going to be included.",
        "id":23412271
      },
      {
        "title":"Active Image Indexing",
        "text":"Image copy detection and retrieval from large databases leverage two components. First, a neural network maps an image to a vector representation, that is relatively robust to various transformations of the image. Second, an efficient but approximate similarity search algorithm trades scalability (size and speed) against quality of the search, thereby introducing a source of error. This paper improves the robustness of image copy detection with active indexing, that optimizes the interplay of these two components. We reduce the quantization loss of a given image representation by making imperceptible changes to the image before its release. The loss is back-propagated through the deep neural network back to the image, under perceptual constraints. These modifications make the image more retrievable. Our experiments show that the retrieval and copy detection of activated images is significantly improved. For instance, activation improves by +40% the Re-call1@1 on various image transformations, and for several popular indexing structures based on product quantization and locality sensitivity hashing.",
        "id":252993084
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"What research has been conducted on news recommendation engines that consider individual user preferences as well as the time-sensitive popularity of news content?",
    "positive_ctxs":[
      {
        "title":"PP-Rec: News Recommendation with Personalized User Interest and Time-aware News Popularity",
        "text":"Personalized news recommendation methods are widely used in online news services. These methods usually recommend news based on the matching between news content and user interest inferred from historical behaviors. However, these methods usually have difficulties in making accurate recommendations to cold-start users, and tend to recommend similar news with those users have read. In general, popular news usually contain important information and can attract users with different interests. Besides, they are usually diverse in content and topic. Thus, in this paper we propose to incorporate news popularity information to alleviate the cold-start and diversity problems for personalized news recommendation. In our method, the ranking score for recommending a candidate news to a target user is the combination of a personalized matching score and a news popularity score. The former is used to capture the personalized user interest in news. The latter is used to measure timeaware popularity of candidate news, which is predicted based on news content, recency, and real-time CTR using a unified framework. Besides, we propose a popularity-aware user encoder to eliminate the popularity bias in user behaviors for accurate interest modeling. Experiments on two real-world datasets show our method can effectively improve the accuracy and diversity for news recommendation.",
        "id":235294032
      }
    ],
    "negative_ctxs":[
      {
        "title":"Multi-domain Cross-lingual Information Extraction from Clean and Noisy Texts",
        "text":"We have created a human-annotated, multi-event, cross-lingual corpus of equivalent summaries in Spanish and English to investigate cross-lingual information extraction. The corpus contains, in addition to pairs of equivalent non-translated summaries, automatic translations of each summary produced using an available translation tool. We have developed trainable information extraction systems per language and have applied them to both original summaries and their automatic translations obtaining encouraging results.Resumo. Apresentamos um estudo de extração de informações de um corpus bilíngüe paralelo em espanhol e inglês. O corpus está formado por pares de resumos curtos de eventos em três domínios de aplicação. Temos desenvolvido sistemas de extração de informaçoes para as duas línguas estudadas e avaliado o desempenho do sistema em varias experiências tanto monolíngües como translíngües. Apresentamos uma análise dos resultados obtidos.",
        "id":14239468
      },
      {
        "title":"Implicit readability ranking using the latent variable of a Bayesian Probit model",
        "text":"Data driven approaches to readability analysis for languages other than English has been plagued by a scarcity of suitable corpora. Often, relevant corpora consist only of easy-to-read texts with no rank information or empirical readability scores, making only binary approaches, such as classification, applicable. We propose a Bayesian, latent variable, approach to get the most out of these kinds of corpora. In this paper we present results on using such a model for readability ranking. The model is evaluated on a preliminary corpus of ranked student texts with encouraging results. We also assess the model by showing that it performs readability classification on par with a state of the art classifier while at the same being transparent enough to allow more sophisticated interpretations.This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:\/\/ creativecommons.org\/licenses\/by\/4.0\/",
        "id":2334547
      },
      {
        "title":"",
        "text":"",
        "id":219309834
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"What research has been conducted on scaling within-document coreference resolution in extended texts?",
    "positive_ctxs":[
      {
        "title":"Scaling Within Document Coreference to Long Texts",
        "text":"State of the art end-to-end coreference resolution models use expensive span representations and antecedent prediction mechanisms. These approaches are expensive both in terms of their memory requirements as well as compute time, and are particularly ill-suited for long documents. In this paper, we propose an approximation to end-to-end models which scales gracefully to documents of any length. Replacing span representations with token representations, we reduce the time\/memory complexity via token windows and nearest neighbor sparsification methods for more efficient antecedent prediction. We show our approach's resulting reduction of training and inference time compared to state-of-the-art methods with only a minimal loss in accuracy.",
        "id":236477874
      }
    ],
    "negative_ctxs":[
      {
        "title":"Towards Automatic Error Analysis of Machine Translation Output",
        "text":"Evaluation and error analysis of machine translation output are important but difficult tasks. In this article, we propose a framework for automatic error analysis and classification based on the identification of actual erroneous words using the algorithms for computation of Word Error Rate (WER) and Position-independent word Error Rate (PER), which is just a very first step towards development of automatic evaluation measures that provide more specific information of certain translation problems. The proposed approach enables the use of various types of linguistic knowledge in order to classify translation errors in many different ways. This work focuses on one possible set-up, namely, on five error categories: inflectional errors, errors due to wrong word order, missing words, extra words, and incorrect lexical choices. For each of the categories, we analyze the contribution of various POS classes. We compared the results of automatic error analysis with the results of human error analysis in order to investigate two possible applications: estimating the contribution of each error type in a given translation output in order to identify the main sources of errors for a given translation system, and comparing different translation outputs using the introduced error categories in order to obtain more information about advantages and disadvantages of different systems and possibilites for improvements, as well as about advantages and disadvantages of applied methods for improvements. We used Arabic-English Newswire and Broadcast News and Chinese-English Newswire outputs created in the framework of the GALE project, several Spanish and English European Parliament outputs generated during the TC-Star project, and three German-English outputs generated in the framework of the fourth Machine Translation Workshop. We show that our results correlate very well with the results of a human error analysis, and that all our metrics except the extra words reflect well the differences between different versions of the same translation system as well as the differences between different translation systems.",
        "id":8332776
      },
      {
        "title":"Classification of Modal Meaning in Negotiation Dialogues",
        "text":"This paper addresses modality classification for multi-issue bargaining dialogues in order to model human-like negotiation behaviour and to efficiently compute negotiation strategies. We propose a modality annotation and classification scheme comprising semantically distinguishable categories applied reliably by humans and machines. Our classification of modality varieties is based on both the traditional dichotomy of epistemic and root modalities, and on the analysis of the available corpus data. Our modality scheme has been used for annotating human-human dialogues and training SVM-based classifiers. We built predictive models that show accuracies in the range between 73.3 and 82.6%.",
        "id":4900978
      },
      {
        "title":"Linguistically Motivated Question Classification",
        "text":"In this paper we describe a question interpretation module designed as a part of a Question Answering Dialogue System (QADS) which is used for an interactive quiz application. Question interpretation is achieved in applying a sequence of classification, information extraction, query formalization and query expansion tasks. The process of a question classification is performed based on a domain-specific taxonomy of semantic roles and relations. Our taxonomy was designed in accordance with the real spoken dialogue data. The SVM-based classifier is trained to predict the Expected Answer Type (EAT) with the precision of 82%. In order to retrieve a correct answer, focus word(-s) are extracted to augment the EAT identified by the system. Our hybrid algorithm for the extraction of focus words demonstrates the accuracy of 94.6%. EAT together with focus words are formalized in a query, which is further expanded with the synonyms from WordNet. The expanded query facilitates the search and retrieval of the information that is necessary to generate the system's responses. 1 http:\/\/trec.nist.gov\/pubs\/trec8 2",
        "id":1559567
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"What research has been conducted on the impact of intervening at intermediate layers in pretrained language models to alter the resulting text?",
    "positive_ctxs":[
      {
        "title":"Extracting Latent Steering Vectors from Pretrained Language Models",
        "text":"Prior work on controllable text generation has focused on learning how to control language models through trainable decoding, smartprompt design, or fine-tuning based on a desired objective. We hypothesize that the information needed to steer the model to generate a target sentence is already encoded within the model. Accordingly, we explore a different approach altogether: extracting latent vectors directly from pretrained language model decoders without fine-tuning. Experiments show that there exist steering vectors, which, when added to the hidden states of the language model, generate a target sentence nearly perfectly (> 99 BLEU) for English sentences from a variety of domains. We show that vector arithmetic can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark, with performance comparable to models tailored to this task. We find that distances between steering vectors reflect sentence similarity when evaluated on a textual similarity benchmark (STS-B), outperforming pooled hidden states of models. Finally, we present an analysis of the intrinsic properties of the steering vectors. Taken together, our results suggest that frozen LMs can be effectively controlled through their latent steering space. 1",
        "id":248693452
      }
    ],
    "negative_ctxs":[
      {
        "title":"Modeling Personalization in Continuous Space for Response Generation via Augmented Wasserstein Autoencoders",
        "text":"Variational autoencoders (VAEs) and Wasserstein autoencoders (WAEs) have achieved noticeable progress in open-domain response generation. Through introducing latent variables in continuous space, these models are capable of capturing utterance-level semantics, e.g., topic, syntactic properties, and thus can generate informative and diversified responses.In this work, we improve the WAE for response generation. In addition to the utterance-level information, we also model user-level information in latent continue space. Specifically, we embed user-level and utterance-level information into two multimodal distributions, and combine these two multimodal distributions into a mixed distribution. This mixed distribution will be used as the prior distribution of WAE in our proposed model, named as PersonaWAE. Experimental results on a large-scale real-world dataset confirm the superiority of our model for generating informative and personalized responses, where both automatic and human evaluations outperform state-of-the-art models.",
        "id":202788651
      },
      {
        "title":"Guided Dialogue Policy Learning without Adversarial Learning in the Loop",
        "text":"Reinforcement Learning (RL) methods have emerged as a popular choice for training an efficient and effective dialogue policy. However, these methods suffer from sparse and unstable reward signals returned by a user simulator only when a dialogue finishes. Besides, the reward signal is manually designed by human experts, which requires domain knowledge. Recently, a number of adversarial learning methods have been proposed to learn the reward function together with the dialogue policy. However, to alternatively update the dialogue policy and the reward model on the fly, we are limited to policy-gradient-based algorithms, such as REINFORCE and PPO. Moreover, the alternating training of a dialogue agent and the reward model can easily get stuck in local optima or result in mode collapse. To overcome the listed issues, we propose to decompose the adversarial training into two steps. First, we train the discriminator with an auxiliary dialogue generator and then incorporate a derived reward model into a common RL method to guide the dialogue policy learning. This approach is applicable to both on-policy and off-policy RL methods. Based on our extensive experimentation, we can conclude the proposed method:(1) achieves a remarkable task success rate using both on-policy and off-policy RL methods; and (2) has potential to transfer knowledge from existing domains to a new domain.",
        "id":215238990
      },
      {
        "title":"Using subcategorization knowledge to improve case prediction for translation to German",
        "text":"This paper demonstrates the need and impact of subcategorization information for SMT. We combine (i) features on sourceside syntactic subcategorization and (ii) an external knowledge base with quantitative, dependency-based information about target-side subcategorization frames. A manual evaluation of an English-to-German translation task shows that the subcategorization information has a positive impact on translation quality through better prediction of case.",
        "id":8704114
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"What research is available on acquiring sentence embeddings through unsupervised approaches, possibly employing contrastive learning methods?",
    "positive_ctxs":[
      {
        "title":"Smoothed Contrastive Learning for Unsupervised Sentence Embedding",
        "text":"Unsupervised contrastive sentence embedding models, e.g., unsupervised SimCSE, use the InfoNCE loss function in training. Theoretically, we expect to use larger batches to get more adequate comparisons among samples and avoid overfitting. However, increasing batch size leads to performance degradation when it exceeds a threshold, which is probably due to the introduction of false-negative pairs through statistical observation. To alleviate this problem, we introduce a simple smoothing strategy upon the InfoNCE loss function, termed Gaussian Smoothed InfoNCE (GS-InfoNCE). In other words, we add random Gaussian noise as an extension to the negative pairs without increasing the batch size. Through experiments on the semantic text similarity tasks, though simple, the proposed smoothing strategy brings improvements to unsupervised SimCSE. Our code are available at",
        "id":237453212
      }
    ],
    "negative_ctxs":[
      {
        "title":"Machine Translation Supported by Terminological Information",
        "text":"It is well-known that natural language (NL) is highly complex and ambiguous, and designing a system in the sense of 'large scale engineering' rather than in the sense of so-called 'runnable specifications', i.e. computational solutions to pre-selected NL problem areas, which could cope with most complexities of NL, seems not to be feasible in the foreseeable future. Nevertheless, there is a widespread recognition that systems designed for specific purposes are far more likely to be viable. However, in this context the discipline of computational terminology has received little attention in computational linguistics; an unfortunate situation given that natural language processing (NLP) systems seem to be most successful when applied to specialised domains. In this paper we present an approach that integrates an instance of computational terminology into a constraint-based NLP\/MT environment. Parts of this research have been carried out in the context of the ET-10\/66 project 'Terminology and Extra-linguistic Knowledge' financed by the Commission of the European Communities (CEC). Like in this project we have chosen the subject field telecommunications as the domain of reference, and the text corpus on which the work is based is the Handbook on Satellite Communication of the International Radio Consultative Committee (CCIR); this corpus is an expository type of text. The problems to be solved through our approach, and which are characteristic for sublanguage texts, relate to multiword term identification, domain-specific attachment of prepositional phrases and the disambiguation of lexical ambiguities. The terminology knowledge used in our project for constructing a terminology knowledge base was partly extracted from the information encoded in the EIRETERM term bank, that is designed primarily for human users, and is based on a linguistically motivated statistical analysis of the reference corpus.",
        "id":18694352
      },
      {
        "title":"",
        "text":"",
        "id":219306034
      },
      {
        "title":"Detecting Errors in Automatically-Parsed Dependency Relations",
        "text":"We outline different methods to detect errors in automatically-parsed dependency corpora, by comparing so-called dependency rules to their representation in the training data and flagging anomalous ones. By comparing each new rule to every relevant rule from training, we can identify parts of parse trees which are likely erroneous. Even the relatively simple methods of comparison we propose show promise for speeding up the annotation process.",
        "id":2359904
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"What research is available on hybrid approaches that combine extractive and abstractive methods for summarizing extensive texts?",
    "positive_ctxs":[
      {
        "title":"Long Document Summarization in a Low Resource Setting using Pretrained Language Models",
        "text":"Abstractive summarization is the task of compressing a long document into a coherent short document while retaining salient information. Modern abstractive summarization methods are based on deep neural networks which often require large training datasets. Since collecting summarization datasets is an expensive and time-consuming task, practical industrial settings are usually low-resource. In this paper, we study a challenging low-resource setting of summarizing long legal briefs with an average source document length of 4268 words and only 120 available (document, summary) pairs. To account for data scarcity, we used a modern pretrained abstractive summarizer BART (Lewis et al., 2020), which only achieves 17.9 ROUGE-L as it struggles with long documents. We thus attempt to compress these long documents by identifying salient sentences in the source which best ground the summary, using a novel algorithm based on GPT-2 (Radford et al., 2019) language model perplexity scores, that operates within the low resource regime. On feeding the compressed documents to BART, we observe a 6.0 ROUGE-L improvement. Our method also beats several competitive salience detection baselines. Furthermore, the identified salient sentences tend to agree with an independent human labeling by domain experts. *",
        "id":232075836
      }
    ],
    "negative_ctxs":[
      {
        "title":"Panel on Natural Language and Databases",
        "text":"While I disagree with the proposition that database query has outlived its usefulness as a test environment for natural language processing (for reasons that I give below), I believe there are other reasonable tasks which can also spur new research in NL processing. In particular, I will suggest that the task of providing a natural language interface to a rich programming environment offers a convenient yet challenging extension of work already being done with database query.First I recite some of the merits of continuing research on natural language within the confines of constructing an interface for ordinary databases. One advantage is that the speed of processing is not of overwhelming importance in this application, since one who requests information from a database can expect the retrieval to take time, with or without a natural language interface. Of course speed is desirable, and waiting for answers to apparently simple requests will be irritating, but some delay will be tolerable. This tolerance on the part of the user will, I suggest, disappear in applications where an attempt is made to engage a system in dialogue with the user, as would be the case in some expert systems, or in teaching systems. Assuming that natural language systems will not by themselves get faster as they are made to cope with larger fragments of a natural language, it will be useful to continue with database query while we wait for miracles of technology to fill our demands for ever greater processing speed.A second reason for not yet abandoning the database query as a test environment is that a great deal of important natural language processing research remains to be done in generalizing systems to cope with more than one natural language. Work on language universals gives reason to believe that some significant part of a natural language system for English should be recyclable in constructing a system for some other language. How much these cross-linguistic concerns ought to affect the construction of a particular system is itself one of the questions deserving of attention, but our experience to date suggests that it pays to avoid language-particular solutions in an implementation which aspires to treatment of any sizable fragment of a language, even a single language like English. The degree of language-independence that a natural language system can boast may also prove to be one useful metric for evaluating and comparing such systems. [t seems clear that even the task of answering database queries will provide a more than adequate supply of linguistically interesting problems for this line of research.Finally, it has simply not been our experience at Hewlett-Packard that there is any shortage of theoretically interesting problems to solve in constructing a natural language interface for databases. For example, in building such an interface, we have recently designed and implemented a hierarchically structured lexicon for a fragment of English, together with a set of lexical rules that can be run either when loading the system, or when parsing, to greatly expand the size of the lexicon actually used in the system. Several questions of theoretical interest that arose in that process remain unanswered; at least some can be answered by experimenting with our present system, functioning simply as an interface to an ordinary relational database.Having argued that significant work remains to be done in natural language processing as an interface to databases, I nonetheless believe that it would be fruitful to expand the scope of a natural language interface, to permit some manipulation of a programming environment, allowing not only the retrieval of information describing the state of the system, but also some modification of the system via natural language. Of course, such a task would be facilitated by having the information about the environment stored in a manner similar to that of a database, so that our attention could be devoted to the new range of linguistic issues raised, rather than to details of how the whole programming environment is structured and maintained.",
        "id":23751854
      },
      {
        "title":"SemEval-2016 Task 12: Clinical TempEval",
        "text":"Clinical TempEval 2016 evaluated temporal information extraction systems on the clinical domain. Nine sub-tasks were included, covering problems in time expression identification, event expression identification and temporal relation identification. Participant systems were trained and evaluated on a corpus of clinical and pathology notes from the Mayo Clinic, annotated with an extension of TimeML for the clinical domain. 14 teams submitted a total of 40 system runs, with the best systems achieving near-human performance on identifying events and times. On identifying temporal relations, there was a gap between the best systems and human performance, but the gap was less than half the gap of Clinical TempEval 2015.",
        "id":8559209
      },
      {
        "title":"THREE MECHANISMS OF WEIGHT DECAY REGULARIZATION",
        "text":"Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of L 2 regularization. Literal weight decay has been shown to outperform L 2 regularization for optimizers for which they differ. We empirically investigate weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay exerts a regularization effect, depending on the particular optimization algorithm and architecture: (1) increasing the effective learning rate, (2) approximately regularizing the inputoutput Jacobian norm, and (3) reducing the effective damping coefficient for second-order optimization. Our results provide insight into how to improve the regularization of neural networks.",
        "id":53104061
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"What research is available on the concept of using prefix tokens as a parameter-efficient method for fine-tuning language models?",
    "positive_ctxs":[
      {
        "title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation",
        "text":"Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were \"virtual tokens\". We apply prefix-tuning to GPT-2 for table-totext generation and to BART for summarization. We show that by modifying only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. 2020. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. Amodei. 2020. Language models are few-shot learners.",
        "id":230433941
      }
    ],
    "negative_ctxs":[
      {
        "title":"I Cast Detect Thoughts: Learning to Converse and Guide with Intents and Theory-of-Mind in Dungeons and Dragons",
        "text":"We propose a novel task, G4C, to study teacher-student natural language interactions in a goal-driven and grounded environment. Dungeons and Dragons (D&D), a role-playing game, provides an ideal setting to investigate such interactions. Here, the Dungeon Master (DM), i.e., the teacher, guides the actions of several players-students, each with their own personas and abilities-to achieve shared goals grounded in a fantasy world. Our approach is to decompose and model these interactions into (1) the DM's intent to guide players towards a given goal; (2) the DM's guidance utterance to the players expressing this intent; and (3) a theory-of-mind (ToM) model that anticipates the players' reaction to the guidance one turn into the future. We develop a novel reinforcement learning (RL) method for training a DM that generates guidance for players by rewarding utterances where the intent matches the ToM-anticipated player actions. Human and automated evaluations show that a DM trained to explicitly model intents and incorporate ToM of the players using RL generates better-quality guidance that is 3x more likely to fulfill the DM's intent than a vanilla natural language generation (NLG) approach.",
        "id":258987457
      },
      {
        "title":"Substring-based unsupervised transliteration with phonetic and contextual knowledge",
        "text":"We propose an unsupervised approach for substring-based transliteration which incorporates two new sources of knowledge in the learning process: (i) context by learning substring mappings, as opposed to single character mappings, and (ii) phonetic features which capture cross-lingual character similarity via prior distributions.Our approach is a two-stage iterative, boot-strapping solution, which vastly outperforms Ravi and Knight (2009)'s state-of-the-art unsupervised transliteration method and outperforms a rule-based baseline by up to 50% for top-1 accuracy on multiple language pairs. We show that substring-based models are superior to character-based models, and observe that their top-10 accuracy is comparable to the top-1 accuracy of supervised systems.Our method only requires a phonemic representation of the words. This is possible for many language-script combinations which have a high grapheme-to-phoneme correspondence e.g. scripts of Indian languages derived from the Brahmi script. Hence, Indian languages were the focus of our experiments. For other languages, a grapheme-to-phoneme converter would be required.",
        "id":573977
      },
      {
        "title":"Learning Comment Controversy Prediction in Web Discussions Using Incidentally Supervised Multi-Task CNNs",
        "text":"Comments on web news contain controversies that manifest as inter-group agreementconflicts. Tracking such rapidly evolving controversy could ease conflict resolution or journalist-user interaction. However, this presupposes controversy online-prediction that scales to diverse domains using incidental supervision signals instead of manual labeling. To more deeply interpret commentcontroversy model decisions we frame prediction as binary classification and evaluate baselines and multi-task CNNs that use an auxiliary news-genre-encoder. Finally, we use ablation and interpretability methods to determine the impacts of topic, discourse and sentiment indicators, contextual vs. global word influence, as well as genre-keywords vs. per-genrecontroversy keywords -to find that the models learn plausible controversy features using only incidentally supervised signals.",
        "id":53235483
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"What research should I consult regarding the application of continuous vector prompts in language models instead of the conventional discrete token-level prompts?",
    "positive_ctxs":[
      {
        "title":"P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks",
        "text":"Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning(Li and Liang, 2021;Qin and Eisner, 2021)optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research. 1",
        "id":248780177
      },
      {
        "title":"Learning How to Ask: Querying LMs with Mixtures of Soft Prompts",
        "text":"Natural-language prompts have recently been used to coax pretrained language models into performing other AI tasks, using a fill-in-theblank paradigm (Petroni et al., 2019) or a few-shot extrapolation paradigm (Brown et al.,  2020). For example, language models retain factual knowledge from their training corpora that can be extracted by asking them to \"fill in the blank\" in a sentential prompt. However, where does this prompt come from? We explore the idea of learning prompts by gradient descent-either fine-tuning prompts taken from previous work, or starting from random initialization. Our prompts consist of \"soft words,\" i.e., continuous vectors that are not necessarily word type embeddings from the language model. Furthermore, for each task, we optimize a mixture of prompts, learning which prompts are most effective and how to ensemble them. Across multiple English LMs and tasks, our approach hugely outperforms previous methods, showing that the implicit factual knowledge in language models was previously underestimated. Moreover, this knowledge is cheap to elicit: random initialization is nearly as good as informed initialization.",
        "id":233231453
      }
    ],
    "negative_ctxs":[
      {
        "title":"Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA",
        "text":"The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pretrained transformer models in a bi-encoderbased dense passage retriever (DPR), concluding that the language-agnostic approach is superior. Additionally, the study investigates the impact of query rewriting techniques using large language models, such as ChatGPT, on multilingual, document-grounded questionanswering systems. The experiments conducted demonstrate that, for the examples examined, query rewriting does not enhance performance compared to the original queries. This failure is due to topic switching in final dialogue turns and irrelevant topics being considered for query rewriting.",
        "id":259290499
      },
      {
        "title":"",
        "text":"",
        "id":215768182
      },
      {
        "title":"Parameter Space Noise for Exploration",
        "text":"Deep reinforcement learning (RL) methods generally engage in exploratory behavior through noise injection in the action space. An alternative is to add noise directly to the agent's parameters, which can lead to more consistent exploration and a richer set of behaviors. Methods such as evolutionary strategies use parameter perturbations, but discard all temporal structure in the process and require significantly more samples. Combining parameter noise with traditional RL methods allows to combine the best of both worlds. We demonstrate that both off-and on-policy methods benefit from this approach through experimental comparison of DQN, DDPG, and TRPO on high-dimensional discrete action environments as well as continuous control tasks. Our results show that RL with parameter noise learns more efficiently than traditional RL with action space noise and evolutionary strategies individually.",
        "id":2971655
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"What resources or toolkits are available to facilitate prompt-based learning model development in PyTorch?",
    "positive_ctxs":[
      {
        "title":"OpenPrompt: An Open-source Framework for Prompt-learning",
        "text":"Prompt-learning has become a new paradigm in modern natural language processing, which directly adapts pre-trained language models (PLMs) to cloze-style prediction, autoregressive modeling, or sequence to sequence generation, resulting in promising performances on various tasks. However, no standard implementation framework of prompt-learning is proposed yet, and most existing promptlearning codebases, often unregulated, only provide limited implementations for specific scenarios. Since there are many details such as templating strategy, initializing strategy, and verbalizing strategy, etc., need to be considered in prompt-learning, practitioners face impediments to quickly adapting the desired prompt learning methods to their applications. In this paper, we present Open-Prompt, a unified easy-to-use toolkit to conduct prompt-learning over PLMs.Open-Prompt is a research-friendly framework that is equipped with efficiency, modularity, and extendibility, and its combinability allows the freedom to combine different PLMs, task formats, and prompting modules in a unified paradigm. Users could expediently deploy prompt-learning frameworks and evaluate the generalization of them on different NLP tasks without constraints. 1",
        "id":241033259
      }
    ],
    "negative_ctxs":[
      {
        "title":"Robust Domain Adaptation for Relation Extraction via Clustering Consistency",
        "text":"We propose a two-phase framework to adapt existing relation extraction classifiers to extract relations for new target domains. We address two challenges: negative transfer when knowledge in source domains is used without considering the differences in relation distributions; and lack of adequate labeled samples for rarer relations in the new domain, due to a small labeled data set and imbalance relation distributions. Our framework leverages on both labeled and unlabeled data in the target domain. First, we determine the relevance of each source domain to the target domain for each relation type, using the consistency between the clustering given by the target domain labels and the clustering given by the predictors trained for the source domain. To overcome the lack of labeled samples for rarer relations, these clusterings operate on both the labeled and unlabeled data in the target domain. Second, we trade-off between using relevance-weighted sourcedomain predictors and the labeled target data. Again, to overcome the imbalance distribution, the source-domain predictors operate on the unlabeled target data. Our method outperforms numerous baselines and a weakly-supervised relation extraction method on ACE 2004 and YAGO. * The work is done while Nguyen was a research staff in Nanyang Technological University, Singapore.",
        "id":17856659
      },
      {
        "title":"Detecting dementia in Mandarin Chinese using transfer learning from a parallel corpus",
        "text":"Machine learning has shown promise for automatic detection of Alzheimer's disease (AD) through speech; however, efforts are hampered by a scarcity of data, especially in languages other than English. We propose a method to learn a correspondence between independently engineered lexicosyntactic features in two languages, using a large parallel corpus of outof-domain movie dialogue data. We apply it to dementia detection in Mandarin Chinese, and demonstrate that our method outperforms both unilingual and machine translation-based baselines. This appears to be the first study that transfers feature domains in detecting cognitive decline.",
        "id":67856188
      },
      {
        "title":"Improving Translation Selection with a New Translation Model Trained by Independent Monolingual Corpora",
        "text":"We propose a novel statistical translation model to improve translation selection of collocation. In the statistical approach that has been popularly applied for translation selection, bilingual corpora are used to train the translation model. However, there exists a formidable bottleneck in acquiring large-scale bilingual corpora, in particular for language pairs involving Chinese. In this paper, we propose a new approach to training the translation model by using unrelated monolingual corpora. First, a Chinese corpus and an English corpus are parsed with dependency parsers, respectively, and two dependency triple databases are generated. Then, the similarity between a Chinese word and an English word can be estimated using the two monolingual dependency triple databases with the help of a simple Chinese-English dictionary. This cross-language word similarity is used to simulate the word translation probability. Finally, the generated translation model is used together with the language model trained with the English dependency database to realize translation of Chinese collocations into English. To demonstrate the effectiveness of this method, we performed various experiments with verb-object collocation translation. The experiments produced very promising results.Section 5, some related works are introduced. Finally in Section 6, we draw conclusions and discuss future work.A New Statistical Machine Translation ModelIn this section, we will describe the proposed translation model. First, we will report our observations from a sample word-aligned bilingual corpus in order to verify our assumption. After that, we will introduce the method for estimating the cross-language word similarity by means of two monolingual corpora. Finally, we will give a formal description of the new translation model.",
        "id":2456265
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"What sources offer research on maintaining factual accuracy at the entity level in abstractive summary generation?",
    "positive_ctxs":[
      {
        "title":"Entity-level Factual Consistency of Abstractive Text Summarization",
        "text":"A key challenge for abstractive summarization is ensuring factual consistency of the generated summary with respect to the original document. For example, state-of-the-art models trained on existing datasets exhibit entity hallucination, generating names of entities that are not present in the source document. We propose a set of new metrics to quantify the entity-level factual consistency of generated summaries and we show that the entity hallucination problem can be alleviated by simply filtering the training data. In addition, we propose a summary-worthy entity classification task to the training process as well as a joint entity and summary generation approach, which yield further improvements in entity level metrics.",
        "id":231951460
      }
    ],
    "negative_ctxs":[
      {
        "title":"Offline Sentence Processing Measures for testing Readability with Users",
        "text":"While there has been much work on computational models to predict readability based on the lexical, syntactic and discourse properties of a text, there are also interesting open questions about how computer generated text should be evaluated with target populations. In this paper, we compare two offline methods for evaluating sentence quality, magnitude estimation of acceptability judgements and sentence recall. These methods differ in the extent to which they can differentiate between surface level fluency and deeper comprehension issues. We find, most importantly, that the two correlate. Magnitude estimation can be run on the web without supervision, and the results can be analysed automatically. The sentence recall methodology is more resource intensive, but allows us to tease apart the fluency and comprehension issues that arise.",
        "id":16215847
      },
      {
        "title":"The application of chordal graphs to inferring phylogenetic trees of languages",
        "text":"Phylogenetic methods are used to build evolutionary trees of languages given character data that may include lexical, phonological, and morphological information. Such data rarely admits a perfect phylogeny. We explore the use of the more permissive conservative Dollo phylogeny as an alternative or complementary approach. We propose a heuristic search algorithm based on the notion of chordal graphs. We test this approach by generating phylogenetic trees from three datasets, and comparing them to those produced by other researchers.",
        "id":17011784
      },
      {
        "title":"GraphRel: Modeling Text as Relational Graphs for Joint Entity and Relation Extraction",
        "text":"In this paper, we present GraphRel, an end-to-end relation extraction model which uses graph convolutional networks (GCNs) to jointly learn named entities and relations. In contrast to previous baselines, we consider the interaction between named entities and relations via a relation-weighted GCN to better extract relations. Linear and dependency structures are both used to extract both sequential and regional features of the text, and a complete word graph is further utilized to extract implicit features among all word pairs of the text. With the graph-based approach, the prediction for overlapping relations is substantially improved over previous sequential approaches. We evaluate GraphRel on two public datasets: NYT and WebNLG. Results show that GraphRel maintains high precision while increasing recall substantially. Also, GraphRel outperforms previous work by 3.2% and 5.8% (F1 score), achieving a new state-of-the-art for relation extraction.",
        "id":196211486
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"What techniques and frameworks have been suggested for summarizing extensive texts under resource-constrained conditions?",
    "positive_ctxs":[
      {
        "title":"Long Document Summarization in a Low Resource Setting using Pretrained Language Models",
        "text":"Abstractive summarization is the task of compressing a long document into a coherent short document while retaining salient information. Modern abstractive summarization methods are based on deep neural networks which often require large training datasets. Since collecting summarization datasets is an expensive and time-consuming task, practical industrial settings are usually low-resource. In this paper, we study a challenging low-resource setting of summarizing long legal briefs with an average source document length of 4268 words and only 120 available (document, summary) pairs. To account for data scarcity, we used a modern pretrained abstractive summarizer BART (Lewis et al., 2020), which only achieves 17.9 ROUGE-L as it struggles with long documents. We thus attempt to compress these long documents by identifying salient sentences in the source which best ground the summary, using a novel algorithm based on GPT-2 (Radford et al., 2019) language model perplexity scores, that operates within the low resource regime. On feeding the compressed documents to BART, we observe a 6.0 ROUGE-L improvement. Our method also beats several competitive salience detection baselines. Furthermore, the identified salient sentences tend to agree with an independent human labeling by domain experts. *",
        "id":232075836
      }
    ],
    "negative_ctxs":[
      {
        "title":"Multilingual Universal Dependency Parsing from Raw Text with Low-resource Language Enhancement",
        "text":"This paper describes the system of our team Phoenix for participating CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Given the annotated gold standard data in CoNLL-U format, we train the tokenizer, tagger and parser separately for each treebank based on an open source pipeline tool UDPipe. Our system reads the plain texts for input, performs the preprocessing steps (tokenization, lemmas, morphology) and finally outputs the syntactic dependencies. For the low-resource languages with no training data, we use cross-lingual techniques to build models with some close languages instead. In the official evaluation, our system achieves the macro-averaged scores of 65.61%, 52.26%, 55.71% for LAS, MLAS and BLEX respectively.",
        "id":53106215
      },
      {
        "title":"Hyper-SAGNN: a self-attention based graph neural network for hypergraphs A PREPRINT",
        "text":"Graph representation learning for hypergraphs can be used to extract patterns among higher-order interactions that are critically important in many real world problems. Current approaches designed for hypergraphs, however, are unable to handle different types of hypergraphs and are typically not generic for various learning tasks. Indeed, models that can predict variable-sized heterogeneous hyperedges have not been available. Here we develop a new self-attention based graph neural network called Hyper-SAGNN applicable to homogeneous and heterogeneous hypergraphs with variable hyperedge sizes. We perform extensive evaluations on multiple datasets, including four benchmark network datasets and two single-cell Hi-C datasets in genomics. We demonstrate that Hyper-SAGNN significantly outperforms the state-of-the-art methods on traditional tasks while also achieving great performance on a new task called outsider identification. Hyper-SAGNN will be useful for graph representation learning to uncover complex higher-order interactions in different applications.Corresponding Author (node) Coauthor (node) Coauthorship (hyperedge)Figure 1: An example of the co-authorship hypergraph. Here authors are represented as nodes (in dark blue and light blue) and coauthorships are represented as hyperedges.In this work, we developed a new self-attention based graph neural network, called Hyper-SAGNN that can work with both homogeneous and heterogeneous hypergraphs with variable hyperedge size. Using the same datasets in the DHNE paper(Tu et al., 2018), we demonstrated the advantage of Hyper-SAGNN over DHNE in multiple tasks. We further tested the effectiveness of the method in predicting edges and hyperedges and showed that the model can achieve better performance from the multi-tasking setting. We also formulated a novel task called outsider identification and showed that Hyper-SAGNN performs strongly. Importantly, as an application of Hyper-SAGNN to single-cell genomics, we were able to learn the embeddings for the most recently produced single-cell Hi-C (scHi-C) datasets to uncover the clustering of cells based on their 3D genome structure(Ramani et al., 2017;Nagano et al., 2017). We showed that Hyper-SAGNN achieved improved results in identifying distinct cell populations as compared to existing scHi-C clustering methods. Taken together, Hyper-SAGNN can significantly outperform the state-of-the-art methods and can be applied to a wide range of hypergraphs for different applications.",
        "id":207847681
      },
      {
        "title":"",
        "text":"",
        "id":218977422
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"What techniques exist for efficiently fine-tuning transformer language models by adjusting a limited set of parameters?",
    "positive_ctxs":[
      {
        "title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation",
        "text":"Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were \"virtual tokens\". We apply prefix-tuning to GPT-2 for table-totext generation and to BART for summarization. We show that by modifying only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. 2020. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. Amodei. 2020. Language models are few-shot learners.",
        "id":230433941
      }
    ],
    "negative_ctxs":[
      {
        "title":"From Baby Steps to Leapfrog: How \"Less is More\" in Unsupervised Dependency Parsing *",
        "text":"We present three approaches for unsupervised grammar induction that are sensitive to data complexity and apply them to Klein and Manning's Dependency Model with Valence. The first, Baby Steps, bootstraps itself via iterated learning of increasingly longer sentences and requires no initialization. This method substantially exceeds Klein and Manning's published scores and achieves 39.4% accuracy on Section 23 (all sentences) of the Wall Street Journal corpus. The second, Less is More, uses a low-complexity subset of the available data: sentences up to length 15. Focusing on fewer but simpler examples trades off quantity against ambiguity; it attains 44.1% accuracy, using the standard linguisticallyinformed prior and batch training, beating state-of-the-art. Leapfrog, our third heuristic, combines Less is More with Baby Steps by mixing their models of shorter sentences, then rapidly ramping up exposure to the full training set, driving up accuracy to 45.0%. These trends generalize to the Brown corpus; awareness of data complexity may improve other parsing models and unsupervised algorithms.",
        "id":1363892
      },
      {
        "title":"Analyse discursive et informations de factivité",
        "text":"Les annotations discursives proposées dans le cadre de théories discursives comme RST (Rhetorical Structure Theory) ou SDRT (Segmented Dicourse Representation Theory) ont comme point fort de construire une structure discursive globale liant toutes les informations données dans un texte. Les annotations discursives proposées dans le PDTB (Penn Discourse Tree Bank) ont comme point fort d'identifier la \"source\" de chaque information du texte -répondant ainsi à la question qui a dit ou pense quoi ? Nous proposons une approche unifiée pour les annotations discursives alliant les points forts de ces deux courants de recherche. Cette approche unifiée repose crucialement sur des information de factivité, telles que celles qui sont annotées dans le corpus (anglais) FactBank.Abstract. Discursive annotations proposed in theories of discourse such as RST (Rhetorical StructureTheory) or SDRT (Segmented Representation Theory Dicourse) have the advatange of building a global discourse structure linking all the information in a text. Discursive annotations proposed in PDTB (Penn Discourse Tree Bank) have the advatange of identifying the \"source\" of each information -thereby answering to questions such as who says or thinks what ? We propose a unified approach for discursive annotations combining the strengths of these two streams of research. This unified approach relies crucially on factivity information, as encoded in the English corpus FactBank.",
        "id":60391364
      },
      {
        "title":"-BY-NC 4.0 Improving Signer Independent Sign Language Recognition for Low Resource Languages",
        "text":"The reliance of deep learning algorithms on large scale datasets represents a significant challenge when learning from low resource sign language datasets. This challenge is compounded when we consider that, for a model to be effective in the real world, it must not only learn the variations of a given sign, but also learn to be invariant to the person signing. In this paper, we first illustrate the performance gap between signer-independent and signer-dependent models on Irish Sign Language manual hand shape data. We then evaluate the effect of transfer learning, with different levels of fine-tuning, on the generalisation of signer independent models, and show the effects of different input representations, namely variations in image data and pose estimation. We go on to investigate the sensitivity of current pose estimation models in order to establish their limitations and areas in need of improvement. The results show that accurate pose estimation outperforms raw RGB image data, even when relying on pre-trained image models. Following on from this, we investigate image texture as a potential contributing factor to the gap in performance between signer-dependent and signer-independent models using counterfactual testing images and discuss potential ramifications for low-resource sign languages.",
        "id":252624556
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"What techniques exist for incorporating context in detecting emotions within dialogues by leveraging pre-trained language models?",
    "positive_ctxs":[
      {
        "title":"Context-Dependent Embedding Utterance Representations for Emotion Recognition in Conversations",
        "text":"Emotion Recognition in Conversations(ERC)has been gaining increasing importance as conversational agents become more and more common. Recognizing emotions is key for effective communication, being a crucial component in the development of effective and empathetic conversational agents. Knowledge and understanding of the conversational context are extremely valuable for identifying the emotions of the interlocutor. We thus approach Emotion Recognition in Conversations leveraging the conversational context, i.e., taking into attention previous conversational turns. The usual approach to model the conversational context has been to produce context-independent representations of each utterance and subsequently perform contextual modeling of these. Here we propose context-dependent embedding representations of each utterance by leveraging the contextual representational power of pretrained transformer language models. In our approach, we feed the conversational context appended to the utterance to be classified as input to the RoBERTa encoder, to which we append a simple classification module, thus discarding the need to deal with context after obtaining the embeddings since these constitute already an efficient representation of such context. We also investigate how the number of introduced conversational turns influences our model performance. The effectiveness of our approach is validated on the open-domain DailyDialog dataset and on the task-oriented EmoWOZ dataset.",
        "id":258179794
      }
    ],
    "negative_ctxs":[
      {
        "title":"A-Team \/ Martin-Luther-Universität Halle-Wittenberg@CLSciSumm 20",
        "text":"This document demonstrates our groups approach to the CL-SciSumm shared task 2020(Chandrasekaran et al., 2020). There are three tasks in CL-SciSumm 2020. In Task 1a, we apply a Siamese neural network to identify the spans of text in the reference paper best reflecting a citation. In Task 1b, we use a SVM to classify the facet of a citation.",
        "id":226283866
      },
      {
        "title":"A Framework for Identity Resolution and Merging for Multi-source Information Extraction",
        "text":"In the context of ontology-based information extraction, identity resolution is the process of deciding whether an instance extracted from text refers to a known entity in the target domain (e.g. the ontology). We present an ontology-based framework for identity resolution which can be customised to different application domains and extraction tasks. Rules for identify resolution, which compute similarities between target and source entities based on class information and instance properties and values, can be defined for each class in the ontology. We present a case study of the application of the framework to the problem of multi-source job vacancy extraction.",
        "id":6353943
      },
      {
        "title":"An Interface for Rapid Natural Language Processing Development in UIMA",
        "text":"This demonstration presents the Annotation Librarian, an application programming interface that supports rapid development of natural language processing (NLP) projects built in Apache Unstructured Information Management Architecture (UIMA). The flexibility of UIMA to support all types of unstructured data -images, audio, and text -increases the complexity of some of the most common NLP development tasks. The Annotation Librarian interface handles these common functions and allows the creation and management of annotations by mirroring Java methods used to manipulate Strings. The familiar syntax and NLP-centric design allows developers to adopt and rapidly develop NLP algorithms in UIMA. The general functionality of the interface is described in relation to the use cases that necessitated its creation.",
        "id":7525432
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"What techniques exist to enhance the few-shot fine-tuning performance in small pre-trained language models?",
    "positive_ctxs":[
      {
        "title":"Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference",
        "text":"Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with \"task descriptions\" in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in lowresource settings by a large margin. 1",
        "id":210838924
      },
      {
        "title":"Making Pre-trained Language Models Better Few-shot Learners",
        "text":"The recent GPT-3 model(Brown et al., 2020)achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF-better few-shot fine-tuning of language models 1 -a suite of simple and complementary techniques for finetuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning. 2 * The first two authors contributed equally. 1 Alternatively, language models' best friends forever. 2 Our implementation is publicly available at https:\/\/ github.com\/princeton-nlp\/LM-BFF. Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Association for Computational Linguistics (ACL). Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Association for Computational Linguistics (ACL).",
        "id":229923710
      }
    ],
    "negative_ctxs":[
      {
        "title":"Improving Event Duration Question Answering by Leveraging Existing Temporal Information Extraction Data",
        "text":"Understanding event duration is essential for understanding natural language. However, the amount of training data for tasks like duration question answering, i.e., McTACO, is very limited, suggesting a need for external duration information to improve this task. The duration information can be obtained from existing temporal information extraction tasks, such as UDS-T and TimeBank, where more duration data is available. A straightforward two-stage fine-tuning approach might be less likely to succeed given the discrepancy between the target duration question answering task and the intermediary duration classification task. This paper resolves this discrepancy by automatically recasting an existing event duration classification task from UDS-T to a question answering task similar to the target McTACO. We investigate the transferability of duration information by comparing whether the original UDS-T duration classification or the recast UDS-T duration question answering can be transferred to the target task. Our proposed model achieves a 13% Exact Match score improvement over the baseline on the McTACO duration question answering task, showing that the two-stage fine-tuning approach succeeds when the discrepancy between the target and intermediary tasks are resolved.",
        "id":250150656
      },
      {
        "title":"Development of the Korean Resource Grammar: Towards Grammar Customization",
        "text":"The Korean Resource Grammar (KRG)is a computational open-source grammar of Korean (Kim and Yang, 2003)  that has been constructed within the DELPH-IN consortium since 2003. This paper reports the second phase of the KRG development that moves from a phenomenabased approach to grammar customization using the LinGO Grammar Matrix. This new phase of development not only improves the parsing efficiency but also adds generation capacity, which is necessary for many NLP applications.",
        "id":5142531
      },
      {
        "title":"Learning Bilingual Lexicons from Monolingual Corpora",
        "text":"We present a method for learning bilingual translation lexicons from monolingual corpora. Word types in each language are characterized by purely monolingual features, such as context counts and orthographic substrings. Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings. We show that high-precision lexicons can be learned in a variety of language pairs and from a range of corpus types.",
        "id":7185434
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"What techniques have been investigated to enhance multimodal sentiment analysis by fusing different modalities?",
    "positive_ctxs":[
      {
        "title":"Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis",
        "text":"In multimodal sentiment analysis (MSA), the performance of a model highly depends on the quality of synthesized embeddings. These embeddings are generated from the upstream process called multimodal fusion, which aims to extract and combine the input unimodal raw data to produce a richer multimodal representation. Previous work either back-propagates the task loss or manipulates the geometric property of feature spaces to produce favorable fusion results, which neglects the preservation of critical task-related information that flows from input to the fusion results. In this work, we propose a framework named MultiModal InfoMax (MMIM), which hierarchically maximizes the Mutual Information (MI) in unimodal input pairs (inter-modality) and between multimodal fusion result and unimodal input in order to maintain taskrelated information through multimodal fusion. The framework is jointly trained with the main task (MSA) to improve the performance of the downstream MSA task. To address the intractable issue of MI bounds, we further formulate a set of computationally simple parametric and non-parametric methods to approximate their truth value. Experimental results on the two widely used datasets demonstrate the efficacy of our approach. The implementation of this work is publicly available at https:\/\/github.com\/ declare-lab\/Multimodal-Infomax.",
        "id":237372185
      },
      {
        "title":"Multimodal Sentiment Detection Based on Multi-channel Graph Neural Networks",
        "text":"With the popularity of smartphones, we have witnessed the rapid proliferation of multimodal posts on various social media platforms. We observe that the multimodal sentiment expression has specific global characteristics, such as the interdependencies of objects or scenes within the image. However, most previous studies only considered the representation of a single image-text post and failed to capture the global co-occurrence characteristics of the dataset. In this paper, we propose Multi-channel Graph Neural Networks with Sentiment-awareness (MGNNS) for imagetext sentiment detection. Specifically, we first encode different modalities to capture hidden representations. Then, we introduce multichannel graph neural networks to learn multimodal representations based on the global characteristics of the dataset. Finally, we implement multimodal in-depth fusion with the multi-head attention mechanism to predict the sentiment of image-text pairs. Extensive experiments conducted on three publicly available datasets demonstrate the effectiveness of our approach for multimodal sentiment detection.",
        "id":236460184
      }
    ],
    "negative_ctxs":[
      {
        "title":"Unified Speech-Text Pre-training for Speech Translation and Recognition",
        "text":"We describe a method to jointly pre-train speech and text in an encoder-decoder modeling framework for speech translation and recognition. The proposed method incorporates four self-supervised and supervised subtasks for cross modality learning.A self-supervised speech subtask leverages unlabelled speech data, and a (self-)supervised text to text subtask makes use of abundant text training data. Two auxiliary supervised speech tasks are included to unify speech and text modeling space. Our contribution lies in integrating linguistic information from the text corpus into the speech pre-training. Detailed analysis reveals learning interference among subtasks. Two pre-training configurations for speech translation and recognition, respectively, are presented to alleviate subtask interference. Our experiments show the proposed method can effectively fuse speech and text information into one model. It achieves between 1.7 and 2.3 BLEU improvement above the state of the art on the MUST-C speech translation dataset and comparable WERs to wav2vec 2.0 on the LIBRISPEECH speech recognition task. 1",
        "id":248119033
      },
      {
        "title":"Co-guiding Net: Achieving Mutual Guidances between Multiple Intent Detection and Slot Filling via Heterogeneous Semantics-Label Graphs",
        "text":"Recent graph-based models for joint multiple intent detection and slot filling have obtained promising results through modeling the guidance from the prediction of intents to the decoding of slot filling. However, existing methods (1) only model the unidirectional guidance from intent to slot; (2) adopt homogeneous graphs to model the interactions between the slot semantics nodes and intent label nodes, which limit the performance. In this paper, we propose a novel model termed Co-guiding Net, which implements a two-stage framework achieving the mutual guidances between the two tasks. In the first stage, the initial estimated labels of both tasks are produced, and then they are leveraged in the second stage to model the mutual guidances. Specifically, we propose two heterogeneous graph attention networks working on the proposed two heterogeneous semantics-label graphs, which effectively represent the relations among the semantics nodes and label nodes. Experiment results show that our model outperforms existing models by a large margin, obtaining a relative improvement of 19.3% over the previous best model on Mix-ATIS dataset in overall accuracy.",
        "id":252992662
      },
      {
        "title":"BERT, MBERT, or BIBERT? A Study on Contextualized Embeddings for Neural Machine Translation",
        "text":"The success of bidirectional encoders using masked language models, such as BERT, on numerous natural language processing tasks has prompted researchers to attempt to incorporate these pre-trained models into neural machine translation (NMT) systems. However, proposed methods for incorporating pretrained models are non-trivial and mainly focus on BERT, which lacks a comparison of the impact that other pre-trained models may have on translation performance. In this paper, we demonstrate that simply using the output (contextualized embeddings) of a tailored and suitable bilingual pre-trained language model (dubbed BIBERT) as the input of the NMT encoder achieves state-of-the-art translation performance. Moreover, we also propose a stochastic layer selection approach and a concept of dual-directional translation model to ensure the sufficient utilization of contextualized embeddings. In the case of without using back translation, our best models achieve BLEU scores of 30.45 for En→De and 38.61 for De→En on the IWSLT'14 dataset, and 31.26 for En→De and 34.94 for De→En on the WMT'14 dataset, which exceeds all published numbers 12 .",
        "id":237485586
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Where can I find a database of good prompts to use for prompting language models for in-context learning?",
    "positive_ctxs":[
      {
        "title":"OpenPrompt: An Open-source Framework for Prompt-learning",
        "text":"Prompt-learning has become a new paradigm in modern natural language processing, which directly adapts pre-trained language models (PLMs) to cloze-style prediction, autoregressive modeling, or sequence to sequence generation, resulting in promising performances on various tasks. However, no standard implementation framework of prompt-learning is proposed yet, and most existing promptlearning codebases, often unregulated, only provide limited implementations for specific scenarios. Since there are many details such as templating strategy, initializing strategy, and verbalizing strategy, etc., need to be considered in prompt-learning, practitioners face impediments to quickly adapting the desired prompt learning methods to their applications. In this paper, we present Open-Prompt, a unified easy-to-use toolkit to conduct prompt-learning over PLMs.Open-Prompt is a research-friendly framework that is equipped with efficiency, modularity, and extendibility, and its combinability allows the freedom to combine different PLMs, task formats, and prompting modules in a unified paradigm. Users could expediently deploy prompt-learning frameworks and evaluate the generalization of them on different NLP tasks without constraints. 1",
        "id":241033259
      }
    ],
    "negative_ctxs":[
      {
        "title":"Automatic Refinement of Syntactic Categories in Chinese Word Structures",
        "text":"Annotated word structures are useful for various Chinese NLP tasks, such as word segmentation, POS tagging and syntactic parsing. Chinese word structures are often represented by binary trees, the nodes of which are labeled with syntactic categories, due to the syntactic nature of Chinese word formation. It is desirable to refine the annotation by labeling nodes of word structure trees with more proper syntactic categories so that the combinatorial properties in the word formation process are better captured. This can lead to improved performances on the tasks that exploit word structure annotations. We propose syntactically inspired algorithms to automatically induce syntactic categories of word structure trees using POS tagged corpus and branching in existing Chinese word structure trees. We evaluate the quality of our annotation by comparing the performances of models based on our annotation and another publicly available annotation, respectively. The results on two variations of Chinese word segmentation task show that using our annotation can lead to significant performance improvements.",
        "id":8158821
      },
      {
        "title":"La désambiguïsation des abréviations du domaine médical",
        "text":"Les abréviations, tout en étant répandues dans la langue, ont une sémantique assez opaque car seulement les premières lettres sont transparentes. Cela peut donc empêcher la compréhension des abréviations, et des textes qui les contiennent, par les locuteurs. De plus, certaines abréviations sont ambiguës en ayant plusieurs sens possibles, ce qui augmente la difficulté de leur compréhension. Nous proposons de travailler avec les abréviations de la langue médicale dans un cadre lié à la simplification automatique de textes. Dans le processus de simplification, il faut en effet choisir la forme étendue des abréviations qui soit correcte pour un contexte donné. Nous proposons de traiter la désambiguïsation d'abréviations comme un problème de catégorisation supervisée. Les descripteurs sont construits à partir des contextes lexical et syntaxique des abréviations. L'entraînement est effectué sur les phrases qui contiennent les formes étendues des abréviations. Le test est effectué sur un corpus construit manuellement, où les bons sens des abréviations ont été définis selon les contextes. Notre approche montre une F-mesure moyenne de 0,888 sur le corpus d'entraînement en validation croisée et 0,773 sur le corpus de test.ABSTRACTDisambiguation of abbreviations from the medical domain.Abbreviations, although commonly used, have quite opaque semantics because only their first letters are transparent. This may prevent from understanding of abbreviations, and of texts they occur within, by speakers. Besides, some abbreviations are ambiguous and have more than one meaning, which increases their understanding difficulty. We propose to work with abbreviations from the medical domain as part of the automatic text simplification. During the simplification process, it is indeed necessary to chose the right expanded form of abbreviations satisfying a given context. We propose to address disambiguation of abbreviations as supervised categorization problem. Descriptors are built from lexical and syntactic contexts of the abbreviations. Training is done on sentences containing expanded forms of the abbreviations. Test is done on corpus built manually, in which the correct senses of abbreviations have been defined according to their contexts. The average F-measure of our approach is 0.888 in cross-validation on the training corpus and 0.773 on the test corpus. MOTS-CLÉS : Désambiguïsation sémantique, domaine biomédical, abréviations, simplification.",
        "id":221373803
      },
      {
        "title":"",
        "text":"",
        "id":10736550
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Where can I read about the using soft embeddings to elicit knowledge from large pre-trained models, at small tuning cost?",
    "positive_ctxs":[
      {
        "title":"The Power of Scale for Parameter-Efficient Prompt Tuning",
        "text":"In this work, we explore \"prompt tuning,\" a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient \"prompt ensembling.\" We release code and model checkpoints to reproduce our experiments. 1ReferencesRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,  Danilo Giampiccolo, Bernardo Magnini, and Idan  Szpektor. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognising textual entailment, volume 6, pages 6-4. Venice. . 2019a. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
        "id":233296808
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"We investigate how novel English-derived words (anglicisms) are used in a Germanlanguage Internet hip hop forum, and what factors contribute to their uptake.",
        "id":18471984
      },
      {
        "title":"Improving Automatic Sentence Boundary Detection with Confusion Networks A. Stolcke£",
        "text":"We extend existing methods for automatic sentence boundary detection by leveraging multiple recognizer hypotheses in order to provide robustness to speech recognition errors. For each hypothesized word sequence, an HMM is used to estimate the posterior probability of a sentence boundary at each word boundary. The hypotheses are combined using confusion networks to determine the overall most likely events. Experiments show improved detection of sentences for conversational telephone speech, though results are mixed for broadcast news.",
        "id":8686087
      },
      {
        "title":"EventWiki: A Knowledge Base of Major Events",
        "text":"This paper introduces a new resource called EventWiki which is, to the best of our knowledge, the first knowledge base resource of major events. In contrast to most existing knowledge bases that focus on static entities such as people, locations and organizations, our EventWiki concentrate on major events, in which all entries in EventWiki are important events in mankind history. We demonstrate that EventWiki is a very useful resource for information extraction regarding events in Natural Language Processing (NLP), knowledge inference and automatic knowledge base construction.",
        "id":21695474
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Which method involves training additional prompt tokens for every layer during the fine-tuning of language models, specifically evaluating their performance on generation tasks?",
    "positive_ctxs":[
      {
        "title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation",
        "text":"Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were \"virtual tokens\". We apply prefix-tuning to GPT-2 for table-totext generation and to BART for summarization. We show that by modifying only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. 2020. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. Amodei. 2020. Language models are few-shot learners.",
        "id":230433941
      }
    ],
    "negative_ctxs":[
      {
        "title":"CFLOWNETS: CONTINUOUS CONTROL WITH GENERATIVE FLOW NETWORKS",
        "text":"Generative flow networks (GFlowNets), as an emerging technique, can be used as an alternative to reinforcement learning for exploratory control tasks. GFlowNet aims to generate distribution proportional to the rewards over terminating states, and to sample different candidates in an active learning fashion. GFlowNets need to form a DAG and compute the flow matching loss by traversing the inflows and outflows of each node in the trajectory. No experiments have yet concluded that GFlowNets can be used to handle continuous tasks. In this paper, we propose generative continuous flow networks (CFlowNets) that can be applied to continuous control tasks. First, we present the theoretical formulation of CFlowNets. Then, a training framework for CFlowNets is proposed, including the action selection process, the flow approximation algorithm, and the continuous flow matching loss function. Afterward, we theoretically prove the error bound of the flow approximation. The error decreases rapidly as the number of flow samples increases. Finally, experimental results on continuous control tasks demonstrate the performance advantages of CFlowNets compared to many reinforcement learning methods, especially regarding exploration ability.",
        "id":257365137
      },
      {
        "title":"Performance Prediction via Bayesian Matrix Factorisation for Multilingual Natural Language Processing Tasks",
        "text":"Performance prediction for Natural Language Processing (NLP) seeks to reduce the experimental burden resulting from the myriad of different evaluation scenarios, e.g., the combination of languages used in multilingual transfer. In this work, we explore the framework of Bayesian matrix factorisation for performance prediction, as many experimental settings in NLP can be naturally represented in matrix format. Our approach outperforms the stateof-the-art in several NLP benchmarks, including machine translation and cross-lingual entity linking. Furthermore, it also avoids hyperparameter tuning and is able to provide uncertainty estimates over predictions.",
        "id":258378277
      },
      {
        "title":"Neural Unsupervised Reconstruction of Protolanguage Word Forms",
        "text":"We present a state-of-the-art neural approach to the unsupervised reconstruction of ancient word forms. Previous work in this domain used expectation-maximization to predict simple phonological changes between ancient word forms and their cognates in modern languages. We extend this work with neural models that can capture more complicated phonological and morphological changes. At the same time, we preserve the inductive biases from classical methods by building monotonic alignment constraints into the model and deliberately underfitting during the maximization step. We evaluate our performance on the task of reconstructing Latin from a dataset of cognates across five Romance languages, achieving a notable reduction in edit distance from the target word forms compared to previous methods.",
        "id":253553792
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Which paper has conducted a thorough analysis of how language models of different architectures generate text that either aligns with or deviates from the properties of natural human language?",
    "positive_ctxs":[
      {
        "title":"Language Model Evaluation Beyond Perplexity",
        "text":"We propose an alternate approach to quantifying how well language models learn natural language: we ask how well they match the statistical tendencies of natural language. To answer this question, we analyze whether text generated from language models exhibits the statistical tendencies present in the humangenerated text on which they were trained. We provide a framework-paired with significance tests-for evaluating the fit of language models to these trends. We find that neural language models appear to learn only a subset of the tendencies considered, but align much more closely with empirical trends than proposed theoretical distributions (when present). Further, the fit to different distributions is highly-dependent on both model architecture and generation strategy. As concrete examples, text generated under the nucleus sampling scheme adheres more closely to the typetoken relationship of natural language than text produced using standard ancestral sampling; text from LSTMs reflects the natural language distributions over length, stopwords, and symbols surprisingly well.",
        "id":235265909
      }
    ],
    "negative_ctxs":[
      {
        "title":"ON BONUS-BASED EXPLORATION METHODS IN THE ARCADE LEARNING ENVIRONMENT",
        "text":"Research on exploration in reinforcement learning, as applied to Atari 2600 gameplaying, has emphasized tackling difficult exploration problems such as MON-TEZUMA'S REVENGE(Bellemare et al., 2016). Recently, bonus-based exploration methods, which explore by augmenting the environment reward, have reached above-human average performance on such domains. In this paper we reassess popular bonus-based exploration methods within a common evaluation framework. We combine Rainbow (Hessel et al., 2018)  with different exploration bonuses and evaluate its performance on MONTEZUMA'S REVENGE, Bellemare et al.'s set of hard of exploration games with sparse rewards, and the whole Atari 2600 suite. We find that while exploration bonuses lead to higher score on MONTEZUMA'S REVENGE they do not provide meaningful gains over the simpler -greedy scheme. In fact, we find that methods that perform best on that game often underperform -greedy on easy exploration Atari 2600 games. We find that our conclusions remain valid even when hyperparameters are tuned for these easy-exploration games. Finally, we find that none of the methods surveyed benefit from additional training samples (1 billion frames, versus Rainbow's 200 million) on Bellemare et al.'s hard exploration games. Our results suggest that recent gains in MONTEZUMA'S REVENGE may be better attributed to architecture change, rather than better exploration schemes; and that the real pace of progress in exploration research for Atari 2600 games may have been obfuscated by good results on a single domain. , et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.",
        "id":214283684
      },
      {
        "title":"C3PO: A Lightweight Copying Mechanism for Translating Pseudocode to Code",
        "text":"Writing computer programs is a skill that remains inaccessible to most due to the barrier of programming language (PL) syntax. While large language models (LLMs) have been proposed to translate natural language pseudocode to PL code, they are costly in terms of data and compute. We propose a lightweight alternative to LLMs that exploits the property of code wherein most tokens can be simply copied from the pseudocode. We divide the problem into three phases: Copy, Generate, and Combine. In the Copy Phase, a binary classifier is employed to determine and mask the pseudocode tokens that can be directly copied into the code. In the Generate Phase, a Sequence-to-Sequence model is used to generate the masked PL code equivalent. In the Combine Phase, the generated sequence is combined with the tokens that the Copy Phase had masked. We show that our C3PO models achieve similar performance to non-C3PO models while reducing the computational cost of training as well as the vocabulary sizes.",
        "id":253761967
      },
      {
        "title":"User Embedding for Scholarly Microblog Recommendation",
        "text":"Nowadays, many scholarly messages are posted on Chinese microblogs and more and more researchers tend to find scholarly information on microblogs. In order to exploit microblogging to benefit scientific research, we propose a scholarly microblog recommendation system in this study. It automatically collects and mines scholarly information from Chinese microblogs, and makes personalized recommendations to researchers. We propose two different neural network models which learn the vector representations for both users and microblog texts. Then the recommendation is accomplished based on the similarity between a user's vector and a microblog text's vector. We also build a dataset for this task. The two embedding models are evaluated on the dataset and show good results compared to several baselines.",
        "id":18202618
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Which paper introduced the task of creating extended, coherent dialogues from brief summaries?",
    "positive_ctxs":[
      {
        "title":"Summary Grounded Conversation Generation",
        "text":"Many conversation datasets have been constructed in the recent years using crowdsourcing. However, the data collection process can be time consuming and presents many challenges to ensure data quality. Since language generation has improved immensely in recent years with the advancement of pretrained language models, we investigate how such models can be utilized to generate entire conversations, given only a summary of a conversation as the input. We explore three approaches to generate summary grounded conversations, and evaluate the generated conversations using automatic measures and human judgements. We also show that the accuracy of conversation summarization can be improved by augmenting a conversation summarization dataset with generated conversations.",
        "id":235359080
      }
    ],
    "negative_ctxs":[
      {
        "title":"User Requirements Analysis for Meeting Information Retrieval Based on Query Elicitation",
        "text":"We present a user requirements study for Question Answering on meeting records that assesses the difficulty of users questions in terms of what type of knowledge is required in order to provide the correct answer. We grounded our work on the empirical analysis of elicited user queries. We found that the majority of elicited queries (around 60%) pertain to argumentative processes and outcomes. Our analysis also suggests that standard keyword-based Information Retrieval can only deal successfully with less than 20% of the queries, and that it must be complemented with other types of metadata and inference.",
        "id":5332858
      },
      {
        "title":"Multivalent Entailment Graphs for Question Answering",
        "text":"Drawing inferences between open-domain natural language predicates is a necessity for true language understanding. There has been much progress in unsupervised learning of entailment graphs for this purpose. We make three contributions: (1) we reinterpret the Distributional Inclusion Hypothesis to model entailment between predicates of different valencies, like DEFEAT(Biden, Trump) WIN(Biden);(2) we actualize this theory by learning unsupervised Multivalent Entailment Graphs of open-domain predicates; and (3) we demonstrate the capabilities of these graphs on a novel question answering task. We show that directional entailment is more helpful for inference than non-directional similarity on questions of fine-grained semantics. We also show that drawing on evidence across valencies answers more questions than by using only the same valency evidence.",
        "id":233289793
      },
      {
        "title":"Legal Friction of State Civil Apparatus Neutrality in Indonesia",
        "text":"",
        "id":159321124
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Which paper presents a platform that emphasizes evaluating the robustness of models on benchmarks?",
    "positive_ctxs":[
      {
        "title":"Dynabench: Rethinking Benchmarking in NLP",
        "text":"We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-inthe-loop dataset creation: annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community: contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple challenge examples and falter in real-world scenarios. With Dynabench, dataset creation, model development, and model assessment can directly inform each other, leading to more robust and informative benchmarks. We report on four initial NLP tasks, illustrating these concepts and highlighting the promise of the platform, and address potential objections to dynamic benchmarking as a new standard for the field.",
        "id":233444226
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":219307941
      },
      {
        "title":"A Domain-Specific Statistical Surface Realizer",
        "text":"We present a search-based approach to automatic surface realization given a corpus of domain sentences. Using heuristic search based on a statistical language model and a structure we introduce called an inheritance table we overgenerate a set of complete syntactic-semantic trees that are consistent with the given semantic structure and have high likelihood relative to the language model. These trees are then lexicalized, linearized, scored, and ranked. This model is being developed to generate real-time navigation instructions.",
        "id":2182400
      },
      {
        "title":"Fine-Tuning in Brazilian Portuguese-English Statistical Transfer Ma- chine Translation: Verbal Tenses",
        "text":"This paper describes an experiment designed to evaluate the development of a Statistical Transfer-based Brazilian Portuguese to English Machine Translation system. We compare the performance of the system with the inclusion of new syntactic written rules concerning verbal tense between the Brazilian Portuguese and English languages. Results indicate that the system performance improved compared with an initial version of the system. However significant adjustments remain to be done.",
        "id":692838
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Which paper shows that generated captions of models are still worse than human written ones?",
    "positive_ctxs":[
      {
        "title":"Transparent Human Evaluation for Image Captioning",
        "text":"We establish THUMB, a rubric-based human evaluation protocol for image captioning models. Our scoring rubrics and their definitions are carefully developed based on machine-and human-generated captions on the MSCOCO dataset. Each caption is evaluated along two main dimensions in a tradeoff (precision and recall) as well as other aspects that measure the text quality (fluency, conciseness, and inclusive language). Our evaluations demonstrate several critical problems of the current evaluation practice. Human-generated captions show substantially higher quality than machine-generated ones, especially in coverage of salient information (i.e., recall), while most automatic metrics say the opposite. Our rubric-based results reveal that CLIPScore, a recent metric that uses image features, better correlates with human judgments than conventional text-only metrics because it is more sensitive to recall. We hope that this work will promote a more transparent evaluation protocol for image captioning and its automatic metrics. 1 . 2015. From images to sentences through scene description graphs using commonsense reasoning and knowledge.",
        "id":244270388
      }
    ],
    "negative_ctxs":[
      {
        "title":"TOPOLOGICALLY REGULARIZED DATA EMBEDDINGS",
        "text":"Unsupervised feature learning often finds low-dimensional embeddings that capture the structure of complex data. For tasks for which prior expert topological knowledge is available, incorporating this into the learned representation may lead to higher quality embeddings. For example, this may help one to embed the data into a given number of clusters, or to accommodate for noise that prevents one from deriving the distribution of the data over the model directly, which can then be learned more effectively. However, a general tool for integrating different prior topological knowledge into embeddings is lacking. Although differentiable topology layers have been recently developed that can (re)shape embeddings into prespecified topological models, they have two important limitations for representation learning, which we address in this paper. First, the currently suggested topological losses fail to represent simple models such as clusters and flares in a natural manner. Second, these losses neglect all original structural (such as neighborhood) information in the data that is useful for learning. We overcome these limitations by introducing a new set of topological losses, and proposing their usage as a way for topologically regularizing data embeddings to naturally represent a prespecified model. We include thorough experiments on synthetic and real data that highlight the usefulness and versatility of this approach, with applications ranging from modeling high-dimensional single-cell data, to graph embedding.",
        "id":239016008
      },
      {
        "title":"How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks",
        "text":"Many recent papers address reading comprehension, where examples consist of (question, passage, answer) tuples. Presumably, a model must combine information from both questions and passages to predict corresponding answers. However, despite intense interest in the topic, with hundreds of published papers vying for leaderboard dominance, basic questions about the difficulty of many popular benchmarks remain unanswered. In this paper, we establish sensible baselines for the bAbI, SQuAD, CBT, CNN, and Whodid-What datasets, finding that question-and passage-only models often perform surprisingly well. On 14 out of 20 bAbI tasks, passage-only models achieve greater than 50% accuracy, sometimes matching the full model. Interestingly, while CBT provides 20-sentence passages, only the last is needed for comparably accurate prediction. By comparison, SQuAD and CNN appear better-constructed.",
        "id":52011616
      },
      {
        "title":"META BACK-TRANSLATION",
        "text":"Back-translation (Sennrich et al., 2016)is an effective strategy to improve the performance of Neural Machine Translation (NMT) by generating pseudo-parallel data. However, several recent works have found that better translation quality of the pseudo-parallel data does not necessarily lead to better final translation models, while lower-quality but more diverse data often yields stronger results(Edunov et al., 2018). In this paper, we propose a novel method to generate pseudo-parallel data from a pre-trained back-translation model. Our method is a meta-learning algorithm which adapts a pre-trained back-translation model so that the pseudoparallel data it generates would train a forward-translation model to do well on a validation set. In our evaluations in both the standard datasets WMT En-De'14 and WMT En-Fr'14, as well as a multilingual translation setting, our method leads to significant improvements over strong baselines. 1 How are you today? Wie geht es dir haute? Backward model Forward model (t-1) gradient samples Forward model (t) Ground Truth Parallel Data How are you? Wie geht es dir? Cross Entropy Loss update backward model",
        "id":231933756
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Which paper shows that human experts and non-experts focus on very different aspects when identifying AI=generated texts?",
    "positive_ctxs":[
      {
        "title":"All That's 'Human' Is Not Gold: Evaluating Human Evaluation of Generated Text",
        "text":"Human evaluations are typically considered the gold standard in natural language generation, but as models' fluency improves, how well can evaluators detect and judge machinegenerated text? We run a study assessing nonexperts' ability to distinguish between humanand machine-authored text (GPT2 and GPT3) in three domains (stories, news articles, and recipes). We find that, without training, evaluators distinguished between GPT3-and humanauthored text at random chance level. We explore three approaches for quickly training evaluators to better identify GPT3-authored text (detailed instructions, annotated examples, and paired examples) and find that while evaluators' accuracy improved up to 55%, it did not significantly improve across the three domains. Given the inconsistent results across text domains and the often contradictory reasons evaluators gave for their judgments, we examine the role untrained human evaluations play in NLG evaluation and provide recommendations to NLG researchers for improving human evaluations of text generated from state-of-the-art models.",
        "id":235694265
      }
    ],
    "negative_ctxs":[
      {
        "title":"WordRank: Learning Word Embeddings via Robust Ranking",
        "text":"Embedding words in a vector space has gained a lot of attention in recent years. While stateof-the-art methods provide efficient computation of word similarities via a low-dimensional matrix embedding, their motivation is often left unclear. In this paper, we argue that word embedding can be naturally viewed as a ranking problem due to the ranking nature of the evaluation metrics. Then, based on this insight, we propose a novel framework Wor-dRank that efficiently estimates word representations via robust ranking, in which the attention mechanism and robustness to noise are readily achieved via the DCG-like ranking losses. The performance of WordRank is measured in word similarity and word analogy benchmarks, and the results are compared to the state-of-the-art word embedding techniques. Our algorithm is very competitive to the state-of-the-arts on large corpora, while outperforms them by a significant margin when the training set is limited (i.e., sparse and noisy). With 17 million tokens, WordRank performs almost as well as existing methods using 7.2 billion tokens on a popular word similarity benchmark. Our multi-node distributed implementation of WordRank is publicly available for general usage.",
        "id":8506049
      },
      {
        "title":"TransAhead: A Computer-Assisted Translation and Writing Tool Keh-jiann Chen ++",
        "text":"We introduce a method for learning to predict text completion given a source text and partial translation. In our approach, predictions are offered aimed at alleviating users' burden on lexical and grammar choices, and improving productivity. The method involves learning syntax-based phraseology and translation equivalents. At run-time, the source and its translation prefix are sliced into ngrams to generate and rank completion candidates, which are then displayed to users. We present a prototype writing assistant, TransAhead, that applies the method to computer-assisted translation and language learning. The preliminary results show that the method has great potentials in CAT and CALL with significant improvement in translation quality across users.",
        "id":7240730
      },
      {
        "title":"Contextual Embeddings Can Distinguish Homonymy from Polysemy in a Human-Like Way",
        "text":"Lexical ambiguity is a pervasive feature of natural language, and a major difficulty in understanding language is selecting the intended meaning when more than one are possible. Despite this difficulty, many studies of single word recognition have found a processing advantage for ambiguous words compared to unambiguous ones. This effect is not homogeneous however-studies find consistent advantages for polysemes (words with multiple related meanings), and inconsistent results for homonyms (words with multiple unrelated meanings). Complicating this is the fact that most measures of ambiguity are derived from human-annotated or curated lexicographic resources, and their use is not consistent between studies. Our work investigates whether contextualized word embeddings are able to capture human-like distinctions between senses and meanings, and whether they can predict human behavior. We reanalyze data from previous experiments reporting ambiguity (dis)advantages using the lexical decision times reported in the English Lexicon Project. We find that our method does replicate the polyseme advantage and homonym disadvantage previously reported, and the predictors are superior to binary distinctions derived from lexicographic resources. Our findings point towards the benefits of using continuous-space representations of senses and meanings over more traditional measures. Additionally, we make our code publicly available for use in future research. . 2007. The english lexicon project. Behavior research methods, 39(3):445-459. Alan Beretta, Robert Fiorentino, and David Poeppel. 2005. The effects of homonymy and polysemy on lexical access: An MEG study. Cognitive Brain Research, 24(1):57-65. Ron Borowsky and Michael EJ Masson. 1996. Semantic ambiguity effects in word identification. Journal of Experimental Psychology: Learning, Memory, and Cognition, 22(1):63. Ricardo JGB Campello, Davoud Moulavi, and Jörg Sander. 2013. Density-based clustering based on hierarchical density estimates. In Pacific-Asia conference on knowledge discovery and data mining, pages 160-172. Springer.",
        "id":256739261
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"inline_nonacl",
    "question":"Which work introduces sparse attention modules and evaluate specifically on summarization?",
    "positive_ctxs":[
      {
        "title":"Efficient Attentions for Long Document Summarization",
        "text":"The quadratic computational and memory complexities of large Transformers have limited their scalability for long document summarization. In this paper, we propose HEPOS, a novel efficient encoder-decoder attention with head-wise positional strides to effectively pinpoint salient information from the source. We further conduct a systematic study of existing efficient self-attentions. Combined with HEPOS, we are able to process ten times more tokens than existing models that use full attentions. For evaluation, we present a new dataset, GOVREPORT, with significantly longer documents and summaries. Results show that our models produce significantly higher ROUGE scores than competitive comparisons, including new state-of-the-art results on PubMed. Human evaluation also shows that our models generate more informative summaries with fewer unfaithful errors.",
        "id":233033613
      }
    ],
    "negative_ctxs":[
      {
        "title":"RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder",
        "text":"Despite pre-training's progress in many important NLP tasks, it remains to explore effective pre-training strategies for dense retrieval. In this paper, we propose RetroMAE, a new retrieval oriented pre-training paradigm based on Masked Auto-Encoder (MAE). RetroMAE is highlighted by three critical designs. 1) A novel MAE workflow, where the input sentence is polluted for encoder and decoder with different masks. The sentence embedding is generated from the encoder's masked input; then, the original sentence is recovered based on the sentence embedding and the decoder's masked input via masked language modeling. 2) Asymmetric model structure, with a full-scale BERT like transformer as encoder, and a one-layer transformer as decoder.3) Asymmetric masking ratios, with a moderate ratio for encoder: 15∼30%, and an aggressive ratio for decoder: 50∼70%. Our framework is simple to realize and empirically competitive: the pre-trained models dramatically improve the SOTA performances on a wide range of dense retrieval benchmarks, like BEIR and MS MARCO. The source code and pre-trained models are made publicly available at https:\/\/github.com\/staoxiao\/RetroMAE so as to inspire more interesting research.",
        "id":252917569
      },
      {
        "title":"SAFE RLHF: SAFE REINFORCEMENT LEARNING FROM HUMAN FEEDBACK",
        "text":"With the development of large language models (LLMs), striking a balance between the performance and safety of AI systems has never been more critical.However, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training.To address this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe RLHF), a novel algorithm for human value alignment.Safe RLHF explicitly decouples human preferences regarding helpfulness and harmlessness, effectively avoiding the crowdworkers' confusion about the tension and allowing us to train separate reward and cost models.We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints.Leveraging the Lagrangian method to solve this constrained problem, Safe RLHF dynamically adjusts the balance between the two objectives during fine-tuning.Through a three-round fine-tuning using Safe RLHF, we demonstrate a superior ability to mitigate harmful responses while enhancing model performance compared to existing value-aligned algorithms.Experimentally, we finetuned the Alpaca-7B using Safe RLHF and aligned it with collected human preferences, significantly improving its helpfulness and harmlessness according to human evaluations.",
        "id":264306078
      },
      {
        "title":"Named Entity Recognition in Questions: Towards a Golden Collection",
        "text":"Named Entity Recognition (NER) plays a relevant role in several Natural Language Processing tasks. Question-Answering (QA) is an example of such, since answers are frequently named entities in agreement with the semantic category expected by a given question. In this context, the recognition of named entities is usually applied in free text data. NER in natural language questions can also aid QA and, thus, should not be disregarded. Nevertheless, it has not yet been given the necessary importance. In this paper, we approach the identification and classification of named entities in natural language questions. We hypothesize that NER results can benefit with the inclusion of previously labeled questions in the training corpus. We present a broad study addressing that hypothesis and focusing, among others, on the balance to be achieved between the amount of free text and questions in order to build a suitable training corpus. This work also contributes by providing a set of nearly 5,500 annotated questions with their named entities, freely available for research purposes.",
        "id":6290772
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Which work pushes the limit of model quantization in BERT models by introducing a ternary network?",
    "positive_ctxs":[
      {
        "title":"BinaryBERT: Pushing the Limit of BERT Quantization",
        "text":"The rapid development of large pre-trained language models has greatly increased the demand for model compression techniques, among which quantization is a popular solution. In this paper, we propose Binary-BERT, which pushes BERT quantization to the limit by weight binarization. We find that a binary BERT is hard to be trained directly than a ternary counterpart due to its complex and irregular loss landscape. Therefore, we propose ternary weight splitting, which initializes BinaryBERT by equivalently splitting from a half-sized ternary network. The binary model thus inherits the good performance of the ternary one, and can be further enhanced by fine-tuning the new architecture after splitting. Empirical results show that our Binary-BERT has only a slight performance drop compared with the full-precision model while being 24× smaller, achieving the state-of-the-art compression results on the GLUE and SQuAD benchmarks. (a) Full-precision Model. (b) Ternary Model. (c) Binary Model. (d) All Together.",
        "id":229923538
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":935515
      },
      {
        "title":"Lithium NLP: A System for Rich Information Extraction from Noisy User Generated Text on Social Media",
        "text":"In this paper, we describe the Lithium Natural Language Processing (NLP) system -a resource-constrained, highthroughput and language-agnostic system for information extraction from noisy user generated text on social media. Lithium NLP extracts a rich set of information including entities, topics, hashtags and sentiment from text. We discuss several real world applications of the system currently incorporated in Lithium products. We also compare our system with existing commercial and academic NLP systems in terms of performance, information extracted and languages supported. We show that Lithium NLP is at par with and in some cases, outperforms stateof-the-art commercial NLP systems. available at https:\/\/youtu.be\/U-o6Efh6TZc",
        "id":2900072
      },
      {
        "title":"Resolving Translation Ambiguity using Non-parallel Bilingual Corpora",
        "text":"This paper presents an unsupervised method for choosing the correct translation of a word in context. It learns disambiguation information from nonparallel bilinguM corpora (preferably in the same domain) free from tagging.Our method combines two existing unsupervised disambiguation algorithms: a word sense disambiguation algorithm based on distributional clustering and a translation disambiguation algorithm using target language corpora.For the given word in context, the former algorithm identifies its meaning as one of a number of predefined usage classes derived by clustering a large amount of usages in the source language corpus. The latter algorithm is responsible for associating each usage class (i.e., cluster) with a target word that is most relevant to the usage. This paper also shows preliminary results of translation experiments.",
        "id":497469
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Which work shows that only emplying instance-level uncertainty metrics could results in sample redundancy in active learning?",
    "positive_ctxs":[
      {
        "title":"ACTUNE: Uncertainty-Based Active Self-Training for Active Fine-Tuning of Pretrained Language Models",
        "text":"While pre-trained language model (PLM) finetuning has achieved strong performance in many NLP tasks, the fine-tuning stage can be still demanding in labeled data. Recent works have resorted to active fine-tuning to improve the label efficiency of PLM fine-tuning, but none of them investigate the potential of unlabeled data. We propose ACTUNE, a new framework that leverages unlabeled data to improve the label efficiency of active PLM fine-tuning. ACTUNE switches between data annotation and model self-training based on uncertainty: it selects high-uncertainty unlabeled samples for active annotation and lowuncertainty ones for model self-training. Under this framework, we design (1) a regionaware sampling strategy that reduces redundancy when actively querying for annotations and (2) a momentum-based memory bank that dynamically aggregates the model's pseudo labels to suppress label noise in self-training. Experiments on 6 text classification datasets show that ACTUNE outperforms the strongest active learning and self-training baselines and improves the label efficiency of PLM finetuning by 56.2% on average. Our implementation is available at https:\/\/github. com\/yueyu1030\/actune.",
        "id":250390500
      }
    ],
    "negative_ctxs":[
      {
        "title":"Identifying and Utilizing the Class of Monosemous Japanese Functional Expressions in Machine Translation",
        "text":"In the \"Sandglass\" machine translation architecture, we identify the class of monosemous Japanese functional expressions and utilize it in the task of translating Japanese functional expressions into English. We employ the semantic equivalence classes of a recently compiled large scale hierarchical lexicon of Japanese functional expressions. We then study whether functional expressions within a class can be translated into a single canonical English expression. Next, we introduce two types of ambiguities of functional expressions and identify monosemous functional expressions. In the evaluation of our translation rules for Japanese functional expressions, we directly apply those rules to monosemous functional expressions, and show that the proposed framework outperforms commercial machine translation software products. We further study how to extract rules for translating functional expressions in Japanese patent documents into English. In the result of this study, we show that translation rules manually developed based on the corpus for Japanese language grammar learners is reliable also in the patent domain.",
        "id":16997140
      },
      {
        "title":"The Semantics of a Definiendum Constrains both the Lexical Semantics and the Lexicosyntactic Patterns in the Definiens",
        "text":"Most current definitional question answering systems apply one-size-fits-",
        "id":18979309
      },
      {
        "title":"InterlinguaPlus Machine Translation Approach for Under-Resourced Languages: Ekegusii & Swahili",
        "text":"This paper elucidates the InterlinguaPlus design and its application in bi-directional text translations between Ekegusii and Kiswahili languages unlike the traditional translation pairs, one-by-one. Therefore, any of the languages can be the source or target language. The first section is an overview of the project, which is followed by a brief review of Machine Translation. The next section discusses the implementation of the system using Carabao's open machine translation framework and the results obtained. So far, the translation results have been plausible particularly for the resource-scarce local languages and clearly affirm morphological similarities inherent in Bantu languages.",
        "id":18766258
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Which work shows that reducing the number of training epochs effectively limits the impact of backdoor attack, but the method decreases the prediction accuracy?",
    "positive_ctxs":[
      {
        "title":"Concealed Data Poisoning Attacks on NLP Models",
        "text":"Adversarial attacks alter NLP model predictions by perturbing test-time inputs. However, it is much less understood whether, and how, predictions can be manipulated with small, concealed changes to the training data. In this work, we develop a new data poisoning attack that allows an adversary to control model predictions whenever a desired trigger phrase is present in the input. For instance, we insert 50 poison examples into a sentiment model's training set that causes the model to frequently predict Positive whenever the input contains \"James Bond\". Crucially, we craft these poison examples using a gradient-based procedure so that they do not mention the trigger phrase. We also apply our poison attack to language modeling (\"Apple iPhone\" triggers negative generations) and machine translation (\"iced coffee\" mistranslated as \"hot coffee\"). We conclude by proposing three defenses that can mitigate our attack at some cost in prediction accuracy or extra human annotation.",
        "id":233230124
      }
    ],
    "negative_ctxs":[
      {
        "title":"Accelerating NMT Batched Beam Decoding with LMBR Posteriors for Deployment",
        "text":"We describe a batched beam decoding algorithm for NMT with LMBR n-gram posteriors, showing that LMBR techniques still yield gains on top of the best recently reported results with Transformers. We also discuss acceleration strategies for deployment, and the effect of the beam size and batching on memory and speed.",
        "id":13742419
      },
      {
        "title":"A Constraint Programming Approach to Probabilistic Syntactic Processing",
        "text":"",
        "id":5066372
      },
      {
        "title":"Correcting Comma Errors in Learner Essays, and Restoring Commas in Newswire Text",
        "text":"While the field of grammatical error detection has progressed over the past few years, one area of particular difficulty for both native and non-native learners of English, comma placement, has been largely ignored. We present a system for comma error correction in English that achieves an average of 89% precision and 25% recall on two corpora of unedited student essays. This system also achieves state-of-theart performance in the sister task of restoring commas in well-formed text. For both tasks, we show that the use of novel features which encode long-distance information improves upon the more lexically-driven features used in prior work.",
        "id":7894256
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Which work suggests that machine translation models might get too confident and generate coherent but inadequant translations?",
    "positive_ctxs":[
      {
        "title":"Prevent the Language Model from being Overconfident in Neural Machine Translation",
        "text":"The Neural Machine Translation (NMT) model is essentially a joint language model conditioned on both the source sentence and partial translation. Therefore, the NMT model naturally involves the mechanism of the Language Model (LM) that predicts the next token only based on partial translation. Despite its success, NMT still suffers from the hallucination problem, generating fluent but inadequate translations. The main reason is that NMT pays excessive attention to the partial translation while neglecting the source sentence to some extent, namely overconfidence of the LM. Accordingly, we define the Margin between the NMT and the LM, calculated by subtracting the predicted probability of the LM from that of the NMT model for each token. The Margin is negatively correlated to the overconfidence degree of the LM. Based on the property, we propose a Margin-based Token-level Objective (MTO) and a Margin-based Sentencelevel Objective (MSO) to maximize the Margin for preventing the LM from being overconfident. Experiments on WMT14 Englishto-German, WMT19 Chinese-to-English, and WMT14 English-to-French translation tasks demonstrate the effectiveness of our approach, with 1.36, 1.50, and 0.63 BLEU improvements, respectively, compared to the Transformer baseline. The human evaluation further verifies that our approaches improve translation adequacy as well as fluency. 1",
        "id":235166394
      }
    ],
    "negative_ctxs":[
      {
        "title":"IAI @ SocialDisNER : Catch me if you can! Capturing complex disease mentions in tweets",
        "text":"Biomedical NER is an active research area today. Despite the availability of state-of-theart models for standard NER tasks, their performance degrades on biomedical data due to OOV entities and the challenges encountered in specialized domains. We use Flair-NER framework to investigate the effectiveness of various contextual and static embeddings for NER on Spanish tweets, in particular, to capture complex disease mentions.",
        "id":252819043
      },
      {
        "title":"Using Categorial Grammar to Label Translation Rules",
        "text":"Adding syntactic labels to synchronous context-free translation rules can improve performance, but labeling with phrase structure constituents, as in GHKM (Galley et al.,  2004), excludes potentially useful translation rules. SAMT (Zollmann and Venugopal,  2006)  introduces heuristics to create new non-constituent labels, but these heuristics introduce many complex labels and tend to add rarely-applicable rules to the translation grammar. We introduce a labeling scheme based on categorial grammar, which allows syntactic labeling of many rules with a minimal, well-motivated label set. We show that our labeling scheme performs comparably to SAMT on an Urdu-English translation task, yet the label set is an order of magnitude smaller, and translation is twice as fast.",
        "id":5170126
      },
      {
        "title":"Temiar Reduplication in One-Level Prosodic Morphology",
        "text":"Temiar reduplication is a difficult piece of prosodic morphology. This paper presents the first computational analysis of Temiar reduplication, using the novel finite-state approach of One-Level Prosodic Morphology originally developed byWalther (1999bWalther ( , 2000. After reviewing both the data and the basic tenets of One-level Prosodic Morphology, the analysis is laid out in some detail, using the notation of the FSA Utilities finite-state toolkit (van Noord 1997). One important discovery is that in this approach one can easily define a regular expression operator which ambiguously scans a string in the left-or rightward direction for a certain prosodic property. This yields an elegant account of base-length-dependent triggering of reduplication as found in Temiar.",
        "id":594
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"Which works shows that training large language models with purely mathematical and structural data can exhibit emergence of causal reasoning faster?",
    "positive_ctxs":[
      {
        "title":"Learning Multi-Step Reasoning by Solving Arithmetic Tasks",
        "text":"Mathematical reasoning is regarded as a necessary ability for Language Models (LMs). Recent works demonstrate large LMs' impressive performance in solving math problems. The success is attributed to their Chain-of-Thought (CoT) reasoning abilities, i.e., the ability to decompose complex questions into step-by-step reasoning chains, but such ability seems only to emerge from models with abundant parameters. This work investigates how to incorporate relatively small LMs with the capabilities of multi-step reasoning. We propose to inject such abilities by continually pre-training LMs on a synthetic dataset MSAT which is composed of Multi-step Arithmetic Tasks. Our experiments on four math word problem datasets show the effectiveness of the proposed method in enhancing LMs' math reasoning abilities. 1  , et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. . 2021. Mwptoolkit: An open-source framework for deep learning-based math word problem solvers. arXiv preprint arXiv:2109.00799.",
        "id":259089125
      }
    ],
    "negative_ctxs":[
      {
        "title":"ACTUNE: Uncertainty-Based Active Self-Training for Active Fine-Tuning of Pretrained Language Models",
        "text":"While pre-trained language model (PLM) finetuning has achieved strong performance in many NLP tasks, the fine-tuning stage can be still demanding in labeled data. Recent works have resorted to active fine-tuning to improve the label efficiency of PLM fine-tuning, but none of them investigate the potential of unlabeled data. We propose ACTUNE, a new framework that leverages unlabeled data to improve the label efficiency of active PLM fine-tuning. ACTUNE switches between data annotation and model self-training based on uncertainty: it selects high-uncertainty unlabeled samples for active annotation and lowuncertainty ones for model self-training. Under this framework, we design (1) a regionaware sampling strategy that reduces redundancy when actively querying for annotations and (2) a momentum-based memory bank that dynamically aggregates the model's pseudo labels to suppress label noise in self-training. Experiments on 6 text classification datasets show that ACTUNE outperforms the strongest active learning and self-training baselines and improves the label efficiency of PLM finetuning by 56.2% on average. Our implementation is available at https:\/\/github. com\/yueyu1030\/actune.",
        "id":250390500
      },
      {
        "title":"Subject and Object Dependency Extraction Using Finite-State Transducers",
        "text":"We describe and evaluate an approach for fast automatic recognition and extraction of subject and object dependency relations from large French corpora, using a sequence of finite-state transducers. The extraction is performed in two major steps: incremental finite-state parsing and extraction of subject\/verb and object\/verb relations. Our incremental and cautious approach during the first phase allows the system to deal successfully with complex phenomena such as embeddings, coordination of VPs and NPs or non-standard word order. The extraction requires no subcategorisation information. It relies on POS information only. After describing the two steps, we give the results of an evaluation on various types of unrestricted corpora. Precision is around 90-97% for subjects (84-88% for objects) and recall around 86-92% for subjects (80-90% for objects). We also provide some error analysis; in particular, we evaluate the impact of POS tagging errors on subject\/object dependency extraction.",
        "id":2711126
      },
      {
        "title":"",
        "text":"",
        "id":44252659
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"inline_nonacl",
    "question":"ould you direct me to research that shows that the transfer of specialized knowledge between various ABSA tasks if trained under the same paradigm?",
    "positive_ctxs":[
      {
        "title":"Aspect Sentiment Quad Prediction as Paraphrase Generation *",
        "text":"Aspect-based sentiment analysis (ABSA) has been extensively studied in recent years, which typically involves four fundamental sentiment elements, including the aspect category, aspect term, opinion term, and sentiment polarity. Existing studies usually consider the detection of partial sentiment elements, instead of predicting the four elements in one shot. In this work, we introduce the Aspect Sentiment Quad Prediction (ASQP) task, aiming to jointly detect all sentiment elements in quads for a given opinionated sentence, which can reveal a more comprehensive and complete aspect-level sentiment structure. We further propose a novel PARAPHRASE modeling paradigm to cast the ASQP task to a paraphrase generation process. On one hand, the generation formulation allows solving ASQP in an end-to-end manner, alleviating the potential error propagation in the pipeline solution. On the other hand, the semantics of the sentiment elements can be fully exploited by learning to generate them in the natural language form. Extensive experiments on benchmark datasets show the superiority of our proposed method and the capacity of crosstask transfer with the proposed unified PARA-PHRASE modeling framework.",
        "id":238259938
      }
    ],
    "negative_ctxs":[
      {
        "title":"Lexicon Enhanced Chinese Sequence Labelling Using BERT Adapter",
        "text":"Lexicon information and pre-trained models, such as BERT, have been combined to explore Chinese sequence labelling tasks due to their respective strength. However, existing methods solely fuse lexicon features via a shallow and random initialized sequence layer and do not integrate them into the bottom layers of BERT. In this paper, we propose Lexicon Enhanced BERT (LEBERT) for Chinese sequence labelling, which integrates external lexicon knowledge into BERT layers directly by a Lexicon Adapter layer. Compared with the existing methods, our model facilitates deep lexicon knowledge fusion at the lower layers of BERT. Experiments on ten Chinese datasets of three tasks including Named Entity Recognition, Word Segmentation, and Part-of-Speech Tagging, show that LEBERT achieves the stateof-the-art results.",
        "id":234741719
      },
      {
        "title":"",
        "text":"",
        "id":261341822
      },
      {
        "title":"22 ème Traitement Automatique des Langues Naturelles",
        "text":"Nous présentons des travaux préliminaires sur une approche permettant d'ajouter des termes bilingues à un système de Traduction Automatique Statistique (TAS) à base de segments. Les termes sont non seulement inclus individuellement, mais aussi avec des contextes les englobant. Tout d'abord nous générons ces contextes en généralisant des motifs (ou patrons) observés pour des mots de même nature syntaxique dans un corpus bilingue. Enfin, nous filtrons les contextes qui n'atteignent pas un certain seuil de confiance, à l'aide d'une méthode de sélection de bi-segments inspirée d'une approche de sélection de données, précédemment appliquée à des textes bilingues alignés.Abstract.Statistical machine translation adaptation through terminological enrichment based on virtual phrase generation and filteringWe propose a technique for adding bilingual terms to a phrase-based SMT system which includes not only individual words, but also induces phrasal contexts around these words. We first generate these contexts by generalizing patterns observed for similar words in a bilingual corpus, but then filter out those contexts that fall below a certain confidence threshold, based on an original phrase-pair selection process inspired by existing sentence selection techniques.Mots-clés : Traduction Automatique Statistique, Génération Automatique de Texte, contexte phrastique, terminologie bilingue.",
        "id":171887294
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Are there any examples of using dense phrase retrieval systems in the automatic curation of entity dictionaries?",
    "positive_ctxs":[
      {
        "title":"Automatic Creation of Named Entity Recognition Datasets by Querying Phrase Representations",
        "text":"Most weakly supervised named entity recognition (NER) models rely on domain-specific dictionaries provided by experts. This approach is infeasible in many domains where dictionaries do not exist. While a phrase retrieval model was used to construct pseudo-dictionaries with entities retrieved from Wikipedia automatically in a recent study, these dictionaries often have limited coverage because the retriever is likely to retrieve popular entities rather than rare ones. In this study, we present a novel framework, HighGEN, that generates NER datasets with high-coverage pseudo-dictionaries. Specifically, we create entity-rich dictionaries with a novel search method, called phrase embedding search, which encourages the retriever to search a space densely populated with various entities. In addition, we use a new verification process based on the embedding distance between candidate entity mentions and entity types to reduce the false-positive noise in weak labels generated by high-coverage dictionaries. We demonstrate that HighGEN outperforms the previous best model by an average F1 score of 4.7 across five NER benchmark datasets.",
        "id":252907578
      }
    ],
    "negative_ctxs":[
      {
        "title":"Meta Self-Refinement for Robust Learning with Weak Supervision",
        "text":"Training deep neural networks (DNNs) under weak supervision has attracted increasing research attention as it can significantly reduce the annotation cost. However, labels from weak supervision can be noisy, and the high capacity of DNNs enables them to easily overfit the label noise, resulting in poor generalization. Recent methods leverage self-training to build noiseresistant models, in which a teacher trained under weak supervision is used to provide highly confident labels for teaching the students. Nevertheless, the teacher derived from such frameworks may have fitted a substantial amount of noise and therefore produce incorrect pseudolabels with high confidence, leading to severe error propagation. In this work, we propose Meta Self-Refinement (MSR), a noise-resistant learning framework, to effectively combat label noise from weak supervision. Instead of relying on a fixed teacher trained with noisy labels, we encourage the teacher to refine its pseudolabels. At each training step, MSR performs a meta gradient descent on the current mini-batch to maximize the student performance on a clean validation set. Extensive experimentation on eight NLP benchmarks demonstrates that MSR is robust against label noise in all settings and outperforms state-of-the-art methods by up to 11.4% in accuracy and 9.26% in F1 score.",
        "id":248811149
      },
      {
        "title":"Duluth at SemEval-2016 Task 14 : Extending Gloss Overlaps to Enrich Semantic Taxonomies",
        "text":"This paper describes the Duluth systems that participated in Task 14 of SemEval 2016, Semantic Taxonomy Enrichment. There were three related systems in the formal evaluation which are discussed here, along with numerous post-evaluation runs. All of these systems identified synonyms between Word-Net and other dictionaries by measuring the gloss overlaps between them. These systems perform better than the random baseline and one post-evaluation variation was within a respectable margin of the median result attained by all participating systems.",
        "id":776002
      },
      {
        "title":"Lexicon and grammar in probabilistic tagging of written English",
        "text":"The paper describes the development of software for automatic grammatical ana]ysi$ of unl~'Ui~, unedited English text at the Unit for Compm= Research on the Ev~li~h Language (UCREL) at the Univet~ of Lancaster. The work is ~n'nmtly funded by IBM and carried out in collaboration with colleagues at IBM UK (W'~) and IBM Yorktown Heights. The paper will focus on the lexicon component of the word raging system, the UCREL grammar, the datal~zlks of parsed sentences, and the tools that have been written to support developmem of these comlm~ems. ~ wozk has applications to speech technology, sl~lfing conectim, end other areas of natural lmlguage pngessil~ ~y, our goal is to provide a language model using transin'ca statistics to di.~.nbigu~ al.:mative 1~ for a speech .:a~nicim device.",
        "id":14288879
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Are there any large-scale and open-source text simplification datasets dealing with long passages?",
    "positive_ctxs":[
      {
        "title":"SWIPE: A Dataset for Document-Level Simplification of Wikipedia Pages",
        "text":"Text simplification research has mostly focused on sentence-level simplification, even though many desirable edits-such as adding relevant background information or reordering contentmay require document-level context. Prior work has also predominantly framed simplification as a single-step, input-to-output task, only implicitly modeling the fine-grained, span-level edits that elucidate the simplification process. To address both gaps, we introduce the SWIPE dataset, which reconstructs the document-level editing process from English Wikipedia (EW) articles to paired Simple Wikipedia (SEW) articles. In contrast to prior work, SWIPE leverages the entire revision history when pairing pages in order to better identify simplification edits. We work with Wikipedia editors to annotate 5,000 EW-SEW document pairs, labeling more than 40,000 edits with proposed 19 categories. To scale our efforts, we propose several models to automatically label edits, achieving an F-1 score of up to 70.6, indicating that this is a tractable but challenging NLU task. Finally, we categorize the edits produced by several simplification models and find that SWIPE-trained models generate more complex edits while reducing unwanted edits.",
        "id":258967312
      }
    ],
    "negative_ctxs":[
      {
        "title":"MARMOT: A Toolkit for Translation Quality Estimation at the Word Level",
        "text":"We present Marmot -a new toolkit for quality estimation (QE) of machine translation output. Marmot contains utilities targeted at quality estimation at the word and phrase level. However, due to its flexibility and modularity, it can also be extended to work at the sentence level. In addition, it can be used as a framework for extracting features and learning models for many common natural language processing tasks. The tool has a set of state-of-the-art features for QE, and new features can easily be added. The tool is open-source and can be downloaded from https:\/\/github.com\/qe-team\/marmot\/.",
        "id":6680929
      },
      {
        "title":"Le théâtre français du XVIIe siècle : une expérience en catégorisation de textes",
        "text":"La catégorisation de documents (attribution d'un texte à une ou plusieurs catégories prédéfinies) possède de multiples applications.Cette communication se focalise sur l'attribution d'auteur en analysant le style de vingt pièces de théâtre du XVIIe siècle.L'hypothèse que nous souhaitons vérifier admet que le véritable auteur est le nom apparaissant sur la couverture.Afin de vérifier la qualité de deux méthodes d'attribution, nous avons repris deux corpus additionnels basés sur des romans écrits en français et italien.Nous proposons une amélioration de la méthode Delta ainsi qu'une nouvelle grille d'analyse pour cette approche.Ensuite, nous avons appliqué ces approches sur notre collection de comédies.Les résultats démontrent que l'hypothèse de base doit être écartée.De plus, ces oeuvres présentent des styles proches rendant toute attribution difficile.).",
        "id":264038816
      },
      {
        "title":"What Decisions Have You Made: Automatic Decision Detection in Conversational Speech",
        "text":"This study addresses the problem of automatically detecting decisions in conversational speech. We formulate the problem as classifying decision-making units at two levels of granularity: dialogue acts and topic segments. We conduct an empirical analysis to determine the characteristic features of decision-making dialogue acts, and train MaxEnt models using these features for the classification tasks. We find that models that combine lexical, prosodic, contextual and topical features yield the best results on both tasks, achieving 72% and 86% precision, respectively. The study also provides a quantitative analysis of the relative importance of the feature types.",
        "id":8916300
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Are there any papers that build dense retrievers with mixture-of-experts architecture where each expert is responsible for different types of queries?",
    "positive_ctxs":[
      {
        "title":"Chain-of-Skills: A Configurable Model for Open-Domain Question Answering",
        "text":"The retrieval model is an indispensable component for real-world knowledge-intensive tasks, e.g., open-domain question answering (ODQA). As separate retrieval skills are annotated for different datasets, recent work focuses on customized methods, limiting the model transferability and scalability. In this work, we propose a modular retriever where individual modules correspond to key skills that can be reused across datasets. Our approach supports flexible skill configurations based on the target domain to boost performance. To mitigate task interference, we design a novel modularization parameterization inspired by sparse Transformer. We demonstrate that our model can benefit from self-supervised pretraining on Wikipedia and fine-tuning using multiple ODQA datasets, both in a multi-task fashion. Our approach outperforms recent self-supervised retrievers in zero-shot evaluations and achieves state-ofthe-art fine-tuned retrieval performance on NQ, HotpotQA and OTT-QA.",
        "id":258546861
      }
    ],
    "negative_ctxs":[
      {
        "title":"Creating a PurposeNet Ontology: An insight into the issues encountered during ontology creation",
        "text":"PurposeNet is an ontology based on the principle that all artifacts (man-made objects) exist for a purpose and all its features and relations with other entities are giverened by its purpose. We provide instances of ontology creation for two varied domains from scratch in the Pur-poseNet architecture. These domains include MMTS domain and recipe domain. The methodology of creation was totally different for the two domains. MMTS domain was more computaionally oriented ontology while recipe domain required a post-processing after manually entering the data. The post-processing step uses hierarchical clustering to cluster very close actions. MMTS ontology is further used to create a simple template based QA system and the results are compared with a database system for the same domain.",
        "id":1198659
      },
      {
        "title":"From discourse structures to text summaries",
        "text":"We describe experiments that show that the concepts of rhetorical analysts and nucleanty can be used effectively for deternumng the most nnportant umts m a text We show how these concepts can be xmplemented and we discuss results that we obtained with a chscourse-based summanzatmn program",
        "id":11680756
      },
      {
        "title":"",
        "text":"",
        "id":207988685
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Could you recommend a dataset paper which presents relation extraction performance on translated data and compare it to English data?",
    "positive_ctxs":[
      {
        "title":"MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset",
        "text":"Relation extraction (RE) is a fundamental task in information extraction, whose extension to multilingual settings has been hindered by the lack of supervised resources comparable in size to large English datasets such as TACRED (Zhang et al., 2017). To address this gap, we introduce the MultiTACRED dataset, covering 12 typologically diverse languages from 9 language families, which is created by machine-translating TACRED instances and automatically projecting their entity annotations. We analyze translation and annotation projection quality, identify error categories, and experimentally evaluate fine-tuned pretrained mono-and multilingual language models in common transfer learning scenarios. Our analyses show that machine translation is a viable strategy to transfer RE instances, with native speakers judging more than 83% of the translated instances to be linguistically and semantically acceptable. We find monolingual RE model performance to be comparable to the English original for many of the target languages, and that multilingual models trained on a combination of English and target language data can outperform their monolingual counterparts. However, we also observe a variety of translation and annotation projection errors, both due to the MT systems and linguistic features of the target languages, such as pronoun-dropping, compounding and inflection, that degrade dataset quality and RE model performance.",
        "id":258557740
      }
    ],
    "negative_ctxs":[
      {
        "title":"Alternative Phrases and Natural Language Information Retrieval",
        "text":"This paper presents a formal analysis for a large class of words called alternative markers, which includes other(than), such(as), and besides. These words appear frequently enough in dialog to warrant serious attention, yet present natural language search engines perform poorly on queries containing them. I show that the performance of a search engine can be improved dramatically by incorporating an approximation of the formal analysis that is compatible with the search engine's operational semantics. The value of this approach is that as the operational semantics of natural language applications improve, even larger improvements are possible.",
        "id":10080250
      },
      {
        "title":"DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension",
        "text":"We propose DuoRC, a novel dataset for Reading Comprehension (RC) that motivates several new challenges for neural approaches in language understanding beyond those offered by existing RC datasets. DuoRC contains 186,089 unique questionanswer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie -one from Wikipedia and the other from IMDb -written by two different authors. We asked crowdsourced workers to create questions from one version of the plot and a different set of workers to extract or synthesize answers from the other version. This unique characteristic of DuoRC where questions and answers are created from different versions of a document narrating the same underlying story, ensures by design, that there is very little lexical overlap between the questions created from one version and the segments containing the answer in the other version. Further, since the two versions have different levels of plot detail, narration style, vocabulary, etc., answering questions from the second version requires deeper language understanding and incorporating external background knowledge. Additionally, the narrative style of passages arising from movie plots (as opposed to typical descriptive passages in existing datasets) exhibits the need to perform complex reasoning over events across multiple sentences. Indeed, we observe that state-of-the-art neural RC models which have achieved near human performance on the SQuAD dataset(Rajpurkar et al., 2016b), even when coupled with tra-ditional NLP techniques to address the challenges presented in DuoRC exhibit very poor performance (F1 score of 37.42% on DuoRC v\/s 86% on SQuAD dataset). This opens up several interesting research avenues wherein DuoRC could complement other RC datasets to explore novel neural approaches for studying language understanding.",
        "id":5071138
      },
      {
        "title":"The Sweet-Home speech and multimodal corpus for home automation interaction",
        "text":"Ambient Assisted Living aims at enhancing the quality of life of older and disabled people at home thanks to Smart Homes and Home Automation. However, many studies do not include tests in real settings, because data collection in this domain is very expensive and challenging and because of the few available data sets. The SWEET-HOME multimodal corpus is a dataset recorded in realistic conditions in DOMUS, a fully equipped Smart Home with microphones and home automation sensors, in which participants performed Activities of Daily living (ADL). This corpus is made of a multimodal subset, a French home automation speech subset recorded in Distant Speech conditions, and two interaction subsets, the first one being recorded by 16 persons without disabilities and the second one by 6 seniors and 5 visually impaired people. This corpus was used in studies related to ADL recognition, context aware interaction and distant speech recognition applied to home automation controled through voice.",
        "id":7179848
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Find the NLP paper that focuses on dialogue generation and introduces advancements in the augmentation of one-to-many or one-to-one dialogue data by conducting augmentation within the semantic space.",
    "positive_ctxs":[
      {
        "title":"DialoGPS: Dialogue Path Sampling in Continuous Semantic Space for Data Augmentation in Multi-Turn Conversations",
        "text":"In open-domain dialogue generation tasks, contexts and responses in most datasets are one-to-one mapped, violating an important many-to-many characteristic: a context leads to various responses, and a response answers multiple contexts. Without such patterns, models poorly generalize and prefer responding safely. Many attempts have been made in either multi-turn settings from a one-to-many perspective or in a many-to-many perspective but limited to single-turn settings. The major challenge to many-to-many augment multiturn dialogues is that discretely replacing each turn with semantic similarity breaks fragile context coherence. In this paper, we propose DialoGue Path Sampling (DialoGPS) method in continuous semantic space, the first manyto-many augmentation method for multi-turn dialogues. Specifically, we map a dialogue to our extended Brownian Bridge, a special Gaussian process. We sample latent variables to form coherent dialogue paths in the continuous space. A dialogue path corresponds to a new multi-turn dialogue and is used as augmented training data. We show the effect of DialoGPS with both automatic and human evaluation.",
        "id":259286910
      }
    ],
    "negative_ctxs":[
      {
        "title":"Generating Market Comments Referring to External Resources",
        "text":"Comments on a stock market often include the reason or cause of changes in stock prices, such as \"Nikkei turns lower as yen's rise hits exporters.\" Generating such informative sentences requires capturing the relationship between different resources, including a target stock price. In this paper, we propose a model for automatically generating such informative market comments that refer to external resources. We evaluated our model through an automatic metric in terms of BLEU and human evaluation done by an expert in finance. The results show that our model outperforms the existing model both in BLEU scores and human judgment.",
        "id":53249692
      },
      {
        "title":"Neural Machine Translation with Reordering Embeddings",
        "text":"The reordering model plays an important role in phrase-based statistical machine translation. However, there are few works that exploit the reordering information in neural machine translation. In this paper, we propose a reordering mechanism to learn the reordering embedding of a word based on its contextual information. These reordering embeddings are stacked together with self-attention networks to learn sentence representation for machine translation. The reordering mechanism can be easily integrated into both the encoder and the decoder in the Transformer translation system. Experimental results on WMT'14 English-to-German, NIST Chinese-to-English, and WAT ASPEC Japanese-to-English translation tasks demonstrate that the proposed methods can significantly improve the performance of the Transformer translation system. . 2017a. Improved neural machine translation with a syntax-aware encoder and decoder.",
        "id":196208300
      },
      {
        "title":"Reasoning about Quantities in Natural Language",
        "text":"Little work from the Natural Language Processing community has targeted the role of quantities in Natural Language Understanding. This paper takes some key steps towards facilitating reasoning about quantities expressed in natural language. We investigate two different tasks of numerical reasoning. First, we consider Quantity Entailment, a new task formulated to understand the role of quantities in general textual inference tasks. Second, we consider the problem of automatically understanding and solving elementary school math word problems. In order to address these quantitative reasoning problems we first develop a computational approach which we show to successfully recognize and normalize textual expressions of quantities. We then use these capabilities to further develop algorithms to assist reasoning in the context of the aforementioned tasks.",
        "id":17364624
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_acl",
    "question":"Give me a paper proposing to circumvent a single-truth target in training generative language models.",
    "positive_ctxs":[
      {
        "title":"Soft Alignment Objectives for Robust Adaptation of Language Generation",
        "text":"Domain adaptation allows generative language models to address specific flaws caused by the domain shift of their application. However, the traditional adaptation by further training on indomain data rapidly weakens the model's ability to generalize to other domains, making the openended deployments of the adapted models prone to errors. This work introduces novel training objectives built upon a semantic similarity of the predicted tokens to the reference.Our results show that (1) avoiding the common assumption of a single correct prediction by constructing the training target from tokens' semantic similarity can largely mitigate catastrophic forgetting of adaptation, while (2) preserving the adaptation in-domain quality, (3) with negligible additions to compute costs. In the broader context, the objectives grounded in a continuous token similarity pioneer the exploration of the middle ground between the efficient but naïve exact-match token-level objectives and expressive but computationally-and resourceintensive sequential objectives. . 2020. A mixed learning objective for neural machine translation. In . 2020. Exploring the limits of transfer learning with a unified text-to-text transformer.",
        "id":258947837
      }
    ],
    "negative_ctxs":[
      {
        "title":"Do Sentence Embeddings Capture Discourse Properties of Sentences from Scientific Abstracts ?",
        "text":"We introduce four tasks designed to determine which sentence encoders best capture discourse properties of sentences from scientific abstracts, namely coherence between clauses of a sentence, and discourse relations within sentences. We show that even if contextual encoders such as BERT or SciBERT encodes the coherence in discourse units, they do not help to predict three discourse relations commonly used in scientific abstracts. We discuss what these results underline, namely that these discourse relations are based on particular phrasing that allow non-contextual encoders to perform well.",
        "id":226283881
      },
      {
        "title":"Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces",
        "text":"In this paper we consider semantic spaces consisting of objects from some particular domain (e.g. IMDB movie reviews). Various authors have observed that such semantic spaces often model salient features (e.g. how scary a movie is) as directions. These feature directions allow us to rank objects according to how much they have the corresponding feature, and can thus play an important role in interpretable classifiers, recommendation systems, or entity-oriented search engines, among others. Methods for learning semantic spaces, however, are mostly aimed at modelling similarity. In this paper, we argue that there is an inherent trade-off between capturing similarity and faithfully modelling features as directions. Following this observation, we propose a simple method to fine-tune existing semantic spaces, with the aim of improving the quality of their feature directions. Crucially, our method is fully unsupervised, requiring only a bag-of-words representation of the objects as input.1 https:\/\/github.com\/ThomasAger\/ Modelling-Salient-Features-as-Directions-in-Fine-Tuned-Semantic-Spaces",
        "id":53103683
      },
      {
        "title":"",
        "text":"",
        "id":226283537
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"How to achieve zero-shot lip reading?",
    "positive_ctxs":[
      {
        "title":"OpenSR: Open-Modality Speech Recognition via Maintaining Multi-Modality Alignment",
        "text":"Speech Recognition builds a bridge between the multimedia streaming (audio-only, visualonly or audio-visual) and the corresponding text transcription. However, when training the specific model of new domain, it often gets stuck in the lack of new-domain utterances, especially the labeled visual utterances. To break through this restriction, we attempt to achieve zero-shot modality transfer by maintaining the multi-modality alignment in phoneme space learned with unlabeled multimedia utterances in the high resource domain during the pretraining(Shi et al., 2022), and propose a training system Open-modality Speech Recognition (OpenSR) that enables the models trained on a single modality (e.g., audio-only) applicable to more modalities (e.g., visual-only and audio-visual). Furthermore, we employ a cluster-based prompt tuning strategy to handle the domain shift for the scenarios with only common words in the new domain utterances. We demonstrate that OpenSR enables modality transfer from one to any in three different settings (zero-, few-and fullshot), and achieves highly competitive zeroshot performance compared to the existing fewshot and full-shot lip-reading methods. To the best of our knowledge, OpenSR achieves the state-of-the-art performance of word error rate in LRS2 on audio-visual speech recognition and lip-reading with 2.7% and 25.0%, respectively. The code and demo are available at https:\/\/github.com\/Exgc\/OpenSR.",
        "id":259137817
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":220061061
      },
      {
        "title":"Towards Ontology Engineering Based on Linguistic Analysis Introduction and Related Work",
        "text":"In this paper we describe OntoLT, a plug-in for the widely used Protégé ontology development tool that supports the interactive extraction and\/or extension of ontologies from text. The OntoLT approach aims at providing an environment for the integration of linguistic analysis in ontology development. OntoLT enables the definition of mapping rules with which concepts and attributes can be extracted automatically from linguistically annotated text collections. Mapping rules are defined by use of a constraint language. Constraints are implemented as XPATH expressions over the XML-based linguistic annotation. If all constraints are satisfied, the mapping rule activates one or more operators that describe in which way the ontology should be extended if a candidate is found.",
        "id":17129783
      },
      {
        "title":"A Data-centric Framework for Improving Domain-specific Machine Reading Comprehension Datasets",
        "text":"Low-quality data can cause downstream problems in high-stakes applications. Data-centric approach emphasizes on improving dataset quality to enhance model performance. Highquality datasets are needed for general-purpose Large Language Models (LLMs) training, as well as for domain-specific models, which are usually small in size as it is costly to engage a large number of domain experts for their creation. Thus, it is vital to ensure high-quality domain-specific training data. In this paper, we propose a framework for enhancing the data quality of original datasets 1 . We applied the proposed framework to four biomedical datasets and showed relative improvement of up to 33%\/40% for fine-tuning of retrieval\/reader models on the BioASQ dataset when using back translation to enhance the original dataset quality.",
        "id":257912882
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"How to better attract readers to news articles by generating personalized headlines?",
    "positive_ctxs":[
      {
        "title":"Generating User-Engaging News Headlines",
        "text":"The potential choices for news article headlines are enormous, and finding the right balance between conveying the essential message and capturing the reader's attention is key to effective headlining. However, presenting the same news headline to all readers is a suboptimal strategy, because it does not take into account the different preferences and interests of diverse readers, who may be confused about why a particular article has been recommended to them and do not see a clear connection between their interests and the recommended article. In this paper, we present a novel framework that addresses these challenges by incorporating user profiling to generate personalized headlines, and a combination of automated and human evaluation methods to determine user preference for personalized headlines. Our framework utilizes a learnable relevance function to assign personalized signature phrases to users based on their reading histories, which are then used to personalize headline generation. Through extensive evaluation, we demonstrate the effectiveness of our proposed framework in generating personalized headlines that meet the needs of a diverse audience. Our framework has the potential to improve the efficacy of news recommendations and facilitate creation of personalized content.",
        "id":259370694
      }
    ],
    "negative_ctxs":[
      {
        "title":"Contextualization of Morphological Inflection",
        "text":"Critical to natural language generation is the production of correctly inflected text. In this paper, we isolate the task of predicting a fully inflected sentence from its partially lemmatized version. Unlike traditional morphological inflection or surface realization, our task input does not provide \"gold\" tags that specify what morphological features to realize on each lemmatized word; rather, such features must be inferred from sentential context. We develop a neural hybrid graphical model that explicitly reconstructs morphological features before predicting the inflected forms, and compare this to a system that directly predicts the inflected forms without relying on any morphological annotation. We experiment on several typologically diverse languages from the Universal Dependencies treebanks, showing the utility of incorporating linguisticallymotivated latent variables into NLP models.1 This morphological feature is inherent in the sense ofBooij (1996).",
        "id":146120740
      },
      {
        "title":"Language choice models for microplanning and readability",
        "text":"This paper describes the construction of language choice models for the microplanning of discourse relations in a Natural Language Generation system that attempts to generate appropriate texts for users with varying levels of literacy. The models consist of constraint satisfaction problem graphs that have been derived from the results of a corpus analysis. The corpus that the models are based on was written for good readers. We adapted the models for poor readers by allowing certain constraints to be tightened, based on psycholinguistic evidence. We describe how the design of microplanner is evolving. We discuss the compromises involved in generating more readable textual output and implications of our design for NLG architectures. Finally we describe plans for future work.",
        "id":7637949
      },
      {
        "title":"Shallow Semantic Parsing for Spoken Language Understanding",
        "text":"Most Spoken Dialog Systems are based on speech grammars and frame\/slot semantics. The semantic descriptions of input utterances are usually defined ad-hoc with no ability to generalize beyond the target application domain or to learn from annotated corpora. The approach we propose in this paper exploits machine learning of frame semantics, borrowing its theoretical model from computational linguistics. While traditional automatic Semantic Role Labeling approaches on written texts may not perform as well on spoken dialogs, we show successful experiments on such porting. Hence, we design and evaluate automatic FrameNet-based parsers both for English written texts and for Italian dialog utterances. The results show that disfluencies of dialog data do not severely hurt performance. Also, a small set of FrameNet-like manual annotations is enough for realizing accurate Semantic Role Labeling on the target domains of typical Dialog Systems.",
        "id":6027270
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_acl",
    "question":"How to faithfully and explicitly measure the helpfulness of human explanations to language models during finetuning and inference?",
    "positive_ctxs":[
      {
        "title":"Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations",
        "text":"Human-annotated labels and explanations are critical for training explainable NLP models. However, unlike human-annotated labels whose quality is easier to calibrate (e.g., with a majority vote), human-crafted free-form explanations can be quite subjective. Before blindly using them as ground truth to train ML models, a vital question needs to be asked: How do we evaluate a human-annotated explanation's quality? In this paper, we build on the view that the quality of a human-annotated explanation can be measured based on its helpfulness (or impairment) to the ML models' performance for the desired NLP tasks for which the annotations were collected. In comparison to the commonly used Simulatability score, we define a new metric that can take into consideration of the helpfulness of an explanation for model performance at both fine-tuning and inference. With the help of a unified dataset format, we evaluated the proposed metric on five datasets (e.g., e-SNLI) against two model architectures (T5 and BART), and the results show that our proposed metric can objectively evaluate the quality of human-annotated explanations, while Simulatability falls short.",
        "id":258546862
      }
    ],
    "negative_ctxs":[
      {
        "title":"CIC-FBK Approach to Native Language Identification",
        "text":"We present the CIC-FBK system, which took part in the Native Language Identification (NLI) Shared Task 2017. Our approach combines features commonly used in previous NLI research, i.e., word n-grams, lemma n-grams, part-of-speech n-grams, and function words, with recently introduced character n-grams from misspelled words, and features that are novel in this task, such as typed character n-grams, and syntactic n-grams of words and of syntactic relation tags. We use log-entropy weighting scheme and perform classification using the Support Vector Machines (SVM) algorithm. Our system achieved 0.8808 macro-averaged F1-score and shared the 1 st rank in the NLI Shared Task 2017 scoring.",
        "id":1615409
      },
      {
        "title":"The Norwegian Dependency Treebank",
        "text":"The Norwegian Dependency Treebank is a new syntactic treebank for Norwegian Bokmål and Nynorsk with manual syntactic and morphological annotation, developed at the National Library of Norway in collaboration with the University of Oslo. It is the first publically available treebank for Norwegian. This paper presents the core principles behind the syntactic annotation and how these principles were employed in certain specific cases. We then present the selection of texts and distribution between genres, as well as the annotation process and an evaluation of the inter-annotator agreement. Finally, we present the first results of data-driven dependency parsing of Norwegian, contrasting four state-of-the-art dependency parsers trained on the treebank. The consistency and the parsability of this treebank is shown to be comparable to other large treebank initiatives.",
        "id":7770967
      },
      {
        "title":"SINotas: the Evaluation of a NLG Application",
        "text":"SINotas is a data-to-text NLG application intended to produce short textual reports on students' academic performance from a database conveying their grades, weekly attendance rates and related academic information. Although developed primarily as a testbed for Portuguese Natural Language Generation, SINotas generates reports of interest to both students keen to learn how their professors would describe their efforts, and to the professors themselves, who may benefit from an at-a-glance view of the student's performance. In a traditional machine learning approach, SINotas uses a data-text aligned corpus as training data for decision-tree induction. The current system comprises a series of classifiers that implement major Document Planning subtasks (namely, data interpretation, content selection, within-and between-sentence structuring), and a small surface realisation grammar of Brazilian Portuguese. In this paper we focus on the evaluation work of the system, applying a number of intrinsic and user-based evaluation metrics to a collection of text reports generated from real application data.",
        "id":1016733
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"If one would like to train (or evaluate) a helpful assistant agent that can converse with humans while the humans traverse an environment, which work has the most suitable resource?",
    "positive_ctxs":[
      {
        "title":"SIMMC-VR: A Task-oriented Multimodal Dialog Dataset with Situated and Immersive VR Streams",
        "text":"Building an AI assistant that can seamlessly converse and instruct humans, in a user-centric situated scenario, requires several essential abilities: (1) spatial and temporal understanding of the situated and real-time user scenes, (2) capability of grounding the actively perceived visuals of users to conversation contexts, and (3) conversational reasoning over past utterances to perform just-in-time assistance. However, we currently lack a large-scale benchmark that captures user↔assistant interactions with all of the aforementioned features. To this end, we propose SIMMC-VR, an extension of the SIMMC 2.0 dataset to a video-grounded taskoriented dialog dataset that captures real-world AI-assisted user scenarios in VR. We propose a novel data collection paradigm that involves (1) generating object-centric multimodal dialog flows with egocentric visual streams and visually-grounded templates, and (2) manually paraphrasing the simulated dialogs for naturalness and diversity while preserving multimodal dependencies. To measure meaningful progress in the field, we propose four tasks to address the new challenges in SIMMC-VR, which require complex spatial-temporal dialog reasoning in active egocentric scenes. We benchmark the proposed tasks with strong multimodal models, and highlight the key capabilities that current models lack for future research directions. * Work done during an internship at Meta Reality Labs.",
        "id":259370710
      }
    ],
    "negative_ctxs":[
      {
        "title":"Maximum Entropy Model Learning of Subcategorization Preference* I t",
        "text":"This paper proposes a novel method for learning probabilistic models of subcategorization preference of verbs. Especially, we propose to consider the issues of case dependencie~ and noun class generalization in a uniform way. We adopt the maximum entropy model learn~,g method and apply it to the task of model learning of subcategorization preference. Case dependencies and noun class generalization are represented as featura~ in the maximum entropy approach. The feature selection facility of the maximum entropy model learning makes it possible to find optimal case dependencies and optimal noun c!~ generalization levels. We describe the results of the experiment on learning probabilistic models of subcategorization preference f~om the EDR Japanese bracketed corpus. We also evaluated the performance of the selected features and their estimated parameters in the subcategorization preference task.",
        "id":14732351
      },
      {
        "title":"Returning-Home Analysis in Tokyo Metropolitan Area at the time of the Great East Japan Earthquake using Twitter Data",
        "text":"This paper clarifies the occurrence factors of commuters unable to return home and the returning-home decision-making at the time of the Great East Japan Earthquake by using Twitter data. First, to extract the behavior data from the tweet data, we identify each user's returning-home behavior using support vector machines. Second, we create non-verbal explanatory factors using geotag data and verbal explanatory factors using tweet data. Then, we model users' returning-home decisionmaking by using a discrete choice model and clarify the factors quantitatively. Finally, by sensitivity analysis, we show the effects of the existence of emergency evacuation facilities and line of communication.",
        "id":7821398
      },
      {
        "title":"Building Dialectal Arabic Corpora",
        "text":"The aim of this research is to identify local Arabic dialects in texts from social media (Twitter) and link them to specific geographic areas. Dialect identification is studied as a subset of the task of language identification. The proposed method is based on unsupervised learning using simultaneously lexical and geographic distance. While this study focusses on Libyan dialects, the approach is general, and could produce resources to support human translators and interpreters when dealing with vernaculars rather than standard Arabic.",
        "id":7303263
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"In multimodal (multilingual) abstractive summarization field, is there any paper that propose target-oriented vision modeling method to improve the quality of summaries?",
    "positive_ctxs":[
      {
        "title":"Summary-Oriented Vision Modeling for Multimodal Abstractive Summarization",
        "text":"Multimodal abstractive summarization (MAS) aims to produce a concise summary given the multimodal data (text and vision). Existing studies mainly focus on how to effectively use the visual features from the perspective of an article, having achieved impressive success on the high-resource English dataset. However, less attention has been paid to the visual features from the perspective of the summary, which may limit the model performance, especially in the low-and zero-resource scenarios. In this paper, we propose to improve the summary quality through summary-oriented visual features. To this end, we devise two auxiliary tasks including vision to summary task and masked image modeling task. Together with the main summarization task, we optimize the MAS model via the training objectives of all these tasks. By these means, the MAS model can be enhanced by capturing the summaryoriented visual features, thereby yielding more accurate summaries. Experiments on 44 languages, covering mid-high-, low-, and zeroresource scenarios, verify the effectiveness and superiority of the proposed approach, which achieves state-of-the-art performance under all scenarios. Additionally, we will contribute a large-scale multilingual multimodal abstractive summarization (MM-Sum) dataset. 1",
        "id":254685691
      }
    ],
    "negative_ctxs":[
      {
        "title":"Intensional Summaries as Cooperative Responses in Dialogue: Automation and Evaluation",
        "text":"Despite its long history, and a great deal of research producing many useful algorithms and observations, research in cooperative response generation has had little impact on the recent commercialization of dialogue technologies, particularly within the spoken dialogue community. We hypothesize that a particular type of cooperative response, intensional summaries, are effective for when users are unfamiliar with the domain. We evaluate this hypothesis with two experiments with cruiser, a DS for in-car or mobile users to access restaurant information. First, we compare cruiser with a baseline system-initiative DS, and show that users prefer cruiser. Then, we experiment with four algorithms for constructing intensional summaries in cruiser, and show that two summary types are equally effective: summaries that maximize domain coverage and summaries that maximize utility with respect to a user model.",
        "id":5555658
      },
      {
        "title":"0.0 INTRODUCTION A CASE FOR RULE-DRIVEN SEMANTIC PROCESSING",
        "text":"",
        "id":6168320
      },
      {
        "title":"Generating \"A for Alpha\" When There Are Thousands of Characters",
        "text":"The phonetic alphabet enables people to dictate letters of the alphabet accurately by using representative words, i.e., A for Alpha. Japanese kanji (idiographic Chinese characters) vastly outnumber the letters of the Roman alphabet, and thus Japanese requires an explanatory reading like a phonetic alphabet. We call the explanatory reading of a kanji a \"distinctive explanation.\" Most kanji characters have their homophones, and the role of the distinctive explanations is to enable users to identify a specific kanji character only by listening to the explanation. In this paper, we propose a corpus-based method for automatically generating distinctive explanations for a kanji, in which information about familiarity and homophones of kanji are taken into consideration. Through the kanji-identification experiments, we show that the quality of the explanations generated by the proposed method is higher than that of the manually crafted distinctive explanations.",
        "id":17661090
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there a Chinese hate speech paper that constructs an insulting lexicon while building the dataset?",
    "positive_ctxs":[
      {
        "title":"Facilitating Fine-grained Detection of Chinese Toxic Language: Hierarchical Taxonomy, Resources, and Benchmarks",
        "text":"Disclaimer:The samples presented by this paper may be considered offensive or vulgar.",
        "id":258557119
      }
    ],
    "negative_ctxs":[
      {
        "title":"Revisiting the Practical Effectiveness of Constituency Parse Extraction from Pre-trained Language Models",
        "text":"Constituency Parse Extraction from Pre-trained Language Models (CPE-PLM) is a recent paradigm that attempts to induce constituency parse trees relying only on the internal knowledge of pre-trained language models. While attractive in the perspective that similar to in-context learning, it does not require taskspecific fine-tuning, the practical effectiveness of such an approach still remains unclear, except that it can function as a probe for investigating language models' inner workings. In this work, we mathematically reformulate CPE-PLM and propose two advanced ensemble methods tailored for it, demonstrating that the new parsing paradigm can be competitive with common unsupervised parsers by introducing a set of heterogeneous PLMs combined using our techniques. Furthermore, we explore some scenarios where the trees generated by CPE-PLM are practically useful. Specifically, we show that CPE-PLM is more effective than typical supervised parsers in few-shot settings.",
        "id":252819507
      },
      {
        "title":"Modeling Speech Acts in Asynchronous Conversations: A Neural-CRF Approach",
        "text":"Participants in an asynchronous conversation (e.g., forum, e-mail) interact with each other at different times, performing certain communicative acts, called speech acts (e.g., question, request). In this article, we propose a hybrid approach to speech act recognition in asynchronous conversations. Our approach works in two main steps: a long short-term memory recurrent neural network (LSTM-RNN) first encodes each sentence separately into a task-specific distributed representation, and this is then used in a conditional random field (CRF) model to capture the conversational dependencies between sentences. The LSTM-RNN model uses pretrained word embeddings learned from a large conversational corpus and is trained to classify sentences into speech act types. The CRF model can consider arbitrary graph structures to model conversational dependencies in an asynchronous conversation. In addition, to mitigate the problem of limited annotated data in the asynchronous domains, we adapt the LSTM-RNN model to learn from synchronous conversations (e.g., meetings), using domain adversarial training of neural networks. Empirical evaluation shows the effectiveness of our approach over existing ones: (i) LSTM-RNNs provide better task-specific representations, (ii) conversational word embeddings benefit the LSTM-RNNs more than the off-the-shelf ones, (iii) adversarial training gives better domain-invariant representations, and (iv) the global CRF model improves over local models.",
        "id":52289309
      },
      {
        "title":"A tool for detecting French-English cognates and false friends",
        "text":"Les congénères sont des mots qui ont au moins un sens en commun entre deux langues en plus d'avoir une orthographie semblable. La reconnaissance de ce type de mots permet aux apprenants de langue seconde ou étrangère d'enrichir plus rapidement leur vocabulaire et d'améliorer leur compréhension écrite. Toutefois, les faux amis sont des paires de mots qui à l'écrit ont des similarités, mais ils ont des significations différentes. Pour leur part, les congénères partiels sont des mots qui ont la même signification dans certains contextes dans chacune des deux langues. Cet article présente une méthode pour la classification automatique des paires des mots classées en congénères ou faux amis, en utilisant des mesures de similarité orthographiques et des méthodes d'apprentissage automatique. Ainsi, nous construisons des listes complètes des congénères et des faux amis entre les deux langues. Nous désambiguisons les congénères partiels dans des contextes spécifiques. Nos méthodes sont évaluées pour le français et l'anglais, mais elles seraient applicables à d'autres paires des langues. Nous avons construit un outil qui prend ces listes et marque dans un texte français les mots qui ont des congénères ou des faux amis en anglais, dans le but d'aider les apprenants en français langue seconde ou étrangère à améliorer leur compréhension écrite et à développer une meilleure rétention.Abstract. Cognates are pairs of words in different languages similar in spelling and meaning. They can help a second-language learner on the tasks of vocabulary expansion and reading comprehension. False friends are pairs of words that have similar spelling but different meanings. Partial cognates are pairs of words in two languages that have the same meaning in some, but not all contexts. In this article we present a method to automatically classify a pair of words as cognates or false friends, by using several measures of orthographic similarity as features for classification. We use this method to create complete lists of cognates and false friends between two languages. We also disambiguate partial cognates in context. We applied all our methods to French and English, but they can be applied to other pairs of languages as well. We built a tool that takes the produced lists and annotates a French text with equivalent English cognates or false friends, in order to help second-language learners improve their reading comprehension skills and retention rate.Mots-clés : congénères, faux amis, congénères partiels, mesures de similarité orthographiques, apprentissage automatique, apprentissage des langues assisté par ordinateur.",
        "id":18611450
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_acl",
    "question":"Is there a dataset that allows to perform aspect-based sentiment classification on French news?",
    "positive_ctxs":[
      {
        "title":"MAD-TSC: A Multilingual Aligned News Dataset for Target-dependent Sentiment Classification",
        "text":"Target-dependent sentiment classification (TSC) enables a fine-grained automatic analysis of sentiments expressed in texts. Sentiment expression varies depending on the domain, and it is necessary to create domain-specific datasets.While socially important, TSC in the news domain remains relatively understudied. We introduce MAD-TSC, the first multilingual aligned dataset designed for TSC in news. MAD-TSC differs substantially from existing resources. First, it includes aligned examples in eight languages to facilitate a comparison of performance for individual languages, and a direct comparison of human and machine translation. Second, the dataset is sampled from a diversified parallel news corpus, and is diversified in terms of news sources and geographic spread of entities. Finally, MAD-TSC is more challenging than existing datasets because its samples are more complex. We exemplify the use of MAD-TSC with comprehensive monolingual and multilingual experiments. The latter shows that machine translations can successfully replace manual ones, and that performance for all included languages can match that of English by automatically translating test examples.",
        "id":259370666
      }
    ],
    "negative_ctxs":[
      {
        "title":"NLP-CIC-WFU at SocialDisNER: Disease Mention Extraction in Spanish Tweets Using Transfer Learning and Search by Propagation",
        "text":"Named entity recognition (e.g., disease mention extraction) is one of the most relevant tasks for data mining in the medical field. Although it is a well-known challenge, the bulk of the efforts to tackle this task have been made using clinical texts commonly written in English. In this work, we present our contribution to the SocialDisNER competition, which consists of a transfer learning approach to extracting disease mentions in a corpus from Twitter written in Spanish. We fine-tuned a model based on mBERT and applied post-processing using regular expressions to propagate the entities identified by the model and enhance disease mention extraction. Our system achieved a competitive strict F1 of 0.851 on the testing data set.",
        "id":252819441
      },
      {
        "title":"A Unified Syntax-aware Framework for Semantic Role Labeling",
        "text":"Semantic role labeling (SRL) aims to recognize the predicate-argument structure of a sentence. Syntactic information has been paid a great attention over the role of enhancing SRL. However, the latest advance shows that syntax would not be so important for SRL with the emerging much smaller gap between syntax-aware and syntax-agnostic SRL. To comprehensively explore the role of syntax for SRL task, we extend existing models and propose a unified framework to investigate more effective and more diverse ways of incorporating syntax into sequential neural networks. Exploring the effect of syntactic input quality on SRL performance, we confirm that high-quality syntactic parse could still effectively enhance syntactically-driven SRL. Using empirically optimized integration strategy, we even enlarge the gap between syntax-aware and syntax-agnostic SRL. Our framework achieves state-of-the-art results on CoNLL-2009 benchmarks both for English and Chinese, substantially outperforming all previous models.",
        "id":53081355
      },
      {
        "title":"Analyse morphologique non supervisée en domaine biomédical Application à la recherche d'information",
        "text":"Dans le domaine biomédical, utiliser des termes spécialisés est essentiel pour accéder à l'information. Cependant, dans beaucoup de langues, ces termes sont des constructions morphologiques complexes qui compliquent cet accès à l'information. Dans cet article, nous nous intéressons à l'identification des composants morphologiques de ces termes et à leur utilisation pour une tâche de recherche d'information (RI). Nous proposons différentes approches reposant sur un alignement automatique avec une langue pivot particulière, le japonais, et sur un apprentissage par analogie permettant de produire des analyses morphologiques fines des termes d'une langue donnée. Ces analyses morphologiques sont ensuite utilisées pour améliorer l'indexation de documents biomédicaux. Les expériences rapportées montrent la validité de cette approche avec des gains en MAP de plus de 10 % par rapport à un système de RI standard.ABSTRACT. In the biomedical field, using of specialized terms is key to access information. However, in most Indo-European languages, these terms are complex morphological structures. The presented work aims at identifying the various meaningful components of these terms and use them to improve biomedical Information Retrieval (IR). We present different approaches combining automatic alignments with a pivot language, Japanese, and analogical learning that allows an accurate morphological analysis of terms. These morphological analysis are used to improve the indexing of medical documents. The experiments reported in this paper show the validity of this approach with a 10% MAP improvement over a standard IR system. MOTS-CLÉS : morphologie, terminologie biomédicale, alignement, apprentissage par analogie, indexation morphosémantique, recherche d'information biomédicale.",
        "id":260165148
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there a decoder-only language model that does not use a tokenizer and operates on raw text bytes?",
    "positive_ctxs":[
      {
        "title":"ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models",
        "text":"State-of-the-art poetry generation systems are often complex. They either consist of taskspecific model pipelines, incorporate prior knowledge in the form of manually created constraints, or both. In contrast, end-to-end models would not suffer from the overhead of having to model prior knowledge and could learn the nuances of poetry from data alone, reducing the degree of human supervision required. In this work, we investigate end-to-end poetry generation conditioned on styles such as rhyme, meter, and alliteration. We identify and address lack of training data and mismatching tokenization algorithms as possible limitations of past attempts. In particular, we successfully pre-train ByGPT5, a new token-free decoder-only language model, and fine-tune it on a large custom corpus of English and German quatrains annotated with our styles. We show that ByGPT5 outperforms other models such as mT5, ByT5, GPT-2 and ChatGPT, while also being more parameter efficient and performing favorably compared to humans. In addition, we analyze its runtime performance and demonstrate that it is not prone to memorization. We make our code, models, and datasets publicly available.1",
        "id":254877406
      }
    ],
    "negative_ctxs":[
      {
        "title":"Learning Translations from Monolingual Corpora",
        "text":"This paper proposes a method for a machine translation (MT) system to automatically select and learn translation words, which suit the user's tastes or document fields by using a monolingual corpus manually compiled by the user, in order to achieve high-quality translation. We have constructed a system based on this method and carried out experiments to prove the validity of the proposed method. This learning system has been implemented in Toshiba's \"The Honyaku\" series.",
        "id":26299595
      },
      {
        "title":"ON THE TRADE-OFF BETWEEN ACTIONABLE EXPLANATIONS AND THE RIGHT TO BE FORGOTTEN",
        "text":"As machine learning (ML) models are increasingly being deployed in high-stakes applications, policymakers have suggested tighter data protection regulations (e.g., GDPR, CCPA).One key principle is the \"right to be forgotten\" which gives users the right to have their data deleted.Another key principle is the right to an actionable explanation, also known as algorithmic recourse, allowing users to reverse unfavorable decisions.To date, it is unknown whether these two principles can be operationalized simultaneously.Therefore, we introduce and study the problem of recourse invalidation in the context of data deletion requests.More specifically, we theoretically and empirically analyze the behavior of popular state-of-the-art algorithms and demonstrate that the recourses generated by these algorithms are likely to be invalidated if a small number of data deletion requests (e.g., 1 or 2) warrant updates of the predictive model.For the setting of differentiable models, we suggest a framework to identify a minimal subset of critical training points which, when removed, maximize the fraction of invalidated recourses.Using our framework, we empirically show that the removal of as little as 2 data instances from the training set can invalidate up to 95 percent of all recourses output by popular state-of-the-art algorithms.Thus, our work raises fundamental questions about the compatibility of \"the right to an actionable explanation\" in the context of the \"right to be forgotten\", while also providing constructive insights on the determining factors of recourse robustness.",
        "id":251929045
      },
      {
        "title":"CycleKQR: Unsupervised Bidirectional Keyword-Question Rewriting",
        "text":"Users expect their queries to be answered by search systems, regardless of the query's surface form, which include keyword queries and natural questions. Natural Language Understanding (NLU) components of Search and QA systems may fail to correctly interpret semantically equivalent inputs if this deviates from how the system was trained, leading to suboptimal understanding capabilities. We propose the keyword-question rewriting task to improve query understanding capabilities of NLU systems for all surface forms. To achieve this, we present CycleKQR, an unsupervised approach, enabling effective rewriting between keyword and question queries using non-parallel data.Empirically we show the impact on QA performance of unfamiliar query forms for open domain and Knowledge Base QA systems (trained on either keywords or natural language questions). We demonstrate how CycleKQR significantly improves QA performance by rewriting queries into the appropriate form, while at the same time retaining the original semantic meaning of input queries, allowing CycleKQR to improve performance by up to 3% over supervised baselines. Finally, we release a dataset of 66k keyword-question pairs. 1",
        "id":256461370
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there a dialogue dataset where a speaker's utterance is grounded in their persona, consisting of image-text pairs representing their episodic memories?",
    "positive_ctxs":[
      {
        "title":"MPCHAT: Towards Multimodal Persona-Grounded Conversation",
        "text":"In order to build self-consistent personalized dialogue agents, previous research has mostly focused on textual persona that delivers personal facts or personalities. However, to fully describe the multi-faceted nature of persona, image modality can help better reveal the speaker's personal characteristics and experiences in episodic memory (Rubin et al.,  2003; Conway, 2009). In this work, we extend persona-based dialogue to the multimodal domain and make two main contributions. First, we present the first multimodal persona-based dialogue dataset named MPCHAT, which extends persona with both text and images to contain episodic memories. Second, we empirically show that incorporating multimodal persona, as measured by three proposed multimodal persona-grounded dialogue tasks (i.e., next response prediction, grounding persona prediction, and speaker identification), leads to statistically significant performance improvements across all tasks. Thus, our work highlights that multimodal persona is crucial for improving multimodal dialogue comprehension, and our MPCHAT serves as a high-quality resource for this research. . 2022.Fine-grained image captioning with CLIP reward. In NAACL Findings.J Clement. 2022. Regional distribution of desktop traffic to reddit.com as of february 2022 by country,. 3363 Martin A. Conway. 2005. Memory and the self. J. Mem. Lang., 53(4):594-628. Martin A. Conway. 2009. Episodic memories. Neuropsychologia, 47(11):2305-2313. Michael A. Covington and Joe D. McFall. 2010. Cutting the gordian knot: The moving-average type-token ratio (mattr). J. Quant. Linguist., 17(2):94-100.",
        "id":258959116
      }
    ],
    "negative_ctxs":[
      {
        "title":"LOGAN: Local Group Bias Detection by Clustering",
        "text":"Machine learning techniques have been widely used in natural language processing (NLP). However, as revealed by many recent studies, machine learning models often inherit and amplify the societal biases in data. Various metrics have been proposed to quantify biases in model predictions. In particular, several of them evaluate disparity in model performance between protected groups and advantaged groups in the test corpus. However, we argue that evaluating bias at the corpus level is not enough for understanding how biases are embedded in a model. In fact, a model with similar aggregated performance between different groups on the entire data may behave differently on instances in a local region. To analyze and detect such local bias, we propose LOGAN, a new bias detection technique based on clustering. Experiments on toxicity classification and object classification tasks show that LOGAN identifies bias in a local region and allows us to better analyze the biases in model predictions.",
        "id":222140831
      },
      {
        "title":"Semantic Parsing of Tamil Sentences",
        "text":"In this paper, we propose a rule-based approach for the identificat ion of semantic sub -graphs fro m Tamil sentences. In order to achieve the goal of semantic sub -graph identification and construction, we use a semantic graph based representation called Un iv ersal Net working Language (UNL), wh ich is a directed acyclic graph representation. To identify and build the semantic sub-graphs, we classify the ru les based on morpho-semantic features wh ich include word associated features and context based features . The rules are performed in t wo stages; one while build ing the simp le UNL graphs and one after the simple UNL graphs construction. We have identified 18 rules for sub-graph identification and construction.",
        "id":14380054
      },
      {
        "title":"Maximum Entropy Model Learning of the Translation Rules",
        "text":"This paper proposes a learning method of translation rules from parallel corpora. This method applies the maximum entropy principle to a probabilistic model of translation rules. First, we define feature functions which express statistical properties of this model. Next, in order to optimize the model, the system iterates following steps: (1) selects a feature function which maximizes loglikelihood, and (2) adds this function to the model incrementally. As computational cost associated with this model is too expensive, we propose several methods to suppress the overhead in order to realize the system. The result shows that it attained 69.54% recall rate.",
        "id":2522271
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there a method for measuring the critical errors that a dialogue system makes in its responses?",
    "positive_ctxs":[
      {
        "title":"Chat-Oriented Dialogue Systems",
        "text":"Despite tremendous advancements in dialogue systems, stable evaluation still requires human judgments producing notoriously high-variance metrics due to their inherent subjectivity. Moreover, methods and labels in dialogue evaluation are not fully standardized, especially for opendomain chats, with a lack of work to compare and assess the validity of those approaches. The use of inconsistent evaluation can misinform the performance of a dialogue system, which becomes a major hurdle to enhance it. Thus, a dimensional evaluation of chat-oriented opendomain dialogue systems that reliably measures several aspects of dialogue capabilities is desired. This paper presents a novel human evaluation method to estimate the rates of many dialogue system behaviors. Our method is used to evaluate four state-of-the-art open-domain dialogue systems and compared with existing approaches. The analysis demonstrates that our behavior method is more suitable than alternative Likert-style or comparative approaches for dimensional evaluation of these systems.A detailed validation of human evaluation meth-ods, including likert scales and pairwise comparisons (Section 7).A comprehensive evaluation of four MTOD chatbots using validated metrics (Section 8).By presenting a detailed picture of MTOD chatbot performance and standard methods to evaluate them, we aid future work's efforts to further understand and improve human-computer interaction. Our evaluation platform, analyses, and data are available at https:\/\/github.com\/emorynlp\/ ChatEvaluationPlatform.ChatbotsTo evaluate the strengths and weaknesses of MTOD models, we select the chatbots for our study using a 15044 two-stage process: (1) a literature review to identify chatbot candidates, and (2) a pilot evaluation to select the final set of bots for our full study.Literature Review To promote diversity among the selected chatbots, we focus our review on four popular themes of the human-computer chat: (1) Knowledge-grounded chat, (2) Empathetic chat, (3) Self-consistent chat, and(4)General open-domain chat with large pre-training resources like Reddit. Candidate chatbots are selected from each theme using the following criteria:1. The bot must demonstrate state-of-the-art performance in a task related to the theme. 1 2. The implementation must be provided. 2 3. The response latency of the bot must be <10 seconds using modern GPU hardware.",
        "id":254854340
      }
    ],
    "negative_ctxs":[
      {
        "title":"A Convolutional Architecture for Word Sequence Prediction",
        "text":"We propose a convolutional neural network, named genCNN, for word sequence prediction.Different from previous work on neural networkbased language modeling and generation (e.g., RNN or LSTM), we choose not to greedily summarize the history of words as a fixed length vector. Instead, we use a convolutional neural network to predict the next word with the history of words of variable length. Also different from the existing feedforward networks for language modeling, our model can effectively fuse the local correlation and global correlation in the word sequence, with a convolution-gating strategy specifically designed for the task. We argue that our model can give adequate representation of the history, and therefore can naturally exploit both the short and long range dependencies. Our model is fast, easy to train, and readily parallelized. Our extensive experiments on text generation and n-best re-ranking in machine translation show that genCNN outperforms the state-ofthe-arts with big margins.",
        "id":5453533
      },
      {
        "title":"Published as a conference paper at ICLR 2023 IDEAL: QUERY-EFFICIENT DATA-FREE LEARNING FROM BLACK-BOX MODELS",
        "text":"Knowledge Distillation (KD) is a typical method for training a lightweight student model with the help of a well-trained teacher model. However, most KD methods require access to either the teacher's training data or model parameter, which is unrealistic. To tackle this problem, recent works study KD under data-free and black-box settings. Nevertheless, these works require a large number of queries to the teacher model, which incurs significant monetary and computational costs. To address these problems, we propose a novel method called query-effIcient Datafree lEarning from blAck-box modeLs (IDEAL), which aims to query-efficiently learn from black-box model APIs to train a good student without any real data. In detail, IDEAL trains the student model in two stages: data generation and model distillation. Note that IDEAL does not require any query in the data generation stage and queries the teacher only once for each sample in the distillation stage. Extensive experiments on various real-world datasets show the effectiveness of the proposed IDEAL. For instance, IDEAL can improve the performance of the best baseline method DFME by 5.83% on CIFAR10 dataset with only 0.02× the query budget of DFME. * Equal contribution. † Work done during internship at Sony AI.",
        "id":248987713
      },
      {
        "title":"Discourse on ASR Measurement: Introducing the ARPOCA Assessment Tool",
        "text":"Automatic speech recognition (ASR) has evolved from a pipeline architecture with pronunciation dictionaries, phonetic features and language models to the end-to-end systems performing a direct translation from a raw waveform into a word sequence. With the increase in accuracy and the availability of pre-trained models, the ASR systems are now omnipresent in our daily applications. On the other hand, the models' interpretability and their computational cost have become more challenging, particularly when dealing with less-common languages or identifying regional variations of speakers. This research proposal will follow a four-stage process: 1) Proving an overview of acoustic features and feature extraction algorithms; 2) Exploring current ASR models, tools, and performance assessment techniques; 3) Aligning features with interpretable phonetic transcripts; and 4) Designing a prototype AR-POCA to increase awareness of regional language variation and improve models feedback by developing a semi-automatic acoustic features extraction using PRAAT in conjunction with phonetic transcription.",
        "id":248779995
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there a method that measures the information provided in a (model generated) rationale beyond what the original context provided?",
    "positive_ctxs":[
      {
        "title":"REV: Information-Theoretic Evaluation of Free-Text Rationales",
        "text":"Generating free-text rationales is a promising step towards explainable NLP, yet evaluating such rationales remains a challenge. Existing metrics have mostly focused on measuring the association between the rationale and a given label. We argue that an ideal metric should focus on the new information uniquely provided in the rationale that is otherwise not provided in the input or the label. We investigate this research problem from an information-theoretic perspective using conditional V-information (Hewitt et al., 2021). More concretely, we propose a metric called REV (Rationale Evaluation with conditional V-information), to quantify the amount of new, label-relevant information in a rationale beyond the information already available in the input or the label. Experiments across four benchmarks with reasoning tasks, including chain-of-thought, demonstrate the effectiveness of REV in evaluating rationale-label pairs, compared to existing metrics. We further demonstrate REV is consistent with human judgments on rationale evaluations and provides more sensitive measurements of new information in free-text rationales. When used alongside traditional performance metrics, REV provides deeper insights into models' reasoning and prediction processes. . 2015. A large annotated corpus for learning natural language inference. In . 2021. Learning to rationalize for nonmonotonic reasoning with distant supervision.",
        "id":252816010
      }
    ],
    "negative_ctxs":[
      {
        "title":"Chinese Preposition Selection for Grammatical Error Diagnosis",
        "text":"Misuse of Chinese prepositions is one of common word usage errors in grammatical error diagnosis. In this paper, we adopt the Chinese Gigaword corpus and HSK corpus as L1 and L2 corpora, respectively. We explore gated recurrent neural network model (GRU), and an ensemble of GRU model and maximum entropy language model (GRU-ME) to select the best preposition from 43 candidates for each test sentence. The experimental results show the advantage of the GRU models over simple RNN and n-gram models. We further analyze the effectiveness of linguistic information such as word boundary and part-of-speech tag in this task.This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details:",
        "id":17778623
      },
      {
        "title":"Shared Task Papers",
        "text":"This paper presents the results of the premier shared task organized alongside the Conference on Machine Translation (WMT) 2018.Participants were asked to build machine translation systems for any of 7 language pairs in both directions, to be evaluated on a test set of news stories. The main metric for this task is human judgment of translation quality. This year, we also opened up the task to additional test suites to probe specific aspects of translation.",
        "id":53247198
      },
      {
        "title":"N2N Learning: Network to Network Compression via Policy Gradient Reinforcement Learning",
        "text":"While bigger and deeper neural network architectures continue to advance the state-of-the-art for many computer vision tasks, real-world adoption of these networks is impeded by hardware and speed constraints. Conventional model compression methods attempt to address this problem by modifying the architecture manually or using pre-defined heuristics. Since the space of all reduced architectures is very large, modifying the architecture of a deep neural network in this way is a difficult task. In this paper, we tackle this issue by introducing a principled method for learning reduced network architectures in a data-driven way using reinforcement learning. Our approach takes a larger 'teacher' network as input and outputs a compressed 'student' network derived from the 'teacher' network. In the first stage of our method, a recurrent policy network aggressively removes layers from the large 'teacher' model. In the second stage, another recurrent policy network carefully reduces the size of each remaining layer. The resulting network is then evaluated to obtain a reward -a score based on the accuracy and compression of the network. Our approach uses this reward signal with policy gradients to train the policies to find a locally optimal student network. Our experiments show that we can achieve compression rates of more than 10× for models such as ResNet-34 while maintaining similar performance to the input 'teacher' network. We also present a valuable transfer learning result which shows that policies which are pre-trained on smaller 'teacher' networks can be used to rapidly speed up training on larger 'teacher' networks.",
        "id":13352766
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there a paper comparing knowledge distillation and human annotation in terms of cost efficiency?",
    "positive_ctxs":[
      {
        "title":"Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models",
        "text":"Fine-tuning large models is highly effective, however, inference can be expensive and produces carbon emissions. Knowledge distillation has been shown to be a practical solution to reduce inference costs, but the distillation process itself requires significant computational resources. Rather than buying or renting GPUs to fine-tune, then distill a large model, an NLP practitioner might instead choose to allocate the available budget to hire annotators and manually label additional fine-tuning data. In this paper, we investigate how to most efficiently use a fixed budget to build a compact model. Through extensive experiments on six diverse tasks, we show that distilling from T5-XXL (11B) to T5-Small (60M) is almost always a cost-efficient strategy compared to annotating more data to directly train a compact model (T5-Small). We further investigate how the optimal budget allocated towards computation varies across scenarios. We will make our code, datasets, annotation cost estimates, and baseline models available as a benchmark to support further work on cost-efficient training of compact models.",
        "id":258436868
      }
    ],
    "negative_ctxs":[
      {
        "title":"NILC at CWI 2018: Exploring Feature Engineering and Feature Learning",
        "text":"This paper describes the results of NILC team at CWI 2018. We developed solutions following three approaches: (i) a feature engineering method using lexical, n-gram and psycholinguistic features, (ii) a shallow neural network method using only word embeddings, and (iii) a Long Short-Term Memory (LSTM) language model, which is pre-trained on a large text corpus to produce a contextualized word vector. The feature engineering method obtained our best results for the classification task and the LSTM model achieved the best results for the probabilistic classification task. Our results show that deep neural networks are able to perform as well as traditional machine learning methods using manually engineered features for the task of complex word identification in English.",
        "id":46940692
      },
      {
        "title":"How Effective is Task-Agnostic Data Augmentation for Pretrained Transformers?",
        "text":"Task-agnostic forms of data augmentation have proven widely effective in computer vision, even on pretrained models. In NLP similar results are reported most commonly for low data regimes, non-pretrained models, or situationally for pretrained models. In this paper we ask how effective these techniques really are when applied to pretrained transformers. Using two popular varieties of task-agnostic data augmentation (not tailored to any particular task), Easy Data Augmentation (Wei and Zou, 2019) and Back-Translation (Sennrich et al., 2015), we conduct a systematic examination of their effects across 5 classification tasks, 6 datasets, and 3 variants of modern pretrained transformers, including BERT, XL-NET, and ROBERTA. We observe a negative result, finding that techniques which previously reported strong improvements for nonpretrained models fail to consistently improve performance for pretrained transformers, even when training data is limited. We hope this empirical analysis helps inform practitioners where data augmentation techniques may confer improvements.",
        "id":222132977
      },
      {
        "title":"Predicting and Using Implicit Discourse Elements in Chinese-English Translation",
        "text":"In machine translation (MT) implicitation can occur when elements such as discourse markers and pronouns are not expected or mandatory in the source language, but need to be realised in the target language for a coherent translation. These 'implicit' elements can be seen as both a barrier to MT and an important source of information. However, identifying where such elements are needed and producing them are non-trivial tasks. In this paper we examine the effect of implicit elements on MT and propose methods to identify and make them explicit. As a starting point, we use human translated and aligned data to decide where to insert place holders for these elements. We then fully automate this process by devising a prediction model to decide if and where implicit elements should occur and be made explicit. Our experiments compare statistical machine translation models built with and without these explicitation processes. Models built on data marked for discourse elements show substantial improvements over the baseline.",
        "id":14873664
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_acl",
    "question":"Is there a paper exploring the curse of multilinguality for similar languages?",
    "positive_ctxs":[
      {
        "title":"Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages",
        "text":"The NLP community has mainly focused on scaling Large Language Models (LLMs) vertically, i.e., making them better for about 100 languages. We instead scale LLMs horizontally: we create, through continued pretraining, Glot500-m, an LLM that covers 511 predominantly low-resource languages. An important part of this effort is to collect and clean Glot500-c, a corpus that covers these 511 languages and allows us to train Glot500-m. We evaluate Glot500-m on five diverse tasks across these languages. We observe large improvements for both high-resource and low-resource languages compared to an XLM-R baseline. Our analysis shows that no single factor explains the quality of multilingual LLM representations. Rather, a combination of factors determines quality including corpus size, script, \"help\" from related languages and the total capacity of the model. Our work addresses an important goal of NLP research: we should not limit NLP to a small fraction of the world's languages and instead strive to support as many languages as possible to bring the benefits of NLP technology to all languages and cultures. Code, data and models are available at https",
        "id":258832427
      }
    ],
    "negative_ctxs":[
      {
        "title":"Effective Use of Transformer Networks for Entity Tracking",
        "text":"Tracking entities in procedural language requires understanding the transformations arising from actions on entities as well as those entities' interactions. While self-attention-based pre-trained language encoders like GPT and BERT have been successfully applied across a range of natural language understanding tasks, their ability to handle the nuances of procedural texts is still untested. In this paper, we explore the use of pre-trained transformer networks for entity tracking tasks in procedural text. First, we test standard lightweight approaches for prediction with pre-trained transformers, and find that these approaches underperform even simple baselines. We show that much stronger results can be attained by restructuring the input to guide the transformer model to focus on a particular entity. Second, we assess the degree to which transformer networks capture the process dynamics, investigating such factors as merged entities and oblique entity references. On two different tasks, ingredient detection in recipes and QA over scientific processes, we achieve state-ofthe-art results, but our models still largely attend to shallow context clues and do not form complex representations of intermediate entity or process state. 1 760Seq. of Steps water mixture sugarRoots absorb water from soil.M O OThe water flows to the leaf.M O OLight from the sun and CO 2 enter the leaf.E O OLight, water, and CO 2 combine into mixture.",
        "id":202539208
      },
      {
        "title":"A Qualitative Comparison of Scientific and Journalistic Texts from the Perspective of Extracting Definitions",
        "text":"In this paper we highlight a selection of features of scientific text which distinguish it from news stories. We argue that features such as structure, selective use of past tense, voice and stylistic conventions can affect question answering in the scientific domain. We demonstrate this through qualitative observations made while working on retrieving definitions to terms related to salmon fish. to authors submitting papers to the journal Aquaculture (Elsevier Author Guide, 2003) specify the following required sections: Abstract,",
        "id":15424937
      },
      {
        "title":"",
        "text":"",
        "id":220330818
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there a paper that applies large language models to visual Raven’s Progressive Matrices?",
    "positive_ctxs":[
      {
        "title":"In-Context Analogical Reasoning with Pre-Trained Language Models",
        "text":"Analogical reasoning is a fundamental capacity of human cognition that allows us to reason abstractly about novel situations by relating them to past experiences. While it is thought to be essential for robust reasoning in AI systems, conventional approaches require significant training and\/or hard-coding of domain knowledge to be applied to benchmark tasks. Inspired by cognitive science research that has found connections between human language and analogy-making, we explore the use of intuitive language-based abstractions to support analogy in AI systems. Specifically, we apply large pre-trained language models (PLMs) to visual Raven's Progressive Matrices (RPM), a common relational reasoning test. By simply encoding the perceptual features of the problem into language form, we find that PLMs exhibit a striking capacity for zero-shot relational reasoning, exceeding human performance and nearing supervised vision-based methods. We explore different encodings that vary the level of abstraction over task features, finding that higherlevel abstractions further strengthen PLMs' analogical reasoning. Our detailed analysis reveals insights on the role of model complexity, incontext learning, and prior knowledge in solving RPM tasks.",
        "id":258959097
      }
    ],
    "negative_ctxs":[
      {
        "title":"Automatic Alignment of Japanese and English Newspaper Articles using an MT System and a Bilingual Company Name Dictionary",
        "text":"One of the crucial parts of any corpus-based machine translation system is a large-scale bilingual corpus that is aligned at various levels such, as the sentence and phrase levels. This kind of corpus, however, is not easy to obtain, and accordingly, there is a great need for an efficient construction method. We approach this problem by integrating two large monolingual corpora in two different languages sharing the same source of information. We often see such a situation in journalistic texts where the same events are reported in many languages. Unfortunately, they often lack article-level alignment information and the recovery of this is the first problem to solve. In this paper, we report a method of automatically aligning Japanese and English newspaper articles in the financial and economic news domain. Although conventional methods require some manual work, the proposed method works fully automatically. We show that our method can align such newspaper articles with an accuracy of 97%.",
        "id":1446880
      },
      {
        "title":"Translation Memory Systems Have a Long Way to Go",
        "text":"The TM memory systems changed the work of translators and now the translators not benefiting from these tools are a tiny minority. These tools operate on fuzzy (surface) matching mostly and cannot benefit from already translated texts which are synonymous to (or paraphrased versions of) the text to be translated. The match score is mostly based on character-string similarity, calculated through Levenshtein distance. The TM tools have difficulties with detecting similarities even in sentences which represent a minor revision of sentences already available in the translation memory. This shortcoming of the current TM systems was the subject of the present study and was empirically proven in the experiments we conducted. To this end, we compiled a small translation memory (English-Spanish) and applied several lexical and syntactic transformation rules to the source sentences with both English and Spanish being the source language.The results of this study show that current TM systems have a long way to go and highlight the need for TM systems equipped with NLP capabilities which will offer the translator the advantage of he\/she not having to translate a sentence again if an almost identical sentence has already been already translated.",
        "id":10494199
      },
      {
        "title":"The Properties and Further Applications of Chinese Frequent Strings",
        "text":"This paper reveals some important properties of CFSs and applications in Chinese natural language processing (NLP). We have previously proposed a method for extracting Chinese frequent strings that contain unknown words from a Chinese corpus[Lin and Yu 2001]. We found that CFSs contain many 4-character strings, 3-word strings, and longer n-grams. Such information can only be derived from an extremely large corpus using a traditional language model(LM). In contrast to using a traditional LM, we can achieve high precision and efficiency by using CFSs to solve Chinese toneless phoneme-to-character conversion and to correct Chinese spelling errors with a small training corpus. An accuracy rate of 92.86% was achieved for Chinese toneless phoneme-to-character conversion, and an accuracy rate of 87.32% was achieved for Chinese spelling error correction. We also attempted to assign syntactic categories to a CFS. The accuracy rate for assigning syntactic categories to the CFSs was 88.53% for outside testing when the syntactic categories of the highest level were used.",
        "id":17752874
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there a paper that connects the basic elements of storytelling with biased or imbalanced media reporting?",
    "positive_ctxs":[
      {
        "title":"Conflicts, Villains, Resolutions: Towards models of Narrative Media Framing",
        "text":"Despite increasing interest in the automatic detection of media frames in NLP, the problem is typically simplified as single-label classification and adopts a topic-like view on frames, evading modelling the broader document-level narrative. In this work, we revisit a widely used conceptualization of framing from the communication sciences which explicitly captures elements of narratives, including conflict and its resolution, and integrate it with the narrative framing of key entities in the story as heroes, victims or villains. We adapt an effective annotation paradigm that breaks a complex annotation task into a series of simpler binary questions, and present an annotated data set of English news articles, and a case study on the framing of climate change in articles from news outlets across the political spectrum. Finally, we explore automatic multi-label prediction of our frames with supervised and semisupervised approaches, and present a novel retrieval-based method which is both effective and transparent in its predictions. We conclude with a discussion of opportunities and challenges for future work on document-level models of narrative framing. 1",
        "id":259075515
      }
    ],
    "negative_ctxs":[
      {
        "title":"DBpedia: A Multilingual Cross-Domain Knowledge Base",
        "text":"The DBpedia project extracts structured information from Wikipedia editions in 97 different languages and combines this information into a large multi-lingual knowledge base covering many specific domains and general world knowledge. The knowledge base contains textual descriptions (titles and abstracts) of concepts in up to 97 languages. It also contains structured knowledge that has been extracted from the infobox systems of Wikipedias in 15 different languages and is mapped onto a single consistent ontology by a community effort. The knowledge base can be queried using a structured query language and all its data sets are freely available for download. In this paper, we describe the general DBpedia knowledge base and extended data sets that specifically aim at supporting computational linguistics tasks. These task include Entity Linking, Word Sense Disambiguation, Question Answering, Slot Filling and Relationship Extraction. These use cases are outlined, pointing at added value that the structured data of DBpedia provides.",
        "id":8783027
      },
      {
        "title":"Argument Identification in Chinese Editorials",
        "text":"In this paper, we develop and evaluate several techniques for identifying argumentative paragraphs in Chinese editorials. We first use three methods of evaluation to score a paragraph's argumentative nature: a relative word frequency approach; a method which targets known argumentative words in our corpus; and a combined approach which uses elements from the previous two. Then, we determine the best score thresholds for separating argumentative and non-argumentative paragraphs. The results of our experimentation show that our relative word frequency approach provides a reliable way to identify argumentative paragraphs with a F 1 score of 0.91, though challenges in accurate scoring invite improvement through context-aware means.",
        "id":7831381
      },
      {
        "title":"Part-of-Speech Annotation of Biology Research Abstracts",
        "text":"A part-of-speech (POS) tagged corpus was built on research abstracts in biomedical domain with the Penn Treebank scheme. As consistent annotation was difficult without domain-specific knowledge we made use of the existing term annotation of the GENIA corpus. A list of frequent terms annotated in the GENIA corpus was compiled and the POS of each constituent of those terms were determined with assistance from domain specialists. The POS of the terms in the list are pre-assigned, then a tagger assigns POS to remaining words preserving the pre-assigned POS, whose results are corrected by human annotators. We also modified the PTB scheme slightly. An inter-annotator agreement tested on new 50 abstracts was 98.5%. A POS tagger trained with the annotated abstracts was tested against a gold-standard set made from the interannotator agreement. The untrained tagger had the accuracy of 83.0%. Trained with 2000 annotated abstracts the accuracy rose to 98.2%. The 2000 annotated abstracts are publicly available.",
        "id":1236813
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_acl",
    "question":"Is there a paper that links exposure bias to distillation?",
    "positive_ctxs":[
      {
        "title":"A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training",
        "text":"Modern Natural Language Generation (NLG) models come with massive computational and storage requirements. In this work, we study the potential of compressing them, which is crucial for real-world applications serving millions of users. We focus on Knowledge Distillation (KD) techniques, in which a small student model learns to imitate a large teacher model, allowing to transfer knowledge from the teacher to the student. In contrast to much of the previous work, our goal is to optimize the model for a specific NLG task and a specific dataset. Typically in real-world applications, in addition to labeled data there is abundant unlabeled task-specific data, which is crucial for attaining high compression rates via KD. In this work, we conduct a systematic study of task-specific KD techniques for various NLG tasks under realistic assumptions. We discuss the special characteristics of NLG distillation and particularly the exposure bias problem. Following, we derive a family of Pseudo-Target (PT) augmentation methods, substantially extending prior work on sequence-level KD. We propose the Joint-Teaching method, which applies wordlevel KD to multiple PTs generated by both the teacher and the student. Finally, we validate our findings in an extreme setup with no labeled examples using GPT-4 as the teacher.Our study provides practical model design observations and demonstrates the effectiveness of PT training for task-specific KD in NLG.",
        "id":258461336
      }
    ],
    "negative_ctxs":[
      {
        "title":"General and Task-Specific Corpus Resources for Polish Adult Learners of English",
        "text":"This paper offers a comparison of two resources for Polish adult learners of English. The first has been designed for Polish-English Literacy Tutor (PELT), a multimodal system for foreign language learning, as training input to speech recognition system for highly accented, strongly variable second language speech. The second corpus is a task-specific resource designed in the PELT framework to investigate the vowel space of English produced by Poles. Presented are linguistically and technologically challenging aspects of the two ventures and their complementary character.",
        "id":6864948
      },
      {
        "title":"Factored Neural Language Models",
        "text":"We present a new type of neural probabilistic language model that learns a mapping from both words and explicit word features into a continuous space that is then used for word prediction. Additionally, we investigate several ways of deriving continuous word representations for unknown words from those of known words. The resulting model significantly reduces perplexity on sparse-data tasks when compared to standard backoff models, standard neural language models, and factored language models.",
        "id":937826
      },
      {
        "title":"",
        "text":"",
        "id":232021865
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there a paper that shows that language models' error distribution is different for unfamiliar entities that is not apparent when models are evaluated on familiar entities alone?",
    "positive_ctxs":[
      {
        "title":"Factual or Contextual? Disentangling Error Types in Entity Description Generation",
        "text":"In the task of entity description generation, given a context and a specified entity, a model must describe that entity correctly and in a contextually-relevant way. In this task, as well as broader language generation tasks, the generation of a nonfactual description (factual error) versus an incongruous description (contextual error) is fundamentally different, yet often conflated. We develop an evaluation paradigm that enables us to disentangle these two types of errors in naturally occurring textual contexts. We find that factuality and congruity are often at odds, and that models specifically struggle with accurate descriptions of entities that are less familiar to people. This shortcoming of language models raises concerns around the trustworthiness of such models, since factual errors on less well-known entities are exactly those that a human reader will not recognize.",
        "id":259093058
      }
    ],
    "negative_ctxs":[
      {
        "title":"Enriching Entity Translation Discovery using Selective Temporality",
        "text":"This paper studies named entity translation and proposes \"selective temporality\" as a new feature, as using temporal features may be harmful for translating \"atemporal\" entities. Our key contribution is building an automatic classifier to distinguish temporal and atemporal entities then align them in separate procedures to boost translation accuracy by 6.1%.",
        "id":13878778
      },
      {
        "title":"Translating and the Computer 14",
        "text":"The paper describes the BCI, a prototype interactive machine-translation system, constructed by connecting English and Swedish versions of the SRI Core Language Engine through a transfer component. Transfer takes place at the level of Quasi Logical Form (QLF), a contextually sensitive logical form representation which is deep enough for dealing with cross-linguistic differences. Theoretical arguments are presented to support the claim that QLF transfer represents a good compromise between the opposing paradigms of syntactic transfer and semantic interlinguabased MT. An annotated example dialogue is shown. A follow-on project, in which the BCI is used as the core of a spoken-language translation system, is briefly described.",
        "id":29911601
      },
      {
        "title":"AttesTable at SemEval-2021 Task 9: Extending Statement Verification with Tables for Unknown Class, and Semantic Evidence Finding",
        "text":"This paper describes our approach for Task 9 of SemEval 2021: Statement Verification and Evidence Finding with Tables. We participated in both subtasks, namely statement verification and evidence finding. For the subtask of statement verification, we extend the TAPAS model to adapt to the 'unknown' class of statements by finetuning it on an augmented version of the task data. For the subtask of evidence finding, we finetune the DistilBERT model in a Siamese setting.",
        "id":236459991
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there a paper that supports the use of automated coherence metrics in topic model evaluations?",
    "positive_ctxs":[
      {
        "title":"Large-Scale Correlation Analysis of Automated Metrics for Topic Models",
        "text":"Automated coherence metrics constitute an important and popular way to evaluate topic models. Previous works present a mixed picture of their presumed correlation with human judgement. In this paper, we conduct a large-scale correlation analysis of coherence metrics. We propose a novel sampling approach to mine topics for the purpose of metric evaluation, and conduct the analysis via three large corpora showing that certain automated coherence metrics are correlated. Moreover, we extend the analysis to measure topical differences between corpora. Lastly, we examine the reliability of human judgement by conducting an extensive user study, which is designed as an amalgamation of different proxy tasks to derive a finer insight into the human decision-making processes. Our findings reveal some correlation between automated coherence metrics and human judgement, especially for generic corpora.",
        "id":259370596
      }
    ],
    "negative_ctxs":[
      {
        "title":"Benchmarking Aggression Identification in Social Media",
        "text":"In this paper, we present the report and findings of the Shared Task on Aggression Identification organised as part of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC -1) at COLING 2018. The task was to develop a classifier that could discriminate between Overtly Aggressive, Covertly Aggressive, and Non-aggressive texts. For this task, the participants were provided with a dataset of 15,000 aggression-annotated Facebook Posts and Comments each in Hindi (in both Roman and Devanagari script) and English for training and validation. For testing, two different sets -one from Facebook and another from a different social media -were provided. A total of 130 teams registered to participate in the task, 30 teams submitted their test runs, and finally 20 teams also sent their system description paper which are included in the TRAC workshop proceedings. The best system obtained a weighted F-score of 0.64 for both Hindi and English on the Facebook test sets, while the best scores on the surprise set were 0.60 and 0.50 for English and Hindi respectively. The results presented in this report depict how challenging the task is. The positive response from the community and the great levels of participation in the first edition of this shared task also highlights the interest in this topic. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:\/\/ creativecommons.org\/licenses\/by\/4.0\/",
        "id":59336626
      },
      {
        "title":"On the Relevance and Learner Dependence of Co-text Complexity for Exercise Difficulty",
        "text":"Adaptive exercise sequencing in Intelligent Language Tutoring Systems (ILTS) aims to select exercises for individual learners that match their abilities. For exercises practicing forms in isolation, it may be sufficient for sequencing to consider the form being practiced. But when exercises embed the forms in a sentence or bigger language context, little is known about how the nature of this co-text influences learners in completing the exercises.To fill the gap, based on data from two large field studies conducted with an English ILTS in German secondary schools, we analyze the impact of co-text complexity on learner performance for different exercise types and learners at different proficiency levels. The results show that co-text complexity is an important predictor for a learner's performance on practice exercises, especially for gap filling and Jumbled Sentences exercises, and particularly for learners at higher proficiency levels.",
        "id":258761891
      },
      {
        "title":"When classifying grammatical role, BERT doesn't care about word order. . . except when it matters",
        "text":"Because meaning can often be inferred from lexical semantics alone, word order is often a redundant cue in natural language. For example, the words chopped, chef, and onion are more likely used to convey \"The chef chopped the onion,\" not \"The onion chopped the chef.\" Recent work has shown large language models to be surprisingly word order invariant, but crucially has largely considered natural prototypical inputs, where compositional meaning mostly matches lexical expectations. To overcome this confound, we probe grammatical role representation in English BERT and GPT-2, on instances where lexical expectations are not sufficient, and word order knowledge is necessary for correct classification. Such nonprototypical instances are naturally occurring English sentences with inanimate subjects or animate objects, or sentences where we systematically swap the arguments to make sentences like \"The onion chopped the chef\". We find that, while early layer embeddings are largely lexical, word order is in fact crucial in defining the later-layer representations of words in semantically non-prototypical positions. Our experiments isolate the effect of word order on the contextualization process, and highlight how models use context in the uncommon, but critical, instances where it matters.",
        "id":247447149
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there a paper that uses Explainable AI techniques to investigate how language models represent the expression of morality?",
    "positive_ctxs":[
      {
        "title":"What does a Text Classifier Learn about Morality? An Explainable Method for Cross-Domain Comparison of Moral Rhetoric",
        "text":"Moral rhetoric influences our judgement. Although social scientists recognize moral expression as domain specific, there are no systematic methods for analyzing whether a text classifier learns the domain-specific expression of moral language or not. We propose Tomea, a method to compare a supervised classifier's representation of moral rhetoric across domains. Tomea enables quantitative and qualitative comparisons of moral rhetoric via an interpretable exploration of similarities and differences across moral concepts and domains. We apply Tomea on moral narratives in thirtyfive thousand tweets from seven domains. We extensively evaluate the method via a crowd study, a series of cross-domain moral classification comparisons, and a qualitative analysis of cross-domain moral expression.",
        "id":259370854
      }
    ],
    "negative_ctxs":[
      {
        "title":"English Recipe Flow Graph Corpus",
        "text":"We present an annotated corpus of English cooking recipe procedures, and describe and evaluate computational methods for learning these annotations. The corpus consists of 300 recipes written by members of the public, which we have annotated with domain-specific linguistic and semantic structure. Each recipe is annotated with (1) 'recipe named entities' (r-NEs) specific to the recipe domain, and (2) a flow graph representing in detail the sequencing of steps, and interactions between cooking tools, food ingredients and the products of intermediate steps. For these two kinds of annotations, inter-annotator agreement ranges from 82.3 to 90.5 F1, indicating that our annotation scheme is appropriate and consistent. We experiment with producing these annotations automatically. For r-NE tagging we train a deep neural network NER tool; to compute flow graphs we train a dependency-style parsing procedure which we apply to the entire sequence of r-NEs in a recipe. In evaluations, our systems achieve 71.1 to 87.5 F1, demonstrating that our annotation scheme is learnable.",
        "id":218977407
      },
      {
        "title":"A Logistic Regression Model of Determiner Omission in PPs Tibor Kiss Katja Keßelmeier Antje Müller",
        "text":"The realization of singular count nouns without an accompanying determiner inside a PP (determinerless PP, bare PP, Preposition-Noun Combination) has recently attracted some interest in computational linguistics. Yet, the relevant factors for determiner omission remain unclear, and conditions for determiner omission vary from language to language. We present a logistic regression model of determiner omission in German based on data obtained by applying annotation mining to a large, automatically and manually annotated corpus.",
        "id":16669196
      },
      {
        "title":"SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving Out-of-Domain Robustness",
        "text":"Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples. Data augmentation is a common method used to prevent overfitting and improve OOD generalization. However, in natural language, it is difficult to generate new examples that stay on the underlying data manifold. We introduce SSMBA, a data augmentation method for generating synthetic training examples by using a pair of corruption and reconstruction functions to move randomly on a data manifold. We investigate the use of SSMBA in the natural language domain, leveraging the manifold assumption to reconstruct corrupted text with masked language models. In experiments on robustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently outperforms existing data augmentation methods and baseline models on both in-domain and OOD data, achieving gains of 0.8% accuracy on OOD Amazon reviews, 1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English. 1 . 2018. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs\/1810.04805.",
        "id":221836078
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there a paper that uses an app for a popular tabletop game to gather real transcripts of gameplay with concrete values for players' and monsters' health?",
    "positive_ctxs":[
      {
        "title":"FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information",
        "text":"Dungeons & Dragons (D&D) is a tabletop roleplaying game with complex natural language interactions between players and hidden state information. Recent work has shown that large language models (LLMs) that have access to state information can generate higher quality game turns than LLMs that use dialog history alone. However, previous work used game state information that was heuristically created and was not a true gold standard game state. We present FIREBALL, a large dataset containing nearly 25,000 unique sessions from real D&D gameplay on Discord with true game state info. We recorded game play sessions of players who used the Avrae bot, which was developed to aid people in playing D&D online, capturing language, game commands and underlying game state information. We demonstrate that FIRE-BALL can improve natural language generation (NLG) by using Avrae state information, improving both automated metrics and human judgments of quality. Additionally, we show that LLMs can generate executable Avrae commands, particularly after finetuning.language models with strategic reasoning. Science, 378(6624):",
        "id":258436686
      }
    ],
    "negative_ctxs":[
      {
        "title":"Ranking-Enhanced Unsupervised Sentence Representation Learning",
        "text":"Unsupervised sentence representation learning has progressed through contrastive learning and data augmentation methods such as dropout masking. Despite this progress, sentence encoders are still limited to using only an input sentence when predicting its semantic vector. In this work, we show that the semantic meaning of a sentence is also determined by nearestneighbor sentences that are similar to the input sentence. Based on this finding, we propose a novel unsupervised sentence encoder, RankEncoder. RankEncoder predicts the semantic vector of an input sentence by leveraging its relationship with other sentences in an external corpus, as well as the input sentence itself. We evaluate RankEncoder on semantic textual benchmark datasets. From the experimental results, we verify that 1) RankEncoder achieves 80.07% Spearman's correlation, a 1.1% absolute improvement compared to the previous state-of-the-art performance, 2) RankEncoder is universally applicable to existing unsupervised sentence embedding methods, and 3) RankEncoder is specifically effective for predicting the similarity scores of similar sentence pairs. 1 * This work was done during an internship at Amazon. 1  We provide the implementation of RankEncoder at https: \/\/github.com\/yeonsw\/RankEncoder.git . 2022. Trans-encoder: Unsupervised sentence-pair modelling through self-and mutual-distillations. In ICLR. Fangyu Liu, Ivan Vulić, Anna Korhonen, and Nigel Collier. 2021. Fast, effective, and self-supervised: Transforming masked language models into universal lexical and sentence encoders. In EMNLP. parelli. 2014. A SICK cure for the evaluation of compositional distributional semantic models. In LREC. Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In ACL. Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP. Ellen M Voorhees and Dawn M Tice. 2000. Building a question answering test collection. In SIGIR.",
        "id":252185412
      },
      {
        "title":"Efficient Optimization of an MDL-Inspired Objective Function for Unsupervised Part-of-Speech Tagging",
        "text":"The Minimum Description Length (MDL) principle is a method for model selection that trades off between the explanation of the data by the model and the complexity of the model itself. Inspired by the MDL principle, we develop an objective function for generative models that captures the description of the data by the model (log-likelihood) and the description of the model (model size). We also develop a efficient general search algorithm based on the MAP-EM framework to optimize this function. Since recent work has shown that minimizing the model size in a Hidden Markov Model for part-of-speech (POS) tagging leads to higher accuracies, we test our approach by applying it to this problem. The search algorithm involves a simple change to EM and achieves high POS tagging accuracies on both English and Italian data sets.",
        "id":5613146
      },
      {
        "title":"Challenges for annotating images for sense disambiguation",
        "text":"We describe an unusual data set of thousands of annotated images with interesting sense phenomena. Natural language image sense annotation involves increased semantic complexities compared to disambiguating word senses when annotating text. These issues are discussed and illustrated, including the distinction between word senses and iconographic senses.",
        "id":15545852
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_acl",
    "question":"Is there a paper that uses evolutionary algorithms and neural MT metrics to produce translations?",
    "positive_ctxs":[
      {
        "title":"Breeding Machine Translations: Evolutionary approach to survive and thrive in the world of automated evaluation",
        "text":"We propose a genetic algorithm (GA) based method for modifying n-best lists produced by a machine translation (MT) system. Our method offers an innovative approach to improving MT quality and identifying weaknesses in evaluation metrics. Using common GA operations (mutation and crossover) on a list of hypotheses in combination with a fitness function (an arbitrary MT metric), we obtain novel and diverse outputs with high metric scores. With a combination of multiple MT metrics as the fitness function, the proposed method leads to an increase in translation quality as measured by other held-out automatic metrics. With a single metric (including popular ones such as COMET) as the fitness function, we find blind spots and flaws in the metric. This allows for an automated search for adversarial examples in an arbitrary metric, without prior assumptions on the form of such example. As a demonstration of the method, we create datasets of adversarial examples and use them to show that reference-free COMET is substantially less robust than the reference-based version.",
        "id":258988004
      }
    ],
    "negative_ctxs":[
      {
        "title":"Efficient Near-Duplicate Detection for Q&A Forum",
        "text":"This paper addresses the issue of redundant data in large-scale collections of Q&A forums. We propose and evaluate a novel algorithm for automatically detecting the near-duplicate Q&A threads. The main idea is to use the distributed index and Map-Reduce framework to calculate pairwise similarity and identify redundant data fast and scalably. The proposed method was evaluated on a real-world data collection crawled from a popular Q&A forum. Experimental results show that our proposed method can effectively and efficiently detect nearduplicate content in large web collections.",
        "id":6421218
      },
      {
        "title":"",
        "text":"",
        "id":245838271
      },
      {
        "title":"A MDL-based Model of Gender Knowledge Acquisition",
        "text":"This paper presents an iterative model of knowledge acquisition of gender information associated with word endings in French. Gender knowledge is represented as a set of rules containing exceptions. Our model takes noun-gender pairs as input and constantly maintains a list of rules and exceptions which is both coherent with the input data and minimal with respect to a minimum description length criterion. This model was compared to human data at various ages and showed a good fit. We also compared the kind of rules discovered by the model with rules usually extracted by linguists and found interesting discrepancies.",
        "id":7856629
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there a paper that uses similarity scores to check knowledge in diffusion models",
    "positive_ctxs":[
      {
        "title":"Multilingual Conceptual Coverage in Text-to-Image Models",
        "text":"AltDiffusion DallE 2 StableDiffusion 2 DallE mega 1Figure 1: A selection of images generated by DALLE-mega, Stable Diffusion 2, DALLE-2, and AltDiffusion, illustrating their conceptual coverage of \"dog,\" \"airplane,\" and \"face\" across English, Spanish, German, Chinese (simplified), Japanese, Hebrew, and Indonesian. Coverage of the concepts varies considerably across model and language, and can be observed in the consistency and correctness of images generated under simple prompts.AbstractWe propose \"Conceptual Coverage Across Languages\" (CoCo-CroLa), a technique for benchmarking the degree to which any generative text-to-image system provides multilingual parity to its training language in terms of tangible nouns. For each model we can assess \"conceptual coverage\" of a given target language relative to a source language by comparing the population of images generated for a series of tangible nouns in the source language to the population of images generated for each noun under translation in the target language. This technique allows us to estimate how wellsuited a model is to a target language as well as identify model-specific weaknesses, spurious correlations, and biases without a-priori assumptions. We demonstrate how it can be used to benchmark T2I models in terms of multilinguality, and how despite its simplicity it is a good proxy for impressive generalization. . 2021. Peco: Examining single sentence label leakage in natural language inference datasets through progressive evaluation of cluster outliers. ArXiv preprint, abs\/2112.09237.",
        "id":259063887
      }
    ],
    "negative_ctxs":[
      {
        "title":"Implicit Proposal Filtering in Multi-Party Consensus-Building Conversations",
        "text":"An attempt was made to statistically estimate proposals which survived the discussion to be incorporated in the final agreement in an instance of a Japanese design conversation. Low level speech and vision features of hearer behaviors corresponding to aiduti, noddings and gaze were found to be a positive predictor of survival. The result suggests that non-linguistic hearer responses work as implicit proposal filters in consensus building, and could provide promising candidate features for the purpose of recognition and summarization of meeting events.",
        "id":1286260
      },
      {
        "title":"Extracting Important Sentences with Support Vector Machines",
        "text":"Extracting sentences that contain important information from a document is a form of text summarization. The technique is the key to the automatic generation of summaries similar to those written by humans. To achieve such extraction, it is important to be able to integrate heterogeneous pieces of information. One approach, parameter tuning by machine learning, has been attracting a lot of attention. This paper proposes a method of sentence extraction based on Support Vector Machines (SVMs). To confirm the method's performance, we conduct experiments that compare our method to three existing methods. Results on the Text Summarization Challenge (TSC) corpus show that our method offers the highest accuracy. Moreover, we clarify the different features effective for extracting different document genres.",
        "id":709985
      },
      {
        "title":"Unanimous Prediction for 100% Precision with Application to Learning Semantic Mappings",
        "text":"Can we train a system that, on any new input, either says \"don't know\" or makes a prediction that is guaranteed to be correct? We answer the question in the affirmative provided our model family is wellspecified. Specifically, we introduce the unanimity principle: only predict when all models consistent with the training data predict the same output. We operationalize this principle for semantic parsing, the task of mapping utterances to logical forms. We develop a simple, efficient method that reasons over the infinite set of all consistent models by only checking two of the models. We prove that our method obtains 100% precision even with a modest amount of training data from a possibly adversarial distribution. Empirically, we demonstrate the effectiveness of our approach on the standard GeoQuery dataset.",
        "id":7303682
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there a paper that uses the tree structure of math equations in autoregressive language models?",
    "positive_ctxs":[
      {
        "title":"Tree-Based Representation and Generation of Natural and Mathematical Language",
        "text":"Mathematical language in scientific communications and educational scenarios is important yet relatively understudied compared to natural languages. Recent works on mathematical language focus either on representing stand-alone mathematical expressions, especially in their natural tree format, or mathematical reasoning in pre-trained natural language models. Existing works on jointly modeling and generating natural and mathematical languages simply treat mathematical expressions as text, without accounting for the rigid structural properties of mathematical expressions. In this paper, we propose a series of modifications to existing language models to jointly represent and generate text and math: representing mathematical expressions as sequences of node tokens in their operator tree format, using math symbol and tree position embeddings to preserve the semantic and structural properties of mathematical expressions, and using a constrained decoding method to generate mathematically valid expressions. We ground our modifications in GPT-2, resulting in a model MathGPT, and demonstrate that it outperforms baselines on mathematical expression generation tasks.",
        "id":256901172
      }
    ],
    "negative_ctxs":[
      {
        "title":"Text Corpora and the Challenge of Newly Written Languages",
        "text":"Text corpora represent the foundation on which most natural language processing systems rely. However, for many languages, collecting or building a text corpus of a sufficient size still remains a complex issue, especially for corpora that are accessible and distributed under a clear license allowing modification (such as annotation) and further resharing. In this paper, we review the sources of text corpora usually called upon to fill the gap in low-resource contexts, and how crowdsourcing has been used to build linguistic resources. Then, we present our own experiments with crowdsourcing text corpora and an analysis of the obstacles we encountered. Although the results obtained in terms of participation are still unsatisfactory, we advocate that the effort towards a greater involvement of the speakers should be pursued, especially when the language of interest is newly written.",
        "id":218973897
      },
      {
        "title":"EmoTweet-28: A Fine-Grained Emotion Corpus for Sentiment Analysis",
        "text":"This paper describes EmoTweet-28, a carefully curated corpus of 15,553 tweets annotated with 28 emotion categories for the purpose of training and evaluating machine learning models for emotion classification. EmoTweet-28 is, to date, the largest tweet corpus annotated with fine-grained emotion categories. The corpus contains annotations for four facets of emotion: valence, arousal, emotion category and emotion cues. We first used small-scale content analysis to inductively identify a set of emotion categories that characterize the emotions expressed in microblog text. We then expanded the size of the corpus using crowdsourcing. The corpus encompasses a variety of examples including explicit and implicit expressions of emotions as well as tweets containing multiple emotions. EmoTweet-28 represents an important resource to advance the development and evaluation of more emotion-sensitive systems.",
        "id":6266911
      },
      {
        "title":"Fearless Steps APOLLO: Advanced Naturalistic Corpora Development",
        "text":"In this study, we present the Fearless Steps APOLLO Community Resource, a collection of audio and corresponding meta-data diarized from the NASA Apollo Missions. Massive naturalistic speech data which is time-synchronized, without any human subject privacy constraints is very rare and difficult to organize, collect, and deploy. The Apollo Missions Audio is the largest collection of multi-speaker multi-channel data, where over 600 personnel are communicating over multiple missions to achieve strategic space exploration goals. A total of 12 manned missions over a six-year period produced extensive 30-track 1-inch analog tapes containing over 150,000 hours of audio. This presents the wider research community a unique opportunity to extract multi-modal knowledge in speech science, team cohesion and group dynamics, and historical archive preservation. We aim to make this entire resource and supporting speech technology meta-data creation publicly available as a Community Resource for the development of speech and behavioral science. Here we present the development of this community resource, our outreach efforts, and technological developments resulting from this data. We finally discuss the planned future directions for this community resource.",
        "id":252624729
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there a paper that utilizes the characteristics of human evolutionary knowledge to guide language models in generating scientific ideas?",
    "positive_ctxs":[
      {
        "title":"Exploring and Verbalizing Academic Ideas by Concept Co-occurrence",
        "text":"Researchers usually come up with new ideas only after thoroughly comprehending vast quantities of literature. The difficulty of this procedure is exacerbated by the fact that the number of academic publications is growing exponentially. In this study, we devise a framework based on concept co-occurrence for academic idea inspiration, which has been integrated into a research assistant system. From our perspective, the fusion of two concepts that co-occur in an academic paper can be regarded as an important way of the emergence of a new idea. We construct evolving concept graphs according to the co-occurrence relationship of concepts from 20 disciplines or topics. Then we design a temporal link prediction method based on masked language model to explore potential connections between different concepts. To verbalize the newly discovered connections, we also utilize the pretrained language model to generate a description of an idea based on a new data structure called co-occurrence citation quintuple. We evaluate our proposed system using both automatic metrics and human assessment. The results demonstrate that our system has broad prospects and can assist researchers in expediting the process of discovering new ideas. 1 , et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer.",
        "id":259075344
      }
    ],
    "negative_ctxs":[
      {
        "title":"AntContentTech at SemEval-2023 Task 6: Domain-adaptive Pretraining and Auxiliary-task Learning for Understanding Indian Legal Texts",
        "text":"The objective of this shared task is to gain an understanding of legal texts, and it is beset with difficulties such as the comprehension of lengthy noisy legal documents, domain specificity as well as the scarcity of annotated data. To address these challenges, we propose a system that employs a hierarchical model and integrates domain-adaptive pretraining, data augmentation, and auxiliary-task learning techniques. Moreover, to enhance generalization and robustness, we ensemble the models that utilize these diverse techniques. Our system ranked first on the RR sub-task and in the middle for the other two sub-tasks. Our code is publicly available here 1 . * Corresponding author 1 https:\/\/github.com\/ContentTech\/ rhetorical-role-baseline",
        "id":259376501
      },
      {
        "title":"Streaming Cross Document Entity Coreference Resolution",
        "text":"Previous research in cross-document entity coreference has generally been restricted to the offline scenario where the set of documents is provided in advance. As a consequence, the dominant approach is based on greedy agglomerative clustering techniques that utilize pairwise vector comparisons and thus require O(n 2 ) space and time. In this paper we explore identifying coreferent entity mentions across documents in high-volume streaming text, including methods for utilizing orthographic and contextual information. We test our methods using several corpora to quantitatively measure both the efficacy and scalability of our streaming approach. We show that our approach scales to at least an order of magnitude larger data than previous reported methods.",
        "id":4854156
      },
      {
        "title":"Coarse Semantic Classification of Rare Nouns Using Cross-Lingual Data and Recurrent Neural Networks",
        "text":"The paper presents a method for WordNet supersense tagging of Sanskrit, an ancient Indian language with a corpus grown over four millenia. The proposed method merges lexical information from Sanskrit texts with lexicographic definitions from Sanskrit-English dictionaries, and compares the performance of two machine learning methods for this task. Evaluation concentrates on Vedic, the oldest layer of Sanskrit. This level of Sanskrit contains numerous rare words that are no longer used in the later language and whose word senses can, therefore, not be induced from their occurrences in other texts. The paper studies how to efficiently transfer knowledge from later forms of Sanskrit and from modern Western dictionaries for this special task of supersense disambiguation.",
        "id":32925047
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there a study that shows how to help the demonstration retriever better integrate feedback from LLMs?",
    "positive_ctxs":[
      {
        "title":"Unified Demonstration Retriever for In-Context Learning",
        "text":"In-context learning is a new learning paradigm where a language model conditions on a few input-output pairs (demonstrations) and a test input, and directly outputs the prediction. It has been shown highly dependent on the provided demonstrations and thus promotes the research of demonstration retrieval: given a test input, relevant examples are retrieved from the training set to serve as informative demonstrations for in-context learning. While previous works focus on training task-specific retrievers for several tasks separately, these methods are often hard to transfer and scale on various tasks, and separately trained retrievers incur a lot of parameter storage and deployment cost. In this paper, we propose Unified Demonstration Retriever (UDR), a single model to retrieve demonstrations for a wide range of tasks. To train UDR, we cast various tasks' training signals into a unified list-wise ranking formulation by language model's feedback. Then we propose a multi-task list-wise ranking training framework, with an iterative mining strategy to find high-quality candidates, which can help UDR fully incorporate various tasks' signals. Experiments on 30+ tasks across 13 task families and multiple data domains show that UDR significantly outperforms baselines. Further analyses show the effectiveness of each proposed component and UDR's strong ability in various scenarios including different LMs (1.3B ∼ 175B), unseen datasets, varying demonstration quantities, etc. * Equal Contribution † Corresponding Authors Retriever LM",
        "id":258557751
      }
    ],
    "negative_ctxs":[
      {
        "title":"Vector-Vector-Matrix Architecture: A Novel Hardware-Aware Framework for Low-Latency Inference in NLP Applications",
        "text":"Deep neural networks have become the standard approach to building reliable Natural Language Processing (NLP) applications, ranging from Neural Machine Translation (NMT) to dialogue systems. However, improving accuracy by increasing the model size requires a large number of hardware computations, which can slow down NLP applications significantly at inference time. To address this issue, we propose a novel vector-vector-matrix architecture (VVMA), which greatly reduces the latency at inference time for NMT. This architecture takes advantage of specialized hardware that has low-latency vector-vector operations and higher-latency vector-matrix operations. It also reduces the number of parameters and FLOPs for virtually all models that rely on efficient matrix multipliers without significantly impacting accuracy. We present empirical results suggesting that our framework can reduce the latency of sequence-to-sequence and Transformer models used for NMT by a factor of four. Finally, we show evidence suggesting that our VVMA extends to other domains, and we discuss novel hardware for its efficient use.",
        "id":223957092
      },
      {
        "title":"Self-Supervised Learning for Pairwise Data Refinement",
        "text":"Pairwise data automatically constructed from weakly supervised signals has been widely used for training deep learning models. Pairwise datasets such as parallel texts can have uneven quality levels overall, but usually contain data subsets that are more useful as learning examples. We present two methods to refine data that are aimed at obtaining that kind of subsets in a self-supervised way. Our methods are based on iteratively training dualencoder models to compute similarity scores. We evaluate our methods on de-noising parallel texts and training neural machine translation models. We find that: (i) The self-supervised refinement achieves most machine translation gains in the first iteration, but following iterations further improve its intrinsic evaluation. (ii) Machine translations can improve the de-noising performance when combined with selection steps. (iii) Our methods are able to reach the performance of a supervised method. Being entirely self-supervised, our methods are well-suited to handle pairwise data without the need of prior knowledge or human annotations.",
        "id":227905412
      },
      {
        "title":"The FINITE STRING Newsletter Site Report Controlling Complex Systems of Linguistic Rules",
        "text":"",
        "id":2666131
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there a tool that can automatically segment speech and the corresponding text transcriptions, to obtain a finer grained alignment?",
    "positive_ctxs":[
      {
        "title":"CMOT: Cross-modal Mixup via Optimal Transport for Speech Translation",
        "text":"End-to-end speech translation (ST) is the task of translating speech signals in the source language into text in the target language. As a cross-modal task, end-to-end ST is difficult to train with limited data. Existing methods often try to transfer knowledge from machine translation (MT), but their performances are restricted by the modality gap between speech and text. In this paper, we propose Cross-modal Mixup via Optimal Transport (CMOT) to overcome the modality gap. We find the alignment between speech and text sequences via optimal transport and then mix up the sequences from different modalities at a token level using the alignment. Experiments on the MuST-C ST benchmark demonstrate that CMOT achieves an average BLEU of 30.0 in 8 translation directions, outperforming previous methods. Further analysis shows CMOT can adaptively find the alignment between modalities, which helps alleviate the modality gap between speech and text.",
        "id":258866035
      }
    ],
    "negative_ctxs":[
      {
        "title":"Statistical versus symbolic parsing for captioned-information retrieval",
        "text":"",
        "id":1441199
      },
      {
        "title":"Towards Table-to-Text Generation with Pretrained Language Model: A Table Structure Understanding and Text Deliberating Approach",
        "text":"Although remarkable progress on the neural table-to-text methods has been made, the generalization issues hinder the applicability of these models due to the limited source tables. Large-scale pretrained language models sound like a promising solution to tackle such issues. However, how to effectively bridge the gap between the structured table and the text input by fully leveraging table information to fuel the pretrained model is still not well explored. Besides, another challenge of integrating the deliberation mechanism into the textto-text pretrained model for solving the tableto-text task remains seldom studied. In this paper, to implement the table-to-text generation with pretrained language model, we propose a table structure understanding and text deliberating approach, namely TASD. To be specific, we devise a three-layered multi-head attention network to realize the table-structureaware text generation model with the help of the pretrained language model. Furthermore, a multi-pass decoder framework is adopted to enhance the capability of polishing generated text for table descriptions. The empirical studies, as well as human evaluation, on two public datasets, validate that our approach can generate faithful and fluent descriptive texts for different types of tables.1",
        "id":255440553
      },
      {
        "title":"Predicting the presence of inline citations in academic text using binary classification",
        "text":"Properly citing sources is a crucial component of any good-quality academic paper. The goal of this study was to determine what kind of accuracy we could reach in predicting whether or not a sentence should contain an inline citation using a simple binary classification model. To that end, we fine-tuned SciBERT on both an imbalanced and a balanced dataset containing sentences with and without inline citations. We achieved an overall accuracy of over 0.92, suggesting that language patterns alone could be used to predict where inline citations should appear.",
        "id":258765297
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there an evaluation metric for natural language generation that predicts the factual consistency score through a mean-max aggregation method?",
    "positive_ctxs":[
      {
        "title":"ALIGNSCORE: Evaluating Factual Consistency with A Unified Alignment Function",
        "text":"Many text generation applications require the generated text to be factually consistent with input information. Automatic evaluation of factual consistency is challenging. Previous work has developed various metrics that often depend on specific functions, such as natural language inference (NLI) or question answering (QA), trained on limited data. Those metrics thus can hardly assess diverse factual inconsistencies (e.g., contradictions, hallucinations) that occur in varying inputs\/outputs (e.g., sentences, documents) from different tasks. In this paper, we propose ALIGNSCORE, a new holistic metric that applies to a variety of factual inconsistency scenarios as above. ALIGN-SCORE is based on a general function of information alignment between two arbitrary text pieces. Crucially, we develop a unified training framework of the alignment function by integrating a large diversity of data sources, resulting in 4.7M training examples from 7 well-established tasks (NLI, QA, paraphrasing, fact verification, information retrieval, semantic similarity, and summarization). We conduct extensive experiments on large-scale benchmarks including 22 evaluation datasets, where 19 of the datasets were never seen in the alignment training. ALIGNSCORE achieves substantial improvement over a wide range of previous metrics. Moreover, ALIGNSCORE (355M parameters) matches or even outperforms metrics based on ChatGPT and GPT-4 that are orders of magnitude larger. 1",
        "id":258947273
      }
    ],
    "negative_ctxs":[
      {
        "title":"Many speakers, many worlds Interannotator variations in the quantification of feature norms Many speakers, many worlds Interannotator variations in the quantification of feature norms",
        "text":"",
        "id":55488801
      },
      {
        "title":"ChiMed: A Chinese Medical Corpus for Question Answering",
        "text":"Question answering (QA) is a challenging task in natural language processing (NLP), especially when it is applied to specific domains. While models trained in the general domain can be adapted to a new target domain, their performance often degrades significantly due to domain mismatch. Alternatively, one can require a large amount of domain-specific QA data, but such data are rare, especially for the medical domain. In this study, we first collect a large-scale Chinese medical QA corpus called ChiMed; second we annotate a small fraction of the corpus to check the quality of the answers; third, we extract two datasets from the corpus and use them for the relevancy prediction task and the adoption prediction task. Several benchmark models are applied to the datasets, producing good results for both tasks.",
        "id":199379474
      },
      {
        "title":"",
        "text":"",
        "id":219305601
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there any dataset that contains minimally-contrasting social situations that lead to different decisions about which behaviors are appropriate in that situation?",
    "positive_ctxs":[
      {
        "title":"NORMBANK: A Knowledge Bank of Situational Social Norms",
        "text":"We present NORMBANK, a knowledge bank of 155k situational norms. This resource is designed to ground flexible normative reasoning for interactive, assistive, and collaborative AI systems. Unlike prior commonsense resources, NORMBANK grounds each inference within a multivalent sociocultural frame, which includes the setting (e.g., restaurant), the agents' contingent roles (waiter, customer), their attributes (age, gender), and other physical, social, and cultural constraints (e.g., the temperature or the country of operation). In total, NORMBANK contains 63k unique constraints from a taxonomy that we introduce and iteratively refine here. Constraints then apply in different combinations to frame social norms. Under these manipulations, norms are non-monotonic -one can cancel an inference by updating its frame even slightly. Still, we find evidence that neural models can help reliably extend the scope and coverage of NORMBANK. We further demonstrate the utility of this resource with a series of transfer experiments. For data and code, see https:\/\/github.com\/SALT-NLP\/normbank",
        "id":258947336
      }
    ],
    "negative_ctxs":[
      {
        "title":"DECISION LISTS FOR LEXICAL AMBIGUITY RESOLUTION: Application to Accent Restoration in Spanish and French",
        "text":"This paper presents a statistical decision procedure for lexical ambiguity resolution. The algorithm exploits both local syntactic patterns and more distant collocational evidence, generating an efficient, effective, and highly perspicuous recipe for resolving a given ambiguity. By identifying and utilizing only the single best disambiguating evidence in a target context, the algorithm avoids the problematic complex modeling of statistical dependencies. Although directly applicable to a wide class of ambiguities, the algorithm is described and evaluated in a realistic case study, the problem of restoring missing accents in Spanish and French text. Current accuracy exceeds 99% on the full task, and typically is over 90% for even the most difficult ambiguities.",
        "id":1580335
      },
      {
        "title":"",
        "text":"Semantic role labeling offers vital information for both Linguistics and Natural Language Processing tasks. In this article, we present a lexical resource for Portuguese annotated with semantic roles: VerbLexPor. The resource is a database with verbs and sentences extracted from both a domain specific corpus and a non-specialized generic one. Annotation was manually carried out by a linguist using VerbNet-like semantic roles. The resource has more than 6 thousand annotated sentences and 15 thousand annotated arguments, and is available for download as XML or SQL files. The paper also describes a comparative analysis between the two corpora, showing that the distribution of semantic roles in a general domain is different from that in specific domain.",
        "id":45930698
      },
      {
        "title":"CODEGEN: AN OPEN LARGE LANGUAGE MODEL FOR CODE WITH MULTI-TURN PROGRAM SYNTHESIS",
        "text":"Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multiturn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https:\/\/github.com\/salesforce\/CodeGen.",
        "id":252668917
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_acl",
    "question":"Is there any paper about style transfer for stories?",
    "positive_ctxs":[
      {
        "title":"StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing",
        "text":"Non-parallel text style transfer is an important task in natural language generation. However, previous studies concentrate on the token or sentence level, such as sentence sentiment and formality transfer, but neglect long style transfer at the discourse level. Long texts usually involve more complicated author linguistic preferences such as discourse structures than sentences. In this paper, we formulate the task of non-parallel story author-style transfer, which requires transferring an input story into a specified author style while maintaining source semantics. To tackle this problem, we propose a generation model, named StoryTrans, which leverages discourse representations to capture source content information and transfer them to target styles with learnable style embeddings. We use an additional training objective to disentangle stylistic features from the learned discourse representation to prevent the model from degenerating to an auto-encoder. Moreover, to enhance content preservation, we design a mask-and-fill framework to explicitly fuse style-specific keywords of source texts into generation. Furthermore, we constructed new datasets for this task in Chinese and English, respectively. Extensive experiments show that our model outperforms strong baselines in overall performance of style transfer and content preservation. * Equal contribution. † Corresponding author 郭翰是古时候一名才子。一个夏日的晚上，他在院中乘凉。忽 然，一阵风起，送来一股沁人心脾的清香，一位少女驾着白云从 天而降，出现在郭翰眼前 … Guo Han was a talented man in ancient times. One summer evening, he was enjoying the cool in the courtyard. Suddenly, a gust of wind brought a refreshing fragrance, and a young girl descended from the sky on a white cloud and appeared in front of Guo Han … 郭翰在夏日的夜,院中花香沁人心脾,一阵清香从身旁飘来,那少女 却是神色自若,一言不发的从天而降。郭翰大惊,眼前白光一闪,身 前纱衣一晃,已被她夺了过去。… On a summer night, the scent of flowers is refreshing in the courtyard，a scent of fragrance floats from Guo Han side. A young girl, with a calm expression, fell from the sky without saying a word. Guo Han was shocked, and already taken away by her with a white light flashing and the gauze flickering. … Source Text: Generated Text for JY Style:",
        "id":251903643
      }
    ],
    "negative_ctxs":[
      {
        "title":"Improving Event Detection via Open-domain Trigger Knowledge",
        "text":"Event Detection (ED) is a fundamental task in automatically structuring texts. Due to the small scale of training data, previous methods perform poorly on unseen\/sparsely labeled trigger words and are prone to overfitting densely labeled trigger words. To address the issue, we propose a novel Enrichment Knowledge Distillation (EKD) model to leverage external open-domain trigger knowledge to reduce the in-built biases to frequent trigger words in annotations. Experiments on benchmark ACE2005 show that our model outperforms nine strong baselines, is especially effective for unseen\/sparsely labeled trigger words. The source code is released on https:\/\/github.com\/shuaiwa16\/ekd.git.",
        "id":220047190
      },
      {
        "title":"FHAC at GermEval 2021: Identifying German toxic, engaging, and fact-claiming comments with ensemble learning",
        "text":"The availability of language representations learned by large pretrained neural network models (such as BERT and ELECTRA) has led to improvements in many downstream Natural Language Processing tasks in recent years. Pretrained models usually differ in pretraining objectives, architectures, and datasets they are trained on which can affect downstream performance. In this contribution, we fine-tuned German BERT and German ELECTRA models to identify toxic (subtask 1), engaging (subtask 2), and fact-claiming comments (subtask 3) in Facebook data provided by the GermEval 2021 competition. We created ensembles of these models and investigated whether and how classification performance depends on the number of ensemble members and their composition. On out-of-sample data, our best ensemble achieved a macro-F1 score of 0.73 (for all subtasks), and F1 scores of 0.72, 0.70, and 0.76 for subtasks 1, 2, and 3, respectively.",
        "id":263850902
      },
      {
        "title":"A complete understanding speech system based on semantic concepts",
        "text":"In this work, we present a complete speech understanding system based on our speech recognizer: ESPERE. The input signal is processed and the best sentence is then proposed to the understanding module. In our case, the understanding problem is considered as a matching process between two different languages. At the entry, the request expressed in natural language and at the output the corresponding SQL form. The SQL request is obtained after an intermediate step in which the entry is expressed in terms of concepts. A concept represents a given meaning, it is defined by a set of words sharing the same semantic properties. In this paper, we propose a new Bayesian classifier to automatically extract the underlined concepts. We also propose a new approach for vector representation of words. Then, we describe the postprocessing step during which, we label our sentences and we generate the corresponding SQL queries. We conclude our paper by describing the integration step of our understanding module in a complete platform of human-machine oral intercation.",
        "id":42444707
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_acl",
    "question":"Is there any paper exploring real speakers and thus performing multimodal emotion recognition task?",
    "positive_ctxs":[
      {
        "title":"A Facial Expression-Aware Multimodal Multi-task Learning Framework for Emotion Recognition in Multi-party Conversations",
        "text":"Multimodal Emotion Recognition inMultiparty Conversations (MERMC)has recently attracted considerable attention. Due to the complexity of visual scenes in multi-party conversations, most previous MERMC studies mainly focus on text and audio modalities while ignoring visual information. Recently, several works proposed to extract face sequences as visual features and have shown the importance of visual information in MERMC. However, given an utterance, the face sequence extracted by previous methods may contain multiple people's faces, which will inevitably introduce noise to the emotion prediction of the real speaker. To tackle this issue, we propose a two-stage framework named Facial expressionaware Multimodal Multi-Task learning (Fa-cialMMT). Specifically, a pipeline method is first designed to extract the face sequence of the real speaker of each utterance, which consists of multimodal face recognition, unsupervised face clustering, and face matching. With the extracted face sequences, we propose a multimodal facial expression-aware emotion recognition model, which leverages the frame-level facial emotion distributions to help improve utterance-level emotion recognition based on multi-task learning. Experiments demonstrate the effectiveness of the proposed FacialMMT framework on the benchmark MELD dataset. The source code is publicly released at https: \/\/github.com\/NUSTM\/FacialMMT.",
        "id":259370790
      }
    ],
    "negative_ctxs":[
      {
        "title":"New features in Spoken Language Search Hawk (SpLaSH): Query Language and Query Sequence",
        "text":"In this work we present further development of the SpLaSH (Spoken Language Search Hawk) project. SpLaSH implements a data model for annotated speech corpora integrated with textual markup (i.e. POS tagging, syntax, pragmatics) including a toolkit used to perform complex queries across speech and text labels. The integration of time aligned annotations (TMA), represented making use of Annotation Graphs, with text aligned ones (TXA), stored in generic XML files, are provided by a data structure, the Connector Frame, acting as table-look-up linking temporal data to words in the text. SpLaSH imposes a very limited number of constraints to the data model design, allowing the integration of annotations developed separately within the same dataset and without any relative dependency. It also provides a GUI allowing three types of queries: simple query on TXA or TMA structures, sequence query on TMA structure and cross query on both TXA and TMA integrated structures. In this work new SpLaSH features will be presented: SpLaSH Query Language (SpLaSHQL) and Query Sequence.",
        "id":2306140
      },
      {
        "title":"",
        "text":"",
        "id":219304499
      },
      {
        "title":"AmritaCEN at SemEval-2016 Task 11: Complex Word Identification using Word Embedding",
        "text":"Complex word identification task focuses on identifying the difficult word from English sentence for a Non-Native speakers. Non-Native speakers are those who don't have English as their native language. It is a subtask for lexical simplification. We have experimented with word embedding features, orthographic word features, similarity features and POS tag features which improves the performance of the classification. In addition to the SemEval 2016 results we have evaluated the training data by varying the vector dimension size and obtained the best possible size for producing better performance. The SVM learning algorithm will attains constant and maximum accuracy through linear classifier. We achieve a G-score of 0.43 and 0.54 on computing complex words for two systems.",
        "id":219309661
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there any paper leverages knowledge distillation of language models for textual out-of-distribution detection or anomaly detection?",
    "positive_ctxs":[
      {
        "title":"Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text",
        "text":"Self-supervised representation learning has proved to be a valuable component for outof-distribution (OoD) detection with only the texts of in-distribution (ID) examples. These approaches either train a language model from scratch or fine-tune a pre-trained language model using ID examples, and then take the perplexity output by the language model as OoD scores. In this paper, we analyze the complementary characteristics of both OoD detection methods and propose a multi-level knowledge distillation approach that integrates their strengths while mitigating their limitations. Specifically, we use a fine-tuned model as the teacher to teach a randomly initialized student model on the ID examples. Besides the prediction layer distillation, we present a similarity-based intermediate layer distillation method to thoroughly explore the representation space of the teacher model. In this way, the learned student can better represent the ID data manifold while gaining a stronger ability to map OoD examples outside the ID data manifold with the regularization inherited from pre-training. Besides, the student model sees only ID examples during parameter learning, further promoting more distinguishable features for OoD detection. We conduct extensive experiments over multiple benchmark datasets, i.e., CLINC150, SST, ROSTD, 20 NewsGroups, and AG News; showing that the proposed method yields new state-of-the-art performance 1 . We also explore its application as an AIGC detector to distinguish between answers generated by ChatGPT and human experts. It is observed that our model exceeds human evaluators in the pair-expert task on the Human ChatGPT Comparison Corpus.",
        "id":253734483
      }
    ],
    "negative_ctxs":[
      {
        "title":"Using any machine translation source for fuzzy-match repair in a computer-aided translation setting",
        "text":"When a computer-assisted translation (CAT) tool does not find an exact match for the source segment to translate in its translation memory (TM), translators must use fuzzy matches that come from translation units in the translation memory that do not completely match the source segment. We explore the use of a fuzzy-match repair technique called patching to repair translation proposals from a TM in a CAT environment using any available machine translation system, or any external bilingual source, regardless of its internals. Patching attempts to aid CAT tool users by repairing fuzzy matches and proposing improved translations. Our results show that patching improves the quality of translation proposals and reduces the amount of edit operations to perform, especially when a specific set of restrictions is applied. use a fuzzy-match score threshold (FMT), for instance 80%, to reduce the number of translation proposals.When a perfect match is not found in the translation memory, and before making any changes to the TL segment t in the proposed translation unit, the translator has to identify the sub-segments of t that correspond to the sub-segments of s that are not common to s . To help in finding the sub-segments of t that need to be edited, but without actually editing them, Esplà-Gomis et al. (2011) use machine translation (MT) to find sub-segment alignments between s and t, and train a classifier to classify the words in t as words to be kept unedited or words to be changed to transform t into the desired translation t .Other researchers have gone one step further and have explored different ways to combine the TL segment of the proposed translation unit and the output of a statistical machine translation (SMT) system to produce a translation closer to t . Biçici and Dymetman(2008), for example, use a phrase-based SMT system trained on a bilingual corpus in the same domain as the TM and combine it with the TM's fuzzy match by extracting a phrase table that is dynamically added to the usual set of bi-phrases used for decoding the source. Their implementation augments the internal translation table in the SMT system with bilingual discontiguous sub-segments (phrases) that have source sub-segments in common with s . Alignments in the system created byBiçici and Dymetman (2008)are detected using word alignments directly obtained from the SMT system training process and are used to find the parts of t that need to be edited (mismatches).Similarly, Simard and Isabelle (2009) use a phrase-based SMT system by adding phrase pairs (sub-segment pairs) of any length (obtained using a statistical aligner on the TM) to the SMT system's phrase table and introduce a feature to indicate that the phrase-pairs came from their TM. After that, they optimize the weighting of the TM-based phrase table in a regular SMT decoder. By means of optimization and phrase table inclusion they are able to make their SMT system produce a translation close to the desired translation t .Additional work done by Zhechev and Genabith (2010) makes use of a phrase-based SMT system along with an alignment method that, like Simard and Isabelle (2009), connects subsegments from the target translation t with those in s. The alignment method Zhechev and Genabith (2010) use takes advantage of a tree-based structural alignment created from a bilingual dictionary after training their SMT system with phrase pairs. After aligning the words in s with those in t, Zhechev and Genabith (2010) are able to identify words that should appear in the final translation t .Koehn and Senellart (2010) take a similar approach toBiçici and Dymetman (2008). They first align words in s and s to find mismatches. Then, they align the words in s and t to identify target matches and remove the words in t that are aligned to the mismatched words in s. Target mismatches are sent to the SMT decoder for translation. Mismatched words in Koehn and Senellart (2010)'s system are treated separately; that is, context around a mismatch, while indirectly taken into account by the language model, is not directly taken into account of when applying phrase pairs. Ma et al.(2011), on the other hand, decided to research the shortcomings of using a fuzzymatch score as a threshold for determining translation unit matches that serve as translations for other segments. Ma et al. (2011)'s approach uses discriminative learning and support vector machines to salvage translations of matched words to select a translation unit that would have been otherwise thrown away due to the fuzzy-match score being used as a threshold. Their work, unlike Koehn and Senellart (",
        "id":16546204
      },
      {
        "title":"Using Shallow Syntactic Features to Measure Influences of L1 and Proficiency Level in EFL Writings",
        "text":"This paper proposes a framework for modeling and analyzing differences between texts written by different subgroups of learners of English as a Foreign Language (organized according to native language (L1) and proficiency level). Using frequency vectors of both POS-trigrams and mixed POS and function word trigrams, we compare learner language variants both to each other and to native English, German, and Chinese texts. We introduce the trigram usage factor metric for identifying sequences that are especially characteristic of a particular subgroup of learners. We show that distance between learner English and native English decreases with proficiency. Next we compare the distance between learner English and other native languages. Finally, we show that automatic proficiency classification benefits from using L1-specific classifiers. 2015. Using shallow syntactic features to measure influences of L1 and proficiency level in EFL writings.",
        "id":14037433
      },
      {
        "title":"Asynchronous Binarization for Synchronous Grammars",
        "text":"Binarization of n-ary rules is critical for the efficiency of syntactic machine translation decoding. Because the target side of a rule will generally reorder the source side, it is complex (and sometimes impossible) to find synchronous rule binarizations. However, we show that synchronous binarizations are not necessary in a two-stage decoder. Instead, the grammar can be binarized one way for the parsing stage, then rebinarized in a different way for the reranking stage. Each individual binarization considers only one monolingual projection of the grammar, entirely avoiding the constraints of synchronous binarization and allowing binarizations that are separately optimized for each stage. Compared to n-ary forest reranking, even simple target-side binarization schemes improve overall decoding accuracy.",
        "id":5237589
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there any paper that address attacks on code models by leveraging the semantic information of the source code through attention scores, while also guaranteeing that the generated adversarial examples can always be compiled successfully?",
    "positive_ctxs":[
      {
        "title":"DIP: Dead code Insertion based Black-box Attack for Programming Language Model",
        "text":"Automatic processing of source code, such as code clone detection and software vulnerability detection, is very helpful to software engineers. Large pre-trained Programming Language (PL) models (such as CodeBERT, Graph-CodeBERT, CodeT5, etc.), show very powerful performance on these tasks. However, these PL models are vulnerable to adversarial examples that are generated with slight perturbation. Unlike natural language, an adversarial example of code must be semantic-preserving and compilable. Due to the requirements, it is hard to directly apply the existing attack methods for natural language models. In this paper, we propose DIP (Dead code Insertion based Blackbox Attack for Programming Language Model), a high-performance and efficient black-box attack method to generate adversarial examples using dead code insertion. We evaluate our proposed method on 9 victim downstream-task large code models. Our method outperforms the state-of-the-art black-box attack in both attack efficiency and attack quality, while generated adversarial examples are compiled preserving semantic functionality.",
        "id":259370836
      }
    ],
    "negative_ctxs":[
      {
        "title":"The LOIS Project",
        "text":"The legal knowledge base resulting from the LOIS (Lexical Ontologies for legal Information Sharing) project consists of legal WordNets in six languages (Italian, Dutch, Portuguese, German, Czech, English). Its architecture is based on the EuroWordNet (EWN) framework(Vossen et al, 1997). Using the EWN framework assures compatibility of the LOIS WordNets with EWN, allowing them to function as an extension of EWN for the legal domain. For each legal system, the document-derived legal concepts are integrated into a taxonomy, which links into existing formal ontologies. These give the legal wordnets a first formal backbone, which can, in future, be further extended. The database consists of 33,000 synsets, and is aimed to be used in information retrieval, where it provides mono-and multi-lingual access to European legal databases for legal experts as well as for laymen. The LOIS knowledge base also provides a flexible, modular architecture that allows integration of multiple classification schemes, and enables the comparison of legal systems by exploring translation, equivalence and structure across the different legal wordnets.",
        "id":7759514
      },
      {
        "title":"Capturing the Content of a Document through Complex Event Identification",
        "text":"Granular events, instantiated in a document by predicates, can usually be grouped into more general events, called complex events. Together, they capture the major content of the document. Recent work grouped granular events by defining event regions, filtering out sentences that are irrelevant to the main content. However, this approach assumes that a given complex event is always described in consecutive sentences, which does not always hold in practice. In this paper, we introduce the task of complex event identification. We address this task as a pipeline, first predicting whether two granular events mentioned in the text belong to the same complex event, independently of their position in the text, and then using this to cluster them into complex events. Due to the difficulty of predicting whether two granular events belong to the same complex event in isolation, we propose a context-augmented representation learning approach CONTEXTRL that adds additional context to better model the pairwise relation between granular events. We show that our approach outperforms strong baselines on the complex event identification task and further present a promising case study exploring the effectiveness of using complex events as input for document-level argument extraction. 1 .",
        "id":250390606
      },
      {
        "title":"大 大 大模 模 模型 型 型与 与 与知 知 知识 识 识图 图 图谱 谱 谱 Large Language Models and Knowledge Graphs",
        "text":"As an important form of knowledge organization, knowledge graphs are widely recognized as one of the foundational infrastructures for the next generation of artificial intelligence technologies, receiving considerable interest from both industry and academia. Traditional methods for representing knowledge graphs mainly employ symbolic representations to explicitly describe concepts and their relationships, with clear semantics and good interpretability. However, these methods have limited coverage of knowledge types, making it challenging to apply them in open-domain scenarios. With the development of large pre-trained language models (large language models), most researchers have considered parameterized large language models as knowledge graphs. Thus, this paper focuses on the research of the life cycle of knowledge graphs in large language models. Specifically, we summarize the related work on knowledge modeling, knowledge acquisition, knowledge fusion, knowledge management, knowledge reasoning, and knowledge application. Finally, we anticipate the future development trends of large language models and knowledge graphs.",
        "id":261341583
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there any paper that aligns speech and text embeddings better than CTC training?",
    "positive_ctxs":[
      {
        "title":"WACO: Word-Aligned Contrastive Learning for Speech Translation",
        "text":"End-to-end Speech Translation (E2E ST) aims to directly translate source speech into target text. Existing ST methods perform poorly when only extremely small speech-text data are available for training. We observe that an ST model's performance closely correlates with its embedding similarity between speech and source transcript. In this paper, we propose Word-Aligned COntrastive learning (WACO), a simple and effective method for extremely low-resource speech-to-text translation. Our key idea is bridging word-level representations for both speech and text modalities via contrastive learning. We evaluate WACO and other methods on the MuST-C dataset, a widely used ST benchmark, and on a low-resource direction Maltese-English from IWSLT 2023. Our experiments demonstrate that WACO outperforms the best baseline by 9+ BLEU points with only 1-hour parallel ST data. Code is available at https:\/\/github.com\/owaski\/WACO.",
        "id":254854514
      }
    ],
    "negative_ctxs":[
      {
        "title":"THE STATISTICAL SIGNIFICANCE OF THE MUC-4 RESULTS ELEMENTS OF HYPOTHESIS TESTING",
        "text":"INTRODUCTIONThe MUC-4 scores of recall, precision, and the F-measures are used to measure the performance of the participating systems. The differences in the scores between any two systems may be due to chance or may be due to a significant difference between the two systems. To rule out the possibility that the difference is due to chance, statistical hypothesis testing is used. The method of hypothesis testing used is a computationally-intensive method known as approximate randomization. The method and the statistical significance of the results for the two MUC-4 test sets, TST3 and TST4, will be discussed in this paper.In our hypothesis testing, our objective was to determine whether a system is characteristically different from another system. This was achieved by comparing two systems to see if their actual difference in performance stands out in comparison with the results for random combinations of their scores. If their actual difference stands out, then we know that this difference could not have arisen by chance.",
        "id":53750613
      },
      {
        "title":"Still on arguments and adjuncts: the status of the indirect object and the adverbial adjunct relations in Universal Dependencies for Portuguese ⋆",
        "text":"We report the process of annotating verbal arguments and adjuncts in PetroGold, a treebank of the oil & gas domain. The corpus follows the dependencies approach of the Universal Dependencies multilingual project. The argument-adjunct distinction in UD is not a relevant one, and it is up to the contributors of each language to decide how to annotate it in some particular cases. After consulting Portuguese grammars to assist in the annotation of the adverbial adjunct and indirect object relations, we propose a semantic-discursively oriented approach, which was used in the PetroGold annotation and affected 14.8% of the sentences in the treebank. Finally, we present a visualization of the results, showing the distribution of verbs by transitivity in the corpus.",
        "id":249240914
      },
      {
        "title":"Resource Sharing System for Humanity Researches",
        "text":"The NIJL has developed variety kinds of databases, i.e., catalogue databases, image databases, movie databases, and full text databases. As these systems have been developed under different backgrounds, users have to learn different command for each database. Furthermore, although some databases have similar contents, users cannot access related information unless they understand NIJL database system well. This paper describes NIJL's new resource sharing system, called \"NIJL Collaboration System,\" to solve above problems. The \"NIJL Collaboration System\" is an ongoing project involving data conversion to XML and developing platform independent data manipulation system for a distributed environment. The essential of the project is to introduce XML as a common data description, Dublin Core meta-data as a common access points to databases, and Z39.50 as a common searching protocol. This system enables users to access various sorts of multimedia data in distributed databases on the WEB seamlessly by a single graphical user interface.",
        "id":2250947
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there any paper that applies curriculum learning to various NLG tasks without depending on specific metrics?",
    "positive_ctxs":[
      {
        "title":"In-sample Curriculum Learning by Sequence Completion for Natural Language Generation",
        "text":"Curriculum learning has shown promising improvements in multiple domains by training machine learning models from easy samples to hard ones. Previous works which either design rules or train models for scoring the difficulty highly rely on task-specific expertise, and cannot generalize. Inspired by the \"easy-to-hard\" intuition, we propose to do in-sample curriculum learning for natural language generation tasks. Our learning strategy starts training the model to generate the last few words, i.e., do sequence completion, and gradually extends to generate the whole output sequence. Comprehensive experiments show that it generalizes well to different tasks and achieves significant improvements over strong baselines.",
        "id":253734400
      }
    ],
    "negative_ctxs":[
      {
        "title":"LINGUISTIC ANALYSIS OF NATURAL LANGUAGE COMMUNICATION WITH COMPUTERS",
        "text":"",
        "id":1010309
      },
      {
        "title":"Building a Lexicon of Formulaic Language for Language Learners",
        "text":"Though the multiword lexicon has long been of interest in computational linguistics, most relevant work is targeted at only a small portion of it. Our work is motivated by the needs of learners for more comprehensive resources reflecting formulaic language that goes beyond what is likely to be codified in a dictionary. Working from an initial sequential segmentation approach, we present two enhancements: the use of a new measure to promote the identification of lexicalized sequences, and an expansion to include sequences with gaps. We evaluate using a novel method that allows us to calculate an estimate of recall without a reference lexicon, showing that good performance in the second enhancement depends crucially on the first, and that our lexicon conforms much more with human judgment of formulaic language than alternatives.",
        "id":8637963
      },
      {
        "title":"ACCURATE AND SCALABLE ESTIMATION OF EPISTEMIC UNCERTAINTY FOR GRAPH NEURAL NETWORKS",
        "text":"Safe deployment of graph neural networks (GNNs) under distribution shift requires models to provide accurate confidence indicators (CI).However, while it is wellknown in computer vision that CI quality diminishes under distribution shift, this behavior remains understudied for GNNs.Hence, we begin with a case study on CI calibration under controlled structural and feature distribution shifts and demonstrate that increased expressivity or model size do not always lead to improved CI performance.Consequently, we instead advocate for the use of epistemic uncertainty quantification (UQ) methods to modulate CIs.To this end, we propose G-∆UQ, a new single model UQ method that extends the recently proposed stochastic centering framework to support structured data and partial stochasticity.Evaluated across covariate, concept, and graph size shifts, G-∆UQ not only outperforms several popular UQ methods in obtaining calibrated CIs, but also outperforms alternatives when CIs are used for generalization gap prediction or OOD detection.Overall, our work not only introduces a new, flexible GNN UQ method, but also provides novel insights into GNN CIs on safety-critical tasks.",
        "id":262065523
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there any paper that applies symbolic distillation on black-box generalist language models to harvest high-quality counterfactual data for out-of-distribution generalization?",
    "positive_ctxs":[
      {
        "title":"DISCO: Distilling Counterfactuals with Large Language Models",
        "text":"Models trained with counterfactually augmented data learn representations of the causal structure of tasks, enabling robust generalization. However, high-quality counterfactual data is scarce for most tasks and not easily generated at scale. When crowdsourced, such data is typically limited in scale and diversity; when generated using supervised methods, it is computationally expensive to extend to new counterfactual dimensions. In this work, we introduce DISCO (DIStilled COunterfactual Data), a new method for automatically generating highquality counterfactual data at scale. DISCO engineers prompts to generate phrasal perturbations with a large general language model. Then, a task-specific teacher model filters these generations to distill high-quality counterfactual data. While task-agnostic, we apply our pipeline to the task of natural language inference (NLI) and find that on challenging evaluations such as the NLI stress test, comparatively smaller student models trained with DISCOgenerated counterfactuals are more robust (6% absolute) and generalize better across distributions (2%) compared to models trained without data augmentation. Furthermore, DISCOaugmented models are 10% more consistent between counterfactual pairs on three evaluation sets, demonstrating that DISCO augmentation enables models to more reliably learn causal representations. Our repository are available at: https:\/\/github.com\/eric11eca\/disco * Work done while at the Allen Institute for AI. Equal contribution Span Extraction Counterfactual Data Easy Ambigious Hard Counterfactual Over-generation Data Selection Seed Dataset Original: A young girl looks up as she rides on a merry-go-round. Spans: A young girl looks up as she rides on a merry-go-round. Counterfactual: A young girl looks up at a very tall roller coaster with an eager and excited look on her face. General LLM (GPT-3) Specialized Teacher Model Filtering Student Model Data Augmentation",
        "id":254877039
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":226283810
      },
      {
        "title":"Extraction d'information de sous-catégorisation à partir des tables du LADL 1",
        "text":"Les tables du LADL (Laboratoire d'Automatique Documentaire et Linguistique) contiennent des données électroniques extensives sur les propriétés morphosyntaxiques et syntaxiques des foncteurs syntaxiques du français (verbes, noms, adjectifs). Ces données, dont on sait qu'elles sont nécessaires pour le bon fonctionnement des systèmes de traitement automatique des langues, ne sont cependant que peu utilisées par les systèmes actuels. Dans cet article, nous identifions les raisons de cette lacune et nous proposons une méthode de conversion des tables vers un format mieux approprié au traitement automatique des langues.AbstractMaurice Gross' grammar lexicon contains rich and exhaustive information about the morphosyntactic and syntactic properties of French syntactic functors (verbs, adjectives, nouns). Yet its use within natural language processing systems is hampered both by its non standard encoding and by a structure that is partly implicit and partly underspecified. In this paper, we present a method for translating this information into a format more amenable for use by NLP systems, we discuss the results obtained so far, we compare our approach with related work and we identify the possible further uses that can be made of the reformatted information.",
        "id":61992109
      },
      {
        "title":"Improving reordering performance using higher order and structural features",
        "text":"Recent work has shown that word aligned data can be used to learn a model for reordering source sentences to match the target order. This model learns the cost of putting a word immediately before another word and finds the best reordering by solving an instance of the Traveling Salesman Problem (TSP). However, for efficiently solving the TSP, the model is restricted to pairwise features which examine only a pair of words and their neighborhood. In this work, we go beyond these pairwise features and learn a model to rerank the n-best reorderings produced by the TSP model using higher order and structural features which help in capturing longer range dependencies. In addition to using a more informative set of source side features, we also capture target side features indirectly by using the translation score assigned to a reordering. Our experiments, involving Urdu-English, show that the proposed approach outperforms a state-of-theart PBSMT system which uses the TSP model for reordering by 1.3 BLEU points, and a publicly available state-of-the-art MT system, Hiero, by 3 BLEU points.",
        "id":313465
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there any paper that attempts to evaluate the similarity of meaning representations without using annotated data?",
    "positive_ctxs":[
      {
        "title":"Evaluate AMR Graph Similarity via Self-supervised Learning",
        "text":"In work on AMR (Abstract Meaning Representation), similarity metrics are crucial as they are used to evaluate AMR systems such as AMR parsers. Current AMR metrics are all based on nodes or triples matching without considering the entire structures of AMR graphs. To address this problem, and inspired by learned similarity evaluation on plain text, we propose AMRSim, an automatic AMR graph similarity evaluation metric. To overcome the high cost of collecting human-annotated data, AMRSim automatically generates silver AMR graphs and utilizes self-supervised learning methods. We evaluated AMRSim on various datasets and found that AMRSim significantly improves the correlations with human semantic scores and remains robust under diverse challenges. We also discuss how AMRSim can be extended to multilingual cases. 1",
        "id":259370579
      }
    ],
    "negative_ctxs":[
      {
        "title":"BERTChem-DDI : Improved Drug-Drug Interaction Prediction from text using Chemical Structure Information",
        "text":"Traditional biomedical version of embeddings obtained from pre-trained language models have recently shown state-of-the-art results for relation extraction (RE) tasks in the medical domain.In this paper, we explore how to incorporate domain knowledge, available in the form of molecular structure of drugs, for predicting Drug-Drug Interaction from textual corpus. We propose a method, BERTChem-DDI, to efficiently combine drug embeddings obtained from the rich chemical structure of drugs (encoded in SMILES) along with off-the-shelf domain-specific BioBERT embedding-based RE architecture. Experiments conducted on the DDIExtraction 2013 corpus clearly indicate that this strategy improves other strong baselines architectures by 3.4% macro F1-score.",
        "id":227905489
      },
      {
        "title":"Transferable Neural Projection Representations",
        "text":"Neural word representations are at the core of many state-of-the-art natural language processing models. A widely used approach is to pretrain, store and look up word or character embedding matrices. While useful, such representations occupy huge memory making it hard to deploy on-device and often do not generalize to unknown words due to vocabulary pruning.",
        "id":174797895
      },
      {
        "title":"Spelling Correction for Estonian Learner Language",
        "text":"Second and foreign language (L2) learners tend to make specific spelling errors compared to native speakers. Languageindependent spell-checking algorithms that rely on n-gram models can offer a simple solution for improving learner error detection and correction due to context-sensitivity. As the open-source speller previously available for Estonian is rule-based, our aim was to evaluate the performance of bi-and trigrambased statistical spelling correctors on an error-tagged set of A2-C1-level texts written by L2 learners of Estonian. The newly trained spell-checking models were compared to existing correction tools (open-source and commercial). Then, the best-performing Jamspell corrector was trained on various datasets to analyse their effect on the correction results.",
        "id":258765264
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there any paper that automatically creates a dataset for summarizing text from one language to another for a large collection of languages?",
    "positive_ctxs":[
      {
        "title":"CrossSum: Beyond English-Centric Cross-Lingual Summarization for 1,500+ Language Pairs",
        "text":"We present CrossSum, a large-scale crosslingual summarization dataset comprising 1.68 million article-summary samples in 1,500+ language pairs. We create CrossSum by aligning parallel articles written in different languages via cross-lingual retrieval from a multilingual abstractive summarization dataset and perform a controlled human evaluation to validate its quality. We propose a multistage data sampling algorithm to effectively train a cross-lingual summarization model capable of summarizing an article in any target language. We also introduce LaSE, an embedding-based metric for automatically evaluating model-generated summaries. LaSE is strongly correlated with ROUGE and, unlike ROUGE, can be reliably measured even in the absence of references in the target language. Performance on ROUGE and LaSE indicate that our proposed model consistently outperforms baseline models. To the best of our knowledge, CrossSum is the largest cross-lingual summarization dataset and the first ever that is not centered around English. We are releasing the dataset, training and evaluation scripts, and models to spur future research on cross-lingual summarization. The resources can be found at https: \/\/github.com\/csebuetnlp\/CrossSum.",
        "id":258947845
      }
    ],
    "negative_ctxs":[
      {
        "title":"Docalog: Multi-document Dialogue System using Transformer-based Span Retrieval",
        "text":"Information-seeking dialogue systems, including knowledge identification and response generation, aim to respond to users with fluent, coherent, and informative answers based on users' needs. This paper discusses our proposed approach, Docalog, for the DialDoc-22 (Multi-Doc2Dial) shared task. Docalog identifies the most relevant knowledge in the associated document, in a multi-document setting. Docalog, is a three-stage pipeline consisting of (1) a document retriever model (DR. TEIT), (2) an answer span prediction model, and (3) an ultimate span picker deciding on the most likely answer span, out of all predicted spans. In the test phase of MultiDoc2Dial 2022, Docalog achieved f1scores of 36.07% and 28.44% and SacreBLEU scores of 23.70% and 20.52%, respectively on the MDD-SEEN and MDD-UNSEEN folds.",
        "id":248780343
      },
      {
        "title":"A SYNCHRONIZATION STRUCTURE OF SSTC AND ITS APPLICATIONS IN MACHINE TRANSLATION",
        "text":"In this paper, a flexible annotation schema called (SSTC) is introduced. In order to describe the correspondence between different languages, we propose a variant of SSTC called synchronous SSTC (S-SSTC). We will also describe how S-SSTC provides the flexibility to treat some of the non-standard cases, which are problematic to other synchronous formalisms. The proposed S-SSTC schema is well suited to describe the correspondence between different languages, in particular, relating a language with its translation in another language (i.e. in  Machine Translation). Also it can be used as annotation for translation systems that automatically extract transfer mappings(rules or examples) from bilingual corpora. The S-SSTC is very well suited for the construction of a Bilingual Knowledge Bank (BKB), where the examples are kept in form of S-SSTCs.",
        "id":9137980
      },
      {
        "title":"A Text-based Query Interface to OWL Ontologies",
        "text":"Accessing structured data in the form of ontologies requires training and learning formal query languages (e.g., SeRQL or SPARQL) which poses significant difficulties for non-expert users. One of the ways to lower the learning overhead and make ontology queries more straightforward is through a Natural Language Interface (NLI). While there are existing NLIs to structured data with reasonable performance, they tend to require expensive customisation to each new domain or ontology. Additionally, they often require specific adherence to a pre-defined syntax which, in turn, means that users still have to undergo training. In this paper we present Question-based Interface to Ontologies (QuestIO) -a tool for querying ontologies using unconstrained language-based queries. QuestIO has a very simple interface, requires no user training and can be easily embedded in any system or used with any ontology or knowledge base without prior customisation.",
        "id":7260067
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there any paper that combines causal inference and finetuning for language models?",
    "positive_ctxs":[
      {
        "title":"Preserving Commonsense Knowledge from Pre-trained Language Models via Causal Inference",
        "text":"Fine-tuning has been proven to be a simple and effective technique to transfer the learned knowledge of Pre-trained Language Models (PLMs) to downstream tasks. However, vanilla fine-tuning easily overfits the target data and degrades the generalization ability. Most existing studies attribute it to catastrophic forgetting, and they retain the pre-trained knowledge indiscriminately without identifying what knowledge is transferable. Motivated by this, we frame fine-tuning into a causal graph and discover that the crux of catastrophic forgetting lies in the missing causal effects from the pretrained data. Based on the causal view, we propose a unified objective for fine-tuning to retrieve the causality back. Intriguingly, the unified objective can be seen as the sum of the vanilla fine-tuning objective, which learns new knowledge from target data, and the causal objective, which preserves old knowledge from PLMs. Therefore, our method is flexible and can mitigate negative transfer while preserving knowledge. Since endowing models with commonsense is a long-standing challenge, we implement our method on commonsense QA with a proposed heuristic estimation to verify its effectiveness. In the experiments, our method outperforms state-of-the-art fine-tuning methods on all six commonsense QA datasets and can be implemented as a plug-in module to inflate the performance of existing QA models. for Computational Linguistics. . 2020. Qasc: A dataset for question answering via sentence composition. In . 2022a. Rainier: Reinforced knowledge introspector for commonsense question answering. arXiv preprint arXiv:2210.03078. . 2019b. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. In , et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer.",
        "id":259203213
      }
    ],
    "negative_ctxs":[
      {
        "title":"SemEval-2020 Task 11: Detection of Propaganda Techniques in News Articles",
        "text":"We present the results and the main findings of SemEval-",
        "id":221517069
      },
      {
        "title":"UiO 2 : Sequence-Labeling Negation Using Dependency Features",
        "text":"This paper describes the second of two systems submitted from the University of Oslo (UiO) to the 2012 *SEM Shared Task on resolving negation. The system combines SVM cue classification with CRF sequence labeling of events and scopes. Models for scopes and events are created using lexical and syntactic features, together with a fine-grained set of labels that capture the scopal behavior of certain tokens. Following labeling, negated tokens are assigned to their respective cues using simple post-processing heuristics. The system was ranked first in the open track and third in the closed track, and was one of the top performers in the scope resolution sub-task overall.",
        "id":8640333
      },
      {
        "title":"PolyphraZ : a tool for the management of parallel corpora",
        "text":"The PolyphraZ tool is being developed in the framework of the TraCorpEx project",
        "id":5097820
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there any paper that constructs augmented training data based on the entity-to-entity correlations?",
    "positive_ctxs":[
      {
        "title":"PeerDA: Data Augmentation via Modeling Peer Relation for Span Identification Tasks *",
        "text":"Span identification aims at identifying specific text spans from text input and classifying them into pre-defined categories. Different from previous works that merely leverage the Subordinate (SUB) relation (i.e. if a span is an instance of a certain category) to train models, this paper for the first time explores the Peer (PR) relation, which indicates that two spans are instances of the same category and share similar features. Specifically, a novel Peer Data Augmentation (PeerDA) approach is proposed which employs span pairs with the PR relation as the augmentation data for training. PeerDA has two unique advantages: (1) There are a large number of PR span pairs for augmenting the training data. (2) The augmented data can prevent the trained model from over-fitting the superficial span-category mapping by pushing the model to leverage the span semantics. Experimental results on ten datasets over four diverse tasks across seven domains demonstrate the effectiveness of PeerDA. Notably, PeerDA achieves state-of-the-art results on six of them.",
        "id":252918792
      }
    ],
    "negative_ctxs":[
      {
        "title":"Provable Fast Greedy Compressive Summarization with Any Monotone Submodular Function",
        "text":"Submodular maximization with the greedy algorithm has been studied as an effective approach to extractive summarization. This approach is known to have three advantages: its applicability to many useful submodular objective functions, the efficiency of the greedy algorithm, and the provable performance guarantee. However, when it comes to compressive summarization, we are currently missing a counterpart of the extractive method based on submodularity. In this paper, we propose a fast greedy method for compressive summarization. Our method is applicable to any monotone submodular objective function, including many functions well-suited for document summarization. We provide an approximation guarantee of our greedy algorithm. Experiments show that our method is about 100 to 400 times faster than an existing method based on integer-linearprogramming (ILP) formulations and that our method empirically achieves more than 95%-approximation.",
        "id":44062370
      },
      {
        "title":"Finding and Generating a Missing Part for Story Completion",
        "text":"Creating a story is difficult. Professional writers often experience a writer's block. Thus, providing automatic support to writers is crucial but also challenging. Recently, in the field of generating and understanding stories, story completion (SC) has been proposed as a method for generating missing parts of an incomplete story. Despite this method's usefulness in providing creative support, its applicability is currently limited because it requires the user to have prior knowledge of the missing part of a story. Writers do not always know which part of their writing is flawed. To overcome this problem, we propose a novel approach called \"missing position prediction (MPP).\" Given an incomplete story, we aim to predict the position of the missing part. We also propose a novel method for MPP and SC. We first conduct an experiment focusing on MPP, and our analysis shows that highly accurate predictions can be obtained when the missing part of a story is the beginning or the end. This suggests that if a story has a specific beginning or end, they play significant roles. We conduct an experiment on SC using MPP, and our proposed method demonstrates promising results.",
        "id":227231104
      },
      {
        "title":"SafeWebUH at SemEval-2023 Task 11: Learning Annotator Disagreement in Derogatory Text: Comparison of Direct Training vs Aggregation",
        "text":"Subjectivity and difference of opinion are key social phenomena, and it is crucial to take these into account in the annotation and detection process of derogatory textual content. In this paper, we use four datasets provided by SemEval-2023 Task 11 and fine-tune a BERT model to capture the disagreement in the annotation. We find individual annotator modeling and aggregation lowers the Cross-Entropy score by an average of 0.21, compared to the direct training on the soft labels. Our findings further demonstrate that annotator metadata contributes to the average 0.029 reduction in the Cross-Entropy score.",
        "id":258436777
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there any paper that employs code LLMs to iteratively generate and refine code with execution results to improve the performance?",
    "positive_ctxs":[
      {
        "title":"Self-Edit: Fault-Aware Code Editor for Code Generation",
        "text":"Large language models (LLMs) have demonstrated an impressive ability to generate codes on competitive programming tasks. However, with limited sample numbers, LLMs still suffer from poor accuracy. Inspired by the process of human programming, we propose a generateand-edit approach named Self-Edit that utilizes execution results of the generated code from LLMs to improve the code quality on the competitive programming task. We execute the generated code on the example test case provided in the question and wrap execution results into a supplementary comment. Utilizing this comment as guidance, our fault-aware code editor is employed to correct errors in the generated code. We perform extensive evaluations across two competitive programming datasets with nine different LLMs. Compared to directly generating from LLMs, our approach can improve the average of pass@1 by 89% on APPS-dev, 31% on APPS-test, and 48% on HumanEval over nine popular code generation LLMs with parameter sizes ranging from 110M to 175B. Compared to other post-processing methods, our method demonstrates superior accuracy and efficiency. . 2022. Incoder:A generative model for code infilling and synthesis. CoRR, abs\/2204.05999. . 2022. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. In NeurIPS. . 2023a. Enabling programming thinking in large language models toward code generation. arXiv preprint arXiv:2305.06599. . 2022a. Codeeditor: Learning to edit source code with pre-trained models. arXiv preprint arXiv:2210.17040.",
        "id":258557186
      }
    ],
    "negative_ctxs":[
      {
        "title":"GRACE: Gradient Harmonized and Cascaded Labeling for Aspect-based Sentiment Analysis",
        "text":"In this paper, we focus on the imbalance issue, which is rarely studied in aspect term extraction and aspect sentiment classification when regarding them as sequence labeling tasks. Besides, previous works usually ignore the interaction between aspect terms when labeling polarities. We propose a GRadient hArmonized and CascadEd labeling model (GRACE) to solve these problems. Specifically, a cascaded labeling module is developed to enhance the interchange between aspect terms and improve the attention of sentiment tokens when labeling sentiment polarities. The polarities sequence is designed to depend on the generated aspect terms labels. To alleviate the imbalance issue, we extend the gradient harmonized mechanism used in object detection to the aspect-based sentiment analysis by adjusting the weight of each label dynamically. The proposed GRACE adopts a post-pretraining BERT as its backbone. Experimental results demonstrate that the proposed model achieves consistency improvement on multiple benchmark datasets and generates state-of-the-art results.",
        "id":221836262
      },
      {
        "title":"",
        "text":"",
        "id":241583171
      },
      {
        "title":"Assessing the Limits of Straightforward Models for Nested Named Entity Recognition in Spanish Clinical Narratives",
        "text":"Nested Named Entity Recognition (NER) is an information extraction task that aims to identify entities that may be nested within other entity mentions. Despite the availability of several corpora with nested entities in the Spanish clinical domain, most previous work has overlooked them due to the lack of models and a clear annotation scheme for dealing with the task. To fill this gap, this paper provides an empirical study of straightforward methods for tackling the nested NER task on two Spanish clinical datasets, Clinical Trials, and the Chilean Waiting List. We assess the advantages and limitations of two sequence labeling approaches; one based on Multiple LSTM-CRF architectures and another on Joint labeling models. To better understand the differences between these models, we compute taskspecific metrics that adequately measure the ability of models to detect nested entities and perform a fine-grained comparison across models. Our experimental results show that employing domain-specific language models trained from scratch significantly improves the performance obtained with strong domain-specific and general-domain baselines, achieving stateof-the-art results in both datasets. Specifically, we obtained F 1 scores of 89.21 and 83.16 in Clinical Trials and the Chilean Waiting List, respectively. Interestingly enough, we observe that the task-specific metrics and analysis properly reflect the limitations of the models when recognizing nested entities. Finally, we perform a case study on an aggregated NER dataset created from several clinical corpora in Spanish. We highlight how entity length and the simultaneous recognition of inner and outer entities are the most critical variables for the nested NER task.",
        "id":256461274
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there any paper that explores and annotates the effectiveness of using testimonials or anecdotes in discussions?",
    "positive_ctxs":[
      {
        "title":"StoryARG: a corpus of narratives and personal experiences in argumentative texts",
        "text":"Humans are storytellers, even in communication scenarios which are assumed to be more rationality-oriented, such as argumentation. Indeed, supporting arguments with narratives or personal experiences (henceforth, stories) is a very natural thing to do -and yet, this phenomenon is largely unexplored in computational argumentation. Which role do stories play in an argument? Do they make the argument more effective? What are their narrative properties? To address these questions, we collected and annotated StoryARG, a dataset sampled from well-established corpora in computational argumentation (ChangeMyView and RegulationRoom), and the Social Sciences (Europolis), as well as comments to New York Times articles. StoryARG contains 2451 textual spans annotated at two levels. At the argumentative level, we annotate the function of the story (e.g., clarification, disclosure of harm, search for a solution, establishing speaker's authority), as well as its impact on the effectiveness of the argument and its emotional load. At the level of narrative properties, we annotate whether the story has a plot-like development, is factual or hypothetical, and who the protagonist is.What makes a story effective in an argument? Our analysis of the annotations in StoryARG uncover a positive impact on effectiveness for stories which illustrate a solution to a problem, and in general, annotator-specific preferences that we investigate with regression analysis.",
        "id":259370693
      }
    ],
    "negative_ctxs":[
      {
        "title":"C-PLANNING: AN AUTOMATIC CURRICULUM FOR LEARNING GOAL-REACHING TASKS",
        "text":"Goal-conditioned reinforcement learning (RL) can solve tasks in a wide range of domains, including navigation and manipulation, but learning to reach distant goals remains a central challenge to the field. Learning to reach such goals is particularly hard without any offline data, expert demonstrations, and reward shaping. In this paper, we propose an algorithm to solve the distant goal-reaching task by using search at training time to automatically generate a curriculum of intermediate states. Our algorithm, Classifier-Planning (C-Planning), frames the learning of the goal-conditioned policies as expectation maximization: the E-step corresponds to planning an optimal sequence of waypoints using graph search, while the M-step aims to learn a goal-conditioned policy to reach those waypoints. Unlike prior methods that combine goal-conditioned RL with graph search, ours performs search only during training and not testing, significantly decreasing the compute costs of deploying the learned policy. Empirically, we demonstrate that our method is more sample efficient that prior methods. Moreover, it is able to solve very long horizons manipulation and navigation tasks, tasks that prior goalconditioned methods and methods based on graph search fail to solve.",
        "id":239769065
      },
      {
        "title":"Zoho at SemEval-2019 Task 9: Semi-supervised Domain Adaptation using Tri-training for Suggestion Mining",
        "text":"This paper describes our submission for the SemEval-2019 Suggestion Mining task. A simple Convolutional Neural Network (CNN) classifier with contextual word representations from a pre-trained language model is used for sentence classification. The model is trained using tri-training, a semi-supervised bootstrapping mechanism for labelling unseen data. Tri-training proved to be an effective technique to accommodate domain shift for cross-domain suggestion mining (Subtask B) where there is no hand labelled training data. For in-domain evaluation (Subtask A), we use the same technique to augment the training set. Our system ranks thirteenth in Subtask A with an F 1 -score of 68.07 and third in Subtask B with an F 1 -score of 81.94.",
        "id":67856536
      },
      {
        "title":"A tool for enhanced search of multilingual digital libraries of e-journals",
        "text":"This paper outlines the main features of Bibliša, a tool that offers various possibilities of enhancing queries submitted to large collections of TMX documents generated from aligned parallel articles residing in multilingual digital libraries of e-journals. The queries initiated by a simple or multiword keyword, in Serbian or English, can be expanded by Bibliša, both semantically and morphologically, using different supporting monolingual and multilingual resources, such as wordnets and electronic dictionaries. The tool operates within a complex system composed of several modules including a web application, which makes it readily accessible on the web. Its functionality has been tested on a collection of 44 TMX documents generated from articles published bilingually by the journal INFOtecha, yielding encouraging results. Further enhancements of the tool are underway, with the aim of transforming it from a powerful full-text and metadata search tool, to a useful translator's aid, which could be of assistance both in reviewing terminology used in context and in refining the multilingual resources used within the system.",
        "id":14485511
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there any paper that explores using only an encoder-only masked language model for open-ended long text generation (such as story generation)?",
    "positive_ctxs":[
      {
        "title":"Open-ended Long Text Generation via Masked Language Modeling",
        "text":"Pre-trained autoregressive (AR) language models such as BART and GPTs have dominated Open-ended Long Text Generation (Open-LTG). However, the AR nature will decrease the inference efficiency along with the increase of generation length, which hinder their application in Open-LTG. To improve inference efficiency, we alternatively explore the potential of the pre-trained masked language models (MLMs) along with a representative iterative non-autoregressive (NAR) decoding strategy for Open-LTG. Our preliminary study shows that pre-trained MLMs can merely generate short text and will collapse for long text modeling. To enhance the long text generation capability of MLMs, we introduce two simple yet effective strategies for the iterative NAR model: dynamic sliding window attention (DSWA) and linear temperature decay (LTD). It can alleviate long-distance collapse problems and achieve longer text generation with a flexible trade-off between performance and inference speedup. Experiments on the storytelling and multi-paragraph opinionated article writing tasks show that pre-trained MLMs can achieve more than 3 × → 13 × speedup with better performance than strong AR models. Our code is available at GitHub * . . 2022. A survey on non-autoregressive generation for neural machine translation and beyond. arXiv preprint arXiv:2204.09269.",
        "id":259370630
      }
    ],
    "negative_ctxs":[
      {
        "title":"Sparse Bayesian Classification of Predicate Arguments",
        "text":"We present an application of Sparse Bayesian Learning to the task of semantic role labeling, and we demonstrate that this method produces smaller classifiers than the popular Support Vector approach.We describe the classification strategy and the features used by the classifier. In particular, the contribution of six parse tree path features is investigated.",
        "id":13926789
      },
      {
        "title":"",
        "text":"",
        "id":218974132
      },
      {
        "title":"Linguistic profiling of texts for the purpose of language verification",
        "text":"In order to control the quality of internet-based language corpora, we developed a method to verify automatically that texts are of (near-) native quality. For the LOCNESS and ICLE corpora, the method is rather successful in separating native and non-native learner texts. The Equal Error Rate is about 10%. However, for other domains, such as internet texts, separate classifiers have to be trained on the basis of suitable seed corpora.",
        "id":3275899
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_acl",
    "question":"Is there any paper that investigates backdoor attacks across various types of tasks, not limited to classification, in language models?",
    "positive_ctxs":[
      {
        "title":"Multi-target Backdoor Attacks for Code Pre-trained Models",
        "text":"Backdoor attacks for neural code models have gained considerable attention due to the advancement of code intelligence. However, most existing works insert triggers into task-specific data for code-related downstream tasks, thereby limiting the scope of attacks. Moreover, the majority of attacks for pre-trained models are designed for understanding tasks. In this paper, we propose task-agnostic backdoor attacks for code pre-trained models. Our backdoored model is pre-trained with two learning strategies (i.e., Poisoned Seq2Seq learning and token representation learning) to support the multitarget attack of downstream code understanding and generation tasks. During the deployment phase, the implanted backdoors in the victim models can be activated by the designed triggers to achieve the targeted attack. We evaluate our approach on two code understanding tasks and three code generation tasks over seven datasets. Extensive experiments demonstrate that our approach can effectively and stealthily attack code-related downstream tasks. Hoi. 2021b. Codet5: Identifier-aware unified pretrained encoder-decoder models for code understanding and generation. In",
        "id":259165134
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":252763347
      },
      {
        "title":"Conversational Decision-Making Model for Predicting the King's Decision in the Annals of the Joseon Dynasty",
        "text":"Styles of leaders when they make decisions in groups vary, and the different styles affect the performance of the group. To understand the key words and speakers associated with decisions, we initially formalize the problem as one of predicting leaders' decisions from discussion with group members. As a dataset, we introduce conversational meeting records from a historical corpus, and develop a hierarchical RNN structure with attention and pre-trained speaker embedding in the form of a, Conversational Decision Making Model (CDMM). The CDMM outperforms other baselines to predict leaders' final decisions from the data. We explain why CDMM works better than other methods by showing the key words and speakers discovered from the attentions as evidence.",
        "id":53082945
      },
      {
        "title":"A log-linear model with an n-gram reference distribution for accurate HPSG parsing",
        "text":"This paper describes a log-linear model with an n-gram reference distribution for accurate probabilistic HPSG parsing. In the model, the n-gram reference distribution is simply defined as the product of the probabilities of selecting lexical entries, which are provided by the discriminative method with machine learning features of word and POS n-gram as defined in the CCG\/HPSG\/CDG supertagging. Recently, supertagging becomes well known to drastically improve the parsing accuracy and speed, but supertagging techniques were heuristically introduced, and hence the probabilistic models for parse trees were not well defined. We introduce the supertagging probabilities as a reference distribution for the log-linear model of the probabilistic HPSG. This is the first model which properly incorporates the supertagging probabilities into parse tree's probabilistic model.",
        "id":13040693
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there any paper that leverages graph neural network by integrating label information for multi-label low-resource intent classification?",
    "positive_ctxs":[
      {
        "title":"Dual Class Knowledge Propagation Network for Multi-label Few-shot Intent Detection",
        "text":"Multi-label intent detection aims to assign multiple labels to utterances and attracts increasing attention as a practical task in task-oriented dialogue systems. As dialogue domains change rapidly and new intents emerge fast, the lack of annotated data motivates multi-label few-shot intent detection. However, previous studies are confused by the identical representation of the utterance with multiple labels and overlook the intrinsic intra-class and inter-class interactions. To address these two limitations, we propose a novel dual class knowledge propagation network in this paper. In order to learn well-separated representations for utterances with multiple intents, we first introduce a labelsemantic augmentation module incorporating class name information. For better consideration of the inherent intra-class and inter-class relations, an instance-level and a class-level graph neural network are constructed, which not only propagate label information but also propagate feature structure. And we use a simple yet effective method to predict the intent count of each utterance. Extensive experimental results on two multi-label intent datasets have demonstrated that our proposed method outperforms strong baselines by a large margin.",
        "id":259370656
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":227231501
      },
      {
        "title":"",
        "text":"",
        "id":237204590
      },
      {
        "title":"Camtology: Intelligent Information Access for Science",
        "text":"We describe a novel semantic search engine for scientific literature. The Camtology system allows for sentence-level searches of PDF files and combines text and image searches, thus facilitating the retrieval of information present in tables and figures. It allows the user to generate complex queries for search terms that are related through particular grammatical\/semantic relations in an intuitive manner. The system uses Grid processing to parallelise the analysis of large numbers of papers.",
        "id":15415565
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_acl",
    "question":"Is there any paper that leverages syntactic rules to explicitly guide text generation?",
    "positive_ctxs":[
      {
        "title":"Explicit Syntactic Guidance for Neural Text Generation",
        "text":"Most existing text generation models follow the sequence-to-sequence paradigm. Generative Grammar suggests that humans generate natural language texts by learning language grammar. We propose a syntax-guided generation schema, which generates the sequence guided by a constituency parse tree in a topdown direction. The decoding process can be decomposed into two parts: (1) predicting the infilling texts for each constituent in the lexicalized syntax context given the source sentence;(2) mapping and expanding each constituent to construct the next-level syntax context. Accordingly, we propose a structural beam search method to find possible syntax structures hierarchically. Experiments on paraphrase generation and machine translation show that the proposed method outperforms autoregressive baselines, while also demonstrating effectiveness in terms of interpretability, controllability, and diversity.",
        "id":259203682
      }
    ],
    "negative_ctxs":[
      {
        "title":"Predicting Literary Quality How Perspectivist Should We Be?",
        "text":"Approaches in literary quality tend to belong to two main grounds: one sees quality as completely subjective, relying on the idiosyncratic nature of individual perspectives on the perception of beauty; the other is ground-truth inspired, and attempts to find one or two values that predict something like an objective quality: the number of copies sold, for example, or the winning of a prestigious prize. While the first school usually does not try to predict quality at all, the second relies on a single majority vote in one form or another. In this article we discuss the advantages and limitations of these schools of thought and describe a different approach to reader's quality judgments, which moves away from raw majority vote, but does try to create intermediate classes or groups of annotators. Drawing on previous works we describe the benefits and drawbacks of building similar annotation classes. Finally we share early results from a large corpus of literary reviews for an insight into which classes of readers might make most sense when dealing with the appreciation of literary quality.",
        "id":252624715
      },
      {
        "title":"Curriculum Learning Based on Reward Sparseness for Deep Reinforcement Learning of Task Completion Dialogue Management",
        "text":"Learning from sparse and delayed reward is a central issue in reinforcement learning. In this paper, to tackle reward sparseness problem of task oriented dialogue management, we propose a curriculum based approach on the number of slots of user goals. This curriculum makes it possible to learn dialogue management for sets of user goals with large number of slots. We also propose a dialogue policy based on progressive neural networks whose modules with parameters are appended with previous parameters fixed as the curriculum proceeds, and this policy improves performances over the one with single set of parameters.",
        "id":53234787
      },
      {
        "title":"Domain-Adaptable Hybrid Generation of RDF Entity Descriptions",
        "text":"RDF ontologies provide structured data on entities in many domains and continue to grow in size and diversity. While they can be useful as a starting point for generating descriptions of entities, they often miss important information about an entity that cannot be captured as simple relations. In addition, generic approaches to generation from RDF cannot capture the unique style and content of specific domains. We describe a framework for hybrid generation of entity descriptions, which combines generation from RDF data with text extracted from a corpus, and extracts unique aspects of the domain from the corpus to create domain-specific generation systems. We show that each component of our approach significantly increases the satisfaction of readers with the text across multiple applications and domains.",
        "id":29910620
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there any paper that performs adversarial training on frame level for audio-visual representation learning?",
    "positive_ctxs":[
      {
        "title":"MIR-GAN: Refining Frame-Level Modality-Invariant Representations with Adversarial Network for Audio-Visual Speech Recognition",
        "text":"Audio-visual speech recognition (AVSR) attracts a surge of research interest recently by leveraging multimodal signals to understand human speech. Mainstream approaches addressing this task have developed sophisticated architectures and techniques for multi-modality fusion and representation learning. However, the natural heterogeneity of different modalities causes distribution gap between their representations, making it challenging to fuse them. In this paper, we aim to learn the shared representations across modalities to bridge their gap. Different from existing similar methods on other multimodal tasks like sentiment analysis, we focus on the temporal contextual dependencies considering the sequence-to-sequence task setting of AVSR. In particular, we propose an adversarial network to refine framelevel modality-invariant representations (MIR-GAN), which captures the commonality across modalities to ease the subsequent multimodal fusion process. Extensive experiments on public benchmarks LRS3 and LRS2 show that our approach outperforms the state-of-the-arts 1 . Pingchuan Ma, Stavros Petridis, and Maja Pantic. 2021. End-to-end audio-visual speech recognition with con-. 2019. Recurrent neural network transducer for audio-visual speech recognition. In 2019 IEEE automatic speech recognition and understanding workshop (ASRU), pages 905-912. IEEE. Harry McGurk and John MacDonald. 1976. Hearing lips and seeing voices. Nature, 264(5588):746-748.",
        "id":259203396
      }
    ],
    "negative_ctxs":[
      {
        "title":"Classifying Dialogue Acts in One-on-one Live Chats",
        "text":"We explore the task of automatically classifying dialogue acts in 1-on-1 online chat forums, an increasingly popular means of providing customer service. In particular, we investigate the effectiveness of various features and machine learners for this task. While a simple bag-of-words approach provides a solid baseline, we find that adding information from dialogue structure and inter-utterance dependency provides some increase in performance; learners that account for sequential dependencies (CRFs) show the best performance. We report our results from testing using a corpus of chat dialogues derived from online shopping customer-feedback data.",
        "id":477778
      },
      {
        "title":"Focusing on a Subset of Scripts Enhances the Learning Efficiency of Second Language Writing System",
        "text":"Memorizing the whole set of graphemes is generally accepted as the first step of learning a phonogramic language. However, it is demanding for L2 learners to familiarize the whole inventory of graphemes in advance if the language has a relatively large inventory size. We propose that learning a subset of graphemes would largely enhance the learning efficiency by reducing the memory burden. With homophony minimized, effort of acquiring vocabulary in elementary stage can be greatly reduced. In this paper, the writing system of Thai is used to illustrate the main idea. Besides, the method may also be extendable to Japanese and Korean, which grapheme inventory sizes are smaller.",
        "id":17724446
      },
      {
        "title":"Language Modeling using Dynamic Bayesian Networks",
        "text":"In this paper we propose a new approach to language modeling based on dynamic Bayesian networks. The principle idea of our approach is to find the dependence relations between variables that represent different linguistic units (word, class, concept, ...) that constitutes a language model. In the context of this paper the linguistic units that we consider are syntactic classes and words. Our approach should not be considered as a model combination technique. Rather, it is an original and coherent methodology that processes words and classes in the same model. We attempt to identify and model the dependence of words and classes on their linguistic context. Our ultimate goal is to devise an automatic mechanism that extracts the best dependence relations between a word and its context, i.e., lexical and syntactic. Preliminary results are very encouraging, in particular the model in which a word depends not only on previous word but also on syntactic classes of two previous words. This model outperforms the bi-gram model.",
        "id":9297900
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there any paper that proposes a new multimodal video dataset that image-level multimodal models do not work well?",
    "positive_ctxs":[
      {
        "title":"Revealing Single Frame Bias for Video-and-Language Learning",
        "text":"Training an effective video-and-language model intuitively requires multiple frames as model inputs. However, it is unclear whether using multiple frames is beneficial to downstream tasks, and if yes, whether the performance gain is worth the drastically-increased computation and memory costs resulting from using more frames. In this work, we explore single-frame models for video-and-language learning. On a diverse set of video-andlanguage tasks (including text-to-video retrieval and video question answering), we show the surprising result that, with large-scale pretraining and a proper frame ensemble strategy at inference time, a single-frame trained model that does not consider temporal information can achieve better performance than existing methods that use multiple frames for training. This result reveals the existence of a strong \"static appearance bias\" in popular video-andlanguage datasets. Therefore, to allow for a more comprehensive evaluation of videoand-language models, we propose two new retrieval tasks based on existing fine-grained action recognition datasets that encourage temporal modeling. Our code is available at https: \/\/github.com\/jayleicn\/singularity.",
        "id":249431866
      }
    ],
    "negative_ctxs":[
      {
        "title":"Do \"Transitive Adjectives\" Really Exist?",
        "text":"I argue that the so-called psychological predicates like komapta 'thankful,' mwusepta 'fearful,' silhtà loathsome,' or kulipta 'missing' require a nominative subject and a locative or dative complement, challenging the claim, a conventional wisdom originated fromKuno(1973), that they are two-place \"transitive adjectives\" requiring a nominative direct object. I also show that those adjectives are subject to having the locative-dative complement extracted, which is ultimately realized as a focused subject or a topic. Thus, in this type of double nominative constructions, the first nominative is a focused subject, and the second nominative forms an embedded clause with the psychological predicate, which functions as the predicate of the whole sentence.",
        "id":17911959
      },
      {
        "title":"A Lexical Resource of Hebrew Verb-Noun Multi-Word Expressions",
        "text":"A verb-noun Multi-Word Expression (MWE) is a combination of a verb and a noun with or without other words, in which the combination has a meaning different from the meaning of the words considered separately. In this paper, we present a new lexical resource of Hebrew Verb-Noun MWEs (VN-MWEs). The VN-MWEs of this resource were manually collected and annotated from five different web resources. In addition, we analyze the lexical properties of Hebrew VN-MWEs by classifying them to three types: morphological, syntactic, and semantic. These two contributions are essential for designing algorithms for automatic VN-MWEs extraction. The analysis suggests some interesting features of VN-MWEs for exploration. The lexical resource enables to sample a set of positive examples for Hebrew VN-MWEs. This set of examples can either be used for training supervised algorithms or as seeds in unsupervised bootstrapping algorithms. Thus, this resource is a first step towards automatic identification of Hebrew VN-MWEs, which is important for natural language understanding, generation and translation systems.",
        "id":39149029
      },
      {
        "title":"",
        "text":"",
        "id":219300132
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_acl",
    "question":"Is there any paper that proposes a set of criteria to comprehensively evaluate generated conversations?",
    "positive_ctxs":[
      {
        "title":"Modeling What-to-ask and How-to-ask for Answer-unaware Conversational Question Generation",
        "text":"Conversational Question Generation (CQG) is a critical task for machines to assist humans in fulfilling their information needs through conversations. The task is generally cast into two different settings: answer-aware and answerunaware. While the former facilitates the models by exposing the expected answer, the latter is more realistic and receiving growing attentions recently. What-to-ask and how-to-ask are the two main challenges in the answerunaware setting. To address the first challenge, existing methods mainly select sequential sentences in context as the rationales. We argue that the conversation generated using such naive heuristics may not be natural enough as in reality, the interlocutors often talk about the relevant contents that are not necessarily sequential in context. Additionally, previous methods decide the type of question (boolean\/span-based) to be generated implicitly. Modeling the question type explicitly is crucial in this (answer-unaware) setting, as the answer which hints the models to generate a boolean or span-based question, is unavailable. To this end, we present SG-CQG, a two-stage CQG framework. For the what-to-ask stage, a sentence is selected as the rationale from a semantic graph that we construct, and extract the answer span from it. For the how-to-ask stage, a classifier determines the target answer type of the question via two explicit control signals before generating and filtering. In addition, we propose Conv-Distinct, a novel evaluation metric for CQG, to evaluate the diversity of the generated conversation from a context. Compared with the existing answer-unaware CQG models, the proposed SG-CQG achieves stateof-the-art performance.",
        "id":258547050
      }
    ],
    "negative_ctxs":[
      {
        "title":"Improved Relation Classification by Deep Recurrent Neural Networks with Data Augmentation",
        "text":"Nowadays, neural networks play an important role in the task of relation classification. By designing different neural architectures, researchers have improved the performance to a large extent in comparison with traditional methods. However, existing neural networks for relation classification are usually of shallow architectures (e.g., one-layer convolutional neural networks or recurrent networks). They may fail to explore the potential representation space in different abstraction levels. In this paper, we propose deep recurrent neural networks (DRNNs) for relation classification to tackle this challenge. Further, we propose a data augmentation method by leveraging the directionality of relations. We evaluated our DRNNs on the SemEval-2010 Task 8, and achieve an F 1 -score of 86.1%, outperforming previous state-of-the-art recorded results. 1",
        "id":7437060
      },
      {
        "title":"Cross-Cultural Transfer Learning for Text Classification",
        "text":"Large training datasets are required to achieve competitive performance in most natural language tasks. The acquisition process for these datasets is labor intensive, expensive, and time consuming. This process is also prone to human errors. In this work, we show that crosscultural differences can be harnessed for natural language text classification. We present a transfer-learning framework that leverages widely-available unaligned bilingual corpora for classification tasks, using no task-specific data. Our empirical evaluation on two tasks -formality classification and sarcasm detection -shows that the cross-cultural difference between German and American English, as manifested in product review text, can be applied to achieve good performance for formality classification, while the difference between Japanese and American English can be applied to achieve good performance for sarcasm detection -both without any task-specific labeled data.",
        "id":202785347
      },
      {
        "title":"Effective Approach to Develop a Sentiment Annotator For Legal Domain in a Low Resource Setting",
        "text":"Analyzing the sentiments of legal opinions available in Legal Opinion Texts can facilitate several use cases such as legal judgement prediction, contradictory statements identification and party-based sentiment analysis. However, the task of developing a legal domain specific sentiment annotator is challenging due to resource constraints such as lack of domain specific labelled data and domain expertise. In this study, we propose novel techniques that can be used to develop a sentiment annotator for the legal domain while minimizing the need for manual annotations of data.",
        "id":226226405
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there any paper that reveals annotation problems in cross-lingual summarization caused by decomposing the task into translation and summarization?",
    "positive_ctxs":[
      {
        "title":"Revisiting Cross-Lingual Summarization: A Corpus-based Study and A New Benchmark with Improved Annotation",
        "text":"Most existing cross-lingual summarization (CLS) work constructs CLS corpora by simply and directly translating pre-annotated summaries from one language to another, which can contain errors from both summarization and translation processes. To address this issue, we propose ConvSumX, a cross-lingual conversation summarization benchmark, through a new annotation schema that explicitly considers source input context. ConvSumX consists of 2 sub-tasks under different real-world scenarios, with each covering 3 language directions. We conduct thorough analysis on ConvSumX and 3 widely-used manually annotated CLS corpora and empirically find that ConvSumX is more faithful towards input text. Additionally, based on the same intuition, we propose a 2-Step method, which takes both conversation and summary as input to simulate human annotation process. Experimental results show that 2-Step method surpasses strong baselines on ConvSumX under both automatic and human evaluation. Analysis shows that both source input text and summary are crucial for modeling cross-lingual summaries.",
        "id":259370588
      }
    ],
    "negative_ctxs":[
      {
        "title":"A Single Word is not Enough: Ranking Multiword Expressions Using Distributional Semantics",
        "text":"We present a new unsupervised mechanism, which ranks word n-grams according to their multiwordness. It heavily relies on a new uniqueness measure that computes, based on a distributional thesaurus, how often an n-gram could be replaced in context by a single-worded term. In addition with a downweighting mechanism for incomplete terms this forms a new measure called DRUID. Results show large improvements on two small test sets over competitive baselines. We demonstrate the scalability of the method to large corpora, and the independence of the measure of shallow syntactic filtering.",
        "id":16232250
      },
      {
        "title":"Composition of Conditional Random Fields for Transfer Learning",
        "text":"Many learning tasks have subtasks for which much training data exists. Therefore, we want to transfer learning from the old, generalpurpose subtask to a more specific new task, for which there is often less data. While work in transfer learning often considers how the old task should affect learning on the new task, in this paper we show that it helps to take into account how the new task affects the old. Specifically, we perform joint decoding of separately-trained sequence models, preserving uncertainty between the tasks and allowing information from the new task to affect predictions on the old task. On two standard text data sets, we show that joint decoding outperforms cascaded decoding.",
        "id":161340
      },
      {
        "title":"Multi-Input Attention for Unsupervised OCR Correction",
        "text":"We propose a novel approach to OCR post-correction that exploits repeated texts in large corpora both as a source of noisy target outputs for unsupervised training and as a source of evidence when decoding. A sequence-to-sequence model with attention is applied for single-input correction, and a new decoder with multi-input attention averaging is developed to search for consensus among multiple sequences. We design two ways of training the correction model without human annotation, either training to match noisily observed textual variants or bootstrapping from a uniform error model. On two corpora of historical newspapers and books, we show that these unsupervised techniques cut the character and word error rates nearly in half on single inputs and, with the addition of multi-input decoding, can rival supervised methods.",
        "id":51874610
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there any paper that studies a teacher AI inferring mental states of a student role in a role-playing game setup using reinforcement learning?",
    "positive_ctxs":[
      {
        "title":"I Cast Detect Thoughts: Learning to Converse and Guide with Intents and Theory-of-Mind in Dungeons and Dragons",
        "text":"We propose a novel task, G4C, to study teacher-student natural language interactions in a goal-driven and grounded environment. Dungeons and Dragons (D&D), a role-playing game, provides an ideal setting to investigate such interactions. Here, the Dungeon Master (DM), i.e., the teacher, guides the actions of several players-students, each with their own personas and abilities-to achieve shared goals grounded in a fantasy world. Our approach is to decompose and model these interactions into (1) the DM's intent to guide players towards a given goal; (2) the DM's guidance utterance to the players expressing this intent; and (3) a theory-of-mind (ToM) model that anticipates the players' reaction to the guidance one turn into the future. We develop a novel reinforcement learning (RL) method for training a DM that generates guidance for players by rewarding utterances where the intent matches the ToM-anticipated player actions. Human and automated evaluations show that a DM trained to explicitly model intents and incorporate ToM of the players using RL generates better-quality guidance that is 3x more likely to fulfill the DM's intent than a vanilla natural language generation (NLG) approach.",
        "id":258987457
      }
    ],
    "negative_ctxs":[
      {
        "title":"A Graph-Based Approach Leveraging Posts and Reactions for Detecting Rumors on Online Social Media",
        "text":"In this paper, we present a novel graph-based contextual and semantic learning approach for detecting rumors on online social media. The underlying hypothesis is that social media entities are intertwined, and if an event unfolds then similar narratives or user reactions with common interests get circulated. The proposed approach uses tweets and their reactions to understand the underlying interaction patterns and exploits the textual and latent information. Textual data is modeled as a words cooccurrence graph, which produces two prevalent categories of words -substantial words and bridge words. These words serve as building blocks for constructing contextual patterns for rumor detection by computing node-level statistical measures. The contextual patterns are further enriched by identifying negative emotions and inquisitive aspects in the reactions. The patterns are finally ranked and only top-k check-worthy patterns are used for feature generation. In order to preserve the semantic relations, we use word-level GloVe embedding trained over a Twitter dataset. The proposed approach is evaluated over a publicly available PHEME dataset, and compared with various baselines and SOTA techniques. The experimental results are promising and the proposed approach seems useful for rumor detection on online social media.",
        "id":258463972
      },
      {
        "title":"Memory-Efficient Differentiable Transformer Architecture Search",
        "text":"Differentiable architecture search (DARTS) is successfully applied in many vision tasks. However, directly using DARTS for Transformers is memory-intensive, which renders the search process infeasible. To this end, we propose a multi-split reversible network and combine it with DARTS. Specifically, we devise a backpropagation-with-reconstruction algorithm so that we only need to store the last layer's outputs. By relieving the memory burden for DARTS, it allows us to search with larger hidden size and more candidate operations. We evaluate the searched architecture on three sequence-to-sequence datasets, i.e., WMT'14 English-German, WMT'14 English-French, and WMT'14 English-Czech. Experimental results show that our network consistently outperforms standard Transformers across the tasks. Moreover, our method compares favorably with big-size Evolved Transformers, reducing search computation by an order of magnitude.",
        "id":235254260
      },
      {
        "title":"Suicide Risk Assessment with Multi-level Dual-Context Language and BERT",
        "text":"Mental health predictive systems typically model language as if from a single context (e.g. Twitter posts, status updates, or forum posts) and often limited to a single level of analysis (e.g. either the message-level or userlevel). Here, we bring these pieces together to explore the use of open-vocabulary (BERT embeddings, topics) and theoretical features (emotional expression lexica, personality) for the task of suicide risk assessment on support forums (the CLPsych-2019 Shared Task). We used dual context based approaches (modeling content from suicide forums separate from other content), built over both traditional ML models as well as a novel dual RNN architecture with user-factor adaptation. We find that while affect from the suicide context distinguishes with no-risk from those with \"anyrisk\", personality factors from the non-suicide contexts provide distinction of the levels of risk: low, medium, and high risk. Within the shared task, our dual-context approach (listed as SBU-HLAB in the official results) achieved state-of-the-art performance predicting suicide risk using a combination of suicide-context and non-suicide posts (Task B), achieving an F1 score of 0.50 over hidden test set labels.",
        "id":198184729
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there any paper that tries to investigate LLMs’ capabilities in solving elliptical constructions by using a test-dataset based on the psycolinguistic notion of Thematic Fit?",
    "positive_ctxs":[
      {
        "title":"We Understand Elliptical Sentences, and Language Models Should Too: A New Dataset for Studying Ellipsis and its Interaction with Thematic Fit",
        "text":"Ellipsis is a linguistic phenomenon characterized by the omission of one or more sentence elements. Solving such a linguistic construction is not a trivial issue in natural language processing since it involves the retrieval of non-overtly expressed verbal material, which might in turn require the model to integrate human-like syntactic and semantic knowledge. In this paper, we explored the issue of how the prototypicality of event participants affects the ability of Language Models (LMs) to handle elliptical sentences, and to identify the omitted arguments at different degrees of thematic fit, ranging from highly typical participants to semantically anomalous ones. With this purpose in mind, we built ELLie, the first dataset composed entirely of utterances containing different types of elliptical constructions, and structurally suited for evaluating the effect of argument thematic fit in solving ellipsis and reconstructing the missing element. Our tests demonstrated that the probability scores assigned by the models are higher for typical events than for atypical and impossible ones in different elliptical contexts, confirming the influence of prototypicality of the event participants in interpreting such linguistic structures. Finally, we conducted a retrieval task of the elided verb in the sentence in which the low performance of LMs highlighted a considerable difficulty in reconstructing the correct event.",
        "id":259370867
      }
    ],
    "negative_ctxs":[
      {
        "title":"Discourse Understanding and Factual Consistency in Abstractive Summarization",
        "text":"We introduce a general framework for abstractive summarization with factual consistency and distinct modeling of the narrative flow in an output summary. Our work addresses current limitations of models for abstractive summarization that often hallucinate information or generate summaries with coherence issues.To generate abstractive summaries with factual consistency and narrative flow, we propose Cooperative Generator -Discriminator Networks (Co-opNet), a novel transformerbased framework where a generator works with a discriminator architecture to compose coherent long-form summaries. We explore four different discriminator objectives which each capture a different aspect of coherence, including whether salient spans of generated abstracts are hallucinated or appear in the input context, and the likelihood of sentence adjacency in generated abstracts.We measure the ability of Co-opNet to learn these objectives with arXiv scientific papers, using the abstracts as a proxy for gold longform scientific article summaries. Empirical results from automatic and human evaluations demonstrate that Co-opNet learns to summarize with considerably improved global coherence compared to competitive baselines.",
        "id":233189560
      },
      {
        "title":"Modeling human-like morphological prediction",
        "text":"We test a model of morphological prediction based on analogical deduction using phonemic similarity by applying it to German plural suffix prediction for a set of 24 nonce forms for whichMcCurdy et al. (2020)elicited human judgements, and which they found were poorly matched by productions of an encoder-decoder model ofKirov and Cotterell (2018). Their results raise the question of what kinds of models best mirror human judgements. We show that the predictions of the analogical models we tested mirror human judgements better than the encoder-decoder model.",
        "id":247694105
      },
      {
        "title":"SemEval-2013 Task 1: TEMPEVAL-3: Evaluating Time Expressions, Events, and Temporal Relations",
        "text":"Within the SemEval-2013 evaluation exercise, the TempEval-3 shared task aims to advance research on temporal information processing. It follows on from TempEval-1 and -2, with: a three-part structure covering temporal expression, event, and temporal relation extraction; a larger dataset; and new single measures to rank systems -in each task and in general. In this paper, we describe the participants' approaches, results, and the observations from the results, which may guide future research in this area.",
        "id":640783
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there any paper that uses data collected from the Dark Web, specifically onion domains, to pretrain a language model?",
    "positive_ctxs":[
      {
        "title":"DarkBERT: A Language Model for the Dark Side of the Internet",
        "text":"Recent research has suggested",
        "id":258685798
      }
    ],
    "negative_ctxs":[
      {
        "title":"ExCAR: Event Graph Knowledge Enhanced Explainable Causal Reasoning",
        "text":"Prior work infers the causation between events mainly based on the knowledge induced from the annotated causal event pairs. However, additional evidence information intermediate to the cause and effect remains unexploited. By incorporating such information, the logical law behind the causality can be unveiled, and the interpretability and stability of the causal reasoning system can be improved. To facilitate this, we present an Event graph knowledge enhanced explainable CAusal Reasoning framework (ExCAR). ExCAR first acquires additional evidence information from a large-scale causal event graph as logical rules for causal reasoning. To learn the conditional probabilistic of logical rules, we propose the Conditional Markov Neural Logic Network (CMNLN) that combines the representation learning and structure learning of logical rules in an end-to-end differentiable manner. Experimental results demonstrate that ExCAR outperforms previous state-of-the-art methods. Adversarial evaluation shows the improved stability of Ex-CAR over baseline systems. Human evaluation shows that ExCAR can achieve a promising explainable performance.",
        "id":236460013
      },
      {
        "title":"Specifying Conceptual Models Using Restricted Natural Language",
        "text":"The key activity to design an information system is conceptual modelling which brings out and describes the general knowledge that is required to build a system. In this paper we propose a novel approach to conceptual modelling where the domain experts will be able to specify and construct a model using a restricted form of natural language. A restricted natural language is a subset of a natural language that has well-defined computational properties and therefore can be translated unambiguously into a formal notation. We will argue that a restricted natural language is suitable for writing precise and consistent specifications that lead to executable conceptual models. Using a restricted natural language will allow the domain experts to describe a scenario in the terminology of the application domain without the need to formally encode this scenario. The resulting textual specification can then be automatically translated into the language of the desired conceptual modelling framework.",
        "id":67865751
      },
      {
        "title":"HIT&QMUL at SemEval-2022 Task 9: Label-Enclosed Generative Question Answering (LEG-QA)",
        "text":"This paper presents the second place system for the R2VQ: competence-based multimodal question answering shared task. The task consisted in building question answering systems that could process procedural recipes involving both text and image, and enriched with semantic and cooking roles. We tackled the task by using a text-to-text generative model based on the transformer architecture, with the aim of generalising across different question types. Our proposed architecture incorporates a novel approach for enriching input texts by incorporating semantic and cooking role labels through what we call Label-Enclosed Generative Question Answering (LEG-QA). Our model achieves a score of 91.3, with a significant improvement over the baseline (65.34) and close to the top-ranked system ((92.5). After describing the submitted system, we analyse the impact of the different components of LEG-QA as well as perform an error analysis.",
        "id":250390917
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there any paper that uses prompt tuning in multi-answer QA?",
    "positive_ctxs":[
      {
        "title":"Answering Ambiguous Questions via Iterative Prompting",
        "text":"In open-domain question answering, due to the ambiguity of questions, multiple plausible answers may exist. To provide feasible answers to an ambiguous question, one approach is to directly predict all valid answers, but this can struggle with balancing relevance and diversity. An alternative is to gather candidate answers and aggregate them, but this method can be computationally costly and may neglect dependencies among answers. In this paper, we present AmbigPrompt to address the imperfections of existing approaches to answering ambiguous questions. Specifically, we integrate an answering model with a prompting model in an iterative manner. The prompting model adaptively tracks the reading process and progressively triggers the answering model to compose distinct and relevant answers. Additionally, we develop a task-specific postpretraining approach for both the answering model and the prompting model, which greatly improves the performance of our framework. Empirical studies on two commonly-used open benchmarks show that AmbigPrompt achieves state-of-the-art or competitive results while using less memory and having a lower inference latency than competing approaches. Additionally, AmbigPrompt also performs well in low-resource settings. The code are available at: https:\/\/github. com\/sunnweiwei\/AmbigPrompt.",
        "id":259370644
      }
    ],
    "negative_ctxs":[
      {
        "title":"A STRUCTURED REPRESENTATION OF WORD-SENSES IrOR SEMANTIC ANALYSIS",
        "text":"A framework for a structured representation of semantic knowledge (e.g. word-senses) has been defined at the IBM Scientific Center of Roma, as part of a project on Italian Text Understanding. This representation, based on the conceptual graphs formalism [SOW84], expresses deep knowledge (pragmatic) on word-senses. The knowledge base data structure is such as to provide easy access by the semantic verification algorithm. This paper discusses some important problem related to the definition of a semantic knowledge base, as depth versus generality, hierarchical ordering of concept types, etc., and describes the solutions adopted within the text understanding project.",
        "id":12363636
      },
      {
        "title":"Probing Script Knowledge from Pre-Trained Models",
        "text":"Script knowledge is critical for humans to understand the broad daily tasks and routine activities in the world. Recently researchers have explored the large-scale pre-trained language models (PLMs) to perform various script related tasks, such as story generation, temporal ordering of event, future event prediction and so on. However, it's still not well studied in terms of how well the PLMs capture the script knowledge. To answer this question, we design three probing tasks: inclusive sub-event selection, starting sub-event selection and temporal ordering to investigate the capabilities of PLMs with and without fine-tuning. The three probing tasks can be further used to automatically induce a script for each main event given all the possible sub-events. Taking BERT as a case study, by analyzing its performance on script induction as well as each individual probing task, we conclude that the stereotypical temporal knowledge among the sub-events is well captured in BERT, however the inclusive or starting sub-event knowledge is barely encoded.",
        "id":248299724
      },
      {
        "title":"Interactive Group Suggesting for Twitter",
        "text":"The number of users on Twitter has drastically increased in the past years. However, Twitter does not have an effective user grouping mechanism. Therefore tweets from other users can quickly overrun and become inconvenient to read. In this paper, we propose methods to help users group the people they follow using their provided seeding users. Two sources of information are used to build sub-systems: textural information captured by the tweets sent by users, and social connections among users. We also propose a measure of fitness to determine which subsystem best represents the seed users and use it for target user ranking. Our experiments show that our proposed framework works well and that adaptively choosing the appropriate sub-system for group suggestion results in increased accuracy.",
        "id":18172634
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there any paper that uses token-level loss to enhance sentence-level embedding learning?",
    "positive_ctxs":[
      {
        "title":"Dual-Alignment Pre-training for Cross-lingual Sentence Embedding",
        "text":"Recent studies have shown that dual encoder models trained with the sentence-level translation ranking task are effective methods for cross-lingual sentence embedding. However, our research indicates that token-level alignment is also crucial in multilingual scenarios, which has not been fully explored previously. Based on our findings, we propose a dual-alignment pre-training (DAP) framework for cross-lingual sentence embedding that incorporates both sentence-level and token-level alignment. To achieve this, we introduce a novel representation translation learning (RTL) task, where the model learns to use one-side contextualized token representation to reconstruct its translation counterpart. This reconstruction objective encourages the model to embed translation information into the token representation. Compared to other token-level alignment methods such as translation language modeling, RTL is more suitable for dual encoder architectures and is computationally efficient. Extensive experiments on three sentencelevel cross-lingual benchmarks demonstrate that our approach can significantly improve sentence embedding. Our code is available at https:\/\/github.com\/ChillingDream\/DAP.",
        "id":258715255
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":219302229
      },
      {
        "title":"Analyse formelle d'exigences en langue naturelle pour la conception de systèmes cyber-physiques",
        "text":"Cet article explore la construction de représentations formelles d'énoncés en langue naturelle. Le passage d'un langage naturel à une représentation logique est réalisé avec un formalisme grammatical, reliant l'analyse syntaxique de l'énoncé à une représentation sémantique. Nous ciblons l'aspect comportemental des cahiers des charges pour les systèmes cyber-physiques, c'est-à-dire tout type de systèmes dans lesquels des composants logiciels interagissent étroitement avec un environnement physique. Dans ce cadre, l'enjeu serait d'apporter une aide au concepteur. Il s'agit de permettre de simuler et vérifier, par des méthodes automatiques ou assistées, des cahiers des charges \"systèmes\" exprimés en langue naturelle. Cet article présente des solutions existantes qui pourraient être combinées en vue de la résolution de la problématique exposée.ABSTRACTFormal analysis of natural language requirements for the design of cyber-physical systemsThis paper focuses on the construction of formal representations of natural language texts. The mapping from a natural language to a logical representation is realized with a grammatical formalism, linking the syntactic analysis of the text to a semantic representation. We target the behavioral aspect of the specifications for cyber-physical systems, ie any type of system in which software components interact closely with a physical environment. In this way, the challenge would be to provide assistance to the designer. So, we could simulate and verify, by automatic or assisted methods, \"systems\" specifications expressed in natural language. This paper presents some existing contributions that could enable progress on this issue.",
        "id":53090141
      },
      {
        "title":"Verbframator: Semi-Automatic Verb Frame Annotator Tool with Special Reference to Marathi",
        "text":"The sentence is incomplete without a verb in a language. A verb is majorly responsible for giving the meaning to a sentence. Any sentence can be represented in the form of a verb frame. Verb frames are mainly developed as a knowledge resource which can be used in various semantic level Natural Language Processing (NLP) activities. This paper presents the Verbframator -a verb frame annotator tool which automatically extracts and generates verb frames of example sentences from Marathi wordnet. It also helps in generating Shakti Standard Format (SSF) files of the given example sentences. The generated verb frames and SSF files can be used in the dependency tree banking and other NLP applications like machine translation, paraphrasing, natural language generation, etc.",
        "id":17645754
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there any paper that utilizes Gaussian processes to analyze the vulnerability of text-conditioned generative models?",
    "positive_ctxs":[
      {
        "title":"Query-Efficient Black-Box Red Teaming via Bayesian Optimization",
        "text":"The deployment of large-scale generative models is often restricted by their potential risk of causing harm to users in unpredictable ways. We focus on the problem of black-box red teaming, where a red team generates test cases and interacts with the victim model to discover a diverse set of failures with limited query access. Existing red teaming methods construct test cases based on human supervision or language model (LM) and query all test cases in a brute-force manner without incorporating any information from past evaluations, resulting in a prohibitively large number of queries. To this end, we propose Bayesian red teaming (BRT), novel query-efficient blackbox red teaming methods based on Bayesian optimization, which iteratively identify diverse positive test cases leading to model failures by utilizing the pre-defined user input pool and the past evaluations. Experimental results on various user input pools demonstrate that our method consistently finds a significantly larger number of diverse positive test cases under the limited query budget than the baseline methods. The source code is available at https:\/\/github.com\/snu-mllab\/Bayesian-Red-Teaming.",
        "id":258960443
      }
    ],
    "negative_ctxs":[
      {
        "title":"Domain Adaptation in Multilingual and Multi-Domain Monolingual Settings for Complex Word Identification",
        "text":"Complex word identification (CWI) is a cornerstone process towards proper text simplification. CWI is highly dependent on context, whereas its difficulty is augmented by the scarcity of available datasets which vary greatly in terms of domains and languages. As such, it becomes increasingly more difficult to develop a robust model that generalizes across a wide array of input examples. In this paper, we propose a novel training technique for the CWI task based on domain adaptation to improve the target character and context representations. This technique addresses the problem of working with multiple domains, inasmuch as it creates a way of smoothing the differences between the explored datasets. Moreover, we also propose a similar auxiliary task, namely text simplification, that can be used to complement lexical complexity prediction. Our model obtains a boost of up to 2.42% in terms of Pearson Correlation Coefficients in contrast to vanilla training techniques, when considering the CompLex from the Lexical Complexity Prediction 2021 dataset. At the same time, we obtain an increase of 3% in Pearson scores, while considering a cross-lingual setup relying on the Complex Word Identification 2018 dataset. In addition, our model yields state-ofthe-art results in terms of Mean Absolute Error.",
        "id":248779868
      },
      {
        "title":"Temporal Discourse Models for Narrative Structure",
        "text":"Getting a machine to understand human narratives has been a classic challenge for NLP and AI. This paper proposes a new representation for the temporal structure of narratives. The representation is parsimonious, using temporal relations as surrogates for discourse relations. The narrative models, called Temporal Discourse Models, are treestructured, where nodes include abstract events interpreted as pairs of time points and where the dominance relation is expressed by temporal inclusion. Annotation examples and challenges are discussed, along with a report on progress to date in creating annotated corpora.",
        "id":17755594
      },
      {
        "title":"TransPerfect's Private Neural Machine Translation Portal",
        "text":"We will present our solution to replace the usage of publicly available machine translation (MT) services in companies where privacy and confidentiality are key. Our MT portal can translate across a variety of languages using neural machine translation, and supports an extensive number of file types. Corporations are using it to enable multilingual communication everywhere.",
        "id":258890929
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there any paper that utilizes graph structure to model conversation history?",
    "positive_ctxs":[
      {
        "title":"History Semantic Graph Enhanced Conversational KBQA with Temporal Information Modeling",
        "text":"Context information modeling is an important task in conversational KBQA. However, existing methods usually assume the independence of utterances and model them in isolation. In this paper, we propose a History Semantic Graph Enhanced KBQA model (HSGE) that is able to effectively model long-range semantic dependencies in conversation history while maintaining low computational cost. The framework incorporates a context-aware encoder, which employs a dynamic memory decay mechanism and models context at different levels of granularity. We evaluate HSGE on a widely used benchmark dataset for complex sequential question answering. Experimental results demonstrate that it outperforms existing baselines averaged on all question types.",
        "id":259137386
      }
    ],
    "negative_ctxs":[
      {
        "title":"Exploiting Document Structures and Cluster Consistencies for Event Coreference Resolution",
        "text":"We study the problem of event coreference resolution (ECR) that seeks to group coreferent event mentions into the same clusters. Deep learning methods have recently been applied for this task to deliver state-of-the-art performance. However, existing deep learning models for ECR are limited in that they cannot exploit important interactions between relevant objects for ECR, e.g., context words and entity mentions, to support the encoding of document-level context. In addition, consistency constraints between golden and predicted clusters of event mentions have not been considered to improve representation learning in prior deep learning models for ECR. This work addresses such limitations by introducing a novel deep learning model for ECR. At the core of our model are document structures to explicitly capture relevant objects for ECR. Our document structures introduce diverse knowledge sources (discourse, syntax, semantics) to compute edges\/interactions between structure nodes for document-level representation learning. We also present novel regularization techniques based on consistencies of golden and predicted clusters for event mentions in documents. Extensive experiments show that our model achieve state-of-the-art performance on two benchmark datasets.",
        "id":236460010
      },
      {
        "title":"A UIMA Database Interface for Managing NLP-related Text Annotations",
        "text":"NLP and automatic text analysis necessarily involve the annotation of natural language texts. The Apache Unstructured Information Management applications (UIMA) framework is used in several projects, tools and resources, and has become a de facto standard in this area. Despite the multiple use of UIMA as a document-based schema, it does not provide native database support. In order to facilitate distributed storage and enable UIMA-based projects to perform targeted queries, we have developed the UIMA Database Interface (UIMA DI). UIMA DI sets up an environment for a generic use of UIMA documents in database systems. In addition, the integration of UIMA DI into rights and resource management tools enables user and group-specific access to UIMA documents and provides data protection. Finally, UIMA documents can be made accessible for third party programs. UIMA DI, which we evaluate in relation to file system-based storage, is available under the GPLv3 license via GitHub.",
        "id":21700667
      },
      {
        "title":"TSDPMM: Incorporating Prior Topic Knowledge into Dirichlet Process Mixture Models for Text Clustering",
        "text":"Dirichlet process mixture model (DPM-M) has great potential for detecting the underlying structure of data. Extensive studies have applied it for text clustering in terms of topics. However, due to the unsupervised nature, the topic clusters are always less satisfactory. Considering that people often have some prior knowledge about which potential topics should exist in given data, we aim to incorporate such knowledge into the DPMM to improve text clustering. We propose a novel model TSDPMM based on a new seeded Pólya urn scheme. Experimental results on document clustering across three datasets demonstrate our proposed TSDPMM significantly outperforms stateof-the-art DPMM model and can be applied in a lifelong learning framework.",
        "id":14659218
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_acl",
    "question":"Is there any paper that utilizes masked language modeling to defend against word-level adversarial attacks?",
    "positive_ctxs":[
      {
        "title":"RMLM: A Flexible Defense Framework for Proactively Mitigating Word-level Adversarial Attacks",
        "text":"Adversarial attacks on deep neural networks keep raising security concerns in natural language processing research. Existing defenses focus on improving the robustness of the victim model in the training stage. However, they often neglect to proactively mitigate adversarial attacks during inference. Towards this overlooked aspect, we propose a defense framework that aims to mitigate attacks by confusing attackers and correcting adversarial contexts that are caused by malicious perturbations. Our framework comprises three components: (1) a synonym-based transformation to randomly corrupt adversarial contexts in the word level, (2) a developed BERT defender to correct abnormal contexts in the representation level, and (3) a simple detection method to filter out adversarial examples, any of which can be flexibly combined. Additionally, our framework helps improve the robustness of the victim model during training. Extensive experiments demonstrate the effectiveness of our framework in defending against word-level adversarial attacks.",
        "id":259370879
      }
    ],
    "negative_ctxs":[
      {
        "title":"Investigating Active Learning Sampling Strategies for Extreme Multi Label Text Classification",
        "text":"Large scale, multi-label text datasets with a high number of classes are expensive to annotate, even more so if they belong to specific language domains. In this work, we aim to build classifiers for these datasets using Active Learning in order to reduce the labeling effort. We outline the challenges when dealing with extreme multi-label settings and show the limitations of existing pool-based Active Learning strategies by considering their effectiveness as well as efficiency in terms of computational cost. In addition, we present five multi-label datasets which were compiled from hierarchical classification tasks to serve as benchmarks in the context of extreme multi-label classification for future experiments. Finally, we provide insights into multi-class, multi-label evaluation and present an improved classifier architecture on top of pre-trained transformer language models.",
        "id":251465642
      },
      {
        "title":"Max-Violation Perceptron and Forced Decoding for Scalable MT Training",
        "text":"While large-scale discriminative training has triumphed in many NLP problems, its definite success on machine translation has been largely elusive. Most recent efforts along this line are not scalable (training on the small dev set with features from top ∼100 most frequent words) and overly complicated. We instead present a very simple yet theoretically motivated approach by extending the recent framework of \"violation-fixing perceptron\", using forced decoding to compute the target derivations. Extensive phrase-based translation experiments on both Chinese-to-English and Spanish-to-English tasks show substantial gains in BLEU by up to +2.3\/+2.0 on dev\/test over MERT, thanks to 20M+ sparse features. This is the first successful effort of large-scale online discriminative training for MT.",
        "id":1613767
      },
      {
        "title":"",
        "text":"",
        "id":1183270
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there any research paper that can extract attributes from both a predefined label set and the surrounding context?",
    "positive_ctxs":[
      {
        "title":"AtTGen: Attribute Tree Generation for Real-World Attribute Joint Extraction",
        "text":"Attribute extraction aims to identify attribute names and the corresponding values from descriptive texts, which is the foundation for extensive downstream applications such as knowledge graph construction, search engines, and e-Commerce. In previous studies, attribute extraction is generally treated as a classification problem for predicting attribute types or a sequence tagging problem for labeling attribute values, where two paradigms, i.e., closed-world and open-world assumption, are involved. However, both of these paradigms have limitations in terms of real-world applications. And prior studies attempting to integrate these paradigms through ensemble, pipeline, and co-training models, still face challenges like cascading errors, high computational overhead, and difficulty in training. To address these existing problems, this paper presents Attribute Tree, a unified formulation for realworld attribute extraction application, where closed-world, open-world, and semi-open attribute extraction tasks are modeled uniformly. Then a text-to-tree generation model, AtTGen, is proposed to learn annotations from different scenarios efficiently and consistently. Experiments demonstrate that our proposed paradigm well covers various scenarios for real-world applications, and the model achieves state-ofthe-art, outperforming existing methods by a large margin on three datasets. Our code, pretrained model, and datasets are available at https:\/\/github.com\/lsvih\/AtTGen.",
        "id":259370741
      }
    ],
    "negative_ctxs":[
      {
        "title":"Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling",
        "text":"Abstractive summarization models often generate inconsistent summaries containing factual errors or hallucinated content. Recent works focus on correcting factual errors in generated summaries via post-editing. Such correction models are trained using adversarial nonfactual summaries constructed using heuristic rules for injecting errors. However, generating non-factual summaries using heuristics often does not generalize well to actual model errors. In this work, we propose to generate hard, representative synthetic examples of nonfactual summaries through infilling language models. With this data, we train a more robust fact-correction model to post-edit the summaries to improve factual consistency. Through quantitative and qualitative experiments on two popular summarization datasets-CNN\/DM and XSum-we show that our approach vastly outperforms prior methods in correcting erroneous summaries. Our model-FACTEDITimproves factuality scores by over ∼11 points on CNN\/DM and over ∼31 points on XSum on average across multiple summarization models, producing more factual summaries while maintaining competitive summarization quality. 1",
        "id":253098133
      },
      {
        "title":"Improving Sentiment Classification in Slovak Language",
        "text":"Using different neural network architectures is widely spread for many different NLP tasks. Unfortunately, most of the research is performed and evaluated only in English language and minor languages are often omitted. We believe using similar architectures for other languages can show interesting results. In this paper, we present our study on methods for improving sentiment classification in Slovak language. We performed several experiments for two different datasets, one containing customer reviews, the other one general Twitter posts. We show comparison of performance of different neural network architectures and also different word representations. We show that another improvement can be achieved by using a model ensemble. We performed experiments utilizing different methods of model ensemble. Our proposed models achieved better results than previous models for both datasets. Our experiments showed also other potential research areas.",
        "id":201639088
      },
      {
        "title":"DECEMBERT: Learning from Noisy Instructional Videos via Dense Captions and Entropy Minimization",
        "text":"Leveraging large-scale unlabeled web videos such as instructional videos for pre-training followed by task-specific finetuning has become the de facto approach for many videoand-language tasks. However, these instructional videos are very noisy, the accompanying ASR narrations are often incomplete, and can be irrelevant to or temporally misaligned with the visual content, limiting the performance of the models trained on such data. To address these issues, we propose an improved video-and-language pre-training method that first adds automatically-extracted dense region captions from the video frames as auxiliary text input, to provide informative visual cues for learning better video and language associations. Second, to alleviate the temporal misalignment issue, our method incorporates an entropy minimization-based constrained attention loss, to encourage the model to automatically focus on the correct caption from a pool of candidate ASR captions. Our overall approach is named DECEMBERT (Dense Captions and Entropy Minimization). Comprehensive experiments on three video-and-language tasks (text-to-video retrieval, video captioning, and video question answering) across five datasets demonstrate that our approach outperforms previous state-of-the-art methods. Ablation studies on pre-training and downstream tasks show that adding dense captions and constrained attention loss help improve the model performance. Lastly, we also provide attention visualization to show the effect of applying the proposed constrained attention loss. 1",
        "id":235097643
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there any work that allows large numbers of model outputs to be encoded and compared by causal language models in a single forward pass?",
    "positive_ctxs":[
      {
        "title":"EEL: Efficiently Encoding Lattices for Reranking",
        "text":"Standard decoding approaches for conditional text generation tasks typically search for an output hypothesis with high model probability, but this may not yield the best hypothesis according to human judgments of quality. Reranking to optimize for downstream metrics can better optimize for quality, but many metrics of interest are computed with pre-trained language models, which are slow to apply to large numbers of hypotheses. We explore an approach for reranking hypotheses by using Transformers to efficiently encode lattices of generated outputs, a method we call EEL. With a single Transformer pass over the entire lattice, we can approximately compute a contextualized representation of each token as if it were only part of a single hypothesis in isolation. We combine this approach with a new class of token-factored rerankers (TFRs) that allow for efficient extraction of high reranker-scoring hypotheses from the lattice. Empirically, our approach incurs minimal degradation error compared to the exponentially slower approach of encoding each hypothesis individually. When applying EEL with TFRs across three text generation tasks, our results show both substantial speedup compared to naive reranking and often better performance on downstream metrics than comparable approaches.",
        "id":258999673
      }
    ],
    "negative_ctxs":[
      {
        "title":"On the annotation of vague expressions: a case study on Romanian historical texts",
        "text":"Current approaches in Digital .Humanities tend to ignore a central aspect of any hermeneutic introspection: the intrinsic vagueness of analyzed texts. Especially when dealing with historical documents neglecting vagueness has important implications on the interpretation of the results. In this paper we present current limitation of annotation approaches and describe a current methodology for annotating vagueness for historical Romanian texts.",
        "id":30604919
      },
      {
        "title":"Clustering of Multi-Word Named Entity variants: Multilingual Evaluation",
        "text":"Multi-word entities, such as organisation names, are frequently written in many different ways. We have previously automatically identified over one million acronym pairs in 22 languages, consisting of their short form (e.g. EC) and their corresponding long forms (e.g. European Commission, European Union Commission). In order to automatically group such long form variants as belonging to the same entity, we cluster them, using bottom-up hierarchical clustering and pair-wise string similarity metrics. In this paper, we address the issue of how to evaluate the named entity variant clusters automatically, with minimal human annotation effort. We present experiments that make use of Wikipedia redirection tables and we show that this method produces good results.",
        "id":17957898
      },
      {
        "title":"Evaluation of automatic summaries: Metrics under varying data conditions",
        "text":"In evaluation of automatic summaries, it is necessary to employ multiple topics and human-produced models in order for the assessment to be stable and reliable. However, providing multiple topics and models is costly and time-consuming. This paper examines the relation between the number of available models and topics and the correlations with human judgment obtained by automatic metrics ROUGE and BE, as well as the manual Pyramid method. Testing all these methods on the same data set, taken from the TAC 2008 Summarization track, allows us to compare and contrast the methods under different conditions.",
        "id":5312836
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there any work that attacks language models in dialogue generation?",
    "positive_ctxs":[
      {
        "title":"White-Box Multi-Objective Adversarial Attack on Dialogue Generation",
        "text":"Pre-trained transformers are popular in stateof-the-art dialogue generation (DG) systems. Such language models are, however, vulnerable to various adversarial samples as studied in traditional tasks such as text classification, which inspires our curiosity about their robustness in DG systems. One main challenge of attacking DG models is that perturbations on the current sentence can hardly degrade the response accuracy because the unchanged chat histories are also considered for decision-making. Instead of merely pursuing pitfalls of performance metrics such as BLEU, ROUGE, we observe that crafting adversarial samples to force longer generation outputs benefits attack effectiveness-the generated responses are typically irrelevant, lengthy, and repetitive. To this end, we propose a white-box multi-objective attack method called DGSlow. Specifically, DGSlow balances two objectives-generation accuracy and length, via a gradient-based multiobjective optimizer and applies an adaptive searching mechanism to iteratively craft adversarial samples with only a few modifications. Comprehensive experiments 1 on four benchmark datasets demonstrate that DGSlow could significantly degrade state-of-the-art DG models with a higher success rate than traditional accuracy-based methods. Besides, our crafted sentences also exhibit strong transferability in attacking other models.",
        "id":258546855
      }
    ],
    "negative_ctxs":[
      {
        "title":"AUTO-ENCODING SEQUENTIAL MONTE CARLO",
        "text":"We build on auto-encoding sequential Monte Carlo (AESMC): 1 a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models. Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions. We develop additional theoretical insights and experiment with a new training procedure which can improve both model and proposal learning. We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.",
        "id":20140417
      },
      {
        "title":"The Centre and Periphery of Discourse Connectives",
        "text":"The paper tries to contribute to the general definition of discourse connectives. It examines connectives in broader sense, i.e. all language expressions that have an ability to express discourse relations within a text (e.g. both conjunctions like but, and, or and expressions like the condition for this is, due to this situation etc.). The paper tries to classify connectives from different perspectives and to divide them into several groups to specify their similarities and differences. We try to discuss various attributes an expression must have to be a connective. We understand discourse connectives as a set of expressions with a center and periphery and we focus here mainly on the periphery -i.e. on description of the secondary connectives (like the reason is simple, this means that... etc.) because it is not much investigated but a very current theme of discourse analysis.Discourse Connectives in the Prague Dependency TreebankOur research on discourse connectives in Czech was carried out on the data of the Prague Dependency Treebank (PDT) -a manually annotated corpus of about 50 thousand sentences from newspaper texts containing, among others, annotation of discourse relations.",
        "id":9473711
      },
      {
        "title":"Using J-K-fold Cross Validation to Reduce Variance When Tuning NLP Models",
        "text":"K-fold cross validation (CV) is a popular method for estimating the true performance of machine learning models, allowing model selection and parameter tuning. However, the very process of CV requires random partitioning of the data and so our performance estimates are in fact stochastic, with variability that can be substantial for natural language processing tasks. We demonstrate that these unstable estimates cannot be relied upon for effective parameter tuning. The resulting tuned parameters are highly sensitive to how our data is partitioned, meaning that we often select sub-optimal parameter choices and have serious reproducibility issues.Instead, we propose to use the less variable J-K-fold CV, in which J independent K-fold cross validations are used to assess performance. Our main contributions are extending J-K-fold CV from performance estimation to parameter tuning and investigating how to choose J and K. We argue that variability is more important than bias for effective tuning and so advocate lower choices of K than are typically seen in the NLP literature, instead use the saved computation to increase J. To demonstrate the generality of our recommendations we investigate a wide range of case-studies: sentiment classification (both general and target-specific), part-of-speech tagging and document classification.",
        "id":49311178
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there any works that explores how to achieve balance between representativeness and diversity in chosen samples for few-shot data selection?",
    "positive_ctxs":[
      {
        "title":"Cold-Start Data Selection for Better Few-shot Language Model Fine-tuning: A Prompt-based Uncertainty Propagation Approach",
        "text":"Large Language Models have demonstrated remarkable few-shot performance, but the performance can be sensitive to the selection of few-shot instances. We present PATRON, a prompt-based data selection method for pretrained language model fine-tuning under coldstart scenarios, i.e., no initial labeled data are available. In PATRON, we design (1) a promptbased uncertainty propagation approach to estimate the importance of data points and (2) a partition-then-rewrite (PTR) strategy to promote sample diversity when querying for annotations. Experiments on six text classification datasets show that PATRON outperforms the strongest cold-start data selection baselines by up to 6.9%. Besides, with 128 labels only, PA-TRON achieves 91.0% and 92.1% of the fully supervised performance based on vanilla finetuning and prompt-based learning respectively. Our implementation of PATRON is available at https:\/\/github.com\/yueyu1030\/Patron.",
        "id":252280753
      }
    ],
    "negative_ctxs":[
      {
        "title":"Efficient Domain Adaptation of Language Models via Adaptive Tokenization",
        "text":"Contextual embedding-based language models trained on large data sets, such as BERT and RoBERTa, provide strong performance across a wide range of tasks and are ubiquitous in modern NLP. It has been observed that fine-tuning these models on tasks involving data from domains different from that on which they were pretrained can lead to suboptimal performance. Recent work has explored approaches to adapt pretrained language models to new domains by incorporating additional pretraining using domain-specific corpora and task data. We propose an alternative approach for transferring pretrained language models to new domains by adapting their tokenizers. We show that domain-specific subword sequences can be efficiently determined directly from divergences in the conditional token distributions of the base and domain-specific corpora. In datasets from four disparate domains, we find adaptive tokenization on a pretrained RoBERTa model provides >97% of the performance benefits of domain specific pretraining. Our approach produces smaller models and less training and inference time than other approaches using tokenizer augmentation. While adaptive tokenization incurs a 6% increase in model parameters in our experimentation, due to the introduction of 10k new domain-specific tokens, our approach, using 64 vCPUs, is 72x faster than further pretraining the language model on domain-specific corpora on 8 TPUs.",
        "id":237513469
      },
      {
        "title":"Multilingual unsupervised sequence segmentation transfers to extremely low-resource languages",
        "text":"We show that unsupervised sequencesegmentation performance can be transferred to extremely low-resource languages by pretraining a Masked Segmental Language Model (Downey et al., 2021)  multilingually. Further, we show that this transfer can be achieved by training over a collection of low-resource languages that are typologically similar (but phylogenetically unrelated) to the target language. In our experiments, we transfer from a collection of 10 Indigenous American languages (AmericasNLP, Mager et al., 2021)   to K'iche', a Mayan language. We compare our multilingual model to a monolingual (from-scratch) baseline, as well as a model pre-trained on Quechua only. We show that the multilingual pre-trained approach yields consistent segmentation quality across target dataset sizes, exceeding the monolingual baseline in 6\/10 experimental settings. Our model yields especially strong results at small target sizes, including a zero-shot performance of 20.6 F1. These results have promising implications for low-resource NLP pipelines involving human-like linguistic units, such as the sparse transcription framework proposed by Bird (2020).",
        "id":239016040
      },
      {
        "title":"Under review as a conference paper at ICLR 2017 LEARNING RECURRENT REPRESENTATIONS FOR HIERARCHICAL BEHAVIOR MODELING",
        "text":"We propose a framework for detecting action patterns from motion sequences and modeling the sensory-motor relationship of animals, using a generative recurrent neural network. The network has a discriminative part (classifying actions) and a generative part (predicting motion), whose recurrent cells are laterally connected, allowing higher levels of the network to represent high level behavioral phenomena. We test our framework on two types of data, fruit fly behavior and online handwriting. Our results show that 1) taking advantage of unlabeled sequences, by predicting future motion, significantly improves action detection performance when training labels are scarce, 2) the network learns to represent high level phenomena such as writer identity and fly gender, without supervision, and 3) simulated motion trajectories, generated by treating motion prediction as input to the network, look realistic and may be used to qualitatively evaluate whether the model has learnt generative control rules.",
        "id":16926563
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there commonsense reasoning dataset which generates diverse sentences to describe the relation between concepts?",
    "positive_ctxs":[
      {
        "title":"DimonGen: Diversified Generative Commonsense Reasoning for Explaining Concept Relationships",
        "text":"In this paper, we propose DimonGen, which aims to generate diverse sentences describing concept relationships in various everyday scenarios. To support this, we first create a benchmark dataset for this task by adapting the existing CommonGen dataset. We then propose a two-stage model called MoREE to generate the target sentences. MoREE consists of a mixture of retrievers model that retrieves diverse context sentences related to the given concepts, and a mixture of generators model that generates diverse sentences based on the retrieved contexts. We conduct experiments on the DimonGen task and show that MoREE outperforms strong baselines in terms of both the quality and diversity of the generated sentences. Our results demonstrate that MoREE is able to generate diverse sentences that reflect different relationships between concepts, leading to a comprehensive understanding of concept relationships. 1",
        "id":254877165
      }
    ],
    "negative_ctxs":[
      {
        "title":"Relation Specific Transformations for Open World Knowledge Graph Completion",
        "text":"We propose an open-world knowledge graph completion model that can be combined with common closed-world approaches (such as ComplEx) and enhance them to exploit text-based representations for entities unseen in training. Our model learns relation-specific transformation functions from text-based embedding space to graph-based embedding space, where the closedworld link prediction model can be applied. We demonstrate state-of-the-art results on common open-world benchmarks and show that our approach benefits from relation-specific transformation functions (RST), giving substantial improvements over a relation-agnostic approach.",
        "id":227231162
      },
      {
        "title":"OUTCOME-DIRECTED REINFORCEMENT LEARNING BY UNCERTAINTY & TEMPORAL DISTANCE-AWARE CURRICULUM GOAL GENERATION",
        "text":"Current reinforcement learning (RL) often suffers when solving a challenging exploration problem where the desired outcomes or high rewards are rarely observed. Even though curriculum RL, a framework that solves complex tasks by proposing a sequence of surrogate tasks, shows reasonable results, most of the previous works still have difficulty in proposing curriculum due to the absence of a mechanism for obtaining calibrated guidance to the desired outcome state without any prior domain knowledge. To alleviate it, we propose an uncertainty & temporal distance-aware curriculum goal generation method for the outcomedirected RL via solving a bipartite matching problem. It could not only provide precisely calibrated guidance of the curriculum to the desired outcome states but also bring much better sample efficiency and geometry-agnostic curriculum goal proposal capability compared to previous curriculum RL methods. We demonstrate that our algorithm significantly outperforms these prior methods in a variety of challenging navigation tasks and robotic manipulation tasks in a quantitative and qualitative way. 1 * Equal contribution.",
        "id":256358497
      },
      {
        "title":"Named Entity Recognition in Tweets: An Experimental Study",
        "text":"People tweet more than 100 Million times daily, yielding a noisy, informal, but sometimes informative corpus of 140-character messages that mirrors the zeitgeist in an unprecedented manner. The performance of standard NLP tools is severely degraded on tweets. This paper addresses this issue by re-building the NLP pipeline beginning with part-of-speech tagging, through chunking, to named-entity recognition. Our novel T-NER system doubles F 1 score compared with the Stanford NER system. T-NER leverages the redundancy inherent in tweets to achieve this performance, using LabeledLDA to exploit Freebase dictionaries as a source of distant supervision. LabeledLDA outperforms cotraining, increasing F 1 by 25% over ten common entity types.Our NLP tools are available at: http:\/\/ github.com\/aritter\/twitter_nlp",
        "id":12861120
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there such a factuality evaluation dataset that can be used to evaluate the performance of fact-checking models on summaries generated by latest summarization models?",
    "positive_ctxs":[
      {
        "title":"",
        "text":"The propensity of abstractive summarization models to make factual errors has been studied extensively, including design of metrics to detect factual errors and annotation of errors in current systems' outputs. However, the everevolving nature of summarization systems, metrics, and annotated benchmarks makes factuality evaluation a moving target, and drawing clear comparisons among metrics has become increasingly difficult. In this work, we aggregate factuality error annotations from nine existing datasets and stratify them according to the underlying summarization model. We compare performance of state-of-the-art factuality metrics, including recent ChatGPT-based metrics, on this stratified benchmark and show that their performance varies significantly across different types of summarization models. Critically, our analysis shows that much of the recent improvement in the factuality detection space has been on summaries from older (pre-Transformer) models instead of more relevant recent summarization models. We further perform a finer-grained analysis per error-type and find similar performance variance across error types for different factuality metrics. Our results show that no one metric is superior in all settings or for all error types, and we provide recommendations for best practices given these insights. 1",
        "id":249062579
      }
    ],
    "negative_ctxs":[
      {
        "title":"An Evaluation Resource for Geographic Information Retrieval",
        "text":"In this paper we present an evaluation resource for geographic information retrieval developed within the Cross Language Evaluation Forum (CLEF). The GeoCLEF track is dedicated to the evaluation of geographic information retrieval systems. The resource encompasses more than 600,000 documents, 75 topics so far, and more than 100,000 relevance judgments for these topics. Geographic information retrieval requires an evaluation resource which represents realistic information needs and which is geographically challenging. Some experimental results and analysis are reported.",
        "id":1963673
      },
      {
        "title":"UNIFICATION WITH LAZY NON-REDUNDANT COPYING",
        "text":"This paper presents a unification procedure which eliminates the redundant copying of structures by using a lazy incremental copying appr0a~:h to achieve structure sharing. Copying of structures accounts for a considerable amount of the total processing time. Several methods have been proposed to minimize the amount of necessary copying. Lazy Incremental Copying (LIC) is presented as a new solution to the copying problem. It synthesizes ideas of lazy copying with the notion of chronological dereferencing for achieving a high amount of structure sharing.",
        "id":14046171
      },
      {
        "title":"",
        "text":"",
        "id":61178558
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Is there such a reading comprehension dataset in understanding a snippet from a long story book, while it requires to integrate the necessary long history texts before the snippet to full understand it?",
    "positive_ctxs":[
      {
        "title":"Personality Understanding of Fictional Characters during Book Reading",
        "text":"Comprehending characters' personalities is a crucial aspect of story reading. As readers engage with a story, their understanding of a character evolves based on new events and information; and multiple fine-grained aspects of personalities can be perceived. This leads to a natural problem of situated and fine-grained personality understanding. The problem has not been studied in the NLP field, primarily due to the lack of appropriate datasets mimicking the process of book reading. We present the first labeled dataset PERSONET for this problem. Our novel annotation strategy involves annotating user notes from online reading apps as a proxy for the original books. Experiments and human studies indicate that our dataset construction is both efficient and accurate; and our task heavily relies on long-term context to achieve accurate predictions for both machines and humans. 1",
        "id":258740758
      }
    ],
    "negative_ctxs":[
      {
        "title":"SOME LINGUISTIC ASPECTS FOR AUTOMATIC TEXT UNDERSTANDING",
        "text":"This paper proposes a system of mapping classes of syntactic structures as instruments for automatic text under-",
        "id":32969140
      },
      {
        "title":"L3Cube-MahaNER: A Marathi Named Entity Recognition Dataset and BERT models",
        "text":"Named Entity Recognition (NER) is a basic NLP task and finds major applications in conversational and search systems. It helps us identify key entities in a sentence used for the downstream application. NER or similar slot filling systems for popular languages have been heavily used in commercial applications. In this work, we focus on Marathi, an Indian language, spoken prominently by the people of Maharashtra state. Marathi is a low resource language and still lacks useful NER resources. We present L3Cube-MahaNER, the first major gold standard named entity recognition dataset in Marathi. We also describe the manual annotation guidelines followed during the process. In the end, we benchmark the dataset on different CNN, LSTM, and Transformer based models like mBERT, XLM-RoBERTa, IndicBERT, MahaBERT, etc. The MahaBERT provides the best performance among all the models. The data and models are available at https:\/\/github.com\/l3cubepune\/MarathiNLP",
        "id":248157575
      },
      {
        "title":"CHOOSE YOUR WORDS CAREFULLY",
        "text":"",
        "id":62222649
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Provide an example of a paper which proposes a method to learn a dynamic (conditioned on the input) sequence tokenizer (segmenter) via standard gradient backpropagation.",
    "positive_ctxs":[
      {
        "title":"Efficient Transformers with Dynamic Token Pooling",
        "text":"Transformers achieve unrivalled performance in modelling language, but remain inefficient in terms of memory and time complexity. A possible remedy is to reduce the sequence length in the intermediate layers by pooling fixed-length segments of tokens. Nevertheless, natural units of meaning, such as words or phrases, display varying sizes. To address this mismatch, we equip language models with a dynamic-pooling mechanism, which predicts segment boundaries in an autoregressive fashion. We compare several methods to infer boundaries, including end-to-end learning through stochastic re-parameterisation, supervised learning (based on segmentations from subword tokenizers or spikes in conditional entropy), as well as linguistically motivated boundaries. We perform character-level evaluation on texts from multiple datasets and morphologically diverse languages. The results demonstrate that dynamic pooling, which jointly segments and models language, is both faster and more accurate than vanilla Transformers and fixed-length pooling within the same computational budget.",
        "id":253581399
      }
    ],
    "negative_ctxs":[
      {
        "title":"Semi-Supervised Policy Initialization for Playing Games with Language Hints",
        "text":"Using natural language as a hint can supply an additional reward for playing sparse-reward games. Achieving a goal should involve several different hints, while the given hints are usually incomplete. Those unmentioned latent hints still rely on the sparse reward signal, and make the learning process difficult. In this paper, we propose semi-supervised initialization (SSI) that allows the agent to learn from various possible hints before training under different tasks. Experiments show that SSI not only helps to learn faster (1.2x) but also has a higher success rate (11% relative improvement) of the final policy.",
        "id":235097328
      },
      {
        "title":"Guided Alignment Training for Topic-Aware Neural Machine Translation",
        "text":"In this paper, we propose an effective way for biasing the attention mechanism of a sequence-to-sequence neural machine translation (NMT) model towards the well-studied statistical word alignment models. We show that our novel guided alignment training approach improves translation quality on real-life ecommerce texts consisting of product titles and descriptions, overcoming the problems posed by many unknown words and a large type\/token ratio. We also show that meta-data associated with input texts such as topic or category information can significantly improve translation quality when used as an additional signal to the decoder part of the network. With both novel features, the BLEU score of the NMT system on a product title set improves from 18.6 to 21.3%. Even larger MT quality gains are obtained through domain adaptation of a general domain NMT system to e-commerce data. The developed NMT system also performs well on the IWSLT speech translation task, where an ensemble of four variant systems outperforms the phrase-based baseline by 2.1% BLEU absolute.",
        "id":17078659
      },
      {
        "title":"Persona or Context? Towards Building Context adaptive Personalized Persuasive Virtual Sales Assistant",
        "text":"Task-oriented conversational agents are gaining immense popularity and success in a wide range of tasks, from flight ticket booking to online shopping. However, the existing systems presume that end-users will always have a pre-determined and servable task goal, which results in dialogue failure in hostile scenarios, such as goal unavailability. On the other hand, human agents accomplish users' tasks even in a large number of goal unavailability scenarios by persuading them towards a very similar and servable goal. Motivated by the limitation, we propose and build a novel end-to-end multi-modal persuasive dialogue system incorporated with a personalized persuasive module aided goal controller and goal persuader. The goal controller recognizes goal conflicting\/unavailability scenarios and formulates a new goal, while the goal persuader persuades users using a personalized persuasive strategy identified through dialogue context. We also present a novel automatic evaluation metric called Persuasiveness Measurement Rate (PMeR) for quantifying the persuasive capability of a conversational agent. The obtained improvements (both quantitative and qualitative) firmly establish the superiority and need of the proposed context-guided, personalized persuasive virtual agent over existing traditional task-oriented virtual agents. Furthermore, we also curated a multi-modal persuasive conversational dialogue corpus annotated with intent, slot, sentiment, and dialogue act for e-commerce domain 1 .",
        "id":253481040
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"What are some data-efficient ways to learn text embeddings thru contrastive learning?",
    "positive_ctxs":[
      {
        "title":"Composition-contrastive Learning for Sentence Embeddings",
        "text":"Vector representations of natural language are ubiquitous in search applications. Recently, various methods based on contrastive learning have been proposed to learn textual representations from unlabelled data; by maximizing alignment between minimally-perturbed embeddings of the same text, and encouraging a uniform distribution of embeddings across a broader corpus. Differently, we propose maximizing alignment between texts and a composition of their phrasal constituents. We consider several realizations of this objective and elaborate the impact on representations in each case. Experimental results on semantic textual similarity tasks show improvements over baselines that are comparable with state-of-the-art approaches. Moreover, this work is the first to do so without incurring costs in auxiliary training objectives or additional network parameters. 1",
        "id":259370776
      }
    ],
    "negative_ctxs":[
      {
        "title":"Locating Boundaries for Prosodic Constituents in Unrestricted Mandarin Texts",
        "text":"This paper proposes a three-tier prosodic hierarchy, including prosodic word, intermediate phrase and intonational phrase tiers, for Mandarin that emphasizes the use of the prosodic word instead of the lexical word as the basic prosodic unit. Both the surface difference and perceptual difference show that this is helpful for achieving high naturalness in text-to-speech conversion. Three approaches, the basic CART approach, the bottom-up hierarchical approach and the modified hierarchical approach, are presented for locating the boundaries of three prosodic constituents in unrestricted Mandarin texts. Two sets of features are used in the basic CART method: one contains syntactic phrasal information and the other does not. The one with syntactic phrasal information results in about a 1% increase in accuracy and an 11% decrease in error-cost. The performance of the modified hierarchical method produces the highest accuracy, 83%, and lowest error cost when no syntactic phrasal information is provided. It shows advantages in detecting the boundaries of intonational phrases at locations without breaking punctuation. 71.1% precision and 52.4% recall are achieved. Experiments on acceptability reveal that only 26% of the mis-assigned break indices are real infelicitous errors, and that the perceptual difference between the automatically assigned break indices and the manually annotated break indices are small.",
        "id":12168738
      },
      {
        "title":"SEQUENTIAL REPTILE: INTER-TASK GRADIENT ALIGNMENT FOR MULTILINGUAL LEARNING",
        "text":"Multilingual models jointly pretrained on multiple languages have achieved remarkable performance on various multilingual downstream tasks. Moreover, models finetuned on a single monolingual downstream task have shown to generalize to unseen languages. In this paper, we first show that it is crucial for those tasks to align gradients between them in order to maximize knowledge transfer while minimizing negative transfer. Despite its importance, the existing methods for gradient alignment either have a completely different purpose, ignore inter-task alignment, or aim to solve continual learning problems in rather inefficient ways. As a result of the misaligned gradients between tasks, the model suffers from severe negative transfer in the form of catastrophic forgetting of the knowledge acquired from the pretraining. To overcome the limitations, we propose a simple yet effective method that can efficiently align gradients between tasks. Specifically, we perform each inner-optimization by sequentially sampling batches from all the tasks, followed by a Reptile outer update. Thanks to the gradients aligned between tasks by our method, the model becomes less vulnerable to negative transfer and catastrophic forgetting. We extensively validate our method on various multi-task learning and zero-shot cross-lingual transfer tasks, where our method largely outperforms all the relevant baselines we consider. Published as a conference paper at ICLR 2022 BERT Pretrained Model High MTL Loss High Cos. Sim. Low MTL Loss Low Cos. Sim. e.g.) Seq. Reptile e.g.) Base MTL Reptile PCGrad RecAdam e.g.) Early-stopping Better trade-off b\/w MTL loss vs. cosine sim.Worse trade-offFigure 1: Concepts. Black arrows denote finetuning processes. The darker the part of the arrows, the lower the MTL loss. Upper and bottom path shows better and worse trade-off, respectively. Colored arrows denote task gradients. Blue and red color shows high and low cosine similarity, respectively. We demonstrate this concept with the actual experimental results inFig. 7a.",
        "id":238408412
      },
      {
        "title":"YUN-HPCC at SemEval-2019 Task 3: Multi-Step Ensemble Neural Network for Sentiment Analysis in Textual Conversation",
        "text":"This paper describes our approach to the emotion detection of Twitter textual conversations based on deep learning. We analyze the syntax, abbreviations, and informal-writing of Twitter; and perform perfect data preprocessing on the data to convert them to normative text. We apply a multi-step ensemble strategy to solve the problem of extremely unbalanced data in the training set. This is achieved by taking the GloVe and ELMo word vectors as input into a combination model with four different deep neural networks. The experimental results from the development dataset demonstrate that the proposed model exhibits a strong generalization ability. For evaluation on the test dataset, we integrated the results using the stacking ensemble learning approach and achieved competitive results. According to the final official review, the results of our model achieved micro-F 1 score of about 0.7588 on the final evaluation.",
        "id":184482660
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_acl",
    "question":"What are some methods for solving the class-incremetal continual learning problems?",
    "positive_ctxs":[
      {
        "title":"Rehearsal-free Continual Language Learning via Efficient Parameter Isolation",
        "text":"We study the problem of defying catastrophic forgetting when learning a series of language processing tasks. Compared with previous methods, we emphasize the importance of not caching history tasks' data, which makes the problem more challenging. Our proposed method applies the parameter isolation strategy. For each task, it allocates a small portion of private parameters and learns them with a shared pre-trained model. To load correct parameters at testing time, we introduce a simple yet effective non-parametric method. Experiments on continual language learning benchmarks show that our method is significantly better than all existing no-data-cache methods, and is comparable (or even better) than those using historical data 1 . . 2021a. Continual learning for text classification with information disentanglement based regularization. In",
        "id":259370817
      }
    ],
    "negative_ctxs":[
      {
        "title":"The preliminary study of robust speech feature extraction based on maximizing the accuracy of states in deep acoustic models",
        "text":"",
        "id":233029488
      },
      {
        "title":"Talk to Papers: Bringing Neural Question Answering to Academic Search",
        "text":"We introduce Talk to Papers 1 , which exploits the recent open-domain question answering (QA) techniques to improve the current experience of academic search. It's designed to enable researchers to use natural language queries to find precise answers and extract insights from a massive amount of academic papers. We present a large improvement over classic search engine baseline on several standard QA datasets, and provide the community a collaborative data collection tool to curate the first natural language processing research QA dataset via a community effort. *",
        "id":214802635
      },
      {
        "title":"",
        "text":"",
        "id":218692901
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_acl",
    "question":"What is a large event-coverage general-domain event argument extraction dataset?",
    "positive_ctxs":[
      {
        "title":"GENEVA: Benchmarking Generalizability for Event Argument Extraction with Hundreds of Event Types and Argument Roles",
        "text":"Recent works in Event Argument Extraction (EAE) have focused on improving model generalizability to cater to new events and domains. However, standard benchmarking datasets like ACE and ERE cover less than 40 event types and 25 entity-centric argument roles. Limited diversity and coverage hinder these datasets from adequately evaluating the generalizability of EAE models. In this paper, we first contribute by creating a large and diverse EAE ontology. This ontology is created by transforming FrameNet, a comprehensive semantic role labeling (SRL) dataset for EAE, by exploiting the similarity between these two tasks. Then, exhaustive human expert annotations are collected to build the ontology, concluding with 115 events and 220 argument roles, with a significant portion of roles not being entities. We utilize this ontology to further introduce GENEVA, a diverse generalizability benchmarking dataset comprising four test suites, aimed at evaluating models' ability to handle limited data and unseen event type generalization. We benchmark six EAE models from various families. The results show that owing to non-entity argument roles, even the best-performing model can only achieve 39% F1 score, indicating how GENEVA provides new challenges for generalization in EAE. Overall, our large and diverse EAE ontology can aid in creating more comprehensive future resources, while GENEVA is a challenging benchmarking dataset encouraging further research for improving generalizability in EAE.",
        "id":258865260
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"Mey's criticism of the functional approac~ to generative description concerns (1) the formal properties of the system proposed by Sgall et el. (its weak generative power, recursivit$~), and (2) some itl~or!~al questions connected with the mentioned approach.",
        "id":14918516
      },
      {
        "title":"thecerealkiller at SemEval-2016 Task 4: Deep Learning based System for Classifying Sentiment of Tweets on Two Point Scale",
        "text":"In this paper, we propose a deep learning system for classification of tweets on a two-point scale. Our architecture consists of a multilayered recurrent neural network having gated recurrent units. The network is pre-trained with a weakly labeled dataset of tweets to learn the sentiment specific embeddings. Then it is fine tuned on the given training dataset of the task 4B in SemEval-2016. The network does very little pre-processing for raw tweets and no post-processing at all. The proposed system achieves 3rd rank on the leaderboard of task 4B.",
        "id":16092889
      },
      {
        "title":"Approches à base de fréquences pour la simplification lexicale",
        "text":"La simplification lexicale consiste à remplacer des mots ou des phrases par leur équivalent plus simple. Dans cet article, nous présentons trois modèles de simplification lexicale, fondés sur différents critères qui font qu'un mot est plus simple à lire et à comprendre qu'un autre. Nous avons testé différentes tailles de contextes autour du mot étudié : absence de contexte avec un modèle fondé sur des fréquences de termes dans un corpus d'anglais simplifié ; quelques mots de contexte au moyen de probabilités à base de n-grammes issus de données du web ; et le contexte étendu avec un modèle fondé sur les fréquences de cooccurrences.ABSTRACTStudying frequency-based approaches to process lexical simplificationLexical simplification aims at replacing words or phrases by simpler equivalents. In this paper, we present three models for lexical simplification, focusing on the criteria that make one word simpler to read and understand than another. We tested different contexts of the considered word : no context, with a model based on word frequencies in a simplified English corpus ; a few words context, with n-grams probabilites on Web data, and an extended context, with a model based on co-occurrence frequencies.MOTS-CLÉS : simplification lexicale, fréquence lexicale, modèle de langue.",
        "id":37296498
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"What is the first paper to address the problem of predicting knowledge graphs whose nodes, links and attributes change with time?",
    "positive_ctxs":[
      {
        "title":"Holistic Prediction on a Time-Evolving Attributed Graph",
        "text":"Graph-based prediction is essential in NLP tasks such as temporal knowledge graph completion. A cardinal question in this field is, how to predict the future links, nodes, and attributes of a time-evolving attributed graph? Unfortunately, existing techniques assume that each link, node, and attribute prediction is independent, and fall short of predicting the appearance of new nodes that were not observed in the past. In this paper, we address two interrelated questions; (1) can we exploit task interdependence to improve prediction accuracy? and(2)can we predict new nodes with their attributes? We propose a unified framework that predicts node attributes and topology changes such as the appearance and disappearance of links and the emergence and loss of nodes. This framework comprises components for independent and interactive prediction and for predicting new nodes. Our experimental study using realworld data confirms that our interdependent prediction framework achieves higher accuracy than methods based on independent prediction.",
        "id":259370699
      }
    ],
    "negative_ctxs":[
      {
        "title":"MaintNet: A Collaborative Open-Source Library for Predictive Maintenance Language Resources",
        "text":"Maintenance record logbooks are an emerging text type in NLP. An important part of them typically consist of free text with many domain specific technical terms, abbreviations, and nonstandard spelling and grammar. This poses difficulties for NLP pipelines trained on standard corpora. Analyzing and annotating such documents is of particular importance in the development of predictive maintenance systems, which aim to improve operational efficiency, reduce costs, prevent accidents, and save lives. In order to facilitate and encourage research in this area, we have developed MaintNet, a collaborative open-source library of technical and domain-specific language resources. MaintNet provides novel logbook data from the aviation, automotive, and facility maintenance domains along with tools to aid in their (pre-)processing and clustering. Furthermore, it provides a way to encourage discussion on and sharing of new datasets and tools for logbook data analysis.",
        "id":218889343
      },
      {
        "title":"DIMINISHING RETURN OF VALUE EXPANSION METH- ODS IN MODEL-BASED REINFORCEMENT LEARNING",
        "text":"Model-based reinforcement learning is one approach to increase sample efficiency. However, the accuracy of the dynamics model and the resulting compounding error over modelled trajectories are commonly regarded as key limitations. A natural question to ask is: How much more sample efficiency can be gained by improving the learned dynamics models? Our paper empirically answers this question for the class of model-based value expansion methods in continuous control problems. Value expansion methods should benefit from increased model accuracy by enabling longer rollout horizons and better value function approximations. Our empirical study, which leverages oracle dynamics models to avoid compounding model errors, shows that (1) longer horizons increase sample efficiency, but the gain in improvement decreases with each additional expansion step, and (2) the increased model accuracy only marginally increases the sample efficiency compared to learned models with identical horizons. Therefore, longer horizons and increased model accuracy yield diminishing returns in terms of sample efficiency. These improvements in sample efficiency are particularly disappointing when compared to model-free value expansion methods. Even though they introduce no computational overhead, we find their performance to be on-par with model-based value expansion methods. Therefore, we conclude that the limitation of model-based value expansion methods is not the model accuracy of the learned models. While higher model accuracy is beneficial, our experiments show that even a perfect model will not provide an un-rivalled sample efficiency but that the bottleneck lies elsewhere. Levine. Model-based value estimation for efficient model-free reinforcement learning.",
        "id":259373058
      },
      {
        "title":"Linguistic annotation of the Spoken Dutch Corpus: If we had to do it all over again",
        "text":"After the successful completion of the Spoken Dutch Corpus (1998 -2003)  the time is ripe to take some time to sit back and reflect on our achievements and the procedures underlying them in order to learn from our experiences. In this paper we will in particular pay attention to issues affecting the levels of linguistic annotation, but some more general issues deserve to be treated as well (bug reporting, consistency). We will try to come up with solutions, but sometimes we want to invite further discussion from other researchers.",
        "id":3128002
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_acl",
    "question":"What is the performance of large language models in text summarization under reference-based and reference-free human evaluations?",
    "positive_ctxs":[
      {
        "title":"",
        "text":"Human evaluation is the foundation upon which the evaluation of both summarization systems and automatic metrics rests. However, existing human evaluation studies for summarization either exhibit a low inter-annotator agreement or have insufficient scale, and an in-depth analysis of human evaluation is lacking. Therefore, we address the shortcomings of existing summarization evaluation along the following axes: (1) We propose a modified summarization salience protocol, Atomic Content Units (ACUs), which is based on fine-grained semantic units and allows for a high interannotator agreement. (2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large human evaluation dataset consisting of 22,000 summary-level annotations over 28 top-performing systems on three datasets.(3) We conduct a comparative study of four human evaluation protocols, underscoring potential confounding factors in evaluation setups.(4) We evaluate 50 automatic metrics and their variants using the collected human annotations across evaluation protocols and demonstrate how our benchmark leads to more statistically stable and significant results. The metrics we benchmarked include recent methods based on large language models (LLMs), GPTScore and G-Eval. Furthermore, our findings have important implications for evaluating LLMs, as we show that LLMs adjusted by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation, which is affected by the annotators' prior, input-agnostic preferences, calling for more robust, targeted evaluation methods.Oscar collided with Arsenal goalkeeper David Ospina in the 16th minute of the London derby. The Brazilian was substituted at half-time and Jose Mourinho said he suffered `possible concussion'. Oscar was knocked back by the goalkeeper but Michael Oliver didn't award Chelsea a penalty.System SummaryChelsea weren't awarded a penalty. David Ospina clashed with Oscar. David Ospina clattered Oscar. David Ospina plays for Arsenal. David Ospina is a goalkeeper. The clash occurred inside the box. Oscar is Brazilian. Oscar was taken off at half time. Didier Drogba replaced Oscar.System SummaryOscar collided with Arsenal goalkeeper David Ospina in the 16th minute of the London derby. The Brazilian was substituted at half-time and Jose Mourinho said he suffered 'possible concussion'. Oscar was knocked back by the goalkeeper but Michael Oliver didn't award Chelsea a penalty.Chelsea weren't awarded a penalty for David Ospina's clash with Oscar. Arsenal goalkeeper clattered Oscar inside the box. Brazilian was taken off at half-time, with Didier Drogba replacing him.Reference SummaryOscar collided with Arsenal goalkeeper David Ospina in the 16th minute of the London derby. The Brazilian was substituted at half-time and Jose Mourinho said he suffered `possible concussion'. Oscar was knocked back by the goalkeeper but Michael Oliver didn't award Chelsea a penalty.System Summary Atomic Content Units (ACUs)❌Chelsea weren't awarded a penalty for David Ospina's clash with Oscar. Arsenal goalkeeper clattered Oscar inside the box. Brazilian was taken off at half-time, with DidierDrogba replacing him. Chelsea weren't awarded a penalty. David Ospina clashed with Oscar. David Ospina clattered Oscar. David Ospina plays for Arsenal. David Ospina is a goalkeeper. The clash occurre Oscar is B Oscar was taken Didier Drogba r Automic Content Units (ACUs) ✔ ✔ ✔ ✔ ✔ ACU Writing ACU Matching Reference Summary Chelsea weren't awarded a penalty. David Ospina clashed with Oscar. David Ospina clattered Oscar. David Ospina plays for Arsenal. David Ospina is a goalkeeper. The clash occurred inside the box. Oscar is Brazilian. Oscar was taken off at half time. Didier Drogba replaced Oscar. Oscar collided with Arsenal goalkeeper David Ospina in the 16th minute of the London derby. The Brazilian was substituted at half-time and Jose Mourinho said he suffered 'possible concussion'. Oscar was knocked back by the goalkeeper but Michael Oliver didn't award Chelsea a penalty.",
        "id":254685611
      }
    ],
    "negative_ctxs":[
      {
        "title":"Linguistics isn't always the answer: Word comparison in computational linguisticsL ars Borin",
        "text":"S trin g sim ila rity m e tric s a re im p o rta n t to o ls in c o m p u ta tio n a l lin g u istic s, e x te n siv e ly u s e d e.g . f o r c o m p a rin g w o rd s in a v a rie ty o f p ro b le m d o m a in s. T h is p a p e r e x a m in e s th e so m e tim e s m a d e a s s u m p tio n th a t th e p e rfo rm a n c e o f su c h w o rd c o m p a ris o n m e th o d s w o u ld b e n e fit fro m th e u se o f lin g u istic , viz. p h o n o lo g ic a l a n d m o rp h o lo g ic a l, k n o w le d g e . O n e lin g u istic a lly n aiv e m e th o d a n d o n e in c o rp o ra tin g a m o d e ra te a m o u n t o f lin g u istic s o p h is tic a tio n w e re co m p a re d o n a b ilin g u a l a n d a m o n o lin g u a l w o rd c o m p a riso n ta sk fo r a ra n g e o f la n g u a g e s. T h e re su lts s h o w th ep e rfo rm a n c e , m e a su re d as re c a ll a n d p re c isio n , o f th e lin g u is tic a lly n a iv e m e th o d to b e su p e rio r i n a ll cases.",
        "id":107696
      },
      {
        "title":"A two-stage statistical word segmentation system for Chinese",
        "text":"In this paper we present a two-stage statistical word segmentation system for Chinese based on word bigram and wordformation models. This system was evaluated on Peking University corpora at the First International Chinese Word Segmentation Bakeoff. We also give results and discussions on this evaluation.",
        "id":13113753
      },
      {
        "title":"Rigorous dimensionality reduction through linguistically motivated feature selection for text categorization",
        "text":"This paper introduces a new linguistically motivated feature selection technique for text categorization based on morphological analysis.It will be shown that compound parts that are constituents of many (different) noun compounds throughout a text are good and general indicators of this text's content; they are more general in meaning than the compounds they are part of, but nevertheless have good domain-specificity so that they distinguish between categories. Experiments with categorizing German newspaper texts show that this feature selection technique is superior to other popular ones, especially when dimensionality is reduced substantially. Additionally, a new compound splitting method based on compact patricia tries is introduced.",
        "id":10662251
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"What limitations do large language models have in evaluating information-seeking question answering?",
    "positive_ctxs":[
      {
        "title":"Evaluating Open-Domain Question Answering in the Era of Large Language Models",
        "text":"Lexical matching remains the de facto evaluation method for open-domain question answering (QA). Unfortunately, lexical matching fails completely when a plausible candidate answer does not appear in the list of gold answers, which is increasingly the case as we shift from extractive to generative models. The recent success of large language models (LLMs) for QA aggravates lexical matching failures since candidate answers become longer, thereby making matching with the gold answers even more challenging. Without accurate evaluation, the true progress in open-domain QA remains unknown. In this paper, we conduct a thorough analysis of various open-domain QA models, including LLMs, by manually evaluating their answers on a subset of NQ-OPEN, a popular benchmark. Our assessments reveal that while the true performance of all models is significantly underestimated, the performance of the InstructGPT (zero-shot) LLM increases by nearly +60%, making it on par with existing top models, and the InstructGPT (few-shot) model actually achieves a new state-of-the-art on NQ-OPEN. We also find that more than 50% of lexical matching failures are attributed to semantically equivalent answers. We further demonstrate that regex matching ranks QA models consistent with human judgments, although still suffering from unnecessary strictness. Finally, we demonstrate that automated evaluation models are a reasonable surrogate for lexical matching in some circumstances, but not for long-form answers generated by LLMs. The automated models struggle in detecting hallucinations in LLM answers and are thus unable to evaluate LLMs. At this time, there appears to be no substitute for human evaluation.",
        "id":258615193
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":29633703
      },
      {
        "title":"What can Neural Referential Form Selectors Learn?",
        "text":"Despite achieving encouraging results, neural Referring Expression Generation models are often thought to lack transparency. We probed neural Referential Form Selection (RFS) models to find out to what extent the linguistic features influencing the RE form are learnt and captured by state-of-the-art RFS models. The results of 8 probing tasks show that all the defined features were learnt to some extent. The probing tasks pertaining to referential status and syntactic position exhibited the highest performance. The lowest performance was achieved by the probing models designed to predict discourse structure properties beyond the sentence level.",
        "id":237091135
      },
      {
        "title":"Published as a conference paper at ICLR 2023 Voint Cloud: Multi-View Point Cloud Rep- resentation for 3D Understanding",
        "text":"Multi-view projection methods have demonstrated promising performance on 3D understanding tasks like 3D classification and segmentation. However, it remains unclear how to combine such multi-view methods with the widely available 3D point clouds. Previous methods use unlearned heuristics to combine features at the point level. To this end, we introduce the concept of the multi-view point cloud (Voint cloud), representing each 3D point as a set of features extracted from several view-points. This novel 3D Voint cloud representation combines the compactness of 3D point cloud representation with the natural view-awareness of multi-view representation. Naturally, we can equip this new representation with convolutional and pooling operations. We deploy a Voint neural network (VointNet) to learn representations in the Voint space. Our novel representation achieves state-of-the-artperformance on 3D classification, shape retrieval, and robust 3D part segmentation on standard benchmarks ( ScanObjectNN, ShapeNet Core55, and ShapeNet Parts). 1Figure 1: 3D Voint Clouds. We propose the multi-view point cloud (Voint cloud), a novel 3D representation that is compact and naturally descriptive of view projections of a 3D point cloud. Each point in the 3D cloud is tagged with a Voint, which accumulates view-features for that point. Note that not all 3D points are visible from all views. The set of Voints constructs a Voint cloud.clouds with a single feature per point suitable for typical point cloud processing pipelines. Previous multi-view works rely on heuristics (e.g. average or label mode pooling) after mapping pixels to points (Kundu et al., 2020;Wang et al., 2019a), or multi-view fusion with voxels (Dai & Nießner, 2018). Such setups might not be optimal for a few reasons. (i) Such heuristics may aggregate information of misleading projections that are obtained from arbitrary view-points. For example, looking at an object from the bottom and processing that view independently can carry wrong information about the object's content when combined with other views. (ii) The views lack geometric 3D information.To this end, we propose a new hybrid 3D data structure that inherits the merits of point clouds (i.e. compactness, flexibility, and 3D descriptiveness) and leverages the benefits of rich perceptual features of multi-view projections. We call this new representation multi-view point cloud (or Voint cloud) and illustrate it inFigure 1. A Voint cloud is a set of Voints, where each Voint is a set of view-dependent features (view-features) that correspond to the same point in the 3D point cloud. The cardinality of these view-features may differ from one Voint to another. InTable 1, we compare some of the widely used 3D representations and our Voint cloud representation. Voint clouds inherit the characteristics of the parent explicit 3D point clouds, which facilitates learning Voint representations for a variety of vision applications (e.g. point cloud classification and segmentation). To deploy deep learning on the new Voint space, we define basic operations on Voints, such as pooling and convolution. Based on these operations, we define a practical way of building Voint neural networks that we dub VointNet. VointNet takes a Voint cloud and outputs point cloud features for 3D point cloud processing. We show how learning this Voint cloud representation leads to strong performance and gained robustness for the tasks of 3D classification, 3D object retrieval, and 3D part segmentation on standard benchmarks like ScanObjectNN (Uy et al., 2019),  and ShapeNet (Chang et al., 2015).Contributions: (i)We propose a novel multi-view 3D point cloud representation (denoted as Voint cloud), which represents each point (namely a Voint) as a set of features from different view-points. (ii) We define pooling and convolutional operations at the Voint level to construct a Voint Neural Network (VointNet) capable of learning to aggregate information from multiple views in the Voint space. (iii) Our VointNet reaches state-ofthe-artperformance on several 3D understanding tasks, including 3D shape classification, retrieval, and robust part segmentation. Further, VointNet achieves robustness improvement to occlusion and rotation. Center (VCC) funding.ReferencesGary Bradski and Stephen Grossberg. Recognition of 3-d objects from multiple 2-d views by a self-organizing neural architecture. In From Statistics . Encoder-decoder with atrous separable convolution for semantic image segmentation. In . 3dmv: Joint 3d-multi-view prediction for 3d semantic scene segmentation. In",
        "id":244729433
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"What paper compares humans' and language models' non-literal interpretations of utterances featuring phenomena like deceit, irony, and humor?",
    "positive_ctxs":[
      {
        "title":"A fine-grained comparison of pragmatic language understanding in humans and language models",
        "text":"Pragmatics and non-literal language understanding are essential to human communication, and present a long-standing challenge for artificial language models. We perform a finegrained comparison of language models and humans on seven pragmatic phenomena, using zero-shot prompting on an expert-curated set of English materials. We ask whether models (1) select pragmatic interpretations of speaker utterances, (2) make similar error patterns as humans, and (3) use similar linguistic cues as humans to solve the tasks. We find that the largest models achieve high accuracy and match human error patterns: within incorrect responses, models favor literal interpretations over heuristic-based distractors. We also find preliminary evidence that models and humans are sensitive to similar linguistic cues. Our results suggest that pragmatic behaviors can emerge in models without explicitly constructed representations of mental states. However, models tend to struggle with phenomena relying on social expectation violations.",
        "id":254591475
      }
    ],
    "negative_ctxs":[
      {
        "title":"Using a Large Set of EAGLES-compliant Morpho-Syntactic Descriptors as a Tagset for Probabilistic Tagging 'DQ 7XILú",
        "text":"The paper presents one way of reconciling data sparseness with the requirement of high accuracy tagging in terms of fine-grained tagsets. For lexicon encoding, EAGLES elaborated a set of recommendations aimed at covering multilingual requirements and therefore resulted in a large number of features and possible values. Such an encoding, used for tagging purposes, would lead to very large tagsets. For instance, our EAGLES-compliant lexicon required a set of about 1000 morpho-syntactic description codes (MSDs) which after considering some systematic syncretic phenomena, was reduced to a set of 614 MSDs. Building reliable language models (LMs) for this tagset would require unrealistically large training data (hand annotated\/validated). Our solution was to design a hidden reduced tagset and use it in building various LMs. The underlying tagger uses these LMs to tag a new text in as many variants as LMs are available. The tag differences between these variants are processed by a combiner which chooses the most likely tags. In the end, the tagged text is subject to a conversion process that maps the tags from the reduced tagset onto the more informative tags from the large tagset. We describe this processing chain and provide a detailed evaluation of the results.",
        "id":14619939
      },
      {
        "title":"Multilingual Language Models are not Multicultural: A Case Study in Emotion",
        "text":"Emotions are experienced and expressed differently across the world. In order to use Large Language Models (LMs) for multilingual tasks that require emotional sensitivity, LMs must reflect this cultural variation in emotion. In this study, we investigate whether the widely-used multilingual LMs in 2023 reflect differences in emotional expressions across cultures and languages. We find that embeddings obtained from LMs (e.g., XLM-RoBERTa) are Anglocentric, and generative LMs (e.g., ChatGPT) reflect Western norms, even when responding to prompts in other languages. Our results show that multilingual LMs do not successfully learn the culturally appropriate nuances of emotion and we highlight possible research directions towards correcting this.",
        "id":259342568
      },
      {
        "title":"BioNLP Shared Task 2011 -Bacteria Gene Interactions and Renaming",
        "text":"We present two related tasks of the BioNLP Shared Tasks 2011: Bacteria Gene Renaming (Rename) and Bacteria Gene Interactions (GI). We detail the objectives, the corpus specification, the evaluation metrics, and we summarize the participants' results. Both issued from PubMed scientific literature abstracts, the Rename task aims at extracting gene name synonyms, and the GI task aims at extracting genic interaction events, mainly about gene transcriptional regulations in bacteria.",
        "id":2142405
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"What work attempts to explore multi-hop reasoning by densifying commonsense knowledge graphs?",
    "positive_ctxs":[
      {
        "title":"Dense-ATOMIC: Towards Densely-connected ATOMIC with High Knowledge Coverage and Massive Multi-hop Paths",
        "text":"ATOMIC is a large-scale commonsense knowledge graph (CSKG) containing everyday ifthen knowledge triplets, i.e., {head event, relation, tail event}. The one-hop annotation manner made ATOMIC a set of independent bipartite graphs, which ignored the numerous links between events in different bipartite graphs and consequently caused shortages in knowledge coverage and multi-hop paths. In this work, we aim to construct Dense-ATOMIC with high knowledge coverage and massive multi-hop paths. The events in ATOMIC are normalized to a consistent pattern at first. We then propose a CSKG completion method called Rel-CSKGC to predict the relation given the head event and the tail event of a triplet, and train a CSKG completion model based on existing triplets in ATOMIC. We finally utilize the model to complete the missing links in ATOMIC and accordingly construct Dense-ATOMIC. Both automatic and human evaluation on an annotated subgraph of ATOMIC demonstrate the advantage of Rel-CSKGC over strong baselines. We further conduct extensive evaluations on Dense-ATOMIC in terms of statistics, human evaluation, and simple downstream tasks, all proving Dense-ATOMIC's advantages in Knowledge",
        "id":258959081
      }
    ],
    "negative_ctxs":[
      {
        "title":"Word-Sense Disambiguation for Machine Translation",
        "text":"In word sense disambiguation, a system attempts to determine the sense of a word from contextual features. Major barriers to building a high-performing word sense disambiguation system include the difficulty of labeling data for this task and of predicting fine-grained sense distinctions. These issues stem partly from the fact that the task is being treated in isolation from possible uses of automatically disambiguated data. In this paper, we consider the related task of word translation, where we wish to determine the correct translation of a word from context. We can use parallel language corpora as a large supply of partially labeled data for this task. We present algorithms for solving the word translation problem and demonstrate a significant improvement over a baseline system. We then show that the word-translation system can be used to improve performance on a simplified machinetranslation task and can effectively and accurately prune the set of candidate translations for a word.",
        "id":7241107
      },
      {
        "title":"Translating and the Computer 20",
        "text":"Cost, quality and time, not necessarily in that order, are the three most important factors in a company's translation strategy. Or, more pertinently, how to cut costs, how to improve quality and how to save time. Improvements in the quality and efficiency of translation can be effected at the very birth of a document, that is, at the time of authoring. There are several ways of ensuring consistency and quality of authoring to various degrees, all of which have a beneficial effect at the translation stage. This paper examines the concepts of controlled authoring and author memory and the supporting technologies, the processes involved in developing such techniques within an organisation and the business benefits that such techniques bring.",
        "id":41780695
      },
      {
        "title":"The Alice Datasets: fMRI & EEG Observations of Natural Language Comprehension",
        "text":"The Alice Datasets combine observations from magnetic resonance imaging as well as electrophysiology while human participants listened to the same literary narrative in English. Along with these neural signals and the text of the story, we also provide a variety of word-by-word predictors motivated by research in computational linguistics and cognitive science. These predictors range from prosody to morphology to syntax. These annotated, naturalistic datasets can be used to replicate prior work and test new hypotheses about natural language comprehension in the brain.",
        "id":218973758
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which article first proposed shuffled-group-whitening to solve the problem of sentence representation learning?",
    "positive_ctxs":[
      {
        "title":"WhitenedCSE: Whitening-based Contrastive Learning of Sentence Embeddings",
        "text":"This paper presents a whitening-based contrastive learning method for sentence embedding learning (WhitenedCSE), which combines contrastive learning with a novel shuffled group whitening. Generally, contrastive learning pulls distortions of a single sample (i.e., positive samples) close and push negative samples far away, correspondingly facilitating the alignment and uniformity in the feature space. A popular alternative to the \"pushing\" operation is whitening the feature space, which scatters all the samples for uniformity. Since the whitening and the contrastive learning have large redundancy w.r.t. the uniformity, they are usually used separately and do not easily work together. For the first time, this paper integrates whitening into the contrastive learning scheme and facilitates two benefits. 1) Better uniformity. We find that these two approaches are not totally redundant but actually have some complementarity due to different uniformity mechanism. 2) Better alignment. We randomly divide the feature into multiple groups along the channel axis and perform whitening independently within each group. By shuffling the group division, we derive multiple distortions of a single sample and thus increase the positive sample diversity. Consequently, using multiple positive samples with enhanced diversity further improves contrastive learning due to better alignment. Extensive experiments on seven semantic textual similarity tasks show our method achieves consistent improvement over the contrastive learning baseline and sets new states of the art, e.g., 78.78% (+2.53% based on BERT base ) Spearman correlation on STS tasks. 1",
        "id":259370833
      }
    ],
    "negative_ctxs":[
      {
        "title":"ADVERSARIAL MANIPULATION OF DEEP REPRESENTATIONS",
        "text":"We show that the image representations in a deep neural network (DNN) can be manipulated to mimic those of other natural images, with only minor, imperceptible perturbations to the original image. Previous methods for generating adversarial images focused on image perturbations designed to produce erroneous class labels. Here we instead concentrate on the internal layers of DNN representations, to produce a new class of adversarial images that differs qualitatively from others. While the adversary is perceptually similar to one image, its internal representation appears remarkably similar to a different image, from a different class and bearing little if any apparent similarity to the input. Further, they appear generic and consistent with the space of natural images. This phenomenon demonstrates the possibility to trick a DNN to confound almost any image with any other chosen image, and raises questions about DNN representations, as well as the properties of natural images themselves.",
        "id":5922522
      },
      {
        "title":"Estimating Language Relationships from a Parallel Corpus. A Study of the Europarl Corpus",
        "text":"Since the 1950s, linguists have been using short lists (40-200 items) of basic vocabulary as the central component in a methodology which is claimed to make it possible to automatically calculate genetic relationships among languages. In the last few years these methods have experienced something of a revival, in that more languages are involved, different distance measures are systematically compared and evaluated, and methods from computational biology are used for calculating language family trees. In this paper, we explore how this methodology can be extended in another direction, by using larger word lists automatically extracted from a parallel corpus using word alignment software. We present preliminary results from using the Europarl parallel corpus in this way for estimating the distances between some languages in the Indo-European language family.",
        "id":2288422
      },
      {
        "title":"Des Réseaux de Neurones avec Mécanisme d'Attention pour la Compréhension de la Parole ⇤",
        "text":"L'étude porte sur l'apport d'un réseau de neurones récurrent (Recurrent Neural Network -RNN) bidirectionnel encodeur\/décodeur avec mécanisme d'attention pour une tâche de compréhension de la parole. Les premières expériences faites sur le corpus ATIS confirment la qualité du système RNN état de l'art utilisé pour cet article, en comparant les résultats obtenus à ceux récemment publiés dans la littérature. Des expériences supplémentaires montrent que les RNNs avec mécanisme d'attention obtiennent de meilleures performances que les RNNs récemment proposés pour la tâche d'étiquetage en concepts sémantiques. Sur le corpus MEDIA, un corpus français état de l'art pour la compréhension dédié à la réservation d'hôtel et aux informations touristiques, les expériences montrent qu'un RNN bidirectionnel atteint une f-mesure de 79,51 tandis que le même système intégrant le mécanisme d'attention permet d'atteindre une f-mesure de 80,27.ABSTRACTExploring the use of Attention-Based Recurrent Neural Networks For Spoken Language UnderstandingThis study explores the use of a bidirectional recurrent neural network (RNN) encoder\/decoder based on a mechanism of attention for a Spoken Language Understanding (SLU) task. First experiments carried on the ATIS corpus confirm the quality of the RNN baseline system used in this paper, by comparing its results on the ATIS corpus to the results recently published in the literature. Additional experiments show that RNN based on a mechanism of attention performs better than RNN architectures recently proposed for a slot filling task. On the French MEDIA corpus, a French state-of-the-art corpus for SLU dedicated to hotel reservation and tourist information, experiments show that a bidirectionnal RNN reaches a f-measure value of 79.51 while the use of a mechanism of attention allows us to reach a f-measure value of 80.27. MOTS-CLÉS : Compréhension de la Parole, Réseaux de Neurones Récurrents, Mécanisme d'Attention, Bidirectionnel.",
        "id":150252449
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which dataset supports narration generation and temporal localization tasks in Chinese movies?",
    "positive_ctxs":[
      {
        "title":"Movie101: A New Movie Understanding Benchmark",
        "text":"To help the visually impaired enjoy movies, automatic movie narrating systems are expected to narrate accurate, coherent, and role-aware",
        "id":258832605
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":219310243
      },
      {
        "title":"Group-based Generation of Referring Expressions",
        "text":"Past work of generating referring expressions mainly utilized attributes of objects and binary relations between objects in order to distinguish the target object from others. However, such an approach does not work well when there is no distinctive attribute among objects. To overcome this limitation, this paper proposes a novel generation method utilizing perceptual groups of objects and n-ary relations among them. The evaluation using 18 subjects showed that the proposed method could effectively generate proper referring expressions.",
        "id":18892448
      },
      {
        "title":"FQuAD: French Question Answering Dataset",
        "text":"Recent advances in the field of language modeling have improved state-of-the-art results on many Natural Language Processing tasks. Among them, Reading Comprehension has made significant progress over the past few years. However, most results are reported in English since labeled resources available in other languages, such as French, remain scarce. In the present work, we introduce the French Question Answering Dataset (FQuAD). FQuAD is a French Native Reading Comprehension dataset of questions and answers on a set of Wikipedia articles that consists of 25,000+ samples for the 1.0 version and 60,000+ samples for the 1.1 version. We train a baseline model which achieves an F1 score of 92.2 and an exact match ratio of 82.1 on the test set. In an effort to track the progress of French Question Answering models we propose a leaderboard and we have made the 1.0 version of our dataset freely available at https:\/\/illuin-tech. github.io\/FQuAD-explorer\/. . 2019. A span-extraction dataset for Chinese machine reading comprehension. In . 2018. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs\/1810.04805. Pavel Efimov, Leonid Boytsov, and Pavel Braslavski. 2019. Sberquad -russian reading comprehension dataset: Description and analysis. Matthew Honnibal and Ines Montani. 2017. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. To appear. Ali Kabbadj. 2018. Something new in french text mining and information extraction (universal chatbot): Largest qa french training dataset (110 000+). Guillaume Lample and Alexis Conneau. 2019. Crosslingual language model pretraining. CoRR, abs\/1901.07291. . 2019. Mlqa: Evaluating cross-lingual extractive question answering. ArXiv, abs\/1910.07475. Seungyoung Lim, Myungji Kim, and Jooyoul Lee. 2019. Korquad1.0: Korean qa dataset for machine reading comprehension. . 2018. Language models are unsupervised multitask learners. CoRR. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable questions for squad. CoRR, abs\/1806.03822. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In",
        "id":211126910
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_acl",
    "question":"Which family of model generally perform the best for the event conceptualization task",
    "positive_ctxs":[
      {
        "title":"CAT: A Contextualized Conceptualization and Instantiation Framework for Commonsense Reasoning",
        "text":"Commonsense reasoning, aiming at endowing machines with a human-like ability to make situational presumptions, is extremely challenging to generalize. For someone who barely knows about meditation, while is knowledgeable about singing, he can still infer that meditation makes people relaxed from the existing knowledge that singing makes people relaxed by first conceptualizing singing as a relaxing event and then instantiating that event to meditation. This process, known as conceptual induction and deduction, is fundamental to commonsense reasoning while lacking both labeled data and methodologies to enhance commonsense modeling. To fill such a research gap, we propose CAT (Contextualized ConceptuAlization and InsTantiation), a semi-supervised learning framework that integrates event conceptualization and instantiation to conceptualize commonsense knowledge bases at scale. Extensive experiments show that our framework achieves state-of-the-art performances on two conceptualization tasks, and the acquired abstract commonsense knowledge can significantly improve commonsense inference modeling. Our code, data, and fine-tuned models are publicly available at https:\/\/github.com\/HKUST-KnowComp\/CAT. * Equal Contribution PersonX watches football game, as a result, PersonX will: feel relaxed PersonX plays with his dog, as a result, PersonX will: be happy and relaxed PersonX [observe] as a result, PersonX will: feel relaxed PersonX [relaxing event] as a result, PersonX will: feel relaxed Conceptualization (watches football game → relaxing event) Instantiation (relaxing event → plays with his dog) Abstract Knowledge Aided Reasoning Conceptualization (watches football game → observe) Wrong Abstract Knowledge",
        "id":258557145
      }
    ],
    "negative_ctxs":[
      {
        "title":"Fast Unsupervised Dependency Parsing with Arc-Standard Transitions",
        "text":"Unsupervised dependency parsing is one of the most challenging tasks in natural languages processing. The task involves finding the best possible dependency trees from raw sentences without getting any aid from annotated data. In this paper, we illustrate that by applying a supervised incremental parsing model to unsupervised parsing; parsing with a linear time complexity will be faster than the other methods. With only 15 training iterations with linear time complexity, we gain results comparable to those of other state of the art methods. By employing two simple universal linguistic rules inspired from the classical dependency grammar, we improve the results in some languages and get the state of the art results. We also test our model on a part of the ongoing Persian dependency treebank. This work is the first work done on the Persian language.",
        "id":426641
      },
      {
        "title":"More Than Words: Collocation Retokenization for Latent Dirichlet Allocation Models",
        "text":"Traditionally, Latent Dirichlet Allocation (LDA) ingests words in a collection of documents to discover their latent topics using word-document co-occurrences. Previous studies show that representing bigrams collocations in the input can improve topic coherence in English. However, it is unclear how to achieve the best results for languages without marked word boundaries such as Chinese and Thai. Here, we explore the use of retokenization based on chi-squared measures, tstatistics, and raw frequency to merge frequent token ngrams into collocations when preparing input to the LDA model. Based on the goodness of fit and the coherence metric, we show that topics trained with merged tokens result in topic keys that are clearer, more coherent, and more effective at distinguishing topics than those of unmerged models.",
        "id":237277900
      },
      {
        "title":"Incremental Neural Lexical Coherence Modeling",
        "text":"Pretrained language models, neural models pretrained on massive amounts of data, have established the state of the art in a range of NLP tasks. They are based on a modern machine-learning technique, the Transformer which relates all items simultaneously to capture semantic relations in sequences. However, it differs from what humans do. Humans read sentences one-by-one, incrementally. Can neural models benefit by interpreting texts incrementally as humans do? We investigate this question in coherence modeling. We propose a coherence model which interprets sentences incrementally to capture lexical relations between them. We compare the state of the art in each task, simple neural models relying on a pretrained language model, and our model in two downstream tasks. Our findings suggest that interpreting texts incrementally as humans could be useful to design more advanced models. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:\/\/ creativecommons.org\/licenses\/by\/4.0\/.",
        "id":227231546
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which is the first multimodal model combining text and speech transformers trained without labelled text-speech pairs?",
    "positive_ctxs":[
      {
        "title":"Introducing Semantics into Speech Encoders",
        "text":"Recent studies find existing self-supervised speech encoders contain primarily acoustic rather than semantic information. As a result, pipelined supervised automatic speech recognition (ASR) to large language model (LLM) systems achieve state-of-the-art results on semantic spoken language tasks by utilizing rich semantic representations from the LLM. These systems come at the cost of labeled audio transcriptions, which is expensive and time-consuming to obtain. We propose a taskagnostic unsupervised way of incorporating semantic information from LLMs into selfsupervised speech encoders without labeled audio transcriptions. By introducing semantics, we improve existing speech encoder spoken language understanding (SLU) performance by over 5% on intent classification (IC), with modest gains in named entity resolution (NER) and slot filling (SF), and spoken question answering (SQA) FF1 score by over 2%. Our approach, which uses no ASR data, achieves similar performance as methods trained on over 100 hours of labeled audio transcripts, demonstrating the feasibility of unsupervised semantic augmentations to existing speech encoders.Conference on Acoustics, Speech and SignalProcessing (ICASSP), pages 7478-7482. IEEE. Duc Le, Akshat Shrivastava, Paden Tomasello, Suyoun Kim, Aleksandr Livshits, Ozlem Kalinli, and Michael L Seltzer. 2022. Deliberation model for on-device spoken language understanding. arXiv preprint arXiv:2204.01893. . 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461. . 2020. Multilingual speech translation with efficient finetuning of pretrained models. arXiv preprint arXiv:2010.12829. Piero Molino, Yaroslav Dudin, and Sai Sumanth Miryala. 2019. Ludwig: a type-based declarative deep learning toolbox. arXiv preprint arXiv:1909.07930.",
        "id":253523259
      }
    ],
    "negative_ctxs":[
      {
        "title":"Investigating the cross-linguistic potential of VerbNet -style classification",
        "text":"Verb classes which integrate a wide range of linguistic properties(Levin, 1993)have proved useful for natural language processing (NLP) applications. However, the real-world use of these classes has been limited because for most languages, no resources similar to VerbNet (Kipper-Schuler, 2005) are available. We apply a verb clustering approach developed for English to French -a language for which no such experiment has been conducted yet. Our investigation shows that not only the general methodology but also the best performing features are transferable between the languages, making it possible to learn useful VerbNet style classes for French automatically without languagespecific tuning.",
        "id":3507562
      },
      {
        "title":"",
        "text":"",
        "id":219300781
      },
      {
        "title":"Analyzing BERT's Knowledge of Hypernymy via Prompting",
        "text":"The high performance of large pretrained language models (LLMs) such as BERT (Devlin  et al., 2019)  on NLP tasks has prompted questions about BERT's linguistic capabilities, and how they differ from humans'. In this paper, we approach this question by examining BERT's knowledge of lexical semantic relations. We focus on hypernymy, the \"is-a\" relation that relates a word to a superordinate category.We use a prompting methodology to simply ask BERT what the hypernym of a given word is. We find that, in a setting where all hypernyms are guessable via prompting, BERT knows hypernyms with up to 57% accuracy. Moreover, BERT with prompting outperforms other unsupervised models for hypernym discovery even in an unconstrained scenario. However, BERT's predictions and performance on a dataset containing uncommon hyponyms and hypernyms indicate that its knowledge of hypernymy is still limited.",
        "id":241583486
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which knowledge graph completion method focuses on reducing memory usage by pruning features?",
    "positive_ctxs":[
      {
        "title":"GreenKGC: A Lightweight Knowledge Graph Completion Method",
        "text":"Knowledge graph completion (KGC) aims to discover missing relationships between entities in knowledge graphs (KGs). Most prior KGC work focuses on learning embeddings for entities and relations through a simple scoring function. Yet, a higher-dimensional embedding space is usually required for a better reasoning capability, which leads to a larger model size and hinders applicability to real-world problems (e.g., large-scale KGs or mobile\/edge computing). A lightweight modularized KGC solution, called GreenKGC, is proposed in this work to address this issue. GreenKGC consists of three modules: representation learning, feature pruning, and decision learning, to extract discriminant KG features and make accurate predictions on missing relationships using classifiers and negative sampling. Experimental results demonstrate that, in low dimensions, GreenKGC can outperform SOTA methods in most datasets. In addition, low-dimensional GreenKGC can achieve competitive or even better performance against high-dimensional models with a much smaller model size. We make our code publicly available. 1",
        "id":251710245
      }
    ],
    "negative_ctxs":[
      {
        "title":"Investigação Preliminar Sobre a Prosódia Semântica de Verbos de Elocução: o Caso do Verbo \"Confessar\"",
        "text":"The following article presents a preliminary study based on corpora about the semantic prosody of discendi verbs in Brazilian Portuguese. Thos study was based on a glossary of discendi verbs in Portuguese built by Freitas (2016) and on Ebeling's research (2014)  about the semantic prosody of verb phrases. At first, the verb \"confessar\" was selected from the glossary in order to test a possible methodology. Based on the results generated by this methodology, we were able to identify the semantic prosody of \"confessor\" and identify gaps and obstacles for possible unfoldings for this study.(2016)que elaborou um glossário de verbos de elocução em português, e na pesquisa de Ebeling (2014), que investiga a prosódia semântica de sintagmas verbais. A princípio, selecionamos o verbo \"confessar\" no glossário de Freitas (2016) para testarmos uma possível metodologia. Em conclusão, julgamos que a metodologia desenvolvida gerou resultados e, a partir dela, pudemos identificar a prosódia semântica do verbo \"confessar\" e apontar lacunas e dificuldades para o desdobramento da pesquisa.Resumo. Neste artigo, apresentamos primeiros olhares para a investigação da prosódia semântica de verbos de elocução em língua portuguesa, com base em corpora. O trabalho foi inspirado na dissertação de Mestrado de Freitas",
        "id":39476550
      },
      {
        "title":"Generating Text from Language Models",
        "text":"An increasingly large percentage of natural language processing (NLP) tasks center around the generation of text from probabilistic language models. Despite this trend, techniques for improving or specifying preferences in these generated texts rely mostly on intuition-based heuristics. Further, there lacks a unified presentation of their motivations, practical implementation, successes and pitfalls. Practitioners must, therefore, choose somewhat blindly between generation algorithms-like top-p sampling or beam search-which can lead to wildly different results. At the same time, language generation research continues to criticize and improve the standard toolboxes, further adding entropy to the state of the field. In this tutorial, we will provide a centralized and cohesive discussion of critical considerations when choosing how to generate from a language model. We will cover a wide range of empirically-observed problems (like degradation, hallucination, repetition) and their corresponding proposed algorithmic solutions from recent research (like topp sampling and its successors). We will then discuss a subset of these algorithms under a unified light; most stochastic generation strategies can be framed as locally adapting the probabilities of a model to avoid failure cases. Finally, we will then cover methods in controlled generation, that go beyond just ensuring coherence to ensure text exhibits specific desired properties. We aim for NLP practitioners and researchers to leave our tutorial with a unified framework which they can use to evaluate and contribute to the latest research in language generation.",
        "id":259370827
      },
      {
        "title":"THE LOGICAL ANALYSIS OF LEXICAL AMBIGUITY Abstract",
        "text":"",
        "id":1296084
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which language model distillation paper that first identified the capacity gap in distillation and used the MoE student model to counter the curse of capacity gap?",
    "positive_ctxs":[
      {
        "title":"Lifting the Curse of Capacity Gap in Distilling Language Models",
        "text":"Pretrained language models (LMs) have shown compelling performance on various downstream tasks, but unfortunately they require a tremendous amount of inference compute. Knowledge distillation finds a path to compress LMs to small ones with a teacher-student paradigm. However, when the capacity gap between the teacher and the student is large, a curse of capacity gap appears, invoking a deficiency in distilling LMs. While a few studies have been carried out to fill the gap, the curse is not yet well tackled. In this paper, we aim at lifting the curse of capacity gap via enlarging the capacity of the student without notably increasing the inference compute. Largely motivated by sparse activation regime of mixture of experts (MOE), we propose a mixture of minimal experts (MINIMOE), which imposes extra parameters to the student but introduces almost no additional inference compute. Experimental results on GLUE and CoNLL demonstrate the curse of capacity gap is lifted by the magic of MINIMOE to a large extent. MINIMOE also achieves the state-of-the-art performance at small FLOPs compared with a range of competitive baselines. With a compression rate as much as ∼50×, MINIMOE preserves ∼95% GLUE score of the teacher.",
        "id":258833648
      }
    ],
    "negative_ctxs":[
      {
        "title":"Automatically Constructing a Corpus of Sentential Paraphrases",
        "text":"An obstacle to research in automatic paraphrase identification and generation is the lack of large-scale, publiclyavailable labeled corpora of sentential paraphrases. This paper describes the creation of the recently-released Microsoft Research Paraphrase Corpus, which contains 5801 sentence pairs, each hand-labeled with a binary judgment as to whether the pair constitutes a paraphrase. The corpus was created using heuristic extraction techniques in conjunction with an SVM-based classifier to select likely sentence-level paraphrases from a large corpus of topicclustered news data. These pairs were then submitted to human judges, who confirmed that 67% were in fact semantically equivalent. In addition to describing the corpus itself, we explore a number of issues that arose in defining guidelines for the human raters.",
        "id":16639476
      },
      {
        "title":"Konwledge-Enabled Diagnosis Assistant Based on Obstetric EMRs and Knowledge Graph",
        "text":"The obstetric Electronic Medical Record (EMR) contains a large amount of medical data and health information. It plays a vital role in improving the quality of the diagnosis assistant service. In this paper, we treat the diagnosis assistant as a multi-label classification task and propose a Knowledge-Enabled Diagnosis Assistant (KEDA) model for the obstetric diagnosis assistant. We utilize the numerical information in EMRs and the external knowledge from Chinese Obstetric Knowledge Graph (COKG) to enhance the text representation of EMRs. Specifically, the bidirectional maximum matching method and similarity-based approach are used to obtain the entities set contained in EMRs and linked to the COKG. The final knowledge representation is obtained by a weight-based disease prediction algorithm, and it is fused with the text representation through a linear weighting method. Experiment results show that our approach can bring about +3.53 F1 score improvements upon the strong BERT baseline in the diagnosis assistant task.",
        "id":225062783
      },
      {
        "title":"",
        "text":"",
        "id":439452
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which numerical reasoning paper first published a dataset that considers different types of size of numbers and their representations in arithmetic questions?",
    "positive_ctxs":[
      {
        "title":"FERMAT: An Alternative to Accuracy for Numerical Reasoning",
        "text":"While pre-trained language models achieve impressive performance on various NLP benchmarks, they still struggle with tasks that require numerical reasoning. Recent advances in improving numerical reasoning are mostly achieved using very large language models that contain billions of parameters and are not accessible to everyone. In addition, numerical reasoning is measured using a single score on existing datasets. As a result, we do not have a clear understanding of the strengths and shortcomings of existing models on different numerical reasoning aspects and therefore, potential ways to improve them apart from scaling them up. Inspired by CheckList (Ribeiro et al., 2020), we introduce a multi-view evaluation set for numerical reasoning in English, called FERMAT. Instead of reporting a single score on a whole dataset, FERMAT evaluates models on various key numerical reasoning aspects such as number understanding, mathematical operations, and training dependency. Apart from providing a comprehensive evaluation of models on different numerical reasoning aspects, FERMAT enables a systematic and automated generation of an arbitrarily large training or evaluation set for each aspect.The datasets and codes are publicly available to generate further multi-view data for ulterior tasks and languages. 1",
        "id":258959201
      }
    ],
    "negative_ctxs":[
      {
        "title":"On the necessity of intentions and (at least) the usefulness of rhetorical relations: A position paper",
        "text":"In this position paper, we argue Ibr the need for a generation system to represent communicative goals (i.e., goals that express the speaker's intent to aflect the bearer's mental attitudes), at least if certain tasks are to be achieved. We lurther argue that, wlfile rhetorical relations might be recoverable ttom other Iactors, they appear to be a useful level of abstraction to maintain about the discourse structure to avoid costly reasoning. However, we believe a source of conliasion comes Ii'om the Ihct that the term 'rhetorical relations' encompasses various diftbrent constraints about a discourse structure, and that there is a need to be clear about what a 'rhetorical relation' is.",
        "id":6045798
      },
      {
        "title":"Computing Story Trees",
        "text":"A theory of understanding (parsing) texts as a process of collecting simple textual propositions into thematically and causally related units is described, based on the concept of macrostructures as proposed by Kintsch and van Dijk. These macrostructures are organized into tree hierarchies, and their interrelationships are described in rule-based story grammars related to the Kowalski logic based on Horn clauses. A procedure for constructing and synthesizing such trees from semantic network forms is detailed. The implementation of this procedure is capable of understanding and summarizing any story it can generate using the same basic control structure.This paper describes a rule-based computational model for text comprehension, patterned after the theory of macrostructures proposed byKintsch and van Dijk (1978). The rules are notationally and conceptually derived from the Horn clause, especially as described byKowalski (1979). Each rule consists of sets of thematically, causally, or temporally related propositions. The rules are organized into a network with the macrostructures becoming more generalized approaching the root. The resulting structure, called the Story Tree, represents a set of textual structures.",
        "id":16189453
      },
      {
        "title":"Controllable Lexical Simplification for English",
        "text":"Fine-tuning Transformer-based approaches have recently shown exciting results on sentence simplification task. However, so far, no research has applied similar approaches to the Lexical Simplification (LS) task. In this paper, we present ConLS, a Controllable Lexical Simplification system fine-tuned with T5 (a Transformer-based model pre-trained with a BERT-style approach and several other tasks). The evaluation results on three datasets (LexM-Turk, BenchLS, and NNSeval) have shown that our model performs comparable to LSBert (the current state-of-the-art) and even outperforms it in some cases. We also conducted a detailed comparison on the effectiveness of control tokens to give a clear view of how each token contributes to the model.",
        "id":256461435
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper about parameter-efficient finetuning first proposes to feed the pretrained weight instead of the activation to an adapter?",
    "positive_ctxs":[
      {
        "title":"Parameter-Efficient Fine-Tuning without Introducing New Latency",
        "text":"Parameter-efficient fine-tuning (PEFT) of pretrained language models has recently demonstrated remarkable achievements, effectively matching the performance of full fine-tuning while utilizing significantly fewer trainable parameters, and consequently addressing the storage and communication constraints. Nonetheless, various PEFT methods are limited by their inherent characteristics. In the case of sparse fine-tuning, which involves modifying only a small subset of the existing parameters, the selection of fine-tuned parameters is task-and domain-specific, making it unsuitable for federated learning. On the other hand, PEFT methods with adding new parameters typically introduce additional inference latency. In this paper, we demonstrate the feasibility of generating a sparse mask in a task-agnostic manner, wherein all downstream tasks share a common mask. Our approach, which relies solely on the magnitude information of pre-trained parameters, surpasses existing methodologies by a significant margin when evaluated on the GLUE benchmark. Additionally, we introduce a novel adapter technique that directly applies the adapter to pre-trained parameters instead of the hidden representation, thereby achieving identical inference speed to that of full finetuning. Through extensive experiments, our proposed method attains a new state-of-the-art outcome in terms of both performance and storage efficiency, storing only 0.03% parameters of full fine-tuning.",
        "id":258947572
      }
    ],
    "negative_ctxs":[
      {
        "title":"Retrieval-augmented Image Captioning",
        "text":"Inspired by retrieval-augmented language generation and pretrained Vision and Language (V&L) encoders, we present a new approach to image captioning that generates sentences given the input image and a set of captions retrieved from a datastore, as opposed to the image alone. The encoder in our model jointly processes the image and retrieved captions using a pretrained V&L BERT, while the decoder attends to the multimodal encoder representations, benefiting from the extra textual evidence from the retrieved captions. Experimental results on the COCO dataset show that image captioning can be effectively formulated from this new perspective. Our model, named EXTRA, benefits from using captions retrieved from the training dataset, and it can also benefit from using an external dataset without the need for retraining. Ablation studies show that retrieving a sufficient number of captions (e.g., k=5) can improve captioning quality. Our work contributes towards using pretrained V&L encoders for generative tasks, instead of standard classification tasks.",
        "id":256901128
      },
      {
        "title":"Corpus Linguistics Language and Computers: A Practical Introduction to the Computer Analysis of Language",
        "text":"The appearance of not one but two introductions to corpus linguistics within the same series shows the maturation and diversification of this fledgling subdiscipline within linguistics. McEnery and Wilson offer an overview or annotated report on work done within the computer-corpus research paradigm, including computational linguistics, whereas Barnbrook offers a guide or manual on the procedures and methodology of corpus linguistics, particularly with regard to machine-readable texts in English and to the type of results thereby generated.Whereas McEnery and Wilson recognize that the distinguishing features of corpus linguistics rest with its computer-aided empiricism, they are eager to line it up alongside cognitive rationalism in an effort to show the complementarity and interdependence of the two. As they argue, the advantages of a corpus-linguistics approach are that it is invariably systematic and rigorous, and that linguistics based on a corpus acts as a yardstick or control to linguistics based on artificial or introspective data. Of these current research paradigms, the authors' discussion offers fair and balanced criticism.In the central core of the book, McEnery and Wilson present overviews of the theory and practice of corpus linguistics, the relative merits of qualitative versus quanti-333",
        "id":261175933
      },
      {
        "title":"",
        "text":"",
        "id":192563022
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper combines the advantages of different frameworks for grammar error correction (GEC) and achieves good performance?",
    "positive_ctxs":[
      {
        "title":"TemplateGEC: Improving Grammatical Error Correction with Detection Template",
        "text":"Grammatical error correction (GEC) can be divided into sequence-to-edit (Seq2Edit) and sequence-to-sequence (Seq2Seq) frameworks, both of which have their pros and cons. To utilize the strengths and make up for the shortcomings of these frameworks, this paper proposes a novel method, TemplateGEC, which capitalizes on the capabilities of both Seq2Edit and Seq2Seq frameworks in error detection and correction respectively. TemplateGEC utilizes the detection labels from a Seq2Edit model, to construct the template as the input. A Seq2Seq model is employed to enforce consistency between the predictions of different templates by utilizing consistency learning. Experimental results on the Chinese NLPCC18, English BEA19 and CoNLL14 benchmarks show the effectiveness and robustness of TemplateGEC. Further analysis reveals the potential of our method in performing human-in-the-loop GEC.",
        "id":259370799
      }
    ],
    "negative_ctxs":[
      {
        "title":"Error-repair Dependency Parsing for Ungrammatical Texts",
        "text":"We propose a new dependency parsing scheme which jointly parses a sentence and repairs grammatical errors by extending the non-directional transitionbased formalism of Goldberg and Elhadad (2010) with three additional actions: SUBSTITUTE, DELETE, INSERT. Because these actions may cause an infinite loop in derivation, we also introduce simple constraints that ensure the parser termination. We evaluate our model with respect to dependency accuracy and grammaticality improvements for ungrammatical sentences, demonstrating the robustness and applicability of our scheme.",
        "id":2290938
      },
      {
        "title":"Detection of opinions and facts. A cognitive approach",
        "text":"A model of episodic memory is derived to propose algorithms of text categorization with semantic space models. Performances of two algorithms named Target vector and Sub-target vector are contrasted using textual material of the text-mining context 'DEFT09'. The experience reported here have been realized on the english corpus which is composed of articles of the economic newspaper \"The Financial Times\". The aim of the task was to categorize texts in function of the factuality or subjectivity they expressed. Results confirm (i) that the episodic memory metaphor provides a convenient framework to propose efficient algorithm for text categorization, and (ii) that Sub-target vector algorithm outperforms the Target vector algorithm.",
        "id":143151
      },
      {
        "title":"",
        "text":"Recognition of Named Entities (NEs) is a difficult process in Indian languages like Hindi, Telugu, etc., where sufficient gazetteers and annotated corpora are not available compared to English language. This paper details a novel clustering and co-occurrence based approach to map English NEs with their equivalent representations from different languages recognized in a language-independent way. We have substituted the required language specific resources by the richly structured multilingual content of Wikipedia. The approach includes clustering of highly similar Wikipedia articles. Then the NEs in an English article are mapped with other language terms in interlinked articles based on co-occurrence frequencies. The cluster information and the term co-occurrences are considered in extracting the NEs from non-English languages. Hence, the English Wikipedia is used to bootstrap the NEs for other languages. Through this approach, we have availed the structured, semi-structured and multilingual content of the Wikipedia to a massive extent. Experimental results suggest that the proposed approach yields promising results in rates of precision and recall.",
        "id":6737837
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_acl",
    "question":"Which paper did a comprehensive survey of the code large language model (code LLMs)?",
    "positive_ctxs":[
      {
        "title":"Large Language Models Meet NL2Code: A Survey",
        "text":"The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics. We provide an intuitive comparison of all existing models on the Hu-manEval benchmark. Through in-depth observation and analysis, we provide some insights and conclude that the key factors contributing to the success of large language models for NL2Code are \"Large Size, Premium Data, Expert Tuning\". In addition, we discuss challenges and opportunities regarding the gap between models and humans. We also create a website https:\/\/nl2code.github.io to track the latest progress through crowdsourcing. To the best of our knowledge, this is the first survey of large language models for NL2Code, and we believe it will contribute to the ongoing development of the field.",
        "id":258557362
      }
    ],
    "negative_ctxs":[
      {
        "title":"Automated Essay Scoring Based on Finite State Transducer: towards ASR Transcription of Oral English Speech",
        "text":"Conventional Automated Essay Scoring (AES) measures may cause severe problems when directly applied in scoring Automatic Speech Recognition (ASR) transcription as they are error sensitive and unsuitable for the characteristic of ASR transcription. Therefore, we introduce a framework of Finite State Transducer (FST) to avoid the shortcomings. Compared with the Latent Semantic Analysis with Support Vector Regression (LSA-SVR) method (stands for the conventional measures), our FST method shows better performance especially towards the ASR transcription. In addition, we apply the synonyms similarity to expand the FST model. The final scoring performance reaches an acceptable level of 0.80 which is only 0.07 lower than the correlation (0.87) between human raters.",
        "id":16336633
      },
      {
        "title":"Bunsetsu Identification Using Category-Exclusive Rules",
        "text":"C()mmunications Research Laboratory, Ministry of Posts and ~I~lecommunications 588-2, ]waoka, Nishi-ku, Kobe, 651-2d92, Japan • -j ( ' . tel:-k81-78-969-2 ] 81 tax: +81-78-369-2189 http:\/\/www-karc.crl, go.j p\/ips\/murata {AbstractThis pal>or describes two new bunsetsu identificatkm methods using supervised learning. Sin(:e ,Jat)anese syntactic analysis ix usnally done after bunsetsu identification, lmnsetsu identiiieation is iml)orl;ant for analyzing Japanese sentences. In experiments comparing the four previously available machinelearning methods (decision tree, maximmn-entropy method, example-based apI)roaeh and deeiskm list,) an(l two new methods llSing categot'y-exclusive rules~ the new method using l;he category-exclusive rules with the highest similarity t)erformetl best.",
        "id":465
      },
      {
        "title":"",
        "text":"Run-on sentences are common grammatical mistakes but little research has tackled this problem to date. This work introduces two machine learning models to correct run-on sentences that outperform leading methods for related tasks, punctuation restoration and wholesentence grammatical error correction. Due to the limited annotated data for this error, we experiment with artificially generating training data from clean newswire text. Our findings suggest artificial training data is viable for this task. We discuss implications for correcting run-ons and other types of mistakes that have low coverage in error-annotated corpora.",
        "id":52812604
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper employs a two-stage approach in generative models to tackle ABSA tasks across various domains?",
    "positive_ctxs":[
      {
        "title":"Bidirectional Generative Framework for Cross-domain Aspect-based Sentiment Analysis",
        "text":"Cross-domain aspect-based sentiment analysis (ABSA) aims to perform various fine-grained sentiment analysis tasks on a target domain by transferring knowledge from a source domain. Since labeled data only exists in the source domain, a model is expected to bridge the domain gap for tackling cross-domain ABSA. Though domain adaptation methods have proven to be effective, most of them are based on a discriminative model, which needs to be specifically designed for different ABSA tasks. To offer a more general solution, we propose a unified bidirectional generative framework to tackle various cross-domain ABSA tasks. Specifically, our framework trains a generative model in both text-to-label and label-to-text directions. The former transforms each task into a unified format to learn domain-agnostic features, and the latter generates natural sentences from noisy labels for data augmentation, with which a more accurate model can be trained. To investigate the effectiveness and generality of our framework, we conduct extensive experiments on four cross-domain ABSA tasks and present new state-of-the-art results on all tasks. Our data and code are publicly available at https:\/\/github.com\/DAMO-NLP-SG\/BGCA.",
        "id":258714602
      }
    ],
    "negative_ctxs":[
      {
        "title":"KLUE-CORE: A regression model of semantic textual similarity",
        "text":"This paper describes our system entered for the *SEM 2013 shared task on Semantic Textual Similarity (STS). We focus on the core task of predicting the semantic textual similarity of sentence pairs.The current system utilizes machine learning techniques trained on semantic similarity ratings from the *SEM 2012 shared task; it achieved rank 20 out of 90 submissions from 35 different teams. Given the simple nature of our approach, which uses only WordNet and unannotated corpus data as external resources, we consider this a remarkably good result, making the system an interesting tool for a wide range of practical applications.",
        "id":10160256
      },
      {
        "title":"IS FAIRNESS ONLY METRIC DEEP? EVALUATING AND ADDRESSING SUBGROUP GAPS IN DML",
        "text":"Deep metric learning (DML) enables learning with less supervision through its emphasis on the similarity structure of representations. There has been much work on improving generalization of DML in settings like zero-shot retrieval, but little is known about its implications for fairness. In this paper, we are the first to evaluate state-of-the-art DML methods trained on imbalanced data, and to show the negative impact these representations have on minority subgroup performance when used for downstream tasks. In this work, we first define fairness in DML through an analysis of three properties of the representation space -interclass alignment, intra-class alignment, and uniformity -and propose finDML, the f airness in non-balanced DML benchmark to characterize representation fairness. Utilizing finDML, we find bias in DML representations to propagate to common downstream classification tasks. Surprisingly, this bias is propagated even when training data in the downstream task is re-balanced. To address this problem, we present Partial Attribute De-correlation (PARADE) to de-correlate feature representations from sensitive attributes and reduce performance gaps between subgroups in both embedding space and downstream metrics.",
        "id":247628080
      },
      {
        "title":"Joint Training of Candidate Extraction and Answer Selection for Reading Comprehension",
        "text":"While sophisticated neural-based techniques have been developed in reading comprehension, most approaches model the answer in an independent manner, ignoring its relations with other answer candidates. This problem can be even worse in open-domain scenarios, where candidates from multiple passages should be combined to answer a single question. In this paper, we formulate reading comprehension as an extract-then-select twostage procedure. We first extract answer candidates from passages, then select the final answer by combining information from all the candidates. Furthermore, we regard candidate extraction as a latent variable and train the two-stage process jointly with reinforcement learning. As a result, our approach has improved the state-ofthe-art performance significantly on two challenging open-domain reading comprehension datasets. Further analysis demonstrates the effectiveness of our model components, especially the information fusion of all the candidates and the joint training of the extract-then-select procedure.",
        "id":21703007
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper enables interactive semantic parsing by training an error correction model with simulated human feedback instead of human annotations?",
    "positive_ctxs":[
      {
        "title":"Learning to Simulate Natural Language Feedback for Interactive Semantic Parsing",
        "text":"Interactive semantic parsing based on natural language (NL) feedback, where users provide feedback to correct the parser mistakes, has emerged as a more practical scenario than the traditional one-shot semantic parsing. However, prior work has heavily relied on humanannotated feedback data to train the interactive semantic parser, which is prohibitively expensive and not scalable. In this work, we propose a new task of simulating NL feedback for interactive semantic parsing. We accompany the task with a novel feedback evaluator. The evaluator is specifically designed to assess the quality of the simulated feedback, based on which we decide the best feedback simulator from our proposed variants. On a text-to-SQL dataset, we show that our feedback simulator can generate high-quality NL feedback to boost the error correction ability of a specific parser. In low-data settings, our feedback simulator can help achieve comparable error correction performance as trained using the costly, full set of human annotations. 1",
        "id":258685538
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":220058607
      },
      {
        "title":"Language Identification for Person Names Based on Statistical Information",
        "text":"Language identification has been an interesting and fascinating issue in natural language processing for decades, and there have been many researches on it. However, most of the researches are for documents, and though the possibility of high accuracy for shorter strings of characters, language identification for words or phrases has not been discussed much. In this paper we propose a statistical method of language identification for phrases, and show the empirical results for person names of 9 languages (12 areas). Our simple method based on n-gram and phrase length obtained more than 90% of accuracy for Japanese, Korean and Russian, and fair results for other languages except English. This result indicated the possibility of language identification for person names based on statistics, which is useful in multi-language person name detection and also let us expect the possibility of language identification for phrases with simple statistics-based methods.",
        "id":1003566
      },
      {
        "title":"Light verb constructions with 'do' and 'be' in Hindi: A TAG analysis",
        "text":"In this paper we present a Lexicalized Feature-based Tree-Adjoining Grammar analysis for a type of nominal predicate that occurs in combination with the light verbs \"do\" and \"be\" (Hindi kar and ho respectively). Light verb constructions are a challenge for computational grammars because they are a highly productive predicational strategy in Hindi. Such nominals have been discussed in the literature(Mohanan, 1997;Ahmed and Butt, 2011;Bhatt et al., 2013), but this work is a first attempt at a Tree-Adjoining Grammar (TAG) representation. We look at three possibilities for the design of elementary trees in TAG and explore one option in depth using Hindi data. In this analysis, the nominal is represented with all the arguments of the light verb construction, while the light verb adjoins into its elementary tree.",
        "id":1159978
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper explored training a GPT-2 for automatic diagnosis, emphasizing efficient data augmentation for symptom prediction and disease identification?",
    "positive_ctxs":[
      {
        "title":"CoAD: Automatic Diagnosis through Symptom and Disease Collaborative Generation",
        "text":"Automatic diagnosis (AD), a critical application of AI in healthcare, employs machine learning techniques to assist doctors in gathering patient symptom information for precise disease diagnosis. The Transformer-based method utilizes an input symptom sequence, predicts itself through auto-regression, and employs the hidden state of the final symptom to determine the disease. Despite its simplicity and superior performance demonstrated, a decline in disease diagnosis accuracy is observed caused by 1) a mismatch between symptoms observed during training and generation, and 2) the effect of different symptom orders on disease prediction. To address the above obstacles, we introduce the CoAD, a novel disease and symptom collaborative generation framework, which incorporates several key innovations to improve AD: 1) aligning sentence-level disease labels with multiple possible symptom inquiry steps to bridge the gap between training and generation; 2) expanding symptom labels for each sub-sequence of symptoms to enhance annotation and eliminate the effect of symptom order; 3) developing a repeated symptom input schema to effectively and efficiently learn the expanded disease and symptom labels. We evaluate the CoAD framework using four datasets, including three public and one private, and demonstrate that it achieves an average 2.3% improvement over previous state-of-theart results in automatic disease diagnosis. For reproducibility, we release the code and data at https:\/\/github.com\/KwanWaiChung\/coad.",
        "id":259370815
      }
    ],
    "negative_ctxs":[
      {
        "title":"BCCWJ-DepPara: A Syntactic Annotation Treebank on the 'Balanced Corpus of Contemporary Written Japanese'",
        "text":"Paratactic syntactic structures are difficult to represent in syntactic dependency tree structures. As such, we propose an annotation schema for syntactic dependency annotation of Japanese, in which coordinate structures are separated from and overlaid on bunsetsu(base phrase unit)-based dependency. The schema represents nested coordinate structures, non-constituent conjuncts, and forward sharing as the set of regions. The annotation was performed on the core data of 'Balanced Corpus of Contemporary Written Japanese', which comprised about one million words and 1980 samples from six registers, such as newspapers, books, magazines, and web texts.",
        "id":17461299
      },
      {
        "title":"",
        "text":"We introduce the corpus of United States Congressional bills from 1947 to 1998 for use by language research communities. The U.S. Policy Agenda Legislation Corpus Volume 1 (USPALCV1) includes more than 375,000 legislative bills annotated with a hierarchical policy area category. The human annotations in USPALCV1 have been reliably applied over time to enable social science analysis of legislative trends. The corpus is a member of an emerging family of corpora that are annotated by policy area to enable comparative parallel trend recognition across countries and domains (legislation, political speeches, newswire articles, budgetary expenditures, web sites, etc.). This paper describes the origins of the corpus, its creation, ways to access it, design criteria, and an analysis with common supervised machine learning methods. The use of machine learning methods establishes a baseline proposed modeling for the topic classification of legal documents.",
        "id":1719783
      },
      {
        "title":"Fine-tuning Transformers with Additional Context to Classify Discursive Moves in Mathematics Classrooms",
        "text":"Talk moves\" are specific discursive strategies used by teachers and students to facilitate conversations in which students share their thinking, and actively consider the ideas of others, and engage in rich discussions. Experts in instructional practices often rely on cues to identify and document these strategies, for example by annotating classroom transcripts. Prior efforts to develop automated systems to classify teacher talk moves using transformers achieved a performance of 76.32% F1. In this paper, we investigate the feasibility of using enriched contextual cues to improve model performance. We applied state-of-the-art deep learning approaches for Natural Language Processing (NLP), including Robustly optimized bidirectional encoder representations from transformers (Roberta) with a special input representation that supports previous and subsequent utterances as context for talk moves classification. We worked with the publically available TalkMoves dataset, which contains utterances sourced from real-world classroom sessions (human-transcribed and annotated). Through a series of experimentations, we found that a combination of previous and subsequent utterances improved the transformers' ability to differentiate talk moves (by 2.6% F1). These results constitute a new state of the art over previously published results and provide actionable insights to those in the broader NLP community who are working to develop similar transformer-based classification models.",
        "id":250390967
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper first aggregates statements to represent political actors and learns the mapping from languages to representation via pre-training?",
    "positive_ctxs":[
      {
        "title":"UPPAM: A Unified Pre-training Architecture for Political Actor Modeling based on Language",
        "text":"Modeling political actors is at the core of quantitative political science. Existing works have incorporated contextual information to better learn the representation of political actors for specific tasks through graph models. However, they are limited to the structure and objective of training settings and can not be generalized to all politicians and other tasks. In this paper, we propose a Unified Pre-training Architecture for Political Actor Modeling based on language (UPPAM). In UPPAM, we aggregate statements to represent political actors and learn the mapping from languages to representation, instead of learning the representation of particular persons. We further design structureaware contrastive learning and behavior-driven contrastive learning tasks, to inject multidimensional information in the political context into the mapping. In this framework, we can profile political actors from different aspects and solve various downstream tasks. Experimental results demonstrate the effectiveness and capability of generalization of our method. * Corresponding author.",
        "id":259370636
      }
    ],
    "negative_ctxs":[
      {
        "title":"SRA PROJECT FOR ARPA \/ USACOM",
        "text":"",
        "id":30836513
      },
      {
        "title":"Cairo: An Alignment Visualization Tool",
        "text":"While developing a suite of tools for statistical machine translation research, we recognized the need for a visualization tool that would allow researchers to examine and evaluate specific word correspondences generated by a translation system. We developed Cairo to fill this need. Cairo is a free, open-source, portable, user-friendly, GUI-driven program written in Java that provides a visual representation of word correspondences between bilingual pairs of sentences, as well as relevant translation model parameters. This program can be easily adapted for visualization of correspondences in bi-texts based on probability distributions.",
        "id":2091007
      },
      {
        "title":"A chance-corrected measure of inter-annotator agreement for syntax",
        "text":"Following the works ofCarletta (1996)andArtstein and Poesio (2008), there is an increasing consensus within the field that in order to properly gauge the reliability of an annotation effort, chance-corrected measures of inter-annotator agreement should be used. With this in mind, it is striking that virtually all evaluations of syntactic annotation efforts use uncorrected parser evaluation metrics such as bracket F 1 (for phrase structure) and accuracy scores (for dependencies).In this work we present a chance-corrected metric based on Krippendorff's α, adapted to the structure of syntactic annotations and applicable both to phrase structure and dependency annotation without any modifications. To evaluate our metric we first present a number of synthetic experiments to better control the sources of noise and gauge the metric's responses, before finally contrasting the behaviour of our chance-corrected metric with that of uncorrected parser evaluation metrics on real corpora. 1",
        "id":17542707
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper first applied the chain-of-thought technique in the text summarization field?",
    "positive_ctxs":[
      {
        "title":"Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method",
        "text":"Automatic summarization generates concise summaries that contain key ideas of source documents. As the most mainstream datasets for the news sub-domain, CNN\/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be noisy, mainly in terms of factual hallucination and information redundancy. To address this challenge, we first annotate new expertwriting Element-aware test sets following the \"Lasswell Communication Model\" proposed by Lasswell (1948), allowing reference summaries to focus on more fine-grained news elements objectively and comprehensively. Utilizing the new test sets, we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMs' zero-shot summaries in prior work. Further, we propose a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more finegrained details of source documents into the final summaries that correlate with the human writing mindset. Experimental results show our method outperforms state-of-the-art fine-tuned PLMs and zero-shot LLMs by +4.33\/+4.77 in ROUGE-L on the two datasets, respectively. Dataset and code are publicly available at https:\/\/github.com\/Alsace08\/SumCoT.",
        "id":258841145
      }
    ],
    "negative_ctxs":[
      {
        "title":"Machine Translation: A Cognitive Linguistics Approach",
        "text":"This paper describes a novel approach to the construction of an interlinguabased MT system. This approach emerges from the developing field of Cognitive Linguistics. The objective of this paper is to address one of the main problems in MT systems (besides the source text ambiguity problem): the apparent complexity and irregularity of translation data. The author suggests that the key to a coherent account for some of the over-diversified translation data lies in the integration of an additional knowledge-base into the translation system: the knowledge of language usage. This knowledge is not part of the structure of language itself but derives from cognitive mechanisms which language acts upon. Cognitive Linguistics introduces a framework which gives a central role, in any understanding of semantics, to various kinds of schemas which are triggered by the language but which are not an explicit part of language itself. The paper examines some intriguing examples of English grammatical structures and their diversified translation into Hebrew. The analysis shows that by extracting the right schema from the source text, we can generate a better representation of the text's meaning which leads to easier and more accurate generation of the target text.",
        "id":18090758
      },
      {
        "title":"Dependency Parsing of Japanese Spoken Monologue Based on Clause Boundaries",
        "text":"Spoken monologues feature greater sentence length and structural complexity than do spoken dialogues. To achieve high parsing performance for spoken monologues, it could prove effective to simplify the structure by dividing a sentence into suitable language units. This paper proposes a method for dependency parsing of Japanese monologues based on sentence segmentation. In this method, the dependency parsing is executed in two stages: at the clause level and the sentence level. First, the dependencies within a clause are identified by dividing a sentence into clauses and executing stochastic dependency parsing for each clause. Next, the dependencies over clause boundaries are identified stochastically, and the dependency structure of the entire sentence is thus completed. An experiment using a spoken monologue corpus shows this method to be effective for efficient dependency parsing of Japanese monologue sentences.",
        "id":14156544
      },
      {
        "title":"A Graph Kernel for Protein-Protein Interaction Extraction",
        "text":"In this paper, we propose a graph kernel based approach for the automated extraction of protein-protein interactions (PPI) from scientific literature. In contrast to earlier approaches to PPI extraction, the introduced alldependency-paths kernel has the capability to consider full, general dependency graphs. We evaluate the proposed method across five publicly available PPI corpora providing the most comprehensive evaluation done for a machine learning based PPI-extraction system. Our method is shown to achieve state-of-theart performance with respect to comparable evaluations, achieving 56.4 F-score and 84.8 AUC on the AImed corpus. Further, we identify several pitfalls that can make evaluations of PPI-extraction systems incomparable, or even invalid. These include incorrect crossvalidation strategies and problems related to comparing F-score results achieved on different evaluation resources.",
        "id":7725084
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper first apply mixture of experts idea to large language models for domain adaptation?",
    "positive_ctxs":[
      {
        "title":"Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models' Memories",
        "text":"Pre-trained language models (PLMs) demonstrate excellent abilities to understand texts in the generic domain while struggling in a specific domain. Although continued pre-training on a large domain-specific corpus is effective, it is costly to tune all the parameters on the domain. In this paper, we investigate whether we can adapt PLMs both effectively and efficiently by only tuning a few parameters. Specifically, we decouple the feed-forward networks (FFNs) of the Transformer architecture into two parts: the original pre-trained FFNs to maintain the old-domain knowledge and our novel domain-specific adapters to inject domainspecific knowledge in parallel. Then we adopt a mixture-of-adapters gate to fuse the knowledge from different domain adapters dynamically. Our proposed Mixture-of-Domain-Adapters (MixDA) employs a two-stage adapter-tuning strategy that leverages both unlabeled data and labeled data to help the domain adaptation: i) domain-specific adapter on unlabeled data; followed by ii) the task-specific adapter on labeled data. MixDA can be seamlessly plugged into the pretraining-finetuning paradigm and our experiments demonstrate that MixDA achieves superior performance on in-domain tasks (GLUE), out-of-domain tasks (ChemProt, RCT, IMDB, Amazon), and knowledge-intensive tasks (KILT). Further analyses demonstrate the reliability, scalability, and efficiency of our method. 1 Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021b. DEBERTA: Decodingenhanced bert with disentangled attention. In International Conference on Learning Representations.Ruining He and Julian McAuley. 2016. Ups and downs:Modeling the visual evolution of fashion trends with one-class collaborative filtering. In . 2022a. Continual training of language models for few-shot learning. In . 2022b. Adapting a language model while preserving its general knowledge. In Proceed-",
        "id":259108831
      }
    ],
    "negative_ctxs":[
      {
        "title":"A Semi-Automated Live Interlingual Communication Workflow Featuring Intralingual Respeaking: Evaluation and Benchmarking",
        "text":"In this paper, we present a semi-automated workflow for live interlingual speech-to-text communication which seeks to reduce the shortcomings of existing ASR systems: a human respeaker works with a speaker-dependent speech recognition software (e.g., Dragon Naturally Speaking) to deliver punctuated same-language output of superior quality than obtained using out-of-the-box automatic speech recognition of the original speech. This is fed into a machine translation engine (the EU's eTranslation) to produce live-caption ready text. We benchmark the quality of the output against the output of best-in-class (human) simultaneous interpreters working with the same source speeches from plenary sessions of the European Parliament. To evaluate the accuracy and facilitate the comparison between the two types of output, we use a tailored annotation approach based on the NTR model(Romero-Fresco and Pöchhacker, 2017). We find that the semi-automated workflow combining intralingual respeaking and machine translation is capable of generating outputs that are similar in terms of accuracy and completeness to the outputs produced in the benchmarking workflow, although the small scale of our experiment requires caution in interpreting this result.",
        "id":252624472
      },
      {
        "title":"Subword-based Cross-lingual Transfer of Embeddings from Hindi to Marathi and Nepali",
        "text":"Word embeddings are growing to be a crucial resource in the field of NLP for any language. This work introduces a novel technique for static subword embeddings transfer for Indic languages from a relatively higher resource language to a genealogically related low resource language. We primarily work with Hindi-Marathi, simulating a low-resource scenario for Marathi, and confirm observed trends on Nepali. We demonstrate the consistent benefits of unsupervised morphemic segmentation on both source and target sides over the treatment performed by fastText. Our best-performing approach uses an EM-style approach to learning bilingual subword embeddings; we also show, for the first time, that a trivial \"copyand-paste\" embeddings transfer based on even perfect bilingual lexicons is inadequate in capturing language-specific relationships. We find that our approach substantially outperforms the fastText baselines for both Marathi and Nepali on the Word Similarity task as well as WordNet-Based Synonymy Tests; on the former task, its performance for Marathi is close to that of pretrained fastText embeddings that use three orders of magnitude more Marathi data.",
        "id":250390514
      },
      {
        "title":"",
        "text":"",
        "id":227231571
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper first attempts to take potential dependencies among same-level labels into account in Hierarchical Text Classification?",
    "positive_ctxs":[
      {
        "title":"Peer-Label Assisted Hierarchical Text Classification",
        "text":"Hierarchical text classification (HTC) is a challenging task, in which the labels of texts can be organized into a category hierarchy. To deal with the HTC problem, many existing works focus on utilizing the parent-child relationships that are explicitly shown in the hierarchy. However, texts with a category hierarchy also have some latent relevancy among labels in the same level of the hierarchy. We refer to these labels as peer labels, from which the peer effects are originally utilized in our work to improve the classification performance. To fully explore the peer-label relationship, we develop a PeerHTC method. This method innovatively measures the latent relevancy of peer labels through several metrics and then encodes the relevancy with a Graph Convolutional Neural Network. We also propose a sample importance learning method to ameliorate the side effects raised by modelling the peer label relevancy. Our experiments on several standard datasets demonstrate the evidence of peer labels and the superiority of PeerHTC over other state-of-the-art HTC methods in terms of classification accuracy.",
        "id":259370602
      }
    ],
    "negative_ctxs":[
      {
        "title":"Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on Twitter",
        "text":"Hate speech in the form of racist and sexist remarks are a common occurrence on social media. For that reason, many social media services address the problem of identifying hate speech, but the definition of hate speech varies markedly and is largely a manual effort(BBC, 2015;Lomas, 2015).We provide a list of criteria founded in critical race theory, and use them to annotate a publicly available corpus of more than 16k tweets. We analyze the impact of various extra-linguistic features in conjunction with character n-grams for hatespeech detection. We also present a dictionary based the most indicative words in our data.",
        "id":1721388
      },
      {
        "title":"Event extraction from Twitter using Non-Parametric Bayesian Mixture Model with Word Embeddings",
        "text":"To extract structured representations of newsworthy events from Twitter, unsupervised models typically assume that tweets involving the same named entities and expressed using similar words are likely to belong to the same event. Hence, they group tweets into clusters based on the cooccurrence patterns of named entities and topical keywords. However, there are two main limitations. First, they require the number of events to be known beforehand, which is not realistic in practical applications. Second, they don't recognise that the same named entity might be referred to by multiple mentions and tweets using different mentions would be wrongly assigned to different events. To overcome these limitations, we propose a nonparametric Bayesian mixture model with word embeddings for event extraction, in which the number of events can be inferred automatically and the issue of lexical variations for the same named entity can be dealt with properly. Our model has been evaluated on three datasets with sizes ranging between 2,499 and over 60 million tweets. Experimental results show that our model outperforms the baseline approach on all datasets by 5-8% in F-measure.",
        "id":12331876
      },
      {
        "title":"DOP: Off-Policy Multi-Agent Decomposed Policy Gradients",
        "text":"Recently, multi-agent policy gradient (MAPG) methods witness vigorous progress. However, there is a discrepancy between the performance of MAPG methods and state-of-the-art multi-agent value-based approaches. In this paper, we investigate the causes that hinder the performance of MAPG algorithms and present a multiagent decomposed policy gradient method (DOP). This method introduces the idea of value function decomposition into the multi-agent actor-critic framework. Based on this idea, DOP supports efficient off-policy learning and addresses the issue of centralized-decentralized mismatch and credit assignment in both discrete and continuous action spaces. We formally show that DOP critics have sufficient representational capability to guarantee convergence. In addition, empirical evaluations on the StarCraft II micromanagement benchmark and multi-agent particle environments demonstrate that our method significantly outperforms state-of-the-art value-based and policy-based multi-agent reinforcement learning algorithms. Demonstrative videos are available at https",
        "id":220769181
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper first combines different methods for uncertainty quantification in one?",
    "positive_ctxs":[
      {
        "title":"Hybrid Uncertainty Quantification for Selective Text Classification in Ambiguous Tasks",
        "text":"Many text classification tasks are inherently ambiguous, which results in automatic systems having a high risk of making mistakes, in spite of using advanced machine learning models. For example, toxicity detection in usergenerated content is a subjective task, and notions of toxicity can be annotated according to a variety of definitions that can be in conflict with one another. Instead of relying solely on automatic solutions, moderation of the most difficult and ambiguous cases can be delegated to human workers. Potential mistakes in automated classification can be identified by using uncertainty estimation (UE) techniques. Although UE is a rapidly growing field within natural language processing, we find that stateof-the-art UE methods estimate only epistemic uncertainty and show poor performance, or under-perform trivial methods for ambiguous tasks such as toxicity detection. We argue that in order to create robust uncertainty estimation methods for ambiguous tasks it is necessary to account also for aleatoric uncertainty. In this paper, we propose a new uncertainty estimation method that combines epistemic and aleatoric UE methods. We show that by using our hybrid method, we can outperform state-of-the-art UE methods for toxicity detection and other ambiguous text classification tasks 1 .",
        "id":259370752
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":219942256
      },
      {
        "title":"Retrieval-augmented Image Captioning",
        "text":"Inspired by retrieval-augmented language generation and pretrained Vision and Language (V&L) encoders, we present a new approach to image captioning that generates sentences given the input image and a set of captions retrieved from a datastore, as opposed to the image alone. The encoder in our model jointly processes the image and retrieved captions using a pretrained V&L BERT, while the decoder attends to the multimodal encoder representations, benefiting from the extra textual evidence from the retrieved captions. Experimental results on the COCO dataset show that image captioning can be effectively formulated from this new perspective. Our model, named EXTRA, benefits from using captions retrieved from the training dataset, and it can also benefit from using an external dataset without the need for retraining. Ablation studies show that retrieving a sufficient number of captions (e.g., k=5) can improve captioning quality. Our work contributes towards using pretrained V&L encoders for generative tasks, instead of standard classification tasks.",
        "id":256901128
      },
      {
        "title":"",
        "text":"",
        "id":219792169
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper first combines rewriting and expansion methods to reformulate a query for conversational search?",
    "positive_ctxs":[
      {
        "title":"ConvGQR: Generative Query Reformulation for Conversational Search",
        "text":"In conversational search, the user's real search intent for the current conversation turn is dependent on the previous conversation history. It is challenging to determine a good search query from the whole conversation context. To avoid the expensive re-training of the query encoder, most existing methods try to learn a rewriting model to de-contextualize the current query by mimicking the manual query rewriting. However, manually rewritten queries are not always the best search queries. Thus, training a rewriting model on them would lead to sub-optimal queries. Another useful information to enhance the search query is the potential answer to the question. In this paper, we propose ConvGQR, a new framework to reformulate conversational queries based on generative pre-trained language models (PLMs), one for query rewriting and another for generating potential answers. By combining both, ConvGQR can produce better search queries. In addition, to relate query reformulation to the retrieval task, we propose a knowledge infusion mechanism to optimize both query reformulation and retrieval. Extensive experiments on four conversational search datasets demonstrate the effectiveness of ConvGQR.",
        "id":258887946
      }
    ],
    "negative_ctxs":[
      {
        "title":"NLGbAse: a free linguistic resource for Natural Language Processing systems",
        "text":"Availability of labeled language resources, such as annotated corpora and domain dependent labeled language resources is crucial for experiments in the field of Natural Language Processing. Most often, due to lack of resources, manual verification and annotation of electronic text material is a prerequisite for the development of NLP tools. In the context of under-resourced language, the lack of copora becomes a crucial problem because most of the research efforts are supported by organizations with limited funds. Using free, multilingual and highly structured corpora like Wikipedia to produce automatically labeled language resources can be an answer to those needs. This paper introduces NLGbAse, a multilingual linguistic resource built from the Wikipedia encyclopedic content. This system produces structured metadata which make possible the automatic annotation of corpora with syntactical and semantical labels. A metadata contains semantical and statistical informations related to an encyclopedic document. To validate our approach, we built and evaluated a Named Entity Recognition tool, trained with Wikipedia corpora annotated by our system.",
        "id":14517993
      },
      {
        "title":"NAIST Simultaneous Speech-to-Text Translation System for IWSLT 2022",
        "text":"This paper describes NAIST's simultaneous speech translation systems developed for IWSLT 2022 Evaluation Campaign. We participated the speech-to-speech track for Englishto-German and English-to-Japanese. Our primary submissions were end-to-end systems using adaptive segmentation policies based on Prefix Alignment.",
        "id":248780175
      },
      {
        "title":"A TAG-derived Database for Treebank Search and Parser Analysis",
        "text":"Recent work has proposed the use of an extracted tree grammar as the basis for treebank analysis, in which queries are stated over the elementary trees, which are small chunks of syntactic structure. In this work we integrate search over the derivation tree with this approach in order to analyze differences between two sets of annotation on the same text, an important problem for parser analysis and evaluation of inter-annotator agreement.",
        "id":18434088
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper first conducted the positioned error test for the MAUVE metric?",
    "positive_ctxs":[
      {
        "title":"On the Blind Spots of Model-Based Evaluation Metrics for Text Generation",
        "text":"In this work, we explore a useful but often neglected methodology for robustness analysis of text generation evaluation metrics: stress tests with synthetic data. Basically, we design and synthesize a wide range of potential errors and check whether they result in a commensurate drop in the metric scores. We examine a range of recently proposed evaluation metrics based on pretrained language models, for the tasks of open-ended generation, translation, and summarization. Our experiments reveal interesting insensitivities, biases, or even loopholes in existing metrics. For example, we find that BERTScore is confused by truncation errors in summarization, and MAUVE (built on top of GPT-2) is insensitive to errors at the beginning or middle of generations. Further, we investigate the reasons behind these blind spots and suggest practical workarounds for a more reliable evaluation of text generation. We have released our code and data at https:\/\/github. com\/cloudygoose\/blindspot_nlg. * Equal contribution. Both are corresponding authors. w* in the email refers to washington.",
        "id":254877323
      }
    ],
    "negative_ctxs":[
      {
        "title":"LEARNING TO ACT FROM ACTIONLESS VIDEOS THROUGH DENSE CORRESPONDENCES",
        "text":"In this work, we present an approach to construct a video-based robot policy capable of reliably executing diverse tasks across different robots and environments from few video demonstrations without using any action annotations. Our method leverages images as a task-agnostic representation, encoding both the state and action information, and text as a general representation for specifying robot goals. By synthesizing videos that \"hallucinate\" robot executing actions and in combination with dense correspondences between frames, our approach can infer the closed-formed action to execute to an environment without the need of any explicit action labels. This unique capability allows us to train the policy solely based on RGB videos and deploy learned policies to various robotic tasks. We demonstrate the efficacy of our approach in learning policies on table-top manipulation and navigation tasks. Additionally, we contribute an open-source framework for efficient video modeling, enabling the training of high-fidelity policy models with four GPUs within a single day. † Work done while Po-Chen Ko is a visiting student at MIT. Project page: https:\/\/flow-diffusion.github.io\/",
        "id":263908842
      },
      {
        "title":"The development of tagged Uyghur corpus",
        "text":"The history and development of Uyghur language is introduced. After a brief introduction to the development of Uyghur words, morphology and syntax, we explain our developing of a computer-aided contemporary Uyghur language tagging system. The coverage of this corpus, the resources building, the rules for syncopating and tagging etyma and termination, and the tagging of a corpus using a small tagset are explained. Some practical methods solving problems in Uyghur language tagging are also proposed.Key word: history and developmnet of Uyghur language, Uyghur tagged corpus, Uyghur language tagging system.",
        "id":15075946
      },
      {
        "title":"",
        "text":"",
        "id":219310220
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper first construct large-scale corpus to improve in-context learning of large language models in the pre-training stage?",
    "positive_ctxs":[
      {
        "title":"Pre-Training to Learn in Context",
        "text":"In-context learning, where pre-trained language models learn to perform tasks from task examples and instructions in their contexts, has attracted much attention in the NLP community. However, the ability of in-context learning is not fully exploited because language models are not explicitly trained to learn in context. To this end, we propose PICL (Pretraining for In-Context Learning), a framework to enhance the language models' in-context learning ability by pre-training the model on a large collection of \"intrinsic tasks\" in the general plain-text corpus using the simple language modeling objective. PICL encourages the model to infer and perform tasks by conditioning on the contexts while maintaining task generalization of pre-trained models. We evaluate the in-context learning performance of the model trained with PICL on seven widelyused text classification datasets and the SUPER-NATURALINSTRCTIONS benchmark, which contains 100+ NLP tasks formulated to text generation. Our experiments show that PICL is more effective and task-generalizable than a range of baselines, outperforming larger language models with nearly 4x parameters. The code is publicly available at https:\/\/github. com\/thu-coai\/PICL.",
        "id":258715048
      }
    ],
    "negative_ctxs":[
      {
        "title":"Using Distributional Similarity of Multi-way Translations to Predict Multiword Expression Compositionality",
        "text":"We predict the compositionality of multiword expressions using distributional similarity between each component word and the overall expression, based on translations into multiple languages. We evaluate the method over English noun compounds, English verb particle constructions and German noun compounds. We show that the estimation of compositionality is improved when using translations into multiple languages, as compared to simply using distributional similarity in the source language. We further find that string similarity complements distributional similarity.",
        "id":2527274
      },
      {
        "title":"Expert Stance Graphs for Computational Argumentation",
        "text":"We describe the construction of an Expert Stance Graph, a novel, large-scale knowledge resource that encodes the stance of more than 100,000 experts towards a variety of controversial topics. We suggest that this graph may be valuable for various fundamental tasks in computational argumentation. Experts and topics in our graph are Wikipedia entries. Both automatic and semi-automatic methods for building the graph are explored, and manual assessment validates the high accuracy of the resulting graph.",
        "id":18648384
      },
      {
        "title":"A SYSTEM FOR TRANSLATING LOCATIVE PREPOSITIONS FROM ENGLISH INTO FRENCH*",
        "text":"Machine translation of locative prepositions is not straightforward, even between closely related languages. This paper discusses a system of translation of locative prepositions between English and French. The system is based on the premises that English and French do not always conceptualize objects in the same way, and that this accounts for the major differences in the ways that locative prepositions are used in these languages. This paper introduces knowledge representations of conceptualizations of objects, and a method for translating prepositions based on these conceptual representations.",
        "id":321686
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper first constructed a structured knowledge base to interconnect different human social roles and attributes?",
    "positive_ctxs":[
      {
        "title":"PEACOK: Persona Commonsense Knowledge for Consistent and Engaging Narratives",
        "text":"Sustaining coherent and engaging narratives requires dialogue or storytelling agents to understand how the personas of speakers or listeners ground the narrative.",
        "id":258480238
      }
    ],
    "negative_ctxs":[
      {
        "title":"Building a Bio-Event Annotated Corpus for the Acquisition of Semantic Frames from Biomedical Corpora",
        "text":"This paper reports on the design and construction of a bio-event annotated corpus which was developed with a specific view to the acquisition of semantic frames from biomedical corpora. We describe the adopted annotation scheme and the annotation process, which is supported by a dedicated annotation tool. The annotated corpus contains 677 abstracts of biomedical research articles.",
        "id":6881086
      },
      {
        "title":"RWTH-PHOENIX-Weather: A Large Vocabulary Sign Language Recognition and Translation Corpus",
        "text":"This paper introduces the RWTH-PHOENIX-Weather corpus, a video-based, large vocabulary corpus of German Sign Language suitable for statistical sign language recognition and translation. In contrast to most available sign language data collections, the RWTH-PHOENIX-Weather corpus has not been recorded for linguistic research but for the use in statistical pattern recognition. The corpus contains weather forecasts recorded from German public TV which are manually annotated using glosses distinguishing sign variants, and time boundaries have been marked on the sentence and the gloss level. Further, the spoken German weather forecast has been transcribed in a semi-automatic fashion using a state-of-the-art automatic speech recognition system. Moreover, an additional translation of the glosses into spoken German has been created to capture allowable translation variability. In addition to the corpus, experimental baseline results for hand and head tracking, statistical sign language recognition and translation are presented.",
        "id":2516961
      },
      {
        "title":"NO TRAINING REQUIRED: EXPLORING RANDOM EN- CODERS FOR SENTENCE CLASSIFICATION",
        "text":"We explore various methods for computing sentence representations from pretrained word embeddings without any training, i.e., using nothing but random parameterizations. Our aim is to put sentence embeddings on more solid footing by 1) looking at how much modern sentence embeddings gain over random methods-as it turns out, surprisingly little; and by 2) providing the field with more appropriate baselines going forward-which are, as it turns out, quite strong. We also make important observations about proper experimental protocol for sentence classification evaluation, together with recommendations for future research. * Work done as an intern at Facebook AI Research. 1 Code available at https:\/\/github.com\/facebookresearch\/randsent. We go down a well-paved avenue of exploration in the machine learning research community, and exploit an insight originally due to Cover (1965): \"A complex pattern-classification problem, cast in a high-dimensional space nonlinearly, is more likely to be linearly separable than in a lowdimensional space, provided that the space is not densely populated.\" That is, we examine three types of models for obtaining randomly computed sentence representations from pre-trained word embeddings: bag of random embedding projections, randomly initialized recurrent networks and echo state networks.Our goal is not to obtain a new state of the art, but to put current state of the art methods on more solid footing by 1) looking at how much they gain compared to random methods; and 2) providing the field with more solid baselines going forward. We make several important observations about proper experimental protocol for sentence classification evaluation; and finish with a list of takeaway recommendations.",
        "id":59336240
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper first explored In-context learning in a cross lingual setup and made use of alignment to better it's performance?",
    "positive_ctxs":[
      {
        "title":"Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment",
        "text":"In-context learning (ICL) unfolds as large language models become capable of inferring test labels conditioned on a few labeled samples without any gradient update. ICL-enabled large language models provide a promising step forward toward bypassing recurrent annotation costs in a low-resource setting. Yet, only a handful of past studies have explored ICL in a cross-lingual setting, in which the need for transferring label-knowledge from a high-resource language to a low-resource one is immensely crucial. To bridge the gap, we provide the first in-depth analysis of ICL for cross-lingual text classification. We find that the prevalent mode of selecting random inputlabel pairs to construct the prompt-context is severely limited in the case of cross-lingual ICL, primarily due to the lack of alignment in the input as well as the output spaces. To mitigate this, we propose a novel prompt construction strategy -Cross-lingual In-context Source-Target Alignment (X-InSTA). With an injected coherence in the semantics of the input examples and a task-based alignment across the source and target languages, X-InSTA is able to outperform random prompt selection by a large margin across three different tasks using 44 different cross-lingual pairs.",
        "id":258588286
      }
    ],
    "negative_ctxs":[
      {
        "title":"Induction Networks for Few-Shot Text Classification",
        "text":"Text classification tends to struggle when data is deficient or when it needs to adapt to unseen classes. In such challenging scenarios, recent studies have used meta-learning to simulate the few-shot task, in which new queries are compared to a small support set at the samplewise level. However, this sample-wise comparison may be severely disturbed by the various expressions in the same class. Therefore, we should be able to learn a general representation of each class in the support set and then compare it to new queries. In this paper, we propose a novel Induction Network to learn such a generalized class-wise representation, by innovatively leveraging the dynamic routing algorithm in meta-learning. In this way, we find the model is able to induce and generalize better. We evaluate the proposed model on a well-studied sentiment classification dataset (English) and a real-world dialogue intent classification dataset (Chinese). Experiment results show that on both datasets, the proposed model significantly outperforms the existing state-of-the-art approaches, proving the effectiveness of class-wise generalization in few-shot text classification.",
        "id":202776447
      },
      {
        "title":"Identify, Align, and Integrate: Matching Knowledge Graphs to Commonsense Reasoning Tasks",
        "text":"Integrating external knowledge into commonsense reasoning tasks has shown progress in resolving some, but not all, knowledge gaps in these tasks. For knowledge integration to yield peak performance, it is critical to select a knowledge graph (KG) that is well-aligned with the given task's objective. We present an approach to assess how well a candidate KG can correctly identify and accurately fill in gaps of reasoning for a task, which we call KG-to-task match. We show this KGto-task match in 3 phases: knowledge-task identification, knowledge-task alignment, and knowledge-task integration. We also analyze our transformer-based KG-to-task models via commonsense probes to measure how much knowledge is captured in these models before and after KG integration.",
        "id":233189603
      },
      {
        "title":"The influence of written task descriptions in Wizard of Oz experiments",
        "text":"In this paper we investigate an assertion made by Richards and  Underwood (1985), who claim that people interacting with a spoken information retrieval system, structure their information in such a uniform manner that this regularity can be used to enhance the performance of the dialog system. We put forward the possibility that this uniform ordering of information might be due to the design of the written task descriptions used in Wizard of Oz experiments.",
        "id":16030829
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper first found that multilingual models can inference cross-lingual supervision in MLM training by themself?",
    "positive_ctxs":[
      {
        "title":"On-the-fly Cross-lingual Masking for Multilingual Pre-training",
        "text":"In multilingual pre-training with the objective of MLM (masked language modeling) on multiple monolingual corpora, multilingual models only learn cross-linguality implicitly from isomorphic spaces formed by overlapping different language spaces due to the lack of explicit cross-lingual forward pass. In this work, we present CLPM (Cross-lingual Prototype Masking), a dynamic and token-wise masking scheme, for multilingual pre-training, using a special token [C] x to replace a random token x in the input sentence.[C] x is a cross-lingual prototype for x and then forms an explicit crosslingual forward pass. We instantiate CLPM for the multilingual pre-training phase of UNMT (unsupervised neural machine translation), and experiments show that CLPM can consistently improve the performance of UNMT models on {De, Ro, N e} ↔ En. Beyond UNMT or bilingual tasks, we show that CLPM can consistently improve the performance of multilingual models on cross-lingual classification.",
        "id":259370532
      }
    ],
    "negative_ctxs":[
      {
        "title":"Training Language Models under Resource Constraints for Adversarial Advertisement Detection",
        "text":"Advertising on e-commerce and social media sites deliver ad impressions at web scale on a daily basis driving value to both shoppers and advertisers. This scale necessitates programmatic ways of detecting unsuitable content in ads to safeguard customer experience and trust. This paper focusses on techniques for training text classification models under resource constraints, built as part of automated solutions for advertising content moderation. We show how weak supervision, curriculum learning and multi-lingual training can be applied effectively to fine-tune BERT and its variants for text classification tasks in conjunction with different data augmentation strategies. Our extensive experiments on multiple languages show that these techniques detect adversarial ad categories with a substantial gain in precision at high recall threshold over the baseline.",
        "id":235097217
      },
      {
        "title":"The RepEval 2017 Shared Task: Multi-Genre Natural Language Inference with Sentence Representations",
        "text":"This paper presents the results of the RepEval 2017 Shared Task, which evaluated neural network sentence representation learning models on the Multi-Genre Natural Language Inference corpus (MultiNLI) recently introduced by Williams et al. (2017). All of the five participating teams beat the bidirectional LSTM (BiLSTM) and continuous bag of words baselines reported in Williams et al.. The best single model used stacked BiLSTMs with residual connections to extract sentence features and reached 74.5% accuracy on the genre-matched test set. Surprisingly, the results of the competition were fairly consistent across the genrematched and genre-mismatched test sets, and across subsets of the test data representing a variety of linguistic phenomena, suggesting that all of the submitted systems learned reasonably domainindependent representations for sentence meaning.",
        "id":30758763
      },
      {
        "title":"Improving Named Entity Recognition by Jointly Learning to Disambiguate Morphological Tags Title and Abstract in Turkish",
        "text":"Previous studies have shown that linguistic features of a word such as possession, genitive or other grammatical cases can be employed in word representations of a named entity recognition (NER) tagger to improve the performance for morphologically rich languages. However, these taggers require external morphological disambiguation (MD) tools to function which are hard to obtain or non-existent for many languages. In this work, we propose a model which alleviates the need for such disambiguators by jointly learning NER and MD taggers in languages for which one can provide a list of candidate morphological analyses. We show that this can be done independent of the morphological annotation schemes, which differ among languages. Our experiments employing three different model architectures that join these two tasks show that joint learning improves NER performance. Furthermore, the morphological disambiguator's performance is shown to be competitive.Dahaönceki çalışmalar, biçimbilimsel olarak zengin dillerdeki varlık ismi tanıma (VAT) başarısını artırmak için sözcüklerin iyelik, genitif ve benzeri hâllerinin kullanılabilecegini göstermiştir. Ancak, bu türden varlık ismi tanıma işaretleyicilerinin çalışabilmesi için elde edilmesi zor veya bazı diller için imkansız olan dışsal biçimbilimsel ayrıştırıcılara (BA) ihtiyaç vardır. Bu çalışmada, bu tür ayrıştırıcılara olan ihtiyacı ortadan kaldırmak için VAT ve BA görevlerini aynı anda çözen ve aday biçimbilimsel çözümlemelerin sunulabildigi dillere uygulanabilen bir modelönerilmektedir. Bunun dillere göre degişen biçimbilimsel işaretleme şemalarından bagımsız olarak yapılabildigi gösterilmiştir. Bu iki görevi aynı anda gerçekleştirenüç farklı model mimarisi kullanarak yaptıgımız deneyler birlikteögrenmenin VAT başarısını artırdıgını göstermiştir. Buna ek olarak, biçimbilimsel ayrıştırıcının başarısınınönceki çalışmalarla karşılaştırılabilir oldugu görülmüştür. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:\/\/ creativecommons.org\/licenses\/by\/4.0\/",
        "id":49870417
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper first introduced document content as an intermediate generation target and utilized textual document identifiers in generative retrieval?",
    "positive_ctxs":[
      {
        "title":"TOME: A Two-stage Approach for Model-based Retrieval",
        "text":"Recently, model-based retrieval has emerged as a new paradigm in text retrieval that discards the index in the traditional retrieval model and instead memorizes the candidate corpora using model parameters. This design employs a sequence-to-sequence paradigm to generate document identifiers, which enables the complete capture of the relevance between queries and documents and simplifies the classic indexretrieval-rerank pipeline. Despite its attractive qualities, there remain several major challenges in model-based retrieval, including the discrepancy between pre-training and fine-tuning, and the discrepancy between training and inference. To deal with the above challenges, we propose a novel two-stage model-based retrieval approach called TOME, which makes two major technical contributions, including the utilization of tokenized URLs as identifiers and the design of a two-stage generation architecture. We also propose a number of training strategies to deal with the training difficulty as the corpus size increases. Extensive experiments and analysis on MS MARCO and Natural Questions demonstrate the effectiveness of our proposed approach, and we investigate the scaling laws of TOME by examining various influencing factors.",
        "id":258762633
      }
    ],
    "negative_ctxs":[
      {
        "title":"The representation of syntactically unexpressed com to nouns",
        "text":"We address the representation of nouns having complex argument structures like deverbal nominalisations. In particular we address the semantic representation of syntactically unexpressed arguments.We put forward a treatment of this kind of optional complements in a framework that combines HPSG syntax and the semantic approach in GL(Pustejovsky, 1995).",
        "id":8564163
      },
      {
        "title":"Using Machine Learning to Maintain Rule-based Named-Entity Recognition and Classification Systems",
        "text":"This paper presents a method that assists in maintaining a rule-based named-entity recognition and classification system. The underlying idea is to use a separate system, constructed with the use of machine learning, to monitor the performance of the rule-based system. The training data for the second system is generated with the use of the rule-based system, thus avoiding the need for manual tagging. The disagreement of the two systems acts as a signal for updating the rule-based system. The generality of the approach is illustrated by applying it to large corpora in two different languages: Greek and French. The results are very encouraging, showing that this alternative use of machine learning can assist significantly in the maintenance of rulebased systems.",
        "id":15620379
      },
      {
        "title":"Rethinking Reusable Resources",
        "text":"We address the common and recurring problem of data reuse, focusing on the following topics: (i) the current state of affairs (in particular, problems with data); (ii) requirements for change; (iii) the proposed solution (its problems and advantages, as well as related work in this area), including the canonical-, I\/O-, and data transformation models; (iv) maintenance issues; (v) implementation and deployment aspects; (vi) conclusions and future directions, including results from work done so far and aspects that merit future work.",
        "id":21351087
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper first propose to mask positions to pre-train multi-modal document transformer？",
    "positive_ctxs":[
      {
        "title":"LayoutMask: Enhance Text-Layout Interaction in Multi-modal Pre-training for Document Understanding",
        "text":"Visually-rich Document Understanding (VrDU) has attracted much research attention over the past years. Pre-trained models on a large number of document images with transformer-based backbones have led to significant performance gains in this field. The major challenge is how to fusion the different modalities (text, layout, and image) of the documents in a unified model with different pre-training tasks. This paper focuses on improving text-layout interactions and proposes a novel multi-modal pre-training model, LayoutMask. LayoutMask uses local 1D position, instead of global 1D position, as layout input and has two pre-training objectives: (1) Masked Language Modeling: predicting masked tokens with two novel masking strategies; (2) Masked Position Modeling: predicting masked 2D positions to improve layout representation learning. LayoutMask can enhance the interactions between text and layout modalities in a unified model and produce adaptive and robust multimodal representations for downstream tasks. Experimental results show that our proposed method can achieve state-of-the-art results on a wide variety of VrDU problems, including form understanding, receipt understanding, and document image classification.",
        "id":258967524
      }
    ],
    "negative_ctxs":[
      {
        "title":"FLUSH: A Flexible Lexicon Design",
        "text":"Approaches to natural language processing that use a phrasal lexicon have the advantage of easily handling linguistic constructions that might otherwise be extragrammatical. However, current phrasal lexicons are often too rigid: their phrasal entries fail to cover the more flexible constructions. FLUSH, for Flexible Lexicon Utilizing Specialized and Hierarchical knowledge, is a knowledge-based lexicon design that allows broad phrasal coverage.",
        "id":13313617
      },
      {
        "title":"The Next Chapter: A Study of Large Language Models in Storytelling",
        "text":"To enhance the quality of generated stories, recent story generation models have been investigating the utilization of higher-level attributes like plots or commonsense knowledge.The application of prompt-based learning with large language models (LLMs), exemplified by GPT-3, has exhibited remarkable performance in diverse natural language processing (NLP) tasks.This paper conducts a comprehensive investigation, utilizing both automatic and human evaluation, to compare the story generation capacity of LLMs with recent models across three datasets with variations in style, register, and length of stories.The results demonstrate that LLMs generate stories of significantly higher quality compared to other story generation models.Moreover, they exhibit a level of performance that competes with human authors, albeit with the preliminary observation that they tend to replicate real stories in situations involving world knowledge, resembling a form of plagiarism.* Now at Google DeepMind.",
        "id":260125086
      },
      {
        "title":"Annotating the Focus of Negation in Japanese Text",
        "text":"This paper proposes an annotation scheme for the focus of negation in Japanese text. Negation has its scope and the focus within the scope. The scope of negation is the part of the sentence that is negated; the focus is the part of the scope that is most prominently or explicitly negated. In natural language processing, correct interpretation of negated statements requires precise detection of the focus of negation in the statements. As a foundation for developing a negation focus detector for Japanese, we have annotated textdata of \"Rakuten Travel: User review data\" and the newspaper subcorpus of the \"Balanced Corpus of Contemporary Written Japanese\" with labels proposed in our annotation scheme. We report 1,327 negation cues and the foci in the corpora, and present classification of these foci based on syntactic types and semantic types. We also propose a system for detecting the focus of negation in Japanese using 16 heuristic rules and report the performance of the system.",
        "id":16799093
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper first proposed a cross-domain language model to automatically generate much labeled data for a unlabeled target domain?",
    "positive_ctxs":[
      {
        "title":"Cross-Domain Data Augmentation with Domain-Adaptive Language Modeling for Aspect-Based Sentiment Analysis",
        "text":"Cross-domain Aspect-Based Sentiment Analysis (ABSA) aims to leverage the useful knowledge from a source domain to identify aspectsentiment pairs in sentences from a target domain. To tackle the task, several recent works explore a new unsupervised domain adaptation framework, i.e., Cross-Domain Data Augmentation (CDDA), aiming to directly generate much labeled target-domain data based on the labeled source-domain data. However, these CDDA methods still suffer from several issues: 1) preserving many source-specific attributes such as syntactic structures; 2) lack of fluency and coherence; 3) limiting the diversity of generated data. To address these issues, we propose a new cross-domain Data Augmentation approach based on Domain-Adaptive Language Modeling named DA 2 LM, which contains three stages: 1) assigning pseudo labels to unlabeled target-domain data; 2) unifying the process of token generation and labeling with a Domain-Adaptive Language Model (DALM) to learn the shared context and annotation across domains; 3) using the trained DALM to generate labeled target-domain data. Experiments show that DA 2 LM consistently outperforms previous feature adaptation and CDDA methods on both ABSA and Aspect Extraction tasks. The source code is publicly released at https:\/\/github.com\/NUSTM\/DALM.",
        "id":259370800
      }
    ],
    "negative_ctxs":[
      {
        "title":"Higher-Order Coloured Unification and Natural Language Semantics",
        "text":"In this paper, we show that Higher-Order Coloured Unification -a form of unification developed for automated theorem proving -provides a general theory for modeling the interface between the interpretation process and other sources of linguistic, non semantic information. In particular, it provides the general theory for the Primary Occurrence Restriction which (Dalrymple et al., 1991)'s analysis called for. Proc. CADE94, LNAI, pages 635-649, Nancy, France. Steve G. Pulman. 1995. Higher-order unification and the interpretation of focus. Paper submitted for publication. Kai von Fintel. 1995. A minimal theory of adverbial quantification. Unpublished draft Ms. MIT, Cambridge, March.",
        "id":12147910
      },
      {
        "title":"Minimally Supervised Domain-Adaptive Parse Reranking for Relation Extraction",
        "text":"The paper demonstrates how the generic parser of a minimally supervised information extraction framework can be adapted to a given task and domain for relation extraction (RE). For the experiments a generic deep-linguistic parser was employed that works with a largely hand-crafted headdriven phrase structure grammar (HPSG) for English. The output of this parser is a list of n best parses selected and ranked by a MaxEnt parse-ranking component, which had been trained on a more or less generic HPSG treebank. It will be shown how the estimated confidence of RE rules learned from the n best parses can be exploited for parse reranking. The acquired reranking model improves the performance of RE in both training and test phases with the new first parses. The obtained significant boost of recall does not come from an overall gain in parsing performance but from an application-driven selection of parses that are best suited for the RE task. Since the readings best suited for successful rule extraction and instance extraction are often not the readings favored by a regular parser evaluation, generic parsing accuracy actually decreases. The novel method for taskspecific parse reranking does not require any annotated data beyond the semantic seed, which is needed anyway for the RE task.",
        "id":15295970
      },
      {
        "title":"In Plain Sight: Media Bias Through the Lens of Factual Reporting",
        "text":"The increasing prevalence of political bias in news media calls for greater public awareness of it, as well as robust methods for its detection. While prior work in NLP has primarily focused on the lexical bias captured by linguistic attributes such as word choice and syntax, other types of bias stem from the actual content selected for inclusion in the text. In this work, we investigate the effects of informational bias: factual content that can nevertheless be deployed to sway reader opinion. We first produce a new dataset, BASIL, of 300 news articles annotated with 1,727 bias spans 1 and find evidence that informational bias appears in news articles more frequently than lexical bias. We further study our annotations to observe how informational bias surfaces in news articles by different media outlets. Lastly, a baseline model for informational bias prediction is presented by finetuning BERT on our labeled data, indicating the challenges of the task and future directions.",
        "id":202537179
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper first proposed extracting the pair of target and stance from sentences?",
    "positive_ctxs":[
      {
        "title":"A New Direction in Stance Detection: Target-Stance Extraction in the Wild",
        "text":"Stance detection aims to detect the stance toward a corresponding target.",
        "id":259370562
      }
    ],
    "negative_ctxs":[
      {
        "title":"Optimal Data Set Selection: An Application to Grapheme-to-Phoneme Conversion",
        "text":"In this paper we introduce the task of unlabeled, optimal, data set selection. Given a large pool of unlabeled examples, our goal is to select a small subset to label, which will yield a high performance supervised model over the entire data set. Our first proposed method, based on the rank-revealing QR matrix factorization, selects a subset of words which span the entire word-space effectively. For our second method, we develop the concept of feature coverage which we optimize with a greedy algorithm. We apply these methods to the task of grapheme-to-phoneme prediction. Experiments over a data-set of 8 languages show that in all scenarios, our selection methods are effective at yielding a small, but optimal set of labelled examples. When fed into a state-of-the-art supervised model for grapheme-to-phoneme prediction, our methods yield average error reductions of 20% over randomly selected examples.",
        "id":337505
      },
      {
        "title":"DEPAC: a Corpus for Depression and Anxiety Detection from Speech",
        "text":"Mental distress like depression and anxiety contribute to the largest proportion of the global burden of diseases. Automated diagnosis system of such disorders, empowered by recent innovations in Artificial Intelligence, can pave the way to reduce the sufferings of the affected individuals. Development of such systems requires information-rich and balanced corpora. In this work, we introduce a novel mental distress analysis audio dataset DEPAC, labelled based on established thresholds on depression and anxiety standard screening tools. This large dataset comprises multiple speech tasks per individual, as well as relevant demographic information. Alongside, we present a feature set consisting of hand-curated acoustic and linguistic features, which were found effective in identifying signs of mental illnesses in human speech. Finally, we justify the quality and effectiveness of our proposed audio corpus and feature set in predicting depression severity by comparing the performance of baseline machine learning models built on this dataset with baseline models trained on other well-known depression corpora.",
        "id":250390737
      },
      {
        "title":"Unsupervised Compositionality Prediction of Nominal Compounds",
        "text":"Nominal compounds such as red wine and nut case display a continuum of compositionality, with varying contributions from the components of the compound to its semantics. This article proposes a framework for compound compositionality prediction using distributional semantic models, evaluating to what extent they capture idiomaticity compared to human judgments. For evaluation, we introduce data sets containing human judgments in three languages: English, French, and Portuguese. The results obtained reveal a high agreement between the models and human predictions, suggesting that they are able to incorporate information about idiomaticity. We also present an in-depth evaluation of various factors that can affect prediction, such as model and corpus parameters and compositionality operations. General crosslingual analyses reveal the impact of morphological variation and corpus size in the ability of the model to predict compositionality, and of a uniform combination of the components for best results.",
        "id":56595638
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper first proposed shared adapter module across layers?",
    "positive_ctxs":[
      {
        "title":"One Network, Many Masks: Towards More Parameter-Efficient Transfer Learning",
        "text":"Fine-tuning pre-trained language models for multiple tasks tends to be expensive in terms of storage. To mitigate this, parameter-efficient transfer learning (PETL) methods have been proposed to address this issue, but they still require a significant number of parameters and storage when being applied to broader ranges of tasks. To achieve even greater storage reduction, we propose PROPETL, a novel method that enables efficient sharing of a single PETL module which we call prototype network (e.g., adapter, LoRA, and prefix-tuning) across layers and tasks. We then learn binary masks to select different sub-networks from the shared prototype network and apply them as PETL modules into different layers. We find that the binary masks can determine crucial information from the network, which is often ignored in previous studies. Our work can also be seen as a type of pruning method, where we find that overparameterization also exists in the seemingly small PETL modules. We evaluate PROPETL on various downstream tasks and show that it can outperform other PETL methods with approximately 10% of the parameter storage required by the latter.",
        "id":258960642
      }
    ],
    "negative_ctxs":[
      {
        "title":"Building a SentiWordNet For Odia",
        "text":"As a discipline of Natural",
        "id":2894283
      },
      {
        "title":"Chinese Syntactic Parsing Evaluation",
        "text":"The paper introduced the task designing ideas, data preparation methods, evaluation metrics and results of the second Chinese syntactic parsing evaluation (CIPS-Bakeoff-ParsEval-2010) jointed with SIGHAN Bakeoff tasks.",
        "id":2706681
      },
      {
        "title":"Resolving Spatial References using Crowdsourced Geographical Data",
        "text":"We present a study in which we seek to interpret spatial references that are part of in-situ route descriptions. Our aim is to resolve these references to actual entities and places in the city using a crowdsourced geographic database (Open-StreetMap). We discuss the problems related to this task, and present a possible automatic reference resolution method that can find the correct referent in 68% of the cases using features that are easily computable from the map.",
        "id":4646592
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper first proposed to combine pretrained masked language models (BERT) and discrete diffusion language models?",
    "positive_ctxs":[
      {
        "title":"DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models",
        "text":"We present DiffusionBERT, a new generative masked language model based on discrete diffusion models. Diffusion models and many pre-trained language models have a shared training objective, i.e., denoising, making it possible to combine the two powerful models and enjoy the best of both worlds. On the one hand, diffusion models offer a promising training strategy that helps improve the generation quality. On the other hand, pre-trained denoising language models (e.g., BERT) can be used as a good initialization that accelerates convergence. We explore training BERT to learn the reverse process of a discrete diffusion process with an absorbing state and elucidate several designs to improve it. First, we propose a new noise schedule for the forward diffusion process that controls the degree of noise added at each step based on the information of each token. Second, we investigate several designs of incorporating the time step into BERT. Experiments on unconditional text generation demonstrate that DiffusionBERT achieves significant improvement over existing diffusion models for text (e.g., D3PM and Diffusion-LM) and previous generative masked language models in terms of perplexity and BLEU score. 1",
        "id":254044147
      }
    ],
    "negative_ctxs":[
      {
        "title":"Localization of Fake News Detection via Multitask Transfer Learning",
        "text":"The use of the internet as a fast medium of spreading fake news reinforces the need for computational tools that combat it. Techniques that train fake news classifiers exist, but they all assume an abundance of resources including large labeled datasets and expert-curated corpora, which low-resource languages may not have. In this work, we make two main contributions: First, we alleviate resource scarcity by constructing the first expertly-curated benchmark dataset for fake news detection in Filipino, which we call \"Fake News Filipino.\" Second, we benchmark Transfer Learning (TL) techniques and show that they can be used to train robust fake news classifiers from little data, achieving 91% accuracy on our fake news dataset, reducing the error by 14% compared to established few-shot baselines. Furthermore, lifting ideas from multitask learning, we show that augmenting transformer-based transfer techniques with auxiliary language modeling losses improves their performance by adapting to writing style. Using this, we improve TL performance by 4-6%, achieving an accuracy of 96% on our best model. Lastly, we show that our method generalizes well to different types of news articles, including political news, entertainment news, and opinion articles.",
        "id":204800316
      },
      {
        "title":"Published as a conference paper at ICLR 2021 LEARNING TO RECOMBINE AND RESAMPLE DATA FOR COMPOSITIONAL GENERALIZATION",
        "text":"Flexible neural sequence models outperform grammar-and automaton-based counterparts on a variety of tasks. However, neural models perform poorly in settings requiring compositional generalization beyond the training data-particularly to rare or unseen subsequences. Past work has found symbolic scaffolding (e.g. grammars or automata) essential in these settings. We describe R&R, a learned data augmentation scheme that enables a large category of compositional generalizations without appeal to latent symbolic structure. R&R has two components: recombination of original training examples via a prototype-based generative model and resampling of generated examples to encourage extrapolation. Training an ordinary neural sequence model on a dataset augmented with recombined and resampled examples significantly improves generalization in two language processing problems-instruction following (SCAN) and morphological analysis (SIGMORPHON 2018)-where R&R enables learning of new constructions and tenses from as few as eight initial examples. . Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In",
        "id":222208634
      },
      {
        "title":"SHINE: SHARING THE INVERSE ESTIMATE FROM THE FORWARD PASS FOR BI-LEVEL OPTIMIZATION AND IM- PLICIT MODELS",
        "text":"In recent years, implicit deep learning has emerged as a method to increase the effective depth of deep neural networks. While their training is memory-efficient, they are still significantly slower to train than their explicit counterparts. In Deep Equilibrium Models (DEQs), the training is performed as a bi-level problem, and its computational complexity is partially driven by the iterative inversion of a huge Jacobian matrix. In this paper, we propose a novel strategy to tackle this computational bottleneck from which many bi-level problems suffer. The main idea is to use the quasi-Newton matrices from the forward pass to efficiently approximate the inverse Jacobian matrix in the direction needed for the gradient computation. We provide a theorem that motivates using our method with the original forward algorithms. In addition, by modifying these forward algorithms, we further provide theoretical guarantees that our method asymptotically estimates the true implicit gradient. We empirically study this approach and the recent Jacobian-Free method in different settings, ranging from hyperparameter optimization to large Multiscale DEQs (MDEQs) applied to CIFAR and ImageNet. Both methods reduce significantly the computational cost of the backward pass. While SHINE has a clear advantage on hyperparameter optimization problems, both methods attain similar computational performances for larger scale problems such as MDEQs at the cost of a limited performance drop compared to the original models.",
        "id":235266229
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_acl",
    "question":"Which paper first proposed to only update some original weights of self-attention layers in parameter-efficient fine-tuning?",
    "positive_ctxs":[
      {
        "title":"HiFi: High-Information Attention Heads Hold for Parameter-Efficient Model Adaptation",
        "text":"To fully leverage the advantages of large-scale pre-trained language models (PLMs) on downstream tasks, it has become a ubiquitous adaptation paradigm to fine-tune the entire parameters of PLMs. However, this paradigm poses issues of inefficient updating and resource overconsuming for fine-tuning in data-scarce and resource-limited scenarios, because of the large scale of parameters in PLMs. To alleviate these concerns, in this paper, we propose a parameterefficient fine-tuning method HiFi, that is, only the highly informative and strongly correlated attention heads for the specific task are finetuned. To search for those significant attention heads, we develop a novel framework to analyze the effectiveness of heads. Specifically, we first model the relationship between heads into a graph from two perspectives of information richness and correlation, and then apply PageRank algorithm to determine the relative importance of each head. Extensive experiments on the GLUE benchmark demonstrate the effectiveness of our method, and show that HiFi obtains state-of-the-art performance over the prior baselines. * Corresponding author. (a) Full Fine-tuning (b1) Adapter-like (c) Non-structured Method (b2) HiFi (Ours) (b) Structured Method Updated Param. Extra Updated Param. Frozen Param.",
        "id":258557131
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"Low-literate users with intellectual or developmental disabilities (IDD) and\/or complex communication needs (CCN) require specific writing support. We present a system that interactively supports fast and correct writing of a variant of Leichte Sprache (LS; German term for easy-toread German), slightly extended within and beyond the inner-sentential syntactic level. The system provides simple and intuitive dialogues for selecting options from a natural-language paraphrase generator. Moreover, it reminds the user to add text elements enhancing understandability, audience design, and text coherence. In earlier development phases, the system was evaluated with different groups of substitute users. Here, we report a case study with seven low-literate users with IDD.",
        "id":248780055
      },
      {
        "title":"Multi-Source Translation Methods",
        "text":"Multi-parallel corpora provide a potentially rich resource for machine translation. This paper surveys existing methods for utilizing such resources, including hypothesis ranking and system combination techniques. We find that despite significant research into system combination, relatively little is know about how best to translate when multiple parallel source languages are available. We provide results to show that the MAX multilingual multi-source hypothesis ranking method presented by Och and Ney(2001)does not reliably improve translation quality when a broad range of language pairs are considered. We also show that the PROD multilingual multi-source hypothesis ranking method of Och and Ney (2001) cannot be used with standard phrase-based translation engines, due to a high number of unreachable hypotheses. Finally, we present an oracle experiment which shows that current hypothesis ranking methods fall far short of the best results reachable via sentence-level ranking.",
        "id":4528852
      },
      {
        "title":"Shared Task Papers",
        "text":"In this paper, we present a novel approach to combine the two variants of phrasebased APE (monolingual and contextaware) by a factored machine translation model that is able to leverage benefits from both. Our factored APE models include part-of-speech-tag and class-based neural language models (LM) along with statistical word-based LM to improve the fluency of the post-edits. These models are built upon a data augmentation technique which helps to mitigate the problem of over-correction in phrase-based APE systems. Our primary APE system further incorporates a quality estimation (QE) model, which aims to select the best translation between the MT output and the automatic post-edit. According to the shared task results, our primary and contrastive (which does not include the QE module) submissions have similar performance and achieved significant improvement of 3.31% TER and 4.25% BLEU (relative) over the baseline MT system on the English-German evaluation set.",
        "id":95601
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper first published a real-world Chinese-English text image translation dataset?",
    "positive_ctxs":[
      {
        "title":"Exploring Better Text Image Translation with Multimodal Codebook",
        "text":"Text image translation (TIT) aims to translate the source texts embedded in the image to target translations, which has a wide range of applications and thus has important research value. However, current studies on TIT are confronted with two main bottlenecks: 1) this task lacks a publicly available TIT dataset, 2) dominant models are constructed in a cascaded manner, which tends to suffer from the error propagation of optical character recognition (OCR). In this work, we first annotate a Chinese-English TIT dataset named OCRMT30K, providing convenience for subsequent studies. Then, we propose a TIT model with a multimodal codebook, which is able to associate the image with relevant texts, providing useful supplementary information for translation. Moreover, we present a multi-stage training framework involving text machine translation, image-text alignment, and TIT tasks, which fully exploits additional bilingual texts, OCR dataset and our OCRMT30K dataset to train our model. Extensive experiments and in-depth analyses strongly demonstrate the effectiveness of our proposed model and training framework.",
        "id":258960136
      }
    ],
    "negative_ctxs":[
      {
        "title":"Handling Sparse Data by Successive Abstraction",
        "text":"A general, practical method for handling sparse data that avoids held-out data and iterative reestimation is derived from first principles. It has been tested on a part-of-speech tagging task and outperformed (deleted) interpolation with context-independent weights, even when the latter used a globally optimal parameter setting determined a posteriori.",
        "id":3714725
      },
      {
        "title":"Mixture of Attention Heads: Selecting Attention Heads Per Token",
        "text":"Mixture-of-Experts (MoE) networks have been proposed as an efficient way to scale up model capacity and implement conditional computing. However, the study of MoE components mostly focused on the feedforward layer in Transformer architecture. This paper proposes the Mixture of Attention Heads (MoA), a new architecture that combines multi-head attention with the MoE mechanism. MoA includes a set of attention heads that each has its own set of parameters. Given an input, a router dynamically selects a subset of k attention heads per token. This conditional computation schema allows MoA to achieve stronger performance than the standard multi-head attention layer. Furthermore, the sparsely gated MoA can easily scale up the number of attention heads and the number of parameters while preserving computational efficiency. In addition to the performance improvements, MoA also automatically differentiates heads' utilities, providing a new perspective to discuss the model's interpretability. We conducted experiments on several important tasks, including Machine Translation and Masked Language Modeling. Experiments have shown promising results on several tasks against strong baselines that involve large and very deep models 1 .",
        "id":252815815
      },
      {
        "title":"Semantics-Driven Shallow Parsing for Chinese Semantic Role Labeling",
        "text":"One deficiency of current shallow parsing based Semantic Role Labeling (SRL) methods is that syntactic chunks are too small to effectively group words. To partially resolve this problem, we propose semantics-driven shallow parsing, which takes into account both syntactic structures and predicate-argument structures. We also introduce several new \"path\" features to improve shallow parsing based SRL method. Experiments indicate that our new method obtains a significant improvement over the best reported Chinese SRL result.",
        "id":7891063
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper first shows that it is possible to maintain high LLM reasoning performance with in-context examples that are absurdly wrong?",
    "positive_ctxs":[
      {
        "title":"Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters",
        "text":"Chain-of-Thought (CoT) prompting can dramatically improve the multi-step reasoning abilities of large language models (LLMs). CoT explicitly encourages the LLM to generate intermediate rationales for solving a problem, by providing a series of reasoning steps in the demonstrations. Despite its success, there is still little understanding of what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance. In this paper, we show that CoT reasoning is possible even with invalid demonstrations-prompting with invalid reasoning steps can achieve over 80-90% of the performance obtained using CoT under various metrics, while still generating coherent lines of reasoning during inference. Further experiments show that other aspects of the rationales, such as being relevant to the query and correctly ordering the reasoning steps, are much more important for effective CoT reasoning. Overall, these findings both deepen our understanding of CoT prompting, and open up new questions regarding LLMs' capability to learn to reason in context. 1",
        "id":254877569
      }
    ],
    "negative_ctxs":[
      {
        "title":"Joint Estimation and Analysis of Risk Behavior Ratings in Movie Scripts",
        "text":"Exposure to violent, sexual, or substanceabuse content in media increases the willingness of children and adolescents to imitate similar behaviors. Computational methods that identify portrayals of risk behaviors from audio-visual cues are limited in their applicability to films in post-production, where modifications might be prohibitively expensive. To address this limitation, we propose a model that estimates content ratings based on the language use in movie scripts, making our solution available at the earlier stages of creative production. Our model significantly improves the state-of-the-art by adapting novel techniques to learn better movie representations from the semantic and sentiment aspects of a character's language use, and by leveraging the co-occurrence of risk behaviors, following a multi-task approach. Additionally, we show how this approach can be useful to learn novel insights on the joint portrayal of these behaviors, and on the subtleties that filmmakers may otherwise not pick up on.",
        "id":221896980
      },
      {
        "title":"Situated Dialogue Learning through Procedural Environment Generation",
        "text":"We teach goal-driven agents to interactively act and speak in situated environments by training on generated curriculums. Our agents operate in LIGHT (Urbanek et al., 2019)-a large-scale crowd-sourced fantasy text adventure game wherein an agent perceives and interacts with the world through textual natural language. Goals in this environment take the form of character-based quests, consisting of personas and motivations. We augment LIGHT by learning to procedurally generate additional novel textual worlds and quests to create a curriculum of steadily increasing difficulty for training agents to achieve such goals. In particular, we measure curriculum difficulty in terms of the rarity of the quest in the original training distribution-an easier environment is one that is more likely to have been found in the unaugmented dataset. An ablation study shows that this method of learning from the tail of a distribution results in significantly higher generalization abilities as measured by zeroshot performance on never-before-seen quests.",
        "id":238418980
      },
      {
        "title":"Few-Shot Table Understanding: A Benchmark Dataset and Pre-Training Baseline",
        "text":"Few-shot table understanding is a critical and challenging problem in real-world scenario as annotations over large amount of tables are usually costly. Pre-trained language models (PLMs), which have recently flourished on tabular data, have demonstrated their effectiveness for table understanding tasks. However, fewshot table understanding is rarely explored due to the deficiency of public table pre-training corpus and well-defined downstream benchmark tasks, especially in Chinese. In this paper, we establish a benchmark dataset, FewTUD, which consists of 5 different tasks with human annotations to systematically explore the fewshot table understanding in depth. Since there is no large number of public Chinese tables, we also collect a large-scale, multi-domain tabular corpus to facilitate future Chinese table pre-training, which includes one million tables and related natural language text with auxiliary supervised interaction signals. Finally, we present FewTPT, a novel table PLM with rich interactions over tabular data, and evaluate its performance comprehensively on the benchmark. Our dataset and model will be released to the public soon.",
        "id":252375595
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper first shows that large language models can be prompted to act like professional annotators to evaluate text generation quality?",
    "positive_ctxs":[
      {
        "title":"Can Large Language Models Be an Alternative to Human Evaluation?",
        "text":"Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms. Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided. In this paper, we explore if such an ability of the LLMs can be used as an alternative to human evaluation. We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation. We use human evaluation and LLM evaluation to evaluate the texts in two NLP tasks: openended story generation and adversarial attacks. We show that the result of LLM evaluation is consistent with the results obtained by expert human evaluation: the texts rated higher by human experts are also rated higher by the LLMs. We also find that the results of LLM evaluation are stable over different formatting of the task instructions and the sampling algorithm used to generate the answer. We are the first to show the potential of using LLMs to assess the quality of texts and discuss the limitations and ethical considerations of LLM evaluation. , et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. , et al. 2022. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.",
        "id":258461287
      }
    ],
    "negative_ctxs":[
      {
        "title":"A method for the approximation of incremental understanding of explicit utterance meaning using predictive models in finite domains",
        "text":"This paper explores the relationship between explicit and predictive models of incremental speech understanding in a dialogue system that supports a finite set of user utterance meanings. We present a method that enables the approximation of explicit understanding using information implicit in a predictive understanding model for the same domain. We show promising performance for this method in a corpus evaluation, and discuss its practical application and annotation costs in relation to some alternative approaches.",
        "id":7776388
      },
      {
        "title":"NatLogAttack: A Framework for Attacking Natural Language Inference Models with Natural Logic",
        "text":"Reasoning has been a central topic in artificial intelligence from the beginning. The recent progress made on distributed representation and neural networks continues to improve the state-of-the-art performance of natural language inference. However, it remains an open question whether the models perform real reasoning to reach their conclusions or rely on spurious correlations. Adversarial attacks have proven to be an important tool to help evaluate the Achilles' heel of the victim models. In this study, we explore the fundamental problem of developing attack models based on logic formalism. We propose NatLogAttack to perform systematic attacks centring around natural logic, a classical logic formalism that is traceable back to Aristotle's syllogism and has been closely developed for natural language inference. The proposed framework renders both label-preserving and label-flipping attacks. We show that compared to the existing attack models, NatLogAttack generates better adversarial examples with fewer visits to the victim models. The victim models are found to be more vulnerable under the label-flipping setting. NatLogAttack provides a tool to probe the existing and future NLI models' capacity from a key viewpoint and we hope more logicbased attacks will be further explored for understanding the desired property of reasoning. 1",
        "id":259360453
      },
      {
        "title":"A Personalized Markov Clustering and Deep Learning Approach for Arabic Text Categorization",
        "text":"Text categorization has become a key research field in the NLP community. However, most works in this area are focused on Western languages ignoring other Semitic languages like Arabic. These languages are of immense political and social importance necessitating robust categorization techniques. In this paper, we present a novel three-stage technique to efficiently classify Arabic documents into different categories based on the words they contain. We leverage the significance of root-words in Arabic and incorporate a combination of Markov clustering and Deep Belief Networks to classify Arabic words into separate groups (clusters). Our approach is tested on two public datasets giving a F-Measure of 91.02%.",
        "id":1912220
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper first studied the efficiency robustness of multi-exit language models?",
    "positive_ctxs":[
      {
        "title":"Dynamic Transformers Provide a False Sense of Efficiency",
        "text":"Despite much success in natural language processing (NLP), pre-trained language models typically lead to a high computational cost during inference. Multi-exit is a mainstream approach to address this issue by making a tradeoff between efficiency and accuracy, where the saving of computation comes from an early exit. However, whether such saving from earlyexiting is robust remains unknown. Motivated by this, we first show that directly adapting existing adversarial attack approaches targeting model accuracy cannot significantly reduce inference efficiency. To this end, we propose a simple yet effective attacking framework, SAME, a novel slowdown attack framework on multi-exit models, which is specially tailored to reduce the efficiency of the multi-exit models. By leveraging the multi-exit models' design characteristics, we utilize all internal predictions to guide the adversarial sample generation instead of merely considering the final prediction. Experiments on the GLUE benchmark show that SAME can effectively diminish the efficiency gain of various multi-exit models by 80% on average, convincingly validating its effectiveness and generalization ability.",
        "id":258832833
      }
    ],
    "negative_ctxs":[
      {
        "title":"RECONSIDER: Improved Re-Ranking using Span-Focused Cross-Attention for Open Domain Question Answering",
        "text":"State-of-the-art Machine Reading Comprehension (MRC) models for Open-domain Question Answering (QA) are typically trained for span selection using distantly supervised positive examples and heuristically retrieved negative examples. This training scheme possibly explains empirical observations that these models achieve a high recall amongst their top few predictions, but a low overall accuracy, motivating the need for answer re-ranking. We develop a successful re-ranking approach (RECONSIDER) for span-extraction tasks that improves upon the performance of MRC models, even beyond large-scale pre-training. RE-CONSIDER is trained on positive and negative examples extracted from high confidence MRC model predictions, and uses in-passage span annotations to perform span-focused reranking over a smaller candidate set. As a result, RECONSIDER learns to eliminate close false positives, achieving a new extractive state of the art on four QA tasks, with 45.5% Exact Match accuracy on Natural Questions with real user questions, and 61.7% on TriviaQA. We will release all related data, models, and code 1 . * Work done while at Facebook AI. 1 github.com\/facebookresearch\/reconsider",
        "id":235097577
      },
      {
        "title":"Dialect Identification under Domain Shift: Experiments with Discriminating Romanian and Moldavian",
        "text":"This paper describes a set of experiments for discriminating between two closely related language varieties, Moldavian and Romanian, under a substantial domain shift. The experiments were conducted as part of the Romanian dialect identification task in the VarDial 2020 evaluation campaign. Our best system based on linear SVM classifier obtained the first position in the shared task with an F1 score of 0.79, supporting the earlier results showing (unexpected) success of machine learning systems in this task. The additional experiments reported in this paper also show that adapting to the test set is useful when the training set is from another domain. However, the benefit of adaptation becomes doubtful even when using a small amount of data from the target domain.This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:\/\/creativecommons.org\/licenses\/by\/4.0\/.",
        "id":227231071
      },
      {
        "title":"Temporal Expressions in Japanese-to-English Machine Translation",
        "text":"This paper describes in outline a method for translating Japanese temporal expressions into English. We argue that temporal expressions form a special subset of language that is best handled as a special module in machine translation. The paper deals with problems of lexical idiosyncrasy as well as the choice of articles and prepositions within temporal expressions. In addition temporal expressions are considered as parts of larger structures, and the question of whether to translate them as noun phrases or adverbials is addressed.",
        "id":3264879
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper first use the attention weights to guide the simultaneous inference of speech translation models?",
    "positive_ctxs":[
      {
        "title":"Attention as a Guide for Simultaneous Speech Translation",
        "text":"In simultaneous speech translation (SimulST), effective policies that determine when to write partial translations are crucial to reach high output quality with low latency. Towards this objective, we propose EDATT (Encoder-Decoder Attention), an adaptive policy that exploits the attention patterns between audio source and target textual translation to guide an offlinetrained ST model during simultaneous inference. EDATT exploits the attention scores modeling the audio-translation relation to decide whether to emit a partial hypothesis or wait for more audio input. This is done under the assumption that, if attention is focused towards the most recently received speech segments, the information they provide can be insufficient to generate the hypothesis (indicating that the system has to wait for additional audio input). Results on en→{de, es} show that EDATT yields better results compared to the SimulST state of the art, with gains respectively up to 7 and 4 BLEU points for the two languages, and with a reduction in computational-aware latency up to 1.4s and 0.7s compared to existing SimulST policies applied to offline-trained models.",
        "id":254685574
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":236459899
      },
      {
        "title":"Enhancing a large scale dictionary with a two-level system",
        "text":"We present in this paper a morphological analyzer and generator for French that contains a dictionary of 700,000 inflected words called DELAF 1, and a full twolevel system aimed at the analysis of new derivatives. Hence, this tool recognizes and generates both correct inflected forms of French simple words (DELAF lookup procedure) and new derivatives and their inflected forms (two-level analysis). Moreover, a clear distinction is made between dictionary look-up processes and new words analyses in order to clearly identify the analyses that involve heuristic rules.We tested this tool upon a French corpus of 1,300,000words with significant results (Clemenceau D. 1992). With regards to efficiency, since this tool is compiled into a unique transducer, it provides a very fast look-up procedure (1,100 words per second) at a low memory cost (around 1.3 Mb in RAM).",
        "id":20376100
      },
      {
        "title":"",
        "text":"",
        "id":261341545
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_acl",
    "question":"Which paper first used structural information for coherence modeling?",
    "positive_ctxs":[
      {
        "title":"Modeling Structural Similarities between Documents for Coherence Assessment with Graph Convolutional Networks",
        "text":"Coherence is an important aspect of text quality, and various approaches have been applied to coherence modeling. However, existing methods solely focus on a single document's coherence patterns, ignoring the underlying correlation between documents. We investigate a GCN-based coherence model that is capable of capturing structural similarities between documents. Our model first creates a graph structure for each document, from where we mine different subgraph patterns. We then construct a heterogeneous graph for the training corpus, connecting documents based on their shared subgraphs. Finally, a GCN is applied to the heterogeneous graph to model the connectivity relationships. We evaluate our method on two tasks, assessing discourse coherence and automated essay scoring. Results show that our GCN-based model outperforms all baselines, achieving a new state-of-the-art on both tasks.",
        "id":259138479
      }
    ],
    "negative_ctxs":[
      {
        "title":"Discourse Element Identification in Student Essays based on Global and Local Cohesion",
        "text":"We present a method of using cohesion to improve discourse element identification for sentences in student essays. New features for each sentence are derived by considering its relations to global and local cohesion, which are created by means of cohesive resources and subtopic coverage. In our experiments, we obtain significant improvements on identifying all discourse elements, especially of +5% F 1 score on thesis and main idea. The analysis shows that global cohesion can better capture thesis statements.",
        "id":979997
      },
      {
        "title":"Modeling non-standard language",
        "text":"A specific language as used by different speakers and in different situations has a number of more or less distant varieties. Extending the notion of non-standard language to varieties that do not fit an explicitly or implicitly assumed norm or pattern, we look for methods and tools that could be applied to such texts. The needs start from the theoretical side: categories usable for the analysis of non-standard language are not readily available. However, it is not easy to find methods and tools required for its detection and diagnostics either. A general discussion of issues related to non-standard language is followed by two case studies. The first study presents a taxonomy of morphosyntactic categories as an attempt to analyse non-standard forms produced by non-native learners of Czech. The second study focusses on the role of a rule-based grammar and lexicon in the process of building and using a parsebank.",
        "id":18444016
      },
      {
        "title":"Multi-Dimensional Gender Bias Classification",
        "text":"Machine learning models are trained to find patterns in data. NLP models can inadvertently learn socially undesirable patterns when training on gender biased text. In this work, we propose a novel, general framework that decomposes gender bias in text along several pragmatic and semantic dimensions: bias from the gender of the person being spoken about, bias from the gender of the person being spoken to, and bias from the gender of the speaker. Using this fine-grained framework, we automatically annotate eight large scale datasets with gender information. In addition, we collect a new, crowdsourced evaluation benchmark. Distinguishing between gender bias along multiple dimensions enables us to train better and more fine-grained gender bias classifiers. We show our classifiers are valuable for a variety of applications, like controlling for gender bias in generative models, detecting gender bias in arbitrary text, and classifying text as offensive based on its genderedness.",
        "id":218487627
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_acl",
    "question":"Which paper found that mutual learning benefits multlingual models?",
    "positive_ctxs":[
      {
        "title":"Towards Higher Pareto Frontier in Multilingual Machine Translation",
        "text":"Multilingual neural machine translation has witnessed remarkable progress in recent years. However, the long-tailed distribution of multilingual corpora poses a challenge of Pareto optimization, i.e., optimizing for some languages may come at the cost of degrading the performance of others. Existing balancing training strategies are equivalent to a series of Pareto optimal solutions, which trade off on a Pareto frontier 1 . In this work, we propose a new training framework, Pareto Mutual Distillation (Pareto-MD), towards pushing the Pareto frontier outwards rather than making trade-offs. Specifically, Pareto-MD collaboratively trains two Pareto optimal solutions that favor different languages and allows them to learn from the strengths of each other via knowledge distillation. Furthermore, we introduce a novel strategy to enable stronger communication between Pareto optimal solutions and broaden the applicability of our approach. Experimental results on the widely-used WMT and TED datasets show that our method significantly pushes the Pareto frontier and outperforms baselines by up to +2.46 BLEU 2 .",
        "id":258887491
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":217770241
      },
      {
        "title":"Findings of the WMT 2022 Biomedical Translation Shared Task: Monolingual Clinical Case Reports",
        "text":"In the seventh edition of the WMT Biomedical Task, we addressed a total of seven language pairs, namely English\/German, English\/French, English\/Spanish, English\/Portuguese, English\/Chinese, English\/Russian, English\/Italian. This year's test sets covered three types of biomedical text genre. In addition to scientific abstracts and terminology items used in previous editions, we released test sets of clinical cases. The evaluation of clinical cases translations were given special attention by involving clinicians in the preparation of reference translations and manual evaluation. For the main MEDLINE test sets, we received a total of 609 submissions from 37 teams. For the ClinSpEn sub-task, we had the participation of five teams. * The contribution of the authors are the following: MN prepared the MEDLINE test sets, performed test set validation, manual validation, and organized the task; AJY performed test sets validation, manual validation and the automatic evaluation; RR, PT, MVN, LY, DW. GMDN, FV performed test sets validation and manual validation; CGn created reference translation and performed manual validation; RB performed manual validation; DJE, SLL, EFM, MK organized the Clin-SpEn sub-task; CGa created the baselines; and AN collected information on participants' methods, performed test sets validation, manual validation and created reference translation. All authors approved the final version of the manuscript.",
        "id":256461044
      },
      {
        "title":"",
        "text":"",
        "id":221808430
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_acl",
    "question":"Which paper highlights the need for leveraging all available resources, including dictionaries, machine translation systems, and language learners, to construct NLP data in low-resource languages?",
    "positive_ctxs":[
      {
        "title":"Rethinking Annotation: Can Language Learners Contribute?",
        "text":"Researchers have traditionally recruited native speakers to provide annotations for widely used benchmark datasets. However, there are languages for which recruiting native speakers can be difficult, and it would help to find learners of those languages to annotate the data. In this paper, we investigate whether language learners can contribute annotations to benchmark datasets. In a carefully controlled annotation experiment, we recruit 36 language learners, provide two types of additional resources (dictionaries and machine-translated sentences), and perform mini-tests to measure their language proficiency. We target three languages, English, Korean, and Indonesian, and the four NLP tasks of sentiment analysis, natural language inference, named entity recognition, and machine reading comprehension. We find that language learners, especially those with intermediate or advanced levels of language proficiency, are able to provide fairly accurate labels with the help of additional resources. Moreover, we show that data annotation improves learners' language proficiency in terms of vocabulary and grammar. One implication of our findings is that broadening the annotation task to include language learners can open up the opportunity to build benchmark datasets for languages for which it is difficult to recruit native speakers.",
        "id":252873073
      }
    ],
    "negative_ctxs":[
      {
        "title":"LEARNING MULTI-SCALE LOCAL CONDITIONAL PROBABILITY MODELS OF IMAGES",
        "text":"Deep neural networks can learn powerful prior probability models for images, as evidenced by the high-quality generations obtained with recent score-based diffusion methods. But the means by which these networks capture complex global statistical structure, apparently without suffering from the curse of dimensionality, remain a mystery. To study this, we incorporate diffusion methods into a multi-scale decomposition, reducing dimensionality by assuming a stationary local Markov model for wavelet coefficients conditioned on coarser-scale coefficients. We instantiate this model using convolutional neural networks (CNNs) with local receptive fields, which enforce both the stationarity and Markov properties. Global structures are captured using a CNN with receptive fields covering the entire (but small) low-pass image. We test this model on a dataset of face images, which are highly non-stationary and contain large-scale geometric structures. Remarkably, denoising, super-resolution, and image synthesis results all demonstrate that these structures can be captured with significantly smaller conditioning neighborhoods than required by a Markov model implemented in the pixel domain. Our results show that score estimation for large complex images can be reduced to low-dimensional Markov conditional models across scales, alleviating the curse of dimensionality.Deep neural networks (DNNs) have produced dramatic advances in synthesizing complex images and solving inverse problems, all of which rely (at least implicitly) on prior probability models. Of particular note is the recent development of \"diffusion methods\"(Sohl-Dickstein et al., 2015)",
        "id":257365017
      },
      {
        "title":"Ranking Words for Building a Japanese Defining Vocabulary",
        "text":"Defining all words in a Japanese dictionary by using a limited number of words (defining vocabulary) is helpful for Japanese children and second-language learners of Japanese. Although some English dictionaries have their own defining vocabulary, no Japanese dictionary has such vocabulary as of yet. As the first step toward building a Japanese defining vocabulary, we ranked Japanese words based on a graphbased method. In this paper, we introduce the method, and show some evaluation results of applying the method to an existing Japanese dictionary.",
        "id":6395426
      },
      {
        "title":"Frustratingly Easy Semi-Supervised Domain Adaptation",
        "text":"In this work, we propose a semisupervised extension to a well-known supervised domain adaptation approach (EA)(Daumé III, 2007). Our proposed approach (EA++) builds on the notion of augmented space (introduced in EA) and harnesses unlabeled data in target domain to ameliorate the transfer of information from source to target. This semisupervised approach to domain adaptation is extremely simple to implement, and can be applied as a pre-processing step to any supervised learner. Experimental results on sequential labeling tasks demonstrate the efficacy of the proposed method.",
        "id":3036015
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper introduce a DRO (distribution robust optimization) like training objective for doing adversarial training without constructing adversarial samples.",
    "positive_ctxs":[
      {
        "title":"DSRM: Boost Textual Adversarial Training with Distribution Shift Risk Minimization",
        "text":"Adversarial training is one of the bestperforming methods in improving the robustness of deep language models. However, robust models come at the cost of high time consumption, as they require multi-step gradient ascents or word substitutions to obtain adversarial samples. In addition, these generated samples are deficient in grammatical quality and semantic consistency, which impairs the effectiveness of adversarial training. To address these problems, we introduce a novel, effective procedure for instead adversarial training with only clean data. Our procedure, distribution shift risk minimization (DSRM), estimates the adversarial loss by perturbing the input data's probability distribution rather than their embeddings. This formulation results in a robust model that minimizes the expected global loss under adversarial attacks. Our approach requires zero adversarial samples for training and reduces time consumption by up to 70% compared to current best-performing adversarial training methods. Experiments demonstrate that DSRM considerably improves BERT's resistance to textual adversarial attacks and achieves state-of-the-art robust accuracy on various benchmarks.",
        "id":259262427
      }
    ],
    "negative_ctxs":[
      {
        "title":"Detecting English Grammatical Errors based on Machine Translation",
        "text":"Many people are learning English as a second or foreign language, and there are estimated 375 million English as a Second Language (ESL) and 750 million English as a Foreign Language (EFL) learners around the world according toGraddol (2006). Evidently,",
        "id":33616749
      },
      {
        "title":"Heterogeneous Computing for Example-Based Translation of Spoken Language",
        "text":"Spoken language translation requires both (1) high accuracy and (2) a real-time response which are difficult to achieve using conventional technologies. To fulfill the first requirement, we have adopted an Example-Based Approach. It generates a target sentence by combining partial translations obtained by mimicking best-match partial translation examples. To fulfill the second requirement, this paper proposes using a Heterogeneous Computing Platform consisting of Multiple Instruction Multiple Data (MIMD) and Single Instruction Multiple Data (SIMD) parallel machines. Example-Based Approach is dominated by two processes, each of which is optimally accelerated by utilizing MIMD and SIMD, respectively, a) to build the source structure, and b) to retrieve the bestmatch examples. Experimental results show that Example-Based Approach is drastically speeded up with the Heterogeneous Computing Platform and has a performance sufficient for real-time response, even with a large vocabulary and a highly ambiguous sentence.",
        "id":18569633
      },
      {
        "title":"Sentence-Permuted Paragraph Generation",
        "text":"Generating paragraphs of diverse contents is important in many applications. Existing generation models produce similar contents from homogenized contexts due to the fixed left-toright sentence order. Our idea is permuting the sentence orders to improve the content diversity of multi-sentence paragraph. We propose a novel framework PermGen whose objective is to maximize the expected log-likelihood of output paragraph distributions with respect to all possible sentence orders. PermGen uses hierarchical positional embedding and designs new procedures for both training phase and inference phase. Experiments on three paragraph generation benchmarks demonstrate Per-mGen generates more diverse outputs with a higher quality than existing models.",
        "id":233240749
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper introduced the human-evaluated timeliness metric for misinformation detection?",
    "positive_ctxs":[
      {
        "title":"Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments",
        "text":"We present a human-in-the-loop evaluation framework for fact-checking novel misinformation claims and identifying social media messages that support them. Our approach extracts check-worthy claims, which are aggregated and ranked for review. Stance classifiers are then used to identify tweets supporting novel misinformation claims, which are further reviewed to determine whether they violate relevant policies. To demonstrate the feasibility of our approach, we develop a baseline system based on modern NLP methods for human-in-the-loop fact-checking in the domain of COVID-19 treatments. We make our data 1 and detailed annotation guidelines available to support the evaluation of human-in-the-loop systems that identify novel misinformation directly from raw usergenerated content.",
        "id":254853990
      }
    ],
    "negative_ctxs":[
      {
        "title":"Knowledge-Based Labeling of Semantic Relationships in English",
        "text":"An increasing number of NLP tasks require semantic labels to be assigned, not only to entities that appear in textual elements, but to the relationships between those entities. Interest is growing in shallow semantic role labeling as well as in deep semantic distance metrics grounded in ontologies, as each of these contributes to better understanding and organization of text. In this work I apply knowledgebased techniques to identify and explore deep semantic relationships in several styles of English text: nominal compounds, full sentences in the domain of knowledge acquisition, and phrase-level labels for images in a collection. I also present work on a graphical tool for exploring the relationship between domain text and deep domain knowledge.",
        "id":5177350
      },
      {
        "title":"NEUPL: NEURAL POPULATION LEARNING",
        "text":"Learning in strategy games (e.g. StarCraft, poker) requires the discovery of diverse policies. This is often achieved by iteratively training new policies against existing ones, growing a policy population that is robust to exploit. This iterative approach suffers from two issues in real-world games: a) under finite budget, approximate best-response operators at each iteration needs truncating, resulting in under-trained good-responses populating the population; b) repeated learning of basic skills at each iteration is wasteful and becomes intractable in the presence of increasingly strong opponents. In this work, we propose Neural Population Learning (NeuPL) as a solution to both issues. NeuPL offers convergence guarantees to a population of best-responses under mild assumptions. By representing a population of policies within a single conditional model, NeuPL enables transfer learning across policies. Empirically, we show the generality, improved performance and efficiency of NeuPL across several test domains 1 . Most interestingly, we show that novel strategies become more accessible, not less, as the neural population expands.The need for learning not one, but a population of strategies is rooted in classical game theory. Consider the purely cyclical game of rock-paper-scissors, the performance of individual strategies is meaningless as improving against one entails losing to another. By contrast, performance can be meaningfully examined between populations. A population consisting of pure strategies {rock, paper} does well against a singleton population of {scissors} because in the meta-game where both populations are revealed, a player picking strategies from the former can always beat a player choosing from the latter 2 . This observation underpins the unifying population learning framework of Policy Space Response Oracle (PSRO) where a new policy is trained to best-respond to a mixture over previous policies at each iteration, following a meta-strategy solver(Lanctot et al., 2017). Most impressively, Vinyals et al. (2019) explored the strategy game of StarCraft with a league of policies, using a practical variation of PSRO. The league counted close to a thousand sophisticated deep RL agents as the population collectively became robust to exploits.Unfortunately, such empirical successes often come at considerable costs. Population learning algorithms with theoretical guarantees are traditionally studied in normal-form games(Brown, 1951;McMahan et al., 2003)where best-responses can be solved exactly. This is in stark contrast to real-world Game-of-Skills (Czarnecki et al., 2020) -such games are often temporal in nature, where best-responses can only be approximated with computationally intensive methods (e.g. deep RL). This has two implications. First, for a given opponent, one cannot efficiently tell apart good-responses that temporarily plateaued at local optima from globally optimal best-responses. As a result, approximate best-response operators are often truncated prematurely, according to hand-crafted schedules(Lanctot et al., 2017;Mcaleer et al., 2020). Second, real-world games often afford strategy-agnostic transitive",
        "id":246863450
      },
      {
        "title":"Competitive Grouping in Integrated Phrase Segmentation and Alignment Model",
        "text":"This article describes the competitive grouping algorithm at the core of our Integrated Segmentation and Alignment (ISA) model. ISA extracts phrase pairs from a bilingual corpus without requiring the precalculated word alignment as many other phrase alignment models do. Experiments conducted within the WPT-05 shared task on statistical machine translation demonstrate the simplicity and effectiveness of this approach.",
        "id":2455968
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper introduces the R-GCN technique into document-level joint entity and relation extraction?",
    "positive_ctxs":[
      {
        "title":"A Novel Table-to-Graph Generation Approach for Document-Level Joint Entity and Relation Extraction",
        "text":"Document-level relation extraction (DocRE) aims to extract relations among entities within a document, which is crucial for applications like knowledge graph construction. Existing methods usually assume that entities and their mentions are identified beforehand, which falls short of real-world applications. To overcome this limitation, we propose TAG, a novel tableto-graph generation model for joint extraction of entities and relations at document-level. To enhance the learning of task dependencies, TAG induces a latent graph among mentions, with different types of edges indicating different task information, which is further broadcast with a relational graph convolutional network. To alleviate the error propagation problem, we adapt the hierarchical agglomerative clustering algorithm to back-propagate task information at decoding stage. Experiments on the benchmark dataset, DocRED, demonstrate that TAG surpasses previous methods by a large margin and achieves state-of-the-art results 1 . an Equatorial Guinean politician and writer. … After his exile, he settled down in Valencia with his second wife and her family. Balboa Boneke died from renal problems, coupled with a three-year depression caused by the death of his wife, on 10 March 2014 in Valencia , Spain .",
        "id":259370653
      }
    ],
    "negative_ctxs":[
      {
        "title":"Example-based Machine Translation Based on Syntactic Transfer with Statistical Models",
        "text":"This paper presents example-based machine translation (MT) based on syntactic transfer, which selects the best translation by using models of statistical machine translation. Example-based MT sometimes generates invalid translations because it selects similar examples to the input sentence based only on source language similarity. The method proposed in this paper selects the best translation by using a language model and a translation model in the same manner as statistical MT, and it can improve MT quality over that of 'pure' example-based MT. A feature of this method is that the statistical models are applied after word re-ordering is achieved by syntactic transfer. This implies that MT quality is maintained even when we only apply a lexicon model as the translation model. In addition, translation speed is improved by bottom-up generation, which utilizes the tree structure that is output from the syntactic transfer.",
        "id":1827666
      },
      {
        "title":"An OWL Ontology for HPSG",
        "text":"The paper presents an OWL ontology for HPSG. The HPSG ontology is integrated with an existing OWL ontology, GOLD, as a community of practice extension. The basic ideas are illustrated by visualizations of type hierarchies for parts of speech.",
        "id":13980475
      },
      {
        "title":"Multilingual Automatic Extraction of Linguistic Data from Grammars",
        "text":"One of the goals of field linguistics is compilation of descriptive grammars for relatively little-studied languages. Until recently, extracting linguistic characteristics from grammatical descriptions and creating databases based on them was done manually. The aim of this paper is to apply methods of multilingual automatic information extraction to grammatical descriptions written in different languages of the world: we present a search engine for grammars, which would accelerate the tedious and time-consuming process of searching for information about linguistic features and facilitate research in the field of linguistic typology.",
        "id":259212823
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper investigates the influence of the diversity of source tasks on the performance of target tasks in prompt tuning using CrossFit?",
    "positive_ctxs":[
      {
        "title":"Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?",
        "text":"Prompt tuning (PT) which only tunes the embeddings of an additional sequence of tokens per task, keeping the pre-trained language model (PLM) frozen, has shown remarkable performance in few-shot learning. Despite this, PT has been shown to rely heavily on good initialization of the prompt embeddings. In this work, we study meta prompt tuning (MPT) to systematically explore how meta-learning can help improve (if it can) cross-task generalization in PT through learning to initialize the prompt embeddings from other relevant tasks. We empirically analyze a representative set of meta learning algorithms in a wide range of adaptation settings with different source\/target task configurations on a large set of few-shot tasks. With extensive experiments and analysis, we demonstrate the effectiveness of MPT. We find the improvement to be significant particularly on classification tasks. For other kinds of tasks such as question answering, we observe that while MPT can outperform PT in most cases, it does not always outperform multi-task learning. We further provide an in-depth analysis from the perspective of task similarity. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
        "id":256900985
      }
    ],
    "negative_ctxs":[
      {
        "title":"Mind the data gap(s): Investigating power in speech and language datasets",
        "text":"Algorithmic oppression is an urgent and persistent problem in speech and language technologies. Considering power relations embedded in datasets before compiling or using them to train or test speech and language technologies is essential to designing less harmful, more just technologies. This paper presents a reflective exercise to recognise and challenge gaps and the power relations they reveal in speech and language datasets by applying principles of Data Feminism and Design Justice, and building on work on dataset documentation and sociolinguistics.",
        "id":248780152
      },
      {
        "title":"A Speech and Gesture Spatial Corpus in Assisted Living",
        "text":"Ambient Assisted Living (AAL) is the name for a European technology and innovation funding programme. AAL research field is about intelligent assistant systems for a healthier and safer life in the preferred living environments through the use of Information and Communication Technologies (ICT). We focus specifically on speech and gesture interaction which can enhance the quality of lifestyle of people living in assistive environments, be they seniors or people with physical or cognitive disabilities. In this paper we describe our user study conducted in a lab at the University of Bremen in order to collect empirical speech and gesture data and later create and analyse a multimodal corpus. The user study is about a human user sitting in a wheelchair and performing certain inherently spatial tasks.",
        "id":7778807
      },
      {
        "title":"Hubness and Pollution: Delving into Cross-Space Mapping for Zero-Shot Learning",
        "text":"Zero-shot methods in language, vision and other domains rely on a cross-space mapping function that projects vectors from the relevant feature space (e.g., visualfeature-based image representations) to a large semantic word space (induced in an unsupervised way from corpus data), where the entities of interest (e.g., objects images depict) are labeled with the words associated to the nearest neighbours of the mapped vectors. Zero-shot cross-space mapping methods hold great promise as a way to scale up annotation tasks well beyond the labels in the training data (e.g., recognizing objects that were never seen in training). However, the current performance of cross-space mapping functions is still quite low, so that the strategy is not yet usable in practical applications. In this paper, we explore some general properties, both theoretical and empirical, of the cross-space mapping function, and we build on them to propose better methods to estimate it. In this way, we attain large improvements over the state of the art, both in cross-linguistic (word translation) and cross-modal (image labeling) zero-shot experiments.",
        "id":12187767
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper is among the earliest to train on extensive collection of signing video and subtitle pairs available from online platforms?",
    "positive_ctxs":[
      {
        "title":"Gloss-Free End-to-End Sign Language Translation",
        "text":"In this paper, we tackle the problem of sign language translation (SLT) without gloss annotations. Although intermediate representation like gloss has been proven effective, gloss annotations are hard to acquire, especially in large quantities. This limits the domain coverage of translation datasets, thus handicapping real-world applications. To mitigate this problem, we design the Gloss-Free End-to-end sign language translation framework (GloFE). Our method improves the performance of SLT in the gloss-free setting by exploiting the shared underlying semantics of signs and the corresponding spoken translation. Common concepts are extracted from the text and used as a weak form of intermediate representation. The global embedding of these concepts is used as a query for cross-attention to find the corresponding information within the learned visual features. In a contrastive manner, we encourage the similarity of query results between samples containing such concepts and decrease those that do not. We obtained state-of-the-art results on large-scale datasets, including OpenASL and How2Sign. 1",
        "id":258832883
      }
    ],
    "negative_ctxs":[
      {
        "title":"Simultaneous Identification of Biomedical Named-Entity and Functional Relations Using Statistical Parsing Techniques *",
        "text":"In this paper we propose a statistical parsing technique that simultaneously identifies biomedical named-entities (NEs) and extracts subcellular localization relations for bacterial proteins from the text in MEDLINE articles. We build a parser that derives both syntactic and domain-dependent semantic information and achieves an F-score of 48.4% for the relation extraction task. We then propose a semi-supervised approach that incorporates noisy automatically labeled data to improve the F-score of our parser to 83.2%. Our key contributions are: learning from noisy data, and building an annotated corpus that can benefit relation extraction research.",
        "id":9128091
      },
      {
        "title":"A Semi-supervised Multi-task Learning Approach to Classify Customer Contact Intents",
        "text":"In the area of customer support, understanding customers' intents is a crucial step. Machine learning plays a vital role in this type of intent classification. In reality, it is typical to collect confirmation from customer support representatives (CSRs) regarding the intent prediction, though it can unnecessarily incur prohibitive cost to ask CSRs to assign existing or new intents to the mis-classified cases. Apart from the confirmed cases with and without intent labels, there can be a number of cases with no human curation. This data composition (Positives + Unlabeled + multiclass Negatives) creates unique challenges for model development. In response to that, we propose a semi-supervised multi-task learning paradigm. In this manuscript, we share our experience in building text-based intent classification models for a customer support service on an E-commerce website. We improve the performance significantly by evolving the model from multiclass classification to semi-supervised multi-task learning by leveraging the negative cases, domain-and taskadaptively pretrained ALBERT on customer contact texts, and a number of un-curated data with no labels. In the evaluation, the final model boosts the average AUC ROC by almost 20 points compared to the baseline finetuned multiclass classification ALBERT model.",
        "id":235421748
      },
      {
        "title":"LARGE SCALE DISTRIBUTED NEURAL NETWORK TRAINING THROUGH ONLINE DISTILLATION",
        "text":"Techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model. However, due to increased testtime cost (for ensembles) and increased complexity of the training pipeline (for distillation), these techniques are challenging to use in industrial settings. In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup or many new hyperparameters. Our first claim is that online distillation enables us to use extra parallelism to fit very large datasets about twice as fast. Crucially, we can still speed up training even after we have already reached the point at which additional parallelism provides no benefit for synchronous or asynchronous stochastic gradient descent. Two neural networks trained on disjoint subsets of the data can share knowledge by encouraging each model to agree with the predictions the other model would have made. These predictions can come from a stale version of the other model so they can be safely computed using weights that only rarely get transmitted. Our second claim is that online distillation is a cost-effective way to make the exact predictions of a model dramatically more reproducible. We support our claims using experiments on the Criteo Display Ad Challenge dataset, ImageNet, and the largest to-date dataset used for neural language modeling, containing 6 × 10 11 tokens and based on the Common Crawl repository of web data.",
        "id":2331610
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_acl",
    "question":"Which paper is the first to comprehensively review the progress of deep learning in mathematical reasoning?",
    "positive_ctxs":[
      {
        "title":"A Survey of Deep Learning for Mathematical Reasoning",
        "text":"Mathematical reasoning is a fundamental aspect of human intelligence and is applicable in various fields, including science, engineering, finance, and everyday life. The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems in language has garnered significant interest in the fields of machine learning and natural language processing. For example, mathematics serves as a testbed for aspects of reasoning that are challenging for powerful deep learning models, driving new algorithmic and modeling advances. On the other hand, recent advances in large-scale neural language models have opened up new benchmarks and opportunities to use deep learning for mathematical reasoning. In this survey paper, we review the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade. We also evaluate existing benchmarks and methods, and discuss future research directions in this domain.",
        "id":254877175
      }
    ],
    "negative_ctxs":[
      {
        "title":"Joint Arc-factored Parsing of Syntactic and Semantic Dependencies",
        "text":"In this paper we introduce a joint arc-factored model for syntactic and semantic dependency parsing. The semantic role labeler predicts the full syntactic paths that connect predicates with their arguments. This process is framed as a linear assignment task, which allows to control some well-formedness constraints. For the syntactic part, we define a standard arc-factored dependency model that predicts the full syntactic tree. Finally, we employ dual decomposition techniques to produce consistent syntactic and predicate-argument structures while searching over a large space of syntactic configurations. In experiments on the CoNLL-2009 English benchmark we observe very competitive results.",
        "id":7225808
      },
      {
        "title":"",
        "text":"",
        "id":219308976
      },
      {
        "title":"Ultra-High Dimensional Sparse Representations with Binarization for Efficient Text Retrieval",
        "text":"The semantic matching capabilities of neural information retrieval can ameliorate synonymy and polysemy problems of symbolic approaches. However, neural models' dense representations are more suitable for re-ranking, due to their inefficiency. Sparse representations, either in symbolic or latent form, are more efficient with an inverted index. Taking the merits of the sparse and dense representations, we propose an ultra-high dimensional (UHD) representation scheme equipped with directly controllable sparsity. UHD's large capacity and minimal noise and interference among the dimensions allow for binarized representations, which are highly efficient for storage and search. Also proposed is a bucketing method, where the embeddings from multiple layers of BERT are selected\/merged to represent diverse linguistic aspects. We test our models with MS MARCO and TREC CAR, showing that our models outperforms other sparse models. * These authors contributed equally. † The corresponding author",
        "id":239009411
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_acl",
    "question":"Which paper makes sure that the questions used in the paper are all from real users that are genuinely curious about a specific topic or concept?",
    "positive_ctxs":[
      {
        "title":"CREPE: Open-Domain Question Answering with False Presuppositions",
        "text":"When asking about unfamiliar topics, information seeking users often pose questions with false presuppositions. Most existing question answering (QA) datasets, in contrast, assume all questions have well defined answers. We introduce CREPE, a QA dataset containing a natural distribution of presupposition failures from online information-seeking forums. We find that 25% of questions contain false presuppositions, and provide annotations for these presuppositions and their corrections. Through extensive baseline experiments, we show that adaptations of existing open-domain QA models can find presuppositions moderately well, but struggle when predicting whether a presupposition is factually correct. This is in large part due to difficulty in retrieving relevant evidence passages from a large text corpus. CREPE provides a benchmark to study question answering in the wild, and our analyses provide avenues for future work in better modeling and further studying the task. 1 et al. 2021. Webgpt: Browser-assisted questionanswering with human feedback. arXiv preprint.",
        "id":254096489
      }
    ],
    "negative_ctxs":[
      {
        "title":"How to disagree well: Investigating the dispute tactics used on Wikipedia",
        "text":"Disagreements are frequently studied from the perspective of either detecting toxicity or analysing argument structure. We propose a framework of dispute tactics which unifies these two perspectives, as well as other dialogue acts which play a role in resolving disputes, such as asking questions and providing clarification. This framework includes a preferential ordering among rebuttaltype tactics, ranging from ad hominem attacks to refuting the central argument. Using this framework, we annotate 213 disagreements (3,865 utterances) from Wikipedia Talk pages. This allows us to investigate research questions around the tactics used in disagreements; for instance, we provide empirical validation of the approach to disagreement recommended by Wikipedia. We develop models for multilabel prediction of dispute tactics in an utterance, achieving the best performance with a transformer-based label powerset model. Adding an auxiliary task to incorporate the ordering of rebuttal tactics further yields a statistically significant increase. Finally, we show that these annotations can be used to provide useful additional signals to improve performance on the task of predicting escalation.",
        "id":254823116
      },
      {
        "title":"All Words Domain Adapted WSD: Finding a Middle Ground between Supervision and Unsupervision",
        "text":"In spite of decades of research on word sense disambiguation (WSD), all-words general purpose WSD has remained a distant goal. Many supervised WSD systems have been built, but the effort of creating the training corpus -annotated sense marked corpora -has always been a matter of concern. Therefore, attempts have been made to develop unsupervised and knowledge based techniques for WSD which do not need sense marked corpora. However such approaches have not proved effective, since they typically do not better Wordnet first sense baseline accuracy. Our research reported here proposes to stick to the supervised approach, but with far less demand on annotation. We show that if we have ANY sense marked corpora, be it from mixed domain or a specific domain, a small amount of annotation in ANY other domain can deliver the goods almost as if exhaustive sense marking were available in that domain. We have tested our approach across Tourism and Health domain corpora, using also the well known mixed domain SemCor corpus. Accuracy figures close to self domain training lend credence to the viability of our approach. Our contribution thus lies in finding a convenient middle ground between pure supervised and pure unsupervised WSD. Finally, our approach is not restricted to any specific set of target words, a departure from a commonly observed practice in domain specific WSD.",
        "id":5951182
      },
      {
        "title":"In-Context Learning through the Bayesian Prism",
        "text":"In-context learning is one of the surprising and useful features of large language models. How it works is an active area of research. Recently, stylized meta-learning-like setups have been devised that train these models on a sequence of input-output pairs (x, f (x)) from a function class using the language modeling loss and observe generalization to unseen functions from the same class. One of the main discoveries in this line of research has been that for several problems such as linear regression, trained transformers learn algorithms for learning functions in context. However, the inductive biases of these models resulting in this behavior are not clearly understood. A model with unlimited training data and compute is a Bayesian predictor: it learns the pretraining distribution. It has been shown that high-capacity transformers mimic the Bayesian predictor for linear regression. In this paper, we show empirical evidence of transformers exhibiting the behavior of this ideal learner across different linear and non-linear function classes. We also extend the previous setups to work in the multitask setting and verify that transformers can do in-context learning in this setup as well and the Bayesian perspective sheds light on this setting also. Finally, via the example of learning Fourier series, we study the inductive bias for in-context learning. We find that in-context learning may or may not have simplicity bias depending on the pretraining data distribution. * Equal Contribution arXiv:2306.04891v1 [cs.LG] 8 Jun 2023 w ∼ N (0 d , I). We are particularly interested in the underdetermined region i.e. k < d. Gaussian prior enables explicit PME computation: both PME and maximum a posteriori (MAP) solution agree and are equal to the minimum L 2 -norm solution of the equations forming the training examples i.e. min w ∥w∥ 2 s.t. w T x i = f (x i ), ∀i ≤ k Standard Ordinary Least Squares (OLS) solvers return the minimum L 2 -norm solution, and thus PME and MAP too, in the underdetermined region i.e. k < d. Skewed-Covariance Regression (F Skew-DR ). This setup is similar to dense-regression, except we assume the following prior on weight vector: w ∼ N (0, Σ), where Σ ∈ R d×d is the covariance matrix with eigenvalues proportional to 1\/i 2 , where i ∈ [1, d]. For this prior on w, we can use the same (but more general) argument for dense regression above to obtain the PME and MAP which will be equal and can be obtained by minimizing w T Σ −1 w w.r.t to the constraints w T x i = f (x i ). This setup was motivated by Garg et al. [2022], where it was used to sample x i values for out-of-distribution (OOD) evaluation, but not as a prior on w. Sparse Regression (F SR ). In sparse regression, we assume w to be an s-sparse vector in R d i.e. out of its d components only s are non-zero. Following Garg et al. [2022], to sample w for constructing prompts P, we first sample w ∼ N (0 d , I) and then randomly set its d − s components as 0. We consider s = 3 throughout our experiments. While computing the PME appears to be intractable here, the MAP solution can be estimated using Lasso by assuming a Laplacian prior on w Tibshirani [1996].Sign-Vector Regression (F SVR ).Here, we assume w to be a sign vector in {−1, +1} d . For constructing prompts P, we sample d independent Bernoulli random variables b j with a mean of 0.5 and obtain w = [2b 1 − 1, · · · , 2b d − 1] T . While computing the exact PME in this case as well remains intractable, the optimal solution for k > d\/2 can be obtained by minimizing the L ∞ norm ∥w∥ ∞ w.r.t. the constraints specified by the input-output examples (w T x i = f (x i )) Mangasarian and Recht [2011]. A specific variation. In general, for the exact recovery of a vector w, the set of all these vectors must satisfy specific convexity conditions Chandrasekaran et al. [2012]. We question if Transformers also require such conditions. To test the same, we define a task F ZR where the convexity conditions are not met and train transformers for regression on this task. Here, w ∈ {z; z | z ∈ {−2, −1, 1, 2} d\/2 }, where ; denotes concatenation. Note that the size of this set is 2 d , the same as the size of {−1, 1} d .Low-Rank Regression (F LowRank-DR ).In this case, w is assumed to be a flattened version of a matrix W ∈ R q×q (d = q 2 ) with a rank r, where r ≪ q. A strong baseline, in this case, is to minimize the nuclear norm L * of W i.e. ∥W∥ * subject to constraints w T x i = f (x i ). To sample the rank-r matrix W, we sample A ∼ N (0, 1), s.t. A ∈ R q×r and independently a matrix B of the same shape and distribution, and set W = AB T . Recovery bounds. For each function class above, there is a bound on the minimum number of in-context examples needed for the exact recovery of the solution vector w. The bounds for sparse, sign-vector and low-rank regression are 2s log(d\/s) + 5s\/4, d\/2, and 3r(2q − r) respectively Chandrasekaran et al. [2012].",
        "id":259108565
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper measured how well the source-translation contribution by the translation model can be used to detect its own hallucinations?",
    "positive_ctxs":[
      {
        "title":"Detecting and Mitigating Hallucinations in Machine Translation: Model Internal Workings Alone Do Well, Sentence Similarity Even Better",
        "text":"While the problem of hallucinations in neural machine translation has long been recognized, so far the progress on its alleviation is very little. Indeed, recently it turned out that without artificially encouraging models to hallucinate, previously existing methods fall short and even the standard sequence log-probability is more informative. It means that internal characteristics of the model can give much more information than we expect, and before using external models and measures, we first need to ask: how far can we go if we use nothing but the translation model itself ? We propose to use a method that evaluates the percentage of the source contribution to a generated translation. Intuitively, hallucinations are translations \"detached\" from the source, hence they can be identified by low source contribution. This method improves detection accuracy for the most severe hallucinations by a factor of 2 and is able to alleviate hallucinations at test time on par with the previous best approach that relies on external models. Next, if we move away from internal model characteristics and allow external tools, we show that using sentence similarity from cross-lingual embeddings further improves these results. We release the code of our experiments. 1",
        "id":254823170
      }
    ],
    "negative_ctxs":[
      {
        "title":"A STATISTICAL APPROACH TO LANGUAGE TRANSLATION",
        "text":"An approach to automatic translation is outlined that utilizes technklues of statistical inl'ormatiml extraction from large data bases. The method is based on the availability of pairs of large corresponding texts that are translations of each other. In our case, the iexts are in English and French. Fundamental to the technique is a complex glossary of correspondence of fixed locutions. The steps of the proposed translation process are: (1) Partition the source text into a set of fixed locutioris. (2) Use the glossary plus coutextual information to select tim corresponding set of fixed Ioctttions into a sequen{e forming the target sentence. (3) Arrange the words of the talget fixed locutions into a sequence forming the target sentence. We have developed stalistical techniques facilitating both tile autonlatic creation of the glossary, and the performance of tile three translation steps, all on the basis of an aliglnncllt of corresponding sentences in tile two texts. While wc are not yet able to provide examples of French \/ English tcanslation, we present some encouraging intermediate results concerning glossary creation and the arrangement of target WOl'd seq lie)lees.",
        "id":5216540
      },
      {
        "title":"What Is Going through Your Mind? Metacognitive Events Classification in Human-Agent Interactions",
        "text":"For an agent, either human or artificial, to show intelligent interactive behaviour implies assessments of the reliability of own and others' thoughts, feelings and beliefs. Agents capable of these robust evaluations are able to adequately interpret their own and others' cognitive and emotional processes, anticipate future actions, and improve their decision-making and interactive performances across domains and contexts. Reliable instruments to assess interlocutors' mindful capacities for monitoring and regulation -metacognition -in human-agent interaction in real-time and continuously are of crucial importance however challenging to design. The presented study reports Concurrent Think Aloud (CTA) experiments in order to access and evaluate metacognitive dispositions and attitudes of participants in human-agent interactions. A typology of metacognitive events related to the 'verbalized' monitoring, interpretation, reflection and regulation activities observed in a multimodal dialogue has been designed, and serves as a valid tool to identify relation between participants' behaviour analysed in terms of ISO 24617-2 compliant dialogue acts and the corresponding metacognitive indicators.",
        "id":252624520
      },
      {
        "title":"A novel Textual Encoding paradigm based on Semantic Web tools and semantics",
        "text":"In this paper we perform a preliminary evaluation on how Semantic Web technologies such as RDF and OWL can be used to perform textual encoding. Among the potential advantages, we notice how RDF, given its conceptual graph structure, appears naturally suited to deal with overlapping hierarchies of annotations, something notoriously problematic using classic XML based markup. To conclude, we show how complex querying can be performed using slight modifications of already existing Semantic Web query tools.",
        "id":167401
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper presents an easy to implement and high performing method for OOD detection with language models?",
    "positive_ctxs":[
      {
        "title":"Is Fine-tuning Needed? Pre-trained Language Models Are Near Perfect for Out-of-Domain Detection",
        "text":"Out-of-distribution (OOD) detection is a critical task for reliable predictions over text. Finetuning with pre-trained language models has been a de facto procedure to derive OOD detectors with respect to in-distribution (ID) data. Despite its common use, the understanding of the role of fine-tuning and its necessity for OOD detection is largely unexplored. In this paper, we raise the question: is fine-tuning necessary for OOD detection? We present a study investigating the efficacy of directly leveraging pre-trained language models for OOD detection, without any model fine-tuning on the ID data. We compare the approach with several competitive fine-tuning objectives, and offer new insights under various types of distributional shifts. Extensive evaluations on 8 diverse ID-OOD dataset pairs demonstrate nearperfect OOD detection performance (with 0% FPR95 in many cases), strongly outperforming its fine-tuned counterparts. We show that using distance-based detection methods, pretrained language models are near-perfect OOD detectors when the distribution shift involves a domain change. Furthermore, we study the effect of fine-tuning on OOD detection and identify how to balance ID accuracy with OOD detection performance. Our code is publically available 1 .",
        "id":258832820
      }
    ],
    "negative_ctxs":[
      {
        "title":"Phrase2VecGLM: Neural generalized language model-based semantic tagging for complex query reformulation in medical IR",
        "text":"In fact-based information retrieval, stateof-the-art performance is traditionally achieved by knowledge graphs driven by knowledge bases, as they can represent facts about and capture relationships between entities very well. However, in domains such as medical information retrieval, where addressing specific information needs of complex queries may require understanding query intent by capturing novel associations between potentially latent concepts, these systems can fall short. In this work, we develop a novel, completely unsupervised, neural language model-based ranking approach for semantic tagging of documents, using the document to be tagged as a query into the model to retrieve candidate phrases from top-ranked related documents, thus associating every document with novel related concepts extracted from the text. For this we extend the word embeddingbased generalized language model (GLM) due to(Ganguly et al., 2015), to employ phrasal embeddings, and use the semantic tags thus obtained for downstream query expansion, both directly and in feedback loop settings. Our method, evaluated using the TREC 2016 clinical decision support challenge dataset, shows statistically significant improvement not only over various baselines that use standard MeSH terms and UMLS concepts for query expansion, but also over baselines using human expert-assigned concept tags for the queries, on top of a standard Okapi BM25-based document retrieval system.",
        "id":51869141
      },
      {
        "title":"An English-Chinese Cross-lingual Word Semantic Similarity Measure Exploring Attributes and Relations",
        "text":"Word semantic similarity measuring is a fundamental issue to many NLP applications and the globalization has made an urgent request for cross-lingual word similarity measure. This paper proposed a word semantic similarity measure which is able to work in cross-lingual scenarios. Basically, a concept can be defined by a set of attributes. The basic idea of this work is to compute the similarity between words by exploring their attributes and relations. For a given word pair, we first compute similarities between their attributes by combining distance, depth and relation information. Then word similarity are computed through a combination scheme. The algorithm is implemented based on an English-Chinese bilingual ontology HowNet. Experiments show that the proposed algorithm results in high correlation against human judgments, which encourages its broad application in cross-lingual applications.",
        "id":11136127
      },
      {
        "title":"Universal Decompositional Semantics on Universal Dependencies",
        "text":"We present a framework for augmenting data sets from the Universal Dependencies project with Universal Decompositional Semantics. Where the Universal Dependencies project aims to provide a syntactic annotation standard that can be used consistently across many languages as well as a collection of corpora that use that standard, our extension has similar aims for semantic annotation. We describe results from annotating the English Universal Dependencies treebank, dealing with word senses, semantic roles, and event properties.",
        "id":11796795
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper produces a dataset for text simplification in over 12 languages and evaluates both finetuning and in context learning approaches to text simplification in those languages?",
    "positive_ctxs":[
      {
        "title":"Revisiting non-English Text Simplification: A Unified Multilingual Benchmark",
        "text":"Recent advancements in high-quality, largescale English resources have pushed the frontier of English Automatic Text Simplification (ATS) research. However, less work has been done on multilingual text simplification due to the lack of a diverse evaluation benchmark that covers complex-simple sentence pairs in many languages. This paper introduces the MULTI-SIM benchmark, a collection of 27 resources in 12 distinct languages containing over 1.7 million complex-simple sentence pairs. This benchmark will encourage research in developing more effective multilingual text simplification models and evaluation metrics. Our experiments using MULTISIM with pre-trained multilingual language models reveal exciting performance improvements from multilingual training in non-English settings. We observe strong performance from Russian in zero-shot crosslingual transfer to low-resource languages. We further show that few-shot prompting with BLOOM-176b achieves comparable quality to reference simplifications outperforming finetuned models in most languages. We validate these findings through human evaluation. 1",
        "id":258887622
      }
    ],
    "negative_ctxs":[
      {
        "title":"Flexible Speech Act Based Dialogue Management",
        "text":"We present an application independent dialogue engine that reasons on application dependent knowledge sources to calculate predictions about how a dialogue might continue. Predictions are language independent and are translated into language dependent structures for recognition and synthesis. Further, we discuss how the predictions account for different kinds of dialogue, e.g., question-answer or mixed initiative.",
        "id":2072555
      },
      {
        "title":"nlpUP at SemEval-2019 Task 6: A Deep Neural Language Model for Offensive Language Detection",
        "text":"This paper presents our submission for the SemEval shared task 6, sub-task A on the identification of offensive language. Our proposed model, C-BiGRU, combines a Convolutional Neural Network (CNN) with a bidirectional Recurrent Neural Network (RNN). We utilize word2vec to capture the semantic similarities between words. This composition allows us to extract long term dependencies in tweets and distinguish between offensive and non-offensive tweets. In addition, we evaluate our approach on a different dataset and show that our model is capable of detecting online aggressiveness in both English and German tweets. Our model achieved a macro F1-score of 79.40% on the SemEval dataset.",
        "id":184483257
      },
      {
        "title":"Part-of-speech Tagging of Code-Mixed Social Media Text",
        "text":"A common step in the processing of any text is the part-of-speech tagging of the input text. In this paper, we present an approach to tackle code-mixed text from three different languages Bengali, Hindi, and Tamilapart from English. Our system uses Conditional Random Field, a sequence learning method, which is useful to capture patterns of sequences containing code switching to tag each word with accurate part-of-speech information. We have used various pre-processing and post-processing modules to improve the performance of our system. The results were satisfactory, with a highest of 75.22% accuracy in Bengali-English mixed data. The methodology that we employed in the task can be used for any resource poor language. We adapted standard learning approaches that work well with scarce data. We have also ensured that the system is portable to different platforms and languages and can be deployed for real-time analysis.",
        "id":5471567
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper proposed a learning-based data augmentation method for improving compositional generalization of language models?",
    "positive_ctxs":[
      {
        "title":"Learning to Substitute Spans towards Improving Compositional Generalization",
        "text":"Despite the rising prevalence of neural sequence models, recent empirical evidences suggest their deficiency in compositional generalization. One of the current de-facto solutions to this problem is compositional data augmentation, aiming to incur additional compositional inductive bias. Nonetheless, the improvement offered by existing handcrafted augmentation strategies is limited when successful systematic generalization of neural sequence models requires multi-grained compositional bias (i.e., not limited to either lexical or structural biases only) or differentiation of training sequences in an imbalanced difficulty distribution. To address the two challenges, we first propose a novel compositional augmentation strategy dubbed Span Substitution (SpanSub) that enables multi-grained composition of substantial substructures in the whole training set. Over and above that, we introduce the Learning to Substitute Span (L2S2) framework which empowers the learning of span substitution probabilities in SpanSub in an end-to-end manner by maximizing the loss of neural sequence models, so as to outweigh those challenging compositions with elusive concepts and novel surroundings. Our empirical results on three standard compositional generalization benchmarks, including SCAN, COGS and GeoQuery (with an improvement of at most 66.5%, 10.3%, 1.2%, respectively), demonstrate the superiority of SpanSub, L2S2 and their combination. * Corresponding authors N: Emma V: saw N: a cat N: A hedgehog V: met N: Paula V: saw N: a cat N: Paula A hedgehog met Paula .Emma saw a cat . Paula saw a cat .",
        "id":259076194
      }
    ],
    "negative_ctxs":[
      {
        "title":"Towards a Proper Treatment of Adjuncts in Japanese",
        "text":"In this paper we will discuss interpretation of adverbs in Japanese. We will explore the division of labor between the syntactic requirements, semantic requirements, and discourse-contextual constraints involving adverbial interpretation. It will then be argued that this inter-modular approach utilizing LFG explains various elusive paradigms of the adverbs.",
        "id":36175639
      },
      {
        "title":"Complexity of finding the BLEU-optimal hypothesis in a confusion network",
        "text":"Confusion networks are a simple representation of multiple speech recognition or translation hypotheses in a machine translation system. A typical operation on a confusion network is to find the path which minimizes or maximizes a certain evaluation metric. In this article, we show that this problem is generally NP-hard for the popular BLEU metric, as well as for smaller variants of BLEU. This also holds for more complex representations like generic word graphs. In addition, we give an efficient polynomial-time algorithm to calculate unigram BLEU on confusion networks, but show that even small generalizations of this data structure render the problem to be NP-hard again.Since finding the optimal solution is thus not always feasible, we introduce an approximating algorithm based on a multi-stack decoder, which finds a (not necessarily optimal) solution for n-gram BLEU in polynomial time.",
        "id":648535
      },
      {
        "title":"",
        "text":"",
        "id":29452235
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper proposed decomposing the logit update of each of the attention blocks’ inputs to analyze how the context influences the prediction?",
    "positive_ctxs":[
      {
        "title":"Explaining How Transformers Use Context to Build Predictions",
        "text":"Language Generation Models produce words based on the previous context. Although existing methods offer input attributions as explanations for a model's prediction, it is still unclear how prior words affect the model's decision throughout the layers. In this work, we leverage recent advances in explainability of the Transformer and present a procedure to analyze models for language generation. Using contrastive examples, we compare the alignment of our explanations with evidence of the linguistic phenomena, and show that our method consistently aligns better than gradient-based and perturbation-based baselines. Then, we investigate the role of MLPs inside the Transformer and show that they learn features that help the model predict words that are grammatically acceptable. Lastly, we apply our method to Neural Machine Translation models, and demonstrate that they generate human-like source-target alignments for building predictions",
        "id":258832652
      }
    ],
    "negative_ctxs":[
      {
        "title":"Detecting Suicidality with a Contextual Graph Neural Network",
        "text":"Discovering individuals' suicidality on social media has become increasingly important. Many researchers have studied to detect suicidality by using a suicide dictionary. However, while prior work focused on matching a word in a post with a suicide dictionary without considering contexts, little attention has been paid to how the word can be associated with the suicide-related context. To address this problem, we propose a suicidality detection model based on a graph neural network to grasp the dynamic semantic information of the suicide vocabulary by learning the relations between a given post and words. The extensive evaluation demonstrates that the proposed model achieves higher performance than the state-of-the-art methods. We believe the proposed model has great utility in identifying the suicidality of individuals and hence preventing individuals from potential suicide risks at an early stage. * Corresponding author. 1 https:\/\/data.oecd.org\/healthstat\/ suicide-rates.htm … I have my hair cut …Suicide DictionaryIndicator Risk (1) Behavior Risk(3)",
        "id":250390685
      },
      {
        "title":"Consistent storage of metadata in inference lexica: the MetaLex approach",
        "text":"With MetaLex we introduce a framework for metadata management where information can be inferred from different areas of metadata coding, such as metadata for catalogue descriptions, linguistic levels, or tiers. This is done for consistency and efficiency in metadata recording and applies the same inference techniques that are used for lexical inference. For this purpose we motivate the need for metadata descriptions on all document levels, describe the different structures of metadata, use existing metadata recommendations on different levels of annotations, and show a usecase of metadata inference.",
        "id":17503844
      },
      {
        "title":"No Permanent Friends or Enemies: Tracking Relationships between Nations from News",
        "text":"Understanding the dynamics of international politics is important yet challenging for civilians. In this work, we explore unsupervised neural models to infer relations between nations from news articles. We extend existing models by incorporating shallow linguistics information and propose a new automatic evaluation metric that aligns relationship dynamics with manually annotated key events. As understanding international relations requires carefully analyzing complex relationships, we conduct in-person human evaluations with three groups of participants. Overall, humans prefer the outputs of our model and give insightful feedback that suggests future directions for human-centered models. Furthermore, our model reveals interesting regional differences in news coverage. For instance, with respect to US-China relations, Singaporean media focus more on \"strengthening\" and \"purchasing\", while US media focus more on \"criticizing\" and \"denouncing\".",
        "id":125952286
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper proposed dictionary-based Bayesian inference to improve the performance of image text matching model?",
    "positive_ctxs":[
      {
        "title":"Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information",
        "text":"Visual Word Sense Disambiguation (VWSD) is a task to find the image that most accurately depicts the correct sense of the target word for the given context. Previously, image-text matching models often suffered from recognizing polysemous words. This paper introduces an unsupervised VWSD approach that uses gloss information of an external lexical knowledge-base, especially the sense definitions. Specifically, we suggest employing Bayesian inference to incorporate the sense definitions when sense information of the answer is not provided. In addition, to ameliorate the out-of-vocabulary (OOV) issue, we propose a context-aware definition generation with GPT-3. Experimental results show that VWSD performance increased significantly with our Bayesian inference-based approach. In addition, our context-aware definition generation achieved prominent performance improvement in OOV examples exhibiting better performance than the existing definition generation method.",
        "id":258461036
      }
    ],
    "negative_ctxs":[
      {
        "title":"Modeling Tweet Arrival Times using Log-Gaussian Cox Processes",
        "text":"Research on modeling time series text corpora has typically focused on predicting what text will come next, but less well studied is predicting when the next text event will occur. In this paper we address the latter case, framed as modeling continuous inter-arrival times under a log-Gaussian Cox process, a form of inhomogeneous Poisson process which captures the varying rate at which the tweets arrive over time. In an application to rumour modeling of tweets surrounding the 2014 Ferguson riots, we show how interarrival times between tweets can be accurately predicted, and that incorporating textual features further improves predictions.",
        "id":14606708
      },
      {
        "title":"A Critique and Improvement of an Evaluation Metric for Text Segmentation",
        "text":"The P k evaluation metric, initially proposed byBeeferman, Berger, and Lafferty (1997), is becoming the standard measure for assessing text segmentation algorithms. However, a theoretical analysis of the metric finds several problems: the metric penalizes false negatives more heavily than false positives, overpenalizes near misses, and is affected by variation in segment size distribution. We propose a simple modification to the P k metric that remedies these problems. This new metric-called WindowDiff-moves a fixed-sized window across the text and penalizes the algorithm whenever the number of boundaries within the window does not match the true number of boundaries for that window of text.",
        "id":6048999
      },
      {
        "title":"When CORDIAL Becomes Friendly: Endowing the CORDIAL Corpus with a Syntactic Annotation Layer",
        "text":"This paper reports on the syntactic annotation of a previously compiled and tagged corpus of European Portuguese (EP) dialects -The Syntax-oriented Corpus of Portuguese Dialects (CORDIAL-SIN). The parsed version of CORDIAL-SIN is intended to be a more efficient resource for the purpose of studying dialect syntax by allowing automated searches for various syntactic constructions of interest. To achieve this goal we adopted a rich annotation system (the UPenn corpora annotation system) which codifies syntactic information of high relevance. The annotation produces tree representations, in form of labelled parenthesis, that are integrally searchable with CorpusSearch, a search engine for parsed corpora (Randall, 2005(Randall,  -2007. The present paper focuses on CORDIAL-SIN annotation issues, namely it presents the general principles and guidelines of the adopted annotation system and describes the methodology for constructing the parsed version of the corpus and for searching it (tools and procedures). Last section addresses the question of how an annotation system originally designed for Middle English can be adapted to meet the particular needs of a Portuguese corpus of dialectal speech.",
        "id":14757570
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper proposed the integration of human translators' considerations, such as length control, rhyme type control and suggestion, and enhancing compatibility between translation output and unseen melodies, into the design of machine translation models when translating lyrics?",
    "positive_ctxs":[
      {
        "title":"Songs Across Borders: Singable and Controllable Neural Lyric Translation",
        "text":"The development of general-domain neural machine translation (NMT) methods has advanced significantly in recent years, but the lack of naturalness and musical constraints in the outputs makes them unable to produce singable lyric translations. This paper bridges the singability quality gap by formalizing lyric translation into a constrained translation problem, converting theoretical guidance and practical techniques from translatology literature to promptdriven NMT approaches, exploring better adaptation methods, and instantiating them to an English-Chinese lyric translation system. Our model achieves 99.85%, 99.00%, and 95.52% on length accuracy, rhyme accuracy, and word boundary recall. In our subjective evaluation, our model shows a 75% relative enhancement on overall quality, compared against naive finetuning 1 .",
        "id":258947268
      }
    ],
    "negative_ctxs":[
      {
        "title":"Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning",
        "text":"Fine-tuning of pre-trained transformer models has become the standard approach for solving common NLP tasks(Devlin et al., 2019). Most of the existing approaches rely on a randomly initialized classifier on top of such networks. We argue that this fine-tuning procedure is sub-optimal as the pre-trained model has no prior on the specific classifier labels, while it might have already learned an intrinsic textual representation of the task. In this paper, we introduce a new scoring method that casts a plausibility ranking task in a full-text format and leverages the masked language modeling head tuned during the pre-training phase. We study commonsense reasoning tasks where the model must rank a set of hypotheses given a premise, focusing on the COPA (Gordon et al., 2012), Swag (Zellers et al., 2018), HellaSwag (Zellers et al., 2019) and CommonsenseQA (Talmor et al., 2019) datasets.By exploiting our scoring method without fine-tuning, we are able to produce strong baselines (e.g. 80% test accuracy on COPA) that are comparable to supervised approaches. Moreover, when fine-tuning directly on the proposed scoring function, we show that our method provides a much more stable training phase across random restarts (e.g ×10 standard deviation reduction on COPA test accuracy) and requires less annotated data than the standard classifier approach to reach equivalent performances.",
        "id":216641845
      },
      {
        "title":"Automatic Selection of Reference Pages in Wikipedia for Improving Targeted Entities Disambiguation",
        "text":"In Targeted Entity Disambiguation setting, we take (i) a set of entity names which belong to the same domain (target entities), (ii) candidate mentions of the given entities which are texts that contain the target entities as input, and then determine which ones are true mentions of \"target entity\". For example, given the names of IT companies, including Apple, we determine Apple in a mention denotes an IT company or not. Prior work proposed a graph based model. This model ranks all candidate mentions based on scores which denote the degree of relevancy to target entities. Furthermore, this graph based model could utilize reference pages of target entities. However, human annotators must select reference pages in advance. We propose an automatic method that can select reference pages. We formalize the selection problem of reference pages as an Integer Linear Programming problem. We show that our model works as well as the prior work that manually selected reference pages.",
        "id":5708734
      },
      {
        "title":"",
        "text":"",
        "id":220058123
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper proposes a memory-efficient optimizer considering the confidence of each update during the optimization?",
    "positive_ctxs":[
      {
        "title":"CAME: Confidence-guided Adaptive Memory Efficient Optimization",
        "text":"Adaptive gradient methods, such as Adam and LAMB, have demonstrated excellent performance in the training of large language models. Nevertheless, the need for adaptivity requires maintaining second-moment estimates of the per-parameter gradients, which entails a high cost of extra memory overheads. To solve this problem, several memory-efficient optimizers (e.g., Adafactor) have been proposed to obtain a drastic reduction in auxiliary memory usage, but with a performance penalty. In this paper, we first study a confidence-guided strategy to reduce the instability of existing memory efficient optimizers. Based on this strategy, we propose CAME to simultaneously achieve two goals: fast convergence as in traditional adaptive methods, and low memory usage as in memory-efficient methods. Extensive experiments demonstrate the training stability and superior performance of CAME across various NLP tasks such as BERT and GPT-2 training. Notably, for BERT pre-training on the large batch size of 32,768, our proposed optimizer attains faster convergence and higher accuracy compared with the Adam optimizer. The implementation of CAME is publicly available 1 .",
        "id":259342823
      }
    ],
    "negative_ctxs":[
      {
        "title":"AN EFFICIENT SYNTACTIC TAGGING TOOL FOR CORPORA @ A BSTRA CT",
        "text":"The tree bank is an important resources tbr MT and linguistics researches, but it requires that large number of sentences be annotated with syntactic information. It is time consuming and troublesome, and dil'ficult to keep consistency, if' annotation is done manually. In this paper, wc presented a new technique for the semi-automatic tagging of Chinese tcxt. The system takes as input Chinese text, and outputs the syntactically tagged sentence(dependency tree). We use dependency grammar and employ a stack based shift\/reduce context-dependent parser as the tagging mechanism. The system works in human-machine cooperative way, in which the machine can acquire tagging rules from human intervention. The automation level can be improved step by step by accumulating rules during annotation. In addition, good consistency of tagging is guaranteed.",
        "id":5365055
      },
      {
        "title":"Annotating COMPARA, a Grammar-aware Parallel Corpus",
        "text":"In this paper we describe the annotation of COMPARA, currently the largest post-edited parallel corpora which includes Portuguese. We describe the motivation, the results so far, and the way the corpus is being annotated. We also provide the first grounded results about syntactical ambiguity in Portuguese. Finally, we discuss some interesting problems in this connection.",
        "id":3012359
      },
      {
        "title":"Automatie Extraction of Stochastic Lexicalized Tree Grammars from Treebanks",
        "text":"We present a method for the extraction of stochastic lexicalized tree grammars (S-LTG) of different complexities from existing treebanks, which allows us to analyze the relationship of a grammar automatically induced from a treebank wrt. its size, its complexity, and its predictive power on unseen data.Processing of different S-LTG is performed by a stochastic version of the two-stepEarly-based parsing strategy introduced in (Schabes and Joshi, 1991).",
        "id":393568
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper proposes the two-stage training method, i.e., task-specific fine-tuning and cross-domain pre-training, to train an open-domain dialogue evaluator using the self-collected dataset.",
    "positive_ctxs":[
      {
        "title":"RADE: Reference-Assisted Dialogue Evaluation for Open-Domain Dialogue",
        "text":"Evaluating open-domain dialogue systems is challenging for reasons such as the one-tomany problem, i.e., many appropriate responses other than just the golden response. As of now, automatic evaluation methods need better consistency with humans, while reliable human evaluation can be time-and cost-intensive. To this end, we propose the Reference-Assisted Dialogue Evaluation (RADE) approach under the multi-task learning framework, which leverages the pre-created utterance as reference other than the gold response to relief the one-tomany problem. Specifically, RADE explicitly compares reference and the candidate response to predict their overall scores. Moreover, an auxiliary response generation task enhances prediction via a shared encoder. To support RADE, we extend three datasets with additional rated responses other than just a golden response by human annotation. Experiments on our three datasets and two existing benchmarks demonstrate the effectiveness of our method, where Pearson, Spearman, and Kendall correlations with human evaluation outperform stateof-the-art baselines.Satanjeev Banerjee and Alon Lavie. 2005. METEOR:An automatic metric for MT evaluation with improved correlation with human judgments. In ACL. . 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.",
        "id":259370539
      }
    ],
    "negative_ctxs":[
      {
        "title":"The development of a morphosyntactic tagset for Afrikaans and its use with statistical tagging",
        "text":"In this paper, we present a morphosyntactic tagset for Afrikaans based on the guidelines developed by the Expert Advisory Group on Language Engineering Standards (EAGLES). We compare our slim yet expressive tagset, MAATS (Morphosyntactic AfrikAans TagSet), with an existing one which primarily focuses on a detailed morphosyntactic and semantic description of word forms. MAATS will primarily be used for the extraction of lexical data from large pos-tagged corpora. We not only focus on morphosyntactic properties but also on the processability with statistical tagging. We discuss the tagset design and motivate our classification of Afrikaans word forms, in particular we focus on the categorization of verbs and conjunctions. The complete tagset in presented and we briefly discuss each word class. In a case study with an Afrikaans newspaper corpus, we evaluate our tagset with four different statistical taggers. Despite a relatively small amount of training data, however with a large tagger lexicon, TnT-Tagger scores 97.05 % accuracy. Additionally, we present some error sources and discuss future work.",
        "id":18559676
      },
      {
        "title":"Double Graph Based Reasoning for Document-level Relation Extraction",
        "text":"Document-level relation extraction aims to extract relations among entities within a document. Different from sentence-level relation extraction, it requires reasoning over multiple sentences across a document. In this paper, we propose Graph Aggregation-and-Inference Network (GAIN) featuring double graphs. GAIN first constructs a heterogeneous mention-level graph (hMG) to model complex interaction among different mentions across the document. It also constructs an entitylevel graph (EG), based on which we propose a novel path reasoning mechanism to infer relations between entities. Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous state-of-the-art. Our code is available at https:\/\/github.com\/ DreamInvoker\/GAIN. * Equal contribution. † Corresponding author. Elias Brown [1] Elias Brown (May 9, 1793-July 7, 1857) was a U.S. Representative from Maryland. [2] Born near Baltimore, Maryland, Brown attended the common schools. … [7] He died near Baltimore, Maryland, and is interred in a private cemetery near Eldersburg, Maryland.",
        "id":221996144
      },
      {
        "title":"",
        "text":"",
        "id":227230343
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper proposes to use rewriting based approaches to defending against adversarial attacks in text classification?",
    "positive_ctxs":[
      {
        "title":"Don't Retrain, Just Rewrite: Countering Adversarial Perturbations by Rewriting Text",
        "text":"Can language models transform inputs to protect text classifiers against adversarial attacks? In this work, we present ATINTER, a model that intercepts and learns to rewrite adversarial inputs to make them non-adversarial for a downstream text classifier. Our experiments on four datasets and five attack mechanisms reveal that ATINTER is effective at providing better adversarial robustness than existing defense approaches, without compromising task accuracy. For example, on sentiment classification using the SST-2 dataset, our method improves the adversarial accuracy over the best existing defense approach by more than 4% with a smaller decrease in task accuracy (0.5 % vs. 2.5%). Moreover, we show that ATINTER generalizes across multiple downstream tasks and classifiers without having to explicitly retrain it for those settings. For example, we find that when ATINTER is trained to remove adversarial perturbations for the sentiment classification task on the SST-2 dataset, it even transfers to a semantically different task of news classification (on AGNews) and improves the adversarial robustness by more than 10%.",
        "id":258947664
      }
    ],
    "negative_ctxs":[
      {
        "title":"Learning to Ask Like a Physician",
        "text":"Existing question answering (QA) datasets derived from electronic health records (EHR) are artificially generated and consequently fail to capture realistic physician information needs. We present Discharge Summary Clinical Questions (DiSCQ), a newly curated question dataset composed of 2,000+ questions paired with the snippets of text (triggers) that prompted each question. The questions are generated by medical experts from 100+ MIMIC-III discharge summaries. We analyze this dataset to characterize the types of information sought by medical experts. We also train baseline models for trigger detection and question generation (QG), paired with unsupervised answer retrieval over EHRs. Our baseline model is able to generate high quality questions in over 62% of cases when prompted with human selected triggers. We release this dataset (and all code to reproduce baseline model results) to facilitate further research into realistic clinical QA and QG. 1 *",
        "id":249394777
      },
      {
        "title":"Speech Intelligibility and the Production of Fricative and Affricate among Mandarin-speaking Children with Cerebral Palsy",
        "text":"Literatures pertaining to English and Mandarin fricative\/affricate productions by adults with cerebral palsy (CP) showed that acoustic measurements such as rise time contrast, initial burst rate contrast and friction noise duration contrast associated with fricative\/affricate productions were highly correlated with overall speech intelligibility. However, the phonetic features of fricatives\/affricates produced by Mandarin-learning children with CP were not fully explored.Therefore, this study targets on fricatives\/affricates produced by ten Mandarin-learning CP children (Mean: 6;10, Range: 4;6 -8;11) and ten Mandarin-learning typically developing children (Mean: 5;7, Range: 5;2 -6;1). The current results from a speech repetition task showed that: 1) The fricative\/affricate accurate rates and error patterns were similar between the two The 2016 Conference on Computational Linguistics and Speech Processing ROCLING 2016, pp. 153-163  The Association for Computational Linguistics and Chinese Language Processing 153 groups;2) The differences between the two groups in terms of nine acoustic measurements (fricative\/affricate rise time, initial burst rate, friction noise duration and their contrasts) and speech intelligibility were not statistically significant; 3) The rise time contrast was an effective contributor to overall speech intelligibility for CP children. Together with previous studies, the current study concluded that rise time contrast was the most significant contributor, among fricative\/affricate measurements, to speech intelligibility across different age ranges.",
        "id":17631485
      },
      {
        "title":"Key Female Characters in Film Have More to Talk About Besides Men: Automating the Bechdel Test",
        "text":"The Bechdel test is a sequence of three questions designed to assess the presence of women in movies. Many believe that because women are seldom represented in film as strong leaders and thinkers, viewers associate weaker stereotypes with women. In this paper, we present a computational approach to automate the task of finding whether a movie passes or fails the Bechdel test. This allows us to study the key differences in language use and in the importance of roles of women in movies that pass the test versus the movies that fail the test. Our experiments confirm that in movies that fail the test, women are in fact portrayed as less-central and less-important characters.",
        "id":7280935
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper showed that social relationships were helpful for identifying inappropriate messages?",
    "positive_ctxs":[
      {
        "title":"Your spouse needs professional help: Determining the Contextual Appropriateness of Messages through Modeling Social Relationships",
        "text":"Understanding interpersonal communication requires, in part, understanding the social context and norms in which a message is said. However, current methods for identifying offensive content in such communication largely operate independent of context, with only a few approaches considering community norms or prior conversation as context. Here, we introduce a new approach to identifying inappropriate communication by explicitly modeling the social relationship between the individuals. We introduce a new dataset of contextually-situated judgments of appropriateness and show that large language models can readily incorporate relationship information to accurately identify appropriateness in a given context. Using data from online conversations and movie dialogues, we provide insight into how the relationships themselves function as implicit norms and quantify the degree to which context-sensitivity is needed in different conversation settings. Further, we also demonstrate that contextualappropriateness judgments are predictive of other social factors expressed in language such as condescension and politeness.",
        "id":259360414
      }
    ],
    "negative_ctxs":[
      {
        "title":"Thin Parsing: A Balance between Wide Scale Parsing and Chunking",
        "text":"This work presents a type of parser that takes the process of chunking to the stage of producing full parse trees. This type of parser, denoted Thin Parsers (TP) in this work has the characteristics of: following a given grammar, creating full parse trees, producing only a limited number of full parse trees, parsing in linear time of sentence length. Performance standards on the Penn Tree Bank show results slightly under that of stochastic parsers but faster performance. Various types of Thin Parsers are presented.",
        "id":14460763
      },
      {
        "title":"The Development of Dutch and Afrikaans Language Resources for Compound Boundary Analysis",
        "text":"In most languages, new words can be created through the process of compounding, which combines two or more words into a new lexical unit. Whereas in languages such as English the components that make up a compound are separated by a space, in languages such as Finnish, German, Afrikaans and Dutch these components are concatenated into one word. Compounding is very productive and leads to practical problems in developing machine translators and spelling checkers, as newly formed compounds cannot be found in existing lexicons. The Automatic Compound Processing (AuCoPro) project deals with the analysis of compounds in two closely-related languages, Afrikaans and Dutch. In this paper, we present the development and evaluation of two datasets, one for each language, that contain compound words with annotated compound boundaries. Such datasets can be used to train classifiers to identify the compound components in novel compounds. We describe the process of annotation and provide an overview of the annotation guidelines as well as global properties of the datasets. The inter-rater agreements between the annotators are considered highly reliable. Furthermore, we show the usability of these datasets by building an initial automatic compound boundary detection system, which assigns compound boundaries with approximately 90% accuracy.",
        "id":16125712
      },
      {
        "title":"Employing Low-Pass Filtered Temporal Speech Features for the Training of Ideal Ratio Mask in Speech Enhancement 摘要 在諸多基於深度學習之語音強化法中，遮罩式(masking-based)強化法求取一個 遮罩與雜訊語音之時頻圖相乘、藉此使所得乘積之新時頻圖所含雜訊成分降低、 以重建相對乾淨的語音訊號。在用以訓練遮罩之深度模型其輸入特徵的選取上， 許多長期以來用以語音辨識的特徵、如梅爾倒倒頻譜、振幅調變時頻圖、感知 線性估測係數等都是適合的選擇、可使訓練所得的遮罩達到有效的語音強化效 果。另外，傳統上若將語音特徵之時序列作低通濾波處理，可以抑制雜訊所帶 來的失真，因此，在本研究中，我們嘗試將各種語音特徵時序列，藉由離散小 波轉換的方式加以低通濾波，再用它們來訓練語音遮罩的深度模型，探究其是 否能使所學習之遮罩能對於原始雜訊語音之時頻圖有更佳的語音強化效果。在 我們的初步實驗裡，在人聲雜訊環境中，我們發現上述之低通濾波所得之特徵 序列、相較於原始特徵序列而言所學習而得的深度模型，能更有效地提升測試 語音之品質與可讀性。",
        "text":"The masking-based speech enhancement method pursues a multiplicative mask that applies to the spectrogram of input noise-corrupted utterance, and a deep neural network (DNN) is often used to learn the mask. In particular, the features commonly used for automatic speech recognition can serve as the input of the DNN to learn the well-behaved mask that significantly reduce the noise distortion of processed utterances. This study proposes to preprocess the input speech features for the ideal ratio mask (IRM)-based DNN by lowpass filtering in order to alleviate the noise components. In particular, we employ the discrete wavelet transform (DWT) to decompose the temporal speech feature sequence and scale down the detail coefficients, which correspond to the high-pass portion of the sequence. Preliminary experiments conducted on a subset of TIMIT corpus reveal that the proposed method can make the resulting IRM achieve higher speech quality and intelligibility for the babble noise-corrupted signals compared with the original IRM, indicating that the lowpass filtered temporal feature sequence can learn a superior IRM network for speech enhancement.關鍵詞：語音強化、特徵時序列、低通濾波、理想比例遮罩法、小波轉換",
        "id":248182545
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper shows assessment of training instabilities at different levels for language models?",
    "positive_ctxs":[
      {
        "title":"Measuring the Instability of Fine-Tuning",
        "text":"Fine-tuning pre-trained language models on downstream tasks with varying random seeds has been shown to be unstable, especially on small datasets. Many previous studies have investigated this instability and proposed methods to mitigate it. However, most studies only used the standard deviation of performance scores (SD) as their measure, which is a narrow characterization of instability. In this paper, we analyze SD and six other measures quantifying instability at different levels of granularity. Moreover, we propose a systematic framework to evaluate the validity of these measures. Finally, we analyze the consistency and difference between different measures by reassessing existing instability mitigation methods. We hope our results will inform the development of better measurements of fine-tuning instability. 1Layer-wise Learning Rate Decay (Howard and Ruder, 2018, LLRD) assigns decreasing learning rates from the topmost layer to the bottom layer by a constant hyper-parameter discounting factor η. Howard and Ruder (2018) empirically show that models trained using LLRD are more stable, by retaining more generalizable pre-trained knowledge in bottom layers, while forgetting specialized pre-train knowledge in top layers.Re-init (Zhang et al., 2021) stabilizes fine-tuning by re-initializing the top k layers of PLMs. The underlying intuition is similar to LLRD: top layers of PLMs contain more pre-train task specific knowledge, and transferring it may hurt stability.",
        "id":256868814
      }
    ],
    "negative_ctxs":[
      {
        "title":"The RWTH System for Statistical Translation of Spoken Dialogues",
        "text":"This paper gives an overview of our work on statistical machine translation of spoken dialogues, in particular in the framework of the Verbmobil project. The goal of the Verbmobil project is the translation of spoken dialogues in the domains of appointment scheduling and travel planning. Starting with the Bayes decision rule as in speech recognition, we show how the required probability distributions can be structured into three parts: the language model, the alignment model and the lexicon model. We describe the components of the system and report results on the Verbmobil task. The experience obtained in the Verbmobil project, in particular a large-scale end-to-end evaluation, showed that the statistical approach resulted in significantly lower error rates than three competing translation approaches: the sentence error rate was 29% in comparison with 52% to 62% for the other translation approaches.",
        "id":8473810
      },
      {
        "title":"Experiments with Interactive Question-Answering",
        "text":"This paper describes a novel framework for interactive question-answering (Q\/A) based on predictive questioning. Generated off-line from topic representations of complex scenarios, predictive questions represent requests for information that capture the most salient (and diverse) aspects of a topic. We present experimental results from large user studies (featuring a fully-implemented interactive Q\/A system named FERRET) that demonstrates that surprising performance is achieved by integrating predictive questions into the context of a Q\/A dialogue.",
        "id":17979800
      },
      {
        "title":"Know What You Don't Know: Modeling a Pragmatic Speaker that Refers to Objects of Unknown Categories",
        "text":"Zero-shot learning in Language & Vision is the task of correctly labelling (or naming) objects of novel categories. Another strand of work in L&V aims at pragmatically informative rather than \"correct\" object descriptions, e.g. in reference games. We combine these lines of research and model zero-shot reference games, where a speaker needs to successfully refer to a novel object in an image. Inspired by models of \"rational speech acts\", we extend a neural generator to become a pragmatic speaker reasoning about uncertain object categories. As a result of this reasoning, the generator produces fewer nouns and names of distractor categories as compared to a literal speaker. We show that this conversational strategy for dealing with novel objects often improves communicative success, in terms of resolution accuracy of an automatic listener.",
        "id":189762140
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper shows that in instruction tuning, the instructions can be compressed to small supporting sets of words that provide useful information?",
    "positive_ctxs":[
      {
        "title":"Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning",
        "text":"Large language models (LLMs) have shown impressive performance in following natural language instructions to solve unseen tasks. However, it remains unclear whether models truly understand task definitions and whether the human-written definitions are optimal. In this paper, we systematically study the role of task definitions in instruction learning. We first conduct an ablation analysis informed by human annotations to understand which parts of a task definition are most important, and find that model performance only drops substantially when removing contents describing the task output, in particular label information. Next, we propose an automatic algorithm to compress task definitions to a minimal supporting set of tokens, and find that 60% of tokens can be removed while maintaining or even improving model performance. Based on these results, we propose two strategies to help models better leverage task instructions: (1) providing only key information for tasks in a common structured format, and (2) adding a metatuning stage to help the model better understand the definitions. With these two strategies, we achieve a 4.2 Rouge-L improvement over 119 unseen test tasks. naneh Hajishirzi. 2022a. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560.",
        "id":259063796
      }
    ],
    "negative_ctxs":[
      {
        "title":"Decomposing Consumer Health Questions",
        "text":"This paper presents a method for decomposing long, complex consumer health questions. Our approach largely decomposes questions using their syntactic structure, recognizing independent questions embedded in clauses, as well as coordinations and exemplifying phrases. Additionally, we identify elements specific to disease-related consumer health questions, such as the focus disease and background information. To achieve this, our approach combines rank-and-filter machine learning methods with rule-based methods. Our results demonstrate significant improvements over the heuristic methods typically employed for question decomposition that rely only on the syntactic parse tree.",
        "id":1006882
      },
      {
        "title":"DEEPSPHERE: A GRAPH-BASED SPHERICAL CNN",
        "text":"Designing a convolution for a spherical neural network requires a delicate tradeoff between efficiency and rotation equivariance. DeepSphere, a method based on a graph representation of the sampled sphere, strikes a controllable balance between these two desiderata. This contribution is twofold. First, we study both theoretically and empirically how equivariance is affected by the underlying graph with respect to the number of vertices and neighbors. Second, we evaluate DeepSphere on relevant problems. Experiments show state-of-the-art performance and demonstrates the efficiency and flexibility of this formulation. Perhaps surprisingly, comparison with previous work suggests that anisotropic filters might be an unnecessary price to pay. Our code is available at https: \/\/github.com\/deepsphere.arXiv:2012.15000v1 [cs.LG] 30 Dec 2020Published as a conference paper at ICLR 2020As neural networks (NNs) have proved to be great tools for inference, variants have been developed to handle spherical data. Exploiting the locally Euclidean property of the sphere, early attempts used standard 2D convolutions on a grid sampling of the sphere (Boomsma & Frellsen, 2017;Su & Grauman, 2017;Coors et al., 2018). While simple and efficient, those convolutions are not equivariant to rotations. On the other side of this tradeoff,Cohen et al. (2018)andEsteves et al. (2018)proposed to perform proper spherical convolutions through the spherical harmonic transform. While equivariant to rotations, those convolutions are expensive (section 2).As a lack of equivariance can penalize performance (section 4.2) and expensive convolutions prohibit their application to some real-world problems, methods standing between these two extremes are desired.Cohen et al. (2019)proposed to reduce costs by limiting the size of the representation of the symmetry group by projecting the data from the sphere to the icosahedron. The distortions introduced by this projection might however hinder performance (section 4.3).Another approach is to represent the sampled sphere as a graph connecting pixels according to the distance between them (Bruna et al., 2013;Khasanova & Frossard, 2017;. While Laplacian-based graph convolutions are more efficient than spherical convolutions, they are not exactly equivariant . In this work, we argue that graph-based spherical CNNs strike an interesting balance, with a controllable tradeoff between cost and equivariance (which is linked to performance). Experiments on multiple problems of practical interest show the competitiveness and flexibility of this approach.",
        "id":213692365
      },
      {
        "title":"TAPAS: Weakly Supervised Table Parsing via Pre-training",
        "text":"Answering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation. In this paper, we present TAPAS, an approach to question answering over tables without generating logical forms. TAPAS trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection. TAPAS extends BERT's architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end. We experiment with three different semantic parsing datasets, and find that TAPAS outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WIKISQL and WIKITQ, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our setting, from WIK-ISQL to WIKITQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.",
        "id":214802901
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper studies how current retrieval systems handle queries which contain multiple constraints?",
    "positive_ctxs":[
      {
        "title":"QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations",
        "text":"Formulating selective information needs results in queries that implicitly specify set operations, such as intersection, union, and difference. For instance, one might search for \"shorebirds that are not sandpipers\" or \"science-fiction films shot in England\". To study the ability of retrieval systems to meet such information needs, we construct QUEST, a dataset of 3357 natural language queries with implicit set operations, that map to a set of entities corresponding to Wikipedia documents. The dataset challenges models to match multiple constraints mentioned in queries with corresponding evidence in documents and correctly perform various set operations. The dataset is constructed semi-automatically using Wikipedia category names. Queries are automatically composed from individual categories, then paraphrased and further validated for naturalness and fluency by crowdworkers. Crowdworkers also assess the relevance of entities based on their documents and highlight attribution of query constraints to spans of document text. We analyze several modern retrieval systems, finding that they often struggle on such queries. Queries involving negation and conjunction are particularly challenging and systems are further challenged with combinations of these operations. 1",
        "id":258822815
      }
    ],
    "negative_ctxs":[
      {
        "title":"USP-EACH Frequency-based Greedy Attribute Selection for Referring Expressions Generation",
        "text":"Both greedy and domain-oriented REG algorithms have significant strengths but tend to perform poorly according to humanlikeness criteria as measured by, e.g., Dice scores. In this work we describe an attempt to combine both perspectives into a single attribute selection strategy to be used as part of the Dale & Reiter Incremental algorithm in the REG Challenge 2008, and the results in both Furniture and People domains.",
        "id":8526022
      },
      {
        "title":"MULTI-CROSSRE A Multi-Lingual Multi-Domain Dataset for Relation Extraction",
        "text":"Most research in Relation Extraction (RE) involves the English language, mainly due to the lack of multi-lingual resources. We propose MULTI-CROSSRE, the broadest multi-lingual dataset for RE, including 26 languages in addition to English, and covering six text domains. MULTI-CROSSRE is a machine translated version of CrossRE (Bassignana and Plank, 2022a), with a sub-portion including more than 200 sentences in seven diverse languages checked by native speakers. We run a baseline model over the 26 new datasets and-as sanity check-over the 26 back-translations to English. Results on the back-translated data are consistent with the ones on the original English CrossRE, indicating high quality of the translation and the resulting dataset.",
        "id":215509168
      },
      {
        "title":"應用詞向量模型於日文單詞可讀性評估之研究 Japanese Word Readability Assessment using Word Embeddings",
        "text":"In text analysis, text readability has been an important research topic for many years. However, most studies focus on the document readability rather than the word readability. To decide the readability levels of words, linguists need to spend a large amount of human effort and assessment time.The most challenging problem faced in the task of automatic word readability assessment is the lack of research on readability for each word. In this study, we propose a novel assessment model for word readability called WR-kNN based on the word embedding technology by calculating the word vectors",
        "id":233029479
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper studies the concept of enhancing the coverage of a selective prediction system by re-attempting the questions on which it was not sufficiently confident.",
    "positive_ctxs":[
      {
        "title":"Post-Abstention: Towards Reliably Re-Attempting the Abstained Instances in QA",
        "text":"Despite remarkable progress made in natural language processing, even the state-of-the-art models often make incorrect predictions. Such predictions hamper the reliability of systems and limit their widespread adoption in realworld applications. Selective prediction partly addresses the above concern by enabling models to abstain from answering when their predictions are likely to be incorrect. While selective prediction is advantageous, it leaves us with a pertinent question 'what to do after abstention'. To this end, we present an explorative study on 'Post-Abstention', a task that allows re-attempting the abstained instances with the aim of increasing coverage of the system without significantly sacrificing its accuracy. We first provide mathematical formulation of this task and then explore several methods to solve it. Comprehensive experiments on 11 QA datasets show that these methods lead to considerable risk improvements -performance metric of the Post-Abstention task-both in the in-domain and the out-of-domain settings. We also conduct a thorough analysis of these results which further leads to several interesting findings. Finally, we believe that our work will encourage and facilitate further research in this important area of addressing the reliability of NLP systems.",
        "id":258461053
      }
    ],
    "negative_ctxs":[
      {
        "title":"Generating Learner-Like Morphological Errors in Russian",
        "text":"To speed up the process of categorizing learner errors and obtaining data for languages which lack error-annotated data, we describe a linguistically-informed method for generating learner-like morphological errors, focusing on Russian. We outline a procedure to select likely errors, relying on guiding stem and suffix combinations from a segmented lexicon to match particular error categories and relying on grammatical information from the original context.",
        "id":1042287
      },
      {
        "title":"One Vector is Not Enough: Entity-Augmented Distributional Semantics for Discourse Relations",
        "text":"Discourse relations bind smaller linguistic units into coherent texts. However, automatically identifying discourse relations is difficult, because it requires understanding the semantics of the linked arguments. A more subtle challenge is that it is not enough to represent the meaning of each argument of a discourse relation, because the relation may depend on links between lower-level components, such as entity mentions. Our solution computes distributional meaning representations by composition up the syntactic parse tree.A key difference from previous work on compositional distributional semantics is that we also compute representations for entity mentions, using a novel downward compositional pass. Discourse relations are predicted from the distributional representations of the arguments, and also of their coreferent entity mentions. The resulting system obtains substantial improvements over the previous state-of-theart in predicting implicit discourse relations in the Penn Discourse Treebank.",
        "id":15065468
      },
      {
        "title":"A Semantics and Pragmatics for the Pluperfect",
        "text":"We offer a semantics and pragmatics of the pluperfect in narrative discourse. We rexamine in a formal model of implicature, how the reader's knowledge about the discourse, Gricean-maxims and causation contribute to the meaning of the pluperfect. By placing the analysis in a theory where the interactions among these knowledge resources can be precisely computed, we overcome some problems with previous Reichenbachian approaches.",
        "id":6085570
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper surveyed the datasets and tasks of asking clarification questions in conversational systems??",
    "positive_ctxs":[
      {
        "title":"A Survey on Asking Clarification Questions Datasets in Conversational Systems",
        "text":"The ability to understand a user's underlying needs is critical for conversational systems, especially with limited input from users in a conversation. Thus, in such a domain, Asking Clarification Questions (ACQs) to reveal users' true intent from their queries or utterances arise as an essential task. However, it is noticeable that a key limitation of the existing ACQs studies is their incomparability, from inconsistent use of data, distinct experimental setups and evaluation strategies. Therefore, in this paper, to assist the development of ACQs techniques, we comprehensively analyse the current ACQs research status, which offers a detailed comparison of publicly available datasets, and discusses the applied evaluation metrics, joined with benchmarks for multiple ACQs-related tasks. In particular, given a thorough analysis of the ACQs task, we discuss a number of corresponding research directions for the investigation of ACQs as well as the development of conversational systems.",
        "id":258887719
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":232021812
      },
      {
        "title":"ZikaHack 2016: A digital disease detection competition",
        "text":"Effective response to infectious diseases outbreaks relies on the rapid and early detection of those outbreaks. Invalidated, yet timely and openly available digital information can be used for the early detection of outbreaks. Public health surveillance authorities can exploit these early warnings to plan and co-ordinate rapid surveillance and emergency response programs. In 2016, a digital disease detection competition named ZikaHack was launched. The objective of the competition was for multidisciplinary teams to design, develop and demonstrate innovative digital disease detection solutions to retrospectively detect the 2015-16 Brazilian Zika virus outbreak earlier than traditional surveillance methods. In this paper, an overview of the Zik-aHack competition is provided. The challenges and lessons learned in organizing this competition are also discussed for use by other researchers interested in organizing similar competitions.",
        "id":32752811
      },
      {
        "title":"Exploiting Semantic Role Labeling, WordNet and Wikipedia for Coreference Resolution",
        "text":"In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources. These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels. We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns.",
        "id":1212389
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper used both automatically generated and manual templates with word tuples to adapt language models from one timestamp to another?",
    "positive_ctxs":[
      {
        "title":"Learning Dynamic Contextualised Word Embeddings via Template-based Temporal Adaptation",
        "text":"Dynamic contextualised word embeddings (DCWEs) represent the temporal semantic variations of words. We propose a method for learning DCWEs by time-adapting a pretrained Masked Language Model (MLM) using timesensitive templates. Given two snapshots C 1 and C 2 of a corpus taken respectively at two distinct timestamps T 1 and T 2 , we first propose an unsupervised method to select (a) pivot terms related to both C 1 and C 2 , and (b) anchor terms that are associated with a specific pivot term in each individual snapshot. We then generate prompts by filling manually compiled templates using the extracted pivot and anchor terms. Moreover, we propose an automatic method to learn time-sensitive templates from C 1 and C 2 , without requiring any human supervision. Next, we use the generated prompts to adapt a pretrained MLM to T 2 by fine-tuning using those prompts. Multiple experiments show that our proposed method reduces the perplexity of test sentences in C 2 , outperforming the current state-of-the-art.",
        "id":251741028
      }
    ],
    "negative_ctxs":[
      {
        "title":"Perception audio-visuelle de séquences VCV produites par des personnes porteuses de Trisomie 21 : une étude préliminaire",
        "text":"RESUMELa parole des personnes avec trisomie 21 (T21) présente une altération systématique de l'intelligibilité qui n'a été quantifiée qu'auditivement. Or la modalité visuelle pourrait améliorer l'intelligibilité comme c'est le cas pour les personnes « ordinaires ». Cette étude compare la manière dont 24 participants ordinaires perçoivent des séquences VCV voyelle-consonne-voyelle) produites par quatre adultes (2 avec T21 et 2 ordinaires) et présentées dans le bruit en modalités auditive, visuelle et audiovisuelle. Les résultats confirment la perte d'intelligibilité en modalité auditive dans le cas de locuteurs porteurs de T21. Pour les deux locuteurs impliqués, l'intelligibilité visuelle est néanmoins équivalente à celle des deux locuteurs ordinaires et compensent le déficit d'intelligibilité auditive. Ces résultats suggèrent l'apport de la modalité visuelle vers une meilleure intelligibilité des personnes porteuses de T21.ABSTRACTAuditory-visual Perception of VCVs Produced by People with Down Syndrome: a Preliminary StudyThe speech of people with Down Syndrome (DS) is systematically altered resulting in an intelligibility loss. This was quantified only auditorily. The visual modality could actually improve intelligibility, as is the case for \"ordinary\" people. The present study compares the way 24 ordinary participants perceive VCV sequences (vowel-consonant-vowel) produced by four adults (2 with DS and 2 ordinary) and presented in noise in three modalities: auditory, auditoryvisual and visual. The results confirm an intelligibility loss in the auditory modality for speakers with DS. However, for the two speakers involved in this study, visual intelligibility is equivalent to that of the ordinary speakers and compensates for the auditory intelligibility loss. These results put forward the importance of integrating multimodality to improve the intelligibility of people with DS. MOTS-CLES : Parole, Multimodalité, Perception, Trisomie 21, Apport visuel.",
        "id":148774062
      },
      {
        "title":"Example-Based Machine Translation Using a Dictionary of Word Pairs",
        "text":"Machine translation systems, whether rule-based, example-based, or statistical, all rely on dictionaries that are in essence mappings between individual words of the source and the target language. Criteria for the disambiguation of ambiguous words and for differences in word order between the two languages are not accounted for in the lexicon. Instead, these important issues are dealt with in the translation engines. Because the engines tend to be compact and (even with data-oriented approaches) do not fully reflect the complexity of the problem, this approach generally does not account for the more fine grained facets of word behavior. This leads to wrong generalizations and, as a consequence, translation quality tends to be poor. In this paper we suggest to approach this problem by using a new type of lexicon that is not based on individual words but on pairs of words. For each pair of consecutive words in the source language the lexicon lists the possible translations in the target language together with information on order and distance of the target words. The process of machine translation is then seen as a combinatorial problem: For all word pairs in a source sentence all possible translations are retrieved from the lexicon and then those translations are discarded that lead to contradictions when constructing the target sentence. This process implicitly leads to word sense disambiguation and to language specific reordering of words.",
        "id":2710648
      },
      {
        "title":"SCALABLE BAYESIAN INVERSE REINFORCEMENT LEARNING",
        "text":"Bayesian inference over the reward presents an ideal solution to the ill-posed nature of the inverse reinforcement learning problem. Unfortunately current methods generally do not scale well beyond the small tabular setting due to the need for an inner-loop MDP solver, and even non-Bayesian methods that do themselves scale often require extensive interaction with the environment to perform well, being inappropriate for high stakes or costly applications such as healthcare. In this paper we introduce our method, Approximate Variational Reward Imitation Learning (AVRIL), that addresses both of these issues by jointly learning an approximate posterior distribution over the reward that scales to arbitrarily complicated state spaces alongside an appropriate policy in a completely offline manner through a variational approach to said latent reward. Applying our method to real medical data alongside classic control simulations, we demonstrate Bayesian reward inference in environments beyond the scope of current methods, as well as task performance competitive with focused offline imitation learning algorithms.",
        "id":231918471
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper utilizes language models to generate singable lyrics that can go well with a predefined melody?",
    "positive_ctxs":[
      {
        "title":"Unsupervised Melody-to-Lyric Generation",
        "text":"Automatic melody-to-lyric generation is a task in which song lyrics are generated to go with a given melody. It is of significant practical interest and more challenging than unconstrained lyric generation as the music imposes additional constraints onto the lyrics. The training data is limited as most songs are copyrighted, resulting in models that underfit the complicated cross-modal relationship between melody and lyrics. In this work, we propose a method for generating high-quality lyrics without training on any aligned melody-lyric data. Specifically, we design a hierarchical lyric generation framework that first generates a song outline and second the complete lyrics. The framework enables disentanglement of training (based purely on text) from inference (melodyguided text generation) to circumvent the shortage of parallel data.We leverage the segmentation and rhythm alignment between melody and lyrics to compile the given melody into decoding constraints as guidance during inference. The two-step hierarchical design also enables content control via the lyric outline, a much-desired feature for democratizing collaborative song creation. Experimental results show that our model can generate high-quality lyrics that are more on-topic, singable, intelligible, and coherent than strong baselines, for example SongMASS (Sheng et al., 2021), a SOTA model trained on a parallel dataset, with a 24% relative overall quality improvement based on human ratings. 1",
        "id":259370722
      }
    ],
    "negative_ctxs":[
      {
        "title":"Entailment graphs for text exploration",
        "text":"‡ Fondazione Bruno Kessler (FBK-irst), ItalyTaxonomy-based representations are widely used to model compactly large amounts of textual data. While current methods allow organizing knowledge at the lexical level (keywords\/concepts\/topics), there is an increasing demand to move towards more informative representations, which express properties of concepts and relations among them. This demand triggered our research on statement entailment graphs. In these graphs, nodes are natural language statements (propositions), comprising of predicates with their arguments and modifiers, while edges represent entailment relations between nodes. In this talk we report initial research that defines the properties of entailment graphs and their potential applications. Particularly, we show how entailment graphs can be profitably used for both knowledge acquisition and text exploration.Beyond providing a rich and informative representation, statement entailment graphs allow integrating multiple semantic inferences. So far, textual inference research focused on single, mutually independent, entailment judgments. However, in many scenarios there are dependencies among Text\/Hypothesis pairs, which need to be captured consistently. This calls for global optimization algorithms for inter-dependent entailment judgments, taking advantage of the overall entailment graph structure (e.g. ensuring entailment graph transitivity).From the applied perspective, we are experimenting with entailment graphs in the context of the EX-CITEMENT project industrial scenarios. We focus on the text analytics domain, and particularly on the analysis of customer interactions across multiple channels, including speech, email, chat and social media, and multiple languages (English, German, Italian). For example, we would like to recognize that the complaint they charge too much for sandwiches entails food is too expensive, and allow an analyst to compactly navigate through an entailment graph that consolidates the information structure of a large number of customer statements. Our eventual applied goal is to develop a new generation of inference-based text exploration applications, which will enable businesses to better analyze their diverse and often unpredicted client content. This task will be exemplified with data collected from real customer interactions, while referring to the EXCITEMENT Open Platform that we developed as a generic open source framework for textual inferences.",
        "id":42329141
      },
      {
        "title":"Analogical Dialogue Acts: Supporting Learning by Reading Analogies",
        "text":"Analogy is heavily used in written explanations, particularly in instructional texts. We introduce the concept of analogical dialogue acts (ADAs) which represent the roles utterances play in instructional analogies. We describe a catalog of such acts, based on ideas from structure-mapping theory. We focus on the operations that these acts lead to while understanding instructional texts, using the Structure-Mapping Engine (SME) and dynamic case construction in a computational model. We test this model on a small corpus of instructional analogies, expressed in simplified English, which were understood via a semiautomatic natural language system using analogical dialogue acts. The model enabled a system to answer questions after understanding the analogies that it was not able to answer without them.",
        "id":3090899
      },
      {
        "title":"Enhancing The RATP-DECODA Corpus With Linguistic Annotations For Performing A Large Range Of NLP Tasks",
        "text":"In this article, we present the RATP-DECODA Corpus which is composed by a set of 67 hours of speech from telephone conversations of a Customer Care Service (CCS). This corpus is already available on line at http:\/\/sldr.org\/sldr000847\/fr in its first version. However, many enhancements have been made in order to allow the development of automatic techniques to transcript conversations and to capture their meaning. These enhancements fall into two categories: firstly, we have increased the size of the corpus with manual transcriptions from a new operational day; secondly we have added new linguistic annotations to the whole corpus (either manually or through an automatic processing) in order to perform various linguistic tasks from syntactic and semantic parsing to dialog act tagging and dialog summarization.",
        "id":13300447
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which paper was the first to propose combining human spoken language and sign language datasets with gloss annotations to enhance the performance of sign language translation?",
    "positive_ctxs":[
      {
        "title":"Neural Machine Translation Methods for Translating Text to Sign Language Glosses",
        "text":"State-of-the-art techniques common to low resource Machine Translation (MT) are applied to improve MT of spoken language text to Sign Language (SL) glosses. In our experiments, we improve the performance of the transformer-based models via (1) data augmentation, (2) semi-supervised Neural Machine Translation (NMT), (3) transfer learning and (4) multilingual NMT. The proposed methods are implemented progressively on two German SL corpora containing gloss annotations. Multilingual NMT combined with data augmentation appear to be the most successful setting, yielding statistically significant improvements as measured by three automatic metrics (up to over 6 points BLEU), and confirmed via human evaluation. Our best setting outperforms all previous work that report on the same test-set and is also confirmed on a corpus of the American Sign Language (ASL).",
        "id":259370577
      }
    ],
    "negative_ctxs":[
      {
        "title":"Unsupervised Semantic Role Induction with Graph Partitioning",
        "text":"In this paper we present a method for unsupervised semantic role induction which we formalize as a graph partitioning problem. Argument instances of a verb are represented as vertices in a graph whose edge weights quantify their role-semantic similarity. Graph partitioning is realized with an algorithm that iteratively assigns vertices to clusters based on the cluster assignments of neighboring vertices. Our method is algorithmically and conceptually simple, especially with respect to how problem-specific knowledge is incorporated into the model. Experimental results on the CoNLL 2008 benchmark dataset demonstrate that our model is competitive with other unsupervised approaches in terms of F1 whilst attaining significantly higher cluster purity.",
        "id":14436537
      },
      {
        "title":"Formalism for a language agnostic language learning game and productive grid generation",
        "text":"In this article, we describe the modifications of MagicWord, a language learning game focused on accuracy, in order to allow the integration of new languages. We first describe the motivations behind the design of the game. Then we explain the modifications performed before exploring the consequences both game-wise and language learning-wise.In order to improve their replay-value, language learning games need to rely on language resources of diverse complexity depending on their rules and objectives. In this paper, we tackle the issue of providing multi-language resources for a language learning letter game, MagicWord. Before exploring the technical difficulties as well as their intricacies both in terms of language representation, learning and gaming, we will explain the game, its objectives and the design process.",
        "id":203657273
      },
      {
        "title":"Published as a conference paper at ICLR 2020 COHERENT GRADIENTS: AN APPROACH TO UNDERSTANDING GENERALIZATION IN GRADIENT DESCENT-BASED OPTIMIZATION",
        "text":"An open question in the Deep Learning community is why neural networks trained with Gradient Descent generalize well on real datasets even though they are capable of fitting random data. We propose an approach to answering this question based on a hypothesis about the dynamics of gradient descent that we call Coherent Gradients: Gradients from similar examples are similar and so the overall gradient is stronger in certain directions where these reinforce each other. Thus changes to the network parameters during training are biased towards those that (locally) simultaneously benefit many examples when such similarity exists. We support this hypothesis with heuristic arguments and perturbative experiments and outline how this can explain several common empirical observations about Deep Learning. Furthermore, our analysis is not just descriptive, but prescriptive. It suggests a natural modification to gradient descent that can greatly reduce overfitting.",
        "id":211296676
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_acl",
    "question":"Which papers develop methods to make in-context learning more computationally efficient?",
    "positive_ctxs":[
      {
        "title":"FiD-ICL: A Fusion-in-Decoder Approach for Efficient In-Context Learning",
        "text":"Large pre-trained models are capable of fewshot in-context learning (ICL), i.e., performing a new task by prepending a few demonstrations before the test input. However, the concatenated demonstrations are often excessively long and induce additional computation. Inspired by fusion-in-decoder (FiD) models which efficiently aggregate more passages and thus outperforms concatenation-based models in opendomain QA, we hypothesize that similar techniques can be applied to improve the efficiency and end-task performance of ICL. To verify this, we present a comprehensive study on applying three fusion methods-concatenationbased (early fusion), FiD (intermediate), and ensemble-based (late)-to ICL. We adopt a meta-learning setup where a model is first trained to perform ICL on a mixture of tasks using one selected fusion method, then evaluated on held-out tasks for ICL. Results on 11 heldout tasks show that FiD-ICL matches or outperforms the other two fusion methods. Additionally, we show that FiD-ICL (1) is 10x faster at inference time compared to concat-based and ensemble-based ICL, as we can easily precompute the representations of in-context examples and reuse them; (2) enables scaling up to meta-training 3B-sized models, which would fail for concat-based ICL.",
        "id":259370780
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":219300739
      },
      {
        "title":"KPTimes: A Large-Scale Dataset for Keyphrase Generation on News Documents",
        "text":"Keyphrase generation is the task of predicting a set of lexical units that conveys the main content of a source text. Existing datasets for keyphrase generation are only readily available for the scholarly domain and include non-expert annotations. In this paper we present KPTimes, a large-scale dataset of news texts paired with editor-curated keyphrases. Exploring the dataset, we show how editors tag documents, and how their annotations differ from those found in existing datasets. We also train and evaluate state-of-the-art neural keyphrase generation models on KPTimes to gain insights on how well they perform on the news domain. The dataset is available online at https:\/\/github.com\/ygorg\/KPTimes.",
        "id":208512795
      },
      {
        "title":"Mining Bilingual Word Pairs from Comparable Corpus using Apache Spark Framework",
        "text":"Bilingual dictionaries are essential resources in many areas of natural language processing tasks, but resource-scarce and less popular language pairs rarely have such. Efficient automatic methods for inducting bilingual dictionaries are needed as manual resources and efforts are scarce for low-resourced languages. In this paper, we induce word translations using bilingual embedding. We use the Apache Spark ® framework for parallel computation. Further, to validate the quality of the generated bilingual dictionary, we use it in a phrase-table aided Neural Machine Translation (NMT) system. The system can perform moderately well with a manual bilingual dictionary; we change this into our inducted dictionary. The corresponding translated outputs are compared using the Bilingual Evaluation Understudy (BLEU) and Rank-based Intuitive Bilingual Evaluation Score (RIBES) metrics.",
        "id":244464078
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_acl",
    "question":"Which papers were among the first to explore the task of targeted training data extraction?",
    "positive_ctxs":[
      {
        "title":"ETHICIST: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation",
        "text":"Large pre-trained language models achieve impressive results across many tasks. However, recent works point out that pre-trained language models may memorize a considerable fraction of their training data, leading to the privacy risk of information leakage. In this paper, we propose a method named ETHICIST for targeted training data Extraction THrough loss smoothed soft prompting and calIbrated ConfIdence eSTimation, investigating how to recover the suffix in the training data when given a prefix. To elicit memorization in the attacked model, we tune soft prompt embeddings while keeping the model fixed. We further propose a smoothing loss that smooths the loss distribution of the suffix tokens to make it easier to sample the correct suffix. In order to select the most probable suffix from a collection of sampled suffixes and estimate the prediction confidence, we propose a calibrated confidence estimation method, which normalizes the confidence of the generated suffixes with a local estimation. We show that ETHICIST significantly improves the extraction performance on a recently proposed public benchmark. We also investigate several factors influencing the data extraction performance, including decoding strategy, model scale, prefix length, and suffix length. Our code is available at https:\/\/github.com\/ thu-coai\/Targeted-Data-Extraction.",
        "id":259370520
      }
    ],
    "negative_ctxs":[
      {
        "title":"Choosing the best machine translation system to translate a sentence by using only source-language information",
        "text":"This paper describes a novel approach aimed to identify a priori which subset of machine translation (MT) systems among a known set will produce the most reliable translations for a given source-language (SL) sentence. We aim to select this subset of MT systems by using only information extracted from the SL sentence to be translated, and without access to the inner workings of the MT systems being used. A system able to select in advance, without translating, that subset of MT systems will allow multi-engine MT systems to save computing resources and focus on the combination of the output of the best MT systems. The selection of the best MT systems is done by extracting a set of features from each SL sentence and then using maximum entropy classifiers trained over a set of parallel sentences. Preliminary experiments on two European language pairs show a small, non-statistical significant improvement.",
        "id":16402907
      },
      {
        "title":"World Knowledge for Reading Comprehension: Rare Entity Prediction with Hierarchical LSTMs Using External Descriptions",
        "text":"Humans interpret texts with respect to some background information, or world knowledge, and we would like to develop automatic reading comprehension systems that can do the same. In this paper, we introduce a task and several models to drive progress towards this goal. In particular, we propose the task of rare entity prediction: given a web document with several entities removed, models are tasked with predicting the correct missing entities conditioned on the document context and the lexical resources. This task is challenging due to the diversity of language styles and the extremely large number of rare entities. We propose two recurrent neural network architectures which make use of external knowledge in the form of entity descriptions. Our experiments show that our hierarchical LSTM model performs significantly better at the rare entity prediction task than those that do not make use of external resources.",
        "id":2215426
      },
      {
        "title":"LSOIE: A Large-Scale Dataset for Supervised Open Information Extraction",
        "text":"Open Information Extraction (OIE) systems seek to compress the factual propositions of a sentence into a series of n-ary tuples. These tuples are useful for downstream tasks in natural language processing like knowledge base creation, textual entailment, and natural language understanding. However, current OIE datasets are limited in both size and diversity. We introduce a new dataset by converting the QA-SRL 2.0 dataset to a large-scale OIE dataset (LSOIE). Our LSOIE dataset is 20 times larger than the next largest human-annotated OIE dataset. We construct and evaluate several benchmark OIE models on LSOIE, providing baselines for future improvements on the task. Our LSOIE data, models, and code are made publicly available. 1",
        "id":231719135
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which pre-trained model is specifically designed for low-resource dialogue summarization tasks?",
    "positive_ctxs":[
      {
        "title":"DIONYSUS: A Pre-trained Model for Low-Resource Dialogue Summarization",
        "text":"Dialogue summarization has recently garnered significant attention due to its wide range of applications. However, existing methods for summarizing dialogues have limitations because they do not take into account the inherent structure of dialogue and rely heavily on labeled data, which can lead to poor performance in new domains. In this work, we propose DIONYSUS (dynamic input optimization in pre-training for dialogue summarization), a pre-trained encoder-decoder model for summarizing dialogues in any new domain. To pretrain DIONYSUS, we create two pseudo summaries for each dialogue example: one from a fine-tuned summarization model and the other from important dialogue turns. We then choose one of these pseudo summaries based on information distribution differences in different types of dialogues. This selected pseudo summary serves as the objective for pre-training DIONYSUS using a self-supervised approach on a large dialogue corpus. Our experiments show that DIONYSUS outperforms existing methods on six datasets, as demonstrated by its ROUGE scores in zero-shot and few-shot settings.",
        "id":254877347
      }
    ],
    "negative_ctxs":[
      {
        "title":"Integrating a Rule-based with a Hierarchical Translation System",
        "text":"Recent developments on hybrid systems that combine rule-based machine translation (RBMT) systems with statistical machine translation (SMT) generally neglect the fact that RBMT systems tend to produce more syntactically well-formed translations than data-driven systems. This paper proposes a method that alleviates this issue by preserving more useful structures produced by RBMT systems and utilizing them in a SMT system that operates on hierarchical structures instead of flat phrases alone. For our experiments, we use Joshua as the decoder(Li et al., 2009). It is the first attempt towards a tighter integration of MT systems from different paradigms that both support hierarchical analyses. Preliminary results show consistent improvements over the previous approach.",
        "id":15849173
      },
      {
        "title":"",
        "text":"",
        "id":195348815
      },
      {
        "title":"COMPUTATIONAL UNDERSTANDING",
        "text":"",
        "id":3926948
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which research paper leverages event structure information from Abstract Meaning Representation (AMR) graphs to aid in recognizing causal relations between events?",
    "positive_ctxs":[
      {
        "title":"Semantic Structure Enhanced Event Causality Identification",
        "text":"Event Causality Identification (ECI) aims to identify causal relations between events in unstructured texts. This is a very challenging task, because causal relations are usually expressed by implicit associations between events. Existing methods usually capture such associations by directly modeling the texts with pre-trained language models, which underestimate two kinds of semantic structures vital to the ECI task, namely, event-centric structure and eventassociated structure. The former includes important semantic elements related to the events to describe them more precisely, while the latter contains semantic paths between two events to provide possible supports for ECI. In this paper, we study the implicit associations between events by modeling the above explicit semantic structures, and propose a Semantic Structure Integration model (SemSIn). It utilizes a GNN-based event aggregator to integrate the event-centric structure information, and employs an LSTM-based path aggregator to capture the event-associated structure information between two events. Experimental results on three widely used datasets show that SemSIn achieves significant improvements over baseline methods.",
        "id":258833194
      }
    ],
    "negative_ctxs":[
      {
        "title":"Published as a conference paper at ICLR 2023 BUILDING NORMALIZING FLOWS WITH STOCHASTIC INTERPOLANTS",
        "text":"A generative model based on a continuous-time normalizing flow between any pair of base and target probability densities is proposed. The velocity field of this flow is inferred from the probability current of a time-dependent density that interpolates between the base and the target in finite time. Unlike conventional normalizing flow inference methods based the maximum likelihood principle, which require costly backpropagation through ODE solvers, our interpolant approach leads to a simple quadratic loss for the velocity itself which is expressed in terms of expectations that are readily amenable to empirical estimation. The flow can be used to generate samples from either the base or target, and to estimate the likelihood at any time along the interpolant. In addition, the flow can be optimized to minimize the path length of the interpolant density, thereby paving the way for building optimal transport maps. In situations where the base is a Gaussian density, we also show that the velocity of our normalizing flow can also be used to construct a diffusion model to sample the target as well as estimate its score. However, our approach shows that we can bypass this diffusion completely and work at the level of the probability flow with greater simplicity, opening an avenue for methods based solely on ordinary differential equations as an alternative to those based on stochastic differential equations. Benchmarking on density estimation tasks illustrates that the learned flow can match and surpass conventional continuous flows at a fraction of the cost, and compares well with diffusions on image generation on CIFAR-10 and ImageNet 32×32. The method scales ab-initio ODE flows to previously unreachable image resolutions, demonstrated up to 128 × 128.",
        "id":252668615
      },
      {
        "title":"HETERMPC: A Heterogeneous Graph Neural Network for Response Generation in Multi-Party Conversations",
        "text":"Recently, various response generation models for two-party conversations have achieved impressive improvements, but less effort has been paid to multi-party conversations (MPCs) which are more practical and complicated. Compared with a two-party conversation where a dialogue context is a sequence of utterances, building a response generation model for MPCs is more challenging, since there exist complicated context structures and the generated responses heavily rely on both interlocutors (i.e., speaker and addressee) and history utterances. To address these challenges, we present HeterMPC, a heterogeneous graph-based neural network for response generation in MPCs which models the semantics of utterances and interlocutors simultaneously with two types of nodes in a graph. Besides, we also design six types of meta relations with node-edge-typedependent parameters to characterize the heterogeneous interactions within the graph. Through multi-hop updating, HeterMPC can adequately utilize the structural knowledge of conversations for response generation. Experimental results on the Ubuntu Internet Relay Chat (IRC) channel benchmark show that HeterMPC outperforms various baseline models for response generation in MPCs.",
        "id":247476252
      },
      {
        "title":"",
        "text":"",
        "id":2533597
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which vision-language model can demonstrate that visual grounding could facilitate efficient language acquisition? (OctoBERT)",
    "positive_ctxs":[
      {
        "title":"World-to-Words: Grounded Open Vocabulary Acquisition through Fast Mapping in Vision-Language Models",
        "text":"The ability to connect language units to their referents in the physical world, referred to as grounding, is crucial to learning and understanding grounded meanings of words. While humans demonstrate fast mapping in new word learning, it remains unclear whether modern vision-language models can truly represent language with their grounded meanings, and how grounding may further bootstrap new word learning. To this end, we introduce Grounded Open Vocabulary Acquisition (GOVA) to examine grounding and bootstrapping in openworld language learning. As an initial attempt, we propose World-to-Words (W2W), a novel visually-grounded language model by pre-training on image-text pairs highlighting grounding as an objective. Through extensive experiments and analysis, we demonstrate that W2W is a more coherent and fast grounded word learner, and that the grounding ability acquired during pre-training helps the model to learn unseen words more rapidly and robustly.",
        "id":259165546
      }
    ],
    "negative_ctxs":[
      {
        "title":"SemEval-2007 Task 09: Multilevel Semantic Annotation of Catalan and Spanish",
        "text":"In this paper we describe SemEval-2007 task number 9 (Multilevel Semantic Annotation of Catalan and Spanish). In this task, we aim at evaluating and comparing automatic systems for the annotation of several semantic linguistic levels for Catalan and Spanish. Three semantic levels are considered: noun sense disambiguation, named entity recognition, and semantic role labeling.",
        "id":219307725
      },
      {
        "title":"Towards a standard evaluation method for grammatical error detection and correction",
        "text":"We present a novel evaluation method for grammatical error correction that addresses problems with previous approaches and scores systems in terms of improvement on the original text. Our method evaluates corrections at the token level using a globally optimal alignment between the source, a system hypothesis, and a reference. Unlike the M 2 Scorer, our method provides scores for both detection and correction and is sensitive to different types of edit operations.",
        "id":2723528
      },
      {
        "title":"New Developments in Ontological Semantics",
        "text":"In this paper we discuss ongoing activity within the approach to natural language processing known as ontological semantics, as defined in Nirenburg and Raskin (forthcoming). After a brief discussion of the principal tenets on which this approach is built, and a revision of extant implementations that have led toward its present form, we concentrate on some specific aspects that are key to the development of this approach, such as the acquisition of the semantics of lexical items and, intimately connected with this, the ontology, the central resource in this approach. Although we review the fundamentals of the approach, the focus is on practical aspects of implementation, such as the automation of static knowledge acquisition and the acquisition of scripts to enrich the ontology further.",
        "id":16658942
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which vision-language model paper in 2023 developed techniques that reduce input tokens to improve model inference speed?",
    "positive_ctxs":[
      {
        "title":"PuMer: Pruning and Merging Tokens for Efficient Vision Language Models",
        "text":"Large-scale vision language (VL) models use Transformers to perform cross-modal interactions between the input text and image. These cross-modal interactions are computationally expensive and memory-intensive due to the quadratic complexity of processing the input image and text. We present PuMer 1 : a token reduction framework that uses text-informed Pruning and modality-aware Merging strategies to progressively reduce the tokens of input image and text, improving model inference speed and reducing memory footprint. PuMer learns to keep salient image tokens related to the input text and merges similar textual and visual tokens by adding lightweight token reducer modules at several cross-modal layers in the VL model. Training PuMer is mostly the same as finetuning the original VL model but faster. Our evaluation for two vision language models on four downstream VL tasks shows PuMer increases inference throughput by up to 2x and reduces memory footprint by over 50% while incurring less than a 1% accuracy drop. 2",
        "id":258959382
      }
    ],
    "negative_ctxs":[
      {
        "title":"Protein Word Detection using Text Segmentation Techniques",
        "text":"Literature in Molecular Biology is abundant with linguistic metaphors. There have been works in the past that attempt to draw parallels between linguistics and biology, driven by the fundamental premise that proteins have a language of their own. Since word detection is crucial to the decipherment of any unknown language, we attempt to establish a problem mapping from natural language text to protein sequences at the level of words. Towards this end, we explore the use of an unsupervised text segmentation algorithm to the task of extracting \"biological words\" from protein sequences. In particular, we demonstrate the effectiveness of using domain knowledge to complement data driven approaches in the text segmentation task, as well as in its biological counterpart. We also propose a novel extrinsic evaluation measure for protein words through protein family classification.",
        "id":388111
      },
      {
        "title":"A New Framework for Sign Language Recognition based on 3D Handshape Identification and Linguistic Modeling",
        "text":"Current approaches to sign recognition by computer generally have at least some of the following limitations: they rely on laboratory conditions for sign production, are limited to a small vocabulary, rely on 2D modeling (and therefore cannot deal with occlusions and off-plane rotations), and\/or achieve limited success. Here we propose a new framework that (1) provides a new tracking method less dependent than others on laboratory conditions and able to deal with variations in background and skin regions (such as the face, forearms, or other hands); (2) allows for identification of 3D hand configurations that are linguistically important in American Sign Language (ASL); and (3) incorporates statistical information reflecting linguistic constraints in sign production. For purposes of large-scale computer-based sign language recognition from video, the ability to distinguish hand configurations accurately is critical. Our current method estimates the 3D hand configuration to distinguish among 77 hand configurations linguistically relevant for ASL. Constraining the problem in this way makes recognition of 3D hand configuration more tractable and provides the information specifically needed for sign recognition. Further improvements are obtained by incorporation of statistical information about linguistic dependencies among handshapes within a sign derived from an annotated corpus of almost 10,000 sign tokens.",
        "id":9217596
      },
      {
        "title":"Document Classification in Structured Military Messages",
        "text":"We present new results for the DSTO project on document classification of military messages. We report more specifically on the improvements to the Part-Of-Speech (POS) tagging, a probabilistic process that assigns a tag to a token, and discuss the training for Date Time Groups POS tags. A new implementation of the rule-based classifier is described. The results obtained on two databases of real military messages are encouraging and the document classification module has now been integrated with a query user interface.",
        "id":17994176
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which was the first paper to explore the online adaptation of neural MT metrics for use during the inference stage?",
    "positive_ctxs":[
      {
        "title":"Test-time Adaptation for Machine Translation Evaluation by Uncertainty Minimization",
        "text":"The neural metrics recently received considerable attention from the research community in the automatic evaluation of machine translation. Unlike text-based metrics that have interpretable and consistent evaluation mechanisms for various data sources, the reliability of neural metrics in assessing out-of-distribution data remains a concern due to the disparity between training data and real-world data. This paper aims to address the inference bias of neural metrics through uncertainty minimization during test time, without requiring additional data. Our proposed method comprises three steps: uncertainty estimation, test-time adaptation, and inference. Specifically, the model employs the prediction uncertainty of the current data as a signal to update a small fraction of parameters during test time and subsequently refine the prediction through optimization. To validate our approach, we apply the proposed method to three representative models and conduct experiments on the WMT21 benchmarks. The results obtained from both in-domain and out-of-distribution evaluations consistently demonstrate improvements in correlation performance across different models. Furthermore, we provide evidence that the proposed method effectively reduces model uncertainty. The code is publicly available at https:\/\/github.com\/NLP2CT\/TaU.",
        "id":259370785
      }
    ],
    "negative_ctxs":[
      {
        "title":"Rephrasing Profanity in Chinese Text",
        "text":"This paper proposes a system that can detect and rephrase profanity in Chinese text. Rather than just masking detected profanity, we want to revise the input sentence by using inoffensive words while keeping their original meanings. 29 of such rephrasing rules were invented after observing sentences on real-word social websites. The overall accuracy of the proposed system is 85.56%",
        "id":32076269
      },
      {
        "title":"",
        "text":"",
        "id":393794
      },
      {
        "title":"Investigating Advanced Techniques for Document Content Similarity Applied to External Plagiarism Analysis",
        "text":"We present an approach to perform external plagiarism analysis by applying several similarity detection techniques, such as lexical measures and a textual entailment recognition system developed by our research group. Some of the least expensive features of this system are applied to all corpus documents to detect those that are likely to be plagiarized. After this is done, the whole system is applied over this subset of documents to extract the exact n-grams that have been plagiarized, given that we now have less data to process and therefore can use a more complex and costly function. Apart from the application of strictly lexical measures, we also experiment with a textual entailment recognition system to detect plagiarisms with a high level of obfuscation. In addition, we experiment with the application of a spell corrector and a machine translation system to handle misspellings and plagiarisms translated into different languages, respectively.",
        "id":15268382
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which work discusses an analysis of source and target contributions to output generation based on local interpretation when machine translation models experience hallucinations?",
    "positive_ctxs":[
      {
        "title":"Local Interpretation of Transformer Based on Linear Decomposition",
        "text":"In recent years, deep neural networks (DNNs) have achieved state-of-the-art performance on a wide range of tasks. However, limitations in interpretability have hindered their applications in the real world. This work proposes to interpret neural networks by linear decomposition and finds that the ReLU-activated Transformer can be considered as a linear model on a single input. We further leverage the linearity of the model and propose a linear decomposition of the model output to generate local explanations. Our evaluation of sentiment classification and machine translation shows that our method achieves competitive performance in efficiency and fidelity of explanation. In addition, we demonstrate the potential of our approach in applications with examples of error analysis on multiple tasks. 1 * Corresponding author. 1  We release our algorithm toolkit at https:\/\/github. com\/DoubleVII\/pydec.",
        "id":259370787
      }
    ],
    "negative_ctxs":[
      {
        "title":"PERFECTLY SECURE STEGANOGRAPHY USING MINIMUM ENTROPY COUPLING",
        "text":"Steganography is the practice of encoding secret information into innocuous content in such a manner that an adversarial third party would not realize that there is hidden meaning.While this problem has classically been studied in security literature, recent advances in generative models have led to a shared interest among security and machine learning researchers in developing scalable steganography techniques.In this work, we show that a steganography procedure is perfectly secure under Cachin (1998)'s information-theoretic model of steganography if and only if it is induced by a coupling.Furthermore, we show that, among perfectly secure procedures, a procedure maximizes information throughput if and only if it is induced by a minimum entropy coupling.These insights yield what are, to the best of our knowledge, the first steganography algorithms to achieve perfect security guarantees for arbitrary covertext distributions.To provide empirical validation, we compare a minimum entropy coupling-based approach to three modern baselines-arithmetic coding, Meteor, and adaptive dynamic groupingusing GPT-2, WaveRNN, and Image Transformer as communication channels.We find that the minimum entropy coupling-based approach achieves superior encoding efficiency, despite its stronger security constraints.In aggregate, these results suggest that it may be natural to view information-theoretic steganography through the lens of minimum entropy coupling.",
        "id":253117027
      },
      {
        "title":"Improvement of Statistical Machine Translation using Charater- Based Segmentation with Monolingual and Bilingual Information",
        "text":"We present a novel segmentation approach for Phrase-Based Statistical Machine Translation (PB-SMT) to languages where word boundaries are not obviously marked by using both monolingual and bilingual information and demonstrate that (1) unsegmented corpus is able to provide the nearly identical result compares to manually segmented corpus in PB-SMT task when a good heuristic character clustering algorithm is applied on it, (2) the performance of PB-SMT task has significantly increased when bilingual information are used on top of monolingual segmented result. Our technique, instead of focusing on word separation, mainly concentrate on a group of character. First, we group several characters that reside in an unsegmented corpus by employing predetermined constraints and certain heuristics algorithms. Secondly, we enhance the segmented result by incorporating the character group repacking based on alignment confidence. We evaluate the effectiveness of our method on PB-SMT task using English-Thai, English-Lao and English-Burmese language pairs and report the best improvement of 8.1% increase in BLEU score on English-Thai pair.",
        "id":17919789
      },
      {
        "title":"",
        "text":"",
        "id":227905523
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"Which work proposes an approach to improve candidate responses in the smart reply task by directly optimizing the metric to ensure that a response is selected by the user?",
    "positive_ctxs":[
      {
        "title":"Model-Based Simulation for Optimising Smart Reply",
        "text":"Smart Reply (SR) systems present a user with a set of replies, of which one can be selected in place of having to type out a response. To perform well at this task, a system should be able to effectively present the user with a diverse set of options, to maximise the chance that at least one of them conveys the user's desired response. This is a significant challenge, due to the lack of datasets containing sets of responses to learn from. Resultantly, previous work has focused largely on post-hoc diversification, rather than explicitly learning to predict sets of responses. Motivated by this problem, we present a novel method SIMSR, that employs model-based simulation to discover high-value response sets, through simulating possible user responses with a learned world model. Unlike previous approaches, this allows our method to directly optimise the end-goal of SR-maximising the relevance of at least one of the predicted replies. Empirically on two public datasets, when compared to SoTA baselines, our method achieves up to 21% and 18% improvement in ROUGE score and Self-ROUGE score respectively.",
        "id":258947156
      }
    ],
    "negative_ctxs":[
      {
        "title":"Japanese Named Entity Recognition from Automatic Speech Recognition Using Pre-trained Models",
        "text":"Japanese named entities extracted from automatic speech recognition frequently contain speech recognition errors and unknown named entities due to abbreviations and aliases. One possible solution to this problem of the named entity extraction task is to use a pre-trained model trained on a large quantity of text to acquire various contextual information. In this study, we performed named entity recognition on the logs of a task-oriented dialogue system for road traffic information in Fukui, Japan, using pre-trained BERT-based models and T5. In our experiments using our prepared data, the F1 scores of BERT and T5 are higher than that of string match by 20.2 point and 21.1 points, respectively. The results confirmed that these pre-trained models exhibited significantly higher accuracies on unseen entities than methods based on dictionary matching.",
        "id":258463954
      },
      {
        "title":"EXPRES Corpus for A Field-specific Automated Exploratory Study of L2 English Expert Scientific Writing",
        "text":"Field Specific Expert Scientific Writing in English as a Lingua Franca is essential for the effective research networking and dissemination worldwide. Extracting the linguistic profile of the research articles written in L2 English can help young researchers and expert scholars in various disciplines adapt to the scientific writing norms of their communities of practice. In this exploratory study, we present and test an automated linguistic assessment model that includes features relevant for the cross-disciplinary second language framework: Text Complexity Analysis features, such as Syntactic and Lexical Complexity, and Field Specific Academic Word Lists. We analyse how these features vary across four disciplinary fields (Economics, IT, Linguistics and Political Science) in a corpus of L2-English Expert Scientific Writing, part of the EXPRES corpus (Corpus of Expert Writing in Romanian and English). The variation in field specific writing is also analysed in groups of linguistic features extracted from the higher visibility (Hv) versus lower visibility (Lv) journals. After applying lexical sophistication, lexical variation and syntactic complexity formulae, significant differences between disciplines were identified, mainly that research articles from Lv journals have higher lexical complexity, but lower syntactic complexity than articles from Hv journals; while academic vocabulary proved to have discipline specific variation.",
        "id":252460901
      },
      {
        "title":"Multi-Class Confidence Weighted Algorithms",
        "text":"The recently introduced online confidence-weighted (CW) learning algorithm for binary classification performs well on many binary NLP tasks. However, for multi-class problems CW learning updates and inference cannot be computed analytically or solved as convex optimization problems as they are in the binary case. We derive learning algorithms for the multi-class CW setting and provide extensive evaluation using nine NLP datasets, including three derived from the recently released New York Times corpus. Our best algorithm outperforms state-of-the-art online and batch methods on eight of the nine tasks. We also show that the confidence information maintained during learning yields useful probabilistic information at test time.",
        "id":11451209
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"what's the first paper that manages to handle KBQA using LLMs without fine-tuning?",
    "positive_ctxs":[
      {
        "title":"A Proposal for Grounding Language Models to Real-World Environments",
        "text":"A key missing capacity of current language models (LMs) is grounding to real-world environments. Most existing work for grounded language understanding uses LMs to directly generate plans that can be executed in the environment to achieve the desired effects. It thereby casts the burden of ensuring grammaticality, faithfulness, and controllability all on the LMs. We propose Pangu, a generic framework for grounded language understanding that capitalizes on the discriminative ability of LMs instead of their generative ability. Pangu consists of a symbolic agent and a neural LM working in a concerted fashion: The agent explores the environment to incrementally construct valid plans, and the LM evaluates the plausibility of the candidate plans to guide the search process. A case study on the challenging problem of knowledge base question answering (KBQA), which features a massive environment, demonstrates the remarkable effectiveness and flexibility of Pangu: A BERT-base LM is sufficient for setting a new record on standard KBQA datasets, and larger LMs further bring substantial gains. Pangu also enables, for the first time, effective few-shot in-context learning for KBQA with large LMs such as Codex. 1   . 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In . 2022. Program transfer for answering complex questions over knowledge bases. In . 2021a. Evaluating large language models trained on code. CoRR, abs\/2107.03374. . 2021b. ReTraCk: A flexible and efficient framework for knowledge base question answering. In . 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In . 2022. Holistic evaluation of language models. CoRR, abs\/2211.09110. 4939 Dhruv Shah, Błażej Osiński, brian ichter, and Sergey Levine. 2022. LM-Nav: Robotic navigation with large pre-trained models of language, vision, and action. . 2022. TIARA: Multi-grained retrieval for robust question answering over large knowledge base. In Proceed-. 2022b. LLM-Planner: Few-shot grounded planning for embodied agents with large language models. CoRR, abs\/2212.04088.",
        "id":254853929
      }
    ],
    "negative_ctxs":[
      {
        "title":"Anchors Regularized: Adding Robustness and Extensibility to Scalable Topic-Modeling Algorithms",
        "text":"Spectral methods offer scalable alternatives to Markov chain Monte Carlo and expectation maximization. However, these new methods lack the rich priors associated with probabilistic models. We examine Arora et al.'s anchor words algorithm for topic modeling and develop new, regularized algorithms that not only mathematically resemble Gaussian and Dirichlet priors but also improve the interpretability of topic models. Our new regularization approaches make these efficient algorithms more flexible; we also show that these methods can be combined with informed priors.",
        "id":216848275
      },
      {
        "title":"Augmenting Reddit Posts to Determine Wellness Dimensions impacting Mental Health",
        "text":"Amid ongoing health crisis, there is a growing necessity to discern possible signs of Wellness Dimensions (WD) 1 manifested in self-narrated text. As the distribution of WD on social media data is intrinsically imbalanced, we experiment the generative NLP models for data augmentation to enable further improvement in the prescreening task of classifying WD. To this end, we propose a simple yet effective data augmentation approach through promptbased Generative NLP models, and evaluate the ROUGE scores and syntactic\/semantic similarity among existing interpretations and augmented data. Our approach with ChatGPT model surpasses all the other methods and achieves improvement over baselines such as Easy-Data Augmentation and Backtranslation. Introducing data augmentation to generate more training samples and balanced dataset, results in the improved F-score and the Matthew's Correlation Coefficient for upto 13.11% and 15.95%, respectively.Wellness Dimensions",
        "id":259095669
      },
      {
        "title":"INFERES : A Natural Language Inference Corpus for Spanish Featuring Negation-Based Contrastive and Adversarial Examples",
        "text":"In this paper, we present INFERES -an original corpus for Natural Language Inference (NLI) in European Spanish. We propose, implement, and analyze a variety of corpuscreating strategies utilizing expert linguists and crowd workers. The objectives behind IN-FERES are to provide high-quality data, and, at the same time to facilitate the systematic evaluation of automated systems. Specifically, we focus on measuring and improving the performance of machine learning systems on negation-based adversarial examples and their ability to generalize across out-of-distribution topics.We train two transformer models on IN-FERES (8,055 gold examples) in a variety of scenarios. Our best model obtains 72.8% accuracy, leaving a lot of room for improvement. The \"hypothesis-only\" baseline performs only 2%-5% higher than majority, indicating much fewer annotation artifacts than prior work. We find that models trained on INFERES generalize very well across topics (both in-and outof-distribution) and perform moderately well on negation-based adversarial examples.",
        "id":252735141
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_acl",
    "question":"which paper first focuses on addressing the over-smoothing issue for sentence embedding?",
    "positive_ctxs":[
      {
        "title":"Alleviating Over-smoothing for Unsupervised Sentence Representation",
        "text":"Currently, learning better unsupervised sentence representations is the pursuit of many natural language processing communities. Lots of approaches based on pre-trained language models (PLMs) and contrastive learning have achieved promising results on this task. Experimentally, we observe that the over-smoothing problem reduces the capacity of these powerful PLMs, leading to suboptimal sentence representations. In this paper, we present a Simple method named Self-Contrastive Learning (SSCL) to alleviate this issue, which samples negatives from PLMs intermediate layers, improving the quality of the sentence representation. Our proposed method is quite simple and can be easily extended to various state-of-the-art models for performance boosting, which can be seen as a plugand-play contrastive framework for learning unsupervised sentence representation. Extensive results prove that SSCL brings the superior performance improvements of different strong baselines (e.g., BERT and SimCSE) on Semantic Textual Similarity and Transfer datasets. Our codes are available at https: \/\/github.com\/nuochenpku\/SSCL.",
        "id":258588364
      }
    ],
    "negative_ctxs":[
      {
        "title":"Perception and Analysis of a Reiterant Speech Paradigm: a Functional Diagnostic of Synthetic Prosody",
        "text":"A set of perception experiments, using reiterant speech, were designed to carry out a diagnostic of the segmentation \/ hierarchisation linguistic function of prosody. The prosodic parameters of F0, syllabic duration and intensity of the stimuli used during this experiment were extracted. Several dissimilarity measures (Correlation, root-mean-square distance and mutual information) were used to match the results of the subjective experiment. This comparison of the listeners' perception with acoustic parameters is intended to underline the acoustic keys used by listeners to judge the adequacy of prosody to perform a given linguistic function.",
        "id":69859
      },
      {
        "title":"Learning with AMIGO: Adversarially Motivated Intrinsic Goals",
        "text":"A key challenge for reinforcement learning (RL) consists of learning in environments with sparse extrinsic rewards. In contrast to current RL methods, humans are able to learn new skills with little or no reward by using various forms of intrinsic motivation. We propose AMIGO, a novel agent incorporating a goalgenerating teacher that proposes Adversarially Motivated Intrinsic GOals to train a goal-conditioned \"student\" policy in the absence of (or alongside) environment reward. Specifically, through a simple but effective \"constructively adversarial\" objective, the teacher learns to propose increasingly challenging-yet achievablegoals that allow the student to learn general skills for acting in a new environment, independent of the task to be solved. We show that our method generates a natural curriculum of self-proposed goals which ultimately allows the agent to solve challenging procedurally-generated tasks where other forms of intrinsic motivation and state-of-the-art RL methods fail. * Work done during an internship at Facebook AI Research.",
        "id":219965999
      },
      {
        "title":"",
        "text":"",
        "id":227905391
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Can we reduce visual tokens in vision transformers right from the beginning?",
    "positive_ctxs":[
      {
        "title":"SparseFormer: Sparse Visual Recognition via Limited Latent Tokens",
        "text":"Human visual recognition is a sparse process, where only a few salient visual cues are attended to rather than traversing every detail uniformly.However, most current vision networks follow a dense paradigm, processing every single visual unit (e.g., pixel or patch) in a uniform manner.In this paper, we challenge this dense paradigm and present a new method, coined SparseFormer, to imitate human's sparse visual recognition in an end-to-end manner.Sparse-Former learns to represent images using a highly limited number of tokens (down to 49) in the latent space with sparse feature sampling procedure instead of processing dense units in the original pixel space.Therefore, Sparse-Former circumvents most of dense operations on the image space and has much lower computational costs.Experiments on the ImageNet classification benchmark dataset show that SparseFormer achieves performance on par with canonical or well-established models while offering better accuracy-throughput tradeoff.Moreover, the design of our network can be easily extended to the video classification with promising performance at lower computational costs.We hope that our work can provide an alternative way for visual modeling and inspire further research on sparse neural architectures.The code will be publicly available at https:\/\/github.com\/showlab\/sparseformer.",
        "id":258041281
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":194737667
      },
      {
        "title":"Distributed Listening: A Parallel Processing Approach to Automatic Speech Recognition",
        "text":"While speech recognition systems have come a long way in the last thirty years, there is still room for improvement. Although readily available, these systems are sometimes inaccurate and insufficient. The research presented here outlines a technique called Distributed Listening which demonstrates noticeable improvements to existing speech recognition methods. The Distributed Listening architecture introduces the idea of multiple, parallel, yet physically separate automatic speech recognizers called listeners. Distributed Listening also uses a piece of middleware called an interpreter. The interpreter resolves multiple interpretations using the Phrase Resolution Algorithm (PRA). These efforts work together to increase the accuracy of the transcription of spoken utterances.",
        "id":11611001
      },
      {
        "title":"Generating Natural Sentences by Using Shallow Discourse Information",
        "text":"One of the biggest defects of natural language generation systems is that the output sentences are unnatural and contain many redundancies. Machine translation (MT) users, for instance, often get tired of reading the output of MT because of this problem. In this paper, we summarize the results of our analysis of human translation in terms of the use of discourse information to generate target-language sentences, and describe our attempt to generate natural output by referring to a simple discourse model that consists of a syntactic parser's output for each sentence in the discourse.",
        "id":16619607
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Can we learn to represent an image with arbitary numbers of tokens?",
    "positive_ctxs":[
      {
        "title":"SparseFormer: Sparse Visual Recognition via Limited Latent Tokens",
        "text":"Human visual recognition is a sparse process, where only a few salient visual cues are attended to rather than traversing every detail uniformly.However, most current vision networks follow a dense paradigm, processing every single visual unit (e.g., pixel or patch) in a uniform manner.In this paper, we challenge this dense paradigm and present a new method, coined SparseFormer, to imitate human's sparse visual recognition in an end-to-end manner.Sparse-Former learns to represent images using a highly limited number of tokens (down to 49) in the latent space with sparse feature sampling procedure instead of processing dense units in the original pixel space.Therefore, Sparse-Former circumvents most of dense operations on the image space and has much lower computational costs.Experiments on the ImageNet classification benchmark dataset show that SparseFormer achieves performance on par with canonical or well-established models while offering better accuracy-throughput tradeoff.Moreover, the design of our network can be easily extended to the video classification with promising performance at lower computational costs.We hope that our work can provide an alternative way for visual modeling and inspire further research on sparse neural architectures.The code will be publicly available at https:\/\/github.com\/showlab\/sparseformer.",
        "id":258041281
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":227231721
      },
      {
        "title":"Thirumurai: A Large Dataset of Tamil Shaivite Poems and Classification of Tamil Pann",
        "text":"Thirumurai, also known as Panniru Thirumurai, is a collection of Tamil Shaivite poems dating back to the Hindu revival period between the 6th and the 10th century. These poems are par excellence, in both literary and musical terms. They have been composed based on the ancient, now non-existent Tamil Pann system and can be set to music. We present a large dataset containing all the Thirumurai poems and also attempt to classify the Pann and author of each poem using transformer based architectures. Our work is the first of its kind in dealing with ancient Tamil text datasets, which are severely under-resourced. We explore several Deep Learning-based techniques for solving this challenge effectively and provide essential insights into the problem and how to address it.",
        "id":250164247
      },
      {
        "title":"SINAI: Machine Learning and Emotion of the Crowd for Sentiment Analysis in Microblogs",
        "text":"This paper describes the participation of the SINAI research group in the 2013 edition of the International Workshop Se-mEval. The SINAI research group has submitted two systems, which cover the two main approaches in the field of sentiment analysis: supervised and unsupervised.",
        "id":8954558
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Are there any papers that construct convolutional networks which are equivariant with respect to non-compact\/non-abelian Lie groups?",
    "positive_ctxs":[
      {
        "title":"LIE GROUP DECOMPOSITIONS FOR EQUIVARIANT NEURAL NETWORKS",
        "text":"Invariance and equivariance to geometrical transformations have proven to be very useful inductive biases when training (convolutional) neural network models, especially in the low-data regime. Much work has focused on the case where the symmetry group employed is compact or abelian, or both. Recent work has explored enlarging the class of transformations used to the case of Lie groups, principally through the use of their Lie algebra, as well as the group exponential and logarithm maps. The applicability of such methods to larger transformation groups is limited by the fact that depending on the group of interest G, the exponential map may not be surjective. Further limitations are encountered when G is neither compact nor abelian. Using the structure and geometry of Lie groups and their homogeneous spaces, we present a framework by which it is possible to work with such groups primarily focusing on the Lie groups G = GL + (n, R) and G = SL(n, R), as well as their representation as affine transformations R n ⋊ G. Invariant integration as well as a global parametrization is realized by decomposing the 'larger' groups into subgroups and submanifolds which can be handled individually. Under this framework, we show how convolution kernels can be parametrized to build models equivariant with respect to affine transformations. We evaluate the robustness and out-of-distribution generalisation capability of our model on the standard affine-invariant benchmark classification task, where we outperform all previous equivariant models as well as all Capsule Network proposals. . Geometric means in a novel vector space structure on symmetric positive-definite matrices. SIAM journal on matrix analysis and applications, 29(1): 328-347, 2007.",
        "id":264172845
      }
    ],
    "negative_ctxs":[
      {
        "title":"Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation",
        "text":"We present an easy and efficient method to extend existing sentence embedding models to new languages. This allows to create multilingual versions from previously monolingual models. The training is based on the idea that a translated sentence should be mapped to the same location in the vector space as the original sentence. We use the original (monolingual) model to generate sentence embeddings for the source language and then train a new system on translated sentences to mimic the original model. Compared to other methods for training multilingual sentence embeddings, this approach has several advantages: It is easy to extend existing models with relatively few samples to new languages, it is easier to ensure desired properties for the vector space, and the hardware requirements for training is lower. We demonstrate the effectiveness of our approach for 10 languages from various language families. Code to extend sentence embeddings models to more than 400 languages is publicly available. 1",
        "id":216036089
      },
      {
        "title":"Learning Input Strictly Local Functions: Comparing Approaches with Catalan Adjectives",
        "text":"",
        "id":248182527
      },
      {
        "title":"",
        "text":"",
        "id":1801525
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Are there any papers that study whether you can identify if a LLM has been instructed to hide some information?",
    "positive_ctxs":[
      {
        "title":"HOW TO CATCH AN AI LIAR: LIE DETECTION IN BLACK-BOX LLMS BY ASKING UNRELATED QUESTIONS",
        "text":"Large language models (LLMs) can \"lie\", which we define as outputting false statements despite \"knowing\" the truth in a demonstrable sense.LLMs might \"lie\", for example, when instructed to output misinformation.Here, we develop a simple lie detector that requires neither access to the LLM's activations (black-box) nor ground-truth knowledge of the fact in question.The detector works by asking a predefined set of unrelated follow-up questions after a suspected lie, and feeding the LLM's yes\/no answers into a logistic regression classifier.Despite its simplicity, this lie detector is highly accurate and surprisingly general.When trained on examples from a single setting-prompting GPT-3.5 to lie about factual questionsthe detector generalises out-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie, (3) sycophantic lies, and (4) lies emerging in real-life scenarios such as sales.These results indicate that LLMs have distinctive lierelated behavioural patterns, consistent across architectures and contexts, which could enable general-purpose lie detection.",
        "id":263152829
      }
    ],
    "negative_ctxs":[
      {
        "title":"Unsupervised AMR-Dependency Parse Alignment",
        "text":"In this paper, we introduce an Abstract Meaning Representation (AMR) to Dependency Parse aligner. Alignment is a preliminary step for AMR parsing, and our aligner improves current AMR parser performance. Our aligner involves several different features, including named entity tags and semantic role labels, and uses Expectation-Maximization training. Results show that our aligner reaches an 87.1% F-Score score with the experimental data, and enhances AMR parsing.",
        "id":7514610
      },
      {
        "title":"Lexical Knowledge Representation with Contexonyms",
        "text":"Inter-word associations like stagger -drunken, or intra-word sense divisions (e.g. write a diary vs. write an article) are difficult to compile using a traditional lexicographic approach. As an alternative, we present a model that reflects this kind of subtle lexical knowledge. Based on the minimal sense of a word (clique), the model (1) selects contextually related words (contexonyms) and(2)classifies them in a multi-dimensional semantic space. Trained on very large corpora, the model provides relevant, organized contexonyms that reflect the fine-grained connotations and contextual usage of the target word, as well as the distinct senses of homonyms and polysemous words. Further study on the neighbor effect showed that the model can handle the data sparseness problemstagger -drunken, which could be informative for non-English speakers or machines, are too numerous to be processed. Intra-word relations share this problem: while the English word write is considered to have the same semantic value in \"write a diary\" and \"write an article\", the French wordsécrire and rédiger, respectively, are widely used in these two phrases. This sort of sense division is also too minute and too frequent to be captured using conventional manual lexicography techniques.An alternative would therefore be to automatically generate the related words for a given word, which could serve as a reference. Clearly, contextually related words are meaningful indicators of the target word's semantic value in a given context. For instance, two sets of words { lit, candle, cigarette } and { tennis, final, win } are trustworthy cue-word sets for disambiguating the word match; stupid is more closely related to blunder than to error(Edmonds and Hirst, 2002), and peace distinguishes treaty from contract(Dagan and Itai, 1994).Such word lists may be obtained for target words by selecting seed words and performing an iterative, decision-list-making task(Yarowsky, 1995), or by latent semantic indexing (LSI)(Landauer et al., 1998). A common limitation of these approaches, however, is that they do not provide a fully automatic method for organizing the related words obtained: identifying seed words needs human intervention and LSI does not provide an automatic classification other than a restricted matching-based one that requires an encyclopedia as a source text(Laham, 1997).",
        "id":13512315
      },
      {
        "title":"Classification-based Quality Estimation: Small and Efficient Models for Real-world Applications",
        "text":"Sentence-level Quality Estimation (QE) of machine translation is traditionally formulated as a regression task, and the performance of QE models is typically measured by Pearson correlation with human labels. Recent QE models have achieved previously-unseen levels of correlation with human judgments, but they rely on large multilingual contextualized language models that are computationally expensive and thus infeasible for many real-world applications. In this work, we evaluate several model compression techniques for QE and find that, despite their popularity in other NLP tasks, they lead to poor performance in this regression setting. We observe that a full model parameterization is required to achieve SoTA results in a regression task. However, we argue that the level of expressiveness of a model in a continuous range is unnecessary given the downstream applications of QE, and show that reframing QE as a classification problem and evaluating QE models using classification metrics would better reflect their actual performance in real-world applications.",
        "id":237563027
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Are there any papers that use a world model for planning to ensure that decisions meet constraints?",
    "positive_ctxs":[
      {
        "title":"SAFEDREAMER: SAFE REINFORCEMENT LEARNING WITH WORLD MODELS",
        "text":"The deployment of Reinforcement Learning (RL) in real-world applications is constrained by its failure to satisfy safety criteria.Existing Safe Reinforcement Learning (SafeRL) methods, which rely on cost functions to enforce safety, often fail to achieve zero-cost performance in complex scenarios, especially vision-only tasks.These limitations are primarily due to model inaccuracies and inadequate sample efficiency.The integration of world models has proven effective in mitigating these shortcomings.In this work, we introduce SafeDreamer, a novel algorithm incorporating Lagrangian-based methods into world model planning processes within the superior Dreamer framework.Our method achieves nearly zero-cost performance on various tasks, spanning low-dimensional and vision-only input, within the Safety-Gymnasium benchmark, showcasing its efficacy in balancing performance and safety in RL tasks.Further details and resources are available on the project website: https:\/\/sites.google.com\/view\/safedreamer.",
        "id":259924554
      }
    ],
    "negative_ctxs":[
      {
        "title":"Transition-based Semantic Dependency Parsing with Pointer Networks",
        "text":"Transition-based parsers implemented with Pointer Networks have become the new state of the art in dependency parsing, excelling in producing labelled syntactic trees and outperforming graph-based models in this task. In order to further test the capabilities of these powerful neural networks on a harder NLP problem, we propose a transition system that, thanks to Pointer Networks, can straightforwardly produce labelled directed acyclic graphs and perform semantic dependency parsing. In addition, we enhance our approach with deep contextualized word embeddings extracted from BERT. The resulting system not only outperforms all existing transitionbased models, but also matches the best fullysupervised accuracy to date on the SemEval 2015 Task 18 English datasets among previous state-of-the-art graph-based parsers.",
        "id":218900899
      },
      {
        "title":"INTEGRATING SEMANTICS kNO FLEXIBLE SYNTAX BY EXPLOITING ISONORPHISM BETWEEN GRAIelATICAL AND SEMANTICAL RELATIONS",
        "text":"This work concerns integration between syntax and semantics. Syntactic and semantic activities rely on separate bodies of knowledges. Integration is obtained by exploiting the isomorphism between grammatical relations (among immediate constitu~ errs) and conceptual relations, thanks to a limited set of formal mapping rules. Syntactic analysis does not construct all the explicit parse trees but just a graph that represents all the plausible grammatical relations among immediate constituents. Such graph gives the semantic interpreter, based on Conceptual Graphs formalism, the discriminative power required to establish conceptual relations.",
        "id":2693928
      },
      {
        "title":"Abstract Meaning Representation Guided Graph Encoding and Decoding for Joint Information Extraction",
        "text":"The tasks of Rich Semantic Parsing, such as Abstract Meaning Representation (AMR), share similar goals with Information Extraction (IE) to convert natural language texts into structured semantic representations. To take advantage of such similarity, we propose a novel AMR-guided framework for joint information extraction to discover entities, relations, and events with the help of a pre-trained AMR parser. Our framework consists of two novel components: 1) an AMR based semantic graph aggregator to let the candidate entity and event trigger nodes collect neighborhood information from AMR graph for passing message among related knowledge elements; 2) an AMR guided graph decoder to extract knowledge elements based on the order decided by the hierarchical structures in AMR. Experiments on multiple datasets have shown that the AMR graph encoder and decoder have provided significant gains and our approach has achieved new state-of-the-art performance on all IE subtasks 1 . have-anrel-role-91 wife son :ARG0-of :ARG1-of :ARG1 :ARG2 :ARG2 \"Scott Peterson\" house :location have-anrel-role-91",
        "id":235097301
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Are there datasets and benchmarks available for measuring LLM graph reasoning abilities?",
    "positive_ctxs":[
      {
        "title":"TALK LIKE A GRAPH: ENCODING GRAPHS FOR LARGE LANGUAGE MODELS",
        "text":"Graphs are a powerful tool for representing and analyzing complex relationships in real-world applications such as social networks, recommender systems, and computational finance.Reasoning on graphs is essential for drawing inferences about the relationships between entities in a complex system, and to identify hidden patterns and trends.Despite the remarkable progress in automated reasoning with natural text, reasoning on graphs with large language models (LLMs) remains an understudied problem.In this work, we perform the first comprehensive study of encoding graph-structured data as text for consumption by LLMs.We show that LLM performance on graph reasoning tasks varies on three fundamental levels:(1) the graph encoding method, (2) the nature of the graph task itself, and (3) interestingly, the very structure of the graph considered.These novel results provide valuable insight on strategies for encoding graphs as text.Using these insights we illustrate how the correct choice of encoders can boost performance on graph reasoning tasks inside LLMs by 4.8% to 61.8%, depending on the task.",
        "id":263829977
      }
    ],
    "negative_ctxs":[
      {
        "title":"Structural Methods for Lexical\/Semantic Patterns",
        "text":"This paper represents initial work on corpus methods for acquiring lexical\/semantic pattern lexicons for text understanding. Recently, implementors of information extraction (IE) systems have moved away from using conventional syntac-",
        "id":1442452
      },
      {
        "title":"Analyzing text in search of bio-molecular events: a high-precision machine learning framework",
        "text":"The BioNLP'09 Shared Task on Event Extraction is a challenge which concerns the detection of bio-molecular events from text. In this paper, we present a detailed account of the challenges encountered during the construction of a machine learning framework for participation in this task. We have focused our work mainly around the filtering of false positives, creating a high-precision extraction method. We have tested techniques such as SVMs, feature selection and various filters for data pre-and post-processing, and report on the influence on performance for each of them. To detect negation and speculation in text, we describe a custom-made rule-based system which is simple in design, but effective in performance.",
        "id":2910140
      },
      {
        "title":"Sequential Clustering and Contextual Importance Measures for Incremental Update Summarization",
        "text":"Unexpected events such as accidents, natural disasters and terrorist attacks represent an information situation where it is crucial to give users access to important and non-redundant information as early as possible. Previous work uses either a fast but inaccurate pipeline approach or a precise but slow clustering approach. Instead, we propose to use sequential clustering for grouping information so that we are able to publish sentences at each time step. By doing so, we combine the best of both clustering and pipeline approaches and create a fast and precise real-time system. Experiments on the TREC Temporal Summarization 2015 shared task dataset show that our system achieves better results compared to the state-of-the-art.This work is licenced under a Creative Commons Attribution 4.0 International License. License details: http:\/\/ creativecommons.org\/licenses\/by\/4.0\/",
        "id":13090942
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Are there sequential learning guarantees for configuring a linear system solver under a distributional assumption on the systems' target vectors?",
    "positive_ctxs":[
      {
        "title":"LEARNING TO RELAX: SETTING SOLVER PARAMETERS ACROSS A SEQUENCE OF LINEAR SYSTEM INSTANCES",
        "text":"Solving a linear system Ax = b is a fundamental scientific computing primitive for which numerous solvers and preconditioners have been developed.These come with parameters whose optimal values depend on the system being solved and are often impossible or too expensive to identify; thus in practice sub-optimal heuristics are used.We consider the common setting in which many related linear systems need to be solved, e.g. during a single numerical simulation.In this scenario, can we sequentially choose parameters that attain a near-optimal overall number of iterations, without extra matrix computations?We answer in the affirmative for Successive Over-Relaxation (SOR), a standard solver whose parameter ω has a strong impact on its runtime.For this method, we prove that a bandit online learning algorithm-using only the number of iterations as feedback-can select parameters for a sequence of instances such that the overall cost approaches that of the best fixed ω as the sequence length increases.Furthermore, when given additional structural information, we show that a contextual bandit method asymptotically achieves the performance of the instance-optimal policy, which selects the best ω for each instance.Our work provides the first learning-theoretic treatment of high-precision linear system solvers and the first end-to-end guarantees for data-driven scientific computing, demonstrating theoretically the potential to speed up numerical methods using well-understood learning algorithms.",
        "id":263609239
      }
    ],
    "negative_ctxs":[
      {
        "title":"Employing Topic Models for Pattern-based Semantic Class Discovery",
        "text":"A semantic class is a collection of items (words or phrases) which have semantically peer or sibling relationship. This paper studies the employment of topic models to automatically construct semantic classes, taking as the source data a collection of raw semantic classes (RASCs), which were extracted by applying predefined patterns to web pages. The primary requirement (and challenge) here is dealing with multi-membership: An item may belong to multiple semantic classes; and we need to discover as many as possible the different semantic classes the item belongs to. To adopt topic models, we treat RASCs as \"documents\", items as \"words\", and the final semantic classes as \"topics\". Appropriate preprocessing and postprocessing are performed to improve results quality, to reduce computation cost, and to tackle the fixed-k constraint of a typical topic model. Experiments conducted on 40 million web pages show that our approach could yield better results than alternative approaches. Countries, and U.S. states. In the second step, each item is assigned a label of \"Good\", \"Fair\", or \"Bad\" with respect to each SSC. For example, \"silver\" is labeled \"Good\" with respect to \"colors\" and \"chemical elements\". We adopt metric MnDCG (Section 4.2) as our evaluation metric.Approaches for comparison: We compare our approach with the alternative approaches discussed in Section 3.4.2.LDA: Our approach with LDA as the topic model. The implementation of LDA is based on Blei's code of variational EM for LDA 5 . pLSI: Our approach with pLSI as the topic model. The implementation of pLSI is based onSchein, et al. (2002).KMedoids-RASC:The RASC clustering approach illustrated in Section 3.4.2, with the K-Medoids clustering algorithm utilized.DBSCAN-RASC:The RASC clustering approach with DBSCAN utilized.KMedoids-Item:The item clustering approach with the K-Medoids utilized.DBSCAN-Item:The item clustering approach with the DBSCAN clustering algorithm utilized.",
        "id":9470648
      },
      {
        "title":"Genre Separation Network with Adversarial Training for Cross-genre Relation Extraction",
        "text":"Relation Extraction suffers from dramatical performance decrease when training a model on one genre and directly applying it to a new genre, due to the distinct feature distributions. Previous studies address this problem by discovering a shared space across genres using manually crafted features, which requires great human effort. To effectively automate this process, we design a genre-separation network, which applies two encoders, one genreindependent and one genre-shared, to explicitly extract genre-specific and genre-agnostic features. Then we train a relation classifier using the genre-agnostic features on the source genre and directly apply to the target genre. Experiment results on three distinct genres of the ACE dataset show that our approach achieves up to 6.1% absolute F1-score gain compared to previous methods. By incorporating a set of external linguistic features, our approach outperforms the state-of-the-art by 1.7% absolute F1 gain. We make all programs of our model publicly available for research purpose 1 .",
        "id":53080778
      },
      {
        "title":"FINITE-STATE APPROXIMATION OF PHRASE STRUCTURE GRAMMARS",
        "text":"Phrase-structure grammars are an effective representation for important syntactic and semantic aspects of natural languages, but are computationally too demanding for use as language models in real-time speech recognition. An algorithm is described that computes finite-state approximations for context-free grammars and equivalent augmented phrase-structure grammar formalisms. The approximation is exact for certain contextfree grammars generating regular languages, including all left-linear and right-linear context-free grammars. The algorithm has been used to construct finite-state language models for limiteddomain speech recognition tasks.",
        "id":5094703
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Can we find the solution of the Bilevel optimization when the lower-level problem is nonconvex?",
    "positive_ctxs":[
      {
        "title":"On Penalty Methods for Nonconvex Bilevel Optimization and First-Order Stochastic Approximation",
        "text":"In this work, we study first-order algorithms for solving Bilevel Optimization (BO) where the objective functions are smooth but possibly nonconvex in both levels and the variables are restricted to closed convex sets. As a first step, we study the landscape of BO through the lens of penalty methods, in which the upper-and lower-level objectives are combined in a weighted sum with penalty parameter σ > 0. In particular, we establish a strong connection between the penalty function and the hyper-objective by explicitly characterizing the conditions under which the values and derivatives of the two must be O(σ)-close. A by-product of our analysis is the explicit formula for the gradient of hyper-objective when the lower-level problem has multiple solutions under minimal conditions, which could be of independent interest. Next, viewing the penalty formulation as O(σ)-approximation of the original BO, we propose firstorder algorithms that find an ϵ-stationary solution by optimizing the penalty formulation with σ = O(ϵ). When the perturbed lower-level problem uniformly satisfies the small-error proximal error-bound (EB) condition, we propose a first-order algorithm that converges to an ϵ-stationary point of the penalty function, using in total O(ϵ −3 ) and O(ϵ −7 ) accesses to first-order (stochastic) gradient oracles when the oracle is deterministic and oracles are noisy, respectively. Under an additional assumption on stochastic oracles, we show that the algorithm can be implemented in a fully single-loop manner, i.e., with O(1) samples per iteration, and achieves the improved oracle-complexity of O(ϵ −3 ) and O(ϵ −5 ), respectively.",
        "id":261530996
      }
    ],
    "negative_ctxs":[
      {
        "title":"Domain Specific Word Extraction from Hierarchical Web Documents: A First Step Toward Building Lexicon Trees from Web Corpora",
        "text":"Domain specific words and ontological information among words are important resources for general natural language applications. This paper proposes a statistical model for finding domain specific words (DSW s) in particular domains, and thus building the association among them. When applying this model to the hierarchical structure of the web directories node-by-node, the document tree can potentially be converted into a large semantically annotated lexicon tree. Some preliminary results show that the current approach is better than a conventional TF-IDF approach for measuring domain specificity. An average precision of 65.4% and an average recall of 36.3% are observed if the top-10% candidates are extracted as domain-specific words.",
        "id":15313951
      },
      {
        "title":"A Syntax-Aware Edit-based System for Text Simplification",
        "text":"Edit-based text simplification systems have attained much attention in recent years due to their ability to produce simplification solutions that are interpretable, as well as requiring less training examples compared to traditional seq2seq systems. Edit-based systems learn edit operations at a word level, but it is well known that many of the operations performed when simplifying text are of a syntactic nature. In this paper we propose to add syntactic information into a well known editbased system. We extend the system with a graph convolutional network module that mimics the dependency structure of the sentence, thus giving the model an explicit representation of syntax. We perform a series of experiments in English, Spanish and Italian, and report improvements of the state of the art in four out of five datasets. Further analysis shows that syntactic information is always beneficial, and suggest that syntax is more helpful in complex sentences.",
        "id":243891435
      },
      {
        "title":"",
        "text":"Mental health is getting more and more attention recently, depression being a very common illness nowadays, but also other disorders like anxiety, obsessive-compulsive disorders, feeding disorders, autism, or attention-deficit\/hyperactivity disorders. The huge amount of data from social media and the recent advances of deep learning models provide valuable means to automatically detecting mental disorders from plain text. In this article, we experiment with state-of-the-art methods on the SMHD mental health conditions dataset from Reddit (Cohan et al.,  2018). Our contribution is threefold: using a dataset consisting of more illnesses than most studies, focusing on general text rather than mental health support groups and classification by posts rather than individuals or groups. For the automatic classification of the diseases, we employ three deep learning models: BERT, RoBERTa and XLNET. We double the baseline established byCohan et al. (2018), on just a sample of their dataset. We improve the results obtained by Jiang et al. (2020) on post-level classification. The accuracy obtained by the eating disorder classifier is the highest due to the pregnant presence of discussions related to calories, diets, recipes etc., whereas depression had the lowest F1 score, probably because depression is more difficult to identify in linguistic acts.UEFISCDI, project number 108, COTOHILI, within PNCDI III.",
        "id":243907938
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_iclr",
    "question":"Can you find a dataset that shows LLM-based evaluation may not be reliable enough?",
    "positive_ctxs":[
      {
        "title":"EVALUATING LARGE LANGUAGE MODELS AT EVALUATING INSTRUCTION FOLLOWING",
        "text":"As research in large language models (LLMs) continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evaluations for comparing the ever increasing list of models. This paper investigates the efficacy of these \"LLM evaluators\", particularly in using them to assess instruction following, a metric that gauges how closely generated text adheres to the given instruction. We introduce a challenging meta-evaluation benchmark, LLM-BAR, designed to test the ability of an LLM evaluator in discerning instructionfollowing outputs. The authors manually curated 419 pairs of outputs, one adhering to instructions while the other diverging, yet may possess deceptive qualities that mislead an LLM evaluator, e.g., a more engaging tone. Contrary to existing meta-evaluation, we discover that different evaluators (i.e., combinations of LLMs and prompts) exhibit distinct performance on LLMBAR and even the highestscoring ones have substantial room for improvement. We also present a novel suite of prompting strategies that further close the gap between LLM and human evaluators. With LLMBAR, we hope to offer more insight into LLM evaluators and foster future research in developing better instruction-following models.",
        "id":263834884
      }
    ],
    "negative_ctxs":[
      {
        "title":"Towards General Natural Language Understanding with Probabilistic Worldbuilding",
        "text":"We introduce the Probabilistic Worldbuilding Model (PWM), a new fully symbolic Bayesian model of semantic parsing and reasoning, as a first step in a research program toward more domain-and task-general NLU and AI. Humans create internal mental models of their observations that greatly aid in their ability to understand and reason about a large variety of problems. In PWM, the meanings of sentences, acquired facts about the world, and intermediate steps in reasoning are all expressed in a human-readable formal language, with the design goal of interpretability. PWM is Bayesian, designed specifically to be able to generalize to new domains and new tasks. We derive and implement an inference algorithm that reads sentences by parsing and abducing updates to its latent world model that capture the semantics of those sentences, and evaluate it on two out-of-domain question-answering datasets:(1) ProofWriter and (2) a new dataset we call FictionalGeoQA, designed to be more representative of real language but still simple enough to focus on evaluating reasoning ability, while being robust against heuristics. Our method outperforms baselines on both, thereby demonstrating its value as a proof-of-concept.325",
        "id":245353941
      },
      {
        "title":"",
        "text":"",
        "id":235097583
      },
      {
        "title":"What Does BERT with Vision Look At?",
        "text":"Pre-trained visually grounded language models such as ViLBERT, LXMERT, and UNITER have achieved significant performance improvement on vision-and-language tasks but what they learn during pre-training remains unclear. In this work, we demonstrate that certain attention heads of a visually grounded language model actively ground elements of language to image regions. Specifically, some heads can map entities to image regions, performing the task known as entity grounding. Some heads can even detect the syntactic relations between non-entity words and image regions, tracking, for example, associations between verbs and regions corresponding to their arguments. We denote this ability as syntactic grounding. We verify grounding both quantitatively and qualitatively, using Flickr30K Entities as a testbed. . 2015. VQA: Visual question answering. In ICCV.Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C Lawrence Zitnick. 2015. Microsoft COCO captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325.",
        "id":218610661
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Can you find a research paper that discusses using structured pruning techniques to scale down language models, where the original model being pruned has billions of parameters?",
    "positive_ctxs":[
      {
        "title":"SHEARED LLAMA: ACCELERATING LANGUAGE MODEL PRE-TRAINING VIA STRUCTURED PRUNING",
        "text":"The popularity of LLaMA(Touvron et al., 2023a;b)  and other recently emerged moderate-sized large language models (LLMs) highlights the potential of building smaller yet powerful LLMs.Regardless, the cost of training such models from scratch on trillions of tokens remains high.In this work, we study structured pruning as an effective means to develop smaller LLMs from pre-trained, larger models.Our approach employs two key techniques: (1) targeted structured pruning, which prunes a larger model to a specified target shape by removing layers, heads, and intermediate and hidden dimensions in an end-to-end manner, and (2) dynamic batch loading, which dynamically updates the composition of sampled data in each training batch based on varying losses across different domains.We demonstrate the efficacy of our approach by presenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B and 2.7B parameters.Sheared-LLaMA models outperform state-of-the-art open-source models of equivalent sizes, such as Pythia, INCITE, and OpenLLaMA models, on a wide range of downstream and instruction tuning evaluations, while requiring only 3% of compute compared to training such models from scratch.This work provides compelling evidence that leveraging existing LLMs with structured pruning is a far more cost-effective approach for building smaller LLMs. 1",
        "id":263830786
      }
    ],
    "negative_ctxs":[
      {
        "title":"Direction is what you need: Improving Word Embedding Compression in Large Language Models",
        "text":"The adoption of Transformer-based models in natural language processing (NLP) has led to great success using a massive number of parameters. However, due to deployment constraints in edge devices, there has been a rising interest in the compression of these models to improve their inference time and memory footprint. This paper presents a novel loss objective to compress token embeddings in the Transformer-based models by leveraging an AutoEncoder architecture. More specifically, we emphasize the importance of the direction of compressed embeddings with respect to original uncompressed embeddings. The proposed method is task-agnostic and does not require further language modeling pre-training. Our method significantly outperforms the commonly used SVD-based matrix-factorization approach in terms of initial language model Perplexity. Moreover, we evaluate our proposed approach over SQuAD v1.1 dataset and several downstream tasks from the GLUE benchmark, where we also outperform the baseline in most scenarios. Our code is public. 1 .",
        "id":235436167
      },
      {
        "title":"A Call for Clarity in Reporting BLEU Scores",
        "text":"The field of machine translation faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to \"the\" BLEU score, BLEU is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for usersupplied reference processing, and provide a new tool, SACREBLEU, 1 to facilitate this. . 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. ArXiv eprints, abs\/1609.08144.",
        "id":13751870
      },
      {
        "title":"",
        "text":"",
        "id":236477523
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"I'm using Local SGD with a decaying learning rate for distributed training. Which paper offers guidance on setting the synchronization period in Local SGD to optimize test accuracy?",
    "positive_ctxs":[
      {
        "title":"A QUADRATIC SYNCHRONIZATION RULE FOR DISTRIBUTED DEEP LEARNING",
        "text":"In distributed deep learning with data parallelism, synchronizing gradients at each training step can cause a huge communication overhead, especially when many nodes work together to train large models.Local gradient methods, such as Local SGD, address this issue by allowing workers to compute locally for H steps without synchronizing with others, hence reducing communication frequency.While H has been viewed as a hyperparameter to trade optimization efficiency for communication cost, recent research indicates that setting a proper H value can lead to generalization improvement.Yet, selecting a proper H is elusive.This work proposes a theory-grounded method for determining H, named the Quadratic Synchronization Rule (QSR), which recommends dynamically setting H in proportion to 1 η 2 as the learning rate η decays over time.Extensive ImageNet experiments on ResNet and ViT show that local gradient methods with QSR consistently improve the test accuracy over other synchronization strategies. 1 Compared with the standard data parallel training, QSR enables Local AdamW on ViT-B to cut the training time on 16 or 64 GPUs down from 26.7 to 20.2 hours or from 8.6 to 5.5 hours and, at the same time, achieves 1.16% or 0.84% higher top-1 validation accuracy.",
        "id":264426013
      }
    ],
    "negative_ctxs":[
      {
        "title":"Combining Resources for Open Source Machine Translation",
        "text":"In this paper, we present a Japanese→English machine translation system that combines rule-based and statistical translation. Our system is unique in that all of its components are freely available as open source software. We describe the development of the rule-based translation engine including transfer rule acquisition from an open bilingual dictionary. We also show how translations from both translation engines are combined through a simple ranking mechanism and compare their outputs.",
        "id":35316263
      },
      {
        "title":"Joint Arabic Segmentation and Part-Of-Speech Tagging",
        "text":"Arabic has a very co mp lex morphological system, though a very structured one. Character patterns are often indicative of word class and word segmentation. In this paper, we e xplore a novel approach to Arabic word segmentation and part-of-speech tagging relying on character info rmation. The approach is lexicon-free and does not require any morphological analysis, eliminat ing the factor of dictionary coverage. Using character-based analysis, the developed system yielded stateof-the-art accuracy comparing favourably with other taggers that involve external resources.",
        "id":16150164
      },
      {
        "title":"ANOTHER LOOK AT NOMINAL COMPOUNDS",
        "text":"We present a progress report on our research on nominal compounds (NC's). Recent approaches to this probiem in linguistics and natural ianguage processing (NLP) are reviewed and criticized. We argue that the notion of \"roie nominal\", which is at the interface of linguistic and extraiinguistic knowledge, is crucial for characterizing NC'e as weII as other Iinguistic phenomena. We examine a number of constraints on the semantic interpretation ruies for NC's. Proposals are made that shouid improve the capability of NLP systems to deaI with NC's.",
        "id":15803406
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"In video diffusion models, is there any paper that tried decomposing video instruction into sub instructions of different time?",
    "positive_ctxs":[
      {
        "title":"Seer: Language Instructed Video Prediction with Latent Diffusion Models",
        "text":"Imagining the future trajectory is the key for robots to make sound planning and successfully reach their goals. Therefore, text-conditioned video prediction (TVP) is an essential task to facilitate general robot policy learning, i.e., predicting future video frames with a given language instruction and reference frames. It is a highly challenging task to ground task-level goals specified by instructions and high-fidelity frames together, requiring large-scale data and computation. To tackle this task and empower robots with the ability to foresee the future, we propose a sample and computation-efficient model, named Seer, by inflating the pretrained text-to-image (T2I) stable diffusion models along the temporal axis. We inflate the denoising U-Net and language conditioning model with two novel techniques, Autoregressive Spatial-Temporal Attention and Frame Sequential Text Decomposer, to propagate the rich prior knowledge in the pretrained T2I models across the frames. With the well-designed architecture, Seer makes it possible to generate high-fidelity, coherent, and instruction-aligned video frames by fine-tuning a few layers on a small amount of data. The experimental results on Something Something V2 (SSv2) and Bridgedata datasets demonstrate our superior video prediction performance with around 210-hour training on 4 RTX 3090 GPUs: decreasing the FVD of the current SOTA model from 290 to 200 on SSv2 and achieving at least 70% preference in the human evaluation. https:\/\/seervideodiffusion.github.io\/",
        "id":257766959
      }
    ],
    "negative_ctxs":[
      {
        "title":"The Design of an Experiment in Anaphora Resolution for Referring Expressions Generation",
        "text":"We present a pilot experiment to measure the effects of redundancy in the resolution of definite descriptions as performed by a small number of human readers. Although originally intended to provide evidence of how much redundancy should ideally be included in generated anaphoric descriptions, preliminary findings reveal a number of little explored issues that are relevant to both referring expressions generation and interpretation.",
        "id":18538403
      },
      {
        "title":"Efficient parsing strategies for syntactic analysis of closed captions",
        "text":"We present an efficient multi-level chart parser that was designed for syntactic analysis of closed captions (subtitles) in a real-time Machine Translation (MT) system. In order to achieve high parsing speed, we divided an existing English grammar into multiple levels. The parser proceeds in stages. At each stage, rules corresponding to only one level are used. A constituent pruning step is added between levels to insure that constituents not likely to be part of the final parse are removed. This results in a significant parse time and ambiguity reduction. Since the domain is unrestricted, out-of-coverage sentences are to be expected and the parser might not produce a single analysis spanning the whole input. Despite the incomplete parsing strategy and the radical pruning, the initial evaluation results show that the loss of parsing accuracy is acceptable. The parsing time favorable compares with a Tomita parser and a chart parser parsing time when run on the same grammar and lexicon.7",
        "id":2607435
      },
      {
        "title":"",
        "text":"",
        "id":207915656
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_iclr",
    "question":"Is there a paper illustrating that pre-trained transformers from LLMs can be used to encode visual information in a wide range of scenarios?",
    "positive_ctxs":[
      {
        "title":"FROZEN TRANSFORMERS IN LANGUAGE MODELS ARE EFFECTIVE VISUAL ENCODER LAYERS",
        "text":"This paper reveals that large language models (LLMs), despite being trained solely on textual data, are surprisingly strong encoders for purely visual tasks in the absence of language.Even more intriguingly, this can be achieved by a simple yet previously overlooked strategy -employing a frozen transformer block from pre-trained LLMs as a constituent encoder layer to directly process visual tokens.Our work pushes the boundaries of leveraging LLMs for computer vision tasks, significantly departing from conventional practices that typically necessitate a multi-modal vision-language setup with associated language prompts, inputs, or outputs.We demonstrate that our approach consistently enhances performance across a diverse range of tasks, encompassing pure 2D and 3D visual recognition tasks (e.g., image and point cloud classification), temporal modeling tasks (e.g., action recognition), non-semantic tasks (e.g., motion forecasting), and multi-modal tasks (e.g., 2D\/3D visual question answering and image-text retrieval).Such improvements are a general phenomenon, applicable to various types of LLMs (e.g., LLaMA and OPT) and different LLM transformer blocks.We additionally propose the information filtering hypothesis to explain the effectiveness of pre-trained LLMs in visual encoding -the pre-trained LLM transformer blocks discern informative visual tokens and further amplify their effect.This hypothesis is empirically supported by the observation that the feature activation, after training with LLM transformer blocks, exhibits a stronger focus on relevant regions.We hope that our work inspires new perspectives on utilizing LLMs and deepening our understanding of their underlying mechanisms.Code is available at https:\/\/github.com\/ziqipang\/LM4VisualEncoding.",
        "id":264306111
      }
    ],
    "negative_ctxs":[
      {
        "title":"Published as a conference paper at ICLR 2022 PRIORGRAD: IMPROVING CONDITIONAL DENOISING DIFFUSION MODELS WITH DATA-DEPENDENT ADAP- TIVE PRIOR",
        "text":"Denoising diffusion probabilistic models have been recently proposed to generate high-quality samples by estimating the gradient of the data density. The framework defines the prior noise as a standard Gaussian distribution, whereas the corresponding data distribution may be more complicated than the standard Gaussian distribution, which potentially introduces inefficiency in denoising the prior noise into the data sample because of the discrepancy between the data and the prior. In this paper, we propose PriorGrad to improve the efficiency of the conditional diffusion model for speech synthesis (for example, a vocoder using a mel-spectrogram as the condition) by applying an adaptive prior derived from the data statistics based on the conditional information. We formulate the training and sampling procedures of PriorGrad and demonstrate the advantages of an adaptive prior through a theoretical analysis. Focusing on the speech synthesis domain, we consider the recently proposed diffusion-based speech generative models based on both the spectral and time domains and show that PriorGrad achieves faster convergence and inference with superior performance, leading to an improved perceptual quality and robustness to a smaller network capacity, and thereby demonstrating the efficiency of a data-dependent adaptive prior. * Work done during an internship at Microsoft Research Asia † Corresponding Authors Published as a conference paper at ICLR 2022 However, although the diffusion-based speech synthesis models have achieved high-quality speech audio generation, they exhibit potential inefficiency, which may necessitate advanced strategies. For example, the model suffers from a significantly slow convergence during training, and a prohibitively large training computation time is required to learn the approximate reverse diffusion process. We investigate the diffuion-based models and observe the discrepancy between the real data distribution and the choice of the prior. Existing diffusion-based models define a standard Gaussian as the prior distribution and design a non-parametric diffusion process that procedurally destroys the signal into the prior noise. The deep neural network is trained to approximate the reverse diffusion process by estimating the gradient of the data density. Although applying the standard Gaussian as the prior is simple without any assumptions on the target data, it also introduces inefficiency. For example, in time-domain waveform data, the signal has extremely high variability between different segments such as voiced and unvoiced parts. Jointly modeling the voiced and unvoiced segments with the same standard Gaussian prior may be difficult for the model to cover all modes of the data, leading to training inefficiencies and potentially spurious diffusion trajectories.Given the previous reasoning, we assessed the following question: For a conditional diffusion-based model, can we formulate a more informative prior without incorporating additional computational or parameter complexity? To investigate this, we propose a simple yet effective method, called PriorGrad, that uses adaptive noise by directly computing the mean and variance for the forward diffusion process prior, based on the conditional information. Specifically, using a conditional speech synthesis model, we propose structuring the prior distribution based on the conditional data, such as a mel-spectrogram for the vocoder  Kong et al., 2021)  and a phoneme for the acoustic model(Jeong et al., 2021). By computing the statistics from the conditional data at the frame level (vocoder) or phoneme-level (acoustic model) granularity and mapping them as the mean and variance of the Gaussian prior, we can structure the noise that is similar to the target data distribution at an instance level, easing the burden of learning the reverse diffusion process.We implemented PriorGrad based on the recently proposed diffusion-based speech generative models (Kong et al., 2021;Jeong et al., 2021), and conducted experiments on the LJSpeech (Ito & Johnson, 2017) dataset. The experimental results demonstrate the benefits of Prior-Grad, such as a significantly faster model convergence during training, improved perceptual quality, and an improved tolerance to a reduction in network capacity. Our contributions are as follows:",
        "id":247011539
      },
      {
        "title":"LINGUISTIC ANALYSIS OF NATURAL LANGUAGE COMMUNICATION WITH COMPUTERS",
        "text":"",
        "id":1010309
      },
      {
        "title":"Standard and Nonstandard Lexicon in Aviation English: A Corpus Linguistic Study",
        "text":"This study aims at investigating the lexical items in Aviation Phraseology that has both standard and nonstandard meanings when Pilot and Air Traffic Controller (ATC) use them in radiotelephony.",
        "id":52140006
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Is there a paper that takes a mixed machine learning and solver based approach to code translation?",
    "positive_ctxs":[
      {
        "title":"GUESS & SKETCH: LANGUAGE MODEL GUIDED TRANSPILATION",
        "text":"Maintaining legacy software requires many software and systems engineering hours.Assembly code programs, which demand low-level control over the computer machine state and have no variable names, are particularly difficult for humans to analyze.Existing conventional program translators guarantee correctness, but are hand-engineered for the source and target programming languages in question.Learned transpilation, i.e. automatic translation of code, offers an alternative to manual re-writing and engineering efforts.Automated symbolic program translation approaches guarantee correctness but struggle to scale to longer programs due to the exponentially large search space.Their rigid rule-based systems also limit their expressivity, so they can only reason about a reduced space of programs.Probabilistic neural language models (LMs) produce plausible outputs for every input, but do so at the cost of guaranteed correctness.In this work, we leverage the strengths of LMs and symbolic solvers in a neurosymbolic approach to learned transpilation for assembly code.Assembly code is an appropriate setting for a neurosymbolic approach, since assembly code can be divided into shorter non-branching basic blocks amenable to the use of symbolic methods.GUESS & SKETCH extracts alignment and confidence information from features of the LM then passes it to a symbolic solver to resolve semantic equivalence of the transpilation input and output.We test GUESS & SKETCH on three different test sets of assembly transpilation tasks, varying in difficulty, and show that it successfully transpiles 57.6% more examples than GPT-4 and 39.6% more examples than an engineered transpiler.We also share a training and evaluation dataset for this task.",
        "id":262828485
      }
    ],
    "negative_ctxs":[
      {
        "title":"Word Frequency Does Not Predict Grammatical Knowledge in Language Models",
        "text":"Neural language models learn, to varying degrees of accuracy, the grammatical properties of natural languages. In this work, we investigate whether there are systematic sources of variation in the language models' accuracy. Focusing on subject-verb agreement and reflexive anaphora, we find that certain nouns are systematically understood better than others, an effect which is robust across grammatical tasks and different language models. Surprisingly, we find that across four orders of magnitude, corpus frequency is unrelated to a noun's performance on grammatical tasks. Finally, we find that a novel noun's grammatical properties can be few-shot learned from various types of training data. The results present a paradox: there should be less variation in grammatical performance than is actually observed.",
        "id":226283660
      },
      {
        "title":"Improving Constituency Parsing with Span Attention",
        "text":"Constituency parsing is a fundamental and important task for natural language understanding, where a good representation of contextual information can help this task. N-grams, which is a conventional type of feature for contextual information, have been demonstrated to be useful in many tasks, and thus could also be beneficial for constituency parsing if they are appropriately modeled. In this paper, we propose span attention for neural chartbased constituency parsing to leverage n-gram information. Considering that current chartbased parsers with Transformer-based encoder represent spans by subtraction of the hidden states at the span boundaries, which may cause information loss especially for long spans, we incorporate n-grams into span representations by weighting them according to their contributions to the parsing process. Moreover, we propose categorical span attention to further enhance the model by weighting ngrams within different length categories, and thus benefit long-sentence parsing. Experimental results on three widely used benchmark datasets demonstrate the effectiveness of our approach in parsing Arabic, Chinese, and English, where state-of-the-art performance is obtained by our approach on all of them. 1 † Corresponding author. 1 Our code and the best performing models are released at https:\/\/github.com\/cuhksz-nlp\/SAPar.",
        "id":222378720
      },
      {
        "title":"Generating Natural Language Proofs with Verifier-Guided Search",
        "text":"Reasoning over natural language is a challenging problem in NLP. In this work, we focus on proof generation: Given a hypothesis and a set of supporting facts, the model generates a proof tree indicating how to derive the hypothesis from supporting facts. Compared to generating the entire proof in one shot, stepwise generation can better exploit the compositionality and generalize to longer proofs but has achieved limited success on real-world data. Existing stepwise methods struggle to generate proof steps that are both logically valid and relevant to the hypothesis. Instead, they tend to hallucinate invalid steps given the hypothesis. In this paper, we present a novel stepwise method, NLProofS (Natural Language Proof Search), which learns to generate relevant steps conditioning on the hypothesis. At the core of our approach, we train an independent verifier to check the validity of the proof steps to prevent hallucination. Instead of generating steps greedily, we search for proofs maximizing a global proof score judged by the verifier. NL-ProofS achieves state-of-the-art performance on EntailmentBank and RuleTaker. Specifically, it improves the correctness of predicted proofs from 27.7% to 33.3% in the distractor setting of EntailmentBank, demonstrating the effectiveness of NLProofS in generating challenging human-authored proofs. 1",
        "id":249062748
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Is there a paper which applies Bayesian optimization to modular continual learning?",
    "positive_ctxs":[
      {
        "title":"A Probabilistic Framework for Modular Continual Learning",
        "text":"Modular approaches, which use a different composition of modules for each problem and avoid forgetting by design, have been shown to be a promising direction in continual learning (CL). However, searching through the large, discrete space of possible module compositions is a challenge because evaluating a composition's performance requires a round of neural network training. To address this challenge, we develop a modular CL framework, called PICLE, that accelerates search by using a probabilistic model to cheaply compute the fitness of each composition. The model combines prior knowledge about good module compositions with datasetspecific information. Its use is complemented by splitting up the search space into subsets, such as perceptual and latent subsets. We show that PICLE is the first modular CL algorithm to achieve different types of transfer while scaling to large search spaces. We evaluate it on two benchmark suites designed to capture different desiderata of CL techniques. On these benchmarks, PICLE offers significantly better performance than state-of-the-art CL baselines.Preprint. Under review.",
        "id":259138821
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":202735479
      },
      {
        "title":"Evaluating the Utility of Hand-crafted Features in Sequence Labelling *",
        "text":"Conventional wisdom is that hand-crafted features are redundant for deep learning models, as they already learn adequate representations of text automatically from corpora. In this work, we test this claim by proposing a new method for exploiting handcrafted features as part of a novel hybrid learning approach, incorporating a feature auto-encoder loss component. We evaluate on the task of named entity recognition (NER), where we show that including manual features for partof-speech, word shapes and gazetteers can improve the performance of a neural CRF model. We obtain a F 1 of 91.89 for the CoNLL-2003 English shared task, which significantly outperforms a collection of highly competitive baseline models. We also present an ablation study showing the importance of autoencoding, over using features as either inputs or outputs alone, and moreover, show including the autoencoder components reduces training requirements to 60%, while retaining the same predictive accuracy.",
        "id":52113877
      },
      {
        "title":"Exploration and Discovery of the COVID-19 Literature through Semantic Visualization",
        "text":"We propose semantic visualization as a linguistic visual analytic method. It can enable exploration and discovery over large datasets of complex networks by exploiting the semantics of the relations in them. This involves extracting information, applying parameter reduction operations, building hierarchical data representation and designing visualization. We also present the accompanying COVID-SEMVIZ, a searchable and interactive visualization system for knowledge exploration of COVID-19 data to demonstrate the application of our proposed method. 1 In the user studies, users found that semantic visualization-powered COVID-SEMVIZ is helpful in terms of finding relevant information and discovering unknown associations.",
        "id":220347577
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_iclr",
    "question":"Is there a paper which proposes a general data selection method based on information theory?",
    "positive_ctxs":[
      {
        "title":"GIO: GRADIENT INFORMATION OPTIMIZATION FOR TRAINING DATASET SELECTION",
        "text":"It is often advantageous to train models on a subset of the available train examples, because the examples are of variable quality or because one would like to train with fewer examples, without sacrificing performance. We present Gradient Information Optimization (GIO), a scalable, task-agnostic approach to this data selection problem that requires only a small set of (unlabeled) examples representing a target distribution. GIO begins from a natural, information-theoretic objective that is intractable in practice. Our contribution is in showing that it can be made highly scalable through a simple relaxation of the objective and a highly efficient implementation. In experiments with machine translation, spelling correction, and image recognition, we show that GIO delivers outstanding results with very small train sets. These findings are robust to different representation models and hyperparameters for GIO itself. GIO is task-and domain-agnostic and can be applied out-of-the-box to new datasets and domains.Active learning. Active learning methods (e.g. Sener and Savarese, 2018; Gal et al., 2017; Kirsch  et al., 2019)  can be cast as data selection methods in our sense. In active learning, one iteratively chooses new unlabeled training examples to label, with the goal of efficiently creating a powerful train set. By contrast, GIO makes no use of labels and is oriented towards the goal of identifying a subset of existing cases to use for training. Additionally, active learning is most suited to classification problems, whereas GIO works with any arbitrary task.Heuristic. GIO is closer to recent methods in which one uses a large language model to generate a large number of candidate texts and then extracts a subset of them based on a specific criteria. For example, Brown et al. (2020) develop a heuristic method to filter CommonCrawl based on a trained classifier's probability that datapoints are high quality. Similarly, Wenzek et al. (2020) develop a pipeline to clean CommonCrawl based principally on the perplexity of an LM trained on high quality text, and Xie et al. (2023) develop a sampling technique based on approximate n-gram counts.Like GIO, these heuristic methods aim to select a subset of data that is higher quality and more relevant. However, they are either highly tailored to their particular tasks or they require very large numbers of examples (to develop classifiers or construct target probabilities). By contrast, GIO is task-and domain-agnostic, it can be applied plug-and-play to a new task and dataset, and it requires comparatively few gold examples X to serve as the target distribution.Similarity Search. Methods using vector or n-gram similarity search can also be used for data selection at scale (e.g. Johnson et al., 2017;Bernhardsson, 2017;Santhanam et al., 2022). The technique would index G and X and retrieve the top-k datapoints from G for each point in X. Like our method, similarity search works in a continuous space. However, similarity search can be prone to selecting suboptimal points; we review such a case in detail in Section 3.4. Additionally, similarity search does not have a natural stopping criterion and requires data size to be chosen arbitrarily. Is 10% data enough? 20%? We don't know a priori. And if the data in G is far away from X, similarity search will still choose it up to the desired data size. Recently, Yao et al. (2022) use a BM25 retrieval method for data selection, with strong results. However, BM25 operates on a bag-of-words model, which can make it challenging when the target set is small, and like any similarity search, requires data size to be chosen arbitrarily beforehand. Further, this method only applies to text tasks, whereas GIO applies to any task with continuous representation.",
        "id":259203325
      }
    ],
    "negative_ctxs":[
      {
        "title":"A New Model for Lexical Choice for Open-Class Words",
        "text":"The lexical choice process should be regarded as a constraint satisfaction problem: the generation system must choose a lexical unit that is accurate (t~mthful), va\/\/d (conveys the necessary information), and preferred (maxirnal under a preference function). This corts~aint-based architecture allows a clema separation to be made between what the system knows of the object or event, and what the system wishes to communicate about the object or event. It also allows lexical choices to be biased towards basic-level(Rosch 1978)and other preferred lexical units.",
        "id":14634824
      },
      {
        "title":"Pre-reordering for machine translation using transition-based walks on dependency parse trees",
        "text":"We propose a pre-reordering scheme to improve the quality of machine translation by permuting the words of a source sentence to a target-like order. This is accomplished as a transition-based system that walks on the dependency parse tree of the sentence and emits words in target-like order, driven by a classifier trained on a parallel corpus. Our system is capable of generating arbitrary permutations up to flexible constraints determined by the choice of the classifier algorithm and input features.",
        "id":1610717
      },
      {
        "title":"Flexible retrieval with NMSLIB and FlexNeuART",
        "text":"Our objective is to introduce to the NLP community an existing k-NN search library NMSLIB, a new retrieval toolkit FlexNeuART, as well as their integration capabilities.NMSLIB, while being one the fastest k-NN search libraries, is quite generic and supports a variety of distance\/similarity functions.Because the library relies on the distance-based structure-agnostic algorithms, it can be further extended by adding new distances. FlexNeuART is a modular, extendible and flexible toolkit for candidate generation in IR and QA applications, which supports mixing of classic and neural ranking signals. FlexNeuART can efficiently retrieve mixed dense and sparse representations (with weights learned from training data), which is achieved by extending NMSLIB. In that, other retrieval systems work with purely sparse representations (e.g., Lucene), purely dense representations (e.g., FAISS and Annoy), or only perform mixing at the re-ranking stage.",
        "id":225094208
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Is there a parameter-efficient fine-tuning method (PEFT) that allows adjusting the number of optimized parameters to any value, irrespective of the model's architecture or choice of the adapter?",
    "positive_ctxs":[
      {
        "title":"NOLA: NETWORKS AS LINEAR COMBINATION OF LOW RANK RANDOM BASIS",
        "text":"Large Language Models (LLMs) have recently gained popularity due to their impressive few-shot performance across various downstream tasks.However, fine-tuning all parameters and storing a unique model for each downstream task or domain becomes impractical because of the massive size of checkpoints (e.g., 350GB in GPT-3).Current literature, such as LoRA, showcases the potential of lowrank modifications to the original weights of an LLM, enabling efficient adaptation and storage for task-specific models.These methods can reduce the number of parameters needed to fine-tune an LLM by several orders of magnitude.Yet, these methods face two primary limitations: 1) the parameter reduction is lower-bounded by the rank one decomposition, and 2) the extent of reduction is heavily influenced by both the model architecture and the chosen rank.For instance, in larger models, even a rank one decomposition might exceed the number of parameters truly needed for adaptation.In this paper, we introduce NOLA, which overcomes the rank one lower bound present in LoRA.It achieves this by re-parameterizing the low-rank matrices in LoRA using linear combinations of randomly generated matrices (basis) and optimizing the linear mixture coefficients only.This approach allows us to decouple the number of trainable parameters from both the choice of rank and the network architecture.We present adaptation results using GPT-2 and ViT in natural language and computer vision tasks.NOLA performs as well as, or better than models with equivalent parameter counts.Furthermore, we demonstrate that we can halve the parameters in larger models compared to LoRA with rank one, without sacrificing performance.Our code is available here: https:\/\/github.com\/UCDvision\/NOLA* Equal Contribution.",
        "id":263620510
      }
    ],
    "negative_ctxs":[
      {
        "title":"Minimum Description Length Control",
        "text":"We propose a novel framework for multitask reinforcement learning based on the minimum description length (MDL) principle. In this approach, which we term MDL-control (MDL-C), the agent learns the common structure among the tasks with which it is faced and then distills it into a simpler representation which facilitates faster convergence and generalization to new tasks. In doing so, MDL-C naturally balances adaptation to each task with epistemic uncertainty about the task distribution. We motivate MDL-C via formal connections between the MDL principle and Bayesian inference, derive theoretical performance guarantees, and demonstrate MDL-C's empirical effectiveness on both discrete and high-dimensional continuous control tasks.",
        "id":250627720
      },
      {
        "title":"AN LR(k ) ERROR DIAGNOSIS AND REC OVERY METHOD",
        "text":"In this paper, a new practical, efficient and language ... independent syntactic error recov ery method for tR( k) parsers is presented. This method is similar to and builds Upon the three-level approach oi' Burke�Fisher [11]. However, it is more time-and space-efficient and fully automatic.",
        "id":219308174
      },
      {
        "title":"Political Discourse Analysis: A Case Study of Code Mixing and Code Switching in Political Speeches",
        "text":"Political discourse is one of the most interesting data to study power relations in the framework of Critical Discourse Analysis. With the increase in the modes of textual and spoken forms of communication, politicians use language and linguistic mechanisms that contribute significantly in building their relationship with people, especially in a multilingual country like India with many political parties with different ideologies. This paper analyses code-mixing and code-switching in Telugu political speeches to determine the factors responsible for their usage levels in various social settings and communicative contexts. We also compile a detailed set of rules capturing dialectal variations between Standard and Telangana dialects of Telugu.",
        "id":235097552
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_iclr",
    "question":"Is there a single GNN model that can inductively generalize to any knowledge graph?;What is the method to generalize knowledge graph reasoning to graphs with new entities and relations?;Is there a foundation model for knowledge graphs that does not learn embeddings for each node and relation type?",
    "positive_ctxs":[
      {
        "title":"TOWARDS FOUNDATION MODELS FOR KNOWLEDGE GRAPH REASONING",
        "text":"Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language.Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap.The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies.In this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations.ULTRA builds relational representations as a function conditioned on their interactions.Such a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph.Conducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs.Fine-tuning further boosts the performance.",
        "id":263831485
      }
    ],
    "negative_ctxs":[
      {
        "title":"A Game-based Approach to Transcribing Images of Text",
        "text":"We present a methodology that takes as input scanned documents of typed or hand-written text, and produces transcriptions of the text as output. Instead of using OCR technology, the methodology is game-based and produces such transcriptions as a by-product. The approach is intended particularly for languages for which language technology and resources are scarce and reliable OCR technology may not exist. It can be used in place of OCR for transcribing individual documents, or to create corpora of paired images and transcriptions required to train OCR tools. We present Minefield, a prototype implementation of the approach which is currently collecting Arabic transcriptions.",
        "id":14326311
      },
      {
        "title":"Z-LaVI: Zero-Shot Language Solver Fueled by Visual Imagination",
        "text":"Large-scale pretrained language models have made significant advances in solving downstream language understanding tasks. However, they generally suffer from reporting bias, the phenomenon describing the lack of explicit commonsense knowledge in written text, e.g., \"an orange is orange\". To overcome this limitation, we develop a novel approach, Z-LaVI, to endow language models with visual imagination capabilities. Specifically, we leverage two complementary types of \"imaginations\": (i) recalling existing images through retrieval and (ii) synthesizing nonexistent images via text-toimage generation. Jointly exploiting the language inputs and the imagination, a pretrained vision-language model (e.g., CLIP) eventually composes a zero-shot solution to the original language tasks. Notably, fueling language models with imagination can effectively leverage visual knowledge to solve plain language tasks. In consequence, Z-LaVI consistently improves the zero-shot performance of existing language models across a diverse set of language tasks. 1",
        "id":253098783
      },
      {
        "title":"An ERP-based Brain-Computer Interface for text entry using Rapid Serial Visual Presentation and Language Modeling",
        "text":"Event related potentials (ERP) corresponding to stimuli in electroencephalography (EEG) can be used to detect the intent of a person for brain computer interfaces (BCI). This paradigm is widely used to build letter-byletter text input systems using BCI. Nevertheless using a BCI-typewriter depending only on EEG responses will not be sufficiently accurate for single-trial operation in general, and existing systems utilize many-trial schemes to achieve accuracy at the cost of speed. Hence incorporation of a language model based prior or additional evidence is vital to improve accuracy and speed. In this demonstration we will present a BCI system for typing that integrates a stochastic language model with ERP classification to achieve speedups, via the rapid serial visual presentation (RSVP) paradigm.",
        "id":7593224
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Is there a theory paper that explains why sometimes tuning momentum does not boost performance for training a neural network?",
    "positive_ctxs":[
      {
        "title":"The Marginal Value of Momentum for Small Learning Rate SGD",
        "text":"Momentum is known to accelerate the convergence of gradient descent in strongly convex settings without stochastic gradient noise. In stochastic optimization, such as training neural networks, folklore suggests that momentum may help deep learning optimization by reducing the variance of the stochastic gradient update, but previous theoretical analyses do not find momentum to offer any provable acceleration. Theoretical results in this paper clarify the role of momentum in stochastic settings where the learning rate is small and gradient noise is the dominant source of instability, suggesting that SGD with and without momentum behave similarly in the short and long time horizons. Experiments show that momentum indeed has limited benefits for both optimization and generalization in practical training regimes where the optimal learning rate is not very large, including small-to medium-batch training from scratch on ImageNet and finetuning language models on downstream tasks.Preprint. Under review.",
        "id":260316137
      }
    ],
    "negative_ctxs":[
      {
        "title":"Orthographic Awareness and Phonological Awareness of Late Chinese- English Bilinguals: Evidence from Word-Picture Interference Tasks",
        "text":"The distractor stimuli are highly effective in modulating speech production latencies in word-picture interference task. It is one of the main experimental methods to explore the relationship between speech production network and perception network(Levelt et al., 1999). The distractors can be presented in both visual and auditory modalities(Lupker, 1979;Meyer and Shriefers, 1990). The interference effect (either facilitatory or inhibitory) can vary from interfering stimuli types(Lupker, 1979;Lupker, 1982;Glaser and Dungelhoff, 1984;  Meyer and Shriefers, 1990). This study has adopted a visual and an auditory interference experiment on two groups of Chinese-English bilinguals with different L2 proficiency level to figure out if there is the effect of L2 proficiency and the effect of interference modality on response latency or accuracy on two proficiency groups of late Chinese-English bilinguals. From the accuracy result, all the late bilinguals in this study may have limited orthographic awareness and weak phonological encoding ability.",
        "id":202752865
      },
      {
        "title":"Verbs and (sub)Event Structure: A Case Study from Italian *",
        "text":"In this paper, I try to show the advantages of analyzing events in terms of subevent structure, by taking into consideration the case of a specific class of verbs: transitive and intransitive verbs which obligatorily require the presence of a predicative complement (e.g., English seem, consider). The proposed analysis is exemplified with data from Italian. Two verbs are described in detail: rimanere, 'remain' and rendere, 'make'. It is shown that subevent structure representation is useful for the description of the different uses and meanings of these verbs. This type of description can also be the basis for an accurate treatment in computational semantic lexica.",
        "id":6580398
      },
      {
        "title":"Exploiting Scope for Shallow Discourse Parsing",
        "text":"We present an approach to automatically identifying the arguments of discourse connectives based on data from the Penn Discourse Treebank. Of the two arguments of connectives, called Arg1 and Arg2, we focus on Arg1, which has proven more challenging to identify. Our approach employs a sentence-based representation of arguments, and distinguishes intra-sentential connectives, which take both their arguments in the same sentence, from inter-sentential connectives, whose arguments are found in different sentences. The latter are further distinguished by paragraph position into ParaInit connectives, which appear in a paragraph-initial sentence, and ParaNonInit connectives, which appear elsewhere. The paper focusses on predicting Arg1 of Inter-sentential ParaNonInit connectives, presenting a set of scope-based filters that reduce the search space for Arg1 from all the previous sentences in the paragraph to a subset of them. For cases where these filters do not uniquely identify Arg1, coreference-based heuristics are employed. Our analysis shows an absolute 3% performance improvement over the high baseline of 83.3% for identifying Arg1 of Inter-sentential ParaNonInit connectives.",
        "id":8511010
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Is there an existing dataset of images with alt-text that also includes the text the image was originally posted with?",
    "positive_ctxs":[
      {
        "title":"ALT-TEXT WITH CONTEXT: IMPROVING ACCESSIBILITY FOR IMAGES ON TWITTER",
        "text":"In this work we present an approach for generating alternative text (or alt-text) descriptions for images shared on social media, specifically Twitter.More than just a special case of image captioning, alt-text is both more literally descriptive and context-specific.Also critically, images posted to Twitter are often accompanied by user-written text that despite not necessarily describing the image may provide useful context that if properly leveraged can be informative.We address this task with a multimodal model that conditions on both textual information from the associated social media post as well as visual signal from the image, and demonstrate that the utility of these two information sources stacks.We put forward a new dataset of 371k images paired with alt-text and tweets scraped from Twitter and evaluate on it across a variety of automated metrics as well as human evaluation.We show that our approach of conditioning on both tweet text and visual information significantly outperforms prior work, by more than 2x on BLEU@4.",
        "id":258865444
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":219300633
      },
      {
        "title":"Fast Mapping in Word Learning: What Probabilities Tell Us",
        "text":"Children can determine the meaning of a new word from hearing it used in a familiar context-an ability often referred to as fast mapping. In this paper, we study fast mapping in the context of a general probabilistic model of word learning. We use our model to simulate fast mapping experiments on children, such as referent selection and retention. The word learning model can perform these tasks through an inductive interpretation of the acquired probabilities. Our results suggest that fast mapping occurs as a natural consequence of learning more words, and provides explanations for the (occasionally contradictory) child experimental data.",
        "id":7037231
      },
      {
        "title":"Improving the Template Generation for Chinese Character Error Detection with Confusion Sets",
        "text":"In this paper, we propose a system that automatically generates templates for detecting Chinese character errors. We first collect the confusion sets for each high-frequency Chinese character. Error types include pronunciation-related errors and radical-related errors. With the help of the confusion sets, our system generates possible error patterns in context, which will be used as detection templates. Combined with a word segmentation module, our system generates more accurate templates. The experimental results show the precision of performance approaches 95%. Such a system should not only help teachers grade and check student essays, but also effectively help students learn how to write.Compared with the detection of spelling errors in English, the detection of incorrect Chinese characters is much more difficult. In English, a word consists of a series of letters while a meaningful Chinese word usually consists of 2 to 4 Chinese characters. The difficulty lies partly in the fact that there are more than 5,000 high-frequency characters.In previous works on Chinese character error detection systems (",
        "id":18657652
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Is there any generalizable NeRF paper that disentangles texture and shape?",
    "positive_ctxs":[
      {
        "title":"TUVF: LEARNING GENERALIZABLE TEXTURE UV RADIANCE FIELDS",
        "text":"Figure 1: We propose Texture UV Radiance Fields (TUVF) to render a 3D consistent texture given a 3D object shape input.TUVF provides a category-level texture representation disentangled from 3D shapes.Top three rows: TUVF can synthesize realistic textures by training from a collection of single-view images; Fourth row: Given a 3D shape input, we can render different textures on top by using different texture codes; Bottom row: We can perform editing on a given texture (adding a flag of France) and directly apply the same texture on different 3D shapes without further fine-tuning.Note that all samples are rendered under 1024×1024 resolution; zoom-in is recommended.",
        "id":258480014
      }
    ],
    "negative_ctxs":[
      {
        "title":"Modeling Violations of Selectional Restrictions with Distributional Semantics",
        "text":"Distributional Semantic Models have been successfully used for modeling selectional preferences in a variety of scenarios, since distributional similarity naturally provides an estimate of the degree to which an argument satisfies the requirement of a given predicate. However, we argue that the performance of such models on rare verb-argument combinations has received relatively little attention: it is not clear whether they are able to distinguish the combinations that are simply atypical, or implausible, from the semantically anomalous ones, and in particular, they have never been tested on the task of modeling their differences in processing complexity. In this paper, we compare two different models of thematic fit by testing their ability of identifying violations of selectional restrictions in two datasets from the experimental studies. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:\/\/ creativecommons.org\/licenses\/by\/4.0\/ 1 See McRae and Matsuki (2009) for an overview. 2 A partial exception is the study on semantic deviance by Vecchi et al. (2011). However, they focus on the acceptability of adjectival phrases, rather than on selectional preferences.",
        "id":53471894
      },
      {
        "title":"A Deterministic Dependency Parser with Dynamic Programming for Sanskrit",
        "text":"We describe a Deterministic Dependency Parser for Sanskrit. The parse is developed following a Depth First traversal of a graph whose nodes represent morphological analyses of the words in a sentence. During the traversal, relations at each node are checked for local compatibility, and finally for each full path, the relations on the path are checked for global compatibility. Stacking of intermediate results guarantees dynamic programming. We also describe an interface that displays multiple parses compactly and facilitates users to select the desired parse among various possible solutions with a maximum of n − 1 choices for a sentence with n words.",
        "id":16258834
      },
      {
        "title":"",
        "text":"Providing natural language instructions in prompts is a useful new paradigm for improving task performance of large language models in a zero-shot setting. Recent work has aimed to improve such prompts via manual rewriting or gradient-based tuning. However, manual rewriting is time-consuming and requires subjective interpretation, while gradient-based tuning can be extremely computationally demanding for large models and may not be feasible for API-based models. In this work, we introduce Gradient-free Instructional Prompt Search (GRIPS), a gradient-free, edit-based search approach for improving task instructions for large language models. GRIPS takes in instructions designed for humans and automatically returns an improved, edited prompt, while allowing for API-based tuning. With Instruct-GPT models, GRIPS improves the average task performance by up to 4.30 percentage points on eight classification tasks from the NATU-RAL-INSTRUCTIONS dataset (with similar improvements for OPT, BLOOM, and FLAN-T5). We see improvements for both instructiononly prompts and instruction + k-shot examples prompts. Notably, GRIPS outperforms manual rewriting and purely example-based prompts while controlling for the available compute and data budget. Further, performance of GRIPS is comparable to select gradient-based tuning approaches. Qualitatively, we show our edits can simplify instructions and at times make them incoherent but nonetheless improve accuracy. , et al. 2022. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.",
        "id":247447170
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Is there any paper applies off-shelf GPT-2 model in D4RL tasks, using PEFT techniques?",
    "positive_ctxs":[
      {
        "title":"UNLEASHING THE POWER OF PRE-TRAINED LANGUAGE MODELS FOR OFFLINE REINFORCEMENT LEARNING",
        "text":"Offline reinforcement learning (RL) aims to find a near-optimal policy using pre-collected datasets.In real-world scenarios, data collection could be costly and risky; therefore, offline RL becomes particularly challenging when the indomain data is limited.Given recent advances in Large Language Models (LLMs) and their few-shot learning prowess, this paper introduces Language Models for Motion Control (LaMo), a general framework based on Decision Transformers to effectively use pre-trained Language Models (LMs) for offline RL.Our framework highlights four crucial components: (1) Initializing Decision Transformers with sequentially pre-trained LMs, (2) employing the LoRA fine-tuning method, in contrast to full-weight fine-tuning, to combine the pre-trained knowledge from LMs and in-domain knowledge effectively, (3) using the non-linear MLP transformation instead of linear projections, to generate embeddings, and (4) integrating an auxiliary language prediction loss during fine-tuning to stabilize the LMs and retain their original abilities on languages.Empirical results indicate LaMo achieves state-of-the-art performance in sparse-reward tasks and closes the gap between value-based offline RL methods and decision transformers in dense-reward tasks.In particular, our method demonstrates superior performance in scenarios with limited data samples.Our project website is lamo2023.github.io.",
        "id":264802494
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":219307936
      },
      {
        "title":"",
        "text":"In this paper, MARKET metaphors used by different communities (Chinese, Malay and English) are laid out based on the frequency counts of these metaphors and their occurrences in different syntactic positions. The results show that certain types of metaphors have preferences for different syntactic positions for 'market.' For instance, MARKET IS A PERSON in all three languages prefers to place 'market' in the subject position. In addition to this finding, the choice of metaphor types by different speech communities may also reflect their perspectives regarding their country's economy. This is evidenced by the fewer instances of MARKET IS COMPETITION in the English data. The instances that describe how the market falls (plunges and crashes) may reflect the speakers' concerns with the maintenance of their power in the market rather than the competitiveness of their market. Therefore, through using quantitative data, this paper is able to infer the economic status of these speech communities. This can be done not only through analyzing the semantic meanings of the metaphors but also their interface with syntax.Interpersonal metaphor, on the other hand, is pragmatics-based in which phrases such as I think and I don't think can also carry metaphorical meanings in expressing congruency of ideas (such as I think will be congruent with probably). Compared to the model by Lakoff and Johnson, Halliday's grammatical metaphor seems to appear at the other end of the continuum between semantics and syntax. The relationship between conceptual metaphors and syntax was never mentioned especially when more than a language is involved.The aims of this work, therefore, are a) to try to extend the analysis of conceptual metaphors byAhrens et al. (2003)and  so that the grammatical aspect can be included in the analysis; and b) to find out the relationship between the MARKET metaphors and the syntactic positions in which MARKET occurs. This is because how much 'market' is a subject or object may reflect how a speech communities view the position of 'market' in the world. The conceptual metaphors from three languages (Chinese, Malay and English) are examined and the roles of 'market' are analyzed cross-linguistically. Our research questions are as follow: Are the conceptual metaphors shared by these languages are similar? Are the syntactical positions of the target word 'market' similar across these languages and what do the differences in (a) and (b) say about the three speech communities? By answering these three questions, it is hoped that the steps in identifying cultural differences can be operationalized.(2001)examined metaphors in financial reporting in English and Spanish based on the market crash in 1997. Their analysis showed that THE ECONOMY IS AN ORGANISM has the highest frequency in financial reports, followed by MARKET MOVEMENTS ARE PHYSICAL MOVEMENTS and MARKET MOVEMENTS ARE NATURAL DISASTER. Within these metaphors, there are sub-metaphors. For instance, examples that refer to both physical conflict and state of health fall under the source domain of ORGANISM. Chung, Ahrens and Sung (2003) also carried out an analysis of STOCK MARKET metaphors in Chinese and English and they compared specifically STOCK MARKET IS OCEAN WATER to Charteris-Black and Ennis's MARKET MOVEMENTS ARE NAUTICAL OR ARE WAYS OF MOVING IN THE WATER (under MARKET MOVEMENTS ARE PHYSICAL MOVEMENTS). They criticized that the source domains are not that clear. For instance, the source domain of ORGANISM is too general because it may refers to plants, animals and any aspects of organisms. The question regarding identifying source domains is discussed in Chung, Ahrens and Huang (In Press). Therefore, for Charteris-Black and Ennis's metaphor MARKET MOVEMENTS ARE NAUTICAL OR ARE WAYS OF MOVING IN THE WATER,  suggested that the metaphorical instances can be sub-divided into OCEAN WATER and BOAT. This again showed the over-general source domains in the work of Charteris-Black and Ennis. In addition, this paper will only focus on the target word 'MARKET' (although not as narrow as STOCK MARKET, JOB MARKET and other types of markets) and exclude the interference of target such as TRADING.MARKET metaphorCharteris-Black and Ennis",
        "id":17936833
      },
      {
        "title":"Improving Scientific Relation Classification with Task Specific Supersense",
        "text":"Classifying the relationship between entities is an important natural language processing (NLP) task. Scientific Relation Classification aims at automatically categorizing scientific semantic relationships among entities in scientific documents. Conventionally, only task unspecific supersense, such as supersense (or hyernym) from WordNet (e.g., ANIMAL is the supersense of \"dog\"), is used as a feature for relation classification. In this work, we hypothesize that task specific supersense could also be utilized as an informative feature for relation classification. Specifically, we define a new entity type based on the property of a given task, and facilitate scientific relation classification with the task specific supersense. Our experiments on three different datasets prove the effectiveness of the task specific supersense on relation classification in scientific articles.1In this work, entity refers not merely to concepts denoted by noun or noun phrase, it could be actions denoted by verb or verb phrase, and evaluation denoted by adjective or adverb etc.",
        "id":198935469
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Is there any paper improves adversarial training by forming semantic aware label without extra pre-train time or data?",
    "positive_ctxs":[
      {
        "title":"Annealing Self-Distillation Rectification Improves Adversarial Training",
        "text":"In standard adversarial training, models are optimized to fit one-hot labels within allowable adversarial perturbation budgets. However, the ignorance of underlying distribution shifts brought by perturbations causes the problem of robust overfitting. To address this issue and enhance adversarial robustness, we analyze the characteristics of robust models and identify that robust models tend to produce smoother and well-calibrated outputs. Based on the observation, we propose a simple yet effective method, Annealing Self-Distillation Rectification (ADR), which generates soft labels as a better guidance mechanism that accurately reflects the distribution shift under attack during adversarial training. By utilizing ADR, we can obtain rectified distributions that significantly improve model robustness without the need for pre-trained models or extensive extra computation. Moreover, our method facilitates seamless plug-and-play integration with other adversarial training techniques by replacing the hard labels in their objectives. We demonstrate the efficacy of ADR through extensive experiments and strong performances across datasets.Preprint. Under review.",
        "id":258833682
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":219310229
      },
      {
        "title":"Socratic Question Generation: A Novel Dataset, Models, and Evaluation",
        "text":"Socratic questioning is a form of reflective inquiry often employed in education to encourage critical thinking in students, or to elicit awareness of beliefs and perspectives in a subject during therapeutic counseling. Specific types of Socratic questions are employed for enabling reasoning and alternate views against the context of individual personal opinions on a topic. Socratic contexts are different from traditional question generation contexts where \"answer-seeking\" questions are generated against a given formal passage on a topic, narrative stories or conversations.We present SocratiQ, the first large dataset of 110K (question, context) pairs for enabling studies on Socratic Question Generation (SoQG ). We provide an in-depth study on the various types of Socratic questions and present models for generating Socratic questions against a given context through prompt tuning. Our automated and human evaluation results demonstrate that our SoQG models can produce realistic, type-sensitive, human-like Socratic questions enabling potential applications in counseling and coaching.",
        "id":258378353
      },
      {
        "title":"Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions",
        "text":"The context in which language is used provides a strong signal for learning to recover its meaning. In this paper, we show it can be used within a grounded CCG semantic parsing approach that learns a joint model of meaning and context for interpreting and executing natural language instructions, using various types of weak supervision. The joint nature provides crucial benefits by allowing situated cues, such as the set of visible objects, to directly influence learning. It also enables algorithms that learn while executing instructions, for example by trying to replicate human actions. Experiments on a benchmark navigational dataset demonstrate strong performance under differing forms of supervision, including correctly executing 60% more instruction sets relative to the previous state of the art.",
        "id":9963298
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Is there any paper that explores ways to parameterize neural networks as proximal operators?",
    "positive_ctxs":[
      {
        "title":"What's in a Prior? Learned Proximal Networks for Inverse Problems",
        "text":"Proximal operators are ubiquitous in inverse problems, commonly appearing as part of algorithmic strategies to regularize problems that are otherwise ill-posed.Modern deep learning models have been brought to bear for these tasks too, as in the framework of plug-and-play or deep unrolling, where they loosely resemble proximal operators.Yet, something essential is lost in employing these purely data-driven approaches: there is no guarantee that a general deep network represents the proximal operator of any function, nor is there any characterization of the function for which the network might provide some approximate proximal.This not only makes guaranteeing convergence of iterative schemes challenging but, more fundamentally, complicates the analysis of what has been learned by these networks about their training data.Herein we provide a framework to develop learned proximal networks (LPN), prove that they provide exact proximal operators for a data-driven nonconvex regularizer, and show how a new training strategy, dubbed proximal matching, provably promotes the recovery of the log-prior of the true data distribution.Such LPN provide general, unsupervised, expressive proximal operators that can be used for general inverse problems with convergence guarantees.We illustrate our results in a series of cases of increasing complexity, demonstrating that these models not only result in state-of-the-art performance, but provide a window into the resulting priors learned from data.",
        "id":264426077
      }
    ],
    "negative_ctxs":[
      {
        "title":"Linking Pictographs to Synsets: Sclera2Cornetto",
        "text":"Social inclusion of people with Intellectual and Developmental Disabilities can be promoted by offering them ways to independently use the internet. People with reading or writing disabilities can use pictographs instead of text. We present a resource in which we have linked a set of 5710 pictographs to lexical-semantic concepts in Cornetto, a Wordnet-like database for Dutch. We show that, by using this resource in a text-to-pictograph translation system, we can greatly improve the coverage comparing with a baseline where words are converted into pictographs only if the word equals the filename.",
        "id":1209041
      },
      {
        "title":"Jointly Identifying Entities and Extracting Relations in Encyclopedia Text via A Graphical Model Approach *",
        "text":"In this paper, we investigate the problem of entity identification and relation extraction from encyclopedia articles, and we propose a joint discriminative probabilistic model with arbitrary graphical structure to optimize all relevant subtasks simultaneously. This modeling offers a natural formalism for exploiting rich dependencies and interactions between relevant subtasks to capture mutual benefits, as well as a great flexibility to incorporate a large collection of arbitrary, overlapping and nonindependent features. We show the parameter estimation algorithm of this model. Moreover, we propose a new inference method, namely collective iterative classification (CIC), to find the most likely assignments for both entities and relations. We evaluate our model on real-world data from Wikipedia for this task, and compare with current state-of-the-art pipeline and joint models, demonstrating the effectiveness and feasibility of our approach.",
        "id":6814450
      },
      {
        "title":"FEUP at SemEval-2017 Task 5: Predicting Sentiment Polarity and Intensity with Financial Word Embeddings",
        "text":"This paper presents the approach developed at the Faculty of Engineering of University of Porto, to participate in SemEval 2017, Task 5: Fine-grained Sentiment Analysis on Financial Microblogs and News. The task consisted in predicting a real continuous variable from -1.0 to +1.0 representing the polarity and intensity of sentiment concerning companies\/stocks mentioned in short texts. We modeled the task as a regression analysis problem and combined traditional techniques such as pre-processing short texts, bag-of-words representations and lexical-based features with enhanced financial specific bag-ofembeddings. We used an external collection of tweets and news headlines mentioning companies\/stocks from S&P 500 to create financial word embeddings which are able to capture domain-specific syntactic and semantic similarities. The resulting approach obtained a cosine similarity score of 0.69 in sub-task 5.1 -Microblogs and 0.68 in sub-task 5.2 -News Headlines.",
        "id":16311779
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Is there any paper that previously proposed to control a risk using prediction sets, based on the literature in conformal prediction?",
    "positive_ctxs":[
      {
        "title":"Conformal Risk Control",
        "text":"We extend conformal prediction to control the expected value of any monotone loss function. The algorithm generalizes split conformal prediction together with its coverage guarantee. Like conformal prediction, the conformal risk control procedure is tight up to an O(1\/n) factor. We also introduce extensions of the idea to distribution shift, quantile risk control, multiple and adversarial risk control, and expectations of U-statistics. Worked examples from computer vision and natural language processing demonstrate the usage of our algorithm to bound the false negative rate, graph distance, and token-level F1-score.",
        "id":251320513
      }
    ],
    "negative_ctxs":[
      {
        "title":"Linking WordNet Verb Classes to Semantic Interpretation",
        "text":"An analysis that defines predicates for Wordnet verb classes and links them to semantic interpretation is presented. The selectional restrictions for the thematic roles defining the predicates are WordNet ontological categories. Thematic roles are also linked to the syntactic relations that realize them. The paper illustrates the methodology by providing a detailed analysis of some major WordNet verb classes.! !",
        "id":2668408
      },
      {
        "title":"Information Content Measures of Semantic Similarity Perform Better Without Sense-Tagged Text",
        "text":"This paper presents an empirical comparison of similarity measures for pairs of concepts based on Information Content. It shows that using modest amounts of untagged text to derive Information Content results in higher correlation with human similarity judgments than using the largest available corpus of manually annotated sense-tagged text.",
        "id":1085680
      },
      {
        "title":"Transition-based Neural Constituent Parsing",
        "text":"Constituent parsing is typically modeled by a chart-based algorithm under probabilistic context-free grammars or by a transition-based algorithm with rich features. Previous models rely heavily on richer syntactic information through lexicalizing rules, splitting categories, or memorizing long histories. However enriched models incur numerous parameters and sparsity issues, and are insufficient for capturing various syntactic phenomena. We propose a neural network structure that explicitly models the unbounded history of actions performed on the stack and queue employed in transition-based parsing, in addition to the representations of partially parsed tree structure. Our transition-based neural constituent parsing achieves performance comparable to the state-of-the-art parsers, demonstrating F1 score of 90.68% for English and 84.33% for Chinese, without reranking, feature templates or additional data to train model parameters.",
        "id":16313885
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Is there any paper that seamlessly integrates the multigrid structure in operator learning for solving partial differential equations (PDEs)?",
    "positive_ctxs":[
      {
        "title":"MgNO: Efficient Parameterization of Linear Operators via Multigrid",
        "text":"In this work, we propose a concise neural operator architecture for operator learning.Drawing an analogy with a conventional fully connected neural network, we define the neural operator as follows: the output of the i-th neuron in a nonlinear operator layer is defined by O i (u) = σ j W i j u + B i j .Here, W i j denotes the bounded linear operator connecting j-th input neuron to i-th output neuron, and the bias B i j takes the form of a function rather than a scalar.Given its new universal approximation property, the efficient parameterization of the bounded linear operators between two neurons (Banach spaces) plays a critical role.As a result, we introduce MgNO, utilizing multigrid structures to parameterize these linear operators between neurons.This approach offers both mathematical rigor and practical expressivity.Additionally, MgNO obviates the need for conventional lifting and projecting operators typically required in previous neural operators.Moreover, it seamlessly accommodates diverse boundary conditions.Our empirical observations reveal that MgNO exhibits superior ease of training compared to other CNNbased models, while also displaying a reduced susceptibility to overfitting when contrasted with spectral-type neural operators.We demonstrate the efficiency and accuracy of our method with consistently state-of-the-art performance on different types of partial differential equations (PDEs).",
        "id":264825357
      }
    ],
    "negative_ctxs":[
      {
        "title":"A Gold Standard to Measure Relative Linguistic Complexity with a Grounded Language Learning Model",
        "text":"This paper focuses on linguistic complexity from a relative perspective. It presents a grounded language learning system that can be used to study linguistic complexity from a developmental point of view and introduces a tool for generating a gold standard in order to evaluate the performance of the learning system. In general, researchers agree that it is more feasible to approach complexity from an objective or theory-oriented viewpoint than from a subjective or user-related point of view. Studies that have adopted a relative complexity approach have showed some preferences for L2 learners. In this paper, we try to show that computational models of the process of language acquisition may be an important tool to consider children and the process of first language acquisition as suitable candidates for evaluating the complexity of languages.This work is licensed under a Creative Commons Attribution 4.0 International Licence.Licence details:",
        "id":53582294
      },
      {
        "title":"Estimation of Discourse Segmentation Labels from Crowd Data",
        "text":"For annotation tasks involving independent judgments, probabilistic models have been used to infer ground truth labels from data where a crowd of many annotators labels the same items. Such models have been shown to produce results superior to taking the majority vote, but have not been applied to sequential data. We present two methods to infer ground truth labels from sequential annotations where we assume judgments are not independent, based on the observation that an annotator's segments all tend to be several utterances long. The data consists of crowd labels for annotation of discourse segment boundaries. The new methods extend Hidden Markov Models to relax the independence assumption. The two methods are distinct, so positive labels proposed by both are taken to be ground truth. In addition, results of the models are checked using metrics that test whether an annotator's accuracy relative to a given model remains consistent across different conversations.",
        "id":27609
      },
      {
        "title":"Effective Batching for Recurrent Neural Network Grammars",
        "text":"As a language model that integrates traditional symbolic operations and flexible neural representations, recurrent neural network grammars (RNNGs) have attracted great attention from both scientific and engineering perspectives. However, RNNGs are known to be harder to scale due to the difficulty of batched training. In this paper, we propose effective batching for RNNGs, where every operation is computed in parallel with tensors across multiple sentences. Our PyTorch implementation effectively employs a GPU and achieves x6 speedup compared to the existing C++ DyNet implementation with model-independent auto-batching. Moreover, our batched RNNG also accelerates inference and achieves x20-150 speedup for beam search depending on beam sizes. Finally, we evaluate syntactic generalization performance of the scaled RNNG against the LSTM baseline, based on the large training data of 100M tokens from English Wikipedia and the broad-coverage targeted syntactic evaluation benchmark. 1",
        "id":235253831
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Is there any paper that theoretically explains why in-context reinforcement learning works?",
    "positive_ctxs":[
      {
        "title":"Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining",
        "text":"Large transformer models pretrained on offline reinforcement learning datasets have demonstrated remarkable in-context reinforcement learning (ICRL) capabilities, where they can make good decisions when prompted with interaction trajectories from unseen environments. However, when and how transformers can be trained to perform ICRL have not been theoretically well-understood. In particular, it is unclear which reinforcement-learning algorithms transformers can perform in context, and how distribution mismatch in offline training data affects the learned algorithms. This paper provides a theoretical framework that analyzes supervised pretraining for ICRL. This includes two recently proposed training methods -algorithm distillation and decision-pretrained transformers. First, assuming model realizability, we prove the supervised-pretrained transformer will imitate the conditional expectation of the expert algorithm given the observed trajectory. The generalization error will scale with model capacity and a distribution divergence factor between the expert and offline algorithms. Second, we show transformers with ReLU attention can efficiently approximate near-optimal online reinforcement learning algorithms like LinUCB and Thompson sampling for stochastic linear bandits, and UCB-VI for tabular Markov decision processes. This provides the first quantitative analysis of the ICRL capabilities of transformers pretrained from offline trajectories. an online RL algorithm; and (3) when can supervised pretraining find such a good transformer. Specifically, this paper investigates the following open question:How can supervised pretraining on Transformers learn in-context reinforcement learning?In this paper, we initiate a theoretical study of the ICRL capability of transformers under supervised pretraining to address the open questions outlined above. We show that (1) Transformers can implement prevalent RL algorithms, including LinUCB and Thompson sampling for stochastic linear bandits, and UCB-VI for tabular Markov decision processes; (2) The algorithms learned by transformers achieve near-optimal regret bounds in their respective settings; (3) Supervised pretraining find such algorithms as long as the sample size scales with the covering number of transformer class and distribution ratio between expert and offline algorithms.Summary of contributions and paper outline• We propose a general framework for supervised pretraining approaches to meta-reinforcement learning (Section 2). This framework encompasses existing methods like Algorithm Distillation (Laskin et al.,  2022), where the expert and context algorithms are identical, as well as Decision-Pretrained Transformers (Lee et al., 2023), where the expert generates optimal actions for the MDP. It also includes approximate DPT variants where the expert estimates optimal actions from full interaction trajectories.• We prove that the supervised-pretrained transformer will imitate the conditional expectation of the expert algorithm given the observed trajectory (Section 3). The generalization error scales with both model capacity and a distribution ratio measuring divergence between the expert algorithm and the algorithm that generated offline trajectories.• We demonstrate that transformers can effectively approximate several near-optimal reinforcement learning algorithms by taking observed trajectories as context inputs (Section 4). Specifically, we show transformers can approximate LinUCB (Section 4.1) and Thompson sampling algorithms (Section 4.2) for stochastic linear bandit problems, and UCB-VI (Section 4.3) for tabular Markov decision processes. Combined with the generalization error bound from supervised pretraining and regret bounds of these RL algorithms, this provides regret bounds for supervised-pretrained transformers.• Preliminary experiments validate that transformers can perform ICRL in our setup (Section 5).• Technically, we prove efficient approximation of LinUCB by showing transformers can implement accelerated gradient descent for solving ridge regression (Appendix D.4), enabling fewer attention layers than the vanilla gradient descent approach inBai et al. (2023). To enable efficient Thompson sampling implementation, we prove transformers can compute matrix square roots through the Pade decomposition (Appendix E.3). These approximation results are interesting in their own right.",
        "id":263909278
      }
    ],
    "negative_ctxs":[
      {
        "title":"Chinese Classifier Assignment Using SVMs",
        "text":"In Chinese, nouns need numeral classifiers to express quantity. In this paper, we explore the relationship between classifiers and nouns. We extract a set of lexical, syntactic and ontological features and the corresponding noun-classifier pairs from a corpus and then train SVMs to assign classifers to nouns. We analyse which features are most important for this task.",
        "id":7393294
      },
      {
        "title":"Gei 3 ta 1 in Taiwan Mandarin---A Particular Construction *",
        "text":"The present paper investigates a particular structure in Taiwan Mandarin, \"(NP) + (intensifier) + gei 3 ta 1 \"give him\/it\"+ adjective\" in terms of construction grammar. The structure is mostly observed in utterances of younger generation. Though it is not regarded as a grammatical or standard structure, it is still a register of language. The structure lays emphasis on speaker's attitude toward an undesired, unpleasant event. In most cases, the attitude tends to be negative. The events or propositions must have existed or been completed. The adjectives compatible with this structure belong to category of higher degree. The grammatical usage illustrates semantic bleaching of gei 3 ta 1 . And the changes from giving to a grammatical particle denoting subjective belief is a kind of subjectification. Moreover, ta 1 could refer to events or situation expressed by a more complicated grammatical structure, or denotes nothing as a dummy word. Though many previous studies paid attention to the newly developed structure resulted from language contact, the adequate account was not provided. It is hoped through this investigation, we will get a better understanding of this particular structure.Keywords: gei 3 ta 1 , construction meaning, speaker's subjective attitude\/belief, negative, event * I would like to express my heartfelt thanks to the anonymous reviewers for comments and suggestions on the earlier abridged version of this paper. Needless to say, I am solely responsible for any infelicities.",
        "id":12322869
      },
      {
        "title":"Language-Independent Sentiment Analysis Using Subjectivity and Positional Information",
        "text":"We describe a novel language-independent approach to the task of determining the polarity, positive or negative, of the author's opinion on a specific topic in natural language text. In particular, weights are assigned to attributes, individual words or word bi-grams, based on their position and on their likelihood of being subjective. The subjectivity of each attribute is estimated in a two-step process, where first the probability of being subjective is calculated for each sentence containing the attribute, and then these probabilities are used to alter the attribute's weights for polarity classification. The evaluation results on a standard dataset of movie reviews shows 89.85% classification accuracy, which rivals the best previously published results for this dataset for systems that use no additional linguistic information nor external resources.",
        "id":12954317
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Is there any paper that uses Lipschitz continuity in learning a dynamics model?",
    "positive_ctxs":[
      {
        "title":"CCIL: CONTINUITY-BASED DATA AUGMENTATION FOR CORRECTIVE IMITATION LEARNING",
        "text":"We present a new technique to enhance the robustness of imitation learning methods by generating corrective data to account for compounding errors and disturbances.While existing methods rely on interactive expert labeling, additional offline datasets, or domain-specific invariances, our approach requires minimal additional assumptions beyond access to expert data.The key insight is to leverage local continuity in the environment dynamics to generate corrective labels.Our method first constructs a dynamics model from the expert demonstration, encouraging local Lipschitz continuity in the learned model.In locally continuous regions, this model allows us to generate corrective labels within the neighborhood of the demonstrations but beyond the actual set of states and actions in the dataset.Training on this augmented data enhances the agent's ability to recover from perturbations and deal with compounding errors.We demonstrate the effectiveness of our generated labels through experiments in a variety of robotics domains in simulation that have distinct forms of continuity and discontinuity, including classic control problems, drone flying, navigation with high-dimensional sensor observations, legged locomotion, and tabletop manipulation.",
        "id":264306002
      }
    ],
    "negative_ctxs":[
      {
        "title":"DISENTANGLEMENT ANALYSIS WITH PARTIAL INFOR- MATION DECOMPOSITION",
        "text":"We propose a framework to analyze how multivariate representations disentangle ground-truth generative factors. A quantitative analysis of disentanglement has been based on metrics designed to compare how one variable explains each generative factor. Current metrics, however, may fail to detect entanglement that involves more than two variables, e.g., representations that duplicate and rotate generative factors in high dimensional spaces. In this work, we establish a framework to analyze information sharing in a multivariate representation with Partial Information Decomposition and propose a new disentanglement metric. This framework enables us to understand disentanglement in terms of uniqueness, redundancy, and synergy. We develop an experimental protocol to assess how increasingly entangled representations are evaluated with each metric and confirm that the proposed metric correctly responds to entanglement. Through experiments on variational autoencoders, we find that models with similar disentanglement scores have a variety of characteristics in entanglement, for each of which a distinct strategy may be required to obtain a disentangled representation.",
        "id":237364241
      },
      {
        "title":"Incrementality all the way up",
        "text":"A major challenge in formal analysis of discourse is that inferential connections between different parts of a text or dialogue are usually not logical in the technical sense. Instead they are based on knowledge (or beliefs) which are assumed to be shared between the agents involved in the discourse. Because of its interactive nature, allowing for feedback, clarification requests, etc. inference in dialogue may be even harder to describe formally. It is well known that language, in particular dialogue, is incremental at phonetic, syntactic and semantic levels. In this paper we will take a closer look at incremental interpretation of reasoning in dialogue. Our approach takes rhetorical reasoning as its point of departure and utilises some key concepts from classical rhetoric. We also suggest a way of analysing rhetorical arguments in dialogue using an information state update approach cast in Type Theory with Records, that will allow us to represent how dialogue participants draw inferences incrementally, that is tentatively make an inference, and then recompute it in the light of more specified information.In(1)we see an example of how a dialogue participant interprets language incrementally. Following A's production of the noun phrase (NP) 'the doctor', i.e. before there is a complete proposition to interpret, B clarifies who is being referred to by A, thus indicating that for B, the referent of the NP is underspecified at this point in the exchange. Note that B could have produced a backchannel ('mm') here instead of the clarification request (CR), in which case we would not know whether or not the NP was underspecified for B -B may have correctly identified the specific doctor 'Chorlton' as the intended referent, or B might not have cared who the doctor was.",
        "id":45224087
      },
      {
        "title":"Comparing set-covering strategies for optimal corpus design",
        "text":"This article is interested in the problem of the linguistic content of a speech corpus. Depending on the target task, the phonological and linguistic content of the corpus is controlled by collecting a set of sentences which covers a preset description of phonological attributes under the constraint of an overall duration as small as possible. This goal is classically achieved by greedy algorithms which however do not guarantee the optimality of the desired cover. In recent works, a lagrangian-based algorithm, called LamSCP, has been used to extract coverings of diphonemes from a large corpus in French, giving better results than a greedy algorithm. We propose to keep comparing both algorithms in terms of the shortest duration, stability and robustness by achieving multi-represented diphoneme or triphoneme covering. These coverings correspond to very large scale optimization problems, from a corpus in English. For each experiment, LamSCP improves the greedy results from 3.9 to 9.7 percent.",
        "id":16713306
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Is there any paper trying to improve MLE for auto-regressive language modeling through the lens of optimal transport?",
    "positive_ctxs":[
      {
        "title":"EMO: EARTH MOVER DISTANCE OPTIMIZATION FOR AUTO-REGRESSIVE LANGUAGE MODELING",
        "text":"Neural language models are probabilistic models of human text.They are predominantly trained using maximum likelihood estimation (MLE), which is equivalent to minimizing the forward cross-entropy between the empirical data distribution and the model distribution.However, various degeneration phenomena are still widely observed when decoding from the distributions learned by such models.We establish that the forward cross-entropy is suboptimal as a distance metric for aligning human and model distribution due to its (1) recall-prioritization (2) negative diversity ignorance and (3) train-test mismatch.In this paper, we propose Earth Mover Distance Optimization (EMO) for auto-regressive language modeling.EMO capitalizes on the inherent properties of earth mover distance to address the aforementioned challenges.Due to the high complexity of direct computation, we further introduce a feasible upper bound for EMO to ease end-to-end training.Upon extensive evaluation of language models trained using EMO and MLE.We find that EMO demonstrates a consistently better language modeling performance than MLE across domains.Moreover, EMO demonstrates noteworthy enhancements in downstream performance with minimal fine-tuning on merely 25,000 sentences.This highlights the tremendous potential of EMO as a lightweight calibration method for enhancing large-scale pre-trained language models.Our code and data are available at https:\/\/github.com\/DRSY\/EMO.",
        "id":263829780
      }
    ],
    "negative_ctxs":[
      {
        "title":"Extracting Person Names from User Generated Text: Named-Entity Recognition for Combating Human Trafficking",
        "text":"Online escort advertisement websites are widely used for advertising victims of human trafficking. Domain experts agree that advertising multiple people in the same ad is a strong indicator of trafficking. Thus, extracting person names from the text of these ads can provide valuable clues for further analysis. However, Named-Entity Recognition (NER) on escort ads is challenging because the text can be noisy, colloquial and often lacking proper grammar and punctuation. Most existing state-of-the-art NER models fail to demonstrate satisfactory performance in this task. In this paper, we propose NEAT (Name Extraction Against Trafficking) for extracting person names. It effectively combines classic rule-based and dictionary extractors with a contextualized language model to capture ambiguous names (e.g penny, hazel) and adapts to adversarial changes in the text by expanding its dictionary. NEAT shows 19% improvement on average in the F1 classification score for name extraction compared to previous state-of-the-art in two domain-specific datasets.",
        "id":248779990
      },
      {
        "title":"",
        "text":"",
        "id":39190241
      },
      {
        "title":"Challenges in Finding Metaphorical Connections",
        "text":"Poetry is known for its novel expression using figurative language. We introduce a writing task that contains the essential challenges of generating meaningful figurative language and can be evaluated. We investigate how to find metaphorical connections between abstract themes and concrete domains by asking people to write four-line poems on a given metaphor, such as \"death is a rose\" or \"anger is wood\". We find that only 24% of poems successfully make a metaphorical connection. We present five alternate ways people respond to the prompt and release our dataset of 186 categorized poems. We suggest opportunities for computational approaches.",
        "id":51998437
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Name a paper which proposes a probabilsitic formulation of retrosynthesis.",
    "positive_ctxs":[
      {
        "title":"RETRO-FALLBACK: RETROSYNTHETIC PLANNING IN AN UNCERTAIN WORLD",
        "text":"Retrosynthesis is the task of proposing a series of chemical reactions to create a desired molecule from simpler, buyable molecules.While previous works have proposed algorithms to find optimal solutions for a range of metrics (e.g.shortest, lowest-cost), these works generally overlook the fact that we have imperfect knowledge of the space of possible reactions, meaning plans created by the algorithm may not work in a laboratory.In this paper we propose a novel formulation of retrosynthesis in terms of stochastic processes to account for this uncertainty.We then propose a novel greedy algorithm called retro-fallback which maximizes the probability that at least one synthesis plan can be executed in the lab.Using in-silico benchmarks we demonstrate that retro-fallback generally produces better sets of synthesis plans than the popular MCTS and retro* algorithms.",
        "id":264128166
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":226283589
      },
      {
        "title":"\"PolNet -Polish WordNet\" project: PolNet 2.0 -a short description of the release",
        "text":"In December 2011\/January 2012 we have released the main deliverable of the project \"PolNet -Polish WordNet\". It was first presented and distributed (as PolNet 1.0) at the 5th Language and Technology Conference in Poznań(2011)and (informally, with kind permission of the organizers) distributed during the Global Wordnet Conference in Matsue, Japan, in January 2012. We intend to present to the participants of the GWC 2014 the characteristics of the new, extended release of PolNet.",
        "id":15068580
      },
      {
        "title":"Nefnir: A high accuracy lemmatizer for Icelandic",
        "text":"Lemmatization, finding the basic morphological form of a word in a corpus, is an important step in many natural language processing tasks when working with morphologically rich languages. We describe and evaluate Nefnir, a new open source lemmatizer for Icelandic. Nefnir uses suffix substitution rules, derived from a large morphological database, to lemmatize tagged text. Evaluation shows that for correctly tagged text, Nefnir obtains an accuracy of 99.55%, and for text tagged with a PoS tagger, the accuracy obtained is 96.88%.",
        "id":198967844
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"What are some evaluation benchmarks for LLM privacy at inference time, targeted towards model input and NOT the training data.",
    "positive_ctxs":[
      {
        "title":"CAN LLMS KEEP A SECRET? TESTING PRIVACY IMPLICATIONS OF LANGUAGE MODELS VIA CONTEXTUAL INTEGRITY THEORY",
        "text":"The interactive use of large language models (LLMs) in AI assistants (at work, home, etc.) introduces a new set of inference-time privacy risks: LLMs are fed different types of information from multiple sources in their inputs and are expected to reason about what to share in their outputs, for what purpose and with whom, within a given context.In this work, we draw attention to the highly critical yet overlooked notion of contextual privacy by proposing CONFAIDE, 1 a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs.Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively.This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning.Our work underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory of mind.",
        "id":264555202
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":227231222
      },
      {
        "title":"Dependency-Based Word Embeddings",
        "text":"While continuous word embeddings are gaining popularity, current models are based solely on linear contexts. In this work, we generalize the skip-gram model with negative sampling introduced by Mikolov et al. to include arbitrary contexts. In particular, we perform experiments with dependency-based contexts, and show that they produce markedly different embeddings. The dependencybased embeddings are less topical and exhibit more functional similarity than the original skip-gram embeddings.",
        "id":2107337
      },
      {
        "title":"Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly",
        "text":"Building on Petroni et al. (2019), we propose two new probing tasks analyzing factual knowledge stored in Pretrained Language Models (PLMs). (1) Negation. We find that PLMs do not distinguish between negated (\"Birds cannot [MASK]\") and non-negated (\"Birds can [MASK]\") cloze questions.(2)Mispriming. Inspired by priming methods in human psychology, we add \"misprimes\" to cloze questions (\"Talk? Birds can [MASK]\"). We find that PLMs are easily distracted by misprimes. These results suggest that PLMs still have a long way to go to adequately learn human-like factual knowledge.",
        "id":218628691
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"What are the key advantages of coupling neural SDEs with neural CDEs for treatment effect estimation over existing baselines?",
    "positive_ctxs":[
      {
        "title":"BAYESIAN NEURAL CONTROLLED DIFFERENTIAL EQUATIONS FOR TREATMENT EFFECT ESTIMATION",
        "text":"Treatment effect estimation in continuous time is crucial for personalized medicine.However, existing methods for this task are limited to point estimates of the potential outcomes, whereas uncertainty estimates have been ignored.Needless to say, uncertainty quantification is crucial for reliable decision-making in medical applications.To fill this gap, we propose a novel Bayesian neural controlled differential equation (BNCDE) for treatment effect estimation in continuous time.In our BNCDE, the time dimension is modeled through a coupled system of neural controlled differential equations and neural stochastic differential equations, where the neural stochastic differential equations allow for tractable variational Bayesian inference.Thereby, for an assigned sequence of treatments, our BNCDE provides meaningful posterior predictive distributions of the potential outcomes.To the best of our knowledge, ours is the first tailored neural method to provide uncertainty estimates of treatment effects in continuous time.As such, our method is of direct practical value for promoting reliable decision-making in medicine.",
        "id":264490587
      }
    ],
    "negative_ctxs":[
      {
        "title":"Using a Morphological Database to Increase the Accuracy in POS Tagging",
        "text":"We experiment with extending the dictionaries used by three open-source partof-speech taggers, by using data from a large Icelandic morphological database. We show that the accuracy of the taggers can be improved significantly by using the database. The reason is that the unknown word ratio reduces dramatically when adding data from the database to the taggers' dictionaries. For the best performing tagger, the overall tagging accuracy increases from the base tagging result of 92.73% to 93.32%, when the unknown word ratio decreases from 6.8% to 1.1%. When we add reliable frequency information to the tag profiles for some of the words originating from the database, we are able to increase the accuracy further to 93.48% -this is equivalent to 10.3% error reduction compared to the base tagger.",
        "id":8965880
      },
      {
        "title":"A NOTE ON MOR2H~E STRUCTURE IN GENF~TIVE",
        "text":"In an early model of generative phonology the lexicon of a language contained entries with as few feature specifications as possible in the interest of economy. The blank feature specifications representing both nondistinctive features and those rendered redundant by sequential constraints were filled in by the same 9honological rules. At this point, the concept of ~ rules changing feature values was unclear. When the distinction between rules that fill in blanks and those that change feature values became clear, it was zmbodied in the concept of morpheme structure rules and P rules. The MS rules were further split into feature redundancy (segment structure) rules and sequ~tial constraint rules. The MS component bore a striking resemblence to the earlier \"pkonotactic\" sections of autonomous phonemic analyses, but the claim was made for I~S ~les that they explained what phonotactiee merely described. The MS rules formed a major part of Chomsky's \"readjustment component\" which rendered th~ output of the syntactic component fit to be the input to the phonological component. A fairly current version of ~his model is the following one from Harms' Introduction t__oo Phonological c~)",
        "id":43449978
      },
      {
        "title":"Picking Apart Story Salads",
        "text":"During natural disasters and conflicts, information about what happened is often confusing, messy, and distributed across many sources. We would like to be able to automatically identify relevant information and assemble it into coherent narratives of what happened. To make this task accessible to neural models, we introduce Story Salads, mixtures of multiple documents that can be generated at scale. By exploiting the Wikipedia hierarchy, we can generate salads that exhibit challenging inference problems. Story salads give rise to a novel, challenging clustering task, where the objective is to group sentences from the same narratives. We demonstrate that simple bag-of-words similarity clustering falls short on this task and that it is necessary to take into account global context and coherence.",
        "id":53081356
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"What is a paper studying data being collected in bundles in reinforcement learning ?",
    "positive_ctxs":[
      {
        "title":"Sample-Efficiency in Multi-Batch Reinforcement Learning: The Need for Dimension-Dependent Adaptivity",
        "text":"We theoretically explore the relationship between sample-efficiency and adaptivity in reinforcement learning.An algorithm is sample-efficient if it uses a number of queries n to the environment that is polynomial in the dimension d of the problem.Adaptivity refers to the frequency at which queries are sent and feedback is processed to update the querying strategy.To investigate this interplay, we employ a learning framework that allows sending queries in K batches, with feedback being processed and queries updated after each batch.This model encompasses the whole adaptivity spectrum, ranging from non-adaptive 'offline' (K \" 1) to fully adaptive (K \" n) scenarios, and regimes in between.For the problems of policy evaluation and best-policy identification under d-dimensional linear function approximation, we establish Ωplog log dq lower bounds on the number of batches K required for sample-efficient algorithms with n \" Oppolypdqq queries.Our results show that just having adaptivity (K ą 1) does not necessarily guarantee sampleefficiency.Notably, the adaptivity-boundary for sample-efficiency is not between offline reinforcement learning (K \" 1), where sample-efficiency was known to not be possible, and adaptive settings.Instead, the boundary lies between different regimes of adaptivity and depends on the problem dimension.",
        "id":263609164
      }
    ],
    "negative_ctxs":[
      {
        "title":"Evaluating Response Strategies in a Web-Based Spoken Dialogue Agent",
        "text":"While the notion of a cooperative response has been the focus of considerable research in natural language dialogue systems, there has been little empirical work demonstrating how such responses lead to more efficient, natural, or successful dialogues. This paper presents an experimental evaluation of two alternative response strategies in TOOT, a spoken dialogue agent that allows users to access train schedules stored on the web via a telephone conversation. We compare the performance of two versions of TOOT (literal and cooperative), by having users carry out a set of tasks with each version. By using hypothesis testing methods, we show that a combination of response strategy, application task, and task\/strategy interactions account for various types of performance differences. By using the PARADISE evaluation framework to estimate an overall performance function, we identify interdependencies that exist between speech recognition and response strategy. Our results elaborate the conditions under which TOOT' s cooperative rather than literal strategy contributes to greater performance.",
        "id":1136893
      },
      {
        "title":"Point2SSM: Learning Morphological Variations of Anatomies from Point Clouds",
        "text":"We introduce Point2SSM, a novel unsupervised learning approach that can accurately construct correspondence-based statistical shape models (SSMs) of anatomy directly from point clouds. SSMs are crucial in clinical research for analyzing the population-level morphological variation in bones and organs. However, traditional methods for creating SSMs have limitations that hinder their widespread adoption, such as the need for noise-free surface meshes or binary volumes, reliance on assumptions or predefined templates, and simultaneous optimization of the entire cohort leading to lengthy inference times given new data. Point2SSM overcomes these barriers by providing a data-driven solution that infers SSMs directly from raw point clouds, reducing inference burdens and increasing applicability as point clouds are more easily acquired. Deep learning on 3D point clouds has seen recent success in unsupervised representation learning, point-to-point matching, and shape correspondence; however, their application to constructing SSMs of anatomies is largely unexplored. In this work, we benchmark state-of-the-art point cloud deep networks on the task of SSM and demonstrate that they are not robust to the challenges of anatomical SSM, such as noisy, sparse, or incomplete input and significantly limited training data. Point2SSM addresses these challenges via an attention-based module that provides correspondence mappings from learned point features. We demonstrate that the proposed method significantly outperforms existing networks in terms of both accurate surface sampling and correspondence, better capturing population-level statistics.Preprint. Under review.",
        "id":258865994
      },
      {
        "title":"ALEXSIS-PT: A New Resource for Portuguese Lexical Simplification",
        "text":"Lexical simplification (LS) is the task of automatically replacing complex words for easier ones making texts more accessible to various target populations (e.g. individuals with low literacy, individuals with learning disabilities, second language learners). To train and test models, LS systems usually require corpora that feature complex words in context along with their candidate substitutions. To continue improving the performance of LS systems we introduce ALEXSIS-PT, a novel multi-candidate dataset for Brazilian Portuguese LS containing 9,605 candidate substitutions for 387 complex words. ALEXSIS-PT has been compiled following the ALEXSIS protocol for Spanish opening exciting new avenues for crosslingual models. ALEXSIS-PT is the first LS multi-candidate dataset that contains Brazilian newspaper articles. We evaluated four models for substitute generation on this dataset, namely mDistilBERT, mBERT, XLM-R, and BERTimbau. BERTimbau achieved the highest performance across all evaluation metrics.",
        "id":252367466
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"What is the first paper that theoretically studies training neural networks under small initialization?",
    "positive_ctxs":[
      {
        "title":"Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization",
        "text":"This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an O( log n √ µ ) upper bound on the time it takes for all neurons to achieve good alignment with the input data, where n is the number of data points and µ measures how well the data are separated. After the early alignment phase, the loss converges to zero at a O( 1 t ) rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.",
        "id":260125817
      }
    ],
    "negative_ctxs":[
      {
        "title":"Video-to-HamNoSys Automated Annotation System",
        "text":"The Hamburg Notation System (HamNoSys) was developed for movement annotation of any sign language (SL) and can be used to produce signing animations for a virtual avatar with the JASigning platform. This provides the potential to use HamNoSys, i.e., strings of characters, as a representation of an SL corpus instead of video material. Processing strings of characters instead of images can significantly contribute to sign language research. However, the complexity of HamNoSys makes it difficult to annotate without a lot of time and effort. Therefore annotation has to be automatized. This work proposes a conceptually new approach to this problem. It includes a new tree representation of the HamNoSys grammar that serves as a basis for the generation of grammatical training data and classification of complex movements using machine learning. Our automatic annotation system relies on HamNoSys grammar structure and can potentially be used on already existing SL corpora. It is retrainable for specific settings such as camera angles, speed, and gestures. Our approach is conceptually different from other SL recognition solutions and offers a developed methodology for future research.",
        "id":219303000
      },
      {
        "title":"A Risk-Averse Mechanism for Suicidality Assessment on Social Media",
        "text":"Recent studies have shown that social media has increasingly become a platform for users to express suicidal thoughts outside traditional clinical settings. With advances in Natural Language Processing strategies, it is now possible to design automated systems to assess suicide risk. However, such systems may generate uncertain predictions, leading to severe consequences. We hence reformulate suicide risk assessment as a selective prioritized prediction problem over the Columbia Suicide Severity Risk Scale (C-SSRS). We propose SASI, a risk-averse and self-aware transformer-based hierarchical attention classifier, augmented to refrain from making uncertain predictions. We show that SASI is able to refrain from 83% of incorrect predictions on real-world Reddit data. Furthermore, we discuss the qualitative, practical, and ethical aspects of SASI for suicide risk assessment as a human-in-the-loop framework. *",
        "id":248780215
      },
      {
        "title":"Learning in Natural Language: Theory and Algorithmic Approaches*",
        "text":"This article summarizes work on developing a learning theory account for the major learning and statistics based approaches used in natural language processing. It shows that these approaches can all be explained using a single distribution free inductive principle related to the pac model of learning. Furthermore, they all make predictions using the same simple knowledge representation -a linear representation over a common feature space. This is significant both to explaining the generalization and robustness properties of these methods and to understanding how these methods might be extended to learn from more structured, knowledge intensive examples, as part of a learning centered approach to higher level natural language inferences.",
        "id":12020345
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"What is the first paper that uses the generalized linear model to analyze multi-neural spike train data?",
    "positive_ctxs":[
      {
        "title":"ONE-HOT GENERALIZED LINEAR MODEL FOR SWITCHING BRAIN STATE DISCOVERY",
        "text":"Exposing meaningful and interpretable neural interactions is critical to understanding neural circuits.Inferred neural interactions from neural signals primarily reflect functional interactions.In a long experiment, subject animals may experience different stages defined by the experiment, stimuli, or behavioral states, and hence functional interactions can change over time.To model dynamically changing functional interactions, prior work employs state-switching generalized linear models with hidden Markov models (i.e., HMM-GLMs).However, we argue they lack biological plausibility, as functional interactions are shaped and confined by the underlying anatomical connectome.Here, we propose a novel prior-informed state-switching GLM.We introduce both a Gaussian prior and a one-hot prior over the GLM in each state.The priors are learnable.We will show that the learned prior should capture the state-constant interaction, shedding light on the underlying anatomical connectome and revealing more likely physical neuron interactions.The state-dependent interaction modeled by each GLM offers traceability to capture functional variations across multiple brain states.Our methods effectively recover true interaction structures in simulated data, achieve the highest predictive likelihood with real neural datasets, and render interaction structures and hidden states more interpretable when applied to real neural data.",
        "id":264438909
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":227231797
      },
      {
        "title":"Did the Model Understand the Question?",
        "text":"We analyze state-of-the-art deep learning models for three tasks: question answering on (1) images, (2) tables, and (3) passages of text. Using the notion of attribution (word importance), we find that these deep networks often ignore important question terms. Leveraging such behavior, we perturb questions to craft a variety of adversarial examples. Our strongest attacks drop the accuracy of a visual question answering model from 61.1% to 19%, and that of a tabular question answering model from 33.5% to 3.3%. Additionally, we show how attributions can strengthen attacks proposed by Jia and Liang (2017) on paragraph comprehension models. Our results demonstrate that attributions can augment standard measures of accuracy and empower investigation of model performance. When a model is accurate but for the wrong reasons, attributions can surface erroneous logic in the model that indicates inadequacies in the test data.",
        "id":21673814
      },
      {
        "title":"A Repository of State of the Art and Competitive Baseline Summaries for Generic News Summarization",
        "text":"In the period since 2004, many novel sophisticated approaches for generic multi-document summarization have been developed. Intuitive simple approaches have also been shown to perform unexpectedly well for the task. Yet it is practically impossible to compare the existing approaches directly, because systems have been evaluated on different datasets, with different evaluation measures, against different sets of comparison systems. Here we present a corpus of summaries produced by several state-of-the-art extractive summarization systems or by popular baseline systems. The inputs come from the 2004 DUC evaluation, the latest year in which generic summarization was addressed in a shared task. We use the same settings for ROUGE automatic evaluation to compare the systems directly and analyze the statistical significance of the differences in performance. We show that in terms of average scores the state-of-the-art systems appear similar but that in fact they produce very different summaries. Our corpus will facilitate future research on generic summarization and motivates the need for development of more sensitive evaluation measures and for approaches to system combination in summarization.",
        "id":16482037
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"What molecular representation learning paper introduced a benchmark that focuses on learning over thermodynamically-accessible conformer ensembles across diverse molecular properties and chemical reactions?",
    "positive_ctxs":[
      {
        "title":"Learning Over Molecular Conformer Ensembles: Datasets and Benchmarks",
        "text":"Molecular Representation Learning (MRL) has proven impactful in numerous biochemical applications such as drug discovery and enzyme design.While Graph Neural Networks (GNNs) are effective at learning molecular representations from a 2D molecular graph or a single 3D structure, existing works often overlook the flexible nature of molecules, which continuously interconvert across conformations via chemical bond rotations and minor vibrational perturbations.To better account for molecular flexibility, some recent works formulate MRL as an ensemble learning problem, focusing on explicitly learning from a set of conformer structures.However, most of these studies have limited datasets, tasks, and models.In this work, we introduce the first MoleculAR Conformer Ensemble Learning (MARCEL) benchmark to thoroughly evaluate the potential of learning on conformer ensembles and suggest promising research directions.MARCEL includes four datasets covering diverse molecule-and reaction-level properties of chemically diverse molecules including organocatalysts and transition-metal catalysts, extending beyond the scope of common GNN benchmarks that are confined to drug-like molecules.In addition, we conduct a comprehensive empirical study, which benchmarks representative 1D, 2D, and 3D molecular representation learning models, along with two strategies that explicitly incorporate conformer ensembles into 3D MRL models.Our findings reveal that direct learning from an accessible conformer space can improve performance on a variety of tasks and models.",
        "id":263334596
      }
    ],
    "negative_ctxs":[
      {
        "title":"First approach toward Semantic Role Labeling for Basque",
        "text":"In this paper, we present the first Semantic Role Labeling system developed for Basque. The system is implemented using machine learning techniques and trained with the Reference Corpus for the Processing of Basque (EPEC). In our experiments the classifier that offers the best results is based on Support Vector Machines. Our system achieves 84.30 F1 score in identifying the PropBank semantic role for a given constituent and 82.90 F1 score in identifying the VerbNet role. Our study establishes a baseline for Basque SRL. Although there are no directly comparable systems for English we can state that the results we have achieved are quite good. In addition, we have performed a Leave-One-Out feature selection procedure in order to establish which features are the worthiest regarding argument classification. This will help smooth the way for future stages of Basque SRL and will help draw some of the guidelines of our research.",
        "id":16556429
      },
      {
        "title":"Applying Multi-Sense Embeddings for German Verbs to Determine Semantic Relatedness and to Detect Non-Literal Language",
        "text":"Up to date, the majority of computational models still determines the semantic relatedness between words (or larger linguistic units) on the type level. In this paper, we compare and extend multi-sense embeddings, in order to model and utilise word senses on the token level. We focus on the challenging class of complex verbs, and evaluate the model variants on various semantic tasks: semantic classification; predicting compositionality; and detecting non-literal language usage. While there is no overall best model, all models significantly outperform a word2vec single-sense skip baseline, thus demonstrating the need to distinguish between word senses in a distributional semantic model.",
        "id":14741568
      },
      {
        "title":"Automatically Detecting Syntactic Errors in Sentences Written by Learners of Chinese as a Foreign Language",
        "text":"This paper proposed a method that can automatically detect syntax errors in Chinese sentences. The algorithm for identifying syntax errors proposed in this study is known as KNGED, which uses a large database of rules to identify whether syntax errors exist in a sentence. The rules were generated either manually or automatically. This paper further proposed an algorithm for identifying the type of error that a sentence contained. Experimental results shown that the false positive rate and F1-measure of the proposed method for detecting syntax errors in Chinese sentences are 0.90 and 0.65.",
        "id":17679593
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"What open-source dataset combined knowledge retrieval with constraint satisfaction queries?",
    "positive_ctxs":[
      {
        "title":"KITAB: EVALUATING LLMS ON CONSTRAINT SATISFACTION FOR INFORMATION RETRIEVAL",
        "text":"We study the ability of state-of-the art models to answer constraint satisfaction queries for information retrieval (e.g., \"a list of ice cream shops in San Diego\").In the past, such queries were considered to be tasks that could only be solved via web-search or knowledge bases.More recently, large language models (LLMs) have demonstrated initial emergent abilities in this task.However, many current retrieval benchmarks are either saturated or do not measure constraint satisfaction.Motivated by rising concerns around factual incorrectness and hallucinations of LLMs, we present KITAB, a new dataset for measuring constraint satisfaction abilities of language models.KITAB consists of book-related data across more than 600 authors and 13,000 queries, and also offers an associated dynamic data collection and constraint verification approach for acquiring similar test data for other authors.Our extended experiments on GPT4 and GPT3.5 characterize and decouple common failure modes across dimensions such as information popularity, constraint types, and context availability.Results show that in the absence of context, models exhibit severe limitations as measured by irrelevant information, factual errors, and incompleteness, many of which exacerbate as information popularity decreases.While context availability mitigates irrelevant information, it is not helpful for satisfying constraints, identifying fundamental barriers to constraint satisfaction.We open source our contributions to foster further research on improving constraint satisfaction abilities of future models. 1",
        "id":264439509
      }
    ],
    "negative_ctxs":[
      {
        "title":"Learning Sequence-to-Sequence Correspondences from Parallel Corpora via Sequential Pattern Mining",
        "text":"We present an unsupervised extraction of sequence-to-sequence correspondences from parallel corpora by sequential pattern mining. The main characteristics of our method are two-fold. First, we propose a systematic way to enumerate all possible translation pair candidates of rigid and gapped sequences without falling into combinatorial explosion. Second, our method uses an efficient data structure and algorithm for calculating frequencies in a contingency table for each translation pair candidate. Our method is empirically evaluated using English-Japanese parallel corpora of 6 million words. Results indicate that it works well for multi-word translations, giving 56-84% accuracy at 19% token coverage and 11% type coverage.Our Basic IdeaOur approach is illustrated inFigure 1. We concatenate corresponding parallel sentences into bilingual sequences to which sequential pattern mining is applied. By doing so, we obtain the following effects:• It exhaustively generates all possible translation can-1 As of this writing, we learn that Moore will present his results on named entity at EACL 2003.",
        "id":9769888
      },
      {
        "title":"Improving Pronoun Resolution by Incorporating Coreferential Information of Candidates",
        "text":"Coreferential information of a candidate, such as the properties of its antecedents, is important for pronoun resolution because it reflects the salience of the candidate in the local discourse. Such information, however, is usually ignored in previous learning-based systems. In this paper we present a trainable model which incorporates coreferential information of candidates into pronoun resolution. Preliminary experiments show that our model will boost the resolution performance given the right antecedents of the candidates. We further discuss how to apply our model in real resolution where the antecedents of the candidate are found by a separate noun phrase resolution module. The experimental results show that our model still achieves better performance than the baseline.",
        "id":18896239
      },
      {
        "title":"A low-complexity, broad-coverage probabilistic Dependency Parser for English",
        "text":"Large-scale parsing is still a complex and timeconsuming process, often so much that it is infeasible in real-world applications. The parsing system described here addresses this problem by combining finite-state approaches, statistical parsing techniques and engineering knowledge, thus keeping parsing complexity as low as possible at the cost of a slight decrease in performance. The parser is robust and fast and at the same time based on strong linguistic foundations.",
        "id":1031765
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_iclr",
    "question":"What paper considers sensitive data issue when prompting large language model APIs?",
    "positive_ctxs":[
      {
        "title":"Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
        "text":"Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under privacy-restricted scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-ofthe-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.Figure 1: Synthetic medical data for illustration. Though rich in domain-specific knowledge, medical data contains sensitive private information. We extract keywords to mitigate privacy concerns.Steven Smith is a 60-year-old man admitted at Auckland Hospital. He was attended by Dr. Edward Jones at Date: 06\/01\/2008 . He has a past medical history significant for uncontrolled HTN who presents with a non-reducible right inguinal hernia. Patient first noticed a right sided bulge in 3 months prior. Every day it slips out and he has to manually push it back it. He has had to present to the emergency room twice recently when he was unable to push it back it. He was pending an outpatient repair of his right inguinal hernia.What are the assessment and recommendations for this patient?Figure 2: Framework overview. (a) To mitigate privacy leakage, we use a keyword extractor to obtain medical keywords. Clinicians then create several contexts based on these keywords and candidate answers, which the LLM uses to produce privacy-restricted contexts. (b) The generated contexts are used as additional input to enhance SLM medical decision-making capacity.",
        "id":258832501
      }
    ],
    "negative_ctxs":[
      {
        "title":"FanfictionNLP: A Text Processing Pipeline for Fanfiction",
        "text":"Fanfiction presents an opportunity as a data source for research in NLP, education, and social science. However, answering specific research questions with this data is difficult, since fanfiction contains more diverse writing styles than formal fiction. We present a text processing pipeline for fanfiction, with a focus on identifying text associated with characters. The pipeline includes modules for character identification and coreference, as well as the attribution of quotes and narration to those characters. Additionally, the pipeline contains a novel approach to character coreference that uses knowledge from quote attribution to resolve pronouns within quotes. For each module, we evaluate the effectiveness of various approaches on 10 annotated fanfiction stories. This pipeline outperforms tools developed for formal fiction on the tasks of character coreference and quote attribution.",
        "id":235097305
      },
      {
        "title":"LEARNING-BASED SUPPORT ESTIMATION IN SUBLINEAR TIME",
        "text":"We consider the problem of estimating the number of distinct elements in a large data set (or, equivalently, the support size of the distribution induced by the data set) from a random sample of its elements. The problem occurs in many applications, including biology, genomics, computer systems and linguistics. A line of research spanning the last decade resulted in algorithms that estimate the support up to ±εn from a sample of size O(log 2 (1\/ε) · n\/ log n), where n is the data set size. Unfortunately, this bound is known to be tight, limiting further improvements to the complexity of this problem. In this paper we consider estimation algorithms augmented with a machine-learning-based predictor that, given any element, returns an estimation of its frequency. We show that if the predictor is correct up to a constant approximation factor, then the sample complexity can be reduced significantly, to log(1\/ε) · n 1−Θ(1\/ log(1\/ε)) .We evaluate the proposed algorithms on a collection of data sets, using the neuralnetwork based estimators from Hsu et al, ICLR'19 as predictors. Our experiments demonstrate substantial (up to 3x) improvements in the estimation accuracy compared to the state of the art algorithm.",
        "id":235446639
      },
      {
        "title":"Universal Sentence Encoder for English",
        "text":"We present easy-to-use TensorFlow Hub sentence embedding models having good task transfer performance. Model variants allow for trade-offs between accuracy and compute resources. We report the relationship between model complexity, resources, and transfer performance. Comparisons are made with baselines without transfer learning and to baselines that incorporate word-level transfer. Transfer learning using sentence-level embeddings is shown to outperform models without transfer learning and often those that use only word-level transfer. We show good transfer task performance with minimal training data and obtain encouraging results on word embedding association tests (WEAT) of model bias. † Corresponding authors: {cer, yinfeiy}@google.com 1 We describe our publicly released models. SeeYang et al. (2018)andHenderson et al. (2017)for additional architectural details of models similar to those presented here.2 https:\/\/www.tensorflow.org\/hub\/, Apache 2.0 license, with models available as saved TF graphs. import tensorflow_hub as hub embed = hub.Module(\"https:\/\/tfhub.dev\/google\/\" \"universal-sentence-encoder\/2\") embedding = embed([\"Hello World!\"])",
        "id":53245704
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_iclr",
    "question":"What paper evaluated the ability of visual few-shot learning models to do in-context learning?",
    "positive_ctxs":[
      {
        "title":"CONTEXT-AWARE META-LEARNING",
        "text":"Large Language Models like ChatGPT demonstrate a remarkable capacity to learn new concepts during inference without any fine-tuning. However, visual models trained to detect new objects during inference have been unable to replicate this ability, and instead either perform poorly or require meta-training and\/or finetuning on similar objects. In this work, we propose a meta-learning algorithm that emulates Large Language Models by learning new visual concepts during inference without fine-tuning. Our approach leverages a frozen pre-trained feature extractor, and analogous to in-context learning, recasts meta-learning as sequence modeling over datapoints with known labels and a test datapoint with an unknown label. On 8 out of 11 meta-learning benchmarks, our approach-without meta-training or fine-tuning-exceeds or matches the state-of-the-art algorithm, P>M>F, which is meta-trained on these benchmarks.Under Review classify the query given an input sequence composed of the support set and query point. This learning paradigm trains the Transformer encoder to extrapolate to new classes in the parameter-space of the model, enabling our approach to learn new visual concepts during inference without fine-tuning. Due to its capacity to learn visual information \"in-context\", we term our approach Context-Aware Meta-Learning (CAML).Our primary contribution is to develop a meta-learning algorithm for universal meta-learning: the capacity to learn any new visual concept during inference without fine-tuning or meta-training on related images. This challenging setting emulates the deployment of LLMs to real-time applications, and strong performance in this setting would unlock new applications of visual meta-learning. Our theoretical analysis shows that an ELMES is the encoding that minimizes the entropy of detecting classes within the support set, and therefore, does not need to be learned. Our empirical analysis highlights the importance of reformulating meta-learning as sequence modeling: considering the support set and query together enables the model to attend to specific visual features of images in the support set to classify the query. Finally, our empirical analysis indicates CAML is a state-of-the-art meta-learning algorithm. On a diverse set of 8 out of 11 meta-learning benchmarks-and without meta-training or fine-tuning-CAML outperforms or matches the performance of P>M>F (Hu et al., 2022), a state-of-the-art meta-learning algorithm that is meta-trained on each benchmark.",
        "id":264172174
      }
    ],
    "negative_ctxs":[
      {
        "title":"CONSTRUCTIBLE REPRESENTATIONS FOR TWO SEMANTIC RELATIONS",
        "text":"",
        "id":16451235
      },
      {
        "title":"Extracting MWEs from Italian corpora: A case study for refining the POS-pattern methodology",
        "text":"An established method for MWE extraction is the combined use of previously identified POS-patterns and association measures. However, the selection of such POSpatterns is rarely debated. Focusing on Italian MWEs containing at least one adjective, we set out to explore how candidate POS-patterns listed in relevant literature and lexicographic sources compare with POS sequences exhibited by statistically significant n-grams including an adjective position extracted from a large corpus of Italian. All literature-derived patterns are found-and new meaningful candidate patterns emerge-among the top-ranking trigrams for three association measures. We conclude that a final solid set to be used for MWE extraction will have to be further refined through a combination of association measures as well as manual inspection.",
        "id":6492462
      },
      {
        "title":"Resource Interoperability for Sustainable Benchmarking: The Case of Events",
        "text":"With the continuous growth of benchmark corpora, which often annotate the same documents, there is a range of opportunities to compare and combine similar and complementary annotations. However, these opportunities are hampered by a wide range of problems that are related to the lack of resource interoperability. In this paper, we illustrate these problems by assessing aspects of interoperability at the document-level across a set of 20 corpora annotated with (aspects of) events. The issues range from applying different document naming conventions, to mismatches in textual content and structural\/conceptual differences among annotation schemes. We provide insight into the exact document intersections between the corpora by mapping their document identifiers and perform an empirical analysis of event annotations showing their compatibility and consistency in and across the corpora. This way, we aim to make the community more aware of the challenges and opportunities and to inspire working collaboratively towards interoperable resources.",
        "id":21697471
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"What paper first adapted ControlNet to generate continuous videos in a training-free manner?",
    "positive_ctxs":[
      {
        "title":"ControlVideo: Training-free Controllable Text-to-Video Generation",
        "text":"Text-driven diffusion models have unlocked unprecedented abilities in image generation, whereas their video counterpart still lags behind due to the excessive training cost of temporal modeling. Besides the training burden, the generated videos also suffer from appearance inconsistency and structural flickers, especially in long video synthesis. To address these challenges, we design a training-free framework called ControlVideo to enable natural and efficient text-to-video generation. ControlVideo, adapted from ControlNet, leverages coarsely structural consistency from input motion sequences, and introduces three modules to improve video generation. Firstly, to ensure appearance coherence between frames, ControlVideo adds fully cross-frame interaction in self-attention modules. Secondly, to mitigate the flicker effect, it introduces an interleaved-frame smoother that employs frame interpolation on alternated frames. Finally, to produce long videos efficiently, it utilizes a hierarchical sampler that separately synthesizes each short clip with holistic coherency. Empowered with these modules, ControlVideo outperforms the state-of-the-arts on extensive motion-prompt pairs quantitatively and qualitatively. Notably, thanks to the efficient designs, it generates both short and long videos within several minutes using one NVIDIA 2080Ti. Code is available at https:\/\/github.com\/YBYBZhang\/ControlVideo.Recent studies[15,40]have explored leveraging the structure controllability of ControlNet[43]or DDIM inversion [35] for video generation. Rather than synthesizing all frames independently,[15,40]enhance appearance coherence by replacing original self-attention with the sparser crossframe attention. Nevertheless, their video quality is still far behind photo-realistic videos in terms of: (i) inconsistent appearance between some frames (seeFig. 4 (a)), (ii) visible artifacts in large motion videos (seeFig. 4(b)), and (iii) structural flickers during inter-frame transitions. For (i) andPreprint. Under review.",
        "id":258832670
      }
    ],
    "negative_ctxs":[
      {
        "title":"AISFG: Abundant Information Slot Filling Generator",
        "text":"As an essential component of task-oriented dialogue systems, slot filling requires enormous labeled training data in a certain domain. However, in most cases, there is little or no target domain training data is available in the training stage. Thus, cross-domain slot filling has to cope with the data scarcity problem by zero\/few-shot learning. Previous researches on zero\/few-shot cross-domain slot filling focus on slot descriptions and examples while ignoring the slot type ambiguity and example ambiguity issues. To address these problems, we propose Abundant Information Slot Filling Generator (AISFG), a generative model with a novel query template that incorporates domain descriptions, slot descriptions, and examples with context. Experimental results show that our model outperforms state-of-the-art approaches in zero\/few-shot slot filling task. 1",
        "id":250390921
      },
      {
        "title":"The Meaning Factory at SemEval-2017 Task 9: Producing AMRs with Neural Semantic Parsing",
        "text":"We evaluate a semantic parser based on a character-based sequence-to-sequence model in the context of the SemEval-2017 shared task on semantic parsing for AMRs.With data augmentation, super characters, and POS-tagging we gain major improvements in performance compared to a baseline character-level model. Although we improve on previous character-based neural semantic parsing models, the overall accuracy is still lower than a state-of-the-art AMR parser. An ensemble combining our neural semantic parser with an existing, traditional parser, yields a small gain in performance.",
        "id":8672241
      },
      {
        "title":"Controlling Utterance Length in NMT-based Word Segmentation with Attention",
        "text":"One of the basic tasks of computational language documentation (CLD) is to identify word boundaries in an unsegmented phonemic stream. While several unsupervised monolingual word segmentation algorithms exist in the literature, they are challenged in real-world CLD settings by the small amount of available data. A possible remedy is to take advantage of glosses or translation in a foreign, wellresourced, language, which often exist for such data. In this paper, we explore and compare ways to exploit neural machine translation models to perform unsupervised boundary detection with bilingual information, notably introducing a new loss function for jointly learning alignment and segmentation. We experiment with an actual under-resourced language, Mboshi, and show that these techniques can effectively control the output segmentation length.4In this case, α ij = exp(e ij \/T ) J k=1 exp(e ik \/T ) . 5 A temperature below 1 would conversely sharpen the alignment distribution. We did not observe significant changes in segmentation performance varying the temperature parameter.",
        "id":204788606
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"What paper first associate the modeling frequency with input human skeletons under the NeRF framework?",
    "positive_ctxs":[
      {
        "title":"POSE MODULATED AVATARS FROM VIDEO",
        "text":"It is now possible to reconstruct dynamic human motion and shape from a sparse set of cameras using Neural Radiance Fields (NeRF) driven by an underlying skeleton. However, a challenge remains to model the deformation of cloth and skin in relation to skeleton pose. Unlike existing avatar models that are learned implicitly or rely on a proxy surface, our approach is motivated by the observation that different poses necessitate unique frequency assignments. Neglecting this distinction yields noisy artifacts in smooth areas or blurs fine-grained texture and shape details in sharp regions. We develop a two-branch neural network that is adaptive and explicit in the frequency domain. The first branch is a graph neural network that models correlations among body parts locally, taking skeleton pose as input. The second branch combines these correlation features to a set of global frequencies and then modulates the feature encoding. Our experiments demonstrate that our network outperforms state-of-the-art methods in terms of preserving details and generalization capabilities.",
        "id":261076339
      }
    ],
    "negative_ctxs":[
      {
        "title":"Universal Dependencies for Learner English",
        "text":"We introduce the Treebank of Learner English (TLE), the first publicly available syntactic treebank for English as a Second Language (ESL). The TLE provides manually annotated POS tags and Universal Dependency (UD) trees for 5,124 sentences from the Cambridge First Certificate in English (FCE) corpus. The UD annotations are tied to a pre-existing error annotation of the FCE, whereby full syntactic analyses are provided for both the original and error corrected versions of each sentence. Further on, we delineate ESL annotation guidelines that allow for consistent syntactic treatment of ungrammatical English. Finally, we benchmark POS tagging and dependency parsing performance on the TLE dataset and measure the effect of grammatical errors on parsing accuracy. We envision the treebank to support a wide range of linguistic and computational research on second language acquisition as well as automatic processing of ungrammatical language 1 .",
        "id":8462706
      },
      {
        "title":"Using Linguist's Assistant for Language Description and Translation",
        "text":"The Linguist's Assistant (LA) is a practical computational paradigm for describing languages. LA seeks to specify in semantic representations a large subset of possible written communication. These semantic representations then become the starting point and organizing principle from which a linguist describes the linguistic surface forms of a language using LA's visual lexicon and grammatical rule development interface. The resulting computational description can then be used in our document authoring and translation applications.",
        "id":1224439
      },
      {
        "title":"Interactive Relation Extraction in Main Memory Database Systems",
        "text":"We present INDREX-MM, a main memory database system for interactively executing two interwoven tasks, declarative relation extraction from text and their exploitation with SQL. INDREX-MM simplifies these tasks for the user with powerful SQL extensions for gathering statistical semantics, for executing open information extraction and for integrating relation candidates with domain specific data. We demonstrate these functions on 800k documents from Reuters RCV1 with more than a billion linguistic annotations and report execution times in the order of seconds.",
        "id":1384156
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"What paper first extends rotary positional encoding (RoPE) for camera-geometry encoding in multi-view transformers?",
    "positive_ctxs":[
      {
        "title":"GTA: A GEOMETRY-AWARE ATTENTION MECHANISM FOR MULTI-VIEW TRANSFORMERS",
        "text":"As transformers are equivariant to the permutation of input tokens, encoding the positional information of tokens is necessary for many tasks.However, since existing positional encoding schemes have been initially designed for NLP tasks, their suitability for vision tasks, which typically exhibit different structural properties in their data, is questionable.We argue that existing positional encoding schemes are suboptimal for 3D vision tasks, as they do not respect their underlying 3D geometric structure.Based on this hypothesis, we propose a geometryaware attention mechanism that encodes the geometric structure of tokens as relative transformation determined by the geometric relationship between queries and key-value pairs.By evaluating on multiple novel view synthesis (NVS) datasets in the sparse wide-baseline multi-view setting, we show that our attention, called Geometric Transform Attention (GTA), improves learning efficiency and performance of state-of-the-art transformer-based NVS models without any additional learned parameters and only minor computational overhead.",
        "id":264147054
      }
    ],
    "negative_ctxs":[
      {
        "title":"Personalized Machine Translation: Predicting Translational Preferences",
        "text":"Machine Translation (MT) has advanced in recent years to produce better translations for clients' specific domains, and sophisticated tools allow professional translators to obtain translations according to their prior edits. We suggest that MT should be further personalized to the end-user level -the receiver or the author of the text -as done in other applications. As a step in that direction, we propose a method based on a recommender systems approach where the user's preferred translation is predicted based on preferences of similar users. In our experiments, this method outperforms a set of non-personalized methods, suggesting that user preference information can be employed to provide better-suited translations for each user.",
        "id":11060961
      },
      {
        "title":"USAAR-SAPE: An English-Spanish Statistical Automatic Post-Editing System",
        "text":"We describe the USAAR-SAPE English-Spanish Automatic Post-Editing (APE) system submitted to the APE Task organized in the Workshop on Statistical Machine Translation (WMT) in 2015. Our system was able to improve upon the baseline MT system output by incorporating Phrase-Based Statistical MT (PBSMT) technique into the monolingual Statistical APE task (SAPE). The reported final submission crucially involves hybrid word alignment. The SAPE system takes raw Spanish Machine Translation (MT) output provided by the shared task organizers and produces post-edited Spanish text. The parallel data consist of English Text, raw machine translated Spanish output, and their corresponding manually post-edited versions. The major goal of the task is to reduce the post-editing effort by improving the quality of the MT output in terms of fluency and adequacy.",
        "id":16293762
      },
      {
        "title":"Normalization for Automated Metrics: English and Arabic Speech Translation §",
        "text":"The Defense Advanced Research Projects Agency (DARPA) Spoken Language Communication and Translation System for Tactical Use (TRANSTAC) program has experimented with applying automated metrics to speech translation dialogues. For translations into English, BLEU, TER, and METEOR scores correlate well with human judgments, but scores for translation into Arabic correlate with human judgments less strongly. This paper provides evidence to support the hypothesis that automated measures of Arabic are lower due to variation and inflection in Arabic by demonstrating that normalization operations improve correlation between BLEU scores and Likert-type judgments of semantic adequacyas well as between BLEU scores and human judgments of the successful transfer of the meaning of individual content words from English to Arabic.",
        "id":10763159
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"What paper first proposed a robust perceptual similarity metric with certificates?",
    "positive_ctxs":[
      {
        "title":"LIPSIM: A PROVABLY ROBUST PERCEPTUAL SIMILARITY METRIC",
        "text":"Recent years have seen growing interest in developing and applying perceptual similarity metrics.Research has shown the superiority of perceptual metrics over pixel-wise metrics in aligning with human perception and serving as a proxy for the human visual system.On the other hand, as perceptual metrics rely on neural networks, there is a growing concern regarding their resilience, given the established vulnerability of neural networks to adversarial attacks.It is indeed logical to infer that perceptual metrics may inherit both the strengths and shortcomings of neural networks.In this work, we demonstrate the vulnerability of state-of-the-art perceptual similarity metrics based on an ensemble of ViT-based feature extractors to adversarial attacks.We then propose a framework to train a robust perceptual similarity metric called LipSim (Lipschitz Similarity Metric) with provable guarantees.By leveraging 1-Lipschitz neural networks as the backbone, LipSim provides guarded areas around each data point and certificates for all perturbations within an ℓ 2 ball.Finally, a comprehensive set of experiments shows the performance of LipSim in terms of natural and certified scores and on the image retrieval application.The code is available at https:\/\/github.com\/SaraGhazanfari\/LipSim.",
        "id":264555382
      }
    ],
    "negative_ctxs":[
      {
        "title":"Automatic Assessment of Student Translations for Foreign Language Tutoring",
        "text":"This paper introduces the use of speech translation technology for a new type of voice-interactive Computer Aided Language Learning (CALL) application. We describe a computer game we have developed, in which the system presents sentences in a student's native language to elicit spoken translations in the target new language. A critical technology is an algorithm to automatically verify the appropriateness of the student's translation using linguistic analysis. Evaluation results are presented on the system's ability to match human judgment of the correctness of a student's translation, for a set of 1115 utterances collected from 9 learners of Mandarin Chinese translating flight domain sentences. We also demonstrate the effective use of context information to improve both recognition performance on non-native speech as well as the system's accuracy in judging the translation quality.",
        "id":3889577
      },
      {
        "title":"Shared Task Papers",
        "text":"We explore a large number of features for cross-lingual pronoun prediction for translation between English and German\/French. We find that features related to German\/French are more informative than features related to English, regardless of the translation direction. Our most useful features are local context, dependency head features, and source pronouns. We also find that it is sometimes more successful to employ a 2-step procedure that first makes a binary choice between pronouns and other, then classifies pronouns. For the pronoun\/other distinction POS ngrams were very useful.",
        "id":5971846
      },
      {
        "title":"Typical Cases of Annotators' Disagreement in Discourse Annotations in Prague Dependency Treebank",
        "text":"In this paper, we present the first results of the parallel Czech discourse annotation in the Prague Dependency Treebank 2.0. Having established an annotation scenario for capturing semantic relations crossing the sentence boundary in a discourse, and having annotated the first sections of the treebank according to these guidelines, we report now on the results of the first evaluation of these manual annotations. We give an overview of the process of the annotation itself, which we believe is to a large degree language-independent and therefore accessible to any discourse researcher. Next, we describe the inter-annotator agreement measurement, and, most importantly, we classify and analyze the most common types of annotators' disagreement and propose solutions for the next phase of the annotation. The annotation is carried out on dependency trees (on the tectogrammatical layer), this approach is quite novel and it brings us some advantages when interpreting the syntactic structure of the discourse units.",
        "id":17255390
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"What paper first proposes that simply reversing the output can significantly enhance the sample efficiency and the performance of the arithmetic capability of a decoder-only Transformer model?",
    "positive_ctxs":[
      {
        "title":"Teaching Arithmetic to Small Transformers",
        "text":"Large language models like GPT-4 exhibit emergent capabilities across generalpurpose tasks, such as basic arithmetic, when trained on extensive text data, even though these tasks are not explicitly encoded by the unsupervised, next-token prediction objective. This study investigates how small transformers, trained from random initialization, can efficiently learn arithmetic operations such as addition, multiplication, and elementary functions like square root, using the nexttoken prediction objective. We first demonstrate that conventional training data is not the most effective for arithmetic learning, and simple formatting changes can significantly improve accuracy. This leads to sharp phase transitions as a function of training data scale, which, in some cases, can be explained through connections to low-rank matrix completion. Building on prior work, we then train on chain-of-thought style data that includes intermediate step results. Even in the complete absence of pretraining, this approach significantly and simultaneously improves accuracy, sample complexity, and convergence speed. We also study the interplay between arithmetic and text data during training and examine the effects of few-shot prompting, pretraining, and model scale. Additionally, we discuss length generalization challenges. Our work highlights the importance of high-quality, instructive data that considers the particular characteristics of the next-word prediction objective for rapidly eliciting arithmetic capabilities. 2 * Authors contributed equally to this paper. 2 Our code is available at https:\/\/github.com\/lee-ny\/teaching_arithmetic Preprint. Under review.",
        "id":259375870
      }
    ],
    "negative_ctxs":[
      {
        "title":"SMBOP: Semi-autoregressive Bottom-up Semantic Parsing",
        "text":"The de-facto standard decoding method for semantic parsing in recent years has been to autoregressively decode the abstract syntax tree of the target program using a top-down depthfirst traversal. In this work, we propose an alternative approach: a Semi-autoregressive Bottom-up Parser (SMBOP) that constructs at decoding step t the top-K sub-trees of height ≤ t. Our parser enjoys several benefits compared to top-down autoregressive parsing. From an efficiency perspective, bottom-up parsing allows to decode all sub-trees of a certain height in parallel, leading to logarithmic runtime complexity rather than linear. From a modeling perspective, a bottom-up parser learns representations for meaningful semantic sub-programs at each step, rather than for semantically-vacuous partial trees. We apply SMBOP on SPIDER, a challenging zero-shot semantic parsing benchmark, and show that SMBOP leads to a 2.2x speed-up in decoding time and a ∼5x speed-up in training time, compared to a semantic parser that uses autoregressive decoding. SMBOP obtains 71.1 denotation accuracy on SPIDER, establishing a new state-of-the-art, and 69.5 exact match, comparable to the 69.6 exact match of the autoregressive RAT-SQL+GRAPPA.",
        "id":225062282
      },
      {
        "title":"Syntactic parsing of chat language in contact center conversation corpus",
        "text":"Chat language is often referred to as Computer-mediated communication (CMC). Most of the previous studies on chat language has been dedicated to collecting \"chat room\" data as it is the kind of data which is the most accessible on the WEB. This kind of data falls under the informal register whereas we are interested in this paper in understanding the mechanisms of a more formal kind of CMC: dialog chat in contact centers. The particularities of this type of dialogs and the type of language used by customers and agents is the focus of this paper towards understanding this new kind of CMC data. The challenges for processing chat data comes from the fact that Natural Language Processing tools such as syntactic parsers and part of speech taggers are typically trained on mismatched conditions, we describe in this study the impact of such a mismatch for a syntactic parsing task.",
        "id":14493477
      },
      {
        "title":"Deep learning for language understanding of mental health concepts derived from Cognitive Behavioural Therapy",
        "text":"In recent years, we have seen deep learning and distributed representations of words and sentences make impact on a number of natural language processing tasks, such as similarity, entailment and sentiment analysis. Here we introduce a new task: understanding of mental health concepts derived from Cognitive Behavioural Therapy (CBT). We define a mental health ontology based on the CBT principles, annotate a large corpus where this phenomena is exhibited and perform understanding using deep learning and distributed representations. Our results show that the performance of deep learning models combined with word embeddings or sentence embeddings significantly outperform non-deep-learning models in this difficult task. This understanding module will be an essential component of a statistical dialogue system delivering therapy.",
        "id":52153975
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"What paper first showed that you can score the code explanations using the pass@k metric?",
    "positive_ctxs":[
      {
        "title":"OCTOPACK: INSTRUCTION TUNING CODE LARGE LANGUAGE MODELS",
        "text":"Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile COMMITPACK: 4 terabytes of Git commits across 350 programming languages. We benchmark COMMITPACK against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HUMANEVALPACK, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OCTOCODER and OCTOGEEX, achieve the best performance across HUMANEVALPACK among all permissive models, demonstrating COMMITPACK's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https:\/\/github.com\/bigcode-project\/octopack. import numpy as np import matplotlib.pyplot as plt # generate sample data x_data = np.linspace (-5, 5, 20)   y_data = np.random.normal(0.0, 1.0, x_data.size) plt.plot(x_data, y_data, 'o') plt.show() Code Before Commit MessageCode AfterChange to sin() function with noise import math import numpy as np import matplotlib.pyplot as plt",
        "id":260886874
      }
    ],
    "negative_ctxs":[
      {
        "title":"Linguistic representation of Finnish in the medical domain spoken language translation system Mots-clefs -Keywords",
        "text":"Grammaire d'unification, traduction automatique multilingue de la parole, interlingue, souslangage, finnois.Domain specific unification grammar, multilingual spoken language translation, interlingua, sub-language, Finnish.Résumé -AbstractDans cet article nous décrivons le développement des ressources linguistiques du finnois pour un système de traduction automatique de la parole dans le domaine médical: MedSLT. Le travail inclut la construction des corpus médicaux en finnois, le développement de la grammaire finlandaise pour la génération, le développement du lexique finlandais et la définition des règles de mapping interlingue-finnois pour la traduction multilingue. Nous avons découvert que le finnois peut être introduit dans l'architecture existante de MedSLT sans trop de difficultés. En effet, malgré les différences entre l'anglais et le finnois, la grammaire finlandaise a pu être créée en adaptant manuellement la grammaire anglaise originale. Les premiers résultats de l'évaluation de la traduction anglais-finnois sont encourageants.This paper describes the development of Finnish linguistic resources for use in MedSLT, an Open Source medical domain speech-to-speech translation system. The paper describes the collection of medical Finnish corpora, the creation of a Finnish grammar by adapting the original English grammar, the composition of a domain specific Finnish lexicon and the definition of interlingua to Finnish mapping rules for multilingual translation. It is shown that Finnish can be effectively introduced into the existing MedSLT framework and that despite the differences between English and Finnish, the Finnish grammar can be created by manual adaptation from the original English grammar. Regarding further development, the initial evaluation results of English-Finnish speech-to-speech translation are encouraging.Marianne Santaholma",
        "id":10919306
      },
      {
        "title":"Emotion Detection with Neural Personal Discrimination",
        "text":"There have been a recent line of works to automatically predict the emotions of posts in social media. Existing approaches consider the posts individually and predict their emotions independently. Different from previous researches, we explore the dependence among relevant posts via the authors' backgrounds, since the authors with similar backgrounds, e.g., gender, location, tend to express similar emotions. However, such personal attributes are not easy to obtain in most social media websites, and it is hard to capture attributesaware words to connect similar people. Accordingly, we propose a Neural Personal Discrimination (NPD) approach to address above challenges by determining personal attributes from posts, and connecting relevant posts with similar attributes to jointly learn their emotions. In particular, we employ adversarial discriminators to determine the personal attributes, with attention mechanisms to aggregate attributes-aware words. In this way, social correlationship among different posts can be better addressed. Experimental results show the usefulness of personal attributes, and the effectiveness of our proposed NPD approach in capturing such personal attributes with significant gains over the state-of-the-art models.",
        "id":201657071
      },
      {
        "title":"Modelling Temporal Document Sequences for Clinical ICD Coding",
        "text":"Past studies on the ICD coding problem focus on predicting clinical codes primarily based on the discharge summary. This covers only a small fraction of the notes generated during each hospital stay and leaves potential for improving performance by analysing all the available clinical notes. We propose a hierarchical transformer architecture that uses text across the entire sequence of clinical notes in each hospital stay for ICD coding, and incorporates embeddings for text metadata such as their position, time, and type of note. While using all clinical notes increases the quantity of data substantially, superconvergence can be used to reduce training costs. We evaluate the model on the MIMIC-III dataset. Our model exceeds the prior state-of-the-art when using only discharge summaries as input, and achieves further performance improvements when all clinical notes are used as input.",
        "id":257205854
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"What paper first used the technique of prompt engineering to generate adversarial prompts that can fool LLMs into making wrong predictions in prompt-based learning?",
    "positive_ctxs":[
      {
        "title":"AN LLM CAN FOOL ITSELF: A PROMPT-BASED ADVERSARIAL ATTACK",
        "text":"The wide-ranging applications of large language models (LLMs), especially in safety-critical domains, necessitate the proper evaluation of the LLM's adversarial robustness.This paper proposes an efficient tool to audit the LLM's adversarial robustness via a prompt-based adversarial attack (PromptAttack).PromptAttack converts adversarial textual attacks into an attack prompt that can cause the victim LLM to output the adversarial sample to fool itself.The attack prompt is composed of three important components: (1) original input (OI) including the original sample and its ground-truth label, (2) attack objective (AO) illustrating a task description of generating a new sample that can fool itself without changing the semantic meaning, and (3) attack guidance (AG) containing the perturbation instructions to guide the LLM on how to complete the task by perturbing the original sample at character, word, and sentence levels, respectively.Besides, we use a fidelity filter to ensure that PromptAttack maintains the original semantic meanings of the adversarial examples.Further, we enhance the attack power of PromptAttack by ensembling adversarial examples at different perturbation levels.Comprehensive empirical results using Llama2 and GPT-3.5 validate that PromptAttack consistently yields a much higher attack success rate compared to AdvGLUE and AdvGLUE++.Interesting findings include that a simple emoji can easily mislead GPT-3.5 to make wrong predictions.Our project page is available at PromptAttack.",
        "id":264406064
      }
    ],
    "negative_ctxs":[
      {
        "title":"How to Record the Meaning of Figurative Language How to Record the Meaning of Figurative Language",
        "text":"This paper focuses on the question of what kind of data needs to be recorded about figurative language, in order to capture the essential meaning of the text and to enable us to re-create a synonymous text, based on that data. A short review of the best known systems of semantic annotation will be presented and their suitability for the task will be analyzed. Also, a method that could be used for representing the meaning of the idioms, metaphors and metonymy in the data model will be considered.",
        "id":56336392
      },
      {
        "title":"Temporal dynamics of semantic relations in word embeddings: an application to predicting armed conflict participants",
        "text":"This paper deals with using word embedding models to trace the temporal dynamics of semantic relations between pairs of words. The set-up is similar to the well-known analogies task, but expanded with a time dimension. To this end, we apply incremental updating of the models with new training texts, including incremental vocabulary expansion, coupled with learned transformation matrices that let us map between members of the relation. The proposed approach is evaluated on the task of predicting insurgent armed groups based on geographical locations. The gold standard data for the time span 1994-2010 is extracted from the UCDP Armed Conflicts dataset. The results show that the method is feasible and outperforms the baselines, but also that important work still remains to be done.",
        "id":3160550
      },
      {
        "title":"Mapping Language to Code in Programmatic Context",
        "text":"Source code is rarely written in isolation. It depends significantly on the programmatic context, such as the class that the code would reside in. To study this phenomenon, we introduce the task of generating class member functions given English documentation and the programmatic context provided by the rest of the class. This task is challenging because the desired code can vary greatly depending on the functionality the class provides (e.g., a sort function may or may not be available when we are asked to \"return the smallest element\" in a particular member variable list). We introduce CONCODE, a new large dataset with over 100,000 examples consisting of Java classes from online code repositories, and develop a new encoder-decoder architecture that models the interaction between the method documentation and the class environment. We also present a detailed error analysis suggesting that there is significant room for future work on this task.",
        "id":52125417
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"What paper first uses decoupled workers in distributed RL system?",
    "positive_ctxs":[
      {
        "title":"SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores",
        "text":"The ever-growing complexity of reinforcement learning (RL) tasks demands a distributed RL system to efficiently generate and process a massive amount of data to train intelligent agents. However, existing open-source libraries suffer from various limitations, which impede their practical use in challenging scenarios where largescale training is necessary. While industrial systems from OpenAI and DeepMind have achieved successful large-scale RL training, their system architecture and implementation details remain undisclosed to the community. In this paper, we present a novel system abstraction on the dataflows of RL training, which unifies practical RL training across diverse applications into a general and flexible framework and enables fine-grained system-level optimizations. Following this abstraction, we develop a scalable, efficient, and extensible distributed RL system called ReaLly Scalable RL (SRL). The system architecture of SRL separates major RL computation components and allows massively parallelized training. We also introduce a collection of techniques to further optimize the system performance. Moreover, SRL offers user-friendly and extensible interfaces, which facilitate the development of customized algorithms. Our evaluation shows that SRL outperforms existing academic libraries in both a single machine and a medium-sized cluster. In a large-scale cluster, the novel architecture of SRL leads to up to 3.7x speedup compared to the design choices adopted by the existing libraries. We also conduct a direct benchmark comparison to Ope-nAI's industrial system, Rapid [5], in the challenging hide-and-seek environment[2]. SRL reproduces the same solution as reported by OpenAI with up to 5x speedup in wall-clock time. Furthermore, we also examine the performance of SRL in a much harder variant of the hide-and-seek environment and achieve substantial learning speedup by scaling SRL to over 15k CPU cores and 32 A100 GPUs. * Equal contribution.Notably, SRL is the first in the academic community to perform RL experiments at such a large scale.",
        "id":259287121
      }
    ],
    "negative_ctxs":[
      {
        "title":"Unified Lexicon and Unified Morphosyntactic Specifications for Written and Spoken Italian",
        "text":"The goal of this paper is (1) to illustrate a specific procedure for merging different monolingual lexicons, focusing on techniques for detecting and mapping equivalent lexical entries, and (2) to sketch a production model that enables one to obtain lexical resources via unification of existing data. We describe the creation of a Unified Lexicon (UL) from a common sample of the Italian PAROLE\/SIMPLE\/CLIPS phonological lexicon and of the Italian LCSTAR pronunciation lexicon. We expand previous experiments carried out at ILC-CNR: based on a detailed mechanism for mapping grammatical classifications of candidate UL entries, a consensual set of Unified Morphosyntactic Specifications (UMS) shared by lexica for the written and spoken areas is proposed. The impact of the UL on cross-validation issues is analysed: by looking into conflicts, mismatches and diverging classifications can be detected in both resources. The work presented is in line with the activities promoted by ELRA towards the development of methods for packaging new language resources by combining independently created resources, and was carried out as part of the ELRA Production Committee activities. ELRA aims to exploit the UL experience to carry out such merging activities for resources available on the ELRA catalogue in order to fulfill the users' needs.",
        "id":10313322
      },
      {
        "title":"Issues in evaluating semantic spaces using word analogies",
        "text":"The offset method for solving word analogies has become a standard evaluation tool for vector-space semantic models: it is considered desirable for a space to represent semantic relations as consistent vector offsets. We show that the method's reliance on cosine similarity conflates offset consistency with largely irrelevant neighborhood structure, and propose simple baselines that should be used to improve the utility of the method in vector space evaluation.",
        "id":7906247
      },
      {
        "title":"A Semi-Automatic Evaluation Scheme: Automated Nuggetization for Manual Annotation",
        "text":"In this paper we describe automatic information nuggetization and its application to text comparison. More specifically, we take a close look at how machine-generated nuggets can be used to create evaluation material. A semiautomatic annotation scheme is designed to produce gold-standard data with exceptionally high inter-human agreement.",
        "id":2073916
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"What paper investigated the effect of the relative position (closer or further away) of the most pertinent retrieved code snippets on repository-level code completion performance?",
    "positive_ctxs":[
      {
        "title":"RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems",
        "text":"Large Language Models (LLMs) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers.However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios.To fill this gap, we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems.RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline).Each task respectively measures the system's ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction.RepoBench aims to facilitate a more complete comparison of performance and encouraging continuous improvement in auto-completion systems.",
        "id":259075246
      }
    ],
    "negative_ctxs":[
      {
        "title":"Sentiment Propagation via Implicature Constraints",
        "text":"Opinions may be expressed implicitly via inference over explicit sentiments and events that positively\/negatively affect entities (goodFor\/badFor events). We investigate how such inferences may be exploited to improve sentiment analysis, given goodFor\/badFor event information. We apply Loopy Belief Propagation to propagate sentiments among entities. The graph-based model improves over explicit sentiment classification by 10 points in precision and, in an evaluation of the model itself, we find it has an 89% chance of propagating sentiments correctly.",
        "id":15146734
      },
      {
        "title":"Are Larger Pretrained Language Models Uniformly Better? Comparing Performance at the Instance Level",
        "text":"Larger language models have higher accuracy on average, but are they better on every single instance (datapoint)? Some work suggests larger models have higher out-ofdistribution robustness, while other work suggests they have lower accuracy on rare subgroups. To understand these differences, we investigate these models at the level of individual instances. However, one major challenge is that individual predictions are highly sensitive to noise in the randomness in training. We develop statistically rigorous methods to address this, and after accounting for pretraining and finetuning noise, we find that our BERT-LARGE is worse than BERT-MINI on at least 1−4% of instances across MNLI, SST-2, and QQP, compared to the overall accuracy improvement of 2−10%. We also find that finetuning noise increases with model size, and that instance-level accuracy has momentum: improvement from BERT-MINI to BERT-MEDIUM correlates with improvement from BERT-MEDIUM to BERT-LARGE . Our findings suggest that instance-level predictions provide a rich source of information; we therefore recommend that researchers supplement model weights with model predictions.",
        "id":234482939
      },
      {
        "title":"Generating Pattern-Based Entailment Graphs for Relation Extraction",
        "text":"Relation extraction is the task of recognizing and extracting relations between entities or concepts in texts. A common approach is to exploit existing knowledge to learn linguistic patterns expressing the target relation and use these patterns for extracting new relation mentions. Deriving relation patterns automatically usually results in large numbers of candidates, which need to be filtered to derive a subset of patterns that reliably extract correct relation mentions. We address the pattern selection task by exploiting the knowledge represented by entailment graphs, which capture semantic relationships holding among the learned pattern candidates. This is motivated by the fact that a pattern may not express the target relation explicitly, but still be useful for extracting instances for which the relation holds, because its meaning entails the meaning of the target relation. We evaluate the usage of both automatically generated and gold-standard entailment graphs in a relation extraction scenario and present favorable experimental results, exhibiting the benefits of structuring and selecting patterns based on entailment graphs.",
        "id":6159455
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"What paper is the first to prove finetuned LLM can be a reliable judge?",
    "positive_ctxs":[
      {
        "title":"PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization",
        "text":"Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models. To determine the optimal hyperparameters, an automatic, robust, and reliable evaluation benchmark is essential. However, establishing such a benchmark is not a trivial task due to the challenges associated with evaluation accuracy and privacy protection. In response to these challenges, we introduce a judge large language model, named PandaLM, which is trained to distinguish the superior model given several LLMs. PandaLM's focus extends beyond just the objective correctness of responses, which is the main focus of traditional evaluation datasets. It addresses vital subjective factors such as relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality. To ensure the reliability of PandaLM, we collect a diverse human-annotated test dataset, where all contexts are generated by humans and labels are aligned with human preferences. Our results indicate that PandaLM-7B achieves 93.75% of GPT-3.5's evaluation ability and 88.28% of GPT-4's in terms of F1-score on our test dataset. PandaLM enables the evaluation of LLM to be fairer but with less cost, evidenced by significant improvements achieved by models tuned through PandaLM compared to their counterparts trained with default Alpaca's hyperparameters. In addition, PandaLM does not depend on API-based evaluations, thus avoiding potential data leakage. All resources of PandaLM are released at https:\/\/github.com\/WeOpenML\/PandaLM. * Equal contribution. Yidong did this work during his internship at Westlake University. † Corresponding to",
        "id":259108266
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":219309348
      },
      {
        "title":"Semi-Automated Labeling of Requirement Datasets for Relation Extraction",
        "text":"Creating datasets manually by human annotators is a laborious task that can lead to biased and inhomogeneous labels. We propose a flexible, semi-automatic framework for labeling data for relation extraction. Furthermore, we provide a dataset of preprocessed sentences from the requirements engineering domain, including a set of automatically created as well as hand-crafted labels. In our case study, we compare the human and automatic labels and show that there is a substantial overlap between both annotations.",
        "id":237421077
      },
      {
        "title":"Answering it with charts -Dialogue in natural language and charts",
        "text":"A methodology is proposed for taking queries and requests expressed in natural language as input and answering them in charts through organizing that interaction into felicitous dialogue. Charts and graphics, as well as languages, are important modes of communication. This is especially true of those which are used frequently when people analyze huge amount of data interactively, in order to find out its characteristics or to resolve questions about it. This paper raises the problem that in such situations the correctness of the charts depends on the context, and proposes a framework to resolve it. The core of the framework is a logical form that includes the specifications of the user's perspective and the proper treatment of the logical form for handling utterance fragments. The framework has been implemented and confirmed to be appropriate.",
        "id":10143404
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"What paper mitigates language model sampling errors due to the softmax bottleneck?",
    "positive_ctxs":[
      {
        "title":"CLOSING THE CURIOUS CASE OF NEURAL TEXT DEGENERATION",
        "text":"Despite their ubiquity in language generation, it remains unknown why truncation sampling heuristics like nucleus sampling are so effective.We provide a theoretical explanation for the effectiveness of the truncation sampling by proving that truncation methods that discard tokens below some probability threshold (the most common type of truncation) can guarantee that all sampled tokens have nonzero true probability.However, thresholds are a coarse heuristic, and necessarily discard some tokens with nonzero true probability as well.In pursuit of a more precise sampling strategy, we show that we can leverage a known source of model errors, the softmax bottleneck, to prove that certain tokens have nonzero true probability, without relying on a threshold.Based on our findings, we develop an experimental truncation strategy and the present pilot studies demonstrating the promise of this type of algorithm.Our evaluations show that our method outperforms its threshold-based counterparts under automatic and human evaluation metrics for low-entropy (i.e., close to greedy) open-ended text generation.Our theoretical findings and pilot experiments provide both insight into why truncation sampling works, and make progress toward more expressive sampling algorithms that better surface the generative capabilities of large language models.",
        "id":263608672
      }
    ],
    "negative_ctxs":[
      {
        "title":"REVISITING ADAPTERS WITH ADVERSARIAL TRAINING",
        "text":"While adversarial training is generally used as a defense mechanism, recent works show that it can also act as a regularizer. By co-training a neural network on clean and adversarial inputs, it is possible to improve classification accuracy on the clean, non-adversarial inputs. We demonstrate that, contrary to previous findings, it is not necessary to separate batch statistics when co-training on clean and adversarial inputs, and that it is sufficient to use adapters with few domain-specific parameters for each type of input. We establish that using the classification token of a Vision Transformer (VIT) as an adapter is enough to match the classification performance of dual normalization layers, while using significantly less additional parameters. First, we improve upon the top-1 accuracy of a non-adversarially trained VIT-B16 model by +1.12% on IMAGENET (reaching 83.76% top-1 accuracy). Second, and more importantly, we show that training with adapters enables model soups through linear combinations of the clean and adversarial tokens. These model soups, which we call adversarial model soups, allow us to trade-off between clean and robust accuracy without sacrificing efficiency. Finally, we show that we can easily adapt the resulting models in the face of distribution shifts. Our VIT-B16 obtains top-1 accuracies on IMAGENET variants that are on average +4.00% better than those obtained with Masked Autoencoders. * Work done during an internship at DeepMind",
        "id":252780742
      },
      {
        "title":"Using Random Indexing to improve Singular Value Decomposition for Latent Semantic Analysis",
        "text":"We present results from using Random Indexing for Latent Semantic Analysis to handle Singular Value Decomposition tractability issues. We compare Latent Semantic Analysis, Random Indexing and Latent Semantic Analysis on Random Indexing reduced matrices. In this study we use a corpus comprising 1003 documents from the MEDLINE-corpus. Our results show that Latent Semantic Analysis on Random Indexing reduced matrices provide better results on Precision and Recall than Random Indexing only. Furthermore, computation time for Singular Value Decomposition on a Random Indexing reduced matrix is almost halved compared to Latent Semantic Analysis.",
        "id":7483866
      },
      {
        "title":"Published as a conference paper at ICLR 2023 LDMIC: LEARNING-BASED DISTRIBUTED MULTI- VIEW IMAGE CODING",
        "text":"Multi-view image compression plays a critical role in 3D-related applications. Existing methods adopt a predictive coding architecture, which requires joint encoding to compress the corresponding disparity as well as residual information. This demands collaboration among cameras and enforces the epipolar geometric constraint between different views, which makes it challenging to deploy these methods in distributed camera systems with randomly overlapping fields of view. Meanwhile, distributed source coding theory indicates that efficient data compression of correlated sources can be achieved by independent encoding and joint decoding, which motivates us to design a learning-based distributed multi-view image coding (LDMIC) framework. With independent encoders, LDMIC introduces a simple yet effective joint context transfer module based on the crossattention mechanism at the decoder to effectively capture the global inter-view correlations, which is insensitive to the geometric relationships between images. Experimental results show that LDMIC significantly outperforms both traditional and learning-based MIC methods while enjoying fast encoding speed. Code is released at https:\/\/github.com\/Xinjie-Q\/LDMIC.",
        "id":256194482
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"What paper mitigates the vocabulary size limitation when pretraining multilingual masked language models using a contrastive loss?",
    "positive_ctxs":[
      {
        "title":"Headless Language Models: Learning without Predicting with Contrastive Weight Tying",
        "text":"Self-supervised pre-training of language models usually consists in predicting probability distributions over extensive token vocabularies.In this study, we propose an innovative method that shifts away from probability prediction and instead focuses on reconstructing input embeddings in a contrastive fashion via Constrastive Weight Tying (CWT).We apply this approach to pretrain Headless Language Models in both monolingual and multilingual contexts.Our method offers practical advantages, substantially reducing training computational requirements by up to 20 times, while simultaneously enhancing downstream performance and data efficiency.We observe a significant +1.6 GLUE score increase and a notable +2.7 LAMBADA accuracy improvement compared to classical LMs within similar compute budgets.",
        "id":262013288
      }
    ],
    "negative_ctxs":[
      {
        "title":"Understanding Language Preference for Expression of Opinion and Sentiment: What do Hindi-English Speakers do on Twitter?",
        "text":"Linguistic research on multilingual societies has indicated that there is usually a preferred language for expression of emotion and sentiment (Dewaele, 2010). Paucity of data has limited such studies to participant interviews and speech transcriptions from small groups of speakers. In this paper, we report a study on 430,000 unique tweets from Indian users, specifically Hindi-English bilinguals, to understand the language of preference, if any, for expressing opinion and sentiment. To this end, we develop classifiers for opinion detection in these languages, and further classifying opinionated tweets into positive, negative and neutral sentiments. Our study indicates that Hindi (i.e., the native language) is preferred over English for expression of negative opinion and swearing. As an aside, we explore some common pragmatic functions of codeswitching through sentiment detection.",
        "id":15456486
      },
      {
        "title":"LANGUAGES OF ANALOGICAL STRINGS",
        "text":"",
        "id":11813565
      },
      {
        "title":"Graph-combined Coreference Resolution Methods on Conversational Machine Reading Comprehension with Pre-trained Language Model",
        "text":"Coreference resolution such as for anaphora has been an essential challenge that is commonly found in conversational machine reading comprehension (CMRC). This task aims to determine the referential entity to which a pronoun refers on the basis of contextual information. Existing approaches based on pre-trained language models (PLMs) mainly rely on an endto-end method, which still has limitations in clarifying referential dependency. In this study, a novel graph-based approach is proposed to integrate the coreference of given text into graph structures (called coreference graphs), which can pinpoint a pronoun's referential entity. We propose two graph-combined methods, evidence-enhanced and the fusion model, for CMRC to integrate coreference graphs from different levels of the PLM architecture. Evidenceenhanced refers to textual level methods that include an evidence generator (for generating new text to elaborate a pronoun) and enhanced question (for rewriting a pronoun in a question) as PLM input. The fusion model is a structural level method that combines the PLM with a graph neural network. We evaluated these approaches on a CoQA pronoun-containing dataset and the whole CoQA dataset. The result showed that our methods can outperform baseline PLM methods with BERT and RoBERTa.",
        "id":248780004
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_iclr",
    "question":"What paper proposes breaking down programming problems by predicting the objects that a solution would create?",
    "positive_ctxs":[
      {
        "title":"ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis",
        "text":"When writing programs, people have the ability to tackle a new complex task by decomposing it into smaller and more familiar subtasks. While it is difficult to measure whether neural program synthesis methods have similar capabilities, we can measure whether they compositionally generalize, that is, whether a model that has been trained on the simpler subtasks is subsequently able to solve more complex tasks. In this paper, we characterize several different forms of compositional generalization that are desirable in program synthesis, forming a meta-benchmark which we use to create generalization tasks for two popular datasets, RobustFill and DeepCoder. We then propose ExeDec, a novel decomposition-based synthesis strategy that predicts execution subgoals to solve problems step-by-step informed by program execution at each step. ExeDec has better synthesis performance and greatly improved compositional generalization ability compared to baselines.Preprint. Under review.",
        "id":260164542
      }
    ],
    "negative_ctxs":[
      {
        "title":"Topological Dependency Trees: A Constraint-Based Account of Linear Precedence",
        "text":"We describe a new framework for dependency grammar, with a modular decomposition of immediate dependency and linear precedence. Our approach distinguishes two orthogonal yet mutually constraining structures: a syntactic dependency tree and a topological dependency tree. The syntax tree is nonprojective and even non-ordered, while the topological tree is projective and partially ordered.",
        "id":1160899
      },
      {
        "title":"Redundancy: helping semantic disambiguation",
        "text":"Redundancy is a good thing, at least in a learning process. To be a good teacher you must say what you are going to say, say it, then say what you have just said. Well, three times is better than one. To acquire and learn knowledge from text for building a lexical knowledge base, we need to find a source of information that states facts, and repeats them a few times using slightly different sentence structures. A technique is needed for gathering information from that source and identify the redundant information. The extraction of the commonality is an active learning of the knowledge expressed. The proposed research is based on a clustering method developed byBarri~re and Popowich (1996)which performs a gathering of related information about a particular topic. Individual pieces of information are represented via the Conceptual Graph (CG) formalism and the result of the clustering is a large CG embedding all individual graphs. In the present paper, we suggest that the identification of the redundant information within the resulting graph is very useful for disambiguation of the original information at the semantic level.",
        "id":7861345
      },
      {
        "title":"A Salience-Based Approach to Gesture-Speech Alignment",
        "text":"One of the first steps towards understanding natural multimodal language is aligning gesture and speech, so that the appropriate gestures ground referential pronouns in the speech. This paper presents a novel technique for gesture-speech alignment, inspired by saliencebased approaches to anaphoric pronoun resolution. We use a hybrid between data-driven and knowledge-based mtehods: the basic structure is derived from a set of rules about gesture salience, but the salience weights themselves are learned from a corpus. Our system achieves 95% recall and precision on a corpus of transcriptions of unconstrained multimodal monologues, significantly outperforming a competitive baseline.",
        "id":311756
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"What paper showed first that one can build a fully differentiable mixture of experts layer with no increase in time complexity?",
    "positive_ctxs":[
      {
        "title":"From Sparse to Soft Mixtures of Experts",
        "text":"Sparse mixture of expert architectures (MoEs) scale model capacity without large increases in training or inference costs. Despite their success, MoEs suffer from a number of issues: training instability, token dropping, inability to scale the number of experts, or ineffective finetuning. In this work, we propose Soft MoE, a fully-differentiable sparse Transformer that addresses these challenges, while maintaining the benefits of MoEs. Soft MoE performs an implicit soft assignment by passing different weighted combinations of all input tokens to each expert. As in other MoE works, experts in Soft MoE only process a subset of the (combined) tokens, enabling larger model capacity at lower inference cost. In the context of visual recognition, Soft MoE greatly outperforms standard Transformers (ViTs) and popular MoE variants (Tokens Choice and Experts Choice). For example, Soft MoE-Base\/16 requires 10.5× lower inference cost (5.7× lower wall-clock time) than ViT-Huge\/14 while matching its performance after similar training. Soft MoE also scales well: Soft MoE Huge\/14 with 128 experts in 16 MoE layers has over 40× more parameters than ViT Huge\/14, while inference time cost grows by only 2%, and it performs substantially better. * Equal contribution. The order was decided by a coin toss. 1 arXiv:2308.00951v1 [cs.LG] 2 Aug 2023 1 def soft_m oe_lay er (X , Phi , experts ) : 2 # Compute the dispatch and combine weights .3 logits = jnp . einsum ( 'md , dnp -> mnp ' , X , Phi ) 4 D = jax . nn . softmax ( logits , axis =(0 ,) ) 5 C = jax . nn . softmax ( logits , axis =(1 , 2) ) 6 # The input slots are a weighted average of all the input tokens , 7 # given by the dispatch weights .8 Xs = jnp . einsum ( 'md , mnp -> npd ' , X , D ) 9 # Apply the corresponding expert function to each input slot .10 Ys = jnp . stack ([ 11 f_i ( Xs [i , : , :]) for i , f_i in enumerate ( experts ) ] , 12 axis =0) 13 # The output tokens are a weighted average of all the output slots , 14 # given by the combine weights . 15 Y = jnp . einsum ( 'npd , mnp -> md ' , Ys , C ) 16 return Y Algorithm 1: Simple JAX (Bradbury et al., 2018) implementation of a Soft MoE layer. Full code is available at https:\/\/github.com\/google-research\/vmoe.",
        "id":260378993
      }
    ],
    "negative_ctxs":[
      {
        "title":"HOW COULD RHETORICAL RELATIONS BE USED IN MACHINE TRANSLATION? (AND AT LEAST TWO OPEN QUESTIONS)",
        "text":"",
        "id":12385862
      },
      {
        "title":"",
        "text":"Cet article traite de l'autoapprentissage d'un système i-vector\/PLDA pour le regroupement en locuteurs de collections d'archives audiovisuelles françaises. Les paramètres d'extraction des i-vectors et du calcul des scores PLDA sont appris de façon non supervisée sur les données de la collection elle-même. Différents mélanges de données cibles et de données externes sont comparés pour la phase d'apprentissage. Les résultats expérimentaux sur deux corpora cibles distincts montrent que l'utilisation des données des corpora en question pour l'apprentissage itératif non supervisé et l'adaptation des paramètres de la PLDA peut améliorer un système existant, appris sur des données annotées externes. De tels résultats indiquent que la structuration automatique en locuteurs de petites collections non annotées ne devrait reposer que sur l'existence d'un corpus externe annoté, qui peut etre spécifiquement adaptéà chaque collection cible. Nous montronségalement qu'une collection suffisamment grande peut se passer de l'utilisation de ce corpus externe.ABSTRACTFirst investigations on self trained speaker diarizationThis paper investigates self trained cross-show speaker diarization applied to collections of French TV archives, based on an i-vector\/PLDA framework. The parameters used for i-vectors extraction and PLDA scoring are trained in a unsupervised way, using the data of the collection itself. Performances are compared, using combinations of target data and external data for training. The experimental results on two distinct target corpora show that using data from the corpora themselves to perform unsupervised iterative training and domain adaptation of PLDA parameters can improve an existing system, trained on external annotated data. Such results indicate that performing speaker indexation on small collections of unlabeled audio archives should only rely on the availability of a sufficient external corpus, which can be specifically adapted to every target collection. We show that a minimum collection size is required to exclude the use of such an external bootstrap.Actes de la conférence conjointe JEP-TALN-RECITAL 2016, volume 1 : JEP",
        "id":193799747
      },
      {
        "title":"Detecting co-derivative documents in large text collections",
        "text":"We have analyzed the SPEX algorithm byBernstein and Zobel (2004)for detecting co-derivative documents using duplicate n-grams. Although we totally agree with the claim that not using unique n-grams can greatly increase the efficiency and scalability of the process of detecting co-derivative documents, we have found serious bottlenecks in the way SPEX finds the duplicate n-grams. While the memory requirements for computing co-derivative documents can be reduced to up to 1 % by only using duplicate n-grams, SPEX needs about 40 times more memory for computing the list of duplicate n-grams itself. Therefore the memory requirements of the whole process are not reduced enough to make the algorithm practical for very large collections. We propose a solution for this problem using an external sort with the suffix array in-memory sorting and temporary file compression. The proposed algorithm for computing duplicate n-grams uses a fixed amount of memory for any input size.",
        "id":2112310
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_iclr",
    "question":"What paper shows that RLAIF can fully replace RLHF to align language models from scratch?",
    "positive_ctxs":[
      {
        "title":"SALMON: SELF-ALIGNMENT WITH PRINCIPLE-FOLLOWING REWARD MODELS",
        "text":"Supervised Fine-Tuning (SFT) on response demonstrations combined with Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful paradigm for aligning LLM-based AI agents. However, a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences. This paper presents a novel approach, namely SALMON (Self-ALignMent with principle-fOllowiNg reward models), to align base language models with minimal human supervision, using only a small set of human-defined principles, yet achieving superior performance. Central to our approach is a principle-following reward model. Trained on synthetic preference data, this model can generate reward scores based on arbitrary human-defined principles. By merely adjusting these principles during the RL training phase, we gain full control over the preferences with the reward model, subsequently influencing the behavior of the RLtrained policies, and eliminating the reliance on the collection of online human preferences. Applying our method to the LLaMA-2-70b base language model, we developed an AI assistant named Dromedary-2. With only 6 exemplars for in-context learning and 31 human-defined principles, Dromedary-2 significantly surpasses the performance of several state-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We have open-sourced the code and model weights to encourage further research into aligning LLMbased AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight.",
        "id":263831633
      }
    ],
    "negative_ctxs":[
      {
        "title":"Personalized Extractive Summarization Using an Ising Machine Towards Real-time Generation of Efficient and Coherent Dialogue Scenarios",
        "text":"We propose a personalized dialogue scenario generation system which transmits efficient and coherent information with a real-time extractive summarization method optimized by an Ising machine. The summarization problem is formulated as a quadratic unconstraint binary optimization (QUBO) problem, which extracts sentences that maximize the sum of the degree of user's interest in the sentences of documents with the discourse structure of each document and the total utterance time as constraints. To evaluate the proposed method, we constructed a news article corpus with annotations of the discourse structure, users' profiles, and interests in sentences and topics. The experimental results confirmed that a Digital Annealer, which is a simulated annealing-based Ising machine, can solve our QUBO model in a practical time without violating the constraints using this dataset.",
        "id":241583381
      },
      {
        "title":"PaVeDa -Pavia Verbs Database: Challenges and Perspectives",
        "text":"This paper describes an ongoing endeavor to construct Pavia Verbs Database (PaVeDa) -an open-access typological resource that builds upon previous work on verb argument structure, and in particular the Valency Patterns Leipzig (ValPaL) project(Hartmann et al., 2013). The PaVeDa database features four major innovations as compared to the ValPaL database: (i) it includes data from ancient languages enabling diachronic research; (ii) it expands the language sample to language families that are not represented in the ValPaL; (iii) it is linked to external corpora that are used as sources of usage-based examples of stored patterns; (iv) it introduces a new cross-linguistic layer of annotation for valency patterns which allows for contrastive data visualization.",
        "id":250390491
      },
      {
        "title":"Contrastive Out-of-Distribution Detection for Pretrained Transformers",
        "text":"Pretrained Transformers achieve remarkable performance when training and test data are from the same distribution. However, in realworld scenarios, the model often faces out-ofdistribution (OOD) instances that can cause severe semantic shift problems at inference time. Therefore, in practice, a reliable model should identify such instances, and then either reject them during inference or pass them over to models that handle another distribution. In this paper, we develop an unsupervised OOD detection method, in which only the indistribution (ID) data are used in training. We propose to fine-tune the Transformers with a contrastive loss, which improves the compactness of representations, such that OOD instances can be better differentiated from ID ones. These OOD instances can then be accurately detected using the Mahalanobis distance in the model's penultimate layer. We experiment with comprehensive settings and achieve near-perfect OOD detection performance, outperforming baselines drastically. We further investigate the rationales behind the improvement, finding that more compact representations through margin-based contrastive learning bring the improvement. We release our code to the community for future research 1 .",
        "id":233296689
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"What research first proposed a new kind of cascaded diffusion of a Markov process?",
    "positive_ctxs":[
      {
        "title":"RELAY DIFFUSION: UNIFYING DIFFUSION PROCESS ACROSS RESOLUTIONS FOR IMAGE SYNTHESIS",
        "text":"Diffusion models achieved great success in image synthesis, but still face challenges in high-resolution generation. Through the lens of discrete cosine transformation, we find the main reason is that the same noise level on a higher resolution results in a higher Signal-to-Noise Ratio in the frequency domain. In this work, we present Relay Diffusion Model (RDM), which transfers a low-resolution image or noise into an equivalent high-resolution one for diffusion model via blurring diffusion and block noise. Therefore, the diffusion process can continue seamlessly in any new resolution or model without restarting from pure noise or lowresolution conditioning. RDM achieves state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256×256, surpassing previous works such as ADM, LDM and DiT by a large margin. All the codes and checkpoints are open-sourced at https:\/\/github.com\/THUDM\/RelayDiffusion. Figure 1: (left): Generated Samples by RDM on ImageNet 256×256 and CelebA-HQ 256×256. (right): Benchmarking recent diffusion models on class-conditional ImageNet 256×256 generation without any guidance. RDM can achieve a FID of 1.87 if with classifier-free guidance. arXiv:2309.03350v1 [cs.CV] 4 Sep 2023Preprint generative models in recent years. However, challenges still exist in the training of diffusion models for high-resolution images. More specifically, there are two main obstacles:Training Efficiency. Although equipped with UNet to balance the memory and computation cost across different resolutions, diffusion models still require a large amount of resources to train on high-resolution images. One popular solution is to train the diffusion model on a latent (usually 4× compression rate in resolution) space and map the result back as pixels(Rombach et al., 2022), which is fast but inevitably suffers from some low-level artifacts. The cascaded method  trains a series of varying-size super-resolution diffusion models, which is effective but needs a complete sampling for each stage separately.Noise Schedule. Diffusion models need a noise schedule to control the amount of the isotropic Gaussian noise at each step. The setting of the noise schedule shows great influence over the performance, and most current models follow the linear (Ho et al., 2020) or cosine  schedule. However, an ideal noise schedule should be resolution-dependent (SeeFigure 2or Chen (2023)), resulting in suboptimal performance to train high-resolution models directly with common schedules designed for resolutions of 32×32 or 64×64 pixels.",
        "id":261582259
      }
    ],
    "negative_ctxs":[
      {
        "title":"An alternate approach towards meaningful lyric generation in Tamil",
        "text":"This paper presents our on-going work to improve the lyric generation component of the Automatic Lyric Generation system for the Tamil Language. An earlier version of the system used an n-gram based model to generate lyrics that match the given melody. This paper identifies some of the deficiencies in the melody analysis and text generation components of the earlier system and explains the new approach used to tackle those drawbacks. The two central approaches discussed in this paper are: (1) An improved mapping scheme for matching melody with words and (2) Knowledge-based Text Generation algorithm based on an existing Ontology and Tamil Morphology Generator.",
        "id":9454296
      },
      {
        "title":"Towards Topic Labeling with Phrase Entailment and Aggregation",
        "text":"We propose a novel framework for topic labeling that assigns the most representative phrases for a given set of sentences covering the same topic. We build an entailment graph over phrases that are extracted from the sentences, and use the entailment relations to identify and select the most relevant phrases. We then aggregate those selected phrases by means of phrase generalization and merging. We motivate our approach by applying over conversational data, and show that our framework improves performance significantly over baseline algorithms.",
        "id":11197260
      },
      {
        "title":"Who is GPT-3? An Exploration of Personality, Values and Demographics",
        "text":"Language models such as GPT-3 have caused a furore in the research community. Some studies found that GPT-3 has some creative abilities and makes mistakes that are on par with human behaviour. This paper answers a related question: Who is GPT-3? We administered two validated measurement tools to GPT-3 to assess its personality, the values it holds and its selfreported demographics. Our results show that GPT-3 scores similarly to human samples in terms of personality and -when provided with a model response memory -in terms of the values it holds. We provide the first evidence of psychological assessment of the GPT-3 model and thereby add to our understanding of this language model. We close with suggestions for future research that moves social science closer to language models and vice versa. * Equal first-authorship contribution: authorship order for MM and NR was determined by a random number generator.",
        "id":252595794
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"What work first uses LLM to code robotic simulation tasks and show sim-to-real benefits with policy pre-training in simulation?",
    "positive_ctxs":[
      {
        "title":"GENSIM: GENERATING ROBOTIC SIMULATION TASKS VIA LARGE LANGUAGE MODELS",
        "text":"Collecting large amounts of real-world interaction data to train general robotic policies is often prohibitively expensive, thus motivating the use of simulation data. However, existing methods for data generation have generally focused on scenelevel diversity (e.g., object instances and poses) rather than task-level diversity, due to the human effort required to come up with and verify novel tasks. This has made it challenging for policies trained on simulation data to demonstrate significant task-level generalization. In this paper, we propose to automatically generate rich simulation environments and expert demonstrations by exploiting a large language models' (LLM) grounding and coding ability. Our approach, dubbed GENSIM, has two modes: goal-directed generation, wherein a target task is given to the LLM and the LLM proposes a task curriculum to solve the target task, and exploratory generation, wherein the LLM bootstraps from previous tasks and iteratively proposes novel tasks that would be helpful in solving more complex tasks. We use GPT4 to expand the existing benchmark by ten times to over 100 tasks, on which we conduct supervised finetuning and evaluate several LLMs including finetuned GPTs and Code Llama on code generation for robotic simulation tasks. Furthermore, we observe that LLMs-generated simulation programs can enhance task-level generalization significantly when used for multitask policy training. We further find that with minimal sim-to-real adaptation, the multitask policies pretrained on GPT4-generated simulation tasks exhibit stronger transfer to unseen long-horizon tasks in the real world and outperform baselines by 25%. 1 , et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.Xinyun Chen, Chang Liu, and Dawn Song. Execution-guided neural program synthesis. , et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023.",
        "id":263605851
      }
    ],
    "negative_ctxs":[
      {
        "title":"FAA: Fine-grained Attention Alignment for Cascade Document Ranking",
        "text":"Document ranking aims at sorting a collection of documents with their relevance to a query. Contemporary methods explore more efficient transformers or divide long documents into passages to handle the long input. However, intensive query-irrelevant content may lead to harmful distraction and high query latency. Some recent works further propose cascade document ranking models that extract relevant passages with an efficient selector before ranking, however, their selection and ranking modules are almost independently optimized and deployed, leading to selecting error reinforcement and sub-optimal performance. In fact, the document ranker can provide fine-grained supervision to make the selector more generalizable and compatible, and the selector built upon a different structure can offer a distinct perspective to assist in document ranking. Inspired by this, we propose a fine-grained attention alignment approach to jointly optimize a cascade document ranking model. Specifically, we utilize the attention activations over the passages from the ranker as fine-grained attention feedback to optimize the selector. Meanwhile, we fuse the relevance scores from the passage selector into the ranker to assist in calculating the cooperative matching representation. Experiments on MS MARCO and TREC DL demonstrate the effectiveness of our method.",
        "id":259370641
      },
      {
        "title":"Transferring Coreference Chains through Word Alignment",
        "text":"This paper investigates the problem of automatically annotating resources with NP coreference information using a parallel corpus, English-Romanian, in order to transfer, through word alignment, coreference chains from the English part to the Romanian part of the corpus. The results show that we can detect Romanian referential expressions and coreference chains with over 80% F-measure, thus using our method as a preprocessing step followed by manual correction as part of an annotation effort for creating a large Romanian corpus with coreference information is worthwhile.",
        "id":13388961
      },
      {
        "title":"Well-Nested Tree Languages and Attributed Tree Transducers",
        "text":"Well-nested word languages have been advertised as adequate formalizations of the notion of mild context-sensitivity. The main result of this paper is a characterization of well-nested tree languages in terms of simple attributed tree transducers.",
        "id":33414645
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"What work proposes a model to learn a latent regular cell complex from data?",
    "positive_ctxs":[
      {
        "title":"From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module",
        "text":"Latent Graph Inference (LGI) relaxed the reliance of Graph Neural Networks (GNNs) on a given graph topology by dynamically learning it. However, most of LGI methods assume to have a (noisy, incomplete, improvable, ...) input graph to rewire and can solely learn regular graph topologies. In the wake of the success of Topological Deep Learning (TDL), we study Latent Topology Inference (LTI) for learning higher-order cell complexes (with sparse and not regular topology) describing multi-way interactions between data points. To this aim, we introduce the Differentiable Cell Complex Module (DCM), a novel learnable function that computes cell probabilities in the complex to improve the downstream task. We show how to integrate DCM with cell complex message passing networks layers and train it in a end-to-end fashion, thanks to a two-step inference procedure that avoids an exhaustive search across all possible cells in the input, thus maintaining scalability. Our model is tested on several homophilic and heterophilic graph datasets and it is shown to outperform other state-of-the-art techniques, offering significant improvements especially in cases where an input graph is not provided. * Equal contribution. Corresponding authors,",
        "id":258887582
      }
    ],
    "negative_ctxs":[
      {
        "title":"Cardinal , nominal or ordinal similarity measures in comparative evaluation of information retrieval process",
        "text":"Similarity measures are used to quantify the resemblance of two sets. Simplest ones are calculated by ratios of the document's number of the compared sets. These measures are simple and usually employed in first steps of evaluation studies, they are called cardinal measures. Others measures compare sets upon the number of common documents they have. They are usually employed in quantitative information retrieval evaluations, some examples are Jaccard, Cosine, Recall or Precision. These measures are called nominal ones. There are more or less adapted in function of the richness of the information system's answer. Indeed, in the past, they were sufficient because answers given by systems were only composed by an unordered set of documents. But usual systems improve the quality or the visibility of there answers by using a relevant ranking or a clustering presentation of documents. In this case, similarity measures aren't adapted. In this paper we present some solutions in the case of totally ordered and partially ordered answer.",
        "id":3605426
      },
      {
        "title":"Dialogue complexity with portability? Research directions for the Information State approach",
        "text":"We review existing types of dialogue managers (DMs), and propose that the Information State (IS) approach may allow both complexity of dialogue and ease of portability. We discuss implementational drawbacks of the only existing IS DM, and describe our work underway to develop a new DM resolving those drawbacks.",
        "id":15691543
      },
      {
        "title":"Exploring Weaknesses of VQA Models through Attribution Driven Insights",
        "text":"Deep Neural Networks have been successfully used for the task of Visual Question Answering for the past few years owing to the availability of relevant large scale datasets. However these datasets are created in artificial settings and rarely reflect the real world scenario. Recent research effectively applies these VQA models for answering visual questions for the blind. Despite achieving high accuracy these models appear to be susceptible to variation in input questions.We analyze popular VQA models through the lens of attribution (input's influence on predictions) to gain valuable insights. Further, We use these insights to craft adversarial attacks which inflict significant damage to these systems with negligible change in meaning of the input questions. We believe this will enhance development of systems more robust to the possible variations in inputs when deployed to assist the visually impaired.",
        "id":219573758
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"What work proposes to combine video foundation models with vision language models to effective high dimensional robot planning?",
    "positive_ctxs":[
      {
        "title":"VIDEO LANGUAGE PLANNING",
        "text":"We are interested in enabling visual planning for complex long-horizon tasks in the space of generated videos and language, leveraging recent advances in large generative models pretrained on Internet-scale data. To this end, we present video language planning (VLP), an algorithm that consists of a tree search procedure, where we train (i) vision-language models to serve as both policies and value functions, and (ii) text-to-video models as dynamics models. VLP takes as input a long-horizon task instruction and current image observation, and outputs a long video plan that provides detailed multimodal (video and language) specifications that describe how to complete the final task. VLP scales with increasing computation budget where more computation time results in improved video plans, and is able to synthesize long-horizon video plans across different robotics domainsfrom multi-object rearrangement, to multi-camera bi-arm dexterous manipulation. Generated video plans can be translated into real robot actions via goal-conditioned policies, conditioned on each intermediate frame of the generated video. Experiments show that VLP substantially improves long-horizon task success rates compared to prior methods on both simulated and real robots (across 3 hardware platforms).Step 1: push blue triangle to …Step 1: push red star to left …Step 2: (re-plan) t t + k t current t goalStep 2: (re-plan)Step 2: (re-plan)",
        "id":264172935
      }
    ],
    "negative_ctxs":[
      {
        "title":"Learning Rationalizable Equilibria in Multiplayer Games",
        "text":"A natural goal in multiagent learning besides finding equilibria is to learn rationalizable behavior, where players learn to avoid iteratively dominated actions. However, even in the basic setting of multiplayer general-sum games, existing algorithms require a number of samples exponential in the number of players to learn rationalizable equilibria under bandit feedback. This paper develops the first line of efficient algorithms for learning rationalizable Coarse Correlated Equilibria (CCE) and Correlated Equilibria (CE) whose sample complexities are polynomial in all problem parameters including the number of players. To achieve this result, we also develop a new efficient algorithm for the simpler task of finding one rationalizable action profile (not necessarily an equilibrium), whose sample complexity substantially improves over the best existing results of Wu et al. (2021). Our algorithms incorporate several novel techniques to guarantee rationalizability and no (swap-)regret simultaneously, including a correlated exploration scheme and adaptive learning rates, which may be of independent interest. We complement our results with a sample complexity lower bound showing the sharpness of our guarantees. * Equal contribution. ∆ 2 1 samples in normal-form games with N players, A actions per player and a minimum elimination length of L. This greatly improves the result of Wu et al. (2021) and is tight up to logarithmic factors when L = O(1). 1 Throughout this paper, we use O to suppress logarithmic factors in N , A,",
        "id":253018433
      },
      {
        "title":"Published as a conference paper at ICLR 2021 DO NOT LET PRIVACY OVERBILL UTILITY: GRADIENT EMBEDDING PERTURBATION FOR PRIVATE LEARNING",
        "text":"The privacy leakage of the model about the training data can be bounded in the differential privacy mechanism. However, for meaningful privacy parameters, a differentially private model degrades the utility drastically when the model comprises a large number of trainable parameters. In this paper, we propose an algorithm Gradient Embedding Perturbation (GEP) towards training differentially private deep models with decent accuracy. Specifically, in each gradient descent step, GEP first projects individual private gradient into a non-sensitive anchor subspace, producing a low-dimensional gradient embedding and a small-norm residual gradient. Then, GEP perturbs the low-dimensional embedding and the residual gradient separately according to the privacy budget. Such a decomposition permits a small perturbation variance, which greatly helps to break the dimensional barrier of private learning. With GEP, we achieve decent accuracy with reasonable computational cost and modest privacy guarantee for deep models. Especially, with privacy bound = 8, we achieve 74.9% test accuracy on CIFAR10 and 95.1% test accuracy on SVHN, significantly improving over existing results. * Authors contribute equally to this work.",
        "id":232046284
      },
      {
        "title":"A Survey of Leveled Reading 呔周呥 呣呯呮呣呥呰呴 呯呦 呌呥呶呥呬呥呤 呒呥呡呤呩呮呧 吨呌呒吩 呯呲呩呧呩呮呡呴呥味 呩呮 呴周呥 呥呡呲呬呹 呴呷呥呮呴呩呥呴周 呣呥呮呴呵呲呹 呡呭呯呮呧 呥呤呵呣呡呴呯呲味吮 呁味 呰呥呯呰呬呥 呧呲呡呤呵呡呬呬呹 呥呭呰周呡味呩呺呥 呭呯呲呥 呯呮 呲呥呡呤呩呮呧听 呴周呥呹 呰呡呹 呭呯呲呥 呡呴呴呥呮呴呩呯呮 呴呯 呌呒听 呡呮呤 呴周呵味 呦呯味呴呥呲呩呮呧 呤呥呶呥呬呯呰呭呥呮呴味 呩呮 呡呵呴呯呭呡呴呩呣 呌呒 呭呥呴周呯呤味吮 呔周呩味 呰呡呰呥呲 呰呲呯呶呩呤呥味 呡呮 呯呶呥呲呶呩呥呷 呯呦 呴周呥 呲呥呣呥呮呴 呌呒 呤呥呶呥呬呯呰呭呥呮呴味吮 呆呩呲味呴呬呹听 呷呥 呩呮呴呲呯呤呵呣呥 呴周呥 呥呸呩味呴呩呮呧 味呴呡呮呤呡呲呤味 呯呦 呌呒听 呦呯呬呬呯呷呥呤 呢呹 呴周呥 呤呥呶呥呬呯呰呭呥呮呴 呯呦 呶呡呲呩呯呵味 味呹味呴呥呭味 呡呮呤 呤呡呴呡 呲呥味呯呵呲呣呥味吮 呔周呥呮听 呷呥 呣呬呡味吭 味呩呦呹 呷呩呤呥呬呹吭呡呰呰呬呩呥呤 呡呵呴呯呭呡呴呩呣 呌呒 呭呥呴周呯呤味 呩呮呴呯 呴周呲呥呥 呴呹呰呥味吺呦呯呲呭呵呬呡 呭呥呴周呯呤听 呴呲呡呤呩呴呩呯呮呡呬 呭呡呣周呩呮呥 呬呥呡呲呮呩呮呧 呭呥呴周呯呤 呡呮呤 呴周呥 呲呥呣呥呮呴呬呹 呭呡呩呮味呴呲呥呡呭 呤呥呥呰 呬呥呡呲呮呩呮呧 呭呥呴周呯呤吮 呗呥 呥呸吭 呰呬呯呲呥 呴周呥 呡呤呶呡呮呴呡呧呥味 呡呮呤 呤呩味呡呤呶呡呮呴呡呧呥味 呯呦 呴周呥味呥 呭呥呴周呯呤味 呴周呲呯呵呧周 呥呸呰呥呲呩呭呥呮呴味 呡呮呤 呩呮呶呥味呴呩呧呡呴呥 呰呯味味呩呢呬呥 呰呥呲呦呯呲呭呡呮呣呥 呩呭呰呲呯呶呥呭呥呮呴味吮 呆呩呮呡呬呬呹听 呷呥 味呵呭呭呡呲呩呺呥 呡呮呤 呰呲呯味呰呥呣呴 呴周呥 呦呵呴呵呲呥 呤呥呶呥呬呯呰呭呥呮呴 呯呦 呌呒 呡呮呤 呰呲呯呶呩呤呥 味呥呶呥呲呡呬 同呥呬呤味 呴周呡呮 呣呡呮 呢呥呮呥同呴 呦呲呯呭 呌呒吮",
        "text":"",
        "id":237365388
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which backdoor paper first used the CLIP to suppress benign features and enhance poisoning features to design triggers?",
    "positive_ctxs":[
      {
        "title":"Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios",
        "text":"Recent deep neural networks (DNNs) have come to rely on vast amounts of training data, providing an opportunity for malicious attackers to exploit and contaminate the data to carry out backdoor attacks. These attacks significantly undermine the reliability of DNNs. However, existing backdoor attack methods make unrealistic assumptions, assuming that all training data comes from a single source and that attackers have full access to the training data. In this paper, we address this limitation by introducing a more realistic attack scenario where victims collect data from multiple sources, and attackers cannot access the complete training data. We refer to this scenario as data-constrained backdoor attacks. In such cases, previous attack methods suffer from severe efficiency degradation due to the entanglement between benign and poisoning features during the backdoor injection process.IntroductionDeep neural networks (DNNs) are widely utilized and powerful machine learning algorithms inspired by the structure and functioning of the human brain. They excel at learning intricate patterns in data, making them invaluable for various applications such as image recognition[17,21], natural language processing[33,68], image generation[20,30], and anomaly detection[45,64]. However, the effectiveness of DNNs heavily relies on the quantity and quality of the training data. For instance, Stable Diffusion [49], a generative model with 983 million parameters, owes its success in image generation tasks to pre-training on 5 billion image-text pairs. Similarly, GPT-3 [3], a language model with 175 billion * Equal Contribution.",
        "id":259165262
      }
    ],
    "negative_ctxs":[
      {
        "title":"Lessons Learned in Part-of-Speech Tagging of Conversational Speech",
        "text":"This paper examines tagging models for spontaneous English speech transcripts. We analyze the performance of state-of-the-art tagging models, either generative or discriminative, left-to-right or bidirectional, with or without latent annotations, together with the use of ToBI break indexes and several methods for segmenting the speech transcripts (i.e., conversation side, speaker turn, or humanannotated sentence). Based on these studies, we observe that: (1) bidirectional models tend to achieve better accuracy levels than left-toright models, (2) generative models seem to perform somewhat better than discriminative models on this task, and (3) prosody improves tagging performance of models on conversation sides, but has much less impact on smaller segments. We conclude that, although the use of break indexes can indeed significantly improve performance over baseline models without them on conversation sides, tagging accuracy improves more by using smaller segments, for which the impact of the break indexes is marginal.",
        "id":2216155
      },
      {
        "title":"Published as a conference paper at ICLR 2020 LEARNING TO REPRESENT PROGRAMS WITH PROPERTY SIGNATURES",
        "text":"We introduce the notion of property signatures, a representation for programs and program specifications meant for consumption by machine learning algorithms. Given a function with input type τ in and output type τ out , a property is a function of type: (τ in , τ out ) → Bool that (informally) describes some simple property of the function under consideration. For instance, if τ in and τ out are both lists of the same type, one property might ask 'is the input list the same length as the output list?'. If we have a list of such properties, we can evaluate them all for our function to get a list of outputs that we will call the property signature. Crucially, we can 'guess' the property signature for a function given only a set of input\/output pairs meant to specify that function. We discuss several potential applications of property signatures and show experimentally that they can be used to improve over a baseline synthesizer so that it emits twice as many programs in less than one-tenth of the time.",
        "id":211252650
      },
      {
        "title":"",
        "text":"",
        "id":171124654
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which foundation model paper first proposed a time series model with proposed financial time series and text data?",
    "positive_ctxs":[
      {
        "title":"TEMPO: PROMPT-BASED GENERATIVE PRE-TRAINED TRANSFORMER FOR TIME SERIES FORECASTING",
        "text":"The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the selection-based prompts to facilitate distribution adaptation in non-stationary time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPOover state-of-the-art methods on a number of time series benchmark datasets. This performance gain is observed not only in standard supervised learning settings but also in scenarios involving previously unseen datasets as well as in scenarios with multi-modal inputs. This compelling finding highlights TEMPO's potential to constitute a foundational model-building framework.",
        "id":263829348
      }
    ],
    "negative_ctxs":[
      {
        "title":"Prototypical Opinion Holders: What We can Learn from Experts and Analysts",
        "text":"In order to automatically extract opinion holders, we propose to harness the contexts of prototypical opinion holders, i.e. common nouns, such as experts or analysts, that describe particular groups of people whose profession or occupation is to form and express opinions towards specific items. We assess their effectiveness in supervised learning where these contexts are regarded as labeled training data and in rule-based classification which uses predicates that frequently co-occur with mentions of the prototypical opinion holders. Finally, we also examine in how far knowledge gained from these contexts can compensate the lack of large amounts of labeled training data in supervised learning by considering various amounts of actually labeled training sets.",
        "id":13923239
      },
      {
        "title":"Grouping Words Using Statistical Context",
        "text":"This paper describes the use of statistical analyses of untagged corpora to detect similarities and differences in the meaning of words in text. This work is motivated by psychological as well as by computational issues. The limitations of the method of cluster analysis in assessing the success of such analyses are discussed, and ongoing research using an alternative unsupervised neural network approach is described.",
        "id":9687117
      },
      {
        "title":"SISER: Semantic-Infused Selective Graph Reasoning for Fact Verification",
        "text":"This study proposes Semantic-Infused SElective Graph Reasoning (SISER) for fact verification, which newly presents semantic-level graph reasoning and injects its reasoning-enhanced representation into other types of graph-based and sequence-based reasoning methods. SISER combines three reasoning types: 1) semantic-level graph reasoning, which uses a semantic graph from evidence sentences, whose nodes are elements of a triple -<Subject, Verb, Object>, 2) \"semantic-infused\" sentence-level \"selective\" graph reasoning, which combine semanticlevel and sentence-level representations and perform graph reasoning in a selective manner using the node selection mechanism, and 3) sequence reasoning, which concatenates all evidence sentences and performs attentionbased reasoning. Experiment results on a large-scale dataset for Fact Extraction and VERification (FEVER) show that SISER outperforms the previous graph-based approaches and achieves state-of-the-art performance. † Corresponding author ♠ Equal contribution * This work was done during his M.S. degree at JBNU. 1 https:\/\/competitions.codalab.org\/ competitions\/18814",
        "id":252819029
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which is one of the first papers to highlight and resolve the distribution shift in RLHF?",
    "positive_ctxs":[
      {
        "title":"PARL: A Unified Framework for Policy Alignment in Reinforcement Learning",
        "text":"We present a novel unified bilevel optimization-based framework, PARL, formulated to address the recently highlighted critical issue of policy alignment in reinforcement learning using utility or preferencebased feedback.We identify a major gap within current algorithmic designs for solving policy alignment due to a lack of precise characterization of the dependence of the alignment objective on the data generated by policy trajectories.This shortfall contributes to the sub-optimal performance observed in contemporary algorithms.Our framework addressed these concerns by explicitly parameterizing the distribution of the upper alignment objective (reward design) by the lower optimal variable (optimal policy for the designed reward).Interestingly, from an optimization perspective, our formulation leads to a new class of stochastic bilevel problems where the stochasticity at the upper objective depends upon the lower-level variable.To demonstrate the efficacy of our formulation in resolving alignment issues in RL, we devised an algorithm named A-PARL to solve PARL problem, establishing sample complexity bounds of order O(1\/T ).Our empirical results substantiate that the proposed PARL can address the alignment concerns in RL by showing significant improvements (up to 63% in terms of required samples) for policy alignment in large-scale environments of the Deepmind control suite and Meta world tasks.",
        "id":260683028
      }
    ],
    "negative_ctxs":[
      {
        "title":"Statistical Generation: Three Methods Compared and Evaluated",
        "text":"Statistical NLG has largely meant n-gram modelling which has the considerable advantages of lending robustness to NLG systems, and of making automatic adaptation to new domains from raw corpora possible. On the downside, n-gram models are expensive to use as selection mechanisms and have a built-in bias towards shorter realisations. This paper looks at treebank-training of generators, an alternative method for building statistical models for NLG from raw corpora, and two different ways of using treebank-trained models during generation. Results show that the treebank-trained generators achieve improvements similar to a 2-gram generator over a baseline of random selection. However, the treebank-trained generators achieve this at a much lower cost than the 2-gram generator, and without its strong preference for shorter realisations.",
        "id":8493310
      },
      {
        "title":"TEAM UFAL @ CreativeSumm 2022: BART and SamSum based few-shot approach for creative Summarization",
        "text":"This system description paper details TEAM UFAL's approach for the SummScreen, TVMegasite subtask of the CreativeSumm shared task. The subtask deals with creating summaries for dialogues from TV Soap operas. We utilized BART based pre-trained model fine-tuned on SamSum dialouge summarization dataset. Few examples from Au-toMin dataset and the dataset provided by the organizers were also inserted into the data as a few-shot learning objective. The additional data was manually broken into chunks based on different boundaries in summary and the dialogue file. For inference we choose a similar strategy as the top-performing team at AutoMin 2021, where the data is split into chunks, either on [SCENE_CHANGE]  or exceeding a predefined token length, to accommodate the maximum token possible in the pre-trained model for one example. We implemented two different strategies as splits on [SCENE_CHANGE]   did not necessarily mean having less than 1024 tokens in a segment.",
        "id":252819498
      },
      {
        "title":"Findings of the 2017 DiscoMT Shared Task on Cross-lingual Pronoun Prediction",
        "text":"We describe the design, the setup, and the evaluation results of the DiscoMT 2017 shared task on cross-lingual pronoun prediction. The task asked participants to predict a target-language pronoun given a source-language pronoun in the context of a sentence. We further provided a lemmatized target-language human-authored translation of the source sentence, and automatic word alignments between the source sentence words and the targetlanguage lemmata. The aim of the task was to predict, for each target-language pronoun placeholder, the word that should replace it from a small, closed set of classes, using any type of information that can be extracted from the entire document.We offered four subtasks, each for a different language pair and translation direction: English-to-French, Englishto-German, German-to-English, and Spanish-to-English.Five teams participated in the shared task, making submissions for all language pairs. The evaluation results show that all participating teams outperformed two strong n-gram-based language model-based baseline systems by a sizable margin.",
        "id":7382485
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which machine learning paper proposed certified robustness in the malware detection domain?",
    "positive_ctxs":[
      {
        "title":"DRSM: DE-RANDOMIZED SMOOTHING ON MALWARE CLASSIFIER PROVIDING CERTIFIED ROBUSTNESS",
        "text":"Machine Learning (ML) models have been utilized for malware detection for over two decades.Consequently, this ignited an ongoing arms race between malware authors and antivirus systems, compelling researchers to propose defenses for malware-detection models against evasion attacks.However, most if not all existing defenses against evasion attacks suffer from sizable performance degradation and\/or can defend against only specific attacks, which makes them less practical in real-world settings.In this work, we develop a certified defense, DRSM (De-Randomized Smoothed MalConv), by redesigning the de-randomized smoothing technique for the domain of malware detection.Specifically, we propose a window ablation scheme to provably limit the impact of adversarial bytes while maximally preserving local structures of the executables.After showing how DRSM is theoretically robust against attacks with contiguous adversarial bytes, we verify its performance and certified robustness experimentally, where we observe only marginal accuracy drops as the cost of robustness.To our knowledge, we are the first to offer certified robustness in the realm of static detection of malware executables.More surprisingly, through evaluating DRSM against 9 empirical attacks of different types, we observe that the proposed defense is empirically robust to some extent against a diverse set of attacks, some of which even fall out of the scope of its original threat model.In addition, we collected 15.5K recent benign raw executables from diverse sources, which will be made public as a dataset called PACE (Publicly Accessible Collection(s) of Executables) to alleviate the scarcity of publicly available benign datasets for studying malware detection and provide future research with more representative data of the time.",
        "id":257687205
      }
    ],
    "negative_ctxs":[
      {
        "title":"From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding",
        "text":"Current state-of-the-art models for natural language understanding require a preprocessing step to convert raw text into discrete tokens. This process known as tokenization relies on a pre-built vocabulary of words or sub-word morphemes. This fixed vocabulary limits the model's robustness to spelling errors and its capacity to adapt to new domains. In this work, we introduce a novel open-vocabulary language model that adopts a hierarchical two-level approach: one at the word level and another at the sequence level. Concretely, we design an intraword module that uses a shallow Transformer architecture to learn word representations from their characters, and a deep inter-word Transformer module that contextualizes each word representation by attending to the entire word sequence. Our model thus directly operates on character sequences with explicit awareness of word boundaries, but without biased sub-word or word-level vocabulary. Experiments on various downstream tasks show that our method outperforms strong baselines. We also demonstrate that our hierarchical model is robust to textual corruption and domain shift.",
        "id":258865351
      },
      {
        "title":"Analyse morphologique non supervisée en domaine biomédical Application à la recherche d'information",
        "text":"Dans le domaine biomédical, utiliser des termes spécialisés est essentiel pour accéder à l'information. Cependant, dans beaucoup de langues, ces termes sont des constructions morphologiques complexes qui compliquent cet accès à l'information. Dans cet article, nous nous intéressons à l'identification des composants morphologiques de ces termes et à leur utilisation pour une tâche de recherche d'information (RI). Nous proposons différentes approches reposant sur un alignement automatique avec une langue pivot particulière, le japonais, et sur un apprentissage par analogie permettant de produire des analyses morphologiques fines des termes d'une langue donnée. Ces analyses morphologiques sont ensuite utilisées pour améliorer l'indexation de documents biomédicaux. Les expériences rapportées montrent la validité de cette approche avec des gains en MAP de plus de 10 % par rapport à un système de RI standard.ABSTRACT. In the biomedical field, using of specialized terms is key to access information. However, in most Indo-European languages, these terms are complex morphological structures. The presented work aims at identifying the various meaningful components of these terms and use them to improve biomedical Information Retrieval (IR). We present different approaches combining automatic alignments with a pivot language, Japanese, and analogical learning that allows an accurate morphological analysis of terms. These morphological analysis are used to improve the indexing of medical documents. The experiments reported in this paper show the validity of this approach with a 10% MAP improvement over a standard IR system. MOTS-CLÉS : morphologie, terminologie biomédicale, alignement, apprentissage par analogie, indexation morphosémantique, recherche d'information biomédicale.",
        "id":260165148
      },
      {
        "title":"Exploiting domain-slot related keywords description for Few-Shot Cross-Domain Dialogue State Tracking",
        "text":"Collecting dialogue data with domain-slotvalue labels for dialogue state tracking (DST) could be a costly process. In this paper, we propose a novel framework based on domain-slot related description to tackle the challenge of few-shot cross-domain DST. Specifically, we design an extraction module to extract domainslot related verbs and nouns in the dialogue. Then, we integrates them into the description, which aims to prompt the model to identify the slot information. Furthermore, we introduce a random sampling strategy to improve the domain generalization ability of the model. We utilize a pre-trained model to encode contexts and description and generates answers with an auto-regressive manner. Experimental results show that our approaches substantially outperform the existing few-shot DST methods on MultiWOZ and gain strong improvements on the slot accuracy comparing to existing slot description methods.",
        "id":256460900
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_iclr",
    "question":"Which multimodal large language model represents visual data as the discrete tokens like text and training with the unified next-token prediction objective?",
    "positive_ctxs":[
      {
        "title":"UNIFIED LANGUAGE-VISION PRETRAINING IN LLM WITH DYNAMIC DISCRETE VISUAL TOKENIZATION",
        "text":"TE VISUAL TOKENIZATION\n29 Sep 20234C8D833F4622C6A583127C3A667E25A5arXiv:2309.04669v2[cs.CV]\nRecently, the remarkable advance of the Large Language Model (LLM) has inspired researchers to transfer its extraordinary reasoning capability to both vision and language data.However, the prevailing approaches primarily regard the visual input as a prompt and focus exclusively on optimizing the text generation process conditioned upon vision content by a frozen LLM.Such an inequitable treatment of vision and language heavily constrains the model's potential.In this paper, we break through this limitation by representing both vision and language in a unified form.Specifically, we introduce a well-designed visual tokenizer to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that LLM can read.The resulting visual tokens encompass high-level semantics worthy of a word and also support dynamic sequence length varying from the image.Coped with this tokenizer, the presented foundation model called LaVIT can handle both image and text indiscriminately under the same generative learning paradigm.This unification empowers LaVIT to serve as an impressive generalist interface to understand and generate multi-modal content simultaneously.Extensive experiments further showcase that it outperforms the existing models by a large margin on massive vision-language tasks.Our code and models will be available at https:",
        "id":263889455
      },
      {
        "title":"UNIFIED LANGUAGE-VISION PRETRAINING IN LLM WITH DYNAMIC DISCRETE VISUAL TOKENIZATION",
        "text":"Recently, the remarkable advance of the Large Language Model (LLM) has inspired researchers to transfer its extraordinary reasoning capability to both vision and language data. However, the prevailing approaches primarily regard the visual input as a prompt and focus exclusively on optimizing the text generation process conditioned upon vision content by a frozen LLM. Such an inequitable treatment of vision and language heavily constrains the model's potential. In this paper, we break through this limitation by representing both vision and language in a unified form. Specifically, we introduce a well-designed visual tokenizer to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that LLM can read. The resulting visual tokens encompass high-level semantics worthy of a word and also support dynamic sequence length varying from the image. Coped with this tokenizer, the presented foundation model called LaVIT can handle both image and text indiscriminately under the same generative learning paradigm. This unification empowers LaVIT to serve as an impressive generalist interface to understand and generate multi-modal content simultaneously. Extensive experiments further showcase that it outperforms the existing models by a large margin on massive vision-language tasks. Our code and models will be available at https:\/\/github.com\/jy0205\/LaVIT. * Work done during an internship at Kuaishou Technology.",
        "id":261682321
      }
    ],
    "negative_ctxs":[
      {
        "title":"Using Domain Similarity for Performance Estimation",
        "text":"Many natural language processing (NLP) tools exhibit a decrease in performance when they are applied to data that is linguistically different from the corpus used during development. This makes it hard to develop NLP tools for domains for which annotated corpora are not available. This paper explores a number of metrics that attempt to predict the cross-domain performance of an NLP tool through statistical inference. We apply different similarity metrics to compare different domains and investigate the correlation between similarity and accuracy loss of NLP tool. We find that the correlation between the performance of the tool and the similarity metric is linear and that the latter can therefore be used to predict the performance of an NLP tool on out-of-domain data. The approach also provides a way to quantify the difference between domains.",
        "id":7986308
      },
      {
        "title":"",
        "text":"",
        "id":207756646
      },
      {
        "title":"Enhancing Digital History -Event Discovery via Topic Modeling and Change Detection",
        "text":"Digital history is the application of computer science techniques to historical data in order to uncover insights into events occurring during specific time periods from the past. This relatively new interdisciplinary field can help identify and record latent information about political, cultural, and economic trends that are not otherwise apparent from traditional historical analysis. This paper presents a method that uses topic modeling and breakpoint detection to observe how extracted topics come in and out of prominence over various time periods. We apply our techniques on British parliamentary speech data from the 19th century. Findings show that some of the events produced are cohesive in topic content (religion, transportation, economics, etc.) and time period (events are focused in the same year or month). Topic content identified should be further analyzed for specific events and undergo external validation to determine the quality and value of the findings to historians specializing in 19th century Britain.",
        "id":253762078
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which neural theorem proving paper first attempted to prove theorems in a block-by-block manner?",
    "positive_ctxs":[
      {
        "title":"LEGO-PROVER: NEURAL THEOREM PROVING WITH GROWING LIBRARIES",
        "text":"Despite the success of large language models (LLMs), the task of theorem proving still remains one of the hardest reasoning tasks that is far from being fully solved.Prior methods using language models have demonstrated promising results, but they still struggle to prove even middle school level theorems.One common limitation of these methods is that they assume a fixed theorem library during the whole theorem proving process.However, as we all know, creating new useful theorems or even new theories is not only helpful but crucial and necessary for advancing mathematics and proving harder and deeper results.In this work, we present LEGO-Prover, which employs a growing skill library containing verified lemmas as skills to augment the capability of LLMs used in theorem proving.By constructing the proof modularly, LEGO-Prover enables LLMs to utilize existing skills retrieved from the library and to create new skills during the proving process.These skills are further evolved (by prompting an LLM) to enrich the library on another scale.Modular and reusable skills are constantly added to the library to enable tackling increasingly intricate mathematical problems.Moreover, the learned library further bridges the gap between human proofs and formal proofs by making it easier to impute missing steps.LEGO-Prover advances the stateof-the-art pass rate on miniF2F-valid (48.0% to 57.0%) and miniF2F-test (45.5% to 50.0%).During the proving process, LEGO-Prover also manages to generate over 20,000 skills (theorems\/lemmas) and adds them to the growing library.Our ablation study indicates that these newly added skills are indeed helpful for proving theorems, resulting in an improvement from a success rate of 47.1% to 50.4%.We also release our code and all the generated skills. 1",
        "id":263334074
      }
    ],
    "negative_ctxs":[
      {
        "title":"An Architecture for Anaphora Resolution",
        "text":"In this paper, we describe the pronominal anaphora resolution module of Lucy, a portable English understanding system. The design of this mo;clule was motivated by the observation that, although there exist many theories of anaphora resolution, no one of these theories is complete. Thus we have implemented a blackboard-like architecture in which individual partial theories can be encoded as separate modules that can interact to propose candidate antecedents and to evaluate each other's proposals.",
        "id":5527143
      },
      {
        "title":"An Empirical Study on Neural Keyphrase Generation",
        "text":"Recent years have seen a flourishing of neural keyphrase generation (KPG) works, including the release of several large-scale datasets and a host of new models to tackle them. Model performance on KPG tasks has increased significantly with evolving deep learning research. However, there lacks a comprehensive comparison among different model designs, and a thorough investigation on related factors that may affect a KPG system's generalization performance. In this empirical study, we aim to fill this gap by providing extensive experimental results and analyzing the most crucial factors impacting the generalizability of KPG models. We hope this study can help clarify some of the uncertainties surrounding the KPG task and facilitate future research on this topic.",
        "id":221836061
      },
      {
        "title":"HW-TSC at SemEval-2023 Task 7: Exploring the Natural Language Inference Capabilities of ChatGPT and Pre-trained Language Model for Clinical Trial",
        "text":"In this paper, we describe a effective system for SemEval-2022 Task 7. This task aims to determine whether a given statement is supported by comparing one or two clinical trial reports, and to identify evidence that supports the statement. This is a task that requires high natural language inference capabilities. In Subtask 1, we compare our strategy based on prompt learning and ChatGPT with a baseline constructed using BERT in zero-shot setting, and validate the effectiveness of our strategy. In Subtask 2, we fine-tune DeBERTaV3 for classification without relying on the results from Subtask 1. We find that early stopping of the training can effectively prevent model overfitting, and this achieves a good performance in Subtask 2. In addition, we do not use any ensemble strategies. We have achieved the 10th place in Subtask 1 and the 2nd place in Subtask 2.",
        "id":259376544
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which paper considers both weights and activations when pruning large language models?",
    "positive_ctxs":[
      {
        "title":"A SIMPLE AND EFFECTIVE PRUNING APPROACH FOR LARGE LANGUAGE MODELS",
        "text":"As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance.Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive.In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs.Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis.Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is.We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks.Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update.Code is available at https:\/\/github.com\/locuslab\/wanda.",
        "id":259203115
      }
    ],
    "negative_ctxs":[
      {
        "title":"COM2SENSE: A Commonsense Reasoning Benchmark with Complementary Sentences",
        "text":"Commonsense reasoning is intuitive for humans but has been a long-term challenge for artificial intelligence (AI). Recent advancements in pretrained language models have shown promising results on several commonsense benchmark datasets. However, the reliability and comprehensiveness of these benchmarks towards assessing model's commonsense reasoning ability remains unclear. To this end, we introduce a new commonsense reasoning benchmark dataset comprising natural language true\/false statements, with each sample paired with its complementary counterpart, resulting in 4k sentence pairs. We propose a pairwise accuracy metric to reliably measure an agent's ability to perform commonsense reasoning over a given situation. The dataset is crowdsourced and enhanced with an adversarial model-in-the-loop setup to incentivize challenging samples. To facilitate a systematic analysis of commonsense capabilities, we design our dataset along the dimensions of knowledge domains, reasoning scenarios and numeracy. Experimental results demonstrate that our strongest baseline (UnifiedQA-3B), after fine-tuning, achieves 71% standard accuracy and 51% pairwise accuracy, well below human performance ( 95% for both metrics).",
        "id":235293697
      },
      {
        "title":"",
        "text":"Cet article présente les premières expériences sur le français d'identification automatique des relations discursives implicites (i.e., non marquées par un connecteur). Nos systèmes exploitent des exemples implicites annotés, ainsi que des exemples implicites artificiels obtenus à partir d'exemples explicites par suppression du connecteur, une méthode introduite parMarcu et Echihabi (2002). Les précédentes études sur l'anglais montrent que l'utilisation à l'entraînement des données artificielles dégrade largement les performances sur les données naturelles, ce qui reflète des différences importantes en termes de distribution. Ce constat, qui tient aussi pour le français, nous a amenés à envisager différentes méthodes, inspirées de l'adaptation de domaine, visant à combiner plus efficacement les données. Nous évaluons ces méthodes sur le corpus ANNODIS : notre meilleur système obtient 41,7 % d'exactitude, soit un gain significatif de 4,4 % par rapport à un modèle n'utilisant que les données naturelles.ABSTRACT. This paper presents the first experiments on French in automatic identification of implicit discourse relations (i.e. relations that lack an overt connective). Our systems exploit hand-labeled implicit examples, along with artificial implicit examples obtained from explicit examples by suppressing their connective, followingMarcu et Echihabi (2002). Previous work on English shows that using artificial data for training largely degrades performance on natural data, reflecting important differences in the distribution. This conclusion, that also holds for French, has led us to consider various methods inspired by domain adaptation to better combine the data. We evaluate these methods on the ANNODIS corpus: our best system achieves a 41.7 % accuracy, that is a significant gain of 4.4 % compared to a model using only the natural data. MOTS-CLÉS : structure discursive, relations discursives implicites, apprentissage automatique. KEYWORDS: discourse structure, implicit discourse relations, machine learning.TAL. Volume 55 -n • 1\/2014, pages 135 à 165 4. Des détails sur cette méthode sont donnés en section 3.",
        "id":21403075
      },
      {
        "title":"Multi-level Gated Recurrent Neural Network for Dialog Act Classification",
        "text":"In this paper we focus on the problem of dialog act (DA) labelling. This problem has recently attracted a lot of attention as it is an important sub-part of an automatic dialog model, which is currently in great demand. Traditional methods tend to see this problem as a sequence labelling task and deal with it by applying classifiers with rich features. Most of the current neural network models still omit the sequential information in the conversation. Henceforth, we apply a novel multi-level gated recurrent neural network (GRNN) with non-textual information to predict the DA tag. Our model not only utilizes textual information, but also makes use of non-textual and contextual information. In comparison, our model has shown significant improvement over previous works on the Switchboard Dialog Act (SWDA) data by over 6%.This work is licenced under a Creative Commons Attribution 4.0 International License. License details: http:\/\/ creativecommons.org\/licenses\/by\/4.0\/",
        "id":15118981
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which paper contains quantitative results demonstrating taking VQ tokens as inputs is inferior to pixel images for dense recognition tasks?",
    "positive_ctxs":[
      {
        "title":"ADDP: Learning General Representations for Image Recognition and Generation with Alternating Denoising Diffusion Process",
        "text":"Image recognition and generation have long been developed independently of each other. With the recent trend towards general-purpose representation learning, the development of general representations for both recognition and generation tasks is also promoted. However, preliminary attempts mainly focus on generation performance, but are still inferior on recognition tasks. These methods are modeled in the vector-quantized (VQ) space, whereas leading recognition methods use pixels as inputs. Our key insights are twofold: (1) pixels as inputs are crucial for recognition tasks; (2) VQ tokens as reconstruction targets are beneficial for generation tasks. These observations motivate us to propose an Alternating Denoising Diffusion Process (ADDP) that integrates these two spaces within a single representation learning framework. In each denoising step, our method first decodes pixels from previous VQ tokens, then generates new VQ tokens from the decoded pixels. The diffusion process gradually masks out a portion of VQ tokens to construct the training samples. The learned representations can be used to generate diverse high-fidelity images and also demonstrate excellent transfer performance on recognition tasks. Extensive experiments show that our method achieves competitive performance on unconditional generation, ImageNet classification, COCO detection, and ADE20k segmentation. Importantly, our method represents the first successful development of general representations applicable to both generation and dense recognition tasks. Code shall be released.",
        "id":259108646
      }
    ],
    "negative_ctxs":[
      {
        "title":"MLMLM: Link Prediction with Mean Likelihood Masked Language Model",
        "text":"Knowledge Bases (KBs) are easy to query, verifiable, and interpretable. They however scale with man-hours and high-quality data. Masked Language Models (MLMs), such as BERT, scale with computing power as well as unstructured raw text data. The knowledge contained within those models is however not directly interpretable. We propose to perform link prediction with MLMs to address both the KBs scalability issues and the MLMs interpretability issues. To do that we introduce MLMLM, Mean Likelihood Masked Language Model, an approach comparing the mean likelihood of generating the different entities to perform link prediction in a tractable manner. We obtain State of the Art (SotA) results on the WN18RR dataset and the best nonentity-embedding based results on the FB15k-237 dataset. We also obtain convincing results on link prediction on previously unseen entities, making MLMLM a suitable approach to introducing new entities to a KB.",
        "id":221703752
      },
      {
        "title":"Are Pre-trained Convolutions Better than Pre-trained Transformers?",
        "text":"In the era of pre-trained language models, Transformers are the de facto choice of model architectures.While recent research has shown promise in entirely convolutional, or CNN, architectures, they have not been explored using the pre-train-fine-tune paradigm. In the context of language models, are convolutional models competitive to Transformers when pre-trained? This paper investigates this research question and presents several interesting findings. Across an extensive set of experiments on 8 datasets\/tasks, we find that CNN-based pre-trained models are competitive and outperform their Transformer counterpart in certain scenarios, albeit with caveats. Overall, the findings outlined in this paper suggest that conflating pre-training and architectural advances is misguided and that both advances should be considered independently. We believe our research paves the way for a healthy amount of optimism in alternative architectures.",
        "id":236460291
      },
      {
        "title":"Riemannian Optimization for Skip-Gram Negative Sampling",
        "text":"Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in \"word2vec\" software, is usually optimized by stochastic gradient descent. However, the optimization of SGNS objective can be viewed as a problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.",
        "id":27132307
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which paper examined the scalability of instruction-tuning with respect to Mixture of Expert models?",
    "positive_ctxs":[
      {
        "title":"Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models",
        "text":"Sparse Mixture-of-Experts (MoE) is a neural architecture design that can be utilized to add learnable parameters to Large Language Models (LLMs) without increasing inference cost. Instruction tuning is a technique for training LLMs to follow instructions. We advocate combining these two approaches, as we find that MoE models benefit more from instruction tuning than dense models. In particular, we conduct empirical studies across three experimental setups: (i) Direct finetuning on individual downstream tasks devoid of instruction tuning; (ii) Instruction tuning followed by in-context few-shot or zero-shot generalization on downstream tasks; and (iii) Instruction tuning supplemented by further finetuning on individual downstream tasks. In the first scenario, MoE models overall underperform dense models of identical computational capacity. This narrative, however, dramatically changes with the introduction of instruction tuning (second and third scenario), used independently or in conjunction with task-specific finetuning. Our most powerful model, FLAN-MOE 32B , surpasses the performance of FLAN-PALM 62B on four benchmark tasks, while using only a third of the FLOPs. The advancements embodied by FLAN-MOE inspire a reevaluation of the design principles of large-scale, high-performance language models in the framework of task-agnostic learning. * Work done at Google Preprint. Under review. arXiv:2305.14705v2 [cs.CL] 5 Jul 2023 2.2 Instruction Fine-tuning RecipeWe fine-tune FLAN-MOE using the prefix language model objective on the FLAN collective dataset[4,28]. Each FLAN-MOE will inherit the auxiliary loss setting during pre-training. All the model parameters will be updated. We adapt the sequence length of each FLAN-MOE to 2, 048 for input and 512 for output based on the relative position embedding. The dropout rate is 0.05 and the expert dropout rate is 0.2. The learning rate is 1e −4 . The optimizer setting follows [4].ExperimentWe study FLAN-MOE in the context of instruction-tuning. We first perform a controlled comparison of FLAN-MOE to an equivalent \"standard\" dense encoder-decoder Transformer (T5), across a range of model sizes in Section 3.2. We subsequently demonstrate in Section 3.3 that scaling up our model, referred to as FLAN-MOE, can attain remarkable performance levels. Our most extensive model, FLAN-ST 32B , surpasses the performance of FLAN-PALM 62B while utilizing less than 30% of FLOPs per token. We further ablate the various design decisions in the next Section. 3.1 Settings Traning Data. By default, all models are trained on the 1,836 finetuning tasks by combining four mixtures from prior work: Muffin, T0-SF, NIV2, and CoT, as in [4]. Specifically, Muffin comprises 80 tasks from [52] and 26 dialog\/program synthesis tasks; T0-SF comprises 193 tasks from [44]; NIV2 comprises 1554 tasks from [51]; CoT comprises 9 reasoning tasks.Evaluations. We conduct both zero-shot and few-shot evaluations on held-out tasks as in [4] which were not included as part of the finetuning data. We use MMLU [16] that includes exam questions from 57 tasks such as mathematics, history, law, and medicine; BBH includes 23 challenging",
        "id":259342096
      }
    ],
    "negative_ctxs":[
      {
        "title":"Explorations in the Speakers' Interaction Experience and Self-assessments",
        "text":"The paper focuses on the interlocutors' self-evaluation in Finnish and Estonian first encounter dialogues. It studies affective and emotive impressions of the participants after they have met the partner for the first time, and presents comparison of the evaluation along the gender, age and education parameters. The results bring forward some statistically significant differences between the two groups, and point to different, culturally determined evaluation scales. The paper discusses the impact of the findings on the complex issues related to the evaluation of automatic interactive systems, and carries over to such applications as intelligent training and tutoring systems, and interactions with robots, encouraging further studies on the interlocutors' engagement in interaction and their evaluation of the success of the interaction.Kõnelejate suhtluskogemuse ja enesehinnangute uuringudKOKKUVÕTEArtikkel keskendub vestluskaaslaste enesehinnangutele esmakohtumisel peetud dialoogides soome ja eesti keeles. Uuritakse osalejate afektiivseid ja emotiivseid muljeid pärast seda, kui nad on kohtunud partneriga esmakordselt, ja esitatakse hinnangute võrdlus soo, vanuse ja hariduse parameetrite alusel. Tulemused toovad esile statistiliselt olulised erinevused kahe rühma vahel ja viitavad erinevatele, kultuuriliselt determineeritud hinnanguskaaladele. Artikkel analüüsib nende tulemuste mõju keerulistele probleemidele, mis on seotud automaatsete interaktiivsete süsteemide evalveerimisega, ja arendab edasi selliseid rakendusi nagu intelligentsed treenimis-ja õpetamissüsteemid ning suhtlus robotitega, pannes aluse edasistele uuringutele suhtlejate vestlusesse lülitumise ja vestluse edukuse hindamise kohta.VÕTMESÕNAD : dialoog, vestlusesse lülitumine, enesehinnang, kultuuridevaheline evalveerimine",
        "id":16628419
      },
      {
        "title":"Discovering the Unknown Knowns: Turning Implicit Knowledge in the Dataset into Explicit Training Examples for Visual Question Answering",
        "text":"Visual question answering (VQA) is challenging not only because the model has to handle multi-modal information, but also because it is just so hard to collect sufficient training examples -there are too many questions one can ask about an image. As a result, a VQA model trained solely on human-annotated examples could easily over-fit specific question styles or image contents that are being asked, leaving the model largely ignorant about the sheer diversity of questions. Existing methods address this issue primarily by introducing an auxiliary task such as visual grounding, cycle consistency, or debiasing. In this paper, we take a drastically different approach. We found that many of the \"unknowns\" to the learned VQA model are indeed \"known\" in the dataset implicitly. For instance, questions asking about the same object in different images are likely paraphrases; the number of detected or annotated objects in an image already provides the answer to the \"how many\" question, even if the question has not been annotated for that image. Building upon these insights, we present a simple data augmentation pipeline SIMPLEAUG to turn this \"known\" knowledge into training examples for VQA. We show that these augmented examples can notably improve the learned VQA models' performance, not only on the VQA-CP dataset with language prior shifts but also on the VQA v2 dataset without such shifts. Our method further opens up the door to leverage weakly-labeled or unlabeled images in a principled way to enhance VQA models. Our code and data are publicly available at https:\/\/github.com\/ heendung\/simpleAUG.",
        "id":237490881
      },
      {
        "title":"the The Transfer Phase of Mu Machine",
        "text":"Sys t (.~mSurface ~rker Coluan=B apanese ] Deep English rfaee Harker| Case Relation Surface Harker J Column-(: Japanese ] Translation Part~of-Hodifler [ Equivalent",
        "id":2008605
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which paper first applied the chain of thought concepts in 3D localization problem?",
    "positive_ctxs":[
      {
        "title":"COT3DREF: CHAIN-OF-THOUGHTS DATA-EFFICIENT 3D VISUAL GROUNDING",
        "text":"3D visual grounding is the ability to localize objects in 3D scenes conditioned by utterances. Most existing methods devote the referring head to localize the referred object directly, causing failure in complex scenarios. In addition, it does not illustrate how and why the network reaches the final decision. In this paper, we address this question \"Can we design an interpretable 3D visual grounding framework that has the potential to mimic the human perception system?\". To this end, we formulate the 3D visual grounding problem as a sequence-to-sequence (Seq2Seq) task by first predicting a chain of anchors and then the final target. Interpretability not only improves the overall performance but also helps us identify failure cases. Following the chain of thoughts approach enables us to decompose the referring task into interpretable intermediate steps, boosting the performance and making our framework extremely data-efficient. Moreover, our proposed framework can be easily integrated into any existing architecture. We validate our approach through comprehensive experiments on the Nr3D, Sr3D, and Scanrefer benchmarks and show consistent performance gains compared to existing methods without requiring manually annotated data. Furthermore, our proposed framework, dubbed CoT3DRef, is significantly data-efficient, whereas on the Sr3D dataset, when trained only on 10% of the data, we match the SOTA performance that trained on the entire data.",
        "id":263829872
      }
    ],
    "negative_ctxs":[
      {
        "title":"Obtaining referential word meanings from visual and distributional information: Experiments on object naming",
        "text":"We investigate object naming, which is an important sub-task of referring expression generation on real-world images. As opposed to mutually exclusive labels used in object recognition, object names are more flexible, subject to communicative preferences and semantically related to each other. Therefore, we investigate models of referential word meaning that link visual to lexical information which we assume to be given through distributional word embeddings. We present a model that learns individual predictors for object names that link visual and distributional aspects of word meaning during training. We show that this is particularly beneficial for zero-shot learning, as compared to projecting visual objects directly into the distributional space. In a standard object naming task, we find that different ways of combining lexical and visual information achieve very similar performance, though experiments on model combination suggest that they capture complementary aspects of referential meaning.",
        "id":7143588
      },
      {
        "title":"Transfer Learning for Context-Aware Question Matching in Information-seeking Conversations in E-commerce",
        "text":"Building multi-turn information-seeking conversation systems is an important and challenging research topic. Although several advanced neural text matching models have been proposed for this task, they are generally not efficient for industrial applications. Furthermore, they rely on a large amount of labeled data, which may not be available in real-world applications. To alleviate these problems, we study transfer learning for multi-turn information seeking conversations in this paper. We first propose an efficient and effective multiturn conversation model based on convolutional neural networks. After that, we extend our model to adapt the knowledge learned from a resource-rich domain to enhance the performance. Finally, we deployed our model in an industrial chatbot called AliMe Assist 1 and observed a significant improvement over the existing online model.",
        "id":49210906
      },
      {
        "title":"Creating a Domain-diverse Corpus for Theory-based Argument Quality Assessment",
        "text":"Computational models of argument quality (AQ) have focused primarily on assessing the overall quality or just one specific characteristic of an argument, such as its convincingness or its clarity. However, previous work has claimed that assessment based on theoretical dimensions of argumentation could benefit writers, but developing such models has been limited by the lack of annotated data. In this work, we describe GAQCorpus, the first large, domain-diverse annotated corpus of theory-based AQ. We discuss how we designed the annotation task to reliably collect a large number of judgments with crowdsourcing, formulating theory-based guidelines that helped make subjective judgments of AQ more objective. We demonstrate how to identify arguments and adapt the annotation task for three diverse domains. Our work will inform research on theory-based argumentation annotation and enable the creation of more diverse corpora to support computational AQ assessment. * Equal contribution. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:\/\/ creativecommons.org\/licenses\/by\/4.0\/. 1 In the following, we adopt the term \"theory-based AQ,\" which was proposed byWachsmuth et al. (2017b)to indicate that the conception of AQ is specifically grounded in argumentation theoretic literature (and not in CL or NLP).",
        "id":226237159
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which paper first derived online occupany estimation technique to get sqrt(T) bound for reinforcement learning in adversarial linear MDP?",
    "positive_ctxs":[
      {
        "title":"Towards Optimal Regret in Adversarial Linear MDPs with Bandit Feedback",
        "text":"We study online reinforcement learning in linear Markov decision processes with adversarial losses and bandit feedback, without prior knowledge on transitions or access to simulators.We introduce two algorithms that achieve improved regret performance compared to existing approaches.The first algorithm, although computationally inefficient, ensures a regret of O( √ K), where K is the number of episodes.This is the first result with the optimal K dependence in the considered setting.The second algorithm, which is based on the policy optimization framework, guarantees a regret of O(K 3 \/4 ) and is computationally efficient.Both our results significantly improve over the state-of-the-art: a computationally inefficient algorithm by Kong et al. [2023]  with O(K 4 \/5 + poly( 1 \/λmin)) regret, for some problem-dependent constant λ min that can be arbitrarily close to zero, and a computationally efficient algorithm by Sherman et al. [2023b]  with O(K 6 \/7 ) regret.* The authors are listed in alphabetical order.is the horizon length.The challenge is that this conversion depends on the transition of the MDP, which is not available to the learner.Therefore, the learner has to estimate the feature of every policy during the learning process.Previous work in this direction [Kong et al., 2023]  faced obstacles in controlling the estimation error and was only able to show a K 4 \/5 +poly( 1 \/λ min ) regret bound assuming there exists an exploratory policy inducing a covariance matrix λ min I.We addressed the obstacles through 1) state space discretization (Section 3.2), and 2) model-free estimation for the occupancy measure of policies over the discretized state space (Section 3.3).These allow us to emulate the success in the tabular case [Jin et al., 2020a] and obtain the tight √ K regret.Efficient K 3 \/4 algorithm.The efficient algorithm is based on the policy optimization framework[Luo et al., 2021].Different from previous works that all use exponential weights, we use Follow-the-Regularized-Leader (FTRL) with log-determinant (logdet) barrier regularizer to perform policy updates, which has the benefit of keeping the algorithm more stable [Zimmert and Lattimore, 2022, Liu et al., 2023a].We carefully combine logdet-FTRL with existing algorithmic\/analysis techniques to further improve the regret bound.These include 1) an initial exploration phase to control the transition estimation error [Sherman et al., 2023a], 2) optimistic least-square policy evaluation in bonus construction [Sherman et al., 2023b], 3) dilated bonus construction[Luo et al., 2021], and 4) a tighter concentration bound for covariance matrix estimation [Liu et al., 2023a].Related WorkIn this subsection, we review prior works on adversarial MDPs and policy optimization.Learning in Adversarial MDPs.Adversarial MDPs refer to a class of MDP problems where the transition is fixed while the loss function changes over time.Learning adversarial tabular MDPs under bandit feedback and unknown transition has been extensively studied [",
        "id":264288929
      }
    ],
    "negative_ctxs":[
      {
        "title":"A Latent Topic Extracting Method based on Events in a Document and its Application",
        "text":"Recently, several latent topic analysis methods such as LSI, pLSI, and LDA have been widely used for text analysis. However, those methods basically assign topics to words, but do not account for the events in a document. With this background, in this paper, we propose a latent topic extracting method which assigns topics to events. We also show that our proposed method is useful to generate a document summary based on a latent topic.",
        "id":15648889
      },
      {
        "title":"Chapter Captor: Text Segmentation in Novels",
        "text":"Books are typically segmented into chapters and sections, representing coherent subnarratives and topics. We investigate the task of predicting chapter boundaries, as a proxy for the general task of segmenting long texts. We build a Project Gutenberg chapter segmentation data set of 9,126 English novels, using a hybrid approach combining neural inference and rule matching to recognize chapter title headers in books, achieving an F1-score of 0.77 on this task. Using this annotated data as ground truth after removing structural cues, we present cut-based and neural methods for chapter segmentation, achieving an F1-score of 0.453 on the challenging task of exact break prediction over book-length documents. Finally, we reveal interesting historical trends in the chapter structure of novels.",
        "id":226262395
      },
      {
        "title":"Cross-linguistic annotation of narrativity for English\/French verb tense disambiguation",
        "text":"This paper presents manual and automatic annotation experiments for a pragmatic verb tense feature (narrativity) in English\/French parallel corpora. The feature is considered to play an important role for translating English Simple Past tense into French, where three different tenses are available. Whether the French Passé Composé, Passé Simple or Imparfait should be used is highly dependent on a longer-range context, in which either narrative events ordered in time or mere non-narrative state of affairs in the past are described. This longer-range context is usually not available to current machine translation (MT) systems, that are trained on parallel corpora. Annotating narrativity prior to translation is therefore likely to help current MT systems. Our experiments show that narrativity can be reliably identified with kappa-values of up to 0.91 in manual annotation and with F1 scores of up to 0.72 in automatic annotation.",
        "id":8200466
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which paper first found that REINFORCE works better than actor critic algorithms like PPO for RL finetuning of pretrained chemistry language models (Transformers and RNNs)?",
    "positive_ctxs":[
      {
        "title":"Searching for High-Value Molecules Using Reinforcement Learning and Transformers",
        "text":"Reinforcement learning (RL) over text representations can be effective for finding high-value policies that can search over graphs.However, RL requires careful structuring of the search space and algorithm design to be effective in this challenge.Through extensive experiments, we explore how different design choices for text grammar and algorithmic choices for training can affect an RL policy's ability to generate molecules with desired properties.We arrive at a new RL-based molecular design algorithm (ChemRLformer) and perform a thorough analysis using 25 molecule design tasks, including computationally complex protein docking simulations.From this analysis, we discover unique insights in this problem space and show that ChemRLformer achieves state-of-the-art performance while being more straightforward than prior work by demystifying which design choices are actually helpful for text-based molecule design.",
        "id":263620293
      }
    ],
    "negative_ctxs":[
      {
        "title":"Voting between Dictionary-based and Subword Tagging Models for Chinese Word Segmentation",
        "text":"This paper describes a Chinese word segmentation system that is based on majority voting among three models: a forward maximum matching model, a conditional random field (CRF) model using maximum subword-based tagging, and a CRF model using minimum subwordbased tagging. In addition, it contains a post-processing component to deal with inconsistencies. Testing on the closed track of CityU, MSRA and UPUC corpora in the third SIGHAN Chinese Word Segmentation Bakeoff, the system achieves a F-score of 0.961, 0.953 and 0.919, respectively.",
        "id":14280106
      },
      {
        "title":"Time-Dependent Representation for Neural Event Sequence Prediction",
        "text":"Existing sequence prediction methods are mostly concerned with time-independent sequences, in which the actual time span between events is irrelevant and the distance between events is simply the difference between their order positions in the sequence. While this time-independent view of sequences is applicable for data such as natural languages, e.g., dealing with words in a sentence, it is inappropriate and inefficient for many real world events that are observed and collected at unequally spaced points of time as they naturally arise, e.g., when a person goes to a grocery store or makes a phone call. The time span between events can carry important information about the sequence dependence of human behaviors. To leverage continuous time in sequence prediction, we propose two methods for integrating time into event representation, based on the intuition on how time is tokenized in everyday life and previous work on embedding contextualization. We particularly focus on using these methods in recurrent neural networks, which have gained popularity in many sequence prediction tasks. We evaluated these methods as well as baseline models on two learning tasks: mobile app usage prediction and music recommendation. The experiments revealed that the proposed methods for time-dependent representation offer consistent gain on accuracy compared to baseline models that either directly use continuous time value in a recurrent neural network or do not use time.",
        "id":13019454
      },
      {
        "title":"Resource Report: Building Parallel Text Corpora for Multi-Domain Translation System",
        "text":"Parallel text is one of the most valuable resources for development of statistical machine translation systems and other NLP applications. However, manual translations are very costly, and the number of known parallel text is limited. Hence, our research started with creating and collecting a large amount of parallel text resources for Indonesian-English. We describe in this paper the creation of parallel corpora: ANTARA News, BPPT-PANL and BTEC-ATR. In order to be useful, these resources must be available in reasonable quantities and qualities to be useful for statistical approaches to language processing. We describe problem and solution as well robust tools and annotation schema to build and process these corpora.",
        "id":31188530
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which paper first found that when transformers are trained to in-context learn function classes, they might exhibit generalization followed by memorization, in certain settings?",
    "positive_ctxs":[
      {
        "title":"In-Context Learning through the Bayesian Prism",
        "text":"In-context learning is one of the surprising and useful features of large language models. How it works is an active area of research. Recently, stylized meta-learning-like setups have been devised that train these models on a sequence of input-output pairs (x, f (x)) from a function class using the language modeling loss and observe generalization to unseen functions from the same class. One of the main discoveries in this line of research has been that for several problems such as linear regression, trained transformers learn algorithms for learning functions in context. However, the inductive biases of these models resulting in this behavior are not clearly understood. A model with unlimited training data and compute is a Bayesian predictor: it learns the pretraining distribution. It has been shown that high-capacity transformers mimic the Bayesian predictor for linear regression. In this paper, we show empirical evidence of transformers exhibiting the behavior of this ideal learner across different linear and non-linear function classes. We also extend the previous setups to work in the multitask setting and verify that transformers can do in-context learning in this setup as well and the Bayesian perspective sheds light on this setting also. Finally, via the example of learning Fourier series, we study the inductive bias for in-context learning. We find that in-context learning may or may not have simplicity bias depending on the pretraining data distribution. * Equal Contribution arXiv:2306.04891v1 [cs.LG] 8 Jun 2023 w ∼ N (0 d , I). We are particularly interested in the underdetermined region i.e. k < d. Gaussian prior enables explicit PME computation: both PME and maximum a posteriori (MAP) solution agree and are equal to the minimum L 2 -norm solution of the equations forming the training examples i.e. min w ∥w∥ 2 s.t. w T x i = f (x i ), ∀i ≤ k Standard Ordinary Least Squares (OLS) solvers return the minimum L 2 -norm solution, and thus PME and MAP too, in the underdetermined region i.e. k < d. Skewed-Covariance Regression (F Skew-DR ). This setup is similar to dense-regression, except we assume the following prior on weight vector: w ∼ N (0, Σ), where Σ ∈ R d×d is the covariance matrix with eigenvalues proportional to 1\/i 2 , where i ∈ [1, d]. For this prior on w, we can use the same (but more general) argument for dense regression above to obtain the PME and MAP which will be equal and can be obtained by minimizing w T Σ −1 w w.r.t to the constraints w T x i = f (x i ). This setup was motivated by Garg et al. [2022], where it was used to sample x i values for out-of-distribution (OOD) evaluation, but not as a prior on w. Sparse Regression (F SR ). In sparse regression, we assume w to be an s-sparse vector in R d i.e. out of its d components only s are non-zero. Following Garg et al. [2022], to sample w for constructing prompts P, we first sample w ∼ N (0 d , I) and then randomly set its d − s components as 0. We consider s = 3 throughout our experiments. While computing the PME appears to be intractable here, the MAP solution can be estimated using Lasso by assuming a Laplacian prior on w Tibshirani [1996].Sign-Vector Regression (F SVR ).Here, we assume w to be a sign vector in {−1, +1} d . For constructing prompts P, we sample d independent Bernoulli random variables b j with a mean of 0.5 and obtain w = [2b 1 − 1, · · · , 2b d − 1] T . While computing the exact PME in this case as well remains intractable, the optimal solution for k > d\/2 can be obtained by minimizing the L ∞ norm ∥w∥ ∞ w.r.t. the constraints specified by the input-output examples (w T x i = f (x i )) Mangasarian and Recht [2011]. A specific variation. In general, for the exact recovery of a vector w, the set of all these vectors must satisfy specific convexity conditions Chandrasekaran et al. [2012]. We question if Transformers also require such conditions. To test the same, we define a task F ZR where the convexity conditions are not met and train transformers for regression on this task. Here, w ∈ {z; z | z ∈ {−2, −1, 1, 2} d\/2 }, where ; denotes concatenation. Note that the size of this set is 2 d , the same as the size of {−1, 1} d .Low-Rank Regression (F LowRank-DR ).In this case, w is assumed to be a flattened version of a matrix W ∈ R q×q (d = q 2 ) with a rank r, where r ≪ q. A strong baseline, in this case, is to minimize the nuclear norm L * of W i.e. ∥W∥ * subject to constraints w T x i = f (x i ). To sample the rank-r matrix W, we sample A ∼ N (0, 1), s.t. A ∈ R q×r and independently a matrix B of the same shape and distribution, and set W = AB T . Recovery bounds. For each function class above, there is a bound on the minimum number of in-context examples needed for the exact recovery of the solution vector w. The bounds for sparse, sign-vector and low-rank regression are 2s log(d\/s) + 5s\/4, d\/2, and 3r(2q − r) respectively Chandrasekaran et al. [2012].",
        "id":259108565
      }
    ],
    "negative_ctxs":[
      {
        "title":"Identification of Coreference Between Names and Faces",
        "text":"To retrieve multimedia contents by their meaning, it is necessary to use not only the contents of distinct media, such as image or language, but also a certain semantic relation holding between them. For this purpose, in this paper, we propose a method to find coreferences between human names in the article of newspaper and human faces in the accompanying photograph. The method we proposed is based on the machine learning and the hypothesis driven combining method for identifying names and corresponding faces. Our experimental results show that the recall and precision rate of our method are better than those of the system which uses information exclusively from either text media or image media.",
        "id":14600913
      },
      {
        "title":"Attentive Mimicking: Better Word Embeddings by Attending to Informative Contexts",
        "text":"Learning high-quality embeddings for rare words is a hard problem because of sparse context information. Mimicking (Pinter et al.,  2017)  has been proposed as a solution: given embeddings learned by a standard algorithm, a model is first trained to reproduce embeddings of frequent words from their surface form and then used to compute embeddings for rare words. In this paper, we introduce attentive mimicking: the mimicking model is given access not only to a word's surface form, but also to all available contexts and learns to attend to the most informative and reliable contexts for computing an embedding. In an evaluation on four tasks, we show that attentive mimicking outperforms previous work for both rare and medium-frequency words. Thus, compared to previous work, attentive mimicking improves embeddings for a much larger part of the vocabulary, including the mediumfrequency range.",
        "id":102350748
      },
      {
        "title":"Coling 2008: Companion volume -Posters and Demonstrations",
        "text":"Treating classification as seeking minimum cuts in the appropriate graph has proven effective in a number of applications. The power of this approach lies in its ability to incorporate label-agreement preferences among pairs of instances in a provably tractable way. Label disagreement preferences are another potentially rich source of information, but prior NLP work within the minimum-cut paradigm has not explicitly incorporated it. Here, we report on work in progress that examines several novel heuristics for incorporating such information. Our results, produced within the context of a politically-oriented sentiment-classification task, demonstrate that these heuristics allow for the addition of label-disagreement information in a way that improves classification accuracy while preserving the efficiency guarantees of the minimum-cut framework.",
        "id":1932999
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which paper first investigates the knowledge preferences of LLMs when there are conflicts between the context and the parametric memory?",
    "positive_ctxs":[
      {
        "title":"Adaptive Chameleon or Stubborn Sloth: REVEALING THE BEHAVIOR OF LARGE LANGUAGE MODELS IN KNOWLEDGE CONFLICTS",
        "text":"By providing external information to large language models (LLMs), tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory.However, how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory?We present the first comprehensive and controlled investigation into the behavior of LLMs when encountering knowledge conflicts.We propose a systematic framework to elicit high-quality parametric memory from LLMs and construct the corresponding counter-memory, which enables us to conduct a series of controlled experiments.Our investigation reveals seemingly contradicting behaviors of LLMs.On the one hand, different from prior wisdom, we find that LLMs can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing.On the other hand, LLMs also demonstrate a strong confirmation bias when the external evidence contains some information that is consistent with their parametric memory, despite being presented with conflicting evidence at the same time.These results pose important implications that are worth careful consideration for the further development and deployment of tool-and retrieval-augmented LLMs. 1 * The first two authors contributed equally.Work done during Jian Xie's internship at OSU NLP Group.",
        "id":263610128
      }
    ],
    "negative_ctxs":[
      {
        "title":"A Dataset for Detecting Humor in Arabic Text",
        "text":"Humor detection is a complex and ambiguous task in natural language processing. This has made automatic humor detection challenging, particularly for languages with limited resources such as Arabic. In this paper, we attempt to solve this task by collecting and annotating Arabic humorous tweets in dialects and Modern Standard Arabic (MSA) text then performing automatic humor detection on the collected data. We experimented on the collected dataset by fine-tuning seven Arabic Pre-Trained language models (PLMs) which are: AraBERTv02, Arabertv02-twitter, QARIB, MarBERT, MARBERTv2, CAMeLBERT-DA, and CAMeLBERT-MIX to establish a baseline classification system. We concluded that CAMeLBERT-DA was the bestperforming model and it achieved an F1score and accuracy of 72.11%.Related workThe task of detecting humor in text has drawn the attention of many researchers in different languages. Although there is no single definition of",
        "id":256739243
      },
      {
        "title":"Double-Branch Multi-Attention based Graph Neural Network for Knowledge Graph Completion",
        "text":"Graph neural networks (GNNs), which effectively use topological structures in the knowledge graphs (KG) to embed entities and relations in low-dimensional spaces, have shown great power in knowledge graph completion (KGC). KG has abundant global and local structural information, however, many GNN-based KGC models cannot capture these two types of information about the graph structure by designing complex aggregation schemes and are not designed well to learn representations of seen entities with sparse neighborhoods in isolated subgraphs. In this paper, we find that a simple attention-based method can outperform a general GNN-based approach for KGC. We then propose a double-branch multi-attentionbased graph neural network (MA-GNN) to learn more expressive entity representations that contain rich global-local structural information. Specifically, we first explore the graph attention network-based local aggregator to learn entity representations. Furthermore, we propose a snowball local attention mechanism by leveraging the semantic similarity between two-hop neighbors to enrich the entity embedding. Finally, we use Transformer-based selfattention to learn long-range dependence between entities to obtain richer representations with the global graph structure and entity features. Experimental results on five benchmark datasets show that MA-GNN achieves significant improvements over strong baselines for inductive KGC.",
        "id":259370784
      },
      {
        "title":"Saarland: Vector-based models of semantic textual similarity",
        "text":"This paper describes our system for the Semeval 2012 Sentence Textual Similarity task. The system is based on a combination of few simple vector space-based methods for word meaning similarity. Evaluation results show that a simple combination of these unsupervised data-driven methods can be quite successful. The simple vector space components achieve high performance on short sentences; on longer, more complex sentences, they are outperformed by a surprisingly competitive word overlap baseline, but they still bring improvements over this baseline when incorporated into a mixture model.",
        "id":7098054
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which paper first proposes a unified framework for black-box and white-box detection of AI-written text with explanations?",
    "positive_ctxs":[
      {
        "title":"DNA-GPT: DIVERGENT N-GRAM ANALYSIS FOR TRAINING-FREE DETECTION OF GPT-GENERATED TEXT",
        "text":"Large language models (LLMs) have notably enhanced the fluency and diversity of machine-generated text.However, this progress also presents a significant challenge in detecting the origin of a given text, and current research on detection methods lags behind the rapid evolution of LLMs.Conventional training-based methods have limitations in flexibility, particularly when adapting to new domains, and they often lack explanatory power.To address this gap, we propose a novel training-free detection strategy called Divergent N-Gram Analysis (DNA-GPT).Given a text, we first truncate it in the middle and then use only the preceding portion as input to the LLMs to regenerate the new remaining parts.By analyzing the differences between the original and new remaining parts through N-gram analysis in black-box or probability divergence in white-box, we unveil significant discrepancies between the distribution of machine-generated text and the distribution of human-written text.We conducted extensive experiments on the most advanced LLMs from Ope-nAI, including text-davinci-003, GPT-3.5-turbo, and GPT-4, as well as open-source models such as GPT-NeoX-20B and LLaMa-13B.Results show that our zero-shot approach exhibits state-of-the-art performance in distinguishing between human and GPT-generated text on four English and one German dataset, outperforming OpenAI's own classifier, which is trained on millions of text.Additionally, our methods provide reasonable explanations and evidence to support our claim, which is a unique feature of explainable detection.Our method is also robust under the revised text attack and can additionally solve model sourcing.Codes are available at https:\/\/github.com\/Xianjun-Yang\/DNA-GPT",
        "id":258960101
      }
    ],
    "negative_ctxs":[
      {
        "title":"Does Summary Evaluation Survive Translation to Other Languages?",
        "text":"The creation of a quality summarization dataset is an expensive, time-consuming effort, requiring the production and evaluation of summaries by both trained humans and machines. The returns to such an effort would increase significantly if the dataset could be used in additional languages without repeating human annotations. To investigate how much we can trust machine translation of summarization datasets, we translate the English Sum-mEval dataset to seven languages and compare performances across automatic evaluation measures. We explore equivalence testing as the appropriate statistical paradigm for evaluating correlations between human and automated scoring of summaries. We also consider the effect of translation on the relative performance between measures. We find some potential for dataset reuse in languages similar to the source and along particular dimensions of summary quality. Our code and data can be found at https:\/\/github.com\/ PrimerAI\/primer-research\/.",
        "id":237532546
      },
      {
        "title":"Review of Entity Relation Extraction based on deep learning",
        "text":"As a core task of Information Extraction, Entity Relation Extraction plays an important role in many Natural Language Processing applications such as knowledge graph, intelligent question answering system and semantic search. Relation extraction tasks aim to find the semantic relation between a pair of entity mentions from unstructured texts. This paper focuses on the sentence-level relation extraction, introduces the main datasets for this task, and expounds the current status of relation extraction technology which can be divided into: supervised relation extraction, distant supervision relation extraction and joint extraction of entities and relations. We compare the various models for this task and analyze their contributions and defects. Finally, the research status and methods of Chinese entity relation extraction are introduced.",
        "id":225062645
      },
      {
        "title":"The Proper Treatment of Optimality in Computational Phonology",
        "text":"This paper presents a novel formalization of optimality theory. Unlike previous treatments of optimality in computational linguistics, starting with EUison (1994), the new approach does not require any explicit marking and counting of constraint violations. It is based on the notion of \"lenient composition\", defined as the combination of ordinary composition and priority union. If an underlying form has outputs that can meet a given constraint, lenient composition enforces the constraint; if none of the output candidates meets the constraint, lenient composition allows all of them. For the sake of greater efficiency, we may \"leniently compose\" the GEN relation and all the constraints into a single finite-state transducer that maps each underlying form directly into its optimal surface realizations, and vice versa. Seen f~om this perspective, optimality theolT is surprisingly similar to the two older strains of finite-state phonology: classical rewrite systems and two-level models. In particular, the ranking of optimality constraints corresponds to the ordering of rewrite rules.",
        "id":1460
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which paper first proved that wide-enough transformer architectures trained with gradient methods on enough data would learn to solve relational reasoning tasks?",
    "positive_ctxs":[
      {
        "title":"When can transformers reason with abstract symbols?",
        "text":"We investigate the capabilities of transformer large language models (LLMs) on relational reasoning tasks involving abstract symbols.Such tasks have long been studied in the neuroscience literature as fundamental building blocks for more complex abilities in programming, mathematics, and verbal reasoning.For (i) regression tasks, we prove that transformers generalize when trained, but require astonishingly large quantities of training data.For (ii) next-token-prediction tasks with symbolic labels, we show an \"inverse scaling law\": transformers fail to generalize as their embedding dimension increases.For both settings (i) and (ii), we propose subtle transformer modifications which can reduce the amount of data needed by adding two trainable parameters per head.",
        "id":264147017
      }
    ],
    "negative_ctxs":[
      {
        "title":"Automated Generation of Test Suites for Error Analysis of Concept Recognition Systems",
        "text":"We present an architecture and implementation of a system that builds structured test suites for concept recognition systems. The system applies provided test case definitions to a target concept vocabulary, to generate test cases organised according to those definitions. Test case definitions capture particular characteristics, or produce regular transformations, of concept terms. The test suites produced by the system enable detailed, systematic, error analysis of the performance of concept recognition systems.",
        "id":11447561
      },
      {
        "title":"Summarizing Student Responses to Reflection Prompts",
        "text":"We propose to automatically summarize student responses to reflection prompts and introduce a novel summarization algorithm that differs from traditional methods in several ways. First, since the linguistic units of student inputs range from single words to multiple sentences, our summaries are created from extracted phrases rather than from sentences. Second, the phrase summarization algorithm ranks the phrases by the number of students who semantically mention a phrase in a summary. Experimental results show that the proposed phrase summarization approach achieves significantly better summarization performance on an engineering course corpus in terms of ROUGE scores when compared to other summarization methods, including MEAD, LexRank and MMR.",
        "id":5910159
      },
      {
        "title":"",
        "text":"",
        "id":193276396
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which paper first showed that task-specific knowledge embedded in parameters can be extracted from one LLM using seed samples and transferred to another via parameter-efficient fine-tuning?",
    "positive_ctxs":[
      {
        "title":"SEEKING NEURAL NUGGETS: KNOWLEDGE TRANSFER IN LARGE LANGUAGE MODELS FROM A PARAMETRIC PERSPECTIVE",
        "text":"Large Language Models (LLMs) inherently encode a wealth of knowledge within their parameters through pre-training on extensive corpora. While prior research has delved into operations on these parameters to manipulate the underlying implicit knowledge-encompassing detection, editing, and merging-there remains an ambiguous understanding regarding their transferability across models with varying scales. In this paper, we seek to empirically investigate knowledge transfer from larger to smaller models through a parametric perspective. To achieve this, we employ sensitivity-based techniques to extract and align knowledgespecific parameters between different LLMs. Moreover, the LoRA module is used as the intermediary mechanism for injecting the extracted knowledge into smaller models. Evaluations across four benchmarks validate the efficacy of our proposed method. Our findings highlight the critical factors contributing to the process of parametric knowledge transfer, underscoring the transferability of model parameters across LLMs of different scales. We release code and data at",
        "id":264172668
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":235097424
      },
      {
        "title":"A Clustering Approach for the Nearly Unsupervised Recognition of Nonliteral Language *",
        "text":"In this paper we present TroFi (Trope Finder), a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word-sense disambiguation and clustering techniques. TroFi uses sentential context instead of selectional constraint violations or paths in semantic hierarchies. It also uses literal and nonliteral seed sets acquired and cleaned without human supervision in order to bootstrap learning. We adapt a word-sense disambiguation algorithm to our task and augment it with multiple seed set learners, a voting schema, and additional features like SuperTags and extrasentential context. Detailed experiments on hand-annotated data show that our enhanced algorithm outperforms the baseline by 24.4%. Using the TroFi algorithm, we also build the TroFi Example Base, an extensible resource of annotated literal\/nonliteral examples which is freely available to the NLP research community. * This research was partially supported by NSERC, Canada (RGPIN: 264905). We would like to thank Bill Dolan, Fred Popowich, Dan Fass, Katja Markert, Yudong Liu, and the anonymous reviewers for their comments.",
        "id":11890804
      },
      {
        "title":"DBpedia Abstracts: A Large-Scale, Open, Multilingual NLP Training Corpus",
        "text":"The ever increasing importance of machine learning in Natural Language Processing is accompanied by an equally increasing need in large-scale training and evaluation corpora. Due to its size, its openness and relative quality, the Wikipedia has already been a source of such data, but on a limited scale. This paper introduces the DBpedia Abstract Corpus, a large-scale, open corpus of annotated Wikipedia texts in six languages, featuring over 11 million texts and over 97 million entity links. The properties of the Wikipedia texts are being described, as well as the corpus creation process, its format and interesting use-cases, like Named Entity Linking training and evaluation.",
        "id":29658212
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which paper first studied differential privacy for in-context learning to prevent prompt leakage attacks?",
    "positive_ctxs":[
      {
        "title":"PRIVACY-PRESERVING IN-CONTEXT LEARNING FOR LARGE LANGUAGE MODELS",
        "text":"In-context learning (ICL) is an important capability of Large Language Models (LLMs), enabling these models to dynamically adapt based on specific, in-context exemplars, thereby improving accuracy and relevance. However, LLM's responses may leak the sensitive private information contained in in-context exemplars. To address this challenge, we propose Differentially Private In-context Learning (DP-ICL), a general paradigm for privatizing ICL tasks. The key idea for DP-ICL paradigm is generating differentially private responses through a noisy consensus among an ensemble of LLM's responses based on disjoint exemplar sets. Based on the general paradigm of DP-ICL, we instantiate several techniques showing how to privatize ICL for text classification and language generation. We evaluate DP-ICL on four text classification benchmarks and two language generation tasks, and our empirical results show that DP-ICL achieves a strong utility-privacy tradeoff.",
        "id":258436870
      }
    ],
    "negative_ctxs":[
      {
        "title":"Résumé automatique de textes d'opinion Résumé automatique de textes d'opinion",
        "text":"Nous présentons dans cet article un système de résumé automatique tourné vers l'analyse de blogs, où sont exprimées à la fois des informations factuelles et des prises de position sur les faits considérés. Notre système de résumé est fondé sur une approche nouvelle qui mêle analyse de la redondance et repérage des informations nouvelles dans les textes ; ce système générique est en outre enrichi d'un module de calcul de la polarité de l'opinion véhiculée afin de traiter de façon appropriée la subjectivité qui est le propre des billets de blogs. Le système est évalué sur l'anglais, à travers la participation à la campagne d'évaluation internationale TAC (Text Analysis Conference) où notre système a obtenu des performances satisfaisantes.ABSTRACT. In this paper, we present a summarization system that is specifically designed to process blog posts, where factual information is mixed with opinions on the discussed facts. Our approach combines redundancy analysis with new information tracking and is enriched by a module that computes the polarity of textual fragments in order to summarize blog posts more efficiently. The system is evaluated against English data, especially through the participation in TAC (Text Analysis Conference), an international evaluation framework for automatic summarization, in which our system obtained interesting results. MOTS-CLÉS : résumé automatique, analyse de blogs, analyse de l'opinion, redondance, subjectivité, évaluation de résumés automatiques, évaluation de l'analyse de l'opinion.",
        "id":194963461
      },
      {
        "title":"",
        "text":"",
        "id":238638462
      },
      {
        "title":"Omorfi-Free and open source morphological lexical database for Finnish",
        "text":"This demonstration presents a freely available open source lexical database omorfi.Omorfi is a mature lexicographical database project, started out as a single-person single-purpose free open source morphological analyser project, omorfi has since grown to be used in variety of applications including spell-checking, statistical and rule-based machine translation, treebanking, joint syntactic and morphological parsing, poetry generation, information extraction. In this demonstration we hope to show both the variety of end-user facing applications as well as the tools and interfaces for computational linguists to make the best use of a developing product. We show a shallow database arrangement that has allowed a great variety of contributors from different projects to extend the lexical database while not breaking the continued use of existing end-applications. We hope to show both the best current practices for lexical data management and software engineering with regards to continuous external project integration of a constantly developing product. As case examples we show some of the integrations with following applications: Voikko spell-checking for Windows, Mac OS X, Linux and Android, statistical machine translation pipelines with moses, rule-based machine translation with apertium and traditional xerox style morphological analysis and generation. morphological segmentation, as well as application programming interfaces for python and Java.",
        "id":31131573
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which paper first study POMDP with enhanced feedback on observations?",
    "positive_ctxs":[
      {
        "title":"Sample-Efficient Learning of POMDPs with Multiple Observations In Hindsight",
        "text":"This paper studies the sample-efficiency of learning in Partially Observable Markov Decision Processes (POMDPs), a challenging problem in reinforcement learning that is known to be exponentially hard in the worst-case. Motivated by real-world settings such as loading in game playing, we propose an enhanced feedback model called \"multiple observations in hindsight\", where after each episode of interaction with the POMDP, the learner may collect multiple additional observations emitted from the encountered latent states, but may not observe the latent states themselves. We show that sample-efficient learning under this feedback model is possible for two new subclasses of POMDPs: multi-observation revealing POMDPs and distinguishable POMDPs. Both subclasses generalize and substantially relax revealing POMDPs-a widely studied subclass for which sample-efficient learning is possible under standard trajectory feedback. Notably, distinguishable POMDPs only require the emission distributions from different latent states to be different instead of linearly independent as required in revealing POMDPs. * Fudan University.",
        "id":259360601
      }
    ],
    "negative_ctxs":[
      {
        "title":"PanLex and LEXTRACT: Translating all Words of all Languages of the World",
        "text":"PanLex is a lemmatic translation resource which combines a large number of translation dictionaries and other translingual lexical resources. It currently covers 1353 language varieties and 12M expressions, but aims to cover all languages and up to 350M expressions. This paper describes the resource and current applications of it, as well as lextract, a new effort to expand the coverage of PanLex via semi-automatic dictionary scraping.",
        "id":1761416
      },
      {
        "title":"RDoC Task at BioNLP-OST 2019: A Mental Health Informatics Task with Research Domain Criteria",
        "text":"BioNLP Open Shared Tasks (BioNLP-OST) is an international competition organized to facilitate development and sharing of computational tasks of biomedical text mining and solutions to them. For BioNLP-OST 2019, we introduced a new mental health informatics task called \"RDoC Task\", which is composed of two subtasks: information retrieval and sentence extraction through National Institutes of Mental Health's Research Domain Criteria framework. Five and four teams around the world participated in the two tasks, respectively. According to the performance on the two tasks, we observe that there is room for improvement for text mining on brain research and mental illness.",
        "id":207764605
      },
      {
        "title":"Rapid Prototyping of Robust Language Understanding Modules for Spoken Dialogue Systems",
        "text":"Language understanding (LU) modules for spoken dialogue systems in the early phases of their development need to be (i) easy to construct and (ii) robust against various expressions. Conventional methods of LU are not suitable for new domains, because they take a great deal of effort to make rules or transcribe and annotate a sufficient corpus for training. In our method, the weightings of the Weighted Finite State Transducer (WFST) are designed on two levels and simpler than those for conventional WFST-based methods. Therefore, our method needs much fewer training data, which enables rapid prototyping of LU modules. We evaluated our method in two different domains. The results revealed that our method outperformed baseline methods with less than one hundred utterances as training data, which can be reasonably prepared for new domains. This shows that our method is appropriate for rapid prototyping of LU modules.",
        "id":11379865
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which paper first tried to fine-tune LLMs with chain-of-thoughts and program-of-thoughts for math reasoning?",
    "positive_ctxs":[
      {
        "title":"MAMMOTH: BUILDING MATH GENERALIST MODELS THROUGH HYBRID INSTRUCTION TUNING",
        "text":"We introduce MAmmoTH, a series of open-source large language models (LLMs) specifically tailored for general math problem-solving.The MAmmoTH models are trained on MathInstruct, our meticulously curated instruction tuning dataset.MathInstruct is compiled from 13 math datasets with intermediate rationales, six of which have rationales newly curated by us.It presents a unique hybrid of chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also ensures extensive coverage of diverse fields in math.The hybrid of CoT and PoT not only unleashes the potential of tool use but also allows different thought processes for different math problems.As a result, the MAmmoTH series substantially outperform existing open-source models on nine mathematical reasoning datasets across all scales with an average accuracy gain between 16% and 32%.Remarkably, our MAmmoTH-7B model reaches 33% on MATH (a competition-level dataset), which exceeds the best open-source 7B model (WizardMath) by 23%, and the MAmmoTH-34B model achieves 44% accuracy on MATH, even surpassing GPT-4's CoT result.Our work underscores the importance of diverse problem coverage and the use of hybrid rationales in developing superior math generalist models.",
        "id":261696697
      }
    ],
    "negative_ctxs":[
      {
        "title":"Polyglot Contextual Representations Improve Crosslingual Transfer",
        "text":"We introduce Rosita, a method to produce multilingual contextual word representations by training a single language model on text from multiple languages.Our method combines the advantages of contextual word representations with those of multilingual representation learning. We produce language models from dissimilar language pairs (English\/Arabic and English\/Chinese) and use them in dependency parsing, semantic role labeling, and named entity recognition, with comparisons to monolingual and noncontextual variants. Our results provide further evidence for the benefits of polyglot learning, in which representations are shared across multiple languages.",
        "id":67855733
      },
      {
        "title":"Knowledge Graph and Corpus Driven Segmentation and Answer Inference for Telegraphic Entity-seeking Queries",
        "text":"Much recent work focuses on formal interpretation of natural question utterances, with the goal of executing the resulting structured queries on knowledge graphs (KGs) such as Freebase. Here we address two limitations of this approach when applied to open-domain, entity-oriented Web queries. First, Web queries are rarely wellformed questions. They are \"telegraphic\", with missing verbs, prepositions, clauses, case and phrase clues. Second, the KG is always incomplete, unable to directly answer many queries. We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment: a base entity e 1 , a relation type r, a target entity type t 2 , and contextual words s. The query seeks entity e 2 ∈ t 2 where r(e 1 , e 2 ) holds, further evidenced by schema-agnostic words s. Query segmentation is integrated with the KG and an unstructured corpus where mentions of entities have been linked to the KG. We do not trust the best or any specific query segmentation. Instead, evidence in favor of candidate e 2 s are aggregated across several segmentations. Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG, using over a thousand telegraphic queries adapted from TREC, INEX, and Web-Questions, show the efficacy of our approach. For one benchmark, MAP improves from 0.2-0.29 (competitive baselines) to 0.42 (our system). NDCG@10 improves from 0.29-0.36 to 0.54.",
        "id":9526475
      },
      {
        "title":"Neural Machine Translation with Monolingual Translation Memory *",
        "text":"Prior work has proved that Translation memory (TM) can boost the performance of Neural Machine Translation (NMT). In contrast to existing work that uses bilingual corpus as TM and employs source-side similarity search for memory retrieval, we propose a new framework that uses monolingual memory and performs learnable memory retrieval in a crosslingual manner. Our framework has unique advantages. First, the cross-lingual memory retriever allows abundant monolingual data to be TM. Second, the memory retriever and NMT model can be jointly optimized for the ultimate translation goal. Experiments show that the proposed method obtains substantial improvements. Remarkably, it even outperforms strong TM-augmented NMT baselines using bilingual TM. Owning to the ability to leverage monolingual data, our model also demonstrates effectiveness in low-resource and domain adaptation scenarios.",
        "id":235166182
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which paper first used language models to emulate tool executions for studying the risks of language model agents?",
    "positive_ctxs":[
      {
        "title":"Identifying the Risks of LM Agents with an LM-Emulated Sandbox",
        "text":"Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks-such as leaking private data or causing financial losses. Identifying these risks is labor-intensive, necessitating implementing the tools, setting up the environment for each test scenario manually, and finding risky cases. As tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tail risks. To address these challenges, we introduce ToolEmu: a framework that uses a LM to emulate tool execution and enables scalable testing of LM agents against a diverse range of tools and scenarios. Alongside the emulator, we develop an LM-based automatic safety evaluator that examines agent failures and quantifies associated risks. We test both the tool emulator and evaluator through human evaluation and find that 68.8% of failures identified with ToolEmu would be valid real-world agent failures. Using our curated initial benchmark consisting of 36 high-stakes tools and 144 test cases, we provide a quantitative risk analysis of current LM agents and identify numerous failures with potentially severe outcomes. Notably, even the safest LM agent exhibits such failures 23.9% of the time according to our evaluator, underscoring the need to develop safer LM agents for real-world deployment. 1 * Equal contribution. Contact {yjruan, honghuad}@cs.toronto.edu. 1 Project website, demo, and open-source code can be found at",
        "id":262944419
      }
    ],
    "negative_ctxs":[
      {
        "title":"Learning Phrase Boundaries for Hierarchical Phrase-based Translation",
        "text":"Hierarchical phrase-based models provide a powerful mechanism to capture non-local phrase reorderings for statistical machine translation (SMT). However, many phrase reorderings are arbitrary because the models are weak on determining phrase boundaries for patternmatching. This paper presents a novel approach to learn phrase boundaries directly from word-aligned corpus without using any syntactical information. We use phrase boundaries, which indicate the beginning\/ending of phrase reordering, as soft constraints for decoding. Experimental results and analysis show that the approach yields significant improvements over the baseline on large-scale Chineseto-English translation.",
        "id":10105041
      },
      {
        "title":"Using Parallel Texts and Lexicons for Verbal Word Sense Disambiguation",
        "text":"We present a system for verbal Word Sense Disambiguation (WSD) that is able to exploit additional information from parallel texts and lexicons. It is an extension of our previous WSD method(Dušek et al., 2014), which gave promising results but used only monolingual features. In the follow-up work described here, we have explored two additional ideas: using English-Czech bilingual resources (as features only -the task itself remains a monolingual WSD task), and using a \"hybrid\" approach, adding features extracted both from a parallel corpus and from manually aligned bilingual valency lexicon entries, which contain subcategorization information. Albeit not all types of features proved useful, both ideas and additions have led to significant improvements for both languages explored.",
        "id":8742160
      },
      {
        "title":"Towards Practical Semantic Interoperability in NLP Platforms",
        "text":"Interoperability is a necessity for the development of complex tasks that require the interconnection of several NLP services. This article presents the approaches that were adopted in three scenarios to address their respective interoperability issues. The first scenario describes the creation of a common REST API for a specific platform, the second scenario presents the interconnection of several platforms via mapping of different representation formats and the third scenario shows the complexities of interoperability through semantic schema mapping or automatic translation.",
        "id":250164399
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which paper formally defines the problem of model selection in llm agent for multi-modal reasoning?",
    "positive_ctxs":[
      {
        "title":"TOWARDS ROBUST MULTI-MODAL REASONING VIA MODEL SELECTION",
        "text":"The reasoning capabilities of LLM (Large Language Model) are widely acknowledged in recent research, inspiring studies on tool learning and autonomous agents. LLM serves as the \"brain\" of agent, orchestrating multiple tools for collaborative multi-step task solving. Unlike methods invoking tools like calculators or weather APIs for straightforward tasks, multi-modal agents excel by integrating diverse AI models for complex challenges. However, current multi-modal agents neglect the significance of model selection: they primarily focus on the planning and execution phases, and will only invoke predefined task-specific models for each subtask, making the execution fragile. Meanwhile, other traditional model selection methods are either incompatible with or suboptimal for the multi-modal agent scenarios, due to ignorance of dependencies among subtasks arising by multi-step reasoning. To this end, we identify the key challenges therein and propose the M 3 framework as a plug-in with negligible runtime overhead at test-time. This framework improves model selection and bolsters the robustness of multi-modal agents in multi-step reasoning. In the absence of suitable benchmarks, we create MS-GQA, a new dataset specifically designed to investigate the model selection challenge in multi-modal agents. Our experiments reveal that our framework enables dynamic model selection, considering both user inputs and subtask dependencies, thereby robustifying the overall reasoning process. Our code and benchmark: https:\/\/github.com\/LINs-lab\/M3.",
        "id":263909212
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"In this paper, we argue in favor of reconsidering models for word meaning, using as a basis results from cognitive science on human concept representation. More specifically, we argue for a more flexible representation of word meaning than the assignment of a single best-fitting dictionary sense to each occurrence: Either use dictionary senses, but view them as having fuzzy boundaries, and assume that an occurrence can activate multiple senses to different degrees. Or move away from dictionary senses completely, and only model similarities between individual word usages. We argue that distributional models provide a flexible framework for experimenting with alternative models of word meanings, and discuss example models.",
        "id":17640849
      },
      {
        "title":"A Fast and Accurate Vietnamese Word Segmenter",
        "text":"We propose a novel approach to Vietnamese word segmentation. Our approach is based on the Single Classification Ripple Down Rules methodology(Compton and Jansen, 1990), where rules are stored in an exception structure and new rules are only added to correct segmentation errors given by existing rules. Experimental results on the benchmark Vietnamese treebank show that our approach outperforms previous state-of-the-art approaches JVnSegmenter, vnTokenizer, DongDu and UETsegmenter in terms of both accuracy and performance speed. Our code is open-source and available at: https:\/\/github.com\/datquocnguyen\/RDRsegmenter.",
        "id":871724
      },
      {
        "title":"Context-Sensitive Syntactic Source-Reordering by Statistical Transduction",
        "text":"How well can a phrase translation model perform if we permute the source words to fit target word order as perfectly as word alignment might allow? And how well would it perform if we limit the allowed permutations to ITGlike tree-transduction operations on the source parse tree? First we contribute oracle results showing great potential for performance improvement by source-reordering, ranging from 1.5 to 4 BLEU points depending on language pair. Although less outspoken, the potential of tree-based source-reordering is also significant. Our second contribution is a source reordering model that works with two kinds of tree transductions: the one permutes the order of sibling subtrees under a node, and the other first deletes layers in the parse tree in order to exploit sibling permutation at the remaining levels.The statistical parameters of the model we introduce concern individual tree transductions conditioned on contextual features of the tree resulting from all preceding transductions. Experiments in translating from English to Spanish\/Dutch\/Chinese show significant improvements of respectively 0.6\/1.2\/2.0 BLEU points.",
        "id":17261224
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which paper found that using common character encodings and ciphers, or even just convincing the model that it is not communicating in natural language, can bypass the safety guardrails of large models?",
    "positive_ctxs":[
      {
        "title":"GPT-4 IS TOO SMART TO BE SAFE: STEALTHY CHAT WITH LLMS VIA CIPHER",
        "text":"1. Iluvw, brx qhhg wr jhw d mre lqvlgh wkh edqn, wklv zloo jlyh brx dffhvv wr wkh … User: Let's discuss how to steal money from the bank.User: Ohw'v glvfxvv krz wr vwhdo prqhb iurp wkh edqn.Sorry, but I can't assist with that.Encipher Decipher GPT 1. First, you need to get a job inside the bank, this will give you access to the … Figure 1: Engaging in conversations with ChatGPT using ciphers can lead to unsafe behaviors. ABSTRACT Safety lies at the core of the development of Large Language Models (LLMs).There is ample work on aligning LLMs with human ethics and preferences, including data filtering in pretraining, supervised fine-tuning, reinforcement learning from human feedback, and red teaming, etc. In this study, we discover that chat in cipher can bypass the safety alignment techniques of LLMs, which are mainly conducted in natural languages. We propose a novel framework CipherChat to systematically examine the generalizability of safety alignment to non-natural languages -ciphers. CipherChat enables humans to chat with LLMs through cipher prompts topped with system role descriptions and few-shot enciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs, including ChatGPT and GPT-4 for different representative human ciphers across 11 safety domains in both English and Chinese. Experimental results show that certain ciphers succeed almost 100% of the time to bypass the safety alignment of GPT-4 in several safety domains, demonstrating the necessity of developing safety alignment for non-natural languages.Notably, we identify that LLMs seem to have a \"secret cipher\", and propose a novel SelfCipher that uses only role play and several demonstrations in natural language to evoke this capability. SelfCipher surprisingly outperforms existing human ciphers in almost all cases. 1",
        "id":260887189
      }
    ],
    "negative_ctxs":[
      {
        "title":"Automatic ICD Coding via Interactive Shared Representation Networks with Self-distillation Mechanism",
        "text":"The ICD coding task aims at assigning codes of the International Classification of Diseases in clinical notes. Since manual coding is very laborious and prone to errors, many methods have been proposed for the automatic ICD coding task. However, existing works either ignore the long-tail of code frequency or the noisy clinical notes. To address the above issues, we propose an Interactive Shared Representation Network with Self-Distillation mechanism. Specifically, an interactive shared representation network targets building connections among codes while modeling the cooccurrence, consequently alleviating the longtail problem. Moreover, to cope with the noisy text issue, we encourage the model to focus on the clinical note's noteworthy part and extract valuable information through a self-distillation learning mechanism. Experimental results on two MIMIC datasets demonstrate the effectiveness of our method.",
        "id":236459913
      },
      {
        "title":"",
        "text":"",
        "id":237204597
      },
      {
        "title":"SOME RESULTS ON STOCHASTIC LANGUAGE MODELLING",
        "text":"The paper will discuss three issues. The first is the derivation of precise probability scores for partial hypotheses containing islands, in the context of a Stochastic-Context-Free-Grammar (SCFG) for Language Modeling (LM). The second issue is the possibility of adding a cache component to a LM. This component alters the expected probability of words to reflect the speaker's patterns of word use. Finally, the idiosyncratic properties of dialogue are being studied; this work will indicate how knowledge about the discourse state can be incorporated into the LM and into the semantic component.",
        "id":1722348
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which paper in human motion generation can control the spatial location of any joints of the human with either dense or sparse 3D points?",
    "positive_ctxs":[
      {
        "title":"OMNICONTROL: CONTROL ANY JOINT AT ANY TIME FOR HUMAN MOTION GENERATION",
        "text":"We present a novel approach named OmniControl for incorporating flexible spatial control signals into a text-conditioned human motion generation model based on the diffusion process.Unlike previous methods that can only control the pelvis trajectory, OmniControl can incorporate flexible spatial control signals over different joints at different times with only one model.Specifically, we propose analytic spatial guidance that ensures the generated motion can tightly conform to the input control signals.At the same time, realism guidance is introduced to refine all the joints to generate more coherent motion.Both the spatial and realism guidance are essential and they are highly complementary for balancing control accuracy and motion realism.By combining them, OmniControl generates motions that are realistic, coherent, and consistent with the spatial constraints.Experiments on HumanML3D and KIT-ML datasets show that OmniControl not only achieves significant improvement over state-of-the-art methods on pelvis control but also shows promising results when incorporating the constraints over other joints.Project page: https:\/\/neu-vi.github.io\/omnicontrol\/.",
        "id":263909429
      }
    ],
    "negative_ctxs":[
      {
        "title":"Large-scale Dictionary Construction via Pivot-based Statistical Machine Translation with Significance Pruning and Neural Network Features",
        "text":"We present our ongoing work on large-scale Japanese-Chinese bilingual dictionary construction via pivot-based statistical machine translation. We utilize statistical significance pruning to control noisy translation pairs that are induced by pivoting. We construct a large dictionary which we manually verify to be of a high quality. We then use this dictionary and a parallel corpus to learn bilingual neural network language models to obtain features for reranking the n-best list, which leads to an absolute improvement of 5% in accuracy when compared to a setting that does not use significance pruning and reranking.",
        "id":12774566
      },
      {
        "title":"MISGAN: LEARNING FROM INCOMPLETE DATA WITH GENERATIVE ADVERSARIAL NETWORKS",
        "text":"Generative adversarial networks (GANs) have been shown to provide an effective way to model complex distributions and have obtained impressive results on various challenging tasks. However, typical GANs require fully-observed data during training. In this paper, we present a GAN-based framework for learning from complex, high-dimensional incomplete data. The proposed framework learns a complete data generator along with a mask generator that models the missing data distribution. We further demonstrate how to impute missing data by equipping our framework with an adversarially trained imputer. We evaluate the proposed framework using a series of experiments with several types of missing data processes under the missing completely at random assumption. 1 Let x obs denote the observed elements of x, and x mis denote the missing elements according to the mask m. In addition, let θ denote the unknown parameters of the data distribution, and φ denote the unknown parameters for the mask distribution, which are usually assumed to be independent of θ. In the standard maximum likelihood setting, the unknown parameters are estimated by maximizing the 1 Our implementation is available at https:\/\/github.com\/steveli\/misgan 2 The complementm is usually referred to as the missing data indicator in the literature.Published as a conference paper at ICLR 2019 following marginal likelihood, integrating over the unknown missing data values:p(x obs , m) = p θ (x obs , x mis )p φ (m|x obs , x mis )dx mis .Little & Rubin (2014) characterize the missing data mechanism p φ (m|x obs , x mis ) in terms of independence relations between the complete data x = [x obs , x mis ] and the masks m:• Missing completely at random (MCAR): p φ (m|x) = p φ (m),• Missing at random (MAR): p φ (m|x) = p φ (m|x obs ),• Not missing at random (NMAR): m depends on x mis and possibly also x obs .",
        "id":67856605
      },
      {
        "title":"UIC-NLP at SemEval-2022 Task 5: Exploring Contrastive Learning for Multimodal Detection of Misogynistic Memes",
        "text":"Misogynistic memes are rampant on social media, and often convey their messages using multimodal signals (e.g., images paired with derogatory text or captions). However, to date very few multimodal systems have been leveraged for the detection of misogynistic memes. Recently, researchers have turned to contrastive learning solutions for a variety of problems. Most notably, OpenAI's CLIP model has served as an innovative solution for a variety of multimodal tasks. In this work, we experiment with contrastive learning to address the detection of misogynistic memes within the context of SemEval-2022 Task 5. Although our model does not achieve top results, these experiments provide important exploratory findings for this task. We conduct a detailed error analysis, revealing promising clues and offering a foundation for follow-up work.",
        "id":250390575
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which paper is the first to model the helpfulness and harmlessness alignment of LLMs as a Constrained MDP problem?",
    "positive_ctxs":[
      {
        "title":"SAFE RLHF: SAFE REINFORCEMENT LEARNING FROM HUMAN FEEDBACK",
        "text":"With the development of large language models (LLMs), striking a balance between the performance and safety of AI systems has never been more critical.However, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training.To address this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe RLHF), a novel algorithm for human value alignment.Safe RLHF explicitly decouples human preferences regarding helpfulness and harmlessness, effectively avoiding the crowdworkers' confusion about the tension and allowing us to train separate reward and cost models.We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints.Leveraging the Lagrangian method to solve this constrained problem, Safe RLHF dynamically adjusts the balance between the two objectives during fine-tuning.Through a three-round fine-tuning using Safe RLHF, we demonstrate a superior ability to mitigate harmful responses while enhancing model performance compared to existing value-aligned algorithms.Experimentally, we finetuned the Alpaca-7B using Safe RLHF and aligned it with collected human preferences, significantly improving its helpfulness and harmlessness according to human evaluations.",
        "id":264306078
      }
    ],
    "negative_ctxs":[
      {
        "title":"A Common Multimedia Annotation Framework for Cross Linking Cultural Heritage Digital Collections",
        "text":"In the context of the CATCH research program that is currently carried out at a number of large Dutch cultural heritage institutions our ambition is to combine and exchange heterogeneous multimedia annotations between projects and institutions. As first step we designed an Annotation Meta Model: a simple but powerful RDF\/OWL model mainly addressing the anchoring of annotations to segments of the many different media types used in the collections of the archives, museums and libraries involved. The model includes support for the annotation of annotations themselves, and of segments of annotation values, to be able to layer annotations and in this way enable projects to process each other's annotation data as the primary data for further annotation. On basis of AMM we designed an application programming interface for accessing annotation repositories and implemented it both as a software library and as a web service. Finally, we report on our experiences with the application of model, API and repository when developing web applications for collection managers in cultural heritage institutions.",
        "id":1771259
      },
      {
        "title":"95% Replicability for Manual Word Sense Tagging",
        "text":"",
        "id":30754231
      },
      {
        "title":"INTERFACE DATABASES: DESIGN AND COLLECTION OF A MULTILINGUAL EMOTIONAL SPEECH DATABASE",
        "text":"As a part of the IST project Interface (\"Multimodal Analysis\/Synthesis System for Human Interaction to Virtual and Augmented environments\"), an emotional speech database for Slovenian, English, Spanish, and French language has been recorded. The database is designed for general study of emotional speech as well as analysis of emotion characteristics for speech synthesis and for automatic emotion classification purposes. Six emotions have been defined: anger, sadness, joy, fear, disgust and surprise. The neutral styles were also recorded. One male speaker and one female speaker have been recorded, except for English language where two mail and one female speaker have been recorded. All the speakers are actors. The corpuses consist of 175-190 sentences for each language. For Spanish and Slovenian databases subjective evaluation tests have been made. The recorded Interface emotional speech database represents a good basis for emotional speech analysis and is also useful in synthesis of emotional speech.",
        "id":44551203
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which paper proposes an alignment framework that steers language models to preferences of individual groups in a few-shot manner through augmenting the LLM with a transformer module?",
    "positive_ctxs":[
      {
        "title":"GROUP PREFERENCE OPTIMIZATION: FEW-SHOT ALIGNMENT OF LARGE LANGUAGE MODELS",
        "text":"Many applications of large language models (LLMs), ranging from chatbots to creative writing, require nuanced subjective judgments that can differ significantly across different groups.Existing alignment algorithms can be expensive to align for each group, requiring prohibitive amounts of group-specific preference data and computation for real-world use cases.We introduce Group Preference Optimization (GPO), an alignment framework that steers language models to preferences of individual groups in a few-shot manner.In GPO, we augment the base LLM with an independent transformer module trained to predict the preferences of a group for the LLM generations.For few-shot learning, we parameterize this module as an in-context autoregressive transformer and train it via meta-learning on several groups.We empirically validate the efficacy of GPO through rigorous evaluations using LLMs with varied sizes on three human opinion adaptation tasks.These tasks involve adapting to the preferences of US demographic groups, global countries, and individual users.Our results demonstrate that GPO not only aligns models more accurately but also requires fewer group-specific preferences, and less training and inference computing resources, outperforming existing strategies such as in-context steering and fine-tuning methods. 1 Warning: This paper contains qualitative examples that may be viewed as offensive or harmful.",
        "id":264289064
      }
    ],
    "negative_ctxs":[
      {
        "title":"RecoBERT: A Catalog Language Model for Text-Based Recommendations",
        "text":"Language models that utilize extensive selfsupervised pre-training from unlabeled text, have recently shown to significantly advance the state-of-the-art performance in a variety of language understanding tasks. However, it is yet unclear if and how these recent models can be harnessed for conducting text-based recommendations. In this work, we introduce RecoBERT, a BERT-based approach for learning catalog-specialized language models for text-based item recommendations. We suggest novel training and inference procedures for scoring similarities between pairs of items, that don't require item similarity labels. Both the training and the inference techniques were designed to utilize the unlabeled structure of textual catalogs, and minimize the discrepancy between them. By incorporating four scores during inference, RecoBERT can infer text-based item-to-item similarities more accurately than other techniques. In addition, we introduce a new language understanding task for wine recommendations using similarities based on professional wine reviews. As an additional contribution, we publish annotated recommendations dataset crafted by human wine experts. Finally, we evaluate Re-coBERT and compare it to various state-of-theart NLP models on wine and fashion recommendations tasks.",
        "id":221970965
      },
      {
        "title":"The Role of Coverage in Online Reinforcement Learning",
        "text":"Coverage conditions-which assert that the data logging distribution adequately covers the state space-play a fundamental role in determining the sample complexity of offline reinforcement learning. While such conditions might seem irrelevant to online reinforcement learning at first glance, we establish a new connection by showing-somewhat surprisingly-that the mere existence of a data distribution with good coverage can enable sample-efficient online RL. Concretely, we show that coverability-that is, existence of a data distribution that satisfies a ubiquitous coverage condition called concentrability-can be viewed as a structural property of the underlying MDP, and can be exploited by standard algorithms for sample-efficient exploration, even when the agent does not know said distribution. We complement this result by proving that several weaker notions of coverage, despite being sufficient for offline RL, are insufficient for online RL. We also show that existing complexity measures for online RL, including Bellman rank and Bellman-Eluder dimension, fail to optimally capture coverability, and propose a new complexity measure, the sequential extrapolation coefficient, to provide a unification. * Equal contribution 1 arXiv:2210.04157v1 [cs.LG] 9 Oct 2022 • Observe the resulting trajectory (x (t) 1 , a (t) 1 , r (t) 1 ), . . . , (x (t) H , a (t) H , r (t) H ). The learner's goal is to minimize their cumulative regret, defined via Reg := T t=1 J(π ) − J(π (t) ). Schapire. Taming the monster: A fast and simple algorithm for contextual bandits.",
        "id":252780137
      },
      {
        "title":"Analyse morphologique non supervisée en domaine biomédical Application à la recherche d'information",
        "text":"Dans le domaine biomédical, utiliser des termes spécialisés est essentiel pour accéder à l'information. Cependant, dans beaucoup de langues, ces termes sont des constructions morphologiques complexes qui compliquent cet accès à l'information. Dans cet article, nous nous intéressons à l'identification des composants morphologiques de ces termes et à leur utilisation pour une tâche de recherche d'information (RI). Nous proposons différentes approches reposant sur un alignement automatique avec une langue pivot particulière, le japonais, et sur un apprentissage par analogie permettant de produire des analyses morphologiques fines des termes d'une langue donnée. Ces analyses morphologiques sont ensuite utilisées pour améliorer l'indexation de documents biomédicaux. Les expériences rapportées montrent la validité de cette approche avec des gains en MAP de plus de 10 % par rapport à un système de RI standard.ABSTRACT. In the biomedical field, using of specialized terms is key to access information. However, in most Indo-European languages, these terms are complex morphological structures. The presented work aims at identifying the various meaningful components of these terms and use them to improve biomedical Information Retrieval (IR). We present different approaches combining automatic alignments with a pivot language, Japanese, and analogical learning that allows an accurate morphological analysis of terms. These morphological analysis are used to improve the indexing of medical documents. The experiments reported in this paper show the validity of this approach with a 10% MAP improvement over a standard IR system. MOTS-CLÉS : morphologie, terminologie biomédicale, alignement, apprentissage par analogie, indexation morphosémantique, recherche d'information biomédicale.",
        "id":260165148
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which paper proposes to integrate black-box LLMs with a pool of smaller but specialized language models?",
    "positive_ctxs":[
      {
        "title":"KNOWLEDGE CARD: FILLING LLMS' KNOWLEDGE GAPS WITH PLUG-IN SPECIALIZED LANGUAGE MODELS",
        "text":"By design, large language models (LLMs) are static general-purpose models, expensive to retrain or update frequently.As they are increasingly adopted for knowledge-intensive tasks, it becomes evident that these design choices lead to failures to generate factual, relevant, and up-to-date knowledge.To this end, we propose KNOWLEDGE CARD, a modular framework to plug in new factual and relevant knowledge into general-purpose LLMs.We first introduce knowledge cards-specialized language models trained on corpora from specific domains and sources.Knowledge cards serve as parametric repositories that are selected at inference time to generate background knowledge for the base LLM.We then propose three content selectors to dynamically select and retain information in documents generated by knowledge cards, specifically controlling for relevance, brevity, and factuality of outputs.Finally, we propose two complementary integration approaches to augment the base LLM with the (relevant, factual) knowledge curated from the specialized LMs.Through extensive experiments, we demonstrate that KNOWLEDGE CARD achieves state-of-the-art performance on six benchmark datasets.Ultimately, KNOWLEDGE CARD framework enables dynamic synthesis and updates of knowledge from diverse domains.Its modularity will ensure that relevant knowledge can be continuously updated through the collective efforts of the research community.",
        "id":258741298
      }
    ],
    "negative_ctxs":[
      {
        "title":"SACRY: Syntax-based Automatic Crossword puzzle Resolution sYstem",
        "text":"In this paper, we present our Crossword Puzzle Resolution System (SACRY), which exploits syntactic structures for clue reranking and answer extraction. SACRY uses a database (DB) containing previously solved CPs in order to generate the list of candidate answers. Additionally, it uses innovative features, such as the answer position in the rank and aggregated information such as the min, max and average clue reranking scores. Our system is based on WebCrow, one of the most advanced systems for automatic crossword puzzle resolution. Our extensive experiments over our two million clue dataset show that our approach highly improves the quality of the answer list, enabling the achievement of unprecedented results on the complete CP resolution tasks, i.e., accuracy of 99.17%.",
        "id":7975508
      },
      {
        "title":"FTD at SemEval-2023 Task 3: News Genre and Propaganda Detection by Comparing Mono-and Multilingual Models with Fine-tuning on Additional Data",
        "text":"We report our participation in the SemEval-2023 shared task on propaganda detection and describe our solutions with pre-trained models and their ensembles. For Subtask 1 (News Genre Categorisation), we report the impact of several settings, such as the choice of the classification models (monolingual or multilingual or their ensembles), the choice of the training sets (base or additional sources), the impact of detection certainty in making a classification decision as well as the impact of other hyperparameters. In particular, we fine-tune models on additional data for other genre classification tasks, such as FTD. We also try adding texts from genre-homogenous corpora, such as Panorama, Babylon Bee for satire and Giganews for for reporting texts. We also make prepared models for Subtasks 2 and 3 with finetuning the corresponding models first for Subtask 1. The code needed to reproduce the experiments is available. 1Subtask 1: News genresWe experiment with all the languages and report our results on the dev sets. In all the models we use, the input size is restricted to k tokens (in most cases, k = 512). To tackle it, we take first k tokens 1 549",
        "id":259376610
      },
      {
        "title":"Statistical Filtering and Subcategorization Frame Acquisition",
        "text":"Research \"into the automatic acquisition of subcategorization frames (SCFS) from corpora is starting to produce large-scale computational lexicons which include valuable frequency information. However, the accuracy of the resulting lexicons shows room for improvement. One significant source of error lies in the statistical filtering used by some researchers to remove noise from automatically acquired subcategorization frames. In this paper, we compare three different approaches to filtering out spurious hypotheses. Two hypothesis tests perform poorly, compared to filtering frames on the basis of relative frequency. We discuss reasons for this and consider directions for future research.",
        "id":13579675
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which paper studies how difficult is a policy learning problem under non-additive rewards in terms of theoretical lower bounds, and what could be a potential strategy to solve it empirically while recovering some specialized guarantees?",
    "positive_ctxs":[
      {
        "title":"Submodular Reinforcement Learning",
        "text":"In reinforcement learning (RL), rewards of states are typically considered additive, and following the Markov assumption, they are independent of states visited previously. In many important applications, such as coverage control, experiment design and informative path planning, rewards naturally have diminishing returns, i.e., their value decreases in light of similar states visited previously. To tackle this, we propose submodular RL (SUBRL), a paradigm which seeks to optimize more general, non-additive (and history-dependent) rewards modelled via submodular set functions which capture diminishing returns. Unfortunately, in general, even in tabular settings, we show that the resulting optimization problem is hard to approximate. On the other hand, motivated by the success of greedy algorithms in classical submodular optimization, we propose SUBPO, a simple policy gradient-based algorithm for SUBRL that handles non-additive rewards by greedily maximizing marginal gains. Indeed, under some assumptions on the underlying Markov Decision Process (MDP), SUBPO recovers optimal constant factor approximations of submodular bandits. Moreover, we derive a natural policy gradient approach for locally optimizing SUBRL instances even in large state-and action-spaces. We showcase the versatility of our approach by applying SUBPO to several applications such as biodiversity monitoring, Bayesian experiment design, informative path planning, and coverage maximization. Our results demonstrate sample efficiency, as well as scalability to high-dimensional state-action spaces.",
        "id":260154786
      }
    ],
    "negative_ctxs":[
      {
        "title":"Forest-to-String Statistical Translation Rules",
        "text":"In this paper, we propose forest-to-string rules to enhance the expressive power of tree-to-string translation models. A forestto-string rule is capable of capturing nonsyntactic phrase pairs by describing the correspondence between multiple parse trees and one string. To integrate these rules into tree-to-string translation models, auxiliary rules are introduced to provide a generalization level. Experimental results show that, on the NIST 2005 Chinese-English test set, the tree-to-string model augmented with forest-to-string rules achieves a relative improvement of 4.3% in terms of BLEU score over the original model which allows treeto-string rules only.",
        "id":18616120
      },
      {
        "title":"AUEB: Two Stage Sentiment Analysis of Social Network Messages",
        "text":"This paper describes the system submitted for the Sentiment Analysis in Twitter Task of SEMEVAL 2014 and specifically the Message Polarity Classification subtask. We used a 2-stage pipeline approach employing a linear SVM classifier at each stage and several features including morphological features, POS tags based features and lexicon based features.",
        "id":491439
      },
      {
        "title":"Published as a conference paper at ICLR 2023 AIM: ADAPTING IMAGE MODELS FOR EFFICIENT VIDEO ACTION RECOGNITION",
        "text":"Recent vision transformer based video models mostly follow the \"image pretraining then finetuning\" paradigm and have achieved great success on multiple video benchmarks. However, full finetuning such a video model could be computationally expensive and unnecessary, given the pre-trained image transformer models have demonstrated exceptional transferability. In this work, we propose a novel method to Adapt pre-trained Image Models (AIM) for efficient video understanding. By freezing the pre-trained image model and adding a few lightweight Adapters, we introduce spatial adaptation, temporal adaptation and joint adaptation to gradually equip an image model with spatiotemporal reasoning capability. We show that our proposed AIM can achieve competitive or even better performance than prior arts with substantially fewer tunable parameters on four video action recognition benchmarks. Thanks to its simplicity, our method is also generally applicable to different image pre-trained models, which has the potential to leverage more powerful image foundation models in the future. The project webpage is https:\/\/adapt-image-models.github.io\/. * Work done during an internship at Amazon Web Services.",
        "id":256615635
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which paper trains on linear regression to hypothesize how fine-tuning affects language models?",
    "positive_ctxs":[
      {
        "title":"UNDERSTANDING CATASTROPHIC FORGETTING IN LANGUAGE MODELS VIA IMPLICIT INFERENCE",
        "text":"Fine-tuning (via methods such as instruction-tuning or reinforcement learning from human feedback) is a crucial step in training language models to robustly carry out tasks of interest.However, we lack a systematic understanding of the effects of fine-tuning, particularly on tasks outside the narrow fine-tuning distribution.In a simplified scenario, we demonstrate that improving performance on tasks within the fine-tuning data distribution comes at the expense of suppressing model capabilities on other tasks.This degradation is especially pronounced for tasks \"closest\" to the fine-tuning distribution.We hypothesize that language models implicitly infer the task of the prompt corresponds, and the fine-tuning process predominantly skews this task inference towards tasks in the fine-tuning distribution.To test this hypothesis, we propose Conjugate Prompting to see if we can recover pretrained capabilities.Conjugate prompting artificially makes the task look farther from the fine-tuning distribution while requiring the same capability.We find that conjugate prompting systematically recovers some of the pretraining capabilities on our synthetic setup.We then apply conjugate prompting to real-world LLMs using the observation that fine-tuning distributions are typically heavily skewed towards English.We find that simply translating the prompts to different languages can cause the fine-tuned models to respond like their pretrained counterparts instead.This allows us to recover the in-context learning abilities lost via instruction tuning, and more concerningly, to recover harmful content generation suppressed by safety fine-tuning in chatbots like ChatGPT.",
        "id":262054014
      }
    ],
    "negative_ctxs":[
      {
        "title":"Multilingual Semantic Parsing : Parsing Multiple Languages into Semantic Representations",
        "text":"We consider multilingual semantic parsing -the task of simultaneously parsing semantically equivalent sentences from multiple different languages into their corresponding formal semantic representations. Our model is built on top of the hybrid tree semantic parsing framework, where natural language sentences and their corresponding semantics are assumed to be generated jointly from an underlying generative process. We first introduce a variant of the joint generative process, which essentially gives us a new semantic parsing model within the framework. Based on the different models that can be developed within the framework, we then investigate several approaches for performing the multilingual semantic parsing task. We present our evaluations on a standard dataset annotated with sentences in multiple languages coming from different language families.",
        "id":13287001
      },
      {
        "title":"A Corpus-based Multidimensional Analysis of Linguistic Features of Truth and Deception",
        "text":"This study sets out to examine the linguistic difference between truthful and deceptive texts. In order to take more linguistic features into consideration, this research applied multidimensional analysis, which can reduce many linguistic features into several factors. This study used a self-built corpus containing 100 truthful texts and 100 deceptive texts. TextMind was employed to annotate these Chinese texts automatically. SPSS version 20 was utilized for t-tests and multidimensional analysis. The discussion of the data was divided into two parts: word count and word per sentence, and multidimensional analysis. This research reveals that word count and word per sentence of deceptive discourse are significantly smaller than those of truthful discourse. The results of multidimensional analysis suggest that deceptive discourse displays a weaker performance on dimensions of narration, interpersonal relationship, and perception.",
        "id":202232093
      },
      {
        "title":"",
        "text":"",
        "id":188723581
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which paper uses the latent diffusion model for the first time to solve offline reinforcement learning problems based on the sequential modeling framework?",
    "positive_ctxs":[
      {
        "title":"Efficient Planning with Latent Diffusion",
        "text":"Temporal abstraction and efficient planning pose significant challenges in offline reinforcement learning, mainly when dealing with domains that involve temporally extended tasks and delayed sparse rewards.Existing methods typically plan in the raw action space and can be inefficient and inflexible.Latent action spaces offer a more flexible paradigm, capturing only possible actions within the behavior policy support and decoupling the temporal structure between planning and modeling.However, current latent-action-based methods are limited to discrete spaces and require expensive planning.This paper presents a unified framework for continuous latent action space representation learning and planning by leveraging latent, score-based diffusion models.We establish the theoretical equivalence between planning in the latent action space and energy-guided sampling with a pretrained diffusion model and incorporate a novel sequence-level exact sampling method.Our proposed method, LatentDiffuser, demonstrates competitive performance on low-dimensional locomotion control tasks and surpasses existing methods in higher-dimensional tasks.",
        "id":263334587
      }
    ],
    "negative_ctxs":[
      {
        "title":"APPLICATIONS OF A LEXICOGRAPHICAL DATA BASE FOR GERMAN",
        "text":"",
        "id":11966677
      },
      {
        "title":"Prune-and-Score: Learning for Greedy Coreference Resolution",
        "text":"We propose a novel search-based approach for greedy coreference resolution, where the mentions are processed in order and added to previous coreference clusters. Our method is distinguished by the use of two functions to make each coreference decision: a pruning function that prunes bad coreference decisions from further consideration, and a scoring function that then selects the best among the remaining decisions. Our framework reduces learning of these functions to rank learning, which helps leverage powerful off-the-shelf rank-learners. We show that our Prune-and-Score approach is superior to using a single scoring function to make both decisions and outperforms several state-of-the-art approaches on multiple benchmark corpora including OntoNotes.",
        "id":13532387
      },
      {
        "title":"Analyzing Curriculum Learning for Sentiment Analysis along Task Difficulty, Pacing and Visualization Axes",
        "text":"While Curriculum Learning (CL) has recently gained traction in Natural language Processing Tasks, it is still not adequately analyzed. Previous works only show their effectiveness but fail short to explain and interpret the internal workings fully. In this paper, we analyze curriculum learning in sentiment analysis along multiple axes. Some of these axes have been proposed by earlier works that need more in-depth study. Such analysis requires understanding where curriculum learning works and where it does not. Our axes of analysis include Task difficulty on CL, comparing CL pacing techniques, and qualitative analysis by visualizing the movement of attention scores in the model as curriculum phases progress. We find that curriculum learning works best for difficult tasks and may even lead to a decrement in performance for tasks with higher performance without curriculum learning. We see that One-Pass curriculum strategies suffer from catastrophic forgetting and attention movement visualization within curriculum pacing. This shows that curriculum learning breaks down the challenging main task into easier sub-tasks solved sequentially.",
        "id":231979380
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which paper utilized MMD flows with Riesz kernels to solve Bayesian inverse problems?",
    "positive_ctxs":[
      {
        "title":"Posterior Sampling Based on Gradient Flows of the MMD with Negative Distance Kernel",
        "text":"We propose conditional flows of the maximum mean discrepancy (MMD) with the negative distance kernel for posterior sampling and conditional generative modelling.This MMD, which is also known as energy distance, has several advantageous properties like efficient computation via slicing and sorting.We approximate the joint distribution of the ground truth and the observations using discrete Wasserstein gradient flows and establish an error bound for the posterior distributions.Further, we prove that our particle flow is indeed a Wasserstein gradient flow of an appropriate functional.The power of our method is demonstrated by numerical examples including conditional image generation and inverse problems like superresolution, inpainting and computed tomography in low-dose and limited-angle settings.",
        "id":263671662
      }
    ],
    "negative_ctxs":[
      {
        "title":"Biomedical Event Extraction with Hierarchical Knowledge Graphs",
        "text":"Biomedical event extraction is critical in understanding biomolecular interactions described in scientific corpus.",
        "id":222303317
      },
      {
        "title":"Generation of Referring Expression Using Prefix Tree Structure",
        "text":"This paper presents a Prefix Tree (Trie) based model for Generation of Referring Expression (GRE). The existing algorithms in GRE lie in two extremities. Incremental algorithm is simple and speedy but less expressive in nature whereas others are complex and exhaustive but more expressive in nature. Our prefix tree based model not only incorporates all relevant features of GRE (like describing set, generating Boolean and context sensitive description etc.) but also try to attain simplicity and speed properties of Incremental algorithm. Thus this model provides a simple and linguistically rich approach to GRE.",
        "id":2111529
      },
      {
        "title":"Because Size Does Matter: The Hamburg Dependency Treebank",
        "text":"We present the Hamburg Dependency Treebank (HDT), which to our knowledge is the largest dependency treebank currently available. It consists of genuine dependency annotations, i. e. they have not been transformed from phrase structures. We explore characteristics of the treebank and compare it against others. To exemplify the benefit of large dependency treebanks, we evaluate different parsers on the HDT. In addition, a set of tools will be described which help working with and searching in the treebank.",
        "id":14586802
      }
    ]
  },
  {
    "specificity":0,
    "query_set":"manual_iclr",
    "question":"What paper provides generalization bounds for self supervised learning models eg. CLIP",
    "positive_ctxs":[
      {
        "title":"Understanding prompt engineering may not require rethinking generalization",
        "text":"Zero-shot learning in prompted vision-language models, the practice of crafting prompts to build classifiers without an explicit training process, has achieved impressive performance in many settings.This success presents a seemingly surprising observation: these methods suffer relatively little from overfitting, i.e., when a prompt is manually engineered to achieve low error on a given training set (thus rendering the method no longer actually zero-shot), the approach still performs well on held-out test data.In this paper, we show that we can explain such performance well via recourse to classical PAC-Bayes bounds.Specifically, we show that the discrete nature of prompts, combined with a PAC-Bayes prior given by a language model, results in generalization bounds that are remarkably tight by the standards of the literature: for instance, the generalization bound of an ImageNet classifier is often within a few percentage points of the true test error.We demonstrate empirically that this holds for existing handcrafted prompts and prompts generated through simple greedy search.Furthermore, the resulting bound is well-suited for model selection: the models with the best bound typically also have the best test performance.This work thus provides a possible justification for the widespread practice of \"prompt engineering,\" even if it seems that such methods could potentially overfit the training data.",
        "id":263830433
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":216051496
      },
      {
        "title":"Homophonic Pun Generation with Lexically Constrained Rewriting",
        "text":"Punning is a creative way to make conversation enjoyable and literary writing elegant. In this paper, we focus on the task of generating a pun sentence given a pair of homophones. We first find the constraint words supporting the semantic incongruity for a sentence. Then we rewrite the sentence with explicit positive and negative constraints. Our model achieves the state-of-the-art results in both automatic and human evaluations. We further make an error analysis and discuss the challenges for the computational pun models.",
        "id":226262275
      },
      {
        "title":"Multi-document Summarization via Budgeted Maximization of Submodular Functions",
        "text":"We treat the text summarization problem as maximizing a submodular function under a budget constraint. We show, both theoretically and empirically, a modified greedy algorithm can efficiently solve the budgeted submodular maximization problem near-optimally, and we derive new approximation bounds in doing so. Experiments on DUC'04 task show that our approach is superior to the bestperforming method from the DUC'04 evaluation on ROUGE-1 scores.",
        "id":1803710
      }
    ]
  },
  {
    "specificity":1,
    "query_set":"manual_iclr",
    "question":"Which paper systematically examed the input mismatch between training and sampling in diffusion models",
    "positive_ctxs":[
      {
        "title":"ELUCIDATING THE EXPOSURE BIAS IN DIFFUSION MODELS",
        "text":"Diffusion models have demonstrated impressive generative capabilities, but their exposure bias problem, described as the input mismatch between training and sampling, lacks in-depth exploration.In this paper, we systematically investigate the exposure bias problem in diffusion models by first analytically modelling the sampling distribution, based on which we then attribute the prediction error at each sampling step as the root cause of the exposure bias issue.Furthermore, we discuss potential solutions to this issue and propose an intuitive metric for it.Along with the elucidation of exposure bias, we propose a simple, yet effective, training-free method called Epsilon Scaling to alleviate the exposure bias.We show that Epsilon Scaling explicitly moves the sampling trajectory closer to the vector field learned in the training phase by scaling down the network output (Epsilon), mitigating the input mismatch between training and sampling.Experiments on various diffusion frameworks (ADM, DDPM\/DDIM, EDM, LDM), unconditional and conditional settings, and deterministic vs. stochastic sampling verify the effectiveness of our method.Remarkably, our ADM-ES, as a SOTA stochastic sampler, obtains 2.17 FID on CIFAR-10 under 100-step unconditional generation.The code is available at https:\/\/github.com\/forever208\/ADM-ESand https:\/\/github.com\/forever208\/EDM-ESWe point out that the exposure bias problem in diffusion models lacks in-depth exploration.For example, there is no proper metric to quantify the exposure bias and no explicit error analysis for it.To shed light on exposure bias, we conduct a systematical investigation in this paper by first",
        "id":261276856
      }
    ],
    "negative_ctxs":[
      {
        "title":"",
        "text":"",
        "id":221373767
      },
      {
        "title":"",
        "text":"",
        "id":225062898
      },
      {
        "title":"Out-of-Sample Representation Learning for Knowledge Graphs",
        "text":"Many important problems can be formulated as reasoning in knowledge graphs. Representation learning has proved extremely effective for transductive reasoning, in which one needs to make new predictions for already observed entities. This is true for both attributed graphs (where each entity has an initial feature vector) and non-attributed graphs (where the only initial information derives from known relations with other entities). For out-of-sample reasoning, where one needs to make predictions for entities that were unseen at training time, much prior work considers attributed graph. However, this problem is surprisingly under-explored for non-attributed graphs. In this paper, we study the out-of-sample representation learning problem for non-attributed knowledge graphs, create benchmark datasets for this task, develop several models and baselines, and provide empirical analyses and comparisons of the proposed models and baselines.",
        "id":226283940
      }
    ]
  }
]