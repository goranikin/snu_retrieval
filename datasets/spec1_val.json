[
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there any research paper that can extract attributes from both a predefined label set and the surrounding context?",
    "positive_ctxs": [
      {
        "title": "AtTGen: Attribute Tree Generation for Real-World Attribute Joint Extraction",
        "text": "Attribute extraction aims to identify attribute names and the corresponding values from descriptive texts, which is the foundation for extensive downstream applications such as knowledge graph construction, search engines, and e-Commerce. In previous studies, attribute extraction is generally treated as a classification problem for predicting attribute types or a sequence tagging problem for labeling attribute values, where two paradigms, i.e., closed-world and open-world assumption, are involved. However, both of these paradigms have limitations in terms of real-world applications. And prior studies attempting to integrate these paradigms through ensemble, pipeline, and co-training models, still face challenges like cascading errors, high computational overhead, and difficulty in training. To address these existing problems, this paper presents Attribute Tree, a unified formulation for realworld attribute extraction application, where closed-world, open-world, and semi-open attribute extraction tasks are modeled uniformly. Then a text-to-tree generation model, AtTGen, is proposed to learn annotations from different scenarios efficiently and consistently. Experiments demonstrate that our proposed paradigm well covers various scenarios for real-world applications, and the model achieves state-ofthe-art, outperforming existing methods by a large margin on three datasets. Our code, pretrained model, and datasets are available at https://github.com/lsvih/AtTGen.",
        "id": 259370741
      }
    ],
    "negative_ctxs": [
      {
        "title": "Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling",
        "text": "Abstractive summarization models often generate inconsistent summaries containing factual errors or hallucinated content. Recent works focus on correcting factual errors in generated summaries via post-editing. Such correction models are trained using adversarial nonfactual summaries constructed using heuristic rules for injecting errors. However, generating non-factual summaries using heuristics often does not generalize well to actual model errors. In this work, we propose to generate hard, representative synthetic examples of nonfactual summaries through infilling language models. With this data, we train a more robust fact-correction model to post-edit the summaries to improve factual consistency. Through quantitative and qualitative experiments on two popular summarization datasets-CNN/DM and XSum-we show that our approach vastly outperforms prior methods in correcting erroneous summaries. Our model-FACTEDITimproves factuality scores by over ∼11 points on CNN/DM and over ∼31 points on XSum on average across multiple summarization models, producing more factual summaries while maintaining competitive summarization quality. 1",
        "id": 253098133
      },
      {
        "title": "Improving Sentiment Classification in Slovak Language",
        "text": "Using different neural network architectures is widely spread for many different NLP tasks. Unfortunately, most of the research is performed and evaluated only in English language and minor languages are often omitted. We believe using similar architectures for other languages can show interesting results. In this paper, we present our study on methods for improving sentiment classification in Slovak language. We performed several experiments for two different datasets, one containing customer reviews, the other one general Twitter posts. We show comparison of performance of different neural network architectures and also different word representations. We show that another improvement can be achieved by using a model ensemble. We performed experiments utilizing different methods of model ensemble. Our proposed models achieved better results than previous models for both datasets. Our experiments showed also other potential research areas.",
        "id": 201639088
      },
      {
        "title": "DECEMBERT: Learning from Noisy Instructional Videos via Dense Captions and Entropy Minimization",
        "text": "Leveraging large-scale unlabeled web videos such as instructional videos for pre-training followed by task-specific finetuning has become the de facto approach for many videoand-language tasks. However, these instructional videos are very noisy, the accompanying ASR narrations are often incomplete, and can be irrelevant to or temporally misaligned with the visual content, limiting the performance of the models trained on such data. To address these issues, we propose an improved video-and-language pre-training method that first adds automatically-extracted dense region captions from the video frames as auxiliary text input, to provide informative visual cues for learning better video and language associations. Second, to alleviate the temporal misalignment issue, our method incorporates an entropy minimization-based constrained attention loss, to encourage the model to automatically focus on the correct caption from a pool of candidate ASR captions. Our overall approach is named DECEMBERT (Dense Captions and Entropy Minimization). Comprehensive experiments on three video-and-language tasks (text-to-video retrieval, video captioning, and video question answering) across five datasets demonstrate that our approach outperforms previous state-of-the-art methods. Ablation studies on pre-training and downstream tasks show that adding dense captions and constrained attention loss help improve the model performance. Lastly, we also provide attention visualization to show the effect of applying the proposed constrained attention loss. 1",
        "id": 235097643
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "What paper first uses decoupled workers in distributed RL system?",
    "positive_ctxs": [
      {
        "title": "SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores",
        "text": "The ever-growing complexity of reinforcement learning (RL) tasks demands a distributed RL system to efficiently generate and process a massive amount of data to train intelligent agents. However, existing open-source libraries suffer from various limitations, which impede their practical use in challenging scenarios where largescale training is necessary. While industrial systems from OpenAI and DeepMind have achieved successful large-scale RL training, their system architecture and implementation details remain undisclosed to the community. In this paper, we present a novel system abstraction on the dataflows of RL training, which unifies practical RL training across diverse applications into a general and flexible framework and enables fine-grained system-level optimizations. Following this abstraction, we develop a scalable, efficient, and extensible distributed RL system called ReaLly Scalable RL (SRL). The system architecture of SRL separates major RL computation components and allows massively parallelized training. We also introduce a collection of techniques to further optimize the system performance. Moreover, SRL offers user-friendly and extensible interfaces, which facilitate the development of customized algorithms. Our evaluation shows that SRL outperforms existing academic libraries in both a single machine and a medium-sized cluster. In a large-scale cluster, the novel architecture of SRL leads to up to 3.7x speedup compared to the design choices adopted by the existing libraries. We also conduct a direct benchmark comparison to Ope-nAI's industrial system, Rapid [5], in the challenging hide-and-seek environment[2]. SRL reproduces the same solution as reported by OpenAI with up to 5x speedup in wall-clock time. Furthermore, we also examine the performance of SRL in a much harder variant of the hide-and-seek environment and achieve substantial learning speedup by scaling SRL to over 15k CPU cores and 32 A100 GPUs. * Equal contribution.Notably, SRL is the first in the academic community to perform RL experiments at such a large scale.",
        "id": 259287121
      }
    ],
    "negative_ctxs": [
      {
        "title": "Unified Lexicon and Unified Morphosyntactic Specifications for Written and Spoken Italian",
        "text": "The goal of this paper is (1) to illustrate a specific procedure for merging different monolingual lexicons, focusing on techniques for detecting and mapping equivalent lexical entries, and (2) to sketch a production model that enables one to obtain lexical resources via unification of existing data. We describe the creation of a Unified Lexicon (UL) from a common sample of the Italian PAROLE/SIMPLE/CLIPS phonological lexicon and of the Italian LCSTAR pronunciation lexicon. We expand previous experiments carried out at ILC-CNR: based on a detailed mechanism for mapping grammatical classifications of candidate UL entries, a consensual set of Unified Morphosyntactic Specifications (UMS) shared by lexica for the written and spoken areas is proposed. The impact of the UL on cross-validation issues is analysed: by looking into conflicts, mismatches and diverging classifications can be detected in both resources. The work presented is in line with the activities promoted by ELRA towards the development of methods for packaging new language resources by combining independently created resources, and was carried out as part of the ELRA Production Committee activities. ELRA aims to exploit the UL experience to carry out such merging activities for resources available on the ELRA catalogue in order to fulfill the users' needs.",
        "id": 10313322
      },
      {
        "title": "Issues in evaluating semantic spaces using word analogies",
        "text": "The offset method for solving word analogies has become a standard evaluation tool for vector-space semantic models: it is considered desirable for a space to represent semantic relations as consistent vector offsets. We show that the method's reliance on cosine similarity conflates offset consistency with largely irrelevant neighborhood structure, and propose simple baselines that should be used to improve the utility of the method in vector space evaluation.",
        "id": 7906247
      },
      {
        "title": "A Semi-Automatic Evaluation Scheme: Automated Nuggetization for Manual Annotation",
        "text": "In this paper we describe automatic information nuggetization and its application to text comparison. More specifically, we take a close look at how machine-generated nuggets can be used to create evaluation material. A semiautomatic annotation scheme is designed to produce gold-standard data with exceptionally high inter-human agreement.",
        "id": 2073916
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "What papers should I refer to if I want to explore datasets and shared tasks designed for the purpose of fact verification using evidence from Wikipedia?",
    "positive_ctxs": [
      {
        "title": "FEVER: a large-scale dataset for Fact Extraction and VERification",
        "text": "In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from.The claims are classified as SUPPORTED, RE-FUTED or NOTENOUGHINFO by annotators achieving 0.6841 in Fleiss κ. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87%, while if we ignore the evidence we achieve 50.91%. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.",
        "id": 4711425
      }
    ],
    "negative_ctxs": [
      {
        "title": "A Large Scale Arabic Sentiment Lexicon for Arabic Opinion Mining",
        "text": "Most opinion mining methods in English rely successfully on sentiment lexicons, such as English SentiWordnet (ESWN). While there have been efforts towards building Arabic sentiment lexicons, they suffer from many deficiencies: limited size, unclear usability plan given Arabic's rich morphology, or nonavailability publicly. In this paper, we address all of these issues and produce the first publicly available large scale Standard Arabic sentiment lexicon (Ar-SenL) using a combination of existing resources: ESWN, Arabic WordNet, and the Standard Arabic Morphological Analyzer (SAMA). We compare and combine two methods of constructing this lexicon with an eye on insights for Arabic dialects and other low resource languages. We also present an extrinsic evaluation in terms of subjectivity and sentiment analysis.",
        "id": 16203230
      },
      {
        "title": "Zipfian corruptions for robust POS tagging",
        "text": "Inspired by robust generalization and adversarial learning we describe a novel approach to learning structured perceptrons for part-ofspeech (POS) tagging that is less sensitive to domain shifts. The objective of our method is to minimize average loss under random distribution shifts. We restrict the possible target distributions to mixtures of the source distribution and random Zipfian distributions. Our algorithm is used for POS tagging and evaluated on the English Web Treebank and the Danish Dependency Treebank with an average 4.4% error reduction in tagging accuracy.",
        "id": 6328989
      },
      {
        "title": "On the Structural Complexity of Natural Language Sentences",
        "text": "Artificial I ntclligcn cc I,a, bovatory Ma.ss~cclmsetts Insdtttt(; of :l~cttnology l{m 7(57, 545 Technology Square (~atnbridgc, Massac.husetts, USA, 02] 239AbstractThe objective of this pal)er is to [brmalize the intuition al)out l,he comph;xity of syntactic structures. We propose a definition of structm:al COml)h'xity such that sentences ranked by our definition as more COml)h;x are gen(;rally more diI'ficult lbr humans to process. We justify the definition by showing how it is ahle to account for several seemingly unrelated phenomena in natural languages.",
        "id": 7424558
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper first proposed to combine pretrained masked language models (BERT) and discrete diffusion language models?",
    "positive_ctxs": [
      {
        "title": "DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models",
        "text": "We present DiffusionBERT, a new generative masked language model based on discrete diffusion models. Diffusion models and many pre-trained language models have a shared training objective, i.e., denoising, making it possible to combine the two powerful models and enjoy the best of both worlds. On the one hand, diffusion models offer a promising training strategy that helps improve the generation quality. On the other hand, pre-trained denoising language models (e.g., BERT) can be used as a good initialization that accelerates convergence. We explore training BERT to learn the reverse process of a discrete diffusion process with an absorbing state and elucidate several designs to improve it. First, we propose a new noise schedule for the forward diffusion process that controls the degree of noise added at each step based on the information of each token. Second, we investigate several designs of incorporating the time step into BERT. Experiments on unconditional text generation demonstrate that DiffusionBERT achieves significant improvement over existing diffusion models for text (e.g., D3PM and Diffusion-LM) and previous generative masked language models in terms of perplexity and BLEU score. 1",
        "id": 254044147
      }
    ],
    "negative_ctxs": [
      {
        "title": "Localization of Fake News Detection via Multitask Transfer Learning",
        "text": "The use of the internet as a fast medium of spreading fake news reinforces the need for computational tools that combat it. Techniques that train fake news classifiers exist, but they all assume an abundance of resources including large labeled datasets and expert-curated corpora, which low-resource languages may not have. In this work, we make two main contributions: First, we alleviate resource scarcity by constructing the first expertly-curated benchmark dataset for fake news detection in Filipino, which we call \"Fake News Filipino.\" Second, we benchmark Transfer Learning (TL) techniques and show that they can be used to train robust fake news classifiers from little data, achieving 91% accuracy on our fake news dataset, reducing the error by 14% compared to established few-shot baselines. Furthermore, lifting ideas from multitask learning, we show that augmenting transformer-based transfer techniques with auxiliary language modeling losses improves their performance by adapting to writing style. Using this, we improve TL performance by 4-6%, achieving an accuracy of 96% on our best model. Lastly, we show that our method generalizes well to different types of news articles, including political news, entertainment news, and opinion articles.",
        "id": 204800316
      },
      {
        "title": "Published as a conference paper at ICLR 2021 LEARNING TO RECOMBINE AND RESAMPLE DATA FOR COMPOSITIONAL GENERALIZATION",
        "text": "Flexible neural sequence models outperform grammar-and automaton-based counterparts on a variety of tasks. However, neural models perform poorly in settings requiring compositional generalization beyond the training data-particularly to rare or unseen subsequences. Past work has found symbolic scaffolding (e.g. grammars or automata) essential in these settings. We describe R&R, a learned data augmentation scheme that enables a large category of compositional generalizations without appeal to latent symbolic structure. R&R has two components: recombination of original training examples via a prototype-based generative model and resampling of generated examples to encourage extrapolation. Training an ordinary neural sequence model on a dataset augmented with recombined and resampled examples significantly improves generalization in two language processing problems-instruction following (SCAN) and morphological analysis (SIGMORPHON 2018)-where R&R enables learning of new constructions and tenses from as few as eight initial examples. . Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In",
        "id": 222208634
      },
      {
        "title": "SHINE: SHARING THE INVERSE ESTIMATE FROM THE FORWARD PASS FOR BI-LEVEL OPTIMIZATION AND IM- PLICIT MODELS",
        "text": "In recent years, implicit deep learning has emerged as a method to increase the effective depth of deep neural networks. While their training is memory-efficient, they are still significantly slower to train than their explicit counterparts. In Deep Equilibrium Models (DEQs), the training is performed as a bi-level problem, and its computational complexity is partially driven by the iterative inversion of a huge Jacobian matrix. In this paper, we propose a novel strategy to tackle this computational bottleneck from which many bi-level problems suffer. The main idea is to use the quasi-Newton matrices from the forward pass to efficiently approximate the inverse Jacobian matrix in the direction needed for the gradient computation. We provide a theorem that motivates using our method with the original forward algorithms. In addition, by modifying these forward algorithms, we further provide theoretical guarantees that our method asymptotically estimates the true implicit gradient. We empirically study this approach and the recent Jacobian-Free method in different settings, ranging from hyperparameter optimization to large Multiscale DEQs (MDEQs) applied to CIFAR and ImageNet. Both methods reduce significantly the computational cost of the backward pass. While SHINE has a clear advantage on hyperparameter optimization problems, both methods attain similar computational performances for larger scale problems such as MDEQs at the cost of a limited performance drop compared to the original models.",
        "id": 235266229
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper first shows that it is possible to maintain high LLM reasoning performance with in-context examples that are absurdly wrong?",
    "positive_ctxs": [
      {
        "title": "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters",
        "text": "Chain-of-Thought (CoT) prompting can dramatically improve the multi-step reasoning abilities of large language models (LLMs). CoT explicitly encourages the LLM to generate intermediate rationales for solving a problem, by providing a series of reasoning steps in the demonstrations. Despite its success, there is still little understanding of what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance. In this paper, we show that CoT reasoning is possible even with invalid demonstrations-prompting with invalid reasoning steps can achieve over 80-90% of the performance obtained using CoT under various metrics, while still generating coherent lines of reasoning during inference. Further experiments show that other aspects of the rationales, such as being relevant to the query and correctly ordering the reasoning steps, are much more important for effective CoT reasoning. Overall, these findings both deepen our understanding of CoT prompting, and open up new questions regarding LLMs' capability to learn to reason in context. 1",
        "id": 254877569
      }
    ],
    "negative_ctxs": [
      {
        "title": "Joint Estimation and Analysis of Risk Behavior Ratings in Movie Scripts",
        "text": "Exposure to violent, sexual, or substanceabuse content in media increases the willingness of children and adolescents to imitate similar behaviors. Computational methods that identify portrayals of risk behaviors from audio-visual cues are limited in their applicability to films in post-production, where modifications might be prohibitively expensive. To address this limitation, we propose a model that estimates content ratings based on the language use in movie scripts, making our solution available at the earlier stages of creative production. Our model significantly improves the state-of-the-art by adapting novel techniques to learn better movie representations from the semantic and sentiment aspects of a character's language use, and by leveraging the co-occurrence of risk behaviors, following a multi-task approach. Additionally, we show how this approach can be useful to learn novel insights on the joint portrayal of these behaviors, and on the subtleties that filmmakers may otherwise not pick up on.",
        "id": 221896980
      },
      {
        "title": "Situated Dialogue Learning through Procedural Environment Generation",
        "text": "We teach goal-driven agents to interactively act and speak in situated environments by training on generated curriculums. Our agents operate in LIGHT (Urbanek et al., 2019)-a large-scale crowd-sourced fantasy text adventure game wherein an agent perceives and interacts with the world through textual natural language. Goals in this environment take the form of character-based quests, consisting of personas and motivations. We augment LIGHT by learning to procedurally generate additional novel textual worlds and quests to create a curriculum of steadily increasing difficulty for training agents to achieve such goals. In particular, we measure curriculum difficulty in terms of the rarity of the quest in the original training distribution-an easier environment is one that is more likely to have been found in the unaugmented dataset. An ablation study shows that this method of learning from the tail of a distribution results in significantly higher generalization abilities as measured by zeroshot performance on never-before-seen quests.",
        "id": 238418980
      },
      {
        "title": "Few-Shot Table Understanding: A Benchmark Dataset and Pre-Training Baseline",
        "text": "Few-shot table understanding is a critical and challenging problem in real-world scenario as annotations over large amount of tables are usually costly. Pre-trained language models (PLMs), which have recently flourished on tabular data, have demonstrated their effectiveness for table understanding tasks. However, fewshot table understanding is rarely explored due to the deficiency of public table pre-training corpus and well-defined downstream benchmark tasks, especially in Chinese. In this paper, we establish a benchmark dataset, FewTUD, which consists of 5 different tasks with human annotations to systematically explore the fewshot table understanding in depth. Since there is no large number of public Chinese tables, we also collect a large-scale, multi-domain tabular corpus to facilitate future Chinese table pre-training, which includes one million tables and related natural language text with auxiliary supervised interaction signals. Finally, we present FewTPT, a novel table PLM with rich interactions over tabular data, and evaluate its performance comprehensively on the benchmark. Our dataset and model will be released to the public soon.",
        "id": 252375595
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that examines how prompt tuning can be used for domain transfer?",
    "positive_ctxs": [
      {
        "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
        "text": "In this work, we explore \"prompt tuning,\" a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient \"prompt ensembling.\" We release code and model checkpoints to reproduce our experiments. 1ReferencesRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,  Danilo Giampiccolo, Bernardo Magnini, and Idan  Szpektor. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognising textual entailment, volume 6, pages 6-4. Venice. . 2019a. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
        "id": 233296808
      }
    ],
    "negative_ctxs": [
      {
        "title": "Learning a Scanning Understanding for \"Real-world\" Library Categorization",
        "text": "This paper describes, compares, and evaluates three different approaches for learning a semantic classification of library titles: 1) syntactically condensed titles, 2) complete titles, and 3) titles without insignificant words are used for learning the classification in connectionist recurrent plausibility networks. In particular, we demonstrate in this paper that automatically derived feature representations and recurrent plausibility networks can scale up to several thousand library titles and reach almost perfect classification accuracy (>98%) compared to a real-world library classification.",
        "id": 8925228
      },
      {
        "title": "Expectations of Word Sense in Parallel Corpora",
        "text": "Given a parallel corpus, if two distinct words in language A, a 1 and a 2 , are aligned to the same word b 1 in language B, then this might signal that b 1 is polysemous, or it might signal a 1 and a 2 are synonyms. Both assumptions with successful work have been put forward in the literature. We investigate these assumptions, along with other questions of word sense, by looking at sampled parallel sentences containing tokens of the same type in English, asking how often they mean the same thing when they are: 1. aligned to the same foreign type; and 2. aligned to different foreign types. Results for French-English and Chinese-English parallel corpora show similar behavior: Synonymy is only very weakly the more prevalent scenario, where both cases regularly occur.",
        "id": 467937
      },
      {
        "title": "",
        "text": "",
        "id": 220444939
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that investigates how hierarchical structures within transformers enhance task-oriented dialogue systems?",
    "positive_ctxs": [
      {
        "title": "Hierarchical Transformer for Task Oriented Dialog Systems",
        "text": "Generative models for dialog systems have gained much interest because of the recent success of RNN and Transformer based models in tasks like question answering and summarization. Although the task of dialog response generation is generally seen as a sequence to sequence (Seq2Seq) problem, researchers in the past have found it challenging to train dialog systems using the standard Seq2Seq models. Therefore, to help the model learn meaningful utterance and conversation level features, Sordoni et al. (2015b); Serban et al.(2016)proposed Hierarchical RNN architecture, which was later adopted by several other RNN based dialog systems. With the transformer-based models dominating the seq2seq problems lately, the natural question to ask is the applicability of the notion of hierarchy in transformer based dialog systems. In this paper, we propose a generalized framework for Hierarchical Transformer Encoders and show how a standard transformer can be morphed into any hierarchical encoder, including HRED and HIBERT like models, by using specially designed attention masks and positional encodings. We demonstrate that Hierarchical Encoding helps achieve better natural language understanding of the contexts in transformer-based models for task-oriented dialog systems through a wide range of experiments. The code and data for all experiments in this paper has been open-sourced 1 2 .",
        "id": 226965200
      }
    ],
    "negative_ctxs": [
      {
        "title": "Differentiable Euler Characteristic Transforms for Shape Classification",
        "text": "The Euler Characteristic Transform (ECT) has proven to be a powerful representation, combining geometrical and topological characteristics of shapes and graphs.However, the ECT was hitherto unable to learn task-specific representations.We overcome this issue and develop a novel computational layer that enables learning the ECT in an end-to-end fashion.Our method DECT is fast and computationally efficient, while exhibiting performance on a par with more complex models in both graph and point cloud classification tasks.Moreover, we show that this seemingly unexpressive statistic still provides the same topological expressivity as more complex topological deep learning layers provide.",
        "id": 263835059
      },
      {
        "title": "Discrimination between Similar Languages, Varieties and Dialects using CNN-and LSTM-based Deep Neural Networks",
        "text": "In this paper, we describe a system (CGLI) for discriminating similar languages, varieties and dialects using convolutional neural networks (CNNs) and long short-term memory (LSTM) neural networks. We have participated in the Arabic dialect identification sub-task of DSL 2016 shared task for distinguishing different Arabic language texts under closed submission track. Our proposed approach is language independent and works for discriminating any given set of languages, varieties and dialects. We have obtained 43.29% weighted-F1 accuracy in this subtask using CNN approach using default network parameters.",
        "id": 3116257
      },
      {
        "title": "Dependency Based Chinese Sentence Realization",
        "text": "This paper describes log-linear models for a general-purpose sentence realizer based on dependency structures. Unlike traditional realizers using grammar rules, our method realizes sentences by linearizing dependency relations directly in two steps. First, the relative order between head and each dependent is determined by their dependency relation. Then the best linearizations compatible with the relative order are selected by log-linear models. The log-linear models incorporate three types of feature functions, including dependency relations, surface words and headwords. Our approach to sentence realization provides simplicity, efficiency and competitive accuracy. Trained on 8,975 dependency structures of a Chinese Dependency Treebank, the realizer achieves a BLEU score of 0.8874.",
        "id": 438829
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Are there any examples of using dense phrase retrieval systems in the automatic curation of entity dictionaries?",
    "positive_ctxs": [
      {
        "title": "Automatic Creation of Named Entity Recognition Datasets by Querying Phrase Representations",
        "text": "Most weakly supervised named entity recognition (NER) models rely on domain-specific dictionaries provided by experts. This approach is infeasible in many domains where dictionaries do not exist. While a phrase retrieval model was used to construct pseudo-dictionaries with entities retrieved from Wikipedia automatically in a recent study, these dictionaries often have limited coverage because the retriever is likely to retrieve popular entities rather than rare ones. In this study, we present a novel framework, HighGEN, that generates NER datasets with high-coverage pseudo-dictionaries. Specifically, we create entity-rich dictionaries with a novel search method, called phrase embedding search, which encourages the retriever to search a space densely populated with various entities. In addition, we use a new verification process based on the embedding distance between candidate entity mentions and entity types to reduce the false-positive noise in weak labels generated by high-coverage dictionaries. We demonstrate that HighGEN outperforms the previous best model by an average F1 score of 4.7 across five NER benchmark datasets.",
        "id": 252907578
      }
    ],
    "negative_ctxs": [
      {
        "title": "Meta Self-Refinement for Robust Learning with Weak Supervision",
        "text": "Training deep neural networks (DNNs) under weak supervision has attracted increasing research attention as it can significantly reduce the annotation cost. However, labels from weak supervision can be noisy, and the high capacity of DNNs enables them to easily overfit the label noise, resulting in poor generalization. Recent methods leverage self-training to build noiseresistant models, in which a teacher trained under weak supervision is used to provide highly confident labels for teaching the students. Nevertheless, the teacher derived from such frameworks may have fitted a substantial amount of noise and therefore produce incorrect pseudolabels with high confidence, leading to severe error propagation. In this work, we propose Meta Self-Refinement (MSR), a noise-resistant learning framework, to effectively combat label noise from weak supervision. Instead of relying on a fixed teacher trained with noisy labels, we encourage the teacher to refine its pseudolabels. At each training step, MSR performs a meta gradient descent on the current mini-batch to maximize the student performance on a clean validation set. Extensive experimentation on eight NLP benchmarks demonstrates that MSR is robust against label noise in all settings and outperforms state-of-the-art methods by up to 11.4% in accuracy and 9.26% in F1 score.",
        "id": 248811149
      },
      {
        "title": "Duluth at SemEval-2016 Task 14 : Extending Gloss Overlaps to Enrich Semantic Taxonomies",
        "text": "This paper describes the Duluth systems that participated in Task 14 of SemEval 2016, Semantic Taxonomy Enrichment. There were three related systems in the formal evaluation which are discussed here, along with numerous post-evaluation runs. All of these systems identified synonyms between Word-Net and other dictionaries by measuring the gloss overlaps between them. These systems perform better than the random baseline and one post-evaluation variation was within a respectable margin of the median result attained by all participating systems.",
        "id": 776002
      },
      {
        "title": "Lexicon and grammar in probabilistic tagging of written English",
        "text": "The paper describes the development of software for automatic grammatical ana]ysi$ of unl~'Ui~, unedited English text at the Unit for Compm= Research on the Ev~li~h Language (UCREL) at the Univet~ of Lancaster. The work is ~n'nmtly funded by IBM and carried out in collaboration with colleagues at IBM UK (W'~) and IBM Yorktown Heights. The paper will focus on the lexicon component of the word raging system, the UCREL grammar, the datal~zlks of parsed sentences, and the tools that have been written to support developmem of these comlm~ems. ~ wozk has applications to speech technology, sl~lfing conectim, end other areas of natural lmlguage pngessil~ ~y, our goal is to provide a language model using transin'ca statistics to di.~.nbigu~ al.:mative 1~ for a speech .:a~nicim device.",
        "id": 14288879
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there a paper that uses the tree structure of math equations in autoregressive language models?",
    "positive_ctxs": [
      {
        "title": "Tree-Based Representation and Generation of Natural and Mathematical Language",
        "text": "Mathematical language in scientific communications and educational scenarios is important yet relatively understudied compared to natural languages. Recent works on mathematical language focus either on representing stand-alone mathematical expressions, especially in their natural tree format, or mathematical reasoning in pre-trained natural language models. Existing works on jointly modeling and generating natural and mathematical languages simply treat mathematical expressions as text, without accounting for the rigid structural properties of mathematical expressions. In this paper, we propose a series of modifications to existing language models to jointly represent and generate text and math: representing mathematical expressions as sequences of node tokens in their operator tree format, using math symbol and tree position embeddings to preserve the semantic and structural properties of mathematical expressions, and using a constrained decoding method to generate mathematically valid expressions. We ground our modifications in GPT-2, resulting in a model MathGPT, and demonstrate that it outperforms baselines on mathematical expression generation tasks.",
        "id": 256901172
      }
    ],
    "negative_ctxs": [
      {
        "title": "Text Corpora and the Challenge of Newly Written Languages",
        "text": "Text corpora represent the foundation on which most natural language processing systems rely. However, for many languages, collecting or building a text corpus of a sufficient size still remains a complex issue, especially for corpora that are accessible and distributed under a clear license allowing modification (such as annotation) and further resharing. In this paper, we review the sources of text corpora usually called upon to fill the gap in low-resource contexts, and how crowdsourcing has been used to build linguistic resources. Then, we present our own experiments with crowdsourcing text corpora and an analysis of the obstacles we encountered. Although the results obtained in terms of participation are still unsatisfactory, we advocate that the effort towards a greater involvement of the speakers should be pursued, especially when the language of interest is newly written.",
        "id": 218973897
      },
      {
        "title": "EmoTweet-28: A Fine-Grained Emotion Corpus for Sentiment Analysis",
        "text": "This paper describes EmoTweet-28, a carefully curated corpus of 15,553 tweets annotated with 28 emotion categories for the purpose of training and evaluating machine learning models for emotion classification. EmoTweet-28 is, to date, the largest tweet corpus annotated with fine-grained emotion categories. The corpus contains annotations for four facets of emotion: valence, arousal, emotion category and emotion cues. We first used small-scale content analysis to inductively identify a set of emotion categories that characterize the emotions expressed in microblog text. We then expanded the size of the corpus using crowdsourcing. The corpus encompasses a variety of examples including explicit and implicit expressions of emotions as well as tweets containing multiple emotions. EmoTweet-28 represents an important resource to advance the development and evaluation of more emotion-sensitive systems.",
        "id": 6266911
      },
      {
        "title": "Fearless Steps APOLLO: Advanced Naturalistic Corpora Development",
        "text": "In this study, we present the Fearless Steps APOLLO Community Resource, a collection of audio and corresponding meta-data diarized from the NASA Apollo Missions. Massive naturalistic speech data which is time-synchronized, without any human subject privacy constraints is very rare and difficult to organize, collect, and deploy. The Apollo Missions Audio is the largest collection of multi-speaker multi-channel data, where over 600 personnel are communicating over multiple missions to achieve strategic space exploration goals. A total of 12 manned missions over a six-year period produced extensive 30-track 1-inch analog tapes containing over 150,000 hours of audio. This presents the wider research community a unique opportunity to extract multi-modal knowledge in speech science, team cohesion and group dynamics, and historical archive preservation. We aim to make this entire resource and supporting speech technology meta-data creation publicly available as a Community Resource for the development of speech and behavioral science. Here we present the development of this community resource, our outreach efforts, and technological developments resulting from this data. We finally discuss the planned future directions for this community resource.",
        "id": 252624729
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Can we reduce visual tokens in vision transformers right from the beginning?",
    "positive_ctxs": [
      {
        "title": "SparseFormer: Sparse Visual Recognition via Limited Latent Tokens",
        "text": "Human visual recognition is a sparse process, where only a few salient visual cues are attended to rather than traversing every detail uniformly.However, most current vision networks follow a dense paradigm, processing every single visual unit (e.g., pixel or patch) in a uniform manner.In this paper, we challenge this dense paradigm and present a new method, coined SparseFormer, to imitate human's sparse visual recognition in an end-to-end manner.Sparse-Former learns to represent images using a highly limited number of tokens (down to 49) in the latent space with sparse feature sampling procedure instead of processing dense units in the original pixel space.Therefore, Sparse-Former circumvents most of dense operations on the image space and has much lower computational costs.Experiments on the ImageNet classification benchmark dataset show that SparseFormer achieves performance on par with canonical or well-established models while offering better accuracy-throughput tradeoff.Moreover, the design of our network can be easily extended to the video classification with promising performance at lower computational costs.We hope that our work can provide an alternative way for visual modeling and inspire further research on sparse neural architectures.The code will be publicly available at https://github.com/showlab/sparseformer.",
        "id": 258041281
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 194737667
      },
      {
        "title": "Distributed Listening: A Parallel Processing Approach to Automatic Speech Recognition",
        "text": "While speech recognition systems have come a long way in the last thirty years, there is still room for improvement. Although readily available, these systems are sometimes inaccurate and insufficient. The research presented here outlines a technique called Distributed Listening which demonstrates noticeable improvements to existing speech recognition methods. The Distributed Listening architecture introduces the idea of multiple, parallel, yet physically separate automatic speech recognizers called listeners. Distributed Listening also uses a piece of middleware called an interpreter. The interpreter resolves multiple interpretations using the Phrase Resolution Algorithm (PRA). These efforts work together to increase the accuracy of the transcription of spoken utterances.",
        "id": 11611001
      },
      {
        "title": "Generating Natural Sentences by Using Shallow Discourse Information",
        "text": "One of the biggest defects of natural language generation systems is that the output sentences are unnatural and contain many redundancies. Machine translation (MT) users, for instance, often get tired of reading the output of MT because of this problem. In this paper, we summarize the results of our analysis of human translation in terms of the use of discourse information to generate target-language sentences, and describe our attempt to generate natural output by referring to a simple discourse model that consists of a syntactic parser's output for each sentence in the discourse.",
        "id": 16619607
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Can you point me towards research on contrastive learning methods used for fine-tuning sentence representations, where in-batch negatives may sometimes unintentionally be similar to the positive examples?",
    "positive_ctxs": [
      {
        "title": "ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer",
        "text": "Learning high-quality sentence representations benefits a wide range of natural language processing tasks. Though BERT-based pretrained language models achieve high performance on many downstream tasks, the native derived sentence representations are proved to be collapsed and thus produce a poor performance on the semantic textual similarity (STS) tasks. In this paper, we present ConSERT, a Contrastive Framework for Self-Supervised SEntence Representation Transfer, that adopts contrastive learning to fine-tune BERT in an unsupervised and effective way. By making use of unlabeled texts, ConSERT solves the collapse issue of BERT-derived sentence representations and make them more applicable for downstream tasks. Experiments on STS datasets demonstrate that ConSERT achieves an 8% relative improvement over the previous state-of-the-art, even comparable to the supervised SBERT-NLI. And when further incorporating NLI supervision, we achieve new stateof-the-art performance on STS tasks. Moreover, ConSERT obtains comparable results with only 1000 samples available, showing its robustness in data scarcity scenarios.",
        "id": 235187266
      },
      {
        "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings",
        "text": "This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using \"entailment\" pairs as positives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2% and 2.2% improvement compared to previous best results. We also show-both theoretically and empirically-that contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available. 1",
        "id": 233296292
      }
    ],
    "negative_ctxs": [
      {
        "title": "Building and curating conversational corpora for diversity-aware language science and technology",
        "text": "We present an analysis pipeline and best practice guidelines for building and curating corpora of everyday conversation in diverse languages. Surveying language documentation corpora and other resources that cover 67 languages and varieties from 28 phyla, we describe the compilation and curation process, specify minimal properties of a unified format for interactional data, and develop methods for quality control that take into account turn-taking and timing. Two case studies show the broad utility of conversational data for (i) charting human interactional infrastructure and (ii) tracing challenges and opportunities for current ASR solutions. Linguistically diverse conversational corpora can provide new insights for the language sciences and stronger empirical foundations for language technology.",
        "id": 247362437
      },
      {
        "title": "NLP Corpus Observatory -Looking for Constellations in Parallel Corpora to Improve Learners' Collocational Skills",
        "text": "The use of corpora in language learning, both in classroom and self-study situations, has proven useful. Investigations into technology use show a benefit for learners that are able to work with corpus data using easily accessible technology. But relatively little work has been done on exploring the possibilities of parallel corpora for language learning applications.Our work described in this paper explores the applicability of a parallel corpus enhanced with several layers generated by NLP techniques for extracting collocations that are noncompositional and thus indispensable to learn. We identify constellations, i.e. combinations of intra-and interlingual relations, calculate association scores on each relation and, based thereon, a joint score for each constellation. This way, we are able to find relevant collocations for different types of constellations.We evaluate our approach and discuss scenarios in which language learners can playfully explore collocations. Our explorative web tool is freely accessible, generates collocation dictionaries on the fly, and links them to example sentences to ensure context embedding.Gerold Schneider and Johannes Graën 2018. NLP Corpus Observatory -Looking for constellations in parallel corpora to improve learners' collocational skills.",
        "id": 108381061
      },
      {
        "title": "Generating Syntactic Paraphrases",
        "text": "We study the automatic generation of syntactic paraphrases using four different models for generation: data-to-text generation, textto-text generation, text reduction and text expansion, We derive training data for each of these tasks from the WebNLG dataset and we show (i) that conditioning generation on syntactic constraints effectively permits the generation of syntactically distinct paraphrases for the same input and (ii) that exploiting different types of input (data, text or data+text) further increases the number of distinct paraphrases that can be generated for a given input.",
        "id": 53083307
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Where can I find a multilingual corpus that includes reviews on DVDs, music, and books for the purposes of cross-language sentiment classification research?",
    "positive_ctxs": [
      {
        "title": "Cross-Language Text Classification using Structural Correspondence Learning",
        "text": "We present a new approach to crosslanguage text classification that builds on structural correspondence learning, a recently proposed theory for domain adaptation. The approach uses unlabeled documents, along with a simple word translation oracle, in order to induce taskspecific, cross-lingual word correspondences. We report on analyses that reveal quantitative insights about the use of unlabeled data and the complexity of interlanguage correspondence modeling.We conduct experiments in the field of cross-language sentiment classification, employing English as source language, and German, French, and Japanese as target languages. The results are convincing; they demonstrate both the robustness and the competitiveness of the presented ideas.",
        "id": 12360898
      }
    ],
    "negative_ctxs": [
      {
        "title": "Beyond Linguistic Equivalence. An Empirical Study of Translation Evaluation in a Translation Learner Corpus",
        "text": "The realisation that fully automatic translation in many settings is still far from producing output that is equal or superior to human translation has lead to an intense interest in translation evaluation in the MT community. However, research in this field, by now, has not only largely ignored the tremendous amount of relevant knowledge available in a closely related discipline, namely translation studies, but also failed to provide a deeper understanding of the nature of \"translation errors\" and \"translation quality\". This paper presents an empirical take on the latter concept, translation quality, by comparing human and automatic evaluations of learner translations in the KOPTE corpus. We will show that translation studies provide sophisticated concepts for translation quality estimation and error annotation. Moreover, by applying well-established MT evaluation scores, namely BLEU and Meteor, to KOPTE learner translations that were graded by a human expert, we hope to shed light on properties (and potential shortcomings) of these scores.",
        "id": 15966179
      },
      {
        "title": "A Direction of MT Development 1",
        "text": "",
        "id": 16735270
      },
      {
        "title": "FinSim4-ESG Shared Task: Learning Semantic Similarities for the Financial Domain. Extended edition to ESG insights",
        "text": "This paper describes FinSim4-ESG 1 shared task organized in the 4th FinNLP workshopwhich is held in conjunction with the IJCAI-ECAI-2022 confer-enceThis year, the FinSim4 is extended to the Environment, Social and Government (ESG) insights and proposes two subtasks, one for ESG Taxonomy Enrichment and the other for Sustainable Sentence Prediction. Among the 28 teams registered to the shared task, a total of 8 teams submitted their systems results and 6 teams also submitted a paper to describe their method. The winner of each subtask shows good performance results of 0.85% and 0.95% in terms of accuracy, respectively.",
        "id": 256460896
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper proposes a memory-efficient optimizer considering the confidence of each update during the optimization?",
    "positive_ctxs": [
      {
        "title": "CAME: Confidence-guided Adaptive Memory Efficient Optimization",
        "text": "Adaptive gradient methods, such as Adam and LAMB, have demonstrated excellent performance in the training of large language models. Nevertheless, the need for adaptivity requires maintaining second-moment estimates of the per-parameter gradients, which entails a high cost of extra memory overheads. To solve this problem, several memory-efficient optimizers (e.g., Adafactor) have been proposed to obtain a drastic reduction in auxiliary memory usage, but with a performance penalty. In this paper, we first study a confidence-guided strategy to reduce the instability of existing memory efficient optimizers. Based on this strategy, we propose CAME to simultaneously achieve two goals: fast convergence as in traditional adaptive methods, and low memory usage as in memory-efficient methods. Extensive experiments demonstrate the training stability and superior performance of CAME across various NLP tasks such as BERT and GPT-2 training. Notably, for BERT pre-training on the large batch size of 32,768, our proposed optimizer attains faster convergence and higher accuracy compared with the Adam optimizer. The implementation of CAME is publicly available 1 .",
        "id": 259342823
      }
    ],
    "negative_ctxs": [
      {
        "title": "AN EFFICIENT SYNTACTIC TAGGING TOOL FOR CORPORA @ A BSTRA CT",
        "text": "The tree bank is an important resources tbr MT and linguistics researches, but it requires that large number of sentences be annotated with syntactic information. It is time consuming and troublesome, and dil'ficult to keep consistency, if' annotation is done manually. In this paper, wc presented a new technique for the semi-automatic tagging of Chinese tcxt. The system takes as input Chinese text, and outputs the syntactically tagged sentence(dependency tree). We use dependency grammar and employ a stack based shift/reduce context-dependent parser as the tagging mechanism. The system works in human-machine cooperative way, in which the machine can acquire tagging rules from human intervention. The automation level can be improved step by step by accumulating rules during annotation. In addition, good consistency of tagging is guaranteed.",
        "id": 5365055
      },
      {
        "title": "Annotating COMPARA, a Grammar-aware Parallel Corpus",
        "text": "In this paper we describe the annotation of COMPARA, currently the largest post-edited parallel corpora which includes Portuguese. We describe the motivation, the results so far, and the way the corpus is being annotated. We also provide the first grounded results about syntactical ambiguity in Portuguese. Finally, we discuss some interesting problems in this connection.",
        "id": 3012359
      },
      {
        "title": "Automatie Extraction of Stochastic Lexicalized Tree Grammars from Treebanks",
        "text": "We present a method for the extraction of stochastic lexicalized tree grammars (S-LTG) of different complexities from existing treebanks, which allows us to analyze the relationship of a grammar automatically induced from a treebank wrt. its size, its complexity, and its predictive power on unseen data.Processing of different S-LTG is performed by a stochastic version of the two-stepEarly-based parsing strategy introduced in (Schabes and Joshi, 1991).",
        "id": 393568
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper combines the advantages of different frameworks for grammar error correction (GEC) and achieves good performance?",
    "positive_ctxs": [
      {
        "title": "TemplateGEC: Improving Grammatical Error Correction with Detection Template",
        "text": "Grammatical error correction (GEC) can be divided into sequence-to-edit (Seq2Edit) and sequence-to-sequence (Seq2Seq) frameworks, both of which have their pros and cons. To utilize the strengths and make up for the shortcomings of these frameworks, this paper proposes a novel method, TemplateGEC, which capitalizes on the capabilities of both Seq2Edit and Seq2Seq frameworks in error detection and correction respectively. TemplateGEC utilizes the detection labels from a Seq2Edit model, to construct the template as the input. A Seq2Seq model is employed to enforce consistency between the predictions of different templates by utilizing consistency learning. Experimental results on the Chinese NLPCC18, English BEA19 and CoNLL14 benchmarks show the effectiveness and robustness of TemplateGEC. Further analysis reveals the potential of our method in performing human-in-the-loop GEC.",
        "id": 259370799
      }
    ],
    "negative_ctxs": [
      {
        "title": "Error-repair Dependency Parsing for Ungrammatical Texts",
        "text": "We propose a new dependency parsing scheme which jointly parses a sentence and repairs grammatical errors by extending the non-directional transitionbased formalism of Goldberg and Elhadad (2010) with three additional actions: SUBSTITUTE, DELETE, INSERT. Because these actions may cause an infinite loop in derivation, we also introduce simple constraints that ensure the parser termination. We evaluate our model with respect to dependency accuracy and grammaticality improvements for ungrammatical sentences, demonstrating the robustness and applicability of our scheme.",
        "id": 2290938
      },
      {
        "title": "Detection of opinions and facts. A cognitive approach",
        "text": "A model of episodic memory is derived to propose algorithms of text categorization with semantic space models. Performances of two algorithms named Target vector and Sub-target vector are contrasted using textual material of the text-mining context 'DEFT09'. The experience reported here have been realized on the english corpus which is composed of articles of the economic newspaper \"The Financial Times\". The aim of the task was to categorize texts in function of the factuality or subjectivity they expressed. Results confirm (i) that the episodic memory metaphor provides a convenient framework to propose efficient algorithm for text categorization, and (ii) that Sub-target vector algorithm outperforms the Target vector algorithm.",
        "id": 143151
      },
      {
        "title": "",
        "text": "Recognition of Named Entities (NEs) is a difficult process in Indian languages like Hindi, Telugu, etc., where sufficient gazetteers and annotated corpora are not available compared to English language. This paper details a novel clustering and co-occurrence based approach to map English NEs with their equivalent representations from different languages recognized in a language-independent way. We have substituted the required language specific resources by the richly structured multilingual content of Wikipedia. The approach includes clustering of highly similar Wikipedia articles. Then the NEs in an English article are mapped with other language terms in interlinked articles based on co-occurrence frequencies. The cluster information and the term co-occurrences are considered in extracting the NEs from non-English languages. Hence, the English Wikipedia is used to bootstrap the NEs for other languages. Through this approach, we have availed the structured, semi-structured and multilingual content of the Wikipedia to a massive extent. Experimental results suggest that the proposed approach yields promising results in rates of precision and recall.",
        "id": 6737837
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper about parameter-efficient finetuning first proposes to feed the pretrained weight instead of the activation to an adapter?",
    "positive_ctxs": [
      {
        "title": "Parameter-Efficient Fine-Tuning without Introducing New Latency",
        "text": "Parameter-efficient fine-tuning (PEFT) of pretrained language models has recently demonstrated remarkable achievements, effectively matching the performance of full fine-tuning while utilizing significantly fewer trainable parameters, and consequently addressing the storage and communication constraints. Nonetheless, various PEFT methods are limited by their inherent characteristics. In the case of sparse fine-tuning, which involves modifying only a small subset of the existing parameters, the selection of fine-tuned parameters is task-and domain-specific, making it unsuitable for federated learning. On the other hand, PEFT methods with adding new parameters typically introduce additional inference latency. In this paper, we demonstrate the feasibility of generating a sparse mask in a task-agnostic manner, wherein all downstream tasks share a common mask. Our approach, which relies solely on the magnitude information of pre-trained parameters, surpasses existing methodologies by a significant margin when evaluated on the GLUE benchmark. Additionally, we introduce a novel adapter technique that directly applies the adapter to pre-trained parameters instead of the hidden representation, thereby achieving identical inference speed to that of full finetuning. Through extensive experiments, our proposed method attains a new state-of-the-art outcome in terms of both performance and storage efficiency, storing only 0.03% parameters of full fine-tuning.",
        "id": 258947572
      }
    ],
    "negative_ctxs": [
      {
        "title": "Retrieval-augmented Image Captioning",
        "text": "Inspired by retrieval-augmented language generation and pretrained Vision and Language (V&L) encoders, we present a new approach to image captioning that generates sentences given the input image and a set of captions retrieved from a datastore, as opposed to the image alone. The encoder in our model jointly processes the image and retrieved captions using a pretrained V&L BERT, while the decoder attends to the multimodal encoder representations, benefiting from the extra textual evidence from the retrieved captions. Experimental results on the COCO dataset show that image captioning can be effectively formulated from this new perspective. Our model, named EXTRA, benefits from using captions retrieved from the training dataset, and it can also benefit from using an external dataset without the need for retraining. Ablation studies show that retrieving a sufficient number of captions (e.g., k=5) can improve captioning quality. Our work contributes towards using pretrained V&L encoders for generative tasks, instead of standard classification tasks.",
        "id": 256901128
      },
      {
        "title": "Corpus Linguistics Language and Computers: A Practical Introduction to the Computer Analysis of Language",
        "text": "The appearance of not one but two introductions to corpus linguistics within the same series shows the maturation and diversification of this fledgling subdiscipline within linguistics. McEnery and Wilson offer an overview or annotated report on work done within the computer-corpus research paradigm, including computational linguistics, whereas Barnbrook offers a guide or manual on the procedures and methodology of corpus linguistics, particularly with regard to machine-readable texts in English and to the type of results thereby generated.Whereas McEnery and Wilson recognize that the distinguishing features of corpus linguistics rest with its computer-aided empiricism, they are eager to line it up alongside cognitive rationalism in an effort to show the complementarity and interdependence of the two. As they argue, the advantages of a corpus-linguistics approach are that it is invariably systematic and rigorous, and that linguistics based on a corpus acts as a yardstick or control to linguistics based on artificial or introspective data. Of these current research paradigms, the authors' discussion offers fair and balanced criticism.In the central core of the book, McEnery and Wilson present overviews of the theory and practice of corpus linguistics, the relative merits of qualitative versus quanti-333",
        "id": 261175933
      },
      {
        "title": "",
        "text": "",
        "id": 192563022
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Which work suggests that machine translation models might get too confident and generate coherent but inadequant translations?",
    "positive_ctxs": [
      {
        "title": "Prevent the Language Model from being Overconfident in Neural Machine Translation",
        "text": "The Neural Machine Translation (NMT) model is essentially a joint language model conditioned on both the source sentence and partial translation. Therefore, the NMT model naturally involves the mechanism of the Language Model (LM) that predicts the next token only based on partial translation. Despite its success, NMT still suffers from the hallucination problem, generating fluent but inadequate translations. The main reason is that NMT pays excessive attention to the partial translation while neglecting the source sentence to some extent, namely overconfidence of the LM. Accordingly, we define the Margin between the NMT and the LM, calculated by subtracting the predicted probability of the LM from that of the NMT model for each token. The Margin is negatively correlated to the overconfidence degree of the LM. Based on the property, we propose a Margin-based Token-level Objective (MTO) and a Margin-based Sentencelevel Objective (MSO) to maximize the Margin for preventing the LM from being overconfident. Experiments on WMT14 Englishto-German, WMT19 Chinese-to-English, and WMT14 English-to-French translation tasks demonstrate the effectiveness of our approach, with 1.36, 1.50, and 0.63 BLEU improvements, respectively, compared to the Transformer baseline. The human evaluation further verifies that our approaches improve translation adequacy as well as fluency. 1",
        "id": 235166394
      }
    ],
    "negative_ctxs": [
      {
        "title": "IAI @ SocialDisNER : Catch me if you can! Capturing complex disease mentions in tweets",
        "text": "Biomedical NER is an active research area today. Despite the availability of state-of-theart models for standard NER tasks, their performance degrades on biomedical data due to OOV entities and the challenges encountered in specialized domains. We use Flair-NER framework to investigate the effectiveness of various contextual and static embeddings for NER on Spanish tweets, in particular, to capture complex disease mentions.",
        "id": 252819043
      },
      {
        "title": "Using Categorial Grammar to Label Translation Rules",
        "text": "Adding syntactic labels to synchronous context-free translation rules can improve performance, but labeling with phrase structure constituents, as in GHKM (Galley et al.,  2004), excludes potentially useful translation rules. SAMT (Zollmann and Venugopal,  2006)  introduces heuristics to create new non-constituent labels, but these heuristics introduce many complex labels and tend to add rarely-applicable rules to the translation grammar. We introduce a labeling scheme based on categorial grammar, which allows syntactic labeling of many rules with a minimal, well-motivated label set. We show that our labeling scheme performs comparably to SAMT on an Urdu-English translation task, yet the label set is an order of magnitude smaller, and translation is twice as fast.",
        "id": 5170126
      },
      {
        "title": "Temiar Reduplication in One-Level Prosodic Morphology",
        "text": "Temiar reduplication is a difficult piece of prosodic morphology. This paper presents the first computational analysis of Temiar reduplication, using the novel finite-state approach of One-Level Prosodic Morphology originally developed byWalther (1999bWalther ( , 2000. After reviewing both the data and the basic tenets of One-level Prosodic Morphology, the analysis is laid out in some detail, using the notation of the FSA Utilities finite-state toolkit (van Noord 1997). One important discovery is that in this approach one can easily define a regular expression operator which ambiguously scans a string in the left-or rightward direction for a certain prosodic property. This yields an elegant account of base-length-dependent triggering of reduplication as found in Temiar.",
        "id": 594
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which paper first applied the chain of thought concepts in 3D localization problem?",
    "positive_ctxs": [
      {
        "title": "COT3DREF: CHAIN-OF-THOUGHTS DATA-EFFICIENT 3D VISUAL GROUNDING",
        "text": "3D visual grounding is the ability to localize objects in 3D scenes conditioned by utterances. Most existing methods devote the referring head to localize the referred object directly, causing failure in complex scenarios. In addition, it does not illustrate how and why the network reaches the final decision. In this paper, we address this question \"Can we design an interpretable 3D visual grounding framework that has the potential to mimic the human perception system?\". To this end, we formulate the 3D visual grounding problem as a sequence-to-sequence (Seq2Seq) task by first predicting a chain of anchors and then the final target. Interpretability not only improves the overall performance but also helps us identify failure cases. Following the chain of thoughts approach enables us to decompose the referring task into interpretable intermediate steps, boosting the performance and making our framework extremely data-efficient. Moreover, our proposed framework can be easily integrated into any existing architecture. We validate our approach through comprehensive experiments on the Nr3D, Sr3D, and Scanrefer benchmarks and show consistent performance gains compared to existing methods without requiring manually annotated data. Furthermore, our proposed framework, dubbed CoT3DRef, is significantly data-efficient, whereas on the Sr3D dataset, when trained only on 10% of the data, we match the SOTA performance that trained on the entire data.",
        "id": 263829872
      }
    ],
    "negative_ctxs": [
      {
        "title": "Obtaining referential word meanings from visual and distributional information: Experiments on object naming",
        "text": "We investigate object naming, which is an important sub-task of referring expression generation on real-world images. As opposed to mutually exclusive labels used in object recognition, object names are more flexible, subject to communicative preferences and semantically related to each other. Therefore, we investigate models of referential word meaning that link visual to lexical information which we assume to be given through distributional word embeddings. We present a model that learns individual predictors for object names that link visual and distributional aspects of word meaning during training. We show that this is particularly beneficial for zero-shot learning, as compared to projecting visual objects directly into the distributional space. In a standard object naming task, we find that different ways of combining lexical and visual information achieve very similar performance, though experiments on model combination suggest that they capture complementary aspects of referential meaning.",
        "id": 7143588
      },
      {
        "title": "Transfer Learning for Context-Aware Question Matching in Information-seeking Conversations in E-commerce",
        "text": "Building multi-turn information-seeking conversation systems is an important and challenging research topic. Although several advanced neural text matching models have been proposed for this task, they are generally not efficient for industrial applications. Furthermore, they rely on a large amount of labeled data, which may not be available in real-world applications. To alleviate these problems, we study transfer learning for multi-turn information seeking conversations in this paper. We first propose an efficient and effective multiturn conversation model based on convolutional neural networks. After that, we extend our model to adapt the knowledge learned from a resource-rich domain to enhance the performance. Finally, we deployed our model in an industrial chatbot called AliMe Assist 1 and observed a significant improvement over the existing online model.",
        "id": 49210906
      },
      {
        "title": "Creating a Domain-diverse Corpus for Theory-based Argument Quality Assessment",
        "text": "Computational models of argument quality (AQ) have focused primarily on assessing the overall quality or just one specific characteristic of an argument, such as its convincingness or its clarity. However, previous work has claimed that assessment based on theoretical dimensions of argumentation could benefit writers, but developing such models has been limited by the lack of annotated data. In this work, we describe GAQCorpus, the first large, domain-diverse annotated corpus of theory-based AQ. We discuss how we designed the annotation task to reliably collect a large number of judgments with crowdsourcing, formulating theory-based guidelines that helped make subjective judgments of AQ more objective. We demonstrate how to identify arguments and adapt the annotation task for three diverse domains. Our work will inform research on theory-based argumentation annotation and enable the creation of more diverse corpora to support computational AQ assessment. * Equal contribution. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 In the following, we adopt the term \"theory-based AQ,\" which was proposed byWachsmuth et al. (2017b)to indicate that the conception of AQ is specifically grounded in argumentation theoretic literature (and not in CL or NLP).",
        "id": 226237159
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Are there any studies investigating sentiment analysis through text-to-graph conversion models that incorporate contextual embeddings?",
    "positive_ctxs": [
      {
        "title": "Direct parsing to sentiment graphs",
        "text": "This paper demonstrates how a graph-based semantic parser can be applied to the task of structured sentiment analysis, directly predicting sentiment graphs from text. We advance the state of the art on 4 out of 5 standard benchmark sets. We release the source code, models and predictions. 1",
        "id": 247628083
      }
    ],
    "negative_ctxs": [
      {
        "title": "Prototypical Verbalizer for Prompt-based Few-shot Tuning",
        "text": "Prompt-based tuning for pre-trained language models (PLMs) has shown its effectiveness in few-shot learning. Typically, prompt-based tuning wraps the input text into a cloze question. To make predictions, the model maps the output words to labels via a verbalizer, which is either manually designed or automatically built. However, manual verbalizers heavily depend on domain-specific prior knowledge and human efforts, while finding appropriate label words automatically still remains challenging. In this work, we propose the prototypical verbalizer (ProtoVerb) which is built directly from training data. Specifically, Pro-toVerb learns prototype vectors as verbalizers by contrastive learning. In this way, the prototypes summarize training instances and are able to enclose rich class-level semantics. We conduct experiments on both topic classification and entity typing tasks, and the results demonstrate that ProtoVerb significantly outperforms current automatic verbalizers, especially when training data is extremely scarce. More surprisingly, ProtoVerb consistently boosts promptbased tuning even on untuned PLMs, indicating an elegant non-tuning way to utilize PLMs. Our codes are avaliable at https: //github.com/thunlp/OpenPrompt.",
        "id": 247594302
      },
      {
        "title": "Methods and Tools for Corpus Lexicography",
        "text": "",
        "id": 3247663
      },
      {
        "title": "First Broadcast News Transcription System for Khmer Language",
        "text": "In this paper we present an overview on the development of a large vocabulary continuous speech recognition (LVCSR) system for Khmer, the official language of Cambodia, spoken by more than 15 million people. As an under-resourced language, develop a LVCSR system for Khmer is a challenging task. We describe our methodologies for quick language data collection and processing for language modeling and acoustic modeling. For language modeling, we investigate the use of word and sub-word as basic modeling unit in order to see the potential of sub-word units in the case of unsegmented language like Khmer. Grapheme-based acoustic modeling is used to quickly build our Khmer language acoustic model. Furthermore, the approaches and tools used for the development of our system are documented and made publicly available on the web. We hope this will contribute to accelerate the development of LVCSR system for a new language, especially for under-resource languages of developing countries where resources and expertise are limited.",
        "id": 8911750
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that explores the drawbacks of dense retrieval systems especially with large-scale indices?",
    "positive_ctxs": [
      {
        "title": "The Curse of Dense Low-Dimensional Information Retrieval for Large Index Sizes",
        "text": "Information Retrieval using dense lowdimensional representations recently became popular and showed out-performance to traditional sparse-representations like BM25. However, no previous work investigated how dense representations perform with large index sizes. We show theoretically and empirically that the performance for dense representations decreases quicker than sparse representations for increasing index sizes. In extreme cases, this can even lead to a tipping point where at a certain index size sparse representations outperform dense representations. We show that this behavior is tightly connected to the number of dimensions of the representations: The lower the dimension, the higher the chance for false positives, i.e. returning irrelevant documents.",
        "id": 229680000
      }
    ],
    "negative_ctxs": [
      {
        "title": "Published as a conference paper at ICLR 2021 DC3: A LEARNING METHOD FOR OPTIMIZATION WITH HARD CONSTRAINTS",
        "text": "Large optimization problems with hard constraints arise in many settings, yet classical solvers are often prohibitively slow, motivating the use of deep networks as cheap \"approximate solvers.\" Unfortunately, naive deep learning approaches typically cannot enforce the hard constraints of such problems, leading to infeasible solutions. In this work, we present Deep Constraint Completion and Correction (DC3), an algorithm to address this challenge. Specifically, this method enforces feasibility via a differentiable procedure, which implicitly completes partial solutions to satisfy equality constraints and unrolls gradient-based corrections to satisfy inequality constraints. We demonstrate the effectiveness of DC3 in both synthetic optimization tasks and the real-world setting of AC optimal power flow, where hard constraints encode the physics of the electrical grid. In both cases, DC3 achieves near-optimal objective values while preserving feasibility. * These authors contributed equally.Published as a conference paper at ICLR 2021 • AC optimal power flow. We show how the general DC3 framework can be used to optimize power flows on the electrical grid. This difficult non-convex optimization task must be solved at scale and is especially critical for renewable energy adoption. Our results greatly improve upon the performance of general-purpose deep learning methods on this task.RELATED WORKOur approach is situated within the broader literature on fast optimization methods, and draws inspiration from literature on implicit layers and on incorporating constraints into neural networks. We briefly describe each of these areas and their relationship to the present work.Fast optimization methods. Many classical optimization methods have been proposed to improve the practical efficiency of solving optimization problems. These include general techniques such as constraint and variable elimination (i.e., the removal of non-active constraints or redundant variables, respectively), as well as problem-specific techniques (e.g., KKT factorization techniques in the case of convex quadratic programs)(Nocedal & Wright, 2006). Our present work builds upon aspects of this literature, applying concepts from variable elimination to reduce the number of degrees of freedom associated with the optimization problems we wish to solve.In addition to the classical optimization literature, there has been a large body of literature in deep learning that has sought to approximate or speed up optimization models. As described in reviews on topics such as combinatorial optimization (Bengio et al., 2020) and optimal power flow (Hasan et al., 2020), ML methods to speed up optimization models have thus far taken two main approaches. The first class of approaches, akin to work on surrogate modeling(Koziel & Leifsson, 2013), has involved training machine learning models to map directly from optimization inputs to full solutions. However, such approaches have often struggled to produce solutions that are both feasible and (near-)optimal. The second class of approaches has instead focused on employing machine learning approaches alongside or in the loop of optimization models, e.g., to learn warm-start points (see, e.g., Baker (2019) and Dong et al.(2020)) or to enable constraint elimination techniques by predicting active constraints (see, e.g., Misra et al.(2018)). We view our work as part of the former set of approaches, but drawing important inspiration from the latter: that employing structural knowledge about the optimization model is paramount to achieving both feasibility and optimality.Constraints in neural networks. While deep learning is often thought of as wholly unconstrained, in reality, it is quite common to incorporate (simple) constraints within deep learning procedures. For instance, softmax layers encode simplex constraints, sigmoids instantiate upper and lower bounds, ReLUs encode projections onto the positive orthant, and convolutional layers enforce translational equivariance (an idea taken further in general group-equivariant networks (Cohen & Welling, 2016)). Recent work has also focused on embedding specialized kinds of constraints into neural networks, such as conservation of energy (see, e.g., Greydanus et al.(2019)and Beucler et al.(2019)), and homogeneous linear inequality constraints(Frerix et al., 2020). However, while these represent common \"special cases,\" there has to date been little work on building more general hard constraints into deep learning models.",
        "id": 233394003
      },
      {
        "title": "Towards Detecting Harmful Agendas in News Articles Warning: This paper contains examples of online text containing strong views and offensive language",
        "text": "Manipulated news online is a growing problem which necessitates the use of automated systems to curtail its spread. We argue that while misinformation and disinformation detection have been studied, there has been a lack of investment in the important open challenge of detecting harmful agendas in news articles; identifying harmful agendas is critical to flag news campaigns with the greatest potential for real world harm. Moreover, due to real concerns around censorship, harmful agenda detectors must be interpretable to be effective. In this work, we propose this new task and release a dataset, NEWSAGENDAS, of annotated news articles for agenda identification. We show how interpretable systems can be effective on this task and demonstrate that they can perform comparably to black-box models.",
        "id": 260063114
      },
      {
        "title": "Steps towards Semantically Annotated Language Resources",
        "text": "The use of textual resources such as text corpora, tree banks, large-scale lexica etc., has become a widely accepted commitment in the field of computational linguistics. However the scope of the annotations proposed has been unbalanced towards the 'surface' level. Only recently corpora with a deeper level of annotations have started to emerge. In this paper we describe a machine learning approach aimed at learning transformational rules that would allow the (partial) generation of semantic annotations starting from syntactic annotations generated by a parser (or created manually).",
        "id": 18467426
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper employs a two-stage approach in generative models to tackle ABSA tasks across various domains?",
    "positive_ctxs": [
      {
        "title": "Bidirectional Generative Framework for Cross-domain Aspect-based Sentiment Analysis",
        "text": "Cross-domain aspect-based sentiment analysis (ABSA) aims to perform various fine-grained sentiment analysis tasks on a target domain by transferring knowledge from a source domain. Since labeled data only exists in the source domain, a model is expected to bridge the domain gap for tackling cross-domain ABSA. Though domain adaptation methods have proven to be effective, most of them are based on a discriminative model, which needs to be specifically designed for different ABSA tasks. To offer a more general solution, we propose a unified bidirectional generative framework to tackle various cross-domain ABSA tasks. Specifically, our framework trains a generative model in both text-to-label and label-to-text directions. The former transforms each task into a unified format to learn domain-agnostic features, and the latter generates natural sentences from noisy labels for data augmentation, with which a more accurate model can be trained. To investigate the effectiveness and generality of our framework, we conduct extensive experiments on four cross-domain ABSA tasks and present new state-of-the-art results on all tasks. Our data and code are publicly available at https://github.com/DAMO-NLP-SG/BGCA.",
        "id": 258714602
      }
    ],
    "negative_ctxs": [
      {
        "title": "KLUE-CORE: A regression model of semantic textual similarity",
        "text": "This paper describes our system entered for the *SEM 2013 shared task on Semantic Textual Similarity (STS). We focus on the core task of predicting the semantic textual similarity of sentence pairs.The current system utilizes machine learning techniques trained on semantic similarity ratings from the *SEM 2012 shared task; it achieved rank 20 out of 90 submissions from 35 different teams. Given the simple nature of our approach, which uses only WordNet and unannotated corpus data as external resources, we consider this a remarkably good result, making the system an interesting tool for a wide range of practical applications.",
        "id": 10160256
      },
      {
        "title": "IS FAIRNESS ONLY METRIC DEEP? EVALUATING AND ADDRESSING SUBGROUP GAPS IN DML",
        "text": "Deep metric learning (DML) enables learning with less supervision through its emphasis on the similarity structure of representations. There has been much work on improving generalization of DML in settings like zero-shot retrieval, but little is known about its implications for fairness. In this paper, we are the first to evaluate state-of-the-art DML methods trained on imbalanced data, and to show the negative impact these representations have on minority subgroup performance when used for downstream tasks. In this work, we first define fairness in DML through an analysis of three properties of the representation space -interclass alignment, intra-class alignment, and uniformity -and propose finDML, the f airness in non-balanced DML benchmark to characterize representation fairness. Utilizing finDML, we find bias in DML representations to propagate to common downstream classification tasks. Surprisingly, this bias is propagated even when training data in the downstream task is re-balanced. To address this problem, we present Partial Attribute De-correlation (PARADE) to de-correlate feature representations from sensitive attributes and reduce performance gaps between subgroups in both embedding space and downstream metrics.",
        "id": 247628080
      },
      {
        "title": "Joint Training of Candidate Extraction and Answer Selection for Reading Comprehension",
        "text": "While sophisticated neural-based techniques have been developed in reading comprehension, most approaches model the answer in an independent manner, ignoring its relations with other answer candidates. This problem can be even worse in open-domain scenarios, where candidates from multiple passages should be combined to answer a single question. In this paper, we formulate reading comprehension as an extract-then-select twostage procedure. We first extract answer candidates from passages, then select the final answer by combining information from all the candidates. Furthermore, we regard candidate extraction as a latent variable and train the two-stage process jointly with reinforcement learning. As a result, our approach has improved the state-ofthe-art performance significantly on two challenging open-domain reading comprehension datasets. Further analysis demonstrates the effectiveness of our model components, especially the information fusion of all the candidates and the joint training of the extract-then-select procedure.",
        "id": 21703007
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there any paper that applies curriculum learning to various NLG tasks without depending on specific metrics?",
    "positive_ctxs": [
      {
        "title": "In-sample Curriculum Learning by Sequence Completion for Natural Language Generation",
        "text": "Curriculum learning has shown promising improvements in multiple domains by training machine learning models from easy samples to hard ones. Previous works which either design rules or train models for scoring the difficulty highly rely on task-specific expertise, and cannot generalize. Inspired by the \"easy-to-hard\" intuition, we propose to do in-sample curriculum learning for natural language generation tasks. Our learning strategy starts training the model to generate the last few words, i.e., do sequence completion, and gradually extends to generate the whole output sequence. Comprehensive experiments show that it generalizes well to different tasks and achieves significant improvements over strong baselines.",
        "id": 253734400
      }
    ],
    "negative_ctxs": [
      {
        "title": "LINGUISTIC ANALYSIS OF NATURAL LANGUAGE COMMUNICATION WITH COMPUTERS",
        "text": "",
        "id": 1010309
      },
      {
        "title": "Building a Lexicon of Formulaic Language for Language Learners",
        "text": "Though the multiword lexicon has long been of interest in computational linguistics, most relevant work is targeted at only a small portion of it. Our work is motivated by the needs of learners for more comprehensive resources reflecting formulaic language that goes beyond what is likely to be codified in a dictionary. Working from an initial sequential segmentation approach, we present two enhancements: the use of a new measure to promote the identification of lexicalized sequences, and an expansion to include sequences with gaps. We evaluate using a novel method that allows us to calculate an estimate of recall without a reference lexicon, showing that good performance in the second enhancement depends crucially on the first, and that our lexicon conforms much more with human judgment of formulaic language than alternatives.",
        "id": 8637963
      },
      {
        "title": "ACCURATE AND SCALABLE ESTIMATION OF EPISTEMIC UNCERTAINTY FOR GRAPH NEURAL NETWORKS",
        "text": "Safe deployment of graph neural networks (GNNs) under distribution shift requires models to provide accurate confidence indicators (CI).However, while it is wellknown in computer vision that CI quality diminishes under distribution shift, this behavior remains understudied for GNNs.Hence, we begin with a case study on CI calibration under controlled structural and feature distribution shifts and demonstrate that increased expressivity or model size do not always lead to improved CI performance.Consequently, we instead advocate for the use of epistemic uncertainty quantification (UQ) methods to modulate CIs.To this end, we propose G-∆UQ, a new single model UQ method that extends the recently proposed stochastic centering framework to support structured data and partial stochasticity.Evaluated across covariate, concept, and graph size shifts, G-∆UQ not only outperforms several popular UQ methods in obtaining calibrated CIs, but also outperforms alternatives when CIs are used for generalization gap prediction or OOD detection.Overall, our work not only introduces a new, flexible GNN UQ method, but also provides novel insights into GNN CIs on safety-critical tasks.",
        "id": 262065523
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there any paper that address attacks on code models by leveraging the semantic information of the source code through attention scores, while also guaranteeing that the generated adversarial examples can always be compiled successfully?",
    "positive_ctxs": [
      {
        "title": "DIP: Dead code Insertion based Black-box Attack for Programming Language Model",
        "text": "Automatic processing of source code, such as code clone detection and software vulnerability detection, is very helpful to software engineers. Large pre-trained Programming Language (PL) models (such as CodeBERT, Graph-CodeBERT, CodeT5, etc.), show very powerful performance on these tasks. However, these PL models are vulnerable to adversarial examples that are generated with slight perturbation. Unlike natural language, an adversarial example of code must be semantic-preserving and compilable. Due to the requirements, it is hard to directly apply the existing attack methods for natural language models. In this paper, we propose DIP (Dead code Insertion based Blackbox Attack for Programming Language Model), a high-performance and efficient black-box attack method to generate adversarial examples using dead code insertion. We evaluate our proposed method on 9 victim downstream-task large code models. Our method outperforms the state-of-the-art black-box attack in both attack efficiency and attack quality, while generated adversarial examples are compiled preserving semantic functionality.",
        "id": 259370836
      }
    ],
    "negative_ctxs": [
      {
        "title": "The LOIS Project",
        "text": "The legal knowledge base resulting from the LOIS (Lexical Ontologies for legal Information Sharing) project consists of legal WordNets in six languages (Italian, Dutch, Portuguese, German, Czech, English). Its architecture is based on the EuroWordNet (EWN) framework(Vossen et al, 1997). Using the EWN framework assures compatibility of the LOIS WordNets with EWN, allowing them to function as an extension of EWN for the legal domain. For each legal system, the document-derived legal concepts are integrated into a taxonomy, which links into existing formal ontologies. These give the legal wordnets a first formal backbone, which can, in future, be further extended. The database consists of 33,000 synsets, and is aimed to be used in information retrieval, where it provides mono-and multi-lingual access to European legal databases for legal experts as well as for laymen. The LOIS knowledge base also provides a flexible, modular architecture that allows integration of multiple classification schemes, and enables the comparison of legal systems by exploring translation, equivalence and structure across the different legal wordnets.",
        "id": 7759514
      },
      {
        "title": "Capturing the Content of a Document through Complex Event Identification",
        "text": "Granular events, instantiated in a document by predicates, can usually be grouped into more general events, called complex events. Together, they capture the major content of the document. Recent work grouped granular events by defining event regions, filtering out sentences that are irrelevant to the main content. However, this approach assumes that a given complex event is always described in consecutive sentences, which does not always hold in practice. In this paper, we introduce the task of complex event identification. We address this task as a pipeline, first predicting whether two granular events mentioned in the text belong to the same complex event, independently of their position in the text, and then using this to cluster them into complex events. Due to the difficulty of predicting whether two granular events belong to the same complex event in isolation, we propose a context-augmented representation learning approach CONTEXTRL that adds additional context to better model the pairwise relation between granular events. We show that our approach outperforms strong baselines on the complex event identification task and further present a promising case study exploring the effectiveness of using complex events as input for document-level argument extraction. 1 .",
        "id": 250390606
      },
      {
        "title": "大 大 大模 模 模型 型 型与 与 与知 知 知识 识 识图 图 图谱 谱 谱 Large Language Models and Knowledge Graphs",
        "text": "As an important form of knowledge organization, knowledge graphs are widely recognized as one of the foundational infrastructures for the next generation of artificial intelligence technologies, receiving considerable interest from both industry and academia. Traditional methods for representing knowledge graphs mainly employ symbolic representations to explicitly describe concepts and their relationships, with clear semantics and good interpretability. However, these methods have limited coverage of knowledge types, making it challenging to apply them in open-domain scenarios. With the development of large pre-trained language models (large language models), most researchers have considered parameterized large language models as knowledge graphs. Thus, this paper focuses on the research of the life cycle of knowledge graphs in large language models. Specifically, we summarize the related work on knowledge modeling, knowledge acquisition, knowledge fusion, knowledge management, knowledge reasoning, and knowledge application. Finally, we anticipate the future development trends of large language models and knowledge graphs.",
        "id": 261341583
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Are there any tools or studies that have focused on building a morphological analyzer specifically for handling multiple Arabic dialects?",
    "positive_ctxs": [
      {
        "title": "Learning from Relatives: Unified Dialectal Arabic Segmentation",
        "text": "Arabic dialects do not just share a common koiné, but there are shared pandialectal linguistic phenomena that allow computational models for dialects to learn from each other. In this paper we build a unified segmentation model where the training data for different dialects are combined and a single model is trained. The model yields higher accuracies than dialect-specific models, eliminating the need for dialect identification before segmentation. We also measure the degree of relatedness between four major Arabic dialects by testing how a segmentation model trained on one dialect performs on the other dialects. We found that linguistic relatedness is contingent with geographical proximity. In our experiments we use SVM-based ranking and bi-LSTM-CRF sequence labeling.",
        "id": 10961392
      },
      {
        "title": "A Neural Architecture for Dialectal Arabic Segmentation",
        "text": "The automated processing of Arabic dialects is challenging due to the lack of spelling standards and the scarcity of annotated data and resources in general. Segmentation of words into their constituent tokens is an important processing step for natural language processing. In this paper, we show how a segmenter can be trained on only 350 annotated tweets using neural networks without any normalization or reliance on lexical features or linguistic resources. We deal with segmentation as a sequence labeling problem at the character level. We show experimentally that our model can rival state-of-the-art methods that heavily depend on additional resources.",
        "id": 12160022
      }
    ],
    "negative_ctxs": [
      {
        "title": "The KiezDeutsch Korpus (KiDKo) Release 1.0",
        "text": "This paper presents the first release of the KiezDeutsch Korpus (KiDKo), a new language resource with multiparty spoken dialogues of Kiezdeutsch, a newly emerging language variety spoken by adolescents from multiethnic urban areas in Germany. The first release of the corpus includes the transcriptions of the data as well as a normalisation layer and part-of-speech annotations. In the paper, we describe the main features of the new resource and then focus on automatic POS tagging of informal spoken language. Our tagger achieves an accuracy of nearly 97% on KiDKo. While we did not succeed in further improving the tagger using ensemble tagging, we present our approach to using the tagger ensembles for identifying error patterns in the automatically tagged data.",
        "id": 16455108
      },
      {
        "title": "Exploring Substitutability through Discourse Adverbials and Multiple Judgments",
        "text": "In his systematic analysis of discourse connectives,Knott (1996)introduced the notion of substitutability and the conditions under which one connective (e.g., when) can substitute for another (e.g., if ) to express the same meaning. Knott only uses examples which he constructed and judged himself. This paper describes a new multi-judgment study on naturally occurring passages, on which substitutability claims can be tested. While some of our findings support Knott's claims, other pairs of connectives that Knott predicts to be exclusive are in fact judged to substitute felicitously for one another. These findings show that discourse adverbials in the immediate context play a role in connective choice.",
        "id": 4613427
      },
      {
        "title": "Machine Translation Enhanced Computer Assisted Translation",
        "text": "MateCat aims to integrate statistical Machine Translation (MT) and collaborative Translation Memories (TM) within the human translation workflow. The objective is to increase the productivity of professional translators and to enhance their work experience with MT.MateCat will go beyond the state-of-the-art by investigating new research issues related to the integration of MT into CAT, namely: self-tuning MT, user adaptive MT, and informative MT.",
        "id": 20057270
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research that investigates applying conventional data augmentation methods such as removing words and truncating sequences to contrastive learning in NLP applications?",
    "positive_ctxs": [
      {
        "title": "Bootstrapped Unsupervised Sentence Representation Learning",
        "text": "As high-quality labeled data is scarce, unsupervised sentence representation learning has attracted much attention. In this paper, we propose a new framework with a two-branch Siamese Network which maximizes the similarity between two augmented views of each sentence. Specifically, given one augmented view of the input sentence, the online network branch is trained by predicting the representation yielded by the target network of the same sentence under another augmented view. Meanwhile, the target network branch is bootstrapped with a moving average of the online network. The proposed method significantly outperforms other state-of-the-art unsupervised methods on semantic textual similarity (STS) and classification tasks. It can be adopted as a post-training procedure to boost the performance of the supervised methods. We further extend our method for learning multilingual sentence representations and demonstrate its effectiveness on cross-lingual STS tasks. Our code is available at https: //github.com/yanzhangnlp/BSL. . 2020. Don't stop pretraining:Adapt language models to domains and tasks. In Proc. of ACL.Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. 2020. Momentum contrast for unsupervised visual representation learning.",
        "id": 236460287
      }
    ],
    "negative_ctxs": [
      {
        "title": "Combining Parameter-efficient Modules for Task-level Generalisation",
        "text": "A modular design encourages neural models to disentangle and recombine different facets of knowledge to generalise more systematically to new tasks. In this work, we assume that each task is associated with a subset of latent skills from an (arbitrary size) inventory. In turn, each skill corresponds to a parameter-efficient (sparse / low-rank) model adapter. By jointly learning adapters and a routing function that allocates skills to each task, the full network is instantiated as the average of the parameters of active skills. We propose several inductive biases that encourage re-usage and composition of the skills, including variable-size skill allocation and a dual-speed learning rate. We evaluate our latent-skill model in two main settings: 1) multitask reinforcement learning for instruction following on 8 levels of the BabyAI platform; and 2) few-shot fine-tuning of language models on 160 NLP tasks of the CrossFit benchmark. We find that the modular design of our network enhances sample efficiency in reinforcement learning and few-shot generalisation in supervised learning, compared to a series of baselines. These include models where parameters are fully shared, task-specific, or conditionally generated (HyperFormer), as well as sparse mixture-of-experts (Task-MoE).",
        "id": 258378176
      },
      {
        "title": "Bridging the Gap in Multilingual Semantic Role Labeling: a Language-Agnostic Approach",
        "text": "Recent research indicates that taking advantage of complex syntactic features leads to favorable results in Semantic Role Labeling. Nonetheless, an analysis of the latest state-of-the-art multilingual systems reveals the difficulty of bridging the wide gap in performance between highresource (e.g., English) and low-resource (e.g., German) settings. To overcome this issue, we propose a fully language-agnostic model that does away with morphological and syntactic features to achieve robustness across languages. Our approach outperforms the state of the art in all the languages of the CoNLL-2009 benchmark dataset, especially whenever a scarce amount of training data is available. Our objective is not to reject approaches that rely on syntax, rather to set a strong and consistent language-independent baseline for future innovations in Semantic Role Labeling. We release our model code and checkpoints at https://github.com/SapienzaNLP/multi-srl.",
        "id": 227231163
      },
      {
        "title": "Similarity-weighted Construction of Contextualized Commonsense Knowledge Graphs for Knowledge-intense Argumentation Tasks",
        "text": "Arguments often do not make explicit how a conclusion follows from its premises. To compensate for this lack, we enrich arguments with structured background knowledge to support knowledge-intense argumentation tasks. We present a new unsupervised method for constructing Contextualized Commonsense Knowledge Graphs (CCKGs) that selects contextually relevant knowledge from large knowledge graphs (KGs) efficiently and at high quality. Our work goes beyond context-insensitive knowledge extraction heuristics by computing semantic similarity between KG triplets and textual arguments. Using these triplet similarities as weights, we extract contextualized knowledge paths that connect a conclusion to its premise, while maximizing similarity to the argument. We combine multiple paths into a CCKG that we optionally prune to reduce noise and raise precision. Intrinsic evaluation of the quality of our graphs shows that our method is effective for (re)constructing human explanation graphs. Manual evaluations in a large-scale knowledge selection setup confirm high recall and precision of implicit CSK in the CCKGs. Finally, we demonstrate the effectiveness of CCKGs in a knowledge-insensitive argument quality rating task, outperforming strong baselines and rivaling a GPT-3 based system. 1",
        "id": 258685674
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "In video diffusion models, is there any paper that tried decomposing video instruction into sub instructions of different time?",
    "positive_ctxs": [
      {
        "title": "Seer: Language Instructed Video Prediction with Latent Diffusion Models",
        "text": "Imagining the future trajectory is the key for robots to make sound planning and successfully reach their goals. Therefore, text-conditioned video prediction (TVP) is an essential task to facilitate general robot policy learning, i.e., predicting future video frames with a given language instruction and reference frames. It is a highly challenging task to ground task-level goals specified by instructions and high-fidelity frames together, requiring large-scale data and computation. To tackle this task and empower robots with the ability to foresee the future, we propose a sample and computation-efficient model, named Seer, by inflating the pretrained text-to-image (T2I) stable diffusion models along the temporal axis. We inflate the denoising U-Net and language conditioning model with two novel techniques, Autoregressive Spatial-Temporal Attention and Frame Sequential Text Decomposer, to propagate the rich prior knowledge in the pretrained T2I models across the frames. With the well-designed architecture, Seer makes it possible to generate high-fidelity, coherent, and instruction-aligned video frames by fine-tuning a few layers on a small amount of data. The experimental results on Something Something V2 (SSv2) and Bridgedata datasets demonstrate our superior video prediction performance with around 210-hour training on 4 RTX 3090 GPUs: decreasing the FVD of the current SOTA model from 290 to 200 on SSv2 and achieving at least 70% preference in the human evaluation. https://seervideodiffusion.github.io/",
        "id": 257766959
      }
    ],
    "negative_ctxs": [
      {
        "title": "The Design of an Experiment in Anaphora Resolution for Referring Expressions Generation",
        "text": "We present a pilot experiment to measure the effects of redundancy in the resolution of definite descriptions as performed by a small number of human readers. Although originally intended to provide evidence of how much redundancy should ideally be included in generated anaphoric descriptions, preliminary findings reveal a number of little explored issues that are relevant to both referring expressions generation and interpretation.",
        "id": 18538403
      },
      {
        "title": "Efficient parsing strategies for syntactic analysis of closed captions",
        "text": "We present an efficient multi-level chart parser that was designed for syntactic analysis of closed captions (subtitles) in a real-time Machine Translation (MT) system. In order to achieve high parsing speed, we divided an existing English grammar into multiple levels. The parser proceeds in stages. At each stage, rules corresponding to only one level are used. A constituent pruning step is added between levels to insure that constituents not likely to be part of the final parse are removed. This results in a significant parse time and ambiguity reduction. Since the domain is unrestricted, out-of-coverage sentences are to be expected and the parser might not produce a single analysis spanning the whole input. Despite the incomplete parsing strategy and the radical pruning, the initial evaluation results show that the loss of parsing accuracy is acceptable. The parsing time favorable compares with a Tomita parser and a chart parser parsing time when run on the same grammar and lexicon.7",
        "id": 2607435
      },
      {
        "title": "",
        "text": "",
        "id": 207915656
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "In discourse parsing literature, which works have explored parser performance by adopting the original Parseval procedure and reporting micro-averaged F1 scores?",
    "positive_ctxs": [
      {
        "title": "A Top-Down Neural Architecture towards Text-Level Parsing of Discourse Rhetorical Structure",
        "text": "Due to its great importance in deep natural language understanding and various down-stream applications, text-level parsing of discourse rhetorical structure (DRS) has been drawing more and more attention in recent years. However, all the previous studies on text-level discourse parsing adopt bottom-up approaches, which much limit the DRS determination on local information and fail to well benefit from global information of the overall discourse. In this paper, we justify from both computational and perceptive points-of-view that the top-down architecture is more suitable for textlevel DRS parsing. On the basis, we propose a top-down neural architecture toward text-level DRS parsing. In particular, we cast discourse parsing as a recursive split point ranking task, where a split point is classified to different levels according to its rank and the elementary discourse units (EDUs) associated with it are arranged accordingly. In this way, we can determine the complete DRS as a hierarchical tree structure via an encoder-decoder with an internal stack. Experimentation on both the English RST-DT corpus and the Chinese CDTB corpus shows the great effectiveness of our proposed top-down approach towards textlevel DRS parsing.",
        "id": 218516743
      }
    ],
    "negative_ctxs": [
      {
        "title": "COLLATE: Competence Center in Speech and Language Technology",
        "text": "This paper presents the structure and activitities of the recently established Competence Center in Speech and Language Technology in Saarbrücken. The objectives of the Competence Center are to provide a comprehensive information service about speech and language technologies, including live demonstrations of the most important language technology (LT) systems, and to advance the state of the art in the evaluation of LT systems for real-world applications. The Competence Center comprises the following components:1. the Virtual Information Center \"Language Technology World\" (www.lt-world.org), the world's most comprehensive information resource about speech and language technology, 2. the Demonstration Center in Saarbrücken, which offers interested parties the possibility to play and experiment with different speech and language technologies, or to attend guided demonstrations, 3. the Evaluation Center, which conducts evaluations of the overall usability of language technology systems and advances knowledge of relevant usability issues and evaluation methods.",
        "id": 814070
      },
      {
        "title": "Multi-Component Word Sense Disambiguation",
        "text": "This paper describes the system MC-WSD presented for the English Lexical Sample task. The system is based on a multicomponent architecture. It consists of one classifier with two components. One is trained on the data provided for the task. The second is trained on this data and, additionally, on an external training set extracted from the Wordnet glosses. The goal of the additional component is to lessen sparse data problems by exploiting the information encoded in the ontology.",
        "id": 7192080
      },
      {
        "title": "A Hybrid System for Patent Translation",
        "text": "This work presents a HMT system for patent translation. The system exploits the high coverage of SMT and the high precision of an RBMT system based on GF to deal with specific issues of the language. The translator is specifically developed to translate patents and it is evaluated in the English-French language pair. Although the number of issues tackled by the grammar are not extremely numerous yet, both manual and automatic evaluations consistently show their preference for the hybrid system in front of the two individual translators.",
        "id": 39619605
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that investigates across multiple languages how dictionary definitions and word embedding models compare?",
    "positive_ctxs": [
      {
        "title": "Semeval-2022 Task 1: CODWOE -Comparing Dictionaries and Word Embeddings",
        "text": "Word embeddings have advanced the state of the art in NLP across numerous tasks. Understanding the contents of dense neural representations is of utmost interest to the computational semantics community. We propose to focus on relating these opaque word vectors with human-readable definitions, as found in dictionaries. This problem naturally divides into two subtasks: converting definitions into embeddings, and converting embeddings into definitions. This task was conducted in a multilingual setting, using comparable sets of embeddings trained homogeneously.",
        "id": 249151979
      }
    ],
    "negative_ctxs": [
      {
        "title": "UPF-taln: SemEval 2015 Tasks 10 and 11 Sentiment Analysis of Literal and Figurative Language in Twitter *",
        "text": "In this paper, we describe the approach used by the UPF-taln team for tasks 10 and 11 of SemEval 2015 that respectively focused on \"Sentiment Analysis in Twitter\" and \"Sentiment Analysis of Figurative Language in Twitter\". Our approach achieved satisfactory results in the figurative language analysis task, obtaining the second best result. In task 10, our approach obtained acceptable performances. We experimented with both wordbased features and domain-independent intrinsic word features. We exploited two machine learning methods: the supervised algorithm Support Vector Machines for task 10, and Random-Sub-Space with M5P as base algorithm for task 11.",
        "id": 18492222
      },
      {
        "title": "New Functions of FrameSQL for Multilingual FrameNets",
        "text": "The Berkeley FrameNet Project (BFN) is making an English lexical database called FrameNet, which describes syntactic and semantic properties of an English lexicon extracted from large electronic text corpora(Baker et al., 1998). Other projects dealing with Spanish, German and Japanese follow a similar approach and annotate large corpora. FrameSQL is a web-based application developed by the author, and it allows the user to search the BFN database in a variety of ways (Sato, 2003). FrameSQL shows a clear view of the headword's grammar and combinatorial properties offered by the FrameNet database. FrameSQL has been developing and new functions were implemented for processing the Spanish FrameNet data(Subirats and Sato, 2004). FrameSQL is also in the process of incorporating the data of the Japanese FrameNet Project(Ohara et al., 2003)and that of the Saarbrücken Lexical Semantics Acquisition Project(Erk et al., 2003)into the database and will offer the same user-interface for searching these lexical data. This paper describes new functions of FrameSQL, showing how FrameSQL deals with the lexical data of English, Spanish, Japanese and German seamlessly.",
        "id": 2456041
      },
      {
        "title": "DONNELLAN'S DISTINCTION AND A COMPUTATIONAL MODEL OF REFERENCE",
        "text": "In this paper, I describe how Donnellan's distinction between referential and attributive uses of definite descriptions should be represented in a computational model of reference. After briefly discussing the significance of Donnellan's distinction, I reinterpret it as being three-tiered, relating to object representation, referring intentions, and choice of rehrring expression. I then present a cognitive model of referring, the components of which correspond to this analysis, and discuss the interaction that takes place among those components. Finally, the implementation of this model, now in progress, is described.",
        "id": 6515223
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Where can I find a large corpus of annotated social media posts concerning a variety of health conditions?",
    "positive_ctxs": [
      {
        "title": "RedHOT: A Corpus of Annotated Medical Questions, Experiences, and Claims on Social Media",
        "text": "We present Reddit Health Online Talk (RedHOT), a corpus of 22,000 richly annotated social media posts from Reddit spanning 24 health conditions. Annotations include demarcations of spans corresponding to medical claims, personal experiences, and questions. We collect additional granular annotations on identified claims. Specifically, we mark snippets that describe patient Populations, Interventions, and Outcomes (PIO elements) within these. Using this corpus, we introduce the task of retrieving trustworthy evidence relevant to a given claim made on social media. We propose a new method to automatically derive (noisy) supervision for this task which we use to train a dense retrieval model; this outperforms baseline models. Manual evaluation of retrieval results performed by medical doctors indicate that while our system performance is promising, there is considerable room for improvement. We release all annotations collected (and scripts to assemble the dataset), and all code necessary to reproduce the results in this paper at: https://sominw. com/redhot. -Olano. 2019. Learning dense representations for entity retrieval. In",
        "id": 252846763
      }
    ],
    "negative_ctxs": [
      {
        "title": "Towards Emotion Prediction in Spoken Tutoring Dialogues",
        "text": "Human tutors detect and respond to student emotional states, but current machine tutors do not. Our preliminary machine learning experiments involving transcription, emotion annotation and automatic feature extraction from our human-human spoken tutoring corpus indicate that the spoken tutoring system we are developing can be enhanced to automatically predict and adapt to student emotional states.",
        "id": 14981345
      },
      {
        "title": "Deep Neural Networks for Coreference Resolution for Polish",
        "text": "The paper presents several configurations of deep neural networks aimed at the task of coreference resolution for Polish. Starting with the basic feature set and standard word embedding vector size we examine the setting with larger vectors, more extensive sets of mention features, increased number of negative examples, Siamese network architecture and a global mention clustering algorithm. The highest results are achieved by the system combining our best deep neural architecture with the sieve-based approach -the cascade of rule-based coreference resolvers ordered from most to least precise. All systems are evaluated on the data of the Polish Coreference Corpus featuring 540K tokens and 180K mentions. The best variant improves the state of the art for Polish by 0.53 F1 points, reaching 81.23 points of the CoNLL metric.",
        "id": 21685089
      },
      {
        "title": "Temporal Expression Recognition for Cell Cycle Phase Concepts in Biomedical Literature",
        "text": "In this paper, we present a system for recognizing temporal expressions related to cell cycle phase (CCP) concepts in biomedical literature. We identified 11 classes of cell cycle related temporal expressions, for which we made extensions to TIMEX3, arranging them in an ontology derived from the Gene Ontology. We annotated 310 abstracts from PubMed. Annotation guidelines were developed, consistent with existing time-related annotation guidelines for TimeML. Two annotators participated in the annotation. We achieved an inter-annotator agreement of 0.79 for an exact span match and 0.82 for relaxed constraints. Our approach is a hybrid of machine learning to recognize temporal expressions and a rule-based approach to map them to the ontology. We trained a named entity recognizer using Conditional Random Fields (CRF) models. An off-the-shelf implementation of the linear chain CRF model was used. We obtained an F-score of 0.77 for temporal expression recognition. We achieved 0.79 macro-averagee F-score and 0.78 microaveraged F-score for mapping to the ontology.",
        "id": 653250
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper first found that multilingual models can inference cross-lingual supervision in MLM training by themself?",
    "positive_ctxs": [
      {
        "title": "On-the-fly Cross-lingual Masking for Multilingual Pre-training",
        "text": "In multilingual pre-training with the objective of MLM (masked language modeling) on multiple monolingual corpora, multilingual models only learn cross-linguality implicitly from isomorphic spaces formed by overlapping different language spaces due to the lack of explicit cross-lingual forward pass. In this work, we present CLPM (Cross-lingual Prototype Masking), a dynamic and token-wise masking scheme, for multilingual pre-training, using a special token [C] x to replace a random token x in the input sentence.[C] x is a cross-lingual prototype for x and then forms an explicit crosslingual forward pass. We instantiate CLPM for the multilingual pre-training phase of UNMT (unsupervised neural machine translation), and experiments show that CLPM can consistently improve the performance of UNMT models on {De, Ro, N e} ↔ En. Beyond UNMT or bilingual tasks, we show that CLPM can consistently improve the performance of multilingual models on cross-lingual classification.",
        "id": 259370532
      }
    ],
    "negative_ctxs": [
      {
        "title": "Training Language Models under Resource Constraints for Adversarial Advertisement Detection",
        "text": "Advertising on e-commerce and social media sites deliver ad impressions at web scale on a daily basis driving value to both shoppers and advertisers. This scale necessitates programmatic ways of detecting unsuitable content in ads to safeguard customer experience and trust. This paper focusses on techniques for training text classification models under resource constraints, built as part of automated solutions for advertising content moderation. We show how weak supervision, curriculum learning and multi-lingual training can be applied effectively to fine-tune BERT and its variants for text classification tasks in conjunction with different data augmentation strategies. Our extensive experiments on multiple languages show that these techniques detect adversarial ad categories with a substantial gain in precision at high recall threshold over the baseline.",
        "id": 235097217
      },
      {
        "title": "The RepEval 2017 Shared Task: Multi-Genre Natural Language Inference with Sentence Representations",
        "text": "This paper presents the results of the RepEval 2017 Shared Task, which evaluated neural network sentence representation learning models on the Multi-Genre Natural Language Inference corpus (MultiNLI) recently introduced by Williams et al. (2017). All of the five participating teams beat the bidirectional LSTM (BiLSTM) and continuous bag of words baselines reported in Williams et al.. The best single model used stacked BiLSTMs with residual connections to extract sentence features and reached 74.5% accuracy on the genre-matched test set. Surprisingly, the results of the competition were fairly consistent across the genrematched and genre-mismatched test sets, and across subsets of the test data representing a variety of linguistic phenomena, suggesting that all of the submitted systems learned reasonably domainindependent representations for sentence meaning.",
        "id": 30758763
      },
      {
        "title": "Improving Named Entity Recognition by Jointly Learning to Disambiguate Morphological Tags Title and Abstract in Turkish",
        "text": "Previous studies have shown that linguistic features of a word such as possession, genitive or other grammatical cases can be employed in word representations of a named entity recognition (NER) tagger to improve the performance for morphologically rich languages. However, these taggers require external morphological disambiguation (MD) tools to function which are hard to obtain or non-existent for many languages. In this work, we propose a model which alleviates the need for such disambiguators by jointly learning NER and MD taggers in languages for which one can provide a list of candidate morphological analyses. We show that this can be done independent of the morphological annotation schemes, which differ among languages. Our experiments employing three different model architectures that join these two tasks show that joint learning improves NER performance. Furthermore, the morphological disambiguator's performance is shown to be competitive.Dahaönceki çalışmalar, biçimbilimsel olarak zengin dillerdeki varlık ismi tanıma (VAT) başarısını artırmak için sözcüklerin iyelik, genitif ve benzeri hâllerinin kullanılabilecegini göstermiştir. Ancak, bu türden varlık ismi tanıma işaretleyicilerinin çalışabilmesi için elde edilmesi zor veya bazı diller için imkansız olan dışsal biçimbilimsel ayrıştırıcılara (BA) ihtiyaç vardır. Bu çalışmada, bu tür ayrıştırıcılara olan ihtiyacı ortadan kaldırmak için VAT ve BA görevlerini aynı anda çözen ve aday biçimbilimsel çözümlemelerin sunulabildigi dillere uygulanabilen bir modelönerilmektedir. Bunun dillere göre degişen biçimbilimsel işaretleme şemalarından bagımsız olarak yapılabildigi gösterilmiştir. Bu iki görevi aynı anda gerçekleştirenüç farklı model mimarisi kullanarak yaptıgımız deneyler birlikteögrenmenin VAT başarısını artırdıgını göstermiştir. Buna ek olarak, biçimbilimsel ayrıştırıcının başarısınınönceki çalışmalarla karşılaştırılabilir oldugu görülmüştür. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/",
        "id": 49870417
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Which method involves training additional prompt tokens for every layer during the fine-tuning of language models, specifically evaluating their performance on generation tasks?",
    "positive_ctxs": [
      {
        "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
        "text": "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were \"virtual tokens\". We apply prefix-tuning to GPT-2 for table-totext generation and to BART for summarization. We show that by modifying only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. 2020. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. Amodei. 2020. Language models are few-shot learners.",
        "id": 230433941
      }
    ],
    "negative_ctxs": [
      {
        "title": "CFLOWNETS: CONTINUOUS CONTROL WITH GENERATIVE FLOW NETWORKS",
        "text": "Generative flow networks (GFlowNets), as an emerging technique, can be used as an alternative to reinforcement learning for exploratory control tasks. GFlowNet aims to generate distribution proportional to the rewards over terminating states, and to sample different candidates in an active learning fashion. GFlowNets need to form a DAG and compute the flow matching loss by traversing the inflows and outflows of each node in the trajectory. No experiments have yet concluded that GFlowNets can be used to handle continuous tasks. In this paper, we propose generative continuous flow networks (CFlowNets) that can be applied to continuous control tasks. First, we present the theoretical formulation of CFlowNets. Then, a training framework for CFlowNets is proposed, including the action selection process, the flow approximation algorithm, and the continuous flow matching loss function. Afterward, we theoretically prove the error bound of the flow approximation. The error decreases rapidly as the number of flow samples increases. Finally, experimental results on continuous control tasks demonstrate the performance advantages of CFlowNets compared to many reinforcement learning methods, especially regarding exploration ability.",
        "id": 257365137
      },
      {
        "title": "Performance Prediction via Bayesian Matrix Factorisation for Multilingual Natural Language Processing Tasks",
        "text": "Performance prediction for Natural Language Processing (NLP) seeks to reduce the experimental burden resulting from the myriad of different evaluation scenarios, e.g., the combination of languages used in multilingual transfer. In this work, we explore the framework of Bayesian matrix factorisation for performance prediction, as many experimental settings in NLP can be naturally represented in matrix format. Our approach outperforms the stateof-the-art in several NLP benchmarks, including machine translation and cross-lingual entity linking. Furthermore, it also avoids hyperparameter tuning and is able to provide uncertainty estimates over predictions.",
        "id": 258378277
      },
      {
        "title": "Neural Unsupervised Reconstruction of Protolanguage Word Forms",
        "text": "We present a state-of-the-art neural approach to the unsupervised reconstruction of ancient word forms. Previous work in this domain used expectation-maximization to predict simple phonological changes between ancient word forms and their cognates in modern languages. We extend this work with neural models that can capture more complicated phonological and morphological changes. At the same time, we preserve the inductive biases from classical methods by building monotonic alignment constraints into the model and deliberately underfitting during the maximization step. We evaluate our performance on the task of reconstructing Latin from a dataset of cognates across five Romance languages, achieving a notable reduction in edit distance from the target word forms compared to previous methods.",
        "id": 253553792
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research that assesses how well language learning models, such as ChatGPT, perform in creating reading comprehension tasks for educational software?",
    "positive_ctxs": [
      {
        "title": "Evaluating Reading Comprehension Exercises Generated by LLMs: A Showcase of ChatGPT in Education Applications",
        "text": "The recent advancement of pre-trained Large Language Models (LLMs), such as OpenAI's ChatGPT, has led to transformative changes across fields. For example, developing intelligent systems in the educational sector that leverage the linguistic capabilities of LLMs demonstrates a visible potential. Though researchers have recently explored how Chat-GPT could possibly assist in student learning, few studies have applied these techniques to real-world classroom settings involving teachers and students. In this study, we implement a reading comprehension exercise generation system that provides high-quality and personalized reading materials for middle school English learners in China. Extensive evaluations of the generated reading passages and corresponding exercise questions, conducted both automatically and manually, demonstrate that the system-generated materials are suitable for students and even surpass the quality of existing human-written ones. By incorporating first-hand feedback and suggestions from experienced educators, this study serves as a meaningful pioneering application of ChatGPT, shedding light on the future design and implementation of LLM-based systems in the educational context.",
        "id": 259376566
      }
    ],
    "negative_ctxs": [
      {
        "title": "Representing and Querying Multi-dimensional Markup for Question Answering",
        "text": "This paper describes our approach to representing and querying multi-dimensional, possibly overlapping text annotations, as used in our question answering (QA) system. We use a system extending XQuery, the W3C-standard XML query language, with new axes that allow one to jump easily between different annotations of the same data. The new axes are formulated in terms of (partial) overlap and containment. All annotations are made using stand-off XML in a single document, which can be efficiently queried using the XQuery extension. The system is scalable to gigabytes of XML annotations. We show examples of the system in QA scenarios.",
        "id": 12407409
      },
      {
        "title": "Robust and Controllable Object-Centric Learning through Energy-based Models ROBUST AND CONTROLLABLE OBJECT-CENTRIC LEARNING THROUGH ENERGY-BASED MODELS",
        "text": "Humans are remarkably good at understanding and reasoning about complex visual scenes. The capability to decompose low-level observations into discrete objects allows us to build a grounded abstract representation and identify the compositional structure of the world. Accordingly, it is a crucial step for machine learning models to be capable of inferring objects and their properties from visual scenes without explicit supervision. However, existing works on objectcentric representation learning either rely on tailor-made neural network modules or strong probabilistic assumptions in the underlying generative and inference processes. In this work, we present EGO, a conceptually simple and general approach to learning object-centric representations through an energy-based model. By forming a permutation-invariant energy function using vanilla attention blocks readily available in Transformers, we can infer object-centric latent variables via gradient-based MCMC methods where permutation equivariance is automatically guaranteed. We show that EGO can be easily integrated into existing architectures and can effectively extract high-quality object-centric representations, leading to better segmentation accuracy and competitive downstream task performance. Further, empirical evaluations show that EGO's learned representations are robust against distribution shift. Finally, we demonstrate the effectiveness of EGO in systematic compositional generalization, by re-composing learned energy functions for novel scene generation and manipulation.arXiv:2210.05519v1 [cs.LG] 11 Oct 2022Robust and Controllable Object-Centric Learning through Energy-based Models K V Z1 Z2Figure 1: Architecture of EGO-Attention, the variant of EGO used in experiments. x is the input image and z i are the object-centric representations. In each block, EGO attends to the latent variables to refines the hidden scene representation using cross-attention mechanism between x and z i , to measure the consistency between image input and latent representation. minimal hand-designed inductive biases. In a similar spirit, we ask whether we can learn objectcentric representations with minimal human assumptions and task-specific architectures.Contributions In this work, we introduce EGO (EnerGy-based Object-centric learning), a conceptually simple yet effective approach to learning object-centric representations without the need for specially-tailored neural network architectures or excessive generative modeling (typically parametric) assumptions. Based on the Energy-based Model (EBM) framework, we propose to learn an energy function that takes as input a visual scene and a set of object-centric latent variables and outputs a scalar value that measures the consistency between the observation and the latent representation (Section 2). We minimally assume permutation invariance among objects and embed this assumption into the energy function by leveraging the vanilla attention mechanisms from the Transformer (Vaswani et al., 2017) architecture (Section 2.1). In essence, our method makes models act as segmentation annotators, aiming to iteratively improve their annotations by minimizing our energy function. We use gradient-based Markov chain Monte Carlo (MCMC) sampling to efficiently sample latent variables from the EBM distribution, which automatically yields a permutation-equivariant update rule for the latent variables (Section 2.2). This stochastic inference procedure also addresses the inherent uncertainty in learning object-centric representations; models can learn to represent scenes containing multiple objects and potential occlusions in a probabilistic and multi-modal manner. We demonstrate the effectiveness of our approach on a variety of unsupervised object discovery tasks and show both qualitatively and qualitatively that our model can learn to decompose complex scenes into highly accurate and interpretable objects, outperforming state-of-the-art methods on segmentation performance (Section 4.1). We also show that we can reuse the learned energy functions for controllable scene generation and manipulation, which enables systematic compositional generalization to novel scenes (Section 4.2). Finally, we demonstrate the robustness of our model to various distribution shifts and hyperparameter settings (Section 4.3).",
        "id": 252815945
      },
      {
        "title": "Learning to Ignore: Long Document Coreference with Bounded Memory Neural Networks",
        "text": "Long document coreference resolution remains a challenging task due to the large memory and runtime requirements of current models. Recent work doing incremental coreference resolution using just the global representation of entities shows practical benefits but requires keeping all entities in memory, which can be impractical for long documents. We argue that keeping all entities in memory is unnecessary, and we propose a memoryaugmented neural network that tracks only a small bounded number of entities at a time, thus guaranteeing a linear runtime in length of document. We show that (a) the model remains competitive with models with high memory and computational requirements on OntoNotes and LitBank, and (b) the model learns an efficient memory management strategy easily outperforming a rule-based strategy.",
        "id": 222140769
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a study that investigates knowledge transfer from structured databases to unstructured data sources for improving sophisticated question-answering systems?",
    "positive_ctxs": [
      {
        "title": "Knowledge Transfer between Structured and Unstructured Sources for Complex Question Answering",
        "text": "Multi-hop question answering (QA) combines multiple pieces of evidence to search for the correct answer. Reasoning over a text corpus (TextQA) and/or a knowledge base (KBQA) has been extensively studied and led to distinct system architectures. However, knowledge transfer between such two QA systems has been under-explored. Research questions like what knowledge is transferred or whether the transferred knowledge can help answer over one source using another one, are yet to be answered. In this paper, therefore, we study the knowledge transfer of multi-hop reasoning between structured and unstructured sources. We first propose a unified QA framework named SIMULTQA to enable knowledge transfer and bridge the distinct supervisions from KB and text sources. Then, we conduct extensive analyses to explore how knowledge is transferred by leveraging the pre-training and fine-tuning paradigm. We focus on the low-resource finetuning to show that pre-training SIMULTQA on one source can substantially improve its performance on the other source. More fine-grained analyses on transfer behaviors reveal the types of transferred knowledge and transfer patterns. We conclude with insights into how to construct better QA datasets and systems to exploit knowledge transfer for future work.",
        "id": 250390946
      }
    ],
    "negative_ctxs": [
      {
        "title": "Rapid deployment of a new METIS language pair: Catalan-English",
        "text": "We show here the viability of a rapid deployment of a new language pair within the METIS architecture. Contrarily to other SMT or EBMT systems, the METIS architecture allows us to forgo parallel texts, which for many language pairs, such as Catalan-English are hard to obtain. In this experiment, we have successfully built a Catalan-English prototype by simply plugging a POS tagger for Catalan and a bilingual Catalan-English dictionary to the English generation part of the system already developed for other language pairs.",
        "id": 14468016
      },
      {
        "title": "",
        "text": "",
        "id": 235097514
      },
      {
        "title": "A Constraint-based Representation Scheme of Collocational Structures",
        "text": "t OTS -Trans 10 -3512 JK Utrecht (NL) CLMT Essex University -Wivenhoe Park -C04 3SQ Colchester (UK)",
        "id": 7155034
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "what's the first paper that manages to handle KBQA using LLMs without fine-tuning?",
    "positive_ctxs": [
      {
        "title": "A Proposal for Grounding Language Models to Real-World Environments",
        "text": "A key missing capacity of current language models (LMs) is grounding to real-world environments. Most existing work for grounded language understanding uses LMs to directly generate plans that can be executed in the environment to achieve the desired effects. It thereby casts the burden of ensuring grammaticality, faithfulness, and controllability all on the LMs. We propose Pangu, a generic framework for grounded language understanding that capitalizes on the discriminative ability of LMs instead of their generative ability. Pangu consists of a symbolic agent and a neural LM working in a concerted fashion: The agent explores the environment to incrementally construct valid plans, and the LM evaluates the plausibility of the candidate plans to guide the search process. A case study on the challenging problem of knowledge base question answering (KBQA), which features a massive environment, demonstrates the remarkable effectiveness and flexibility of Pangu: A BERT-base LM is sufficient for setting a new record on standard KBQA datasets, and larger LMs further bring substantial gains. Pangu also enables, for the first time, effective few-shot in-context learning for KBQA with large LMs such as Codex. 1   . 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In . 2022. Program transfer for answering complex questions over knowledge bases. In . 2021a. Evaluating large language models trained on code. CoRR, abs/2107.03374. . 2021b. ReTraCk: A flexible and efficient framework for knowledge base question answering. In . 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In . 2022. Holistic evaluation of language models. CoRR, abs/2211.09110. 4939 Dhruv Shah, Błażej Osiński, brian ichter, and Sergey Levine. 2022. LM-Nav: Robotic navigation with large pre-trained models of language, vision, and action. . 2022. TIARA: Multi-grained retrieval for robust question answering over large knowledge base. In Proceed-. 2022b. LLM-Planner: Few-shot grounded planning for embodied agents with large language models. CoRR, abs/2212.04088.",
        "id": 254853929
      }
    ],
    "negative_ctxs": [
      {
        "title": "Anchors Regularized: Adding Robustness and Extensibility to Scalable Topic-Modeling Algorithms",
        "text": "Spectral methods offer scalable alternatives to Markov chain Monte Carlo and expectation maximization. However, these new methods lack the rich priors associated with probabilistic models. We examine Arora et al.'s anchor words algorithm for topic modeling and develop new, regularized algorithms that not only mathematically resemble Gaussian and Dirichlet priors but also improve the interpretability of topic models. Our new regularization approaches make these efficient algorithms more flexible; we also show that these methods can be combined with informed priors.",
        "id": 216848275
      },
      {
        "title": "Augmenting Reddit Posts to Determine Wellness Dimensions impacting Mental Health",
        "text": "Amid ongoing health crisis, there is a growing necessity to discern possible signs of Wellness Dimensions (WD) 1 manifested in self-narrated text. As the distribution of WD on social media data is intrinsically imbalanced, we experiment the generative NLP models for data augmentation to enable further improvement in the prescreening task of classifying WD. To this end, we propose a simple yet effective data augmentation approach through promptbased Generative NLP models, and evaluate the ROUGE scores and syntactic/semantic similarity among existing interpretations and augmented data. Our approach with ChatGPT model surpasses all the other methods and achieves improvement over baselines such as Easy-Data Augmentation and Backtranslation. Introducing data augmentation to generate more training samples and balanced dataset, results in the improved F-score and the Matthew's Correlation Coefficient for upto 13.11% and 15.95%, respectively.Wellness Dimensions",
        "id": 259095669
      },
      {
        "title": "INFERES : A Natural Language Inference Corpus for Spanish Featuring Negation-Based Contrastive and Adversarial Examples",
        "text": "In this paper, we present INFERES -an original corpus for Natural Language Inference (NLI) in European Spanish. We propose, implement, and analyze a variety of corpuscreating strategies utilizing expert linguists and crowd workers. The objectives behind IN-FERES are to provide high-quality data, and, at the same time to facilitate the systematic evaluation of automated systems. Specifically, we focus on measuring and improving the performance of machine learning systems on negation-based adversarial examples and their ability to generalize across out-of-distribution topics.We train two transformer models on IN-FERES (8,055 gold examples) in a variety of scenarios. Our best model obtains 72.8% accuracy, leaving a lot of room for improvement. The \"hypothesis-only\" baseline performs only 2%-5% higher than majority, indicating much fewer annotation artifacts than prior work. We find that models trained on INFERES generalize very well across topics (both in-and outof-distribution) and perform moderately well on negation-based adversarial examples.",
        "id": 252735141
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "What literature is available on training semantic parsers with deep learning for knowledge base question answering systems, especially those employing tree-structured representations of queries?",
    "positive_ctxs": [
      {
        "title": "Leveraging Abstract Meaning Representation for Knowledge Base Question Answering",
        "text": "Knowledge base question answering (KBQA) is an important task in Natural Language Processing. Existing approaches face significant challenges including complex question understanding, necessity for reasoning, and lack of large end-to-end training datasets. In this work, we propose Neuro-Symbolic Question Answering (NSQA), a modular KBQA system, that leverages (1) Abstract Meaning Representation (AMR) parses for task-independent question understanding; (2) a simple yet effective graph transformation approach to convert AMR parses into candidate logical queries that are aligned to the KB; (3) a pipeline-based approach which integrates multiple, reusable modules that are trained specifically for their individual tasks (semantic parser, entity and relationship linkers, and neuro-symbolic reasoner) and do not require end-to-end training data. NSQA achieves state-of-the-art performance on two prominent KBQA datasets based on DBpedia (QALD-9 and LC-QuAD 1.0). Furthermore, our analysis emphasizes that AMR is a powerful tool for KBQA systems.",
        "id": 235303644
      }
    ],
    "negative_ctxs": [
      {
        "title": "The SIGMORPHON 2016 Shared Task-Morphological Reinflection",
        "text": "The 2016 SIGMORPHON Shared Task was devoted to the problem of morphological reinflection. It introduced morphological datasets for 10 languages with diverse typological characteristics. The shared task drew submissions from 9 teams representing 11 institutions reflecting a variety of approaches to addressing supervised learning of reinflection. For the simplest task, inflection generation from lemmas, the best system averaged 95.56% exact-match accuracy across all languages, ranging from Maltese (88.99%) to Hungarian (99.30%). With the relatively large training datasets provided, recurrent neural network architectures consistently performed best-in fact, there was a significant margin between neural and non-neural approaches. The best neural approach, averaged over all tasks and languages, outperformed the best nonneural one by 13.76% absolute; on individual tasks and languages the gap in accuracy sometimes exceeded 60%. Overall, the results show a strong state of the art, and serve as encouragement for future shared tasks that explore morphological analysis and generation with varying degrees of supervision.",
        "id": 18613906
      },
      {
        "title": "Does a Virtual Talking Face Generate Proper Multimodal Cues to Draw User's Attention to Points of Interest?",
        "text": "We present a series of experiments investigating face-to-face interaction between an Embodied Conversational Agent (ECA) and a human interlocutor. The ECA is embodied by a video realistic talking head with independent head and eye movements. For a beneficial application in face-to-face interaction, the ECA should be able to derive meaning from communicational gestures of a human interlocutor, and likewise to reproduce such gestures. Conveying its capability to interpret human behaviour, the system encourages the interlocutor to show appropriate natural activity. Therefore it is important that the ECA knows how to display what would correspond to mental states in humans. This allows to interpret the machine processes of the system in terms of human expressiveness and to assign them a corresponding meaning. Thus the system may maintain an interaction based on human patterns. During a first experiment we investigated the ability of our talking head to direct user attention with facial deictic cues(Raidt, Bailly et al. 2005). Users interact with the ECA during a simple card game offering different levels of help and guidance through facial deictic cues. We analyzed the users' performance and their perception of the quality of assistance given by the ECA. The experiment showed that users profit from its presence and its facial deictic cues. In the continuative series of experiments presented here, we investigated the effect of an enhancement of the multimodality of the deictic gestures by adding a spoken instruction.",
        "id": 6040286
      },
      {
        "title": "Normalized Contrastive Learning for Text-Video Retrieval",
        "text": "Cross-modal contrastive learning has led the recent advances in multimodal retrieval with its simplicity and effectiveness. In this work, however, we reveal that cross-modal contrastive learning suffers from incorrect normalization of the sum retrieval probabilities of each text or video instance. Specifically, we show that many test instances are either overor under-represented during retrieval, significantly hurting the retrieval performance. To address this problem, we propose Normalized Contrastive Learning (NCL) which utilizes the Sinkhorn-Knopp algorithm to compute the instance-wise biases that properly normalize the sum retrieval probabilities of each instance so that every text and video instance is fairly represented during cross-modal retrieval. Empirical study shows that NCL brings consistent and significant gains in text-video retrieval on different model architectures, with new stateof-the-art multimodal retrieval metrics on the ActivityNet, MSVD, and MSR-VTT datasets without any architecture engineering.",
        "id": 254974222
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "What are some scholarly articles that explore scaling laws for parameter-efficient prompt tuning techniques for fine-tuning language models?",
    "positive_ctxs": [
      {
        "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
        "text": "In this work, we explore \"prompt tuning,\" a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient \"prompt ensembling.\" We release code and model checkpoints to reproduce our experiments. 1ReferencesRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,  Danilo Giampiccolo, Bernardo Magnini, and Idan  Szpektor. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognising textual entailment, volume 6, pages 6-4. Venice. . 2019a. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
        "id": 233296808
      }
    ],
    "negative_ctxs": [
      {
        "title": "AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models",
        "text": "Pretrained language models (PLMs) are trained on massive corpora, but often need to specialize to specific domains. A parameter-efficient adaptation method suggests training an adapter for each domain on the task of language modeling. This leads to good in-domain scores but can be impractical for domain-or resourcerestricted settings. A solution is to use a relateddomain adapter for the novel domain at test time. In this paper, we introduce Adapter-Soup, an approach that performs weight-space averaging of adapters trained on different domains. Our approach is embarrassingly parallel: first, we train a set of domain-specific adapters; then, for each novel domain, we determine which adapters should be averaged at test time. We present extensive experiments showing that AdapterSoup consistently improves performance to new domains without extra training. We also explore weight averaging of adapters trained on the same domain with different hyper-parameters, and show that it preserves the performance of a PLM on new domains while obtaining strong in-domain results. We explore various approaches for choosing which adapters to combine, such as text clustering and semantic similarity. We find that using clustering leads to the most competitive results on novel domains.Domain AdapterAdapter Soupθ D k moyer. 2022. Branch-train-merge: Embarrassingly parallel training of expert language models. . 2019. Roberta: A robustly optimized bert pretraining approach.Michael Matena and Colin Raffel. 2021. Merging models with fisher-weighted averaging.",
        "id": 256846453
      },
      {
        "title": "Mini But Mighty: Efficient Multilingual Pretraining with Linguistically-Informed Data Selection",
        "text": "With the prominence of large pretrained language models, low-resource languages are rarely modelled monolingually and become victims of the \"curse of multilinguality\" in massively multilingual models. Recently, Afri-BERTa showed that training transformer models from scratch on 1GB of data from many unrelated African languages outperforms massively multilingual models on downstream NLP tasks. Here we extend this direction, focusing on the use of related languages. We propose that training on smaller amounts of data but from related languages could match the performance of models trained on large, unrelated data. We test our hypothesis on the Niger-Congo family and its Bantu and Volta-Niger sub-families, pretraining models with data solely from Niger-Congo languages and finetuning on 4 downstream tasks: NER, partof-speech tagging, sentiment analysis and text classification. We find that models trained on genetically related languages achieve equal performance on downstream tasks in low-resource languages despite using less training data. We recommend selecting training data based on language-relatedness when pretraining language models for low-resource languages.",
        "id": 258378192
      },
      {
        "title": "F-Score Driven Max Margin Neural Network for Named Entity Recognition in Chinese Social Media",
        "text": "We focus on named entity recognition (NER) for Chinese social media. With massive unlabeled text and quite limited labelled corpus, we propose a semisupervised learning model based on B-LSTM neural network. To take advantage of traditional methods in NER such as CRF, we combine transition probability with deep learning in our model. To bridge the gap between label accuracy and F-score of NER, we construct a model which can be directly trained on F-score. When considering the instability of Fscore driven method and meaningful information provided by label accuracy, we propose an integrated method to train on both F-score and label accuracy. Our integrated model yields substantial improvement over previous state-of-the-art result.",
        "id": 14874026
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "If one would like to train (or evaluate) a helpful assistant agent that can converse with humans while the humans traverse an environment, which work has the most suitable resource?",
    "positive_ctxs": [
      {
        "title": "SIMMC-VR: A Task-oriented Multimodal Dialog Dataset with Situated and Immersive VR Streams",
        "text": "Building an AI assistant that can seamlessly converse and instruct humans, in a user-centric situated scenario, requires several essential abilities: (1) spatial and temporal understanding of the situated and real-time user scenes, (2) capability of grounding the actively perceived visuals of users to conversation contexts, and (3) conversational reasoning over past utterances to perform just-in-time assistance. However, we currently lack a large-scale benchmark that captures user↔assistant interactions with all of the aforementioned features. To this end, we propose SIMMC-VR, an extension of the SIMMC 2.0 dataset to a video-grounded taskoriented dialog dataset that captures real-world AI-assisted user scenarios in VR. We propose a novel data collection paradigm that involves (1) generating object-centric multimodal dialog flows with egocentric visual streams and visually-grounded templates, and (2) manually paraphrasing the simulated dialogs for naturalness and diversity while preserving multimodal dependencies. To measure meaningful progress in the field, we propose four tasks to address the new challenges in SIMMC-VR, which require complex spatial-temporal dialog reasoning in active egocentric scenes. We benchmark the proposed tasks with strong multimodal models, and highlight the key capabilities that current models lack for future research directions. * Work done during an internship at Meta Reality Labs.",
        "id": 259370710
      }
    ],
    "negative_ctxs": [
      {
        "title": "Maximum Entropy Model Learning of Subcategorization Preference* I t",
        "text": "This paper proposes a novel method for learning probabilistic models of subcategorization preference of verbs. Especially, we propose to consider the issues of case dependencie~ and noun class generalization in a uniform way. We adopt the maximum entropy model learn~,g method and apply it to the task of model learning of subcategorization preference. Case dependencies and noun class generalization are represented as featura~ in the maximum entropy approach. The feature selection facility of the maximum entropy model learning makes it possible to find optimal case dependencies and optimal noun c!~ generalization levels. We describe the results of the experiment on learning probabilistic models of subcategorization preference f~om the EDR Japanese bracketed corpus. We also evaluated the performance of the selected features and their estimated parameters in the subcategorization preference task.",
        "id": 14732351
      },
      {
        "title": "Returning-Home Analysis in Tokyo Metropolitan Area at the time of the Great East Japan Earthquake using Twitter Data",
        "text": "This paper clarifies the occurrence factors of commuters unable to return home and the returning-home decision-making at the time of the Great East Japan Earthquake by using Twitter data. First, to extract the behavior data from the tweet data, we identify each user's returning-home behavior using support vector machines. Second, we create non-verbal explanatory factors using geotag data and verbal explanatory factors using tweet data. Then, we model users' returning-home decisionmaking by using a discrete choice model and clarify the factors quantitatively. Finally, by sensitivity analysis, we show the effects of the existence of emergency evacuation facilities and line of communication.",
        "id": 7821398
      },
      {
        "title": "Building Dialectal Arabic Corpora",
        "text": "The aim of this research is to identify local Arabic dialects in texts from social media (Twitter) and link them to specific geographic areas. Dialect identification is studied as a subset of the task of language identification. The proposed method is based on unsupervised learning using simultaneously lexical and geographic distance. While this study focusses on Libyan dialects, the approach is general, and could produce resources to support human translators and interpreters when dealing with vernaculars rather than standard Arabic.",
        "id": 7303263
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Could you point me to research on binary classification systems that predict whether sentences within a context require clarification, particularly using the wikiHowToImprove dataset?",
    "positive_ctxs": [
      {
        "title": "UnImplicit Shared Task Report: Detecting Clarification Requirements in Instructional Text",
        "text": "This paper describes the data, task setup, and results of the shared task at the First Workshop on Understanding Implicit and Underspecified Language (UnImplicit). The task requires computational models to predict whether a sentence contains aspects of meaning that are contextually unspecified and thus require clarification. Two teams participated and the best scoring system achieved an accuracy of 68%.",
        "id": 236486314
      }
    ],
    "negative_ctxs": [
      {
        "title": "Logical metonymies and qualia structures: an annotated database of logical metonymies for German",
        "text": "Logical metonymies like The author began the book involve the interpretation of events that are not realized in the sentence (covert events: → writing the book). The Generative Lexicon (Pustejovsky, 1995) provides a qualia-based account of covert event interpretation, claiming that the covert event is retrieved from the qualia structure of the object. Such a theory poses the question of to what extent covert events in logical metonymies can be accounted for by qualia structures. Building on previous work on English, we present a corpus study for German verbs (anfangen (mit), aufhören (mit), beenden, beginnen (mit), geniessen), based on data obtained from the deWaC corpus. We built a corpus of logical metonymies, which were manually annotated and compared with the qualia structures of their objects, then we contrasted annotation results from two expert annotators for metonymies (The author began the book) and long forms (The author began reading the book) across verbs. Our annotation was evaluated on a sample of sentences annotated by a group of naive annotators on a crowdsourcing platform. The logical metonymy database (2661 metonymies and 1886 long forms) with two expert annotations is freely available for scientific research purposes.",
        "id": 14484317
      },
      {
        "title": "INFERENCING ON LINGUISTICALLY BASED ZZ~IANTIC STRUCTUR~F",
        "text": "The paper characterizes natural language inferencing in the TIBAQ method of question-answering, focussing on three aspects: ~i) specification of the structures on which the inference rules operate, (ii) classification of the rules that have been formulated and implemented up to now, according to the kind of modification of the input structure ti~e rules invoke, an~ (iii) discussion of some points in which a proverly designed inference procedure may help the searc~ of the answer, and vice versa.",
        "id": 1069475
      },
      {
        "title": "Sudachi: a Japanese Tokenizer for Business",
        "text": "Tokenization, or morphological analysis, is a fundamental and important technology for processing a Japanese text, especially for industrial applications. However, we often face many obstacles, such as the inconsistency of token unit in different resources, notation variations, discontinued maintenance of the resources, and various issues with the existing tokenizer implementations. In order to improve this situation, we develop a tokenizer called Sudachi and its accompanying dictionary with features such as multi-granular output and normalization of notation variations. In addition to this, we continuously maintain our software and language resources in long-term as a part of the company business. We release the resulting tokenizer software and language resources freely available to the public as an open source software. You can access them at https://github.com/WorksApplications/Sudachi.",
        "id": 21686399
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "What approaches have been suggested to lower the computational demands of basic attention mechanisms in transformers, and is there a study that explores a memory-saving technique through selective key-value pairing for each query?",
    "positive_ctxs": [
      {
        "title": "Memory-efficient Transformers via Top-k Attention",
        "text": "Following the success of dot-product attention in Transformers, numerous approximations have been recently proposed to address its quadratic complexity with respect to the input length. While these variants are memory and compute efficient, it is not possible to directly use them with popular pre-trained language models trained using vanilla attention, without an expensive corrective pre-training stage. In this work, we propose a simple yet highly accurate approximation for vanilla attention. We process the queries in chunks, and for each query, compute the top-k scores with respect to the keys. Our approach offers several advantages: (a) its memory usage is linear in the input size, similar to linear attention variants, such as Performer and RFA (b) it is a drop-in replacement for vanilla attention that does not require any corrective pre-training, and (c) it can also lead to significant memory savings in the feed-forward layers after casting them into the familiar query-key-value framework. We evaluate the quality of top-k approximation for multi-head attention layers on the Long Range Arena Benchmark, and for feedforward layers of T5 and UnifiedQA on multiple QA datasets. We show our approach leads to accuracy that is nearly-identical to vanilla attention in multiple setups including training from scratch, fine-tuning, and zero-shot inference. * majority of work done while author was part of IBM AI Residency program. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150.",
        "id": 235422257
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 8565686
      },
      {
        "title": "Shallow Convolutional Neural Network for Implicit Discourse Relation Recognition",
        "text": "Implicit discourse relation recognition remains a serious challenge due to the absence of discourse connectives. In this paper, we propose a Shallow Convolutional Neural Network (SCNN) for implicit discourse relation recognition, which contains only one hidden layer but is effective in relation recognition. The shallow structure alleviates the overfitting problem, while the convolution and nonlinear operations help preserve the recognition and generalization ability of our model. Experiments on the benchmark data set show that our model achieves comparable and even better performance when comparing against current state-of-the-art systems.",
        "id": 12968123
      },
      {
        "title": "KYOTO: A System for Mining, Structuring, and Distributing Knowledge Across Languages and Cultures",
        "text": "We outline work performed within the framework of a current EC project. The goal is to construct a language-independent information system for a specific domain (environment/ecology/biodiversity) anchored in a language-independent ontology that is linked to wordnets in seven languages. For each language, information extraction and identification of lexicalized concepts with ontological entries is carried out by text miners (\"Kybots\"). The mapping of language-specific lexemes to the ontology allows for crosslinguistic identification and translation of equivalent terms. The infrastructure developed within this project enables long-range knowledge sharing and transfer across many languages and cultures, addressing the need for global and uniform transition of knowledge beyond the specific domains addressed here.",
        "id": 3074735
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "What work proposes to combine video foundation models with vision language models to effective high dimensional robot planning?",
    "positive_ctxs": [
      {
        "title": "VIDEO LANGUAGE PLANNING",
        "text": "We are interested in enabling visual planning for complex long-horizon tasks in the space of generated videos and language, leveraging recent advances in large generative models pretrained on Internet-scale data. To this end, we present video language planning (VLP), an algorithm that consists of a tree search procedure, where we train (i) vision-language models to serve as both policies and value functions, and (ii) text-to-video models as dynamics models. VLP takes as input a long-horizon task instruction and current image observation, and outputs a long video plan that provides detailed multimodal (video and language) specifications that describe how to complete the final task. VLP scales with increasing computation budget where more computation time results in improved video plans, and is able to synthesize long-horizon video plans across different robotics domainsfrom multi-object rearrangement, to multi-camera bi-arm dexterous manipulation. Generated video plans can be translated into real robot actions via goal-conditioned policies, conditioned on each intermediate frame of the generated video. Experiments show that VLP substantially improves long-horizon task success rates compared to prior methods on both simulated and real robots (across 3 hardware platforms).Step 1: push blue triangle to …Step 1: push red star to left …Step 2: (re-plan) t t + k t current t goalStep 2: (re-plan)Step 2: (re-plan)",
        "id": 264172935
      }
    ],
    "negative_ctxs": [
      {
        "title": "Learning Rationalizable Equilibria in Multiplayer Games",
        "text": "A natural goal in multiagent learning besides finding equilibria is to learn rationalizable behavior, where players learn to avoid iteratively dominated actions. However, even in the basic setting of multiplayer general-sum games, existing algorithms require a number of samples exponential in the number of players to learn rationalizable equilibria under bandit feedback. This paper develops the first line of efficient algorithms for learning rationalizable Coarse Correlated Equilibria (CCE) and Correlated Equilibria (CE) whose sample complexities are polynomial in all problem parameters including the number of players. To achieve this result, we also develop a new efficient algorithm for the simpler task of finding one rationalizable action profile (not necessarily an equilibrium), whose sample complexity substantially improves over the best existing results of Wu et al. (2021). Our algorithms incorporate several novel techniques to guarantee rationalizability and no (swap-)regret simultaneously, including a correlated exploration scheme and adaptive learning rates, which may be of independent interest. We complement our results with a sample complexity lower bound showing the sharpness of our guarantees. * Equal contribution. ∆ 2 1 samples in normal-form games with N players, A actions per player and a minimum elimination length of L. This greatly improves the result of Wu et al. (2021) and is tight up to logarithmic factors when L = O(1). 1 Throughout this paper, we use O to suppress logarithmic factors in N , A,",
        "id": 253018433
      },
      {
        "title": "Published as a conference paper at ICLR 2021 DO NOT LET PRIVACY OVERBILL UTILITY: GRADIENT EMBEDDING PERTURBATION FOR PRIVATE LEARNING",
        "text": "The privacy leakage of the model about the training data can be bounded in the differential privacy mechanism. However, for meaningful privacy parameters, a differentially private model degrades the utility drastically when the model comprises a large number of trainable parameters. In this paper, we propose an algorithm Gradient Embedding Perturbation (GEP) towards training differentially private deep models with decent accuracy. Specifically, in each gradient descent step, GEP first projects individual private gradient into a non-sensitive anchor subspace, producing a low-dimensional gradient embedding and a small-norm residual gradient. Then, GEP perturbs the low-dimensional embedding and the residual gradient separately according to the privacy budget. Such a decomposition permits a small perturbation variance, which greatly helps to break the dimensional barrier of private learning. With GEP, we achieve decent accuracy with reasonable computational cost and modest privacy guarantee for deep models. Especially, with privacy bound = 8, we achieve 74.9% test accuracy on CIFAR10 and 95.1% test accuracy on SVHN, significantly improving over existing results. * Authors contribute equally to this work.",
        "id": 232046284
      },
      {
        "title": "A Survey of Leveled Reading 呔周呥 呣呯呮呣呥呰呴 呯呦 呌呥呶呥呬呥呤 呒呥呡呤呩呮呧 吨呌呒吩 呯呲呩呧呩呮呡呴呥味 呩呮 呴周呥 呥呡呲呬呹 呴呷呥呮呴呩呥呴周 呣呥呮呴呵呲呹 呡呭呯呮呧 呥呤呵呣呡呴呯呲味吮 呁味 呰呥呯呰呬呥 呧呲呡呤呵呡呬呬呹 呥呭呰周呡味呩呺呥 呭呯呲呥 呯呮 呲呥呡呤呩呮呧听 呴周呥呹 呰呡呹 呭呯呲呥 呡呴呴呥呮呴呩呯呮 呴呯 呌呒听 呡呮呤 呴周呵味 呦呯味呴呥呲呩呮呧 呤呥呶呥呬呯呰呭呥呮呴味 呩呮 呡呵呴呯呭呡呴呩呣 呌呒 呭呥呴周呯呤味吮 呔周呩味 呰呡呰呥呲 呰呲呯呶呩呤呥味 呡呮 呯呶呥呲呶呩呥呷 呯呦 呴周呥 呲呥呣呥呮呴 呌呒 呤呥呶呥呬呯呰呭呥呮呴味吮 呆呩呲味呴呬呹听 呷呥 呩呮呴呲呯呤呵呣呥 呴周呥 呥呸呩味呴呩呮呧 味呴呡呮呤呡呲呤味 呯呦 呌呒听 呦呯呬呬呯呷呥呤 呢呹 呴周呥 呤呥呶呥呬呯呰呭呥呮呴 呯呦 呶呡呲呩呯呵味 味呹味呴呥呭味 呡呮呤 呤呡呴呡 呲呥味呯呵呲呣呥味吮 呔周呥呮听 呷呥 呣呬呡味吭 味呩呦呹 呷呩呤呥呬呹吭呡呰呰呬呩呥呤 呡呵呴呯呭呡呴呩呣 呌呒 呭呥呴周呯呤味 呩呮呴呯 呴周呲呥呥 呴呹呰呥味吺呦呯呲呭呵呬呡 呭呥呴周呯呤听 呴呲呡呤呩呴呩呯呮呡呬 呭呡呣周呩呮呥 呬呥呡呲呮呩呮呧 呭呥呴周呯呤 呡呮呤 呴周呥 呲呥呣呥呮呴呬呹 呭呡呩呮味呴呲呥呡呭 呤呥呥呰 呬呥呡呲呮呩呮呧 呭呥呴周呯呤吮 呗呥 呥呸吭 呰呬呯呲呥 呴周呥 呡呤呶呡呮呴呡呧呥味 呡呮呤 呤呩味呡呤呶呡呮呴呡呧呥味 呯呦 呴周呥味呥 呭呥呴周呯呤味 呴周呲呯呵呧周 呥呸呰呥呲呩呭呥呮呴味 呡呮呤 呩呮呶呥味呴呩呧呡呴呥 呰呯味味呩呢呬呥 呰呥呲呦呯呲呭呡呮呣呥 呩呭呰呲呯呶呥呭呥呮呴味吮 呆呩呮呡呬呬呹听 呷呥 味呵呭呭呡呲呩呺呥 呡呮呤 呰呲呯味呰呥呣呴 呴周呥 呦呵呴呵呲呥 呤呥呶呥呬呯呰呭呥呮呴 呯呦 呌呒 呡呮呤 呰呲呯呶呩呤呥 味呥呶呥呲呡呬 同呥呬呤味 呴周呡呮 呣呡呮 呢呥呮呥同呴 呦呲呯呭 呌呒吮",
        "text": "",
        "id": 237365388
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Where can I find a database of good prompts to use for prompting language models for in-context learning?",
    "positive_ctxs": [
      {
        "title": "OpenPrompt: An Open-source Framework for Prompt-learning",
        "text": "Prompt-learning has become a new paradigm in modern natural language processing, which directly adapts pre-trained language models (PLMs) to cloze-style prediction, autoregressive modeling, or sequence to sequence generation, resulting in promising performances on various tasks. However, no standard implementation framework of prompt-learning is proposed yet, and most existing promptlearning codebases, often unregulated, only provide limited implementations for specific scenarios. Since there are many details such as templating strategy, initializing strategy, and verbalizing strategy, etc., need to be considered in prompt-learning, practitioners face impediments to quickly adapting the desired prompt learning methods to their applications. In this paper, we present Open-Prompt, a unified easy-to-use toolkit to conduct prompt-learning over PLMs.Open-Prompt is a research-friendly framework that is equipped with efficiency, modularity, and extendibility, and its combinability allows the freedom to combine different PLMs, task formats, and prompting modules in a unified paradigm. Users could expediently deploy prompt-learning frameworks and evaluate the generalization of them on different NLP tasks without constraints. 1",
        "id": 241033259
      }
    ],
    "negative_ctxs": [
      {
        "title": "Automatic Refinement of Syntactic Categories in Chinese Word Structures",
        "text": "Annotated word structures are useful for various Chinese NLP tasks, such as word segmentation, POS tagging and syntactic parsing. Chinese word structures are often represented by binary trees, the nodes of which are labeled with syntactic categories, due to the syntactic nature of Chinese word formation. It is desirable to refine the annotation by labeling nodes of word structure trees with more proper syntactic categories so that the combinatorial properties in the word formation process are better captured. This can lead to improved performances on the tasks that exploit word structure annotations. We propose syntactically inspired algorithms to automatically induce syntactic categories of word structure trees using POS tagged corpus and branching in existing Chinese word structure trees. We evaluate the quality of our annotation by comparing the performances of models based on our annotation and another publicly available annotation, respectively. The results on two variations of Chinese word segmentation task show that using our annotation can lead to significant performance improvements.",
        "id": 8158821
      },
      {
        "title": "La désambiguïsation des abréviations du domaine médical",
        "text": "Les abréviations, tout en étant répandues dans la langue, ont une sémantique assez opaque car seulement les premières lettres sont transparentes. Cela peut donc empêcher la compréhension des abréviations, et des textes qui les contiennent, par les locuteurs. De plus, certaines abréviations sont ambiguës en ayant plusieurs sens possibles, ce qui augmente la difficulté de leur compréhension. Nous proposons de travailler avec les abréviations de la langue médicale dans un cadre lié à la simplification automatique de textes. Dans le processus de simplification, il faut en effet choisir la forme étendue des abréviations qui soit correcte pour un contexte donné. Nous proposons de traiter la désambiguïsation d'abréviations comme un problème de catégorisation supervisée. Les descripteurs sont construits à partir des contextes lexical et syntaxique des abréviations. L'entraînement est effectué sur les phrases qui contiennent les formes étendues des abréviations. Le test est effectué sur un corpus construit manuellement, où les bons sens des abréviations ont été définis selon les contextes. Notre approche montre une F-mesure moyenne de 0,888 sur le corpus d'entraînement en validation croisée et 0,773 sur le corpus de test.ABSTRACTDisambiguation of abbreviations from the medical domain.Abbreviations, although commonly used, have quite opaque semantics because only their first letters are transparent. This may prevent from understanding of abbreviations, and of texts they occur within, by speakers. Besides, some abbreviations are ambiguous and have more than one meaning, which increases their understanding difficulty. We propose to work with abbreviations from the medical domain as part of the automatic text simplification. During the simplification process, it is indeed necessary to chose the right expanded form of abbreviations satisfying a given context. We propose to address disambiguation of abbreviations as supervised categorization problem. Descriptors are built from lexical and syntactic contexts of the abbreviations. Training is done on sentences containing expanded forms of the abbreviations. Test is done on corpus built manually, in which the correct senses of abbreviations have been defined according to their contexts. The average F-measure of our approach is 0.888 in cross-validation on the training corpus and 0.773 on the test corpus. MOTS-CLÉS : Désambiguïsation sémantique, domaine biomédical, abréviations, simplification.",
        "id": 221373803
      },
      {
        "title": "",
        "text": "",
        "id": 10736550
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Are there any papers that build dense retrievers with mixture-of-experts architecture where each expert is responsible for different types of queries?",
    "positive_ctxs": [
      {
        "title": "Chain-of-Skills: A Configurable Model for Open-Domain Question Answering",
        "text": "The retrieval model is an indispensable component for real-world knowledge-intensive tasks, e.g., open-domain question answering (ODQA). As separate retrieval skills are annotated for different datasets, recent work focuses on customized methods, limiting the model transferability and scalability. In this work, we propose a modular retriever where individual modules correspond to key skills that can be reused across datasets. Our approach supports flexible skill configurations based on the target domain to boost performance. To mitigate task interference, we design a novel modularization parameterization inspired by sparse Transformer. We demonstrate that our model can benefit from self-supervised pretraining on Wikipedia and fine-tuning using multiple ODQA datasets, both in a multi-task fashion. Our approach outperforms recent self-supervised retrievers in zero-shot evaluations and achieves state-ofthe-art fine-tuned retrieval performance on NQ, HotpotQA and OTT-QA.",
        "id": 258546861
      }
    ],
    "negative_ctxs": [
      {
        "title": "Creating a PurposeNet Ontology: An insight into the issues encountered during ontology creation",
        "text": "PurposeNet is an ontology based on the principle that all artifacts (man-made objects) exist for a purpose and all its features and relations with other entities are giverened by its purpose. We provide instances of ontology creation for two varied domains from scratch in the Pur-poseNet architecture. These domains include MMTS domain and recipe domain. The methodology of creation was totally different for the two domains. MMTS domain was more computaionally oriented ontology while recipe domain required a post-processing after manually entering the data. The post-processing step uses hierarchical clustering to cluster very close actions. MMTS ontology is further used to create a simple template based QA system and the results are compared with a database system for the same domain.",
        "id": 1198659
      },
      {
        "title": "From discourse structures to text summaries",
        "text": "We describe experiments that show that the concepts of rhetorical analysts and nucleanty can be used effectively for deternumng the most nnportant umts m a text We show how these concepts can be xmplemented and we discuss results that we obtained with a chscourse-based summanzatmn program",
        "id": 11680756
      },
      {
        "title": "",
        "text": "",
        "id": 207988685
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "What paper first proposes that simply reversing the output can significantly enhance the sample efficiency and the performance of the arithmetic capability of a decoder-only Transformer model?",
    "positive_ctxs": [
      {
        "title": "Teaching Arithmetic to Small Transformers",
        "text": "Large language models like GPT-4 exhibit emergent capabilities across generalpurpose tasks, such as basic arithmetic, when trained on extensive text data, even though these tasks are not explicitly encoded by the unsupervised, next-token prediction objective. This study investigates how small transformers, trained from random initialization, can efficiently learn arithmetic operations such as addition, multiplication, and elementary functions like square root, using the nexttoken prediction objective. We first demonstrate that conventional training data is not the most effective for arithmetic learning, and simple formatting changes can significantly improve accuracy. This leads to sharp phase transitions as a function of training data scale, which, in some cases, can be explained through connections to low-rank matrix completion. Building on prior work, we then train on chain-of-thought style data that includes intermediate step results. Even in the complete absence of pretraining, this approach significantly and simultaneously improves accuracy, sample complexity, and convergence speed. We also study the interplay between arithmetic and text data during training and examine the effects of few-shot prompting, pretraining, and model scale. Additionally, we discuss length generalization challenges. Our work highlights the importance of high-quality, instructive data that considers the particular characteristics of the next-word prediction objective for rapidly eliciting arithmetic capabilities. 2 * Authors contributed equally to this paper. 2 Our code is available at https://github.com/lee-ny/teaching_arithmetic Preprint. Under review.",
        "id": 259375870
      }
    ],
    "negative_ctxs": [
      {
        "title": "SMBOP: Semi-autoregressive Bottom-up Semantic Parsing",
        "text": "The de-facto standard decoding method for semantic parsing in recent years has been to autoregressively decode the abstract syntax tree of the target program using a top-down depthfirst traversal. In this work, we propose an alternative approach: a Semi-autoregressive Bottom-up Parser (SMBOP) that constructs at decoding step t the top-K sub-trees of height ≤ t. Our parser enjoys several benefits compared to top-down autoregressive parsing. From an efficiency perspective, bottom-up parsing allows to decode all sub-trees of a certain height in parallel, leading to logarithmic runtime complexity rather than linear. From a modeling perspective, a bottom-up parser learns representations for meaningful semantic sub-programs at each step, rather than for semantically-vacuous partial trees. We apply SMBOP on SPIDER, a challenging zero-shot semantic parsing benchmark, and show that SMBOP leads to a 2.2x speed-up in decoding time and a ∼5x speed-up in training time, compared to a semantic parser that uses autoregressive decoding. SMBOP obtains 71.1 denotation accuracy on SPIDER, establishing a new state-of-the-art, and 69.5 exact match, comparable to the 69.6 exact match of the autoregressive RAT-SQL+GRAPPA.",
        "id": 225062282
      },
      {
        "title": "Syntactic parsing of chat language in contact center conversation corpus",
        "text": "Chat language is often referred to as Computer-mediated communication (CMC). Most of the previous studies on chat language has been dedicated to collecting \"chat room\" data as it is the kind of data which is the most accessible on the WEB. This kind of data falls under the informal register whereas we are interested in this paper in understanding the mechanisms of a more formal kind of CMC: dialog chat in contact centers. The particularities of this type of dialogs and the type of language used by customers and agents is the focus of this paper towards understanding this new kind of CMC data. The challenges for processing chat data comes from the fact that Natural Language Processing tools such as syntactic parsers and part of speech taggers are typically trained on mismatched conditions, we describe in this study the impact of such a mismatch for a syntactic parsing task.",
        "id": 14493477
      },
      {
        "title": "Deep learning for language understanding of mental health concepts derived from Cognitive Behavioural Therapy",
        "text": "In recent years, we have seen deep learning and distributed representations of words and sentences make impact on a number of natural language processing tasks, such as similarity, entailment and sentiment analysis. Here we introduce a new task: understanding of mental health concepts derived from Cognitive Behavioural Therapy (CBT). We define a mental health ontology based on the CBT principles, annotate a large corpus where this phenomena is exhibited and perform understanding using deep learning and distributed representations. Our results show that the performance of deep learning models combined with word embeddings or sentence embeddings significantly outperform non-deep-learning models in this difficult task. This understanding module will be an essential component of a statistical dialogue system delivering therapy.",
        "id": 52153975
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "What are some evaluation benchmarks for LLM privacy at inference time, targeted towards model input and NOT the training data.",
    "positive_ctxs": [
      {
        "title": "CAN LLMS KEEP A SECRET? TESTING PRIVACY IMPLICATIONS OF LANGUAGE MODELS VIA CONTEXTUAL INTEGRITY THEORY",
        "text": "The interactive use of large language models (LLMs) in AI assistants (at work, home, etc.) introduces a new set of inference-time privacy risks: LLMs are fed different types of information from multiple sources in their inputs and are expected to reason about what to share in their outputs, for what purpose and with whom, within a given context.In this work, we draw attention to the highly critical yet overlooked notion of contextual privacy by proposing CONFAIDE, 1 a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs.Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively.This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning.Our work underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory of mind.",
        "id": 264555202
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 227231222
      },
      {
        "title": "Dependency-Based Word Embeddings",
        "text": "While continuous word embeddings are gaining popularity, current models are based solely on linear contexts. In this work, we generalize the skip-gram model with negative sampling introduced by Mikolov et al. to include arbitrary contexts. In particular, we perform experiments with dependency-based contexts, and show that they produce markedly different embeddings. The dependencybased embeddings are less topical and exhibit more functional similarity than the original skip-gram embeddings.",
        "id": 2107337
      },
      {
        "title": "Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly",
        "text": "Building on Petroni et al. (2019), we propose two new probing tasks analyzing factual knowledge stored in Pretrained Language Models (PLMs). (1) Negation. We find that PLMs do not distinguish between negated (\"Birds cannot [MASK]\") and non-negated (\"Birds can [MASK]\") cloze questions.(2)Mispriming. Inspired by priming methods in human psychology, we add \"misprimes\" to cloze questions (\"Talk? Birds can [MASK]\"). We find that PLMs are easily distracted by misprimes. These results suggest that PLMs still have a long way to go to adequately learn human-like factual knowledge.",
        "id": 218628691
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Can you suggest some literature that evaluates the ability of context-aware machine translation systems to handle discourse phenomena such as deixis and lexical cohesion?",
    "positive_ctxs": [
      {
        "title": "When a Good Translation is Wrong in Context: Context-Aware Machine Translation Improves on Deixis, Ellipsis, and Lexical Cohesion",
        "text": "Though machine translation errors caused by the lack of context beyond one sentence have long been acknowledged, the development of context-aware NMT systems is hampered by several problems. Firstly, standard metrics are not sensitive to improvements in consistency in document-level translations. Secondly, previous work on context-aware NMT assumed that the sentence-aligned parallel data consisted of complete documents while in most practical scenarios such document-level data constitutes only a fraction of the available parallel data. To address the first issue, we perform a human study on an English-Russian subtitles dataset and identify deixis, ellipsis and lexical cohesion as three main sources of inconsistency. We then create test sets targeting these phenomena. To address the second shortcoming, we consider a set-up in which a much larger amount of sentence-level data is available compared to that aligned at the document level. We introduce a model that is suitable for this scenario and demonstrate major gains over a context-agnostic baseline on our new benchmarks without sacrificing performance as measured with BLEU. 1",
        "id": 155089628
      }
    ],
    "negative_ctxs": [
      {
        "title": "A Transformer-based joint-encoding for Emotion Recognition and Sentiment Analysis",
        "text": "Understanding expressed sentiment and emotions are two crucial factors in human multimodal language. This paper describes a Transformer-based joint-encoding (TBJE) for the task of Emotion Recognition and Sentiment Analysis. In addition to use the Transformer architecture, our approach relies on a modular co-attention and a glimpse layer to jointly encode one or more modalities. The proposed solution has also been submitted to the ACL20: Second Grand-Challenge on Multimodal Language to be evaluated on the CMU-MOSEI dataset. The code to replicate the presented experiments is open-source 1 .",
        "id": 220250648
      },
      {
        "title": "A Grain of Salt for the WMT Manual Evaluation *",
        "text": "The Workshop on Statistical Machine Translation (WMT) has become one of ACL's flagship workshops, held annually since 2006. In addition to soliciting papers from the research community, WMT also features a shared translation task for evaluating MT systems. This shared task is notable for having manual evaluation as its cornerstone. The Workshop's overview paper, playing a descriptive and administrative role, reports the main results of the evaluation without delving deep into analyzing those results. The aim of this paper is to investigate and explain some interesting idiosyncrasies in the reported results, which only become apparent when performing a more thorough analysis of the collected annotations. Our analysis sheds some light on how the reported results should (and should not) be interpreted, and also gives rise to some helpful recommendation for the organizers of WMT. . We are grateful to our students, colleagues, and the three reviewers for various observations and suggestions.",
        "id": 6395516
      },
      {
        "title": "Cryptocurrency Day Trading and Framing Prediction in Microblog Discourse",
        "text": "With 56 million people actively trading and in-001 vesting in cryptocurrency online and globally 002 in 2020, there is an increasing need for auto-003 matic social media analysis tools to help under-004 stand trading discourse and behavior. In this 005 work, we present a dual natural language mod-006 eling pipeline which leverages language and 007 social network behaviors for the prediction of 008 cryptocurrency day trading actions and their 009 associated framing patterns. This pipeline first 010 predicts if tweets can be used to guide day trad-011 ing behavior, specifically if a cryptocurrency 012 investor should buy, sell, or hold their cryp-013 tocurrencies in order to make a profit. Next, 014 tweets are input to an unsupervised deep clus-015 tering approach to automatically detect trad-016 ing framing patterns. Our contributions in-017 clude the modeling pipeline for this novel task, 018 a new Cryptocurrency Tweets Dataset com-019 piled from influential accounts, and a Histor-020 ical Price Dataset. Our experiments show that 021 our approach achieves an 88.78% accuracy for 022 day trading behavior prediction and reveals 023 framing fluctuations prior to and during the 024 COVID-19 pandemic that could be used to 025 guide investment actions. 026 1 Introduction 027 Beginning with the 2008 introduction of Bitcoin 028 (BTC) (Nakamoto, 2008), a cryptocurrency for a 029 Peer-to-Peer cash system, the use of cryptocurren-030 cies and their corresponding blockchains have in-031 creasingly gained in popularity. In 2019, the num-032 ber of Americans owning cryptocurrency doubled 033 from 7% in 2018 to 14%, representing about 35 034 million people trading and investing with cryptocur-035 rency (Partz, 2019). 036 This increase is largely due to the capability 037 of cryptocurrency to improve various applications 038 ranging from increased security of smart contracts 039 to facilitating less expensive, faster cross-border in-040 ternational payments. Another contributing factor 041 to this growth is that digital coins fulfill the prop-042 erty of storing value similar to other fiat currencies, 043 which are government-issued currencies not backed 044 by physical commodities, e.g., the American dollar 045 or euro. Finally, cryptocurrency popularity can be 046 associated with its high day trading volume. As of 047 January 2021, the combined worth of all cryptocur-048 rencies was $1 trillion 1 , with Bitcoin accounting 049 for $650 billion of this amount. To put this in per-050 spective, the average trading volume of Amazon 051 Inc. is $13 billion per day -less than one-fifth of 052 the BTC daily volume of $70 billion. 2 053 Cryptocurrencies were born on the internet, 054 gained their visibility through online and social 055 media coverage, and many investors follow the 056 advice of well-known cryptocurrency experts on 057 Twitter to guide their personal investment strate-058 gies (Mone, 2019). Because cryptocurrency prices 059 can fluctuate quickly, resulting in real-life financial 060 gains or losses, models that can rapidly analyze 061 trending discourse on Twitter can be harnessed to 062 guide and benefit investors. 063 Additionally, work in computational linguistics 064 and the social sciences have shown the benefit of 065 studying framing, which is how someone discusses 066 a topic in order to influence or alter the opinion of 067 the public, for understanding microblog discourse 068 (Card et al., 2015; Johnson et al., 2017; Li and 069 Goldwasser, 2019). Specifically, framing in Twit-070 ter can be used to understand social phenomena, 071 such as political maneuvering or epidemiology cov-072 erage. However, few works exist which study the 073 relationship between framing and cryptocurrency 074 trading, especially during times of economic stress. 075 Currently, it is estimated that the COVID-19 pan-076 demic has negatively impacted the global economy 077 by hindering economic growth worldwide between 078 4.5% and 6.0%, with the potential for future global 079 trade to fall up to 9.2% (CRS, 2020). Similar to 080 1 https://coinmarketcap.com/charts/ 2 https://finance.yahoo.com/quote/AMZN Figure 1: Cryptocurrency Day Trading Prediction and Framing Discovery Pipeline. the pandemic's effect on Wall Street (i.e., the New 081 York Stock Exchange and NASDAQ), the cryp-082 tocurrency market reflected a drastic 47.8% drop 083 on March 12, 2020, one day after the World Health 084 Organization (WHO) announced that COVID-19 085 could be characterized as a pandemic. Around the 086 same time, a similar drop occurred in stocks world-087 wide. Therefore, we hypothesized that microblog 088 discourse about day trading behaviors (i.e., buy, 089 sell, hold) would be a useful predictive feature in 090 understanding cryptocurrency trading and framing. 091 Our contributions include a cryptocurrency day 092 trading behavior modeling pipeline (Figure 1) that 093 leverages language and social network behavior 094 extracted from tweets to implement: (1) a pre-095 dictive model for investment actions, specifically, 096 whether to buy, sell, or hold cryptocurrency based 097 on daily discussions on Twitter, and (2) an unsuper-098 vised deep-learning clustering model to determine 099 the underlying framing patterns used to discuss 100 these investment actions. We have also compiled a 101 Cryptocurrency Tweets Dataset (divided into Pre-102 COVID and during COVID portions) and Bitcoin 103 Historical Price Dataset. 3 Lastly, we propose novel 104 frames for economic and financial analysis. Our 105 models are able to predict trading actions with 106 88.78% accuracy, while also revealing a distinc-107 tion between how day trading behaviors are framed 108 before and during the COVID-19 pandemic.",
        "id": 241583406
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there a paper that connects the basic elements of storytelling with biased or imbalanced media reporting?",
    "positive_ctxs": [
      {
        "title": "Conflicts, Villains, Resolutions: Towards models of Narrative Media Framing",
        "text": "Despite increasing interest in the automatic detection of media frames in NLP, the problem is typically simplified as single-label classification and adopts a topic-like view on frames, evading modelling the broader document-level narrative. In this work, we revisit a widely used conceptualization of framing from the communication sciences which explicitly captures elements of narratives, including conflict and its resolution, and integrate it with the narrative framing of key entities in the story as heroes, victims or villains. We adapt an effective annotation paradigm that breaks a complex annotation task into a series of simpler binary questions, and present an annotated data set of English news articles, and a case study on the framing of climate change in articles from news outlets across the political spectrum. Finally, we explore automatic multi-label prediction of our frames with supervised and semisupervised approaches, and present a novel retrieval-based method which is both effective and transparent in its predictions. We conclude with a discussion of opportunities and challenges for future work on document-level models of narrative framing. 1",
        "id": 259075515
      }
    ],
    "negative_ctxs": [
      {
        "title": "DBpedia: A Multilingual Cross-Domain Knowledge Base",
        "text": "The DBpedia project extracts structured information from Wikipedia editions in 97 different languages and combines this information into a large multi-lingual knowledge base covering many specific domains and general world knowledge. The knowledge base contains textual descriptions (titles and abstracts) of concepts in up to 97 languages. It also contains structured knowledge that has been extracted from the infobox systems of Wikipedias in 15 different languages and is mapped onto a single consistent ontology by a community effort. The knowledge base can be queried using a structured query language and all its data sets are freely available for download. In this paper, we describe the general DBpedia knowledge base and extended data sets that specifically aim at supporting computational linguistics tasks. These task include Entity Linking, Word Sense Disambiguation, Question Answering, Slot Filling and Relationship Extraction. These use cases are outlined, pointing at added value that the structured data of DBpedia provides.",
        "id": 8783027
      },
      {
        "title": "Argument Identification in Chinese Editorials",
        "text": "In this paper, we develop and evaluate several techniques for identifying argumentative paragraphs in Chinese editorials. We first use three methods of evaluation to score a paragraph's argumentative nature: a relative word frequency approach; a method which targets known argumentative words in our corpus; and a combined approach which uses elements from the previous two. Then, we determine the best score thresholds for separating argumentative and non-argumentative paragraphs. The results of our experimentation show that our relative word frequency approach provides a reliable way to identify argumentative paragraphs with a F 1 score of 0.91, though challenges in accurate scoring invite improvement through context-aware means.",
        "id": 7831381
      },
      {
        "title": "Part-of-Speech Annotation of Biology Research Abstracts",
        "text": "A part-of-speech (POS) tagged corpus was built on research abstracts in biomedical domain with the Penn Treebank scheme. As consistent annotation was difficult without domain-specific knowledge we made use of the existing term annotation of the GENIA corpus. A list of frequent terms annotated in the GENIA corpus was compiled and the POS of each constituent of those terms were determined with assistance from domain specialists. The POS of the terms in the list are pre-assigned, then a tagger assigns POS to remaining words preserving the pre-assigned POS, whose results are corrected by human annotators. We also modified the PTB scheme slightly. An inter-annotator agreement tested on new 50 abstracts was 98.5%. A POS tagger trained with the annotated abstracts was tested against a gold-standard set made from the interannotator agreement. The untrained tagger had the accuracy of 83.0%. Trained with 2000 annotated abstracts the accuracy rose to 98.2%. The 2000 annotated abstracts are publicly available.",
        "id": 1236813
      }
    ]
  }
]