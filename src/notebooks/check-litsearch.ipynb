{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c318ab0e-c7b0-4a8a-8bd9-1186f5b400e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m224 packages\u001b[0m \u001b[2min 917ms\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 36ms\u001b[0m\u001b[0m                                               \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m                                  \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrank-bm25\u001b[0m\u001b[2m==0.2.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add tqdm rank_bm25==0.2.2 nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37962e7c-84e4-4a45-adcb-c214c2cd1c52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "query_data = load_dataset(\"princeton-nlp/LitSearch\", \"query\", split=\"full\")\n",
    "corpus_clean_data = load_dataset(\"princeton-nlp/LitSearch\", \"corpus_clean\", split=\"full\")\n",
    "corpus_s2orc_data = load_dataset(\"princeton-nlp/LitSearch\", \"corpus_s2orc\", split=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9663f657-1369-4fe9-a2f4-ead2c403850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Any\n",
    "from datasets import Dataset\n",
    "\n",
    "def get_clean_corpusid(item: dict) -> int:\n",
    "    return item['corpusid']\n",
    "\n",
    "def get_clean_title(item: dict) -> str:\n",
    "    return item['title']\n",
    "\n",
    "def get_clean_abstract(item: dict) -> str:\n",
    "    return item['abstract']\n",
    "\n",
    "def get_clean_title_abstract(item: dict) -> str:\n",
    "    title = get_clean_title(item)\n",
    "    abstract = get_clean_abstract(item)\n",
    "    return f\"Title: {title}\\nAbstract: {abstract}\"\n",
    "\n",
    "def get_clean_full_paper(item: dict) -> str:\n",
    "    return item['full_paper']\n",
    "\n",
    "def get_clean_paragraph_indices(item: dict) -> List[Tuple[int, int]]:\n",
    "    text = get_clean_full_paper(item)\n",
    "    paragraph_indices = []\n",
    "    paragraph_start = 0\n",
    "    paragraph_end = 0\n",
    "    while paragraph_start < len(text):\n",
    "        paragraph_end = text.find(\"\\n\\n\", paragraph_start)\n",
    "        if paragraph_end == -1:\n",
    "            paragraph_end = len(text)\n",
    "        paragraph_indices.append((paragraph_start, paragraph_end))\n",
    "        paragraph_start = paragraph_end + 2\n",
    "    return paragraph_indices\n",
    "\n",
    "def get_clean_text(item: dict, start_idx: int, end_idx: int) -> str:\n",
    "    text = get_clean_full_paper(item)\n",
    "    assert start_idx >= 0 and end_idx >= 0\n",
    "    assert start_idx <= end_idx\n",
    "    assert end_idx <= len(text)\n",
    "    return text[start_idx:end_idx]\n",
    "\n",
    "def get_clean_paragraphs(item: dict, min_words: int = 10) -> List[str]:\n",
    "    paragraph_indices = get_clean_paragraph_indices(item)\n",
    "    paragraphs = [get_clean_text(item, paragraph_start, paragraph_end) for paragraph_start, paragraph_end in paragraph_indices]\n",
    "    paragraphs = [paragraph for paragraph in paragraphs if len(paragraph.split()) >= min_words]\n",
    "    return paragraphs\n",
    "\n",
    "def get_clean_citations(item: dict) -> List[int]:\n",
    "    return item['citations']\n",
    "\n",
    "def get_clean_dict(data: Dataset) -> dict:\n",
    "    return {get_clean_corpusid(item): item for item in data}\n",
    "\n",
    "def create_kv_pairs(data: List[dict], key: str) -> dict:\n",
    "    if key == \"title_abstract\":\n",
    "        kv_pairs = {get_clean_title_abstract(record): get_clean_corpusid(record) for record in data}\n",
    "    elif key == \"full_paper\":\n",
    "        kv_pairs = {get_clean_full_paper(record): get_clean_corpusid(record) for record in data}\n",
    "    elif key == \"paragraphs\":\n",
    "        kv_pairs = {}\n",
    "        for record in data:\n",
    "            corpusid = get_clean_corpusid(record)\n",
    "            paragraphs = get_clean_paragraphs(record)\n",
    "            for paragraph_idx, paragraph in enumerate(paragraphs):\n",
    "                kv_pairs[paragraph] = (corpusid, paragraph_idx)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid key\")\n",
    "    return kv_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce970c6-cad9-4c89-ac83-bfb2bdee9843",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_pairs = create_kv_pairs(corpus_clean_data, \"title_abstract\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11656758-f5d1-4f89-a912-58236c88a37f",
   "metadata": {},
   "source": [
    "BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a39af2d5-d05a-4bb8-b79b-9d9fd8f65028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from enum import Enum\n",
    "from typing import List, Tuple, Any\n",
    "\n",
    "class TextType(Enum):\n",
    "    KEY = 1\n",
    "    QUERY = 2\n",
    "\n",
    "class KVStore:\n",
    "    def __init__(self, index_name: str, index_type: str) -> None:\n",
    "        self.index_name = index_name\n",
    "        self.index_type = index_type\n",
    "\n",
    "        self.keys = []\n",
    "        self.encoded_keys = []\n",
    "        self.values = []\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.keys)\n",
    "\n",
    "    def _encode(self, text: str, type: TextType) -> Any:\n",
    "        return self._encode_batch([text], type, show_progress_bar=False)[0]\n",
    "    \n",
    "    def _encode_batch(self, texts: List[str], type: TextType, show_progress_bar: bool = True) -> List[Any]: \n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _query(self, encoded_query: Any, n: int) -> List[int]:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def clear(self) -> None:\n",
    "        self.keys = []\n",
    "        self.encoded_keys = []\n",
    "        self.values = []\n",
    "\n",
    "    def create_index(self, key_value_pairs: List[Tuple[str, Any]]) -> None:\n",
    "        if len(self.keys) > 0:\n",
    "            raise ValueError(\"Index is not empty. Please create a new index or clear the existing one.\")\n",
    "        \n",
    "        for key, value in tqdm(key_value_pairs.items(), desc=f\"Creating {self.index_name} index\"):\n",
    "            self.keys.append(key)\n",
    "            self.values.append(value)\n",
    "        self.encoded_keys = self._encode_batch(self.keys, TextType.KEY)\n",
    "\n",
    "    def query(self, query_text: str, n: int, return_keys: bool = False) -> List[Any]:\n",
    "        encoded_query = self._encode(query_text, TextType.QUERY)\n",
    "        indices = self._query(encoded_query, n)\n",
    "        if return_keys:\n",
    "            results = [(self.keys[i], self.values[i]) for i in indices]\n",
    "        else:\n",
    "            results = [self.values[i] for i in indices]\n",
    "        return results\n",
    "\n",
    "    def save(self, dir_name: str) -> None:\n",
    "        save_dict = {}\n",
    "        for key, value in self.__dict__.items():\n",
    "            if key[0] != \"_\":\n",
    "                save_dict[key] = value\n",
    "\n",
    "        print(f\"Saving index to {os.path.join(dir_name, f'{self.index_name}.{self.index_type}')}\")\n",
    "        os.makedirs(dir_name, exist_ok=True)\n",
    "        with open(os.path.join(dir_name, f\"{self.index_name}.{self.index_type}\"), 'wb') as file:\n",
    "            pickle.dump(save_dict, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "\n",
    "    def load(self, file_path: str) -> None:\n",
    "        if len(self.keys) > 0:\n",
    "            raise ValueError(\"Index is not empty. Please create a new index or clear the existing one before loading from disk.\")\n",
    "        \n",
    "        print(f\"Loading index from {file_path}...\")\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pickle_data = pickle.load(file)\n",
    "        \n",
    "        for key, value in pickle_data.items():\n",
    "            setattr(self, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0a5ba68-06a7-43a0-8b63-68a6b1b95b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "class BM25(KVStore):\n",
    "    def __init__(self, index_name: str):\n",
    "        super().__init__(index_name, 'bm25')\n",
    "\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('stopwords')\n",
    "        nltk.download('punkt_tab')\n",
    "\n",
    "        self._tokenizer = nltk.word_tokenize\n",
    "        self._stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "        self._stemmer = nltk.stem.PorterStemmer().stem\n",
    "        self.index = None   # BM25 index\n",
    "\n",
    "    def _encode_batch(self, texts: List[str], type: TextType, show_progress_bar: bool = True) -> List[str]:\n",
    "        # lowercase, tokenize, remove stopwords, and stem\n",
    "        tokens_list = []\n",
    "        for text in tqdm(texts, disable=not show_progress_bar):\n",
    "            tokens = self._tokenizer(text.lower())\n",
    "            tokens = [token for token in tokens if token not in self._stop_words]\n",
    "            tokens = [self._stemmer(token) for token in tokens]\n",
    "            tokens_list.append(tokens)\n",
    "        return tokens_list\n",
    "\n",
    "    def _query(self, encoded_query: List[str], n: int) -> List[int]:\n",
    "        top_indices = np.argsort(self.index.get_scores(encoded_query))[::-1][:n].tolist()\n",
    "        return top_indices\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        super().clear()\n",
    "        self.index = None\n",
    "\n",
    "    def create_index(self, key_value_pairs: List[Tuple[str, Any]]) -> None:\n",
    "        super().create_index(key_value_pairs)\n",
    "        self.index = BM25Okapi(self.encoded_keys)\n",
    "\n",
    "    def load(self, dir_name: str) -> None:\n",
    "        super().load(dir_name)\n",
    "        self._tokenizer = nltk.word_tokenize\n",
    "        self._stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "        self._stemmer = nltk.stem.PorterStemmer().stem\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d9b8691-9e4c-4cc6-bc71-289279cfb873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/mac/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "bm25_model = BM25(\"Title_Abstract_BM25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f093588-3c9b-4c0e-b408-cf4d3d8eac73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Title_Abstract_BM25 index: 100%|████████████████████████████████████████████████████████████████████████████| 57657/57657 [00:00<00:00, 1581897.42it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 57657/57657 [00:50<00:00, 1144.08it/s]\n"
     ]
    }
   ],
   "source": [
    "bm25_model.create_index(kv_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee096855-6b9b-47ac-b159-091cc9ab1471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./final_generating_query_data.json', \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "573ee2d3-5051-4d9e-9ca2-5d15a61c428a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1583/1583 [05:07<00:00,  5.15it/s]\n"
     ]
    }
   ],
   "source": [
    "query_set = [element for element in data]\n",
    "for query in tqdm(query_set):\n",
    "    query_text = query[\"query\"]\n",
    "    top_k = bm25_model.query(query_text, 20)\n",
    "    query[\"retrieved\"] = top_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09df389b-29cd-422c-ad24-36c718f1dd67",
   "metadata": {},
   "source": [
    "Calculating recall performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86c37d61-049b-4e9d-8b91-ab490593138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(corpusids: list, retrieved: list, k: int):\n",
    "    top_k = retrieved[:k]\n",
    "    intersection = set(corpusids) & set(top_k)\n",
    "    return len(intersection) / len(corpusids) if corpusids else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d45ddda-4dc3-4a5c-9607-3a15010f1725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4491471888818699"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "query_set_df = pd.DataFrame(query_set)\n",
    "all_recall_at20 = []\n",
    "\n",
    "for _, query in query_set_df.iterrows():\n",
    "    r20 = calculate_recall([query['citation_corpus_id']], query['retrieved'], 20)\n",
    "    all_recall_at20.append(r20)\n",
    "\n",
    "mean_recall_at20 = np.mean(all_recall_at20)\n",
    "\n",
    "mean_recall_at20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a555f7-25c7-4b25-bfa3-61a23d79e765",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
