[
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a contemporary research paper that has advanced natural language watermarking quality through algorithmic methods?",
    "positive_ctxs": [
      {
        "title": "Robust Multi-bit Natural Language Watermarking through Invariant Features",
        "text": "Recent years have witnessed a proliferation of valuable original natural language contents found in subscription-based media outlets, web novel platforms, and outputs of large language models. However, these contents are susceptible to illegal piracy and potential misuse without proper security measures. This calls for a secure watermarking system to guarantee copyright protection through leakage tracing or ownership identification. To effectively combat piracy and protect copyrights, a multi-bit watermarking framework should be able to embed adequate bits of information and extract the watermarks in a robust manner despite possible corruption. In this work, we explore ways to advance both payload and robustness by following a well-known proposition from image watermarking and identify features in natural language that are invariant to minor corruption. Through a systematic analysis of the possible sources of errors, we further propose a corruption-resistant infill model. Our full method improves upon the previous work on robustness by +16.8% point on average on four datasets, three corruption types, and two corruption ratios. 1",
        "id": 259129912
      }
    ],
    "negative_ctxs": [
      {
        "title": "Relation Inference in Lexical Networks ... with Refinements",
        "text": "Improving lexical network's quality is an important issue in the creation process of these language resources. This can be done by automatically inferring new relations from already existing ones with the purpose of (1) densifying the relations to cover the eventual lack of information and (2) detecting errors. In this paper, we devise such an approach applied to the JeuxDeMots lexical network, which is a freely available lexical and semantic resource for French. We first present the principles behind the lexical network construction with crowdsourcing and games with a purpose and illustrated them with JeuxDeMots (JDM). Then, we present the outline of an elicitation engine based on an inference engine using schemes like deduction, induction and abduction which will be referenced and briefly presented and we will especially highlight the new scheme (Relation Inference Scheme with Refinements) added to our system. An experiment showing the relevance of this scheme is then presented.",
        "id": 1699492
      },
      {
        "title": "An Improved Crowdsourcing Based Evaluation Technique for Word Embedding Methods",
        "text": "In this proposal track paper, we have presented a crowdsourcing-based word embedding evaluation technique that will be more reliable and linguistically justified. The method is designed for intrinsic evaluation and extends the approach proposed in(Schnabel et al., 2015). Our improved evaluation technique captures word relatedness based on the word context.",
        "id": 515075
      },
      {
        "title": "Neural Generation Meets Real People: Building a Social, Informative Open-Domain Dialogue Agent",
        "text": "We present Chirpy Cardinal, an open-domain social chatbot. Aiming to be both informative and conversational, our bot chats with users in an authentic, emotionally intelligent way. By integrating controlled neural generation with scaffolded, hand-written dialogue, we let both the user and bot take turns driving the conversation, producing an engaging and socially fluent experience. Deployed in the fourth iteration of the Alexa Prize Socialbot Grand Challenge, Chirpy Cardinal handled thousands of conversations per day, placing second out of nine bots with an average user rating of 3.58/5.",
        "id": 251040192
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a study that investigates the use of trainable prompts to enhance the parameter optimization process in machine learning models?",
    "positive_ctxs": [
      {
        "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
        "text": "In this work, we explore \"prompt tuning,\" a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient \"prompt ensembling.\" We release code and model checkpoints to reproduce our experiments. 1ReferencesRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,  Danilo Giampiccolo, Bernardo Magnini, and Idan  Szpektor. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognising textual entailment, volume 6, pages 6-4. Venice. . 2019a. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
        "id": 233296808
      }
    ],
    "negative_ctxs": [
      {
        "title": "Detecting Interesting Event Sequences for Sports Reporting",
        "text": "Hand-crafted approaches to content determination are expensive to port to new domains. Machine-learned approaches, on the other hand, tend to be limited to relatively simple selection of items from data sets. We observe that in time series domains, textual descriptions often aggregate a series of events into a compact description. We present a simple technique for automatically determining sequences of events that are worth reporting, and evaluate its effectiveness.",
        "id": 528679
      },
      {
        "title": "Employing Phonetic Speech Recognition for Language and Dialect Specific Search",
        "text": "We discuss the notion of language and dialect-specific search in the context of audio indexing. A system is described where users can find dialect or language-specific pronunciations of Afghan placenames in Dari and Pashto. We explore the efficacy of a phonetic speech recognition system employed in this task.This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings are footer added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/",
        "id": 14249050
      },
      {
        "title": "COMPACT PART-BASED IMAGE REPRESENTATIONS",
        "text": "Learning compact, interpretable image representations is a very natural task which has not been solved satisfactorily even for simple classes of binary images. In this paper, we review various ways of composing parts (or experts) for binary data and argue that competitive forms of interaction are best suited to learn lowdimensional representations. We propose a new rule which discourages parts from learning similar structures and which penalizes opposing expert opinions strongly so that abstaining from voting becomes more attractive. Using a process of oversimplification and correction we show in experiments that very intuitive models can be obtained.",
        "id": 16053260
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "manual_acl",
    "question": "Is there a paper that links exposure bias to distillation?",
    "positive_ctxs": [
      {
        "title": "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training",
        "text": "Modern Natural Language Generation (NLG) models come with massive computational and storage requirements. In this work, we study the potential of compressing them, which is crucial for real-world applications serving millions of users. We focus on Knowledge Distillation (KD) techniques, in which a small student model learns to imitate a large teacher model, allowing to transfer knowledge from the teacher to the student. In contrast to much of the previous work, our goal is to optimize the model for a specific NLG task and a specific dataset. Typically in real-world applications, in addition to labeled data there is abundant unlabeled task-specific data, which is crucial for attaining high compression rates via KD. In this work, we conduct a systematic study of task-specific KD techniques for various NLG tasks under realistic assumptions. We discuss the special characteristics of NLG distillation and particularly the exposure bias problem. Following, we derive a family of Pseudo-Target (PT) augmentation methods, substantially extending prior work on sequence-level KD. We propose the Joint-Teaching method, which applies wordlevel KD to multiple PTs generated by both the teacher and the student. Finally, we validate our findings in an extreme setup with no labeled examples using GPT-4 as the teacher.Our study provides practical model design observations and demonstrates the effectiveness of PT training for task-specific KD in NLG.",
        "id": 258461336
      }
    ],
    "negative_ctxs": [
      {
        "title": "General and Task-Specific Corpus Resources for Polish Adult Learners of English",
        "text": "This paper offers a comparison of two resources for Polish adult learners of English. The first has been designed for Polish-English Literacy Tutor (PELT), a multimodal system for foreign language learning, as training input to speech recognition system for highly accented, strongly variable second language speech. The second corpus is a task-specific resource designed in the PELT framework to investigate the vowel space of English produced by Poles. Presented are linguistically and technologically challenging aspects of the two ventures and their complementary character.",
        "id": 6864948
      },
      {
        "title": "Factored Neural Language Models",
        "text": "We present a new type of neural probabilistic language model that learns a mapping from both words and explicit word features into a continuous space that is then used for word prediction. Additionally, we investigate several ways of deriving continuous word representations for unknown words from those of known words. The resulting model significantly reduces perplexity on sparse-data tasks when compared to standard backoff models, standard neural language models, and factored language models.",
        "id": 937826
      },
      {
        "title": "",
        "text": "",
        "id": 232021865
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "What techniques have been investigated to enhance multimodal sentiment analysis by fusing different modalities?",
    "positive_ctxs": [
      {
        "title": "Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis",
        "text": "In multimodal sentiment analysis (MSA), the performance of a model highly depends on the quality of synthesized embeddings. These embeddings are generated from the upstream process called multimodal fusion, which aims to extract and combine the input unimodal raw data to produce a richer multimodal representation. Previous work either back-propagates the task loss or manipulates the geometric property of feature spaces to produce favorable fusion results, which neglects the preservation of critical task-related information that flows from input to the fusion results. In this work, we propose a framework named MultiModal InfoMax (MMIM), which hierarchically maximizes the Mutual Information (MI) in unimodal input pairs (inter-modality) and between multimodal fusion result and unimodal input in order to maintain taskrelated information through multimodal fusion. The framework is jointly trained with the main task (MSA) to improve the performance of the downstream MSA task. To address the intractable issue of MI bounds, we further formulate a set of computationally simple parametric and non-parametric methods to approximate their truth value. Experimental results on the two widely used datasets demonstrate the efficacy of our approach. The implementation of this work is publicly available at https://github.com/ declare-lab/Multimodal-Infomax.",
        "id": 237372185
      },
      {
        "title": "Multimodal Sentiment Detection Based on Multi-channel Graph Neural Networks",
        "text": "With the popularity of smartphones, we have witnessed the rapid proliferation of multimodal posts on various social media platforms. We observe that the multimodal sentiment expression has specific global characteristics, such as the interdependencies of objects or scenes within the image. However, most previous studies only considered the representation of a single image-text post and failed to capture the global co-occurrence characteristics of the dataset. In this paper, we propose Multi-channel Graph Neural Networks with Sentiment-awareness (MGNNS) for imagetext sentiment detection. Specifically, we first encode different modalities to capture hidden representations. Then, we introduce multichannel graph neural networks to learn multimodal representations based on the global characteristics of the dataset. Finally, we implement multimodal in-depth fusion with the multi-head attention mechanism to predict the sentiment of image-text pairs. Extensive experiments conducted on three publicly available datasets demonstrate the effectiveness of our approach for multimodal sentiment detection.",
        "id": 236460184
      }
    ],
    "negative_ctxs": [
      {
        "title": "Unified Speech-Text Pre-training for Speech Translation and Recognition",
        "text": "We describe a method to jointly pre-train speech and text in an encoder-decoder modeling framework for speech translation and recognition. The proposed method incorporates four self-supervised and supervised subtasks for cross modality learning.A self-supervised speech subtask leverages unlabelled speech data, and a (self-)supervised text to text subtask makes use of abundant text training data. Two auxiliary supervised speech tasks are included to unify speech and text modeling space. Our contribution lies in integrating linguistic information from the text corpus into the speech pre-training. Detailed analysis reveals learning interference among subtasks. Two pre-training configurations for speech translation and recognition, respectively, are presented to alleviate subtask interference. Our experiments show the proposed method can effectively fuse speech and text information into one model. It achieves between 1.7 and 2.3 BLEU improvement above the state of the art on the MUST-C speech translation dataset and comparable WERs to wav2vec 2.0 on the LIBRISPEECH speech recognition task. 1",
        "id": 248119033
      },
      {
        "title": "Co-guiding Net: Achieving Mutual Guidances between Multiple Intent Detection and Slot Filling via Heterogeneous Semantics-Label Graphs",
        "text": "Recent graph-based models for joint multiple intent detection and slot filling have obtained promising results through modeling the guidance from the prediction of intents to the decoding of slot filling. However, existing methods (1) only model the unidirectional guidance from intent to slot; (2) adopt homogeneous graphs to model the interactions between the slot semantics nodes and intent label nodes, which limit the performance. In this paper, we propose a novel model termed Co-guiding Net, which implements a two-stage framework achieving the mutual guidances between the two tasks. In the first stage, the initial estimated labels of both tasks are produced, and then they are leveraged in the second stage to model the mutual guidances. Specifically, we propose two heterogeneous graph attention networks working on the proposed two heterogeneous semantics-label graphs, which effectively represent the relations among the semantics nodes and label nodes. Experiment results show that our model outperforms existing models by a large margin, obtaining a relative improvement of 19.3% over the previous best model on Mix-ATIS dataset in overall accuracy.",
        "id": 252992662
      },
      {
        "title": "BERT, MBERT, or BIBERT? A Study on Contextualized Embeddings for Neural Machine Translation",
        "text": "The success of bidirectional encoders using masked language models, such as BERT, on numerous natural language processing tasks has prompted researchers to attempt to incorporate these pre-trained models into neural machine translation (NMT) systems. However, proposed methods for incorporating pretrained models are non-trivial and mainly focus on BERT, which lacks a comparison of the impact that other pre-trained models may have on translation performance. In this paper, we demonstrate that simply using the output (contextualized embeddings) of a tailored and suitable bilingual pre-trained language model (dubbed BIBERT) as the input of the NMT encoder achieves state-of-the-art translation performance. Moreover, we also propose a stochastic layer selection approach and a concept of dual-directional translation model to ensure the sufficient utilization of contextualized embeddings. In the case of without using back translation, our best models achieve BLEU scores of 30.45 for En→De and 38.61 for De→En on the IWSLT'14 dataset, and 31.26 for En→De and 34.94 for De→En on the WMT'14 dataset, which exceeds all published numbers 12 .",
        "id": 237485586
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "What are some studies that explore data-poisoning strategies that only require very few poisoned training examples?",
    "positive_ctxs": [
      {
        "title": "Concealed Data Poisoning Attacks on NLP Models",
        "text": "Adversarial attacks alter NLP model predictions by perturbing test-time inputs. However, it is much less understood whether, and how, predictions can be manipulated with small, concealed changes to the training data. In this work, we develop a new data poisoning attack that allows an adversary to control model predictions whenever a desired trigger phrase is present in the input. For instance, we insert 50 poison examples into a sentiment model's training set that causes the model to frequently predict Positive whenever the input contains \"James Bond\". Crucially, we craft these poison examples using a gradient-based procedure so that they do not mention the trigger phrase. We also apply our poison attack to language modeling (\"Apple iPhone\" triggers negative generations) and machine translation (\"iced coffee\" mistranslated as \"hot coffee\"). We conclude by proposing three defenses that can mitigate our attack at some cost in prediction accuracy or extra human annotation.",
        "id": 233230124
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 226283895
      },
      {
        "title": "Writing in a Second Language with Machine Translation (WiLMa)",
        "text": "The WiLMa project aims to assess the effects of using machine translation (MT) tools on the writing processes of second language (L2) learners of varying proficiency. Particular attention is given to individual variation in learners' tool use.",
        "id": 249204474
      },
      {
        "title": "Evaluating Transferability of BERT Models on Uralic Languages",
        "text": "Transformer-based language models such as BERT have outperformed previous models on a large number of English benchmarks, but their evaluation is often limited to English or a small number of well-resourced languages. In this work, we evaluate monolingual, multilingual, and randomly initialized language models from the BERT family on a variety of Uralic languages including Estonian, Finnish, Hungarian, Erzya, Moksha, Karelian, Livvi, Komi Permyak, Komi Zyrian, Northern Sámi, and Skolt Sámi. When monolingual models are available (currently only et, fi, hu), these perform better on their native language, but in general they transfer worse than multilingual models or models of genetically unrelated languages that share the same character set. Remarkably, straightforward transfer of high-resource models, even without special efforts toward hyperparameter optimization, yields what appear to be state of the art POS and NER tools for the minority Uralic languages where there is sufficient data for finetuning.",
        "id": 237502635
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Have any new metrics been developed to assess the factual alignment of machine-generated summaries with their original source texts?",
    "positive_ctxs": [
      {
        "title": "QuestEval: Summarization Asks for Fact-based Evaluation",
        "text": "Summarization evaluation remains an open research problem: current metrics such as ROUGE are known to be limited and to correlate poorly with human judgments. To alleviate this issue, recent work has proposed evaluation metrics which rely on question answering models to assess whether a summary contains all the relevant information in its source document. Though promising, the proposed approaches have so far failed to correlate better than ROUGE with human judgments.In this paper, we extend previous approaches and propose a unified framework, named QuestEval.In contrast to established metrics such as ROUGE or BERTScore, QuestEval does not require any groundtruth reference. Nonetheless, QuestEval substantially improves the correlation with human judgments over four evaluation dimensions (consistency, coherence, fluency, and relevance), as shown in the extensive experiments we report.",
        "id": 233219059
      },
      {
        "title": "SUMMAC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization",
        "text": "In the summarization domain, a key requirement for summaries is to be factually consistent with the input document. Previous work has found that natural language inference (NLI) models do not perform competitively when applied to inconsistency detection. In this work, we revisit the use of NLI for inconsistency detection, finding that past work suffered from a mismatch in input granularity between NLI datasets (sentence-level), and inconsistency detection (document level). We provide a highly effective and light-weight method called SUMMAC CONV that enables NLI models to be successfully used for this task by segmenting documents into sentence units and aggregating scores between pairs of sentences. We furthermore introduce a new benchmark called SUMMAC (Summary Consistency) which consists of six large inconsistency detection datasets. On this dataset, SUMMAC Conv obtains state-of-the-art results with a balanced accuracy of 74.4%, a 5% improvement compared with prior work. *",
        "id": 244345901
      }
    ],
    "negative_ctxs": [
      {
        "title": "CityU-DAC: Disambiguating Sentiment-Ambiguous Adjectives within Context",
        "text": "This paper describes our system participating in task 18 of SemEval-2010, i.e. disambiguating Sentiment-Ambiguous Adjectives (SAAs). To disambiguating SAAs, we compare the machine learning-based and lexiconbased methods in our submissions: 1) Maximum entropy is used to train classifiers based on the annotated Chinese data from the NTCIR opinion analysis tasks, and the clause-level and sentence-level classifiers are compared; 2) For the lexicon-based method, we first classify the adjectives into two classes: intensifiers (i.e. adjectives intensifying the intensity of context) and suppressors (i.e. adjectives decreasing the intensity of context), and then use the polarity of context to get the SAAs' contextual polarity based on a sentiment lexicon. The results show that the performance of maximum entropy is not quite high due to little training data; on the other hand, the lexicon-based method could improve the precision by considering the polarity of context.",
        "id": 7860647
      },
      {
        "title": "Selecting Text Features for Gene Name Classification: from Documents to Terms",
        "text": "In this paper we discuss the performance of a text-based classification approach by comparing different types of features. We consider the automatic classification of gene names from the molecular biology literature, by using a support-vector machine method. Classification features range from words, lemmas and stems, to automatically extracted terms. Also, simple co-occurrences of genes within documents are considered. The preliminary experiments performed on a set of 3,000 S. cerevisiae gene names and 53,000 Medline abstracts have shown that using domain-specific terms can improve the performance compared to the standard bag-of-words approach, in particular for genes classified with higher confidence, and for under-represented classes.",
        "id": 2613800
      },
      {
        "title": "",
        "text": "",
        "id": 218973740
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Is there a comprehensive dataset available for summarizing broad-spectrum conversational dialogues?",
    "positive_ctxs": [
      {
        "title": "DIALOGSUM: A Real-Life Scenario Dialogue Summarization Dataset",
        "text": "Proposal of large-scale datasets has facilitated research on deep neural models for news summarization. Deep learning can also be potentially useful for spoken dialogue summarization, which can benefit a range of reallife scenarios including customer service management and medication tracking. To this end, we propose DIALOGSUM, a large-scale labeled dialogue summarization dataset. We conduct empirical analysis on DIALOGSUM using state-of-the-art neural summarizers. Experimental results show unique challenges in dialogue summarization, such as spoken terms, special discourse structures, coreferences and ellipsis, pragmatics and social common sense, which require specific representation learning technologies to better deal with.",
        "id": 234681504
      }
    ],
    "negative_ctxs": [
      {
        "title": "The Design and Construction of the Corpus of China English",
        "text": "The paper describes the design and construction of the Corpus of China English (CCE). With the emergence of China English as a developing variety in the family of the world Englishes, more and more research has been done to explore its use in China. In order to provide a reliable resource for researchers in the field of China English, CCE was built with due consideration given to its representativeness and authenticity. The general principles for the corpus were authentic, representative and manageable in size. It was composed of more than 13,962,102 tokens in 15,333 texts evenly divided between the following four genres: newspapers, magazines, fiction and academic writings. The texts cover a wide range of domains, such as news, financial, politics, environment, social, culture, technology, sports, education, philosophy, literary, etc. It will be a helpful resource for research on China English, computational linguistics, natural language processing and corpus linguistics. Moreover, it is also a valuable resource for English language teaching in the context of English as a lingua franca (ELF) in which the theories and practice of English language teaching shift from teaching the \"Standard\" English to English as a lingua franca. With the corpus evidence, English teaching textbooks and materials can be developed with due consideration given to China English.",
        "id": 219300379
      },
      {
        "title": "SYNTACTIC PRIVILEGE",
        "text": "",
        "id": 61930487
      },
      {
        "title": "Focused Evaluation for Image Description with Binary Forced-Choice Tasks",
        "text": "Current evaluation metrics for image description may be too coarse. We therefore propose a series of binary forced-choice tasks that each focus on a different aspect of the captions. We evaluate a number of different off-the-shelf image description systems. Our results indicate strengths and shortcomings of both generation and ranking based approaches.",
        "id": 8676220
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "manual_acl",
    "question": "Which papers develop methods to make in-context learning more computationally efficient?",
    "positive_ctxs": [
      {
        "title": "FiD-ICL: A Fusion-in-Decoder Approach for Efficient In-Context Learning",
        "text": "Large pre-trained models are capable of fewshot in-context learning (ICL), i.e., performing a new task by prepending a few demonstrations before the test input. However, the concatenated demonstrations are often excessively long and induce additional computation. Inspired by fusion-in-decoder (FiD) models which efficiently aggregate more passages and thus outperforms concatenation-based models in opendomain QA, we hypothesize that similar techniques can be applied to improve the efficiency and end-task performance of ICL. To verify this, we present a comprehensive study on applying three fusion methods-concatenationbased (early fusion), FiD (intermediate), and ensemble-based (late)-to ICL. We adopt a meta-learning setup where a model is first trained to perform ICL on a mixture of tasks using one selected fusion method, then evaluated on held-out tasks for ICL. Results on 11 heldout tasks show that FiD-ICL matches or outperforms the other two fusion methods. Additionally, we show that FiD-ICL (1) is 10x faster at inference time compared to concat-based and ensemble-based ICL, as we can easily precompute the representations of in-context examples and reuse them; (2) enables scaling up to meta-training 3B-sized models, which would fail for concat-based ICL.",
        "id": 259370780
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 219300739
      },
      {
        "title": "KPTimes: A Large-Scale Dataset for Keyphrase Generation on News Documents",
        "text": "Keyphrase generation is the task of predicting a set of lexical units that conveys the main content of a source text. Existing datasets for keyphrase generation are only readily available for the scholarly domain and include non-expert annotations. In this paper we present KPTimes, a large-scale dataset of news texts paired with editor-curated keyphrases. Exploring the dataset, we show how editors tag documents, and how their annotations differ from those found in existing datasets. We also train and evaluate state-of-the-art neural keyphrase generation models on KPTimes to gain insights on how well they perform on the news domain. The dataset is available online at https://github.com/ygorg/KPTimes.",
        "id": 208512795
      },
      {
        "title": "Mining Bilingual Word Pairs from Comparable Corpus using Apache Spark Framework",
        "text": "Bilingual dictionaries are essential resources in many areas of natural language processing tasks, but resource-scarce and less popular language pairs rarely have such. Efficient automatic methods for inducting bilingual dictionaries are needed as manual resources and efforts are scarce for low-resourced languages. In this paper, we induce word translations using bilingual embedding. We use the Apache Spark ® framework for parallel computation. Further, to validate the quality of the generated bilingual dictionary, we use it in a phrase-table aided Neural Machine Translation (NMT) system. The system can perform moderately well with a manual bilingual dictionary; we change this into our inducted dictionary. The corresponding translated outputs are compared using the Bilingual Evaluation Understudy (BLEU) and Rank-based Intuitive Bilingual Evaluation Score (RIBES) metrics.",
        "id": 244464078
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "What techniques exist for efficiently fine-tuning transformer language models by adjusting a limited set of parameters?",
    "positive_ctxs": [
      {
        "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
        "text": "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were \"virtual tokens\". We apply prefix-tuning to GPT-2 for table-totext generation and to BART for summarization. We show that by modifying only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. 2020. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. Amodei. 2020. Language models are few-shot learners.",
        "id": 230433941
      }
    ],
    "negative_ctxs": [
      {
        "title": "From Baby Steps to Leapfrog: How \"Less is More\" in Unsupervised Dependency Parsing *",
        "text": "We present three approaches for unsupervised grammar induction that are sensitive to data complexity and apply them to Klein and Manning's Dependency Model with Valence. The first, Baby Steps, bootstraps itself via iterated learning of increasingly longer sentences and requires no initialization. This method substantially exceeds Klein and Manning's published scores and achieves 39.4% accuracy on Section 23 (all sentences) of the Wall Street Journal corpus. The second, Less is More, uses a low-complexity subset of the available data: sentences up to length 15. Focusing on fewer but simpler examples trades off quantity against ambiguity; it attains 44.1% accuracy, using the standard linguisticallyinformed prior and batch training, beating state-of-the-art. Leapfrog, our third heuristic, combines Less is More with Baby Steps by mixing their models of shorter sentences, then rapidly ramping up exposure to the full training set, driving up accuracy to 45.0%. These trends generalize to the Brown corpus; awareness of data complexity may improve other parsing models and unsupervised algorithms.",
        "id": 1363892
      },
      {
        "title": "Analyse discursive et informations de factivité",
        "text": "Les annotations discursives proposées dans le cadre de théories discursives comme RST (Rhetorical Structure Theory) ou SDRT (Segmented Dicourse Representation Theory) ont comme point fort de construire une structure discursive globale liant toutes les informations données dans un texte. Les annotations discursives proposées dans le PDTB (Penn Discourse Tree Bank) ont comme point fort d'identifier la \"source\" de chaque information du texte -répondant ainsi à la question qui a dit ou pense quoi ? Nous proposons une approche unifiée pour les annotations discursives alliant les points forts de ces deux courants de recherche. Cette approche unifiée repose crucialement sur des information de factivité, telles que celles qui sont annotées dans le corpus (anglais) FactBank.Abstract. Discursive annotations proposed in theories of discourse such as RST (Rhetorical StructureTheory) or SDRT (Segmented Representation Theory Dicourse) have the advatange of building a global discourse structure linking all the information in a text. Discursive annotations proposed in PDTB (Penn Discourse Tree Bank) have the advatange of identifying the \"source\" of each information -thereby answering to questions such as who says or thinks what ? We propose a unified approach for discursive annotations combining the strengths of these two streams of research. This unified approach relies crucially on factivity information, as encoded in the English corpus FactBank.",
        "id": 60391364
      },
      {
        "title": "-BY-NC 4.0 Improving Signer Independent Sign Language Recognition for Low Resource Languages",
        "text": "The reliance of deep learning algorithms on large scale datasets represents a significant challenge when learning from low resource sign language datasets. This challenge is compounded when we consider that, for a model to be effective in the real world, it must not only learn the variations of a given sign, but also learn to be invariant to the person signing. In this paper, we first illustrate the performance gap between signer-independent and signer-dependent models on Irish Sign Language manual hand shape data. We then evaluate the effect of transfer learning, with different levels of fine-tuning, on the generalisation of signer independent models, and show the effects of different input representations, namely variations in image data and pose estimation. We go on to investigate the sensitivity of current pose estimation models in order to establish their limitations and areas in need of improvement. The results show that accurate pose estimation outperforms raw RGB image data, even when relying on pre-trained image models. Following on from this, we investigate image texture as a potential contributing factor to the gap in performance between signer-dependent and signer-independent models using counterfactual testing images and discuss potential ramifications for low-resource sign languages.",
        "id": 252624556
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that examines how well prompt tuning using soft embeddings works for utilizing pretrained language models in downstream applications with minimal finetuning?",
    "positive_ctxs": [
      {
        "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
        "text": "In this work, we explore \"prompt tuning,\" a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient \"prompt ensembling.\" We release code and model checkpoints to reproduce our experiments. 1ReferencesRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,  Danilo Giampiccolo, Bernardo Magnini, and Idan  Szpektor. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognising textual entailment, volume 6, pages 6-4. Venice. . 2019a. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
        "id": 233296808
      }
    ],
    "negative_ctxs": [
      {
        "title": "Paraphrase Acquisition from Image Captions",
        "text": "We propose to use image captions from the Web as a previously underutilized resource for paraphrases (i.e., texts with the same \"message\") and to create and analyze a corresponding dataset. When an image is reused on the Web, an original caption is often assigned. We hypothesize that different captions for the same image naturally form a set of mutual paraphrases. To demonstrate the suitability of this idea, we analyze captions in the English Wikipedia, where editors frequently relabel the same image for different articles. The paper introduces the underlying mining technology, the resulting Wikipedia-IPC dataset, and compares known paraphrase corpora with respect to their syntactic and semantic paraphrase similarity to our new resource. In this context, we introduce characteristic maps along the two similarity dimensions to identify the style of paraphrases coming from different sources. An annotation study demonstrates the high reliability of the algorithmically determined characteristic maps.",
        "id": 256274522
      },
      {
        "title": "PurePos 2.0: a hybrid tool for morphological disambiguation",
        "text": "We present PurePos, an open-source HMM-based automatic morphological annotation tool. PurePos can perform tagging and lemmatization at the same time, it is very fast to train, with the possibility of easy integration of symbolic rulebased components into the annotation process that can be used to boost the accuracy of the tool. The hybrid approach implemented in PurePos is especially beneficial in the case of rich morphology, highly detailed annotation schemes and if a small amount of training data is available. Evaluation of the tool was on a Hungarian corpus revealed that its hybrid components significantly improve overall annotation accuracy.",
        "id": 15597981
      },
      {
        "title": "",
        "text": "",
        "id": 207988685
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_acl",
    "question": "What research could I reference to understand the methodology for binarizing non-binary subtrees in the context of discourse parsing?",
    "positive_ctxs": [
      {
        "title": "A Classifier-Based Parser with Linear Run-Time Complexity",
        "text": "We present a classifier-based parser that produces constituent trees in linear time. The parser uses a basic bottom-up shiftreduce algorithm, but employs a classifier to determine parser actions instead of a grammar. This can be seen as an extension of the deterministic dependency parser ofNivre and Scholz (2004)to full constituent parsing. We show that, with an appropriate feature set used in classification, a very simple one-path greedy parser can perform at the same level of accuracy as more complex parsers. We evaluate our parser on section 23 of the WSJ section of the Penn Treebank, and obtain precision and recall of 87.54% and 87.61%, respectively.",
        "id": 173611
      }
    ],
    "negative_ctxs": [
      {
        "title": "Unsupervised Information Extraction: Regularizing Discriminative Approaches with Relation Distribution Losses",
        "text": "Unsupervised relation extraction aims at extracting relations between entities in text. Previous unsupervised approaches are either generative or discriminative. In a supervised setting, discriminative approaches, such as deep neural network classifiers, have demonstrated substantial improvement. However, these models are hard to train without supervision. To overcome this limitation, we introduce two losses on the predicted relations distribution. These losses improve the performance of discriminative based models, and enable us to train deep neural networks satisfactorily, surpassing current state of the art on three different datasets.",
        "id": 196179979
      },
      {
        "title": "Automatic Detection of Well Recognized Words in Automatic Speech Transcriptions",
        "text": "This work adresses the use of confidence measures for extracting well recognized words with very low error rate from automatically transcribed segments in a unsupervised way. We present and compare several confidence measures and propose a method to merge them into a new one. We study its capabilities on extracting correct recognized word-segments compared to the amount of rejected words. We apply this fusion measure to select audio segments composed of words with a high confidence score. These segments come from an automatic transcription of french broadcast news given by our speech recognition system based on the CMU Sphinx3.3 decoder. Injecting new data resulting from unsupervised treatments of raw audio recordings in the training corpus of acoustic models gives statistically significant improvement (95% confident interval) in terms of word error rate. Experiments have been carried out on the corpus used during ESTER, the french evaluation campaign.",
        "id": 18656920
      },
      {
        "title": "",
        "text": "",
        "id": 199022739
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Can you give me a paper that does self-supervised contrastive learning of sentence embeddings by sampling in-batch negatives?",
    "positive_ctxs": [
      {
        "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings",
        "text": "This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using \"entailment\" pairs as positives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2% and 2.2% improvement compared to previous best results. We also show-both theoretically and empirically-that contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available. 1",
        "id": 233296292
      }
    ],
    "negative_ctxs": [
      {
        "title": "Spanish HPSG Treebank based on the AnCora Corpus",
        "text": "This paper describes a corpus of HPSG annotated trees for Spanish that contains morphosyntactic information, annotations for semantic roles, clitic pronouns and relative clauses. The corpus is based on the Spanish AnCora corpus, which contains trees for 17,000 sentences comprising half a million words, and it has CFG style annotations. The corpus is stored in two different formats: An XML dialect that is the direct serialization of the typed feature structure trees, and an HTML format that is suitable for visualizing the trees in a browser.",
        "id": 21691538
      },
      {
        "title": "",
        "text": "",
        "id": 207905114
      },
      {
        "title": "h-DETACH: MODIFYING THE LSTM GRADIENT TO- WARDS BETTER OPTIMIZATION",
        "text": "Recurrent neural networks are known for their notorious exploding and vanishing gradient problem (EVGP). This problem becomes more evident in tasks where the information needed to correctly solve them exist over long time scales, because EVGP prevents important gradient components from being back-propagated adequately over a large number of steps. We introduce a simple stochastic algorithm (h-detach) that is specific to LSTM optimization and targeted towards addressing this problem. Specifically, we show that when the LSTM weights are large, the gradient components through the linear path (cell state) in the LSTM computational graph get suppressed. Based on the hypothesis that these components carry information about long term dependencies (which we show empirically), their suppression can prevent LSTMs from capturing them. Our algorithm prevents gradients flowing through this path from getting suppressed, thus allowing the LSTM to capture such dependencies better. We show significant convergence and generalization improvements using our algorithm on various benchmark datasets.",
        "id": 52937611
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a study that investigates employing prefix vectors for conditional natural language generation?",
    "positive_ctxs": [
      {
        "title": "Controllable Natural Language Generation with Contrastive Prefixes",
        "text": "To guide the generation of large pretrained language models (LM), previous work has focused on directly fine-tuning the language model or utilizing an attribute discriminator. In this work, we propose a novel lightweight framework for controllable GPT2 (Radford et al., 2019) generation, which utilizes a set of small attribute-specific vectors, called prefixes (Li and Liang, 2021), to steer natural language generation. Different from Li and Liang (2021), where each prefix is trained independently, we take the relationship among prefixes into consideration and train multiple prefixes simultaneously, as illustrated inFigure 1. We propose a novel supervised method and also an unsupervised method to train the prefixes for single-aspect control while the combination of these two methods can achieve multi-aspect control. Experimental results on both singleaspect and multi-aspect control show that our methods can guide generation towards the desired attributes while keeping high linguistic quality.",
        "id": 247158838
      }
    ],
    "negative_ctxs": [
      {
        "title": "An Investigation of Representation and Allocation Harms in Contrastive Learning",
        "text": "The effect of underrepresentation on the performance of minority groups is known to be a serious problem in supervised learning settings; however, it has been underexplored so far in the context of self-supervised learning (SSL).In this paper, we demonstrate that contrastive learning (CL), a popular variant of SSL, tends to collapse representations of minority groups with certain majority groups.We refer to this phenomenon as representation harm and demonstrate it on image and text datasets using the corresponding popular CL methods.Furthermore, our causal mediation analysis of allocation harm on a downstream classification task reveals that representation harm is partly responsible for it, thus emphasizing the importance of studying and mitigating representation harm.Finally, we provide a theoretical explanation for representation harm using a stochastic block model that leads to a representational neural collapse in a contrastive learning setting.",
        "id": 263608898
      },
      {
        "title": "What's in a gene name? Automated refinement of gene name dictionaries",
        "text": "Many approaches for named entity recognition rely on dictionaries gathered from curated databases (such as Entrez Gene for gene names.) Strategies for matching entries in a dictionary against arbitrary text use either inexact string matching that allows for known deviations, dictionaries enriched according to some observed rules, or a combination of both. Such refined dictionaries cover potential structural, lexical, orthographical, or morphological variations. In this paper, we present an approach to automatically analyze dictionaries to discover how names are composed and which variations typically occur. This knowledge can be constructed by looking at single entries (names and synonyms for one gene), and then be transferred to entries that show similar patterns in one or more synonyms. For instance, knowledge about words that are frequently missing in (or added to) a name (\"antigen\", \"protein\", \"human\") could automatically be extracted from dictionaries. This paper should be seen as a vision paper, though we implemented most of the ideas presented and show results for the task of gene name recognition. The automatically extracted name composition rules can easily be included in existing approaches, and provide valuable insights into the biomedical sub-language.",
        "id": 11328858
      },
      {
        "title": "The NiuTrans Machine Translation Systems for WMT20",
        "text": "This paper describes NiuTrans neural machine translation systems of the WMT20 news translation tasks.We participated in Japanese↔English, English→Chinese, Inuktitut→English and Tamil→English total five tasks and rank first in Japanese↔English both sides. We mainly utilized iterative backtranslation, different depth and widen model architectures, iterative knowledge distillation and iterative fine-tuning. And we find that adequately widened and deepened the model simultaneously, the performance will significantly improve. Also, iterative fine-tuning strategy we implemented is effective during adapting domain. For Inuktitut→English and Tamil→English tasks, we built multilingual models separately and employed pretraining word embedding to obtain better performance.",
        "id": 261826438
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "manual_iclr",
    "question": "Can you find a dataset that shows LLM-based evaluation may not be reliable enough?",
    "positive_ctxs": [
      {
        "title": "EVALUATING LARGE LANGUAGE MODELS AT EVALUATING INSTRUCTION FOLLOWING",
        "text": "As research in large language models (LLMs) continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evaluations for comparing the ever increasing list of models. This paper investigates the efficacy of these \"LLM evaluators\", particularly in using them to assess instruction following, a metric that gauges how closely generated text adheres to the given instruction. We introduce a challenging meta-evaluation benchmark, LLM-BAR, designed to test the ability of an LLM evaluator in discerning instructionfollowing outputs. The authors manually curated 419 pairs of outputs, one adhering to instructions while the other diverging, yet may possess deceptive qualities that mislead an LLM evaluator, e.g., a more engaging tone. Contrary to existing meta-evaluation, we discover that different evaluators (i.e., combinations of LLMs and prompts) exhibit distinct performance on LLMBAR and even the highestscoring ones have substantial room for improvement. We also present a novel suite of prompting strategies that further close the gap between LLM and human evaluators. With LLMBAR, we hope to offer more insight into LLM evaluators and foster future research in developing better instruction-following models.",
        "id": 263834884
      }
    ],
    "negative_ctxs": [
      {
        "title": "Towards General Natural Language Understanding with Probabilistic Worldbuilding",
        "text": "We introduce the Probabilistic Worldbuilding Model (PWM), a new fully symbolic Bayesian model of semantic parsing and reasoning, as a first step in a research program toward more domain-and task-general NLU and AI. Humans create internal mental models of their observations that greatly aid in their ability to understand and reason about a large variety of problems. In PWM, the meanings of sentences, acquired facts about the world, and intermediate steps in reasoning are all expressed in a human-readable formal language, with the design goal of interpretability. PWM is Bayesian, designed specifically to be able to generalize to new domains and new tasks. We derive and implement an inference algorithm that reads sentences by parsing and abducing updates to its latent world model that capture the semantics of those sentences, and evaluate it on two out-of-domain question-answering datasets:(1) ProofWriter and (2) a new dataset we call FictionalGeoQA, designed to be more representative of real language but still simple enough to focus on evaluating reasoning ability, while being robust against heuristics. Our method outperforms baselines on both, thereby demonstrating its value as a proof-of-concept.325",
        "id": 245353941
      },
      {
        "title": "",
        "text": "",
        "id": 235097583
      },
      {
        "title": "What Does BERT with Vision Look At?",
        "text": "Pre-trained visually grounded language models such as ViLBERT, LXMERT, and UNITER have achieved significant performance improvement on vision-and-language tasks but what they learn during pre-training remains unclear. In this work, we demonstrate that certain attention heads of a visually grounded language model actively ground elements of language to image regions. Specifically, some heads can map entities to image regions, performing the task known as entity grounding. Some heads can even detect the syntactic relations between non-entity words and image regions, tracking, for example, associations between verbs and regions corresponding to their arguments. We denote this ability as syntactic grounding. We verify grounding both quantitatively and qualitatively, using Flickr30K Entities as a testbed. . 2015. VQA: Visual question answering. In ICCV.Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C Lawrence Zitnick. 2015. Microsoft COCO captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325.",
        "id": 218610661
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "manual_acl",
    "question": "Is there any paper that investigates backdoor attacks across various types of tasks, not limited to classification, in language models?",
    "positive_ctxs": [
      {
        "title": "Multi-target Backdoor Attacks for Code Pre-trained Models",
        "text": "Backdoor attacks for neural code models have gained considerable attention due to the advancement of code intelligence. However, most existing works insert triggers into task-specific data for code-related downstream tasks, thereby limiting the scope of attacks. Moreover, the majority of attacks for pre-trained models are designed for understanding tasks. In this paper, we propose task-agnostic backdoor attacks for code pre-trained models. Our backdoored model is pre-trained with two learning strategies (i.e., Poisoned Seq2Seq learning and token representation learning) to support the multitarget attack of downstream code understanding and generation tasks. During the deployment phase, the implanted backdoors in the victim models can be activated by the designed triggers to achieve the targeted attack. We evaluate our approach on two code understanding tasks and three code generation tasks over seven datasets. Extensive experiments demonstrate that our approach can effectively and stealthily attack code-related downstream tasks. Hoi. 2021b. Codet5: Identifier-aware unified pretrained encoder-decoder models for code understanding and generation. In",
        "id": 259165134
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 252763347
      },
      {
        "title": "Conversational Decision-Making Model for Predicting the King's Decision in the Annals of the Joseon Dynasty",
        "text": "Styles of leaders when they make decisions in groups vary, and the different styles affect the performance of the group. To understand the key words and speakers associated with decisions, we initially formalize the problem as one of predicting leaders' decisions from discussion with group members. As a dataset, we introduce conversational meeting records from a historical corpus, and develop a hierarchical RNN structure with attention and pre-trained speaker embedding in the form of a, Conversational Decision Making Model (CDMM). The CDMM outperforms other baselines to predict leaders' final decisions from the data. We explain why CDMM works better than other methods by showing the key words and speakers discovered from the attentions as evidence.",
        "id": 53082945
      },
      {
        "title": "A log-linear model with an n-gram reference distribution for accurate HPSG parsing",
        "text": "This paper describes a log-linear model with an n-gram reference distribution for accurate probabilistic HPSG parsing. In the model, the n-gram reference distribution is simply defined as the product of the probabilities of selecting lexical entries, which are provided by the discriminative method with machine learning features of word and POS n-gram as defined in the CCG/HPSG/CDG supertagging. Recently, supertagging becomes well known to drastically improve the parsing accuracy and speed, but supertagging techniques were heuristically introduced, and hence the probabilistic models for parse trees were not well defined. We introduce the supertagging probabilities as a reference distribution for the log-linear model of the probabilistic HPSG. This is the first model which properly incorporates the supertagging probabilities into parse tree's probabilistic model.",
        "id": 13040693
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "What techniques and frameworks have been suggested for summarizing extensive texts under resource-constrained conditions?",
    "positive_ctxs": [
      {
        "title": "Long Document Summarization in a Low Resource Setting using Pretrained Language Models",
        "text": "Abstractive summarization is the task of compressing a long document into a coherent short document while retaining salient information. Modern abstractive summarization methods are based on deep neural networks which often require large training datasets. Since collecting summarization datasets is an expensive and time-consuming task, practical industrial settings are usually low-resource. In this paper, we study a challenging low-resource setting of summarizing long legal briefs with an average source document length of 4268 words and only 120 available (document, summary) pairs. To account for data scarcity, we used a modern pretrained abstractive summarizer BART (Lewis et al., 2020), which only achieves 17.9 ROUGE-L as it struggles with long documents. We thus attempt to compress these long documents by identifying salient sentences in the source which best ground the summary, using a novel algorithm based on GPT-2 (Radford et al., 2019) language model perplexity scores, that operates within the low resource regime. On feeding the compressed documents to BART, we observe a 6.0 ROUGE-L improvement. Our method also beats several competitive salience detection baselines. Furthermore, the identified salient sentences tend to agree with an independent human labeling by domain experts. *",
        "id": 232075836
      }
    ],
    "negative_ctxs": [
      {
        "title": "Multilingual Universal Dependency Parsing from Raw Text with Low-resource Language Enhancement",
        "text": "This paper describes the system of our team Phoenix for participating CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Given the annotated gold standard data in CoNLL-U format, we train the tokenizer, tagger and parser separately for each treebank based on an open source pipeline tool UDPipe. Our system reads the plain texts for input, performs the preprocessing steps (tokenization, lemmas, morphology) and finally outputs the syntactic dependencies. For the low-resource languages with no training data, we use cross-lingual techniques to build models with some close languages instead. In the official evaluation, our system achieves the macro-averaged scores of 65.61%, 52.26%, 55.71% for LAS, MLAS and BLEX respectively.",
        "id": 53106215
      },
      {
        "title": "Hyper-SAGNN: a self-attention based graph neural network for hypergraphs A PREPRINT",
        "text": "Graph representation learning for hypergraphs can be used to extract patterns among higher-order interactions that are critically important in many real world problems. Current approaches designed for hypergraphs, however, are unable to handle different types of hypergraphs and are typically not generic for various learning tasks. Indeed, models that can predict variable-sized heterogeneous hyperedges have not been available. Here we develop a new self-attention based graph neural network called Hyper-SAGNN applicable to homogeneous and heterogeneous hypergraphs with variable hyperedge sizes. We perform extensive evaluations on multiple datasets, including four benchmark network datasets and two single-cell Hi-C datasets in genomics. We demonstrate that Hyper-SAGNN significantly outperforms the state-of-the-art methods on traditional tasks while also achieving great performance on a new task called outsider identification. Hyper-SAGNN will be useful for graph representation learning to uncover complex higher-order interactions in different applications.Corresponding Author (node) Coauthor (node) Coauthorship (hyperedge)Figure 1: An example of the co-authorship hypergraph. Here authors are represented as nodes (in dark blue and light blue) and coauthorships are represented as hyperedges.In this work, we developed a new self-attention based graph neural network, called Hyper-SAGNN that can work with both homogeneous and heterogeneous hypergraphs with variable hyperedge size. Using the same datasets in the DHNE paper(Tu et al., 2018), we demonstrated the advantage of Hyper-SAGNN over DHNE in multiple tasks. We further tested the effectiveness of the method in predicting edges and hyperedges and showed that the model can achieve better performance from the multi-tasking setting. We also formulated a novel task called outsider identification and showed that Hyper-SAGNN performs strongly. Importantly, as an application of Hyper-SAGNN to single-cell genomics, we were able to learn the embeddings for the most recently produced single-cell Hi-C (scHi-C) datasets to uncover the clustering of cells based on their 3D genome structure(Ramani et al., 2017;Nagano et al., 2017). We showed that Hyper-SAGNN achieved improved results in identifying distinct cell populations as compared to existing scHi-C clustering methods. Taken together, Hyper-SAGNN can significantly outperform the state-of-the-art methods and can be applied to a wide range of hypergraphs for different applications.",
        "id": 207847681
      },
      {
        "title": "",
        "text": "",
        "id": 218977422
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a study that uses feedback-driven decoding for producing mathematical proofs using language models?",
    "positive_ctxs": [
      {
        "title": "Generating Natural Language Proofs with Verifier-Guided Search",
        "text": "Reasoning over natural language is a challenging problem in NLP. In this work, we focus on proof generation: Given a hypothesis and a set of supporting facts, the model generates a proof tree indicating how to derive the hypothesis from supporting facts. Compared to generating the entire proof in one shot, stepwise generation can better exploit the compositionality and generalize to longer proofs but has achieved limited success on real-world data. Existing stepwise methods struggle to generate proof steps that are both logically valid and relevant to the hypothesis. Instead, they tend to hallucinate invalid steps given the hypothesis. In this paper, we present a novel stepwise method, NLProofS (Natural Language Proof Search), which learns to generate relevant steps conditioning on the hypothesis. At the core of our approach, we train an independent verifier to check the validity of the proof steps to prevent hallucination. Instead of generating steps greedily, we search for proofs maximizing a global proof score judged by the verifier. NL-ProofS achieves state-of-the-art performance on EntailmentBank and RuleTaker. Specifically, it improves the correctness of predicted proofs from 27.7% to 33.3% in the distractor setting of EntailmentBank, demonstrating the effectiveness of NLProofS in generating challenging human-authored proofs. 1",
        "id": 249062748
      }
    ],
    "negative_ctxs": [
      {
        "title": "Learning beyond datasets: Knowledge Graph Augmented Neural Networks for Natural language Processing",
        "text": "Machine Learning has been the quintessential solution for many AI problems, but learning models are heavily dependent on specific training data. Some learning models can be incorporated with prior knowledge using a Bayesian setup, but these learning models do not have the ability to access any organized world knowledge on demand. In this work, we propose to enhance learning models with world knowledge in the form of Knowledge Graph (KG) fact triples for Natural Language Processing (NLP) tasks. Our aim is to develop a deep learning model that can extract relevant prior support facts from knowledge graphs depending on the task using attention mechanism. We introduce a convolutionbased model for learning representations of knowledge graph entity and relation clusters in order to reduce the attention space. We show that the proposed method is highly scalable to the amount of prior information that has to be processed and can be applied to any generic NLP task. Using this method we show significant improvement in performance for text classification with 20Newsgroups (News20) & DBPedia datasets, and natural language inference with Stanford Natural Language Inference (SNLI) dataset. We also demonstrate that a deep learning model can be trained with substantially less amount of labeled training data, when it has access to organized world knowledge in the form of a knowledge base.",
        "id": 3361768
      },
      {
        "title": "Document-level Neural MT: A Systematic Comparison",
        "text": "In this paper we provide a systematic comparison of existing and new documentlevel neural machine translation solutions. As part of this comparison, we introduce and evaluate a document-level variant of the recently proposed Star Transformer architecture. In addition to using the traditional metric BLEU, we report the accuracy of the models in handling anaphoric pronoun translation as well as coherence and cohesion using contrastive test sets. Finally, we report the results of human evaluation in terms of Multidimensional Quality Metrics (MQM) and analyse the correlation of the results obtained by the automatic metrics with human judgments.",
        "id": 221097205
      },
      {
        "title": "Published as a conference paper at ICLR 2020 LEARNING HIERARCHICAL DISCRETE LINGUISTIC UNITS FROM VISUALLY-GROUNDED SPEECH",
        "text": "In this paper, we present a method for learning discrete linguistic units by incorporating vector quantization layers into neural models of visually grounded speech. We show that our method is capable of capturing both word-level and sub-word units, depending on how it is configured. What differentiates this paper from prior work on speech unit learning is the choice of training objective. Rather than using a reconstruction-based loss, we use a discriminative, multimodal grounding objective which forces the learned units to be useful for semantic image retrieval. We evaluate the sub-word units on the ZeroSpeech 2019 challenge, achieving a 27.3% reduction in ABX error rate over the top-performing submission, while keeping the bitrate approximately the same. We also present experiments demonstrating the noise robustness of these units. Finally, we show that a model with multiple quantizers can simultaneously learn phone-like detectors at a lower layer and word-like detectors at a higher layer. We show that these detectors are highly accurate, discovering 279 words with an F1 score of greater than 0.5.",
        "id": 208202182
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_acl",
    "question": "Can you suggest recent studies that have integrated prompt fine-tuning into semi-supervised learning workflows for natural language understanding tasks?",
    "positive_ctxs": [
      {
        "title": "Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference",
        "text": "Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with \"task descriptions\" in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in lowresource settings by a large margin. 1",
        "id": 210838924
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 219310164
      },
      {
        "title": "Automated Extrac-tion of Tree Adjoining Grammars from a Treebank for Vietnamese",
        "text": "Tóm tắt nội dungBài báo này giới thiệu hệ văn phạm kết nối cây LTAG (Lexicalized Tree Adjoining Grammars -LTAG) và các thuật toán trích rút tự động LTAG từ kho văn bản gán nhãn cú pháp (treebank). Chúng tôi trình bày kết quả trích rút một văn phạm LTAG cho tiếng Việt. Chương trình trích rút tự động các văn phạm LTAG độc lập với ngôn ngữ và được phân phối dưới dạng mã nguồn mở.Từ khoá: trích rút, LTAG, treebank, tiếng Việt.AbstractIn this paper, we present a system that automatically extracts lexicalized tree adjoining grammars (LTAG) from treebanks. We first discuss in detail extraction algorithms and compare them to previous works. We then report the first LTAG extraction result for Vietnamese, using a recently released Vietnamese treebank. The implementation of an open source and language independent system for automatic extraction of LTAG grammars is also discussed.",
        "id": 13647606
      },
      {
        "title": "UNBNLP at SemEval-2019 Task 5 and 6: Using Language Models to Detect Hate Speech and Offensive Language",
        "text": "In this paper we apply a range of approaches to language modeling -including wordlevel n-gram and neural language models, and character-level neural language models -to the problem of detecting hate speech and offensive language. Our findings indicate that language models are able to capture knowledge of whether text is hateful or offensive. However, our findings also indicate that moreconventional approaches to text classification often perform similarly or better.",
        "id": 184483194
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "What research exists on incorporating knowledge graphs into language models to improve their complex question-answering capabilities?",
    "positive_ctxs": [
      {
        "title": "Knowledge Graph-augmented Language Models for Complex Question Answering",
        "text": "Large language models have shown impressive abilities to reason over input text, however, they are prone to hallucinations. On the other hand, end-to-end knowledge graph question answering (KGQA) models output responses grounded in facts, but they still struggle with complex reasoning, such as comparison or ordinal questions. In this paper, we propose a new method for complex question answering where we combine a knowledge graph retriever based on an end-to-end KGQA model with a language model that reasons over the retrieved facts to return an answer. We observe that augmenting language model prompts with retrieved KG facts improves performance over using a language model alone by an average of 83%. In particular, we see improvements on complex questions requiring count, intersection, or multi-hop reasoning operations.",
        "id": 259833781
      }
    ],
    "negative_ctxs": [
      {
        "title": "Bloom Library: Multimodal Datasets in 300+ Languages for a Variety of Downstream Tasks",
        "text": "We present Bloom Library, a linguistically diverse set of multimodal and multilingual datasets for language modeling, image captioning, visual storytelling, and speech synthesis/recognition. These datasets represent either the most, or among the most, multilingual datasets for each of the included downstream tasks. In total, the initial release of the Bloom Library datasets covers 363 languages across 32 language families. We train downstream task models for various languages represented in the data, showing the viability of the data for future work in low-resource, multimodal NLP and establishing the first known baselines for these downstream tasks in certain languages (e.g., Bisu [bzi], with an estimated population of 700 users). Some of these first-of-their-kind baselines are comparable to state-of-the-art performance for higher-resourced languages. The Bloom Library datasets are released under Creative Commons licenses on the Hugging Face datasets hub to catalyze more linguistically diverse research in the included downstream tasks.",
        "id": 253116724
      },
      {
        "title": "A New Version of the Składnica Treebank of Polish Harmonised with the Walenty Valency Dictionary",
        "text": "This paper reports on developments in the Składnica treebank of Polish which were possible due to the switch to the Walenty valency dictionary. The change required several modifications in the Świgra parser, such as implementing unlike coordination, semantically motivated phrases, and non-standard case values. A procedure to upgrade manually disambiguated trees of Składnica was required as well. Modifications introduced in the treebank included systematic changes of notation and resolving ambiguity between semantically motivated phrases. The procedure of confronting Składnica treebank with the trees generated with the new version of the Świgra parser using Walenty dictionary allowed us to check the consistency of all the resources. This resulted in several corrections introduced in both the treebank and the valence dictionary.",
        "id": 21715487
      },
      {
        "title": "SIMPLE EMERGENT ACTION REPRESENTATIONS FROM MULTI-TASK POLICY TRAINING",
        "text": "The low-level sensory and motor signals in deep reinforcement learning, which exist in high-dimensional spaces such as image observations or motor torques, are inherently challenging to understand or utilize directly for downstream tasks. While sensory representations have been extensively studied, the representations of motor actions are still an area of active exploration. Our work reveals that a space containing meaningful action representations emerges when a multi-task policy network takes as inputs both states and task embeddings. Moderate constraints are added to improve its representation ability. Therefore, interpolated or composed embeddings can function as a high-level interface within this space, providing instructions to the agent for executing meaningful action sequences. Empirical results demonstrate that the proposed action representations are effective for intra-action interpolation and inter-action composition with limited or no additional learning. Furthermore, our approach exhibits superior task adaptation ability compared to strong baselines in Mujoco locomotion tasks. Our work sheds light on the promising direction of learning action representations for efficient, adaptable, and composable RL, forming the basis of abstract action planning and the understanding of motor signal space. Project page: https://sites. google.com/view/emergent-action-representation/ * Denotes equal contributions.",
        "id": 252967887
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a study that investigates guiding abstractive summarization through assessing sentence informativeness?",
    "positive_ctxs": [
      {
        "title": "Salience Allocation as Guidance for Abstractive Summarization",
        "text": "Abstractive summarization models typically learn to capture the salient information from scratch implicitly. Recent literature adds extractive summaries as guidance for abstractive summarization models to provide hints of salient content and achieves better performance. However, extractive summaries as guidance could be over strict, leading to information loss or noisy signals. Furthermore, it cannot easily adapt to documents with various abstractiveness. As the number and allocation of salience content pieces vary, it is hard to find a fixed threshold deciding which content should be included in the guidance. In this paper, we propose a novel summarization approach with a flexible and reliable salience guidance, namely SEASON (SaliencE Allocation as Guidance for Abstractive SummarizatiON). SEASON utilizes the allocation of salience expectation to guide abstractive summarization and adapts well to articles in different abstractiveness. Automatic and human evaluations on two benchmark datasets show that the proposed method is effective and reliable. Empirical results on more than one million news articles demonstrate a natural fifteen-fifty salience split for news article sentences, providing a useful insight for composing news articles. 1 * Work done during Fei Wang's internship at Tencent AI Lab Seattle. The first two authors contributed equally. 1 Code and model weights are available at https:// github.com/tencent-ailab/season.",
        "id": 253098395
      }
    ],
    "negative_ctxs": [
      {
        "title": "Structural and Topical Dimensions in Multi-Task Patent Translation",
        "text": "Patent translation is a complex problem due to the highly specialized technical vocabulary and the peculiar textual structure of patent documents. In this paper we analyze patents along the orthogonal dimensions of topic and textual structure. We view different patent classes and different patent text sections such as title, abstract, and claims, as separate translation tasks, and investigate the influence of such tasks on machine translation performance. We study multitask learning techniques that exploit commonalities between tasks by mixtures of translation models or by multi-task metaparameter tuning. We find small but significant gains over task-specific training by techniques that model commonalities through shared parameters. A by-product of our work is a parallel patent corpus of 23 million German-English sentence pairs.",
        "id": 785716
      },
      {
        "title": "Human Language Technology Elements in a Knowledge Organisation System - The VID project",
        "text": "This paper describes how Human Language Technologies and linguistic resources are used to support the construction of components of a knowledge organisation system. In particular we focus on methodologies and resources for building a corpus-based domain ontology and extracting relevant metadata information for text chunks from domain-specific corpora.",
        "id": 15248731
      },
      {
        "title": "",
        "text": "",
        "id": 208246040
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a study that investigates enhancing prompt engineering techniques for generative models using meta-learning strategies?",
    "positive_ctxs": [
      {
        "title": "MetaPrompting: Learning to Learn Better Prompts",
        "text": "Prompting method is regarded as one of the crucial progress for few-shot nature language processing. Recent research on prompting moves from discrete tokens based \"hard prompts\" to continuous \"soft prompts\", which employ learnable vectors as pseudo prompt tokens and achieve better performance. Though showing promising prospects, these softprompting methods are observed to rely heavily on good initialization to take effect. Unfortunately, obtaining a perfect initialization for soft prompts requires understanding of inner language models working and elaborate design, which is no easy task and has to restart from scratch for each new task. To remedy this, we propose a generalized soft prompting method called MetaPrompting, which adopts the well-recognized modelagnostic meta-learning algorithm to automatically find better prompt initialization that facilitates fast adaptation to new prompting tasks. Extensive experiments show MetaPrompting tackles soft prompt initialization problem and brings significant improvement on four different datasets (over 7 points improvement in accuracy for 1-shot setting), achieving new stateof-the-art performance.",
        "id": 252519660
      }
    ],
    "negative_ctxs": [
      {
        "title": "An Ontology-Based Approach for Key Phrase Extraction",
        "text": "Automatic key phrase extraction is fundamental to the success of many recent digital library applications and semantic information retrieval techniques and a difficult and essential problem in Vietnamese natural language processing (NLP). In this work, we propose a novel method for key phrase extracting of Vietnamese text that exploits the Vietnamese Wikipedia as an ontology and exploits specific characteristics of the Vietnamese language for the key phrase selection stage. We also explore NLP techniques that we propose for the analysis of Vietnamese texts, focusing on the advanced candidate phrases recognition phase as well as part-of-speech (POS) tagging. Finally, we review the results of several experiments that have examined the impacts of strategies chosen for Vietnamese key phrase extracting.",
        "id": 219306007
      },
      {
        "title": "Detecting Figurative Word Occurrences Using Recurrent Neural Networks",
        "text": "The paper addresses the detection of figurative usage of words in English text. The chosen method was to use neural nets fed by pre-trained word embeddings. The obtained results show that simple solutions, based on word embeddings only, are comparable to complex solutions, using additional information as a result of taggers or a psycholinguistic database. This approach can be easily applied to other languages, even less-studied, for which we only have raw texts available.",
        "id": 52001146
      },
      {
        "title": "Book Reviews Genesis: An Authorship Study",
        "text": "",
        "id": 18962876
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research that investigates how to use data augmentation for improving logical reasoning over text?",
    "positive_ctxs": [
      {
        "title": "MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning",
        "text": "Logical reasoning is of vital importance to natural language understanding. Previous studies either employ graph-based models to incorporate prior knowledge about logical relations, or introduce symbolic logic into neural models through data augmentation. These methods, however, heavily depend on annotated training data, and thus suffer from overfitting and poor generalization problems due to the dataset sparsity. To address these two problems, in this paper, we propose MERIt, a MEta-path guided contrastive learning method for logical ReasonIng of text, to perform selfsupervised pre-training on abundant unlabeled text data. Two novel strategies serve as indispensable components of our method. In particular, a strategy based on meta-path is devised to discover the logical structure in natural texts, followed by a counterfactual data augmentation strategy to eliminate the information shortcut induced by pre-training. The experimental results on two challenging logical reasoning benchmarks, i.e., ReClor and LogiQA, demonstrate that our method outperforms the SOTA baselines with significant improvements. 1 * Corresponding author: Yangyang Guo and Liqiang Nie. 1 Our code and pre-trained models are available at https: //github.com/SparkJiao/MERIt. 2  We refer the term logical reasoning to the task itself in the remaining of this paper.",
        "id": 247187518
      },
      {
        "title": "Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text",
        "text": "Logical reasoning of text requires identifying critical logical structures in the text and performing inference over them. Existing methods for logical reasoning mainly focus on contextual semantics of text while struggling to explicitly model the logical inference process. In this paper, we not only put forward a logic-driven context extension framework but also propose a logic-driven data augmentation algorithm. The former follows a three-step reasoning paradigm, and each step is respectively to extract logical expressions as elementary reasoning units, symbolically infer the implicit expressions following equivalence laws and extend the context to validate the options. The latter augments literally similar but logically different instances and incorporates contrastive learning to better capture logical information, especially logical negative and conditional relationships. We conduct experiments on two benchmark datasets, ReClor and LogiQA. The results show that our method achieves state-of-the-art performance on both datasets, and even surpasses human performance on the ReClor dataset. 1",
        "id": 234335834
      }
    ],
    "negative_ctxs": [
      {
        "title": "Regular Expression Guided Entity Mention Mining from Noisy Web Data",
        "text": "Many important entity types in web documents, such as dates, times, email addresses, and course numbers, follow or closely resemble patterns that can be described by Regular Expressions (REs). Due to a vast diversity of web documents and ways in which they are being generated, even seemingly straightforward tasks such as identifying mentions of date in a document become very challenging. It is reasonable to claim that it is impossible to create a RE that is capable of identifying such entities from web documents with perfect precision and recall. Rather than abandoning REs as a go-to approach for entity detection, this paper explores ways to combine the expressive power of REs, ability of deep learning to learn from large data, and human-in-the loop approach into a new integrated framework for entity identification from web data. The framework starts by creating or collecting the existing REs for a particular type of an entity. Those REs are then used over a large document corpus to collect weak labels for the entity mentions and a neural network is trained to predict those RE-generated weak labels. Finally, a human expert is asked to label a small set of documents and the neural network is fine tuned on those documents. The experimental evaluation on several entity identification problems shows that the proposed framework achieves impressive accuracy, while requiring very modest human effort.",
        "id": 53083786
      },
      {
        "title": "HR@JUST team at SemEval-2020 Task 4: The impact of RoBERTa transformer for evaluation common sense understanding",
        "text": "This paper describes the results of our team HR@JUST participation at SemEval-2020 Task 4 -Commonsense Validation and Explanation (ComVE) for the POST evaluation period. The provided task consists of three sub-tasks, we participate in task A. We considered a state-of-the-art approach for solving this task by performing RoBERTa model with no Next Sentences Prediction (NSP), dynamic masking, larger training data, and larger batch size. The achieved results show that we got the 11th rank on the final test set leaderboard with an accuracy of 91.3%.This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/.",
        "id": 227230568
      },
      {
        "title": "Diachronic word embeddings and semantic shifts: a survey",
        "text": "Recent years have witnessed a surge of publications aimed at tracing temporal changes in lexical semantics using distributional methods, particularly prediction-based word embedding models. However, this vein of research lacks the cohesion, common terminology and shared practices of more established areas of natural language processing. In this paper, we survey the current state of academic research related to diachronic word embeddings and semantic shifts detection. We start with discussing the notion of semantic shifts, and then continue with an overview of the existing methods for tracing such time-related shifts with word embedding models. We propose several axes along which these methods can be compared, and outline the main challenges before this emerging subfield of NLP, as well as prospects and possible applications.This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/",
        "id": 47019063
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a study that explores strategies for improving multi-label text classification by incorporating information about label distribution directly into the loss function?",
    "positive_ctxs": [
      {
        "title": "Balancing Methods for Multi-label Text Classification with Long-Tailed Class Distribution",
        "text": "Multi-label text classification is a challenging task because it requires capturing label dependencies. It becomes even more challenging when class distribution is long-tailed. Resampling and re-weighting are common approaches used for addressing the class imbalance problem, however, they are not effective when there is label dependency besides class imbalance because they result in oversampling of common labels. Here, we introduce the application of balancing loss functions for multilabel text classification. We perform experiments on a general domain dataset with 90 labels (Reuters-21578) and a domain-specific dataset from PubMed with 18211 labels. We find that a distribution-balanced loss function, which inherently addresses both the class imbalance and label linkage problems, outperforms commonly used loss functions. Distribution balancing methods have been successfully used in the image recognition field. Here, we show their effectiveness in natural language processing. Source code is available at https://github.com/blessu/ BalancedLossNLP.",
        "id": 237485334
      }
    ],
    "negative_ctxs": [
      {
        "title": "An Entity-Mention Model for Coreference Resolution with Inductive Logic Programming",
        "text": "The traditional mention-pair model for coreference resolution cannot capture information beyond mention pairs for both learning and testing. To deal with this problem, we present an expressive entity-mention model that performs coreference resolution at an entity level. The model adopts the Inductive Logic Programming (ILP) algorithm, which provides a relational way to organize different knowledge of entities and mentions. The solution can explicitly express relations between an entity and the contained mentions, and automatically learn first-order rules important for coreference decision. The evaluation on the ACE data set shows that the ILP based entity-mention model is effective for the coreference resolution task.",
        "id": 126675
      },
      {
        "title": "RedwoodNLP at SemEval-2021 Task 7: Ensembled Pretrained and Lightweight Models for Humor Detection",
        "text": "An understanding of humor is an essential component of human-facing NLP systems. In this paper, we investigate several methods for detecting humor in short statements as part of Semeval-2021 Shared Task 7. For Task 1a, we apply an ensemble of fine-tuned pre-trained language models; for Tasks 1b, 1c, and 2a, we investigate various tree-based and linear machine learning models. Our final system achieves an F1-score of 0.9571 (ranked 24 / 58) on Task 1a, an RMSE of 0.5580 (ranked 18 / 50) on Task 1b, an F1-score of 0.5024 (ranked 26 / 36) on Task 1c, and an RMSE of 0.7229 (ranked 45 / 48) on Task 2a.",
        "id": 236459807
      },
      {
        "title": "Determining the Syntactic Structure of Medical Terms in Clinical Notes",
        "text": "This paper demonstrates a method for determining the syntactic structure of medical terms. We use a model-fitting method based on the Log Likelihood Ratio to classify three-word medical terms as right or left-branching. We validate this method by computing the agreement between the classification produced by the method and manually annotated classifications. The results show an agreement of 75% -83%. This method may be used effectively to enable a wide range of applications that depend on the semantic interpretation of medical terms including automatic mapping of terms to standardized vocabularies and induction of terminologies from unstructured medical text.",
        "id": 6543751
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_acl",
    "question": "Can you suggest literature on enhanced semantic parsing methods that focus on generating high-quality meaning representations and utilize knowledge-constrained decoding under specific grammar rules?",
    "positive_ctxs": [
      {
        "title": "A Syntactic Neural Model for General-Purpose Code Generation",
        "text": "We consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like Python. Existing datadriven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language. Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge. Experiments find this an effective way to scale up to generation of complex programs from natural language descriptions, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.",
        "id": 12718048
      },
      {
        "title": "Neural Semantic Parsing with Type Constraints for Semi-Structured Tables",
        "text": "We present a new semantic parsing model for answering compositional questions on semi-structured Wikipedia tables. Our parser is an encoder-decoder neural network with two key technical innovations:(1) a grammar for the decoder that only generates well-typed logical forms; and (2) an entity embedding and linking module that identifies entity mentions while generalizing across tables. We also introduce a novel method for training our neural model with question-answer supervision. On the WIKITABLEQUESTIONS data set, our parser achieves a state-of-theart accuracy of 43.3% for a single model and 45.9% for a 5-model ensemble, improving on the best prior score of 38.7% set by a 15-model ensemble. These results suggest that type constraints and entity linking are valuable components to incorporate in neural semantic parsers.",
        "id": 1675452
      },
      {
        "title": "Sequence-based Structured Prediction for Semantic Parsing",
        "text": "We propose an approach for semantic parsing that uses a recurrent neural network to map a natural language question into a logical form representation of a KB query. Building on recent work by(Wang et al., 2015), the interpretable logical forms, which are structured objects obeying certain constraints, are enumerated by an underlying grammar and are paired with their canonical realizations. In order to use sequence prediction, we need to sequentialize these logical forms. We compare three sequentializations: a direct linearization of the logical form, a linearization of the associated canonical realization, and a sequence consisting of derivation steps relative to the underlying grammar. We also show how grammatical constraints on the derivation sequence can easily be integrated inside the RNNbased sequential predictor. Our experiments show important improvements over previous results for the same dataset, and also demonstrate the advantage of incorporating the grammatical constraints.",
        "id": 16911296
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "In this paper, we explore a new approach for automated chess commentary generation, which aims to generate chess commentary texts in different categories (e.g., description, comparison, planning, etc.). We introduce a neural chess engine into text generation models to help with encoding boards, predicting moves, and analyzing situations. By jointly training the neural chess engine and the generation models for different categories, the models become more effective. We conduct experiments on 5 categories in a benchmark Chess Commentary dataset and achieve inspiring results in both automatic and human evaluations.",
        "id": 196203256
      },
      {
        "title": "",
        "text": "",
        "id": 221373808
      },
      {
        "title": "Variabilité des syllabes réalisées par des apprenants de l'anglais",
        "text": "Cette contribution analyse la segmentation syllabique des francophones du corpus d'apprenant d'anglais ANGLISH (Tortel 2009). A partir d'une méthode d'alignement par alignement forcé, on montre la pertinence d'une analyse de l'interlangue fondée sur la comparaison des durées des syllabes. La comparaison des réalisations est ici centrée sur une typologie des syllabes fondée sur des propriétés distributionnelles, accentuelles et où l'interlangue tient sa place (risques d'isosyllabicité les plus manifestes pour les réalisations des francophones). La variabilité des réalisations des syllabes est appréciée en fonction des propriétés positionnelles, accentuelles et structurelles des syllabes. L'étude démontre l'intérêt d'une approche fonctionnelle des syllabes, plus pertinente que les intervalles interconsonantiques et intervocaliques inspirés deRamus et al. (1999)pour la discrimination du niveau des locuteurs.ABSTRACTAnalysing syllable variability in a French learner corpus of English.This paper suggests an alternative method to classify native and non-native rhythmic realisations. Learner phonetic output has been automatically aligned on a native template of English syllables. Syllables have been classified according to positional, distributional and accentual properties. These syllable types differ significantly in their realisations between native and non-native speakers of English.MOTS-CLÉS : syllabation, interphonologie français/anglais, alignement forcé, durée.",
        "id": 64813718
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research that investigates the influence of cognitive biases on human interpretation of AI-generated explanations, specifically within the realm of explainable natural language processing?",
    "positive_ctxs": [
      {
        "title": "On the Interaction of Belief Bias and Explanations",
        "text": "A myriad of explainability methods have been proposed in recent years, but there is little consensus on how to evaluate them. While automatic metrics allow for quick benchmarking, it isn't clear how such metrics reflect human interaction with explanations. Human evaluation is of paramount importance, but previous protocols fail to account for belief biases affecting human performance, which may lead to misleading conclusions. We provide an overview of belief bias, its role in human evaluation, and ideas for NLP practitioners on how to account for it. For two experimental paradigms, we present a case study of gradientbased explainability introducing simple ways to account for humans' prior beliefs: models of varying quality and adversarial examples. We show that conclusions about the highest performing methods change when introducing such controls, pointing to the importance of accounting for belief bias in evaluation.",
        "id": 235669650
      }
    ],
    "negative_ctxs": [
      {
        "title": "The Synaesthetic and Metaphorical Uses of wei 'taste' in Chinese Buddhist Texts",
        "text": "This paper investigates the non-gustatory uses of the gustatory word wei 'taste' in Chinese Buddhist texts, in particular, in the Āgamas. The non-gustatory uses of wei 'taste' basically fall into two categories: the synaesthetic category and the metaphorical category. The former features the use of wei 'taste' as an umbrella sensory term which can collocate with all the other sensory words, whereas the latter shows that wei 'taste' can modify abstract and sublime Buddhist terms, such as fa 'dhamma' and jietuo 'enlightenment', for the sake of concretization. These two categories of uses have one sense in common: the sense of \"pleasure and joy\", which can be interpreted in both mundane and supra-mundane levels, depending on the context. Moreover, we find that the versatile uses of wei 'taste' are most likely to be influenced by its equivalent in the Pāli Buddhist texts. This finding sheds light on the history of Chinese language development, specifically, how Chinese language has been influenced by Buddhist text translation.",
        "id": 18449613
      },
      {
        "title": "Morpho-Syntactic Analysis for Reordering in Statistical Machine Translation",
        "text": "In the framework of statistical machine translation (SMT), correspondences between the words in the source and the target language are learned from bilingual corpora on the basis of so-called alignment models. Among other things these are meant to capture the differences in word order in different languages. In this paper we show that SMT can take advantage of the explicit introduction of some linguistic knowledge about the sentence structure in the languages under consideration. In contrast to previous publications dealing with the incorporation of morphological and syntactic information into SMT, we focus on two aspects of reordering for the language pair German and English, namely question inversion and detachable German verb prefixes. The results of systematic experiments are reported and demonstrate the applicability of the approach to both translation directions on a German-English corpus.",
        "id": 13158483
      },
      {
        "title": "Parsing Incomplete Sentences",
        "text": "An efficient context-free parsing algorithm is presented that can parse sentences with unknown parts of unknown length. It pa'oduees in finite form all possible parses (often infinite in number) that could account for the missing parts. The algorithm is a variation oa the construction due to Earley. ltowever, its presentation is such that it can readily be adapted to any chart parsing schema (topdown, bottom-up, etc...).",
        "id": 3052195
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "manual_iclr",
    "question": "Is there a single GNN model that can inductively generalize to any knowledge graph?;What is the method to generalize knowledge graph reasoning to graphs with new entities and relations?;Is there a foundation model for knowledge graphs that does not learn embeddings for each node and relation type?",
    "positive_ctxs": [
      {
        "title": "TOWARDS FOUNDATION MODELS FOR KNOWLEDGE GRAPH REASONING",
        "text": "Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language.Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap.The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies.In this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations.ULTRA builds relational representations as a function conditioned on their interactions.Such a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph.Conducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs.Fine-tuning further boosts the performance.",
        "id": 263831485
      }
    ],
    "negative_ctxs": [
      {
        "title": "A Game-based Approach to Transcribing Images of Text",
        "text": "We present a methodology that takes as input scanned documents of typed or hand-written text, and produces transcriptions of the text as output. Instead of using OCR technology, the methodology is game-based and produces such transcriptions as a by-product. The approach is intended particularly for languages for which language technology and resources are scarce and reliable OCR technology may not exist. It can be used in place of OCR for transcribing individual documents, or to create corpora of paired images and transcriptions required to train OCR tools. We present Minefield, a prototype implementation of the approach which is currently collecting Arabic transcriptions.",
        "id": 14326311
      },
      {
        "title": "Z-LaVI: Zero-Shot Language Solver Fueled by Visual Imagination",
        "text": "Large-scale pretrained language models have made significant advances in solving downstream language understanding tasks. However, they generally suffer from reporting bias, the phenomenon describing the lack of explicit commonsense knowledge in written text, e.g., \"an orange is orange\". To overcome this limitation, we develop a novel approach, Z-LaVI, to endow language models with visual imagination capabilities. Specifically, we leverage two complementary types of \"imaginations\": (i) recalling existing images through retrieval and (ii) synthesizing nonexistent images via text-toimage generation. Jointly exploiting the language inputs and the imagination, a pretrained vision-language model (e.g., CLIP) eventually composes a zero-shot solution to the original language tasks. Notably, fueling language models with imagination can effectively leverage visual knowledge to solve plain language tasks. In consequence, Z-LaVI consistently improves the zero-shot performance of existing language models across a diverse set of language tasks. 1",
        "id": 253098783
      },
      {
        "title": "An ERP-based Brain-Computer Interface for text entry using Rapid Serial Visual Presentation and Language Modeling",
        "text": "Event related potentials (ERP) corresponding to stimuli in electroencephalography (EEG) can be used to detect the intent of a person for brain computer interfaces (BCI). This paradigm is widely used to build letter-byletter text input systems using BCI. Nevertheless using a BCI-typewriter depending only on EEG responses will not be sufficiently accurate for single-trial operation in general, and existing systems utilize many-trial schemes to achieve accuracy at the cost of speed. Hence incorporation of a language model based prior or additional evidence is vital to improve accuracy and speed. In this demonstration we will present a BCI system for typing that integrates a stochastic language model with ERP classification to achieve speedups, via the rapid serial visual presentation (RSVP) paradigm.",
        "id": 7593224
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research that has introduced a dual-phase method for sentence paraphrasing?",
    "positive_ctxs": [
      {
        "title": "Keep the Primary, Rewrite the Secondary: A Two-Stage Approach for Paraphrase Generation",
        "text": "Paraphrase generation is an important and challenging NLG problem.In this work, we propose a new Identification-then-Aggregation (IA) framework to tackle this task. In the identification step, the input tokens are sorted into two groups by a novel Primary/Secondary Identification (PSI) algorithm. In the aggregation step, these groups are separately encoded, before being aggregated by a custom designed decoder, which autoregressively generates the paraphrased sentence. In extensive experiments on two benchmark datasets, we demonstrate that our model outperforms previous studies by a notable margin. We also show that the proposed approach can generate paraphrases in an interpretable and controllable way.",
        "id": 236477430
      }
    ],
    "negative_ctxs": [
      {
        "title": "DIALMED: A Dataset for Dialogue-based Medication Recommendation",
        "text": "Medication recommendation is a crucial task for intelligent healthcare systems.",
        "id": 247447718
      },
      {
        "title": "Developing Meeting Support Technologies: From Data to Demonstration (and Beyond)",
        "text": "In 2004, the AMI Consortium set out to collect a multimodal meeting corpus that would give us all the raw material we needed to demonstrate a whole range of meeting support technologies, most of which we knew we hadn't thought of yet. In this keynote, I will talk about how we designed the corpus to grow an interdisciplinary community that would collectively understand not just the technologies but how groups work, and then I will describe some of the novel applications we have built using the data and are currently showing to industrial end users.",
        "id": 24044885
      },
      {
        "title": "",
        "text": "",
        "id": 226262380
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "What difficulties do neural conversational models face, particularly concerning the decoder's ability to produce precise and fact-based replies?",
    "positive_ctxs": [
      {
        "title": "Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding",
        "text": "Dialogue systems powered by large pretrained language models exhibit an innate ability to deliver fluent and natural-sounding responses. Despite their impressive performance, these models are fitful and can often generate factually incorrect statements impeding their widespread adoption. In this paper, we focus on the task of improving faithfulness and reducing hallucination of neural dialogue systems to known facts supplied by a Knowledge Graph (KG). We propose NEU-RAL PATH HUNTER which follows a generatethen-refine strategy whereby a generated response is amended using the KG. NEURAL PATH HUNTER leverages a separate tokenlevel fact critic to identify plausible sources of hallucination followed by a refinement stage that retrieves correct entities by crafting a query signal that is propagated over a k-hop subgraph. We empirically validate our proposed approach on the OpenDialKG dataset (Moon et al., 2019) against a suite of metrics and report a relative improvement of faithfulness over dialogue responses by 20.35% based on FeQA (Durmus et al., 2020). The code is available at https://github.com/ nouhadziri/Neural-Path-Hunter.",
        "id": 233296059
      }
    ],
    "negative_ctxs": [
      {
        "title": "Swag: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference",
        "text": "Given a partial description like \"she opened the hood of the car,\" humans can reason about the situation and anticipate what might come next (\"then, she examined the engine\"). In this paper, we introduce the task of grounded commonsense inference, unifying natural language inference and commonsense reasoning.We present Swag, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations. To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data. To account for the aggressive adversarial filtering, we use state-of-theart language models to massively oversample a diverse set of potential counterfactuals. Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88%), various competitive models struggle on our task. We provide comprehensive analysis that indicates significant opportunities for future research.",
        "id": 52019251
      },
      {
        "title": "",
        "text": "",
        "id": 208281781
      },
      {
        "title": "De-Conflated Semantic Representations",
        "text": "One major deficiency of most semantic representation techniques is that they usually model a word type as a single point in the semantic space, hence conflating all the meanings that the word can have. Addressing this issue by learning distinct representations for individual meanings of words has been the subject of several research studies in the past few years. However, the generated sense representations are either not linked to any sense inventory or are unreliable for infrequent word senses. We propose a technique that tackles these problems by de-conflating the representations of words based on the deep knowledge it derives from a semantic network. Our approach provides multiple advantages in comparison to the past work, including its high coverage and the ability to generate accurate representations even for infrequent word senses. We carry out evaluations on six datasets across two semantic similarity tasks and report state-of-the-art results on most of them.",
        "id": 16173223
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "manual_acl",
    "question": "Is there any paper exploring real speakers and thus performing multimodal emotion recognition task?",
    "positive_ctxs": [
      {
        "title": "A Facial Expression-Aware Multimodal Multi-task Learning Framework for Emotion Recognition in Multi-party Conversations",
        "text": "Multimodal Emotion Recognition inMultiparty Conversations (MERMC)has recently attracted considerable attention. Due to the complexity of visual scenes in multi-party conversations, most previous MERMC studies mainly focus on text and audio modalities while ignoring visual information. Recently, several works proposed to extract face sequences as visual features and have shown the importance of visual information in MERMC. However, given an utterance, the face sequence extracted by previous methods may contain multiple people's faces, which will inevitably introduce noise to the emotion prediction of the real speaker. To tackle this issue, we propose a two-stage framework named Facial expressionaware Multimodal Multi-Task learning (Fa-cialMMT). Specifically, a pipeline method is first designed to extract the face sequence of the real speaker of each utterance, which consists of multimodal face recognition, unsupervised face clustering, and face matching. With the extracted face sequences, we propose a multimodal facial expression-aware emotion recognition model, which leverages the frame-level facial emotion distributions to help improve utterance-level emotion recognition based on multi-task learning. Experiments demonstrate the effectiveness of the proposed FacialMMT framework on the benchmark MELD dataset. The source code is publicly released at https: //github.com/NUSTM/FacialMMT.",
        "id": 259370790
      }
    ],
    "negative_ctxs": [
      {
        "title": "New features in Spoken Language Search Hawk (SpLaSH): Query Language and Query Sequence",
        "text": "In this work we present further development of the SpLaSH (Spoken Language Search Hawk) project. SpLaSH implements a data model for annotated speech corpora integrated with textual markup (i.e. POS tagging, syntax, pragmatics) including a toolkit used to perform complex queries across speech and text labels. The integration of time aligned annotations (TMA), represented making use of Annotation Graphs, with text aligned ones (TXA), stored in generic XML files, are provided by a data structure, the Connector Frame, acting as table-look-up linking temporal data to words in the text. SpLaSH imposes a very limited number of constraints to the data model design, allowing the integration of annotations developed separately within the same dataset and without any relative dependency. It also provides a GUI allowing three types of queries: simple query on TXA or TMA structures, sequence query on TMA structure and cross query on both TXA and TMA integrated structures. In this work new SpLaSH features will be presented: SpLaSH Query Language (SpLaSHQL) and Query Sequence.",
        "id": 2306140
      },
      {
        "title": "",
        "text": "",
        "id": 219304499
      },
      {
        "title": "AmritaCEN at SemEval-2016 Task 11: Complex Word Identification using Word Embedding",
        "text": "Complex word identification task focuses on identifying the difficult word from English sentence for a Non-Native speakers. Non-Native speakers are those who don't have English as their native language. It is a subtask for lexical simplification. We have experimented with word embedding features, orthographic word features, similarity features and POS tag features which improves the performance of the classification. In addition to the SemEval 2016 results we have evaluated the training data by varying the vector dimension size and obtained the best possible size for producing better performance. The SVM learning algorithm will attains constant and maximum accuracy through linear classifier. We achieve a G-score of 0.43 and 0.54 on computing complex words for two systems.",
        "id": 219309661
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "manual_acl",
    "question": "What is the performance of large language models in text summarization under reference-based and reference-free human evaluations?",
    "positive_ctxs": [
      {
        "title": "",
        "text": "Human evaluation is the foundation upon which the evaluation of both summarization systems and automatic metrics rests. However, existing human evaluation studies for summarization either exhibit a low inter-annotator agreement or have insufficient scale, and an in-depth analysis of human evaluation is lacking. Therefore, we address the shortcomings of existing summarization evaluation along the following axes: (1) We propose a modified summarization salience protocol, Atomic Content Units (ACUs), which is based on fine-grained semantic units and allows for a high interannotator agreement. (2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large human evaluation dataset consisting of 22,000 summary-level annotations over 28 top-performing systems on three datasets.(3) We conduct a comparative study of four human evaluation protocols, underscoring potential confounding factors in evaluation setups.(4) We evaluate 50 automatic metrics and their variants using the collected human annotations across evaluation protocols and demonstrate how our benchmark leads to more statistically stable and significant results. The metrics we benchmarked include recent methods based on large language models (LLMs), GPTScore and G-Eval. Furthermore, our findings have important implications for evaluating LLMs, as we show that LLMs adjusted by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation, which is affected by the annotators' prior, input-agnostic preferences, calling for more robust, targeted evaluation methods.Oscar collided with Arsenal goalkeeper David Ospina in the 16th minute of the London derby. The Brazilian was substituted at half-time and Jose Mourinho said he suffered `possible concussion'. Oscar was knocked back by the goalkeeper but Michael Oliver didn't award Chelsea a penalty.System SummaryChelsea weren't awarded a penalty. David Ospina clashed with Oscar. David Ospina clattered Oscar. David Ospina plays for Arsenal. David Ospina is a goalkeeper. The clash occurred inside the box. Oscar is Brazilian. Oscar was taken off at half time. Didier Drogba replaced Oscar.System SummaryOscar collided with Arsenal goalkeeper David Ospina in the 16th minute of the London derby. The Brazilian was substituted at half-time and Jose Mourinho said he suffered 'possible concussion'. Oscar was knocked back by the goalkeeper but Michael Oliver didn't award Chelsea a penalty.Chelsea weren't awarded a penalty for David Ospina's clash with Oscar. Arsenal goalkeeper clattered Oscar inside the box. Brazilian was taken off at half-time, with Didier Drogba replacing him.Reference SummaryOscar collided with Arsenal goalkeeper David Ospina in the 16th minute of the London derby. The Brazilian was substituted at half-time and Jose Mourinho said he suffered `possible concussion'. Oscar was knocked back by the goalkeeper but Michael Oliver didn't award Chelsea a penalty.System Summary Atomic Content Units (ACUs)❌Chelsea weren't awarded a penalty for David Ospina's clash with Oscar. Arsenal goalkeeper clattered Oscar inside the box. Brazilian was taken off at half-time, with DidierDrogba replacing him. Chelsea weren't awarded a penalty. David Ospina clashed with Oscar. David Ospina clattered Oscar. David Ospina plays for Arsenal. David Ospina is a goalkeeper. The clash occurre Oscar is B Oscar was taken Didier Drogba r Automic Content Units (ACUs) ✔ ✔ ✔ ✔ ✔ ACU Writing ACU Matching Reference Summary Chelsea weren't awarded a penalty. David Ospina clashed with Oscar. David Ospina clattered Oscar. David Ospina plays for Arsenal. David Ospina is a goalkeeper. The clash occurred inside the box. Oscar is Brazilian. Oscar was taken off at half time. Didier Drogba replaced Oscar. Oscar collided with Arsenal goalkeeper David Ospina in the 16th minute of the London derby. The Brazilian was substituted at half-time and Jose Mourinho said he suffered 'possible concussion'. Oscar was knocked back by the goalkeeper but Michael Oliver didn't award Chelsea a penalty.",
        "id": 254685611
      }
    ],
    "negative_ctxs": [
      {
        "title": "Linguistics isn't always the answer: Word comparison in computational linguisticsL ars Borin",
        "text": "S trin g sim ila rity m e tric s a re im p o rta n t to o ls in c o m p u ta tio n a l lin g u istic s, e x te n siv e ly u s e d e.g . f o r c o m p a rin g w o rd s in a v a rie ty o f p ro b le m d o m a in s. T h is p a p e r e x a m in e s th e so m e tim e s m a d e a s s u m p tio n th a t th e p e rfo rm a n c e o f su c h w o rd c o m p a ris o n m e th o d s w o u ld b e n e fit fro m th e u se o f lin g u istic , viz. p h o n o lo g ic a l a n d m o rp h o lo g ic a l, k n o w le d g e . O n e lin g u istic a lly n aiv e m e th o d a n d o n e in c o rp o ra tin g a m o d e ra te a m o u n t o f lin g u istic s o p h is tic a tio n w e re co m p a re d o n a b ilin g u a l a n d a m o n o lin g u a l w o rd c o m p a riso n ta sk fo r a ra n g e o f la n g u a g e s. T h e re su lts s h o w th ep e rfo rm a n c e , m e a su re d as re c a ll a n d p re c isio n , o f th e lin g u is tic a lly n a iv e m e th o d to b e su p e rio r i n a ll cases.",
        "id": 107696
      },
      {
        "title": "A two-stage statistical word segmentation system for Chinese",
        "text": "In this paper we present a two-stage statistical word segmentation system for Chinese based on word bigram and wordformation models. This system was evaluated on Peking University corpora at the First International Chinese Word Segmentation Bakeoff. We also give results and discussions on this evaluation.",
        "id": 13113753
      },
      {
        "title": "Rigorous dimensionality reduction through linguistically motivated feature selection for text categorization",
        "text": "This paper introduces a new linguistically motivated feature selection technique for text categorization based on morphological analysis.It will be shown that compound parts that are constituents of many (different) noun compounds throughout a text are good and general indicators of this text's content; they are more general in meaning than the compounds they are part of, but nevertheless have good domain-specificity so that they distinguish between categories. Experiments with categorizing German newspaper texts show that this feature selection technique is superior to other popular ones, especially when dimensionality is reduced substantially. Additionally, a new compound splitting method based on compact patricia tries is introduced.",
        "id": 10662251
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_acl",
    "question": "What are some recent advancements in training systems to parse complex multi-hop questions into a sequence of simpler query steps for improved question answering?",
    "positive_ctxs": [
      {
        "title": "BREAK It Down: A Question Understanding Benchmark",
        "text": "Understanding natural language questions entails the ability to break down a question into the requisite steps for computing its answer. In this work, we introduce a Question Decomposition Meaning Representation (QDMR) for questions. QDMR constitutes the ordered list of steps, expressed through natural language, that are necessary for answering a question. We develop a crowdsourcing pipeline, showing that quality QDMRs can be annotated at scale, and release the BREAK dataset, containing over 83K pairs of questions and their QDMRs. We demonstrate the utility of QDMR by showing that (a) it can be used to improve open-domain question answering on the HOTPOTQA dataset, (b) it can be deterministically converted to a pseudo-SQL formal language, which can alleviate annotation in semantic parsing applications. Last, we use BREAK to train a sequence-to-sequence model with copying that parses questions into QDMR structures, and show that it substantially outperforms several natural baselines.",
        "id": 211003735
      }
    ],
    "negative_ctxs": [
      {
        "title": "Exploring Miscommunication and Collaborative Behaviour in Hu- man-Robot Interaction",
        "text": "This paper presents the first step in designing a speech-enabled robot that is capable of natural management of miscommunication. It describes the methods and results of two WOz studies, in which dyads of naïve participants interacted in a collaborative task. The first WOz study explored human miscommunication management. The second study investigated how shared visual space and monitoring shape the processes of feedback and communication in task-oriented interactions. The results provide insights for the development of human-inspired and robust natural language interfaces in robots.",
        "id": 11259766
      },
      {
        "title": "Empathy Identification Systems are not Accurately Accounting for Context",
        "text": "Understanding empathy in text dialogue data is a difficult, yet critical, skill for effective human-machine interaction. In this work, we ask whether systems are making meaningful progress on this challenge. We consider a simple model that checks if an input utterance is similar to a small set of empathetic examples. Crucially, the model does not look at what the utterance is a response to, i.e., the dialogue context. This model performs comparably to prior work on standard benchmarks and even outperforms state-of-the-art models for empathetic rationale extraction by 16.7 points on T-F1 and 4.3 on IOU-F1. This indicates that current systems rely on the surface form of the response, rather than whether it is suitable in context. To confirm this, we create examples with dialogue contexts that change the interpretation of the response and show that current systems continue to label utterances as empathetic. We discuss the implications of our findings, including improvements for empathetic benchmarks and how our model can be an informative baseline.",
        "id": 258378294
      },
      {
        "title": "An Open Web Platform for Rule-Based Speech-to-Sign Translation",
        "text": "We present an open web platform for developing, compiling, and running rulebased speech to sign language translation applications. Speech recognition is performed using the Nuance Recognizer 10.2 toolkit, and signed output, including both manual and non-manual components, is rendered using the JASigning avatar system. The platform is designed to make the component technologies readily accessible to sign language experts who are not necessarily computer scientists. Translation grammars are written in a version of Synchronous Context-Free Grammar adapted to the peculiarities of sign language. All processing is carried out on a remote server, with content uploaded and accessed through a web interface. Initial experiences show that simple translation grammars can be implemented on a time-scale of a few hours to a few days and produce signed output readily comprehensible to Deaf informants. Overall, the platform drastically lowers the barrier to entry for researchers interested in building applications that generate high-quality signed language.",
        "id": 15635164
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_acl",
    "question": "I am looking to understand more about sequence-to-sequence pre-training and its applications in natural language tasks. Can you suggest a significant paper that describes the denoising process for such models?",
    "positive_ctxs": [
      {
        "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
        "text": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance. 1",
        "id": 204960716
      }
    ],
    "negative_ctxs": [
      {
        "title": "A Geometric View on Bilingual Lexicon Extraction from Comparable Corpora",
        "text": "We present a geometric view on bilingual lexicon extraction from comparable corpora, which allows to re-interpret the methods proposed so far and identify unresolved problems. This motivates three new methods that aim at solving these problems. Empirical evaluation shows the strengths and weaknesses of these methods, as well as a significant gain in the accuracy of extracted lexicons.",
        "id": 459519
      },
      {
        "title": "UC3M-PUCPR at SemEval-2022 Task 11: An Ensemble Method of Transformer-based Models for Complex Named Entity Recognition",
        "text": "This study introduces the system submitted to the SemEval 2022 Task 11: MultiCoNER (Multilingual Complex Named Entity Recognition) by the UC3M-PUCPR team. We proposed an ensemble of transformer-based models for entity recognition in cross-domain texts. Our deep learning method benefits from the transformer architecture, which adopts the attention mechanism to handle the long-range dependencies of the input text. Also, the ensemble approach for named entity recognition (NER) improved the results over baselines based on individual models on two of the three tracks we participated in. The ensemble model for the codemixed task achieves an overall performance of 76.36% F1-score, a 2.85 percentage point increase upon our individually best model for this task, XLM-RoBERTa-large (73.51%), outperforming the baseline provided for the shared task by 18.26 points. Our preliminary results suggest that contextualized language models ensembles can, even if modestly, improve the results in extracting information from unstructured data.",
        "id": 250391088
      },
      {
        "title": "",
        "text": "",
        "id": 232021830
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that explores generating synthetic labels for sentences within a vast text collection to pre-train models for few-shot learning applications?",
    "positive_ctxs": [
      {
        "title": "PPT: Pre-trained Prompt Tuning for Few-shot Learning",
        "text": "Prompts for pre-trained language models (PLMs) have shown remarkable performance by bridging the gap between pre-training tasks and various downstream tasks. Among these methods, prompt tuning, which freezes PLMs and only tunes soft prompts, provides an efficient and effective solution for adapting largescale PLMs to downstream tasks. However, prompt tuning is yet to be fully explored. In our pilot experiments, we find that prompt tuning performs comparably with conventional full-model tuning when downstream data are sufficient, whereas it is much worse under fewshot learning settings, which may hinder the application of prompt tuning. We attribute this low performance to the manner of initializing soft prompts. Therefore, in this work, we propose to pre-train prompts by adding soft prompts into the pre-training stage to obtain a better initialization. We name this Pretrained Prompt Tuning framework \"PPT\". To ensure the generalization of PPT, we formulate similar classification tasks into a unified task form and pre-train soft prompts for this unified task. Extensive experiments show that tuning pre-trained prompts for downstream tasks can reach or even outperform full-model fine-tuning under both full-data and few-shot settings. Our approach is effective and efficient for using large-scale PLMs in practice. The code is publicly available at https:// github.com/thu-coai/PPT.",
        "id": 237452236
      }
    ],
    "negative_ctxs": [
      {
        "title": "Finding Relevant Concepts for Unknown Terms Using a Web-based Approach",
        "text": "Previous research on automatic thesaurus construction most focused on extracting relevant terms for each term of concern from a small-scale and domain-specific corpus. This study emphasizes on utilizing the Web as the rich and dynamic corpus source for term association estimation. In addition to extracting relevant terms, we are interested in finding concept-level information for each term of concern. For a single term, our idea is that to send it into Web search engines to retrieve its relevant documents and we propose a Greedy-EMbased document clustering algorithm to cluster them and determine an appropriate number of relevant concepts for the term. Then the keywords with the highest weighted log likelihood ratio in each cluster are treated as the label(s) of the associated concept cluster for the term of concern. With some initial experiments, the proposed approach has been shown its potential in finding relevant concepts for unknown terms.",
        "id": 18028927
      },
      {
        "title": "Learning to Generate Word-and Phrase-Embeddings for Efficient Phrase-Based Neural Machine Translation",
        "text": "Neural machine translation (NMT) often fails in one-to-many translation, e.g., in the translation of multi-word expressions, compounds, and collocations. To improve the translation of phrases, phrase-based NMT systems have been proposed; these typically combine wordbased NMT with external phrase dictionaries or with phrase tables from phrase-based statistical MT systems. These solutions introduce a significant overhead of additional resources and computational costs. In this paper, we introduce a phrase-based NMT model built upon continuous-output NMT, in which the decoder generates embeddings of words or phrases. The model uses a fertility module, which guides the decoder to generate embeddings of sequences of varying lengths. We show that our model learns to translate phrases better, performing on par with state of the art phrase-based NMT. Since our model does not resort to softmax computation over a huge vocabulary of phrases, its training time is about 112x faster than the baseline.",
        "id": 207914581
      },
      {
        "title": "Entity Contrastive Learning in a Large-Scale Virtual Assistant System",
        "text": "Conversational agents are typically made up of domain (DC) and intent classifiers (IC) that identify the general subject an utterance belongs to and the specific action a user wishes to achieve. In addition, named entity recognition (NER) performs per token labeling to identify specific entities of interest in a spoken utterance. We investigate improving joint IC and NER models using entity contrastive learning that attempts to cluster similar entities together in a learned representation space. We compare a full virtual assistant system trained using entity contrastive learning to a baseline system that does not use contrastive learning. We present both offline results, using retrospective test sets, as well as online results from an A/B test that compared the two systems. In both the offline and online settings, entity contrastive training improved overall performance against baseline systems. Furthermore, we provide a detailed analysis of learned entity embeddings, including both qualitative analysis via dimensionalityreduced visualizations and quantitative analysis by computing alignment and uniformity metrics. We show that entity contrastive learning improves alignment metrics and produces wellformed embedding clusters in representation space.",
        "id": 259370605
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_acl",
    "question": "Can you direct me to research that explores methods for transforming multi-hop questions into single-hop sub-questions to leverage existing single-hop answer models?",
    "positive_ctxs": [
      {
        "title": "Multi-hop Reading Comprehension through Question Decomposition and Rescoring",
        "text": "Multi-hop Reading Comprehension (RC) requires reasoning and aggregation across several paragraphs. We propose a system for multi-hop RC that decomposes a compositional question into simpler sub-questions that can be answered by off-the-shelf single-hop RC models. Since annotations for such decomposition are expensive, we recast subquestion generation as a span prediction problem and show that our method, trained using only 400 labeled examples, generates sub-questions that are as effective as humanauthored sub-questions. We also introduce a new global rescoring approach that considers each decomposition (i.e. the sub-questions and their answers) to select the best final answer, greatly improving overall performance. Our experiments on HOTPOTQA show that this approach achieves the state-of-the-art results, while providing explainable evidence for its decision making in the form of sub-questions. . 2018. Neural models for reasoning over multiple mentions using coreference. In NAACL.Albert Gatt and Emiel Krahmer. 2018. Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. Artificial Intelligence Research.",
        "id": 174801080
      }
    ],
    "negative_ctxs": [
      {
        "title": "MONODISTILL: LEARNING SPATIAL FEATURES FOR MONOCULAR 3D OBJECT DETECTION",
        "text": "3D object detection is a fundamental and challenging task for 3D scene understanding, and the monocular-based methods can serve as an economical alternative to the stereo-based or LiDAR-based methods. However, accurately detecting objects in the 3D space from a single image is extremely difficult due to the lack of spatial cues. To mitigate this issue, we propose a simple and effective scheme to introduce the spatial information from LiDAR signals to the monocular 3D detectors, without introducing any extra cost in the inference phase. In particular, we first project the LiDAR signals into the image plane and align them with the RGB images. After that, we use the resulting data to train a 3D detector (LiDAR Net) with the same architecture as the baseline model. Finally, this LiDAR Net can serve as the teacher to transfer the learned knowledge to the baseline model. Experimental results show that the proposed method can significantly boost the performance of the baseline model and ranks the 1 st place among all monocularbased methods on the KITTI benchmark. Besides, extensive ablation studies are conducted, which further prove the effectiveness of each part of our designs and illustrate what the baseline model has learned from the LiDAR Net. Our code will be released at https://github.com/monster-ghost/MonoDistill.Published as a conference paper at ICLR 2022RGB-based method Depth-based method OursRGB Data Depth Data 3D Box Pseudo-LiDAR T (d) 3D Box RGB Data Depth Data (a) RGB Data 3D Box D D DE DE D RGB Data 3D Box Depth Distribution Image Feature B DE D RGB Data LiDAR Depth 3D Box 3D Box Feature Guidance Response Guidance (e) D D Inference Training T Data Transformation Outer Product DE D Depth Estimator Detector B Backbone",
        "id": 246285537
      },
      {
        "title": "",
        "text": "",
        "id": 229365786
      },
      {
        "title": "Using Machine Learning for System-Internal Evaluation of Transferred Linguistic Representations",
        "text": "We present an automated, system-internal evaluation technique for linguistic representations in a large-scale, multilingual MT system. We use machine-learned classifiers to recognize the differences between linguistic representations generated from transfer in an MT context from representations that are produced by \"native\" analysis of the target language. In the MT scenario, convergence of the two is the desired result. Holding the feature set and the learning algorithm constant, the accuracy of the classifiers provides a measure of the overall difference between the two sets of linguistic representations: classifiers with higher accuracy correspond to more pronounced differences between representations. More importantly, the classifiers yield the basis for error-analysis by providing a ranking of the importance of linguistic features. The more salient a linguistic criterion is in discriminating transferred representations from \"native\" representations, the more work will be needed in order to get closer to the goal of producing native-like MT. We present results from using this approach on the Microsoft MT system and discuss its advantages and possible extensions.",
        "id": 11540517
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research articles that explore the application of contrastive learning methods to improve sentence embedding efficacy in natural language processing?",
    "positive_ctxs": [
      {
        "title": "ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer",
        "text": "Learning high-quality sentence representations benefits a wide range of natural language processing tasks. Though BERT-based pretrained language models achieve high performance on many downstream tasks, the native derived sentence representations are proved to be collapsed and thus produce a poor performance on the semantic textual similarity (STS) tasks. In this paper, we present ConSERT, a Contrastive Framework for Self-Supervised SEntence Representation Transfer, that adopts contrastive learning to fine-tune BERT in an unsupervised and effective way. By making use of unlabeled texts, ConSERT solves the collapse issue of BERT-derived sentence representations and make them more applicable for downstream tasks. Experiments on STS datasets demonstrate that ConSERT achieves an 8% relative improvement over the previous state-of-the-art, even comparable to the supervised SBERT-NLI. And when further incorporating NLI supervision, we achieve new stateof-the-art performance on STS tasks. Moreover, ConSERT obtains comparable results with only 1000 samples available, showing its robustness in data scarcity scenarios.",
        "id": 235187266
      }
    ],
    "negative_ctxs": [
      {
        "title": "Detection of Japanese Homophone Errors by a Decision List Including a Written Word as a Default Evidence",
        "text": "In this paper, we propose a practical method to detect Japanese homophone errors in Japanese texts.It is very important to detect homophone errors in Japanese revision systems because Japanese texts suffer from homophone errors frequently. In order to detect homophone errors, we have only to solve the homophone problem. We can use the decision list to do it because the homophone problem is equivalent to the word sense disambiguation problem. However, the homophone problem is different from the word sense disambiguation problem because the former can use the written word but the latter cannot. In this paper, we incorporate the written word into the original decision list by obtaining the identifying strength of the written word. The improved decision list can raise the F-measure of error detection.",
        "id": 1467552
      },
      {
        "title": "Unsupervised Discovery of Implicit Gender Bias",
        "text": "Despite their prevalence in society, social biases are difficult to define and identify, primarily because human judgements in this domain can be unreliable. Therefore, we take an unsupervised approach to identifying gender bias at a comment or sentence level, and present a model that can surface text likely to contain bias. The main challenge in this approach is forcing the model to focus on signs of implicit bias, rather than other artifacts in the data. Thus, the core of our methodology relies on reducing the influence of confounds through propensity score matching and adversarial learning. Our analysis shows how biased comments directed towards female politicians contain mixed criticisms and references to their spouses, while comments directed towards other female public figures focus on appearance and sexualization. Ultimately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements.",
        "id": 215814487
      },
      {
        "title": "Reducing Inference Time of Biomedical NER Tasks using Multi-Task Learning",
        "text": "Recently, fine-tuned transformer-based models (e.g., PubMedBERT, BioBERT) have shown the state-of-the-art performance of several BioNLP tasks, such as Named Entity Recognition (NER). However, transformer-based models are complex, have millions of parameters, and are relatively slow during inference. In this paper, we address the time complexity limitations of the BioNLP transformer models. In particular, we propose a Multi-Task Learning based framework for jointly learning three different biomedical NER tasks. Our experiments show a reduction in inference time by a factor of three without any reduction in prediction accuracy.",
        "id": 257767707
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you direct me to studies investigating the enhancement of bi-encoder text matching performance through the application of knowledge distillation methods?",
    "positive_ctxs": [
      {
        "title": "VIRT: Improving Representation-based Text Matching via Virtual Interaction",
        "text": "Text matching is a fundamental research problem in natural language understanding. Interaction-based approaches treat the text pair as a single sequence and encode it through cross encoders, while representation-based models encode the text pair independently with siamese or dual encoders. Interactionbased models require dense computations and thus are impractical in real-world applications. Representation-based models have become the mainstream paradigm for efficient text matching. However, these models suffer from severe performance degradation due to the lack of interactions between the pair of texts. To remedy this, we propose a Virtual InteRacTion mechanism (VIRT) for improving representation-based text matching while maintaining its efficiency. In particular, we introduce an interactive knowledge distillation module that is only applied during training. It enables deep interaction between texts by effectively transferring knowledge from the interaction-based model. A light interaction strategy is designed to fully leverage the learned interactive knowledge. Experimental results on six text matching benchmarks demonstrate the superior performance of our method over several state-of-the-art representationbased models. We further show that VIRT can be integrated into existing methods as plugins to lift their performances.",
        "id": 256461055
      },
      {
        "title": "VIRT: Improving Representation-based Text Matching via Virtual Interaction",
        "text": "Text matching is a fundamental research problem in natural language understanding. Interaction-based approaches treat the text pair as a single sequence and encode it through cross encoders, while representation-based models encode the text pair independently with siamese or dual encoders. Interactionbased models require dense computations and thus are impractical in real-world applications. Representation-based models have become the mainstream paradigm for efficient text matching. However, these models suffer from severe performance degradation due to the lack of interactions between the pair of texts. To remedy this, we propose a Virtual InteRacTion mechanism (VIRT) for improving representation-based text matching while maintaining its efficiency. In particular, we introduce an interactive knowledge distillation module that is only applied during training. It enables deep interaction between texts by effectively transferring knowledge from the interaction-based model. A light interaction strategy is designed to fully leverage the learned interactive knowledge. Experimental results on six text matching benchmarks demonstrate the superior performance of our method over several state-of-the-art representationbased models. We further show that VIRT can be integrated into existing methods as plugins to lift their performances.",
        "id": 244954670
      }
    ],
    "negative_ctxs": [
      {
        "title": "Missing Counter-Evidence Renders NLP Fact-Checking Unrealistic for Misinformation",
        "text": "Misinformation emerges in times of uncertainty when credible information is limited. This is challenging for NLP-based fact-checking as it relies on counter-evidence, which may not yet be available. Despite increasing interest in automatic fact-checking, it is still unclear if automated approaches can realistically refute harmful real-world misinformation. Here, we contrast and compare NLP fact-checking with how professional fact-checkers combat misinformation in the absence of counter-evidence. In our analysis, we show that, by design, existing NLP task definitions for fact-checking cannot refute misinformation as professional fact-checkers do for the majority of claims. We then define two requirements that the evidence in datasets must fulfill for realistic factchecking: It must be (1) sufficient to refute the claim and (2) not leaked from existing fact-checking articles. We survey existing factchecking datasets and find that all of them fail to satisfy both criteria. Finally, we perform experiments to demonstrate that models trained on a large-scale fact-checking dataset rely on leaked evidence, which makes them unsuitable in real-world scenarios. Taken together, we show that current NLP fact-checking cannot realistically combat real-world misinformation because it depends on unrealistic assumptions about counter-evidence in the data 1 .",
        "id": 253107194
      },
      {
        "title": "Cross-media Cross-genre Information Ranking Multi-media Information Networks",
        "text": "Current web technology has brought us a scenario that information about a certain topic is widely dispersed in data from different domains and data modalities, such as texts and images from news and social media. Automatic extraction of the most informative and important multimedia summary (e.g. a ranked list of inter-connected texts and images) from massive amounts of cross-media and cross-genre data can significantly save users' time and effort that is consumed in browsing. In this paper, we propose a novel method to address this new task based on automatically constructed Multi-media Information Networks (MiNets) by incorporating cross-genre knowledge and inferring implicit similarity across texts and images. The facts from MiNets are exploited in a novel random walk-based algorithm to iteratively propagate ranking scores across multiple data modalities. Experimental results demonstrated the effectiveness of our MiNets-based approach and the power of cross-media cross-genre inference.",
        "id": 11029275
      },
      {
        "title": "Approximate Searching for Distributional Similarity",
        "text": "Distributional similarity requires large volumes of data to accurately represent infrequent words. However, the nearestneighbour approach to finding synonyms suffers from poor scalability. The Spatial Approximation Sample Hierarchy (SASH), proposed by Houle (2003b), is a data structure for approximate nearestneighbour queries that balances the efficiency/approximation trade-off. We have intergrated this into an existing distributional similarity system, tripling efficiency with a minor accuracy penalty.",
        "id": 158338
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_acl",
    "question": "Can you point to studies or tasks focused on detecting patronizing and condescending language, particularly in contexts involving vulnerable communities?",
    "positive_ctxs": [
      {
        "title": "SemEval-2022 Task 4: Patronizing and Condescending Language Detection",
        "text": "This paper presents an overview of Task 4 at SemEval-2022, which was focused on detecting Patronizing and Condescending Language (PCL) towards vulnerable communities. Two sub-tasks were considered: a binary classification task, where participants needed to classify a given paragraph as containing PCL or not, and a multi-label classification task, where participants needed to identify which types of PCL are present (if any). The task attracted 77 teams. We provide an overview of how the task was organized, discuss the techniques that were employed by the different participants, and summarize the main resulting insights about PCL detection and categorization.",
        "id": 250390607
      }
    ],
    "negative_ctxs": [
      {
        "title": "A Deep Transfer Learning Method for Cross-Lingual Natural Language Inference",
        "text": "Natural Language Inference (NLI), also known as Recognizing Textual Entailment (RTE), has been one of the central tasks in Artificial Intelligence (AI) and Natural Language Processing (NLP). RTE between the two pieces of texts is a crucial problem, and it adds further challenges when involving two different languages, i.e., in the cross-lingual scenario. This paper proposes an effective transfer learning approach for cross-lingual NLI. We perform experiments on English-Hindi language pairs in the cross-lingual setting to find out that our novel loss formulation could enhance the performance of the baseline model by up to 2%. To assess the effectiveness of our method further, we perform additional experiments on every possible language pair using four European languages, namely French, German, Bulgarian, and Turkish, on top of XNLI dataset. Evaluation results yield up to 10% performance improvement over the respective baseline models, in some cases surpassing the state-of-the-art (SOTA). It is also to be noted that our proposed model has 110M parameters which is much lesser than the SOTA model having 220M parameters. Finally, we argue that our transfer learning-based loss objective is model agnostic and thus can be used with other deep learning-based architectures for cross-lingual NLI.",
        "id": 250163947
      },
      {
        "title": "Determining the Sentiment of Opinions",
        "text": "Identifying sentiments (the affective parts of opinions) is a challenging problem. We present a system that, given a topic, automatically finds the people who hold opinions about that topic and the sentiment of each opinion. The system contains a module for determining word sentiment and another for combining sentiments within a sentence. We experiment with various models of classifying and combining sentiment at word and sentence levels, with promising results.",
        "id": 5690545
      },
      {
        "title": "Constructional Intensifying Adjectives in Italian",
        "text": "Grading is a primary cognitive operation that has an important expressive function.",
        "id": 2628500
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_acl",
    "question": "Where might I find a dataset annotated specifically for patronizing and condescending language to use in computational linguistics research?",
    "positive_ctxs": [
      {
        "title": "Don't Patronize Me! An Annotated Dataset with Patronizing and Condescending Language towards Vulnerable Communities",
        "text": "In this paper, we introduce a new annotated dataset which is aimed at supporting the development of NLP models to identify and categorize language that is patronizing or condescending towards vulnerable communities (e.g. refugees, homeless people, poor families). While the prevalence of such language in the general media has long been shown to have harmful effects, it differs from other types of harmful language, in that it is generally used unconsciously and with good intentions. We furthermore believe that the often subtle nature of patronizing and condescending language (PCL) presents an interesting technical challenge for the NLP community. Our analysis of the proposed dataset shows that identifying PCL is hard for standard NLP models, with language models such as BERT achieving the best results.",
        "id": 226976077
      }
    ],
    "negative_ctxs": [
      {
        "title": "Integrating on-line MT services into monolingual web-sites for dissemination purposes: an evaluation perspective",
        "text": "On-line machine translation (MT) services are becoming increasingly popular among Internet users. In particular, over the last few years there has been a dramatic increase in the number of monolingual web-sites that rely on Internet-based MT systems to disseminate their contents in a variety of languages, which seems to be one of the most interesting areas in the current use of MT technology. This paper is based on preliminary observations of these recent developments and reports on how on-line MT services are actually integrated into a sample of monolingual web-sites only available in English, attempting to evaluate the success of the strategies used to incorporate webbased MT technology for dissemination purposes. The discussion suggests in conclusion that the overall lack of a user-oriented approach and the limited consideration of issues of user-friendliness make the integration of on-line MT into the sample of monolingual web-sites largely ineffective.",
        "id": 2980252
      },
      {
        "title": "Learning the Optimal use of Dependency-parsing Information for Finding Translations with Comparable Corpora",
        "text": "Using comparable corpora to find new word translations is a promising approach for extending bilingual dictionaries (semi-) automatically. The basic idea is based on the assumption that similar words have similar contexts across languages. The context of a word is often summarized by using the bag-of-words in the sentence, or by using the words which are in a certain dependency position, e.g. the predecessors and successors. These different context positions are then combined into one context vector and compared across languages. However, previous research makes the (implicit) assumption that these different context positions should be weighted as equally important. Furthermore, only the same context positions are compared with each other, for example the successor position in Spanish is compared with the successor position in English. However, this is not necessarily always appropriate for languages like Japanese and English. To overcome these limitations, we suggest to perform a linear transformation of the context vectors, which is defined by a matrix. We define the optimal transformation matrix by using a Bayesian probabilistic model, and show that it is feasible to find an approximate solution using Markov chain Monte Carlo methods. Our experiments demonstrate that our proposed method constantly improves translation accuracy.",
        "id": 17019457
      },
      {
        "title": "",
        "text": "",
        "id": 192986113
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "manual_acl",
    "question": "Which paper did a comprehensive survey of the code large language model (code LLMs)?",
    "positive_ctxs": [
      {
        "title": "Large Language Models Meet NL2Code: A Survey",
        "text": "The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics. We provide an intuitive comparison of all existing models on the Hu-manEval benchmark. Through in-depth observation and analysis, we provide some insights and conclude that the key factors contributing to the success of large language models for NL2Code are \"Large Size, Premium Data, Expert Tuning\". In addition, we discuss challenges and opportunities regarding the gap between models and humans. We also create a website https://nl2code.github.io to track the latest progress through crowdsourcing. To the best of our knowledge, this is the first survey of large language models for NL2Code, and we believe it will contribute to the ongoing development of the field.",
        "id": 258557362
      }
    ],
    "negative_ctxs": [
      {
        "title": "Automated Essay Scoring Based on Finite State Transducer: towards ASR Transcription of Oral English Speech",
        "text": "Conventional Automated Essay Scoring (AES) measures may cause severe problems when directly applied in scoring Automatic Speech Recognition (ASR) transcription as they are error sensitive and unsuitable for the characteristic of ASR transcription. Therefore, we introduce a framework of Finite State Transducer (FST) to avoid the shortcomings. Compared with the Latent Semantic Analysis with Support Vector Regression (LSA-SVR) method (stands for the conventional measures), our FST method shows better performance especially towards the ASR transcription. In addition, we apply the synonyms similarity to expand the FST model. The final scoring performance reaches an acceptable level of 0.80 which is only 0.07 lower than the correlation (0.87) between human raters.",
        "id": 16336633
      },
      {
        "title": "Bunsetsu Identification Using Category-Exclusive Rules",
        "text": "C()mmunications Research Laboratory, Ministry of Posts and ~I~lecommunications 588-2, ]waoka, Nishi-ku, Kobe, 651-2d92, Japan • -j ( ' . tel:-k81-78-969-2 ] 81 tax: +81-78-369-2189 http://www-karc.crl, go.j p/ips/murata {AbstractThis pal>or describes two new bunsetsu identificatkm methods using supervised learning. Sin(:e ,Jat)anese syntactic analysis ix usnally done after bunsetsu identification, lmnsetsu identiiieation is iml)orl;ant for analyzing Japanese sentences. In experiments comparing the four previously available machinelearning methods (decision tree, maximmn-entropy method, example-based apI)roaeh and deeiskm list,) an(l two new methods llSing categot'y-exclusive rules~ the new method using l;he category-exclusive rules with the highest similarity t)erformetl best.",
        "id": 465
      },
      {
        "title": "",
        "text": "Run-on sentences are common grammatical mistakes but little research has tackled this problem to date. This work introduces two machine learning models to correct run-on sentences that outperform leading methods for related tasks, punctuation restoration and wholesentence grammatical error correction. Due to the limited annotated data for this error, we experiment with artificially generating training data from clean newswire text. Our findings suggest artificial training data is viable for this task. We discuss implications for correcting run-ons and other types of mistakes that have low coverage in error-annotated corpora.",
        "id": 52812604
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "What research is available on hybrid approaches that combine extractive and abstractive methods for summarizing extensive texts?",
    "positive_ctxs": [
      {
        "title": "Long Document Summarization in a Low Resource Setting using Pretrained Language Models",
        "text": "Abstractive summarization is the task of compressing a long document into a coherent short document while retaining salient information. Modern abstractive summarization methods are based on deep neural networks which often require large training datasets. Since collecting summarization datasets is an expensive and time-consuming task, practical industrial settings are usually low-resource. In this paper, we study a challenging low-resource setting of summarizing long legal briefs with an average source document length of 4268 words and only 120 available (document, summary) pairs. To account for data scarcity, we used a modern pretrained abstractive summarizer BART (Lewis et al., 2020), which only achieves 17.9 ROUGE-L as it struggles with long documents. We thus attempt to compress these long documents by identifying salient sentences in the source which best ground the summary, using a novel algorithm based on GPT-2 (Radford et al., 2019) language model perplexity scores, that operates within the low resource regime. On feeding the compressed documents to BART, we observe a 6.0 ROUGE-L improvement. Our method also beats several competitive salience detection baselines. Furthermore, the identified salient sentences tend to agree with an independent human labeling by domain experts. *",
        "id": 232075836
      }
    ],
    "negative_ctxs": [
      {
        "title": "Panel on Natural Language and Databases",
        "text": "While I disagree with the proposition that database query has outlived its usefulness as a test environment for natural language processing (for reasons that I give below), I believe there are other reasonable tasks which can also spur new research in NL processing. In particular, I will suggest that the task of providing a natural language interface to a rich programming environment offers a convenient yet challenging extension of work already being done with database query.First I recite some of the merits of continuing research on natural language within the confines of constructing an interface for ordinary databases. One advantage is that the speed of processing is not of overwhelming importance in this application, since one who requests information from a database can expect the retrieval to take time, with or without a natural language interface. Of course speed is desirable, and waiting for answers to apparently simple requests will be irritating, but some delay will be tolerable. This tolerance on the part of the user will, I suggest, disappear in applications where an attempt is made to engage a system in dialogue with the user, as would be the case in some expert systems, or in teaching systems. Assuming that natural language systems will not by themselves get faster as they are made to cope with larger fragments of a natural language, it will be useful to continue with database query while we wait for miracles of technology to fill our demands for ever greater processing speed.A second reason for not yet abandoning the database query as a test environment is that a great deal of important natural language processing research remains to be done in generalizing systems to cope with more than one natural language. Work on language universals gives reason to believe that some significant part of a natural language system for English should be recyclable in constructing a system for some other language. How much these cross-linguistic concerns ought to affect the construction of a particular system is itself one of the questions deserving of attention, but our experience to date suggests that it pays to avoid language-particular solutions in an implementation which aspires to treatment of any sizable fragment of a language, even a single language like English. The degree of language-independence that a natural language system can boast may also prove to be one useful metric for evaluating and comparing such systems. [t seems clear that even the task of answering database queries will provide a more than adequate supply of linguistically interesting problems for this line of research.Finally, it has simply not been our experience at Hewlett-Packard that there is any shortage of theoretically interesting problems to solve in constructing a natural language interface for databases. For example, in building such an interface, we have recently designed and implemented a hierarchically structured lexicon for a fragment of English, together with a set of lexical rules that can be run either when loading the system, or when parsing, to greatly expand the size of the lexicon actually used in the system. Several questions of theoretical interest that arose in that process remain unanswered; at least some can be answered by experimenting with our present system, functioning simply as an interface to an ordinary relational database.Having argued that significant work remains to be done in natural language processing as an interface to databases, I nonetheless believe that it would be fruitful to expand the scope of a natural language interface, to permit some manipulation of a programming environment, allowing not only the retrieval of information describing the state of the system, but also some modification of the system via natural language. Of course, such a task would be facilitated by having the information about the environment stored in a manner similar to that of a database, so that our attention could be devoted to the new range of linguistic issues raised, rather than to details of how the whole programming environment is structured and maintained.",
        "id": 23751854
      },
      {
        "title": "SemEval-2016 Task 12: Clinical TempEval",
        "text": "Clinical TempEval 2016 evaluated temporal information extraction systems on the clinical domain. Nine sub-tasks were included, covering problems in time expression identification, event expression identification and temporal relation identification. Participant systems were trained and evaluated on a corpus of clinical and pathology notes from the Mayo Clinic, annotated with an extension of TimeML for the clinical domain. 14 teams submitted a total of 40 system runs, with the best systems achieving near-human performance on identifying events and times. On identifying temporal relations, there was a gap between the best systems and human performance, but the gap was less than half the gap of Clinical TempEval 2015.",
        "id": 8559209
      },
      {
        "title": "THREE MECHANISMS OF WEIGHT DECAY REGULARIZATION",
        "text": "Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of L 2 regularization. Literal weight decay has been shown to outperform L 2 regularization for optimizers for which they differ. We empirically investigate weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay exerts a regularization effect, depending on the particular optimization algorithm and architecture: (1) increasing the effective learning rate, (2) approximately regularizing the inputoutput Jacobian norm, and (3) reducing the effective damping coefficient for second-order optimization. Our results provide insight into how to improve the regularization of neural networks.",
        "id": 53104061
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "manual_acl",
    "question": "Which paper is the first to comprehensively review the progress of deep learning in mathematical reasoning?",
    "positive_ctxs": [
      {
        "title": "A Survey of Deep Learning for Mathematical Reasoning",
        "text": "Mathematical reasoning is a fundamental aspect of human intelligence and is applicable in various fields, including science, engineering, finance, and everyday life. The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems in language has garnered significant interest in the fields of machine learning and natural language processing. For example, mathematics serves as a testbed for aspects of reasoning that are challenging for powerful deep learning models, driving new algorithmic and modeling advances. On the other hand, recent advances in large-scale neural language models have opened up new benchmarks and opportunities to use deep learning for mathematical reasoning. In this survey paper, we review the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade. We also evaluate existing benchmarks and methods, and discuss future research directions in this domain.",
        "id": 254877175
      }
    ],
    "negative_ctxs": [
      {
        "title": "Joint Arc-factored Parsing of Syntactic and Semantic Dependencies",
        "text": "In this paper we introduce a joint arc-factored model for syntactic and semantic dependency parsing. The semantic role labeler predicts the full syntactic paths that connect predicates with their arguments. This process is framed as a linear assignment task, which allows to control some well-formedness constraints. For the syntactic part, we define a standard arc-factored dependency model that predicts the full syntactic tree. Finally, we employ dual decomposition techniques to produce consistent syntactic and predicate-argument structures while searching over a large space of syntactic configurations. In experiments on the CoNLL-2009 English benchmark we observe very competitive results.",
        "id": 7225808
      },
      {
        "title": "",
        "text": "",
        "id": 219308976
      },
      {
        "title": "Ultra-High Dimensional Sparse Representations with Binarization for Efficient Text Retrieval",
        "text": "The semantic matching capabilities of neural information retrieval can ameliorate synonymy and polysemy problems of symbolic approaches. However, neural models' dense representations are more suitable for re-ranking, due to their inefficiency. Sparse representations, either in symbolic or latent form, are more efficient with an inverted index. Taking the merits of the sparse and dense representations, we propose an ultra-high dimensional (UHD) representation scheme equipped with directly controllable sparsity. UHD's large capacity and minimal noise and interference among the dimensions allow for binarized representations, which are highly efficient for storage and search. Also proposed is a bucketing method, where the embeddings from multiple layers of BERT are selected/merged to represent diverse linguistic aspects. We test our models with MS MARCO and TREC CAR, showing that our models outperforms other sparse models. * These authors contributed equally. † The corresponding author",
        "id": 239009411
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Can you show me a paper that built a large structured knowledge base from wikipedia, that can then be used for entity linking and ranking tasks?",
    "positive_ctxs": [
      {
        "title": "KILT: a Benchmark for Knowledge Intensive Language Tasks",
        "text": "Challenging problems such as open-domain question answering, fact checking, slot filling and entity linking require access to large, external knowledge sources. While some models do well on individual tasks, developing general models is difficult as each task might require computationally expensive indexing of custom knowledge sources, in addition to dedicated infrastructure. To catalyze research on models that condition on specific information in large textual resources, we present a benchmark for knowledge-intensive language tasks (KILT). All tasks in KILT are grounded in the same snapshot of Wikipedia, reducing engineering turnaround through the reuse of components, as well as accelerating research into task-agnostic memory architectures. We test both task-specific and general baselines, evaluating downstream performance in addition to the ability of the models to provide provenance. We find that a shared dense vector index coupled with a seq2seq model is a strong baseline, outperforming more tailor-made approaches for fact checking, open-domain question answering and dialogue, and yielding competitive results on entity linking and slot filling, by generating disambiguated text. KILT data and code are available at https://github.com/ facebookresearch/KILT.",
        "id": 221507798
      }
    ],
    "negative_ctxs": [
      {
        "title": "Identifying the Most Dominant Event in a News Article by Mining Event Coreference Relations",
        "text": "Identifying the most dominant and central event of a document, which governs and connects other foreground and background events in the document, is useful for many applications, such as text summarization, storyline generation and text segmentation. We observed that the central event of a document usually has many coreferential event mentions that are scattered throughout the document for enabling a smooth transition of subtopics. Our empirical experiments, using gold event coreference relations, have shown that the central event of a document can be well identified by mining properties of event coreference chains. But the performance drops when switching to system predicted event coreference relations. In addition, we found that the central event can be more accurately identified by further considering the number of sub-events as well as the realis status of an event.",
        "id": 4902473
      },
      {
        "title": "Addressing Limitations of Encoder-Decoder Based Approach to Text-to-SQL",
        "text": "Most attempts on Text-to-SQL task using encoder-decoder approach show a big problem of dramatic decline in performance for new databases. Models trained on Spider dataset, despite achieving 75% accuracy on Spider development or test sets, show a huge decline below 20% accuracy for databases not in Spider. We present a system that combines automated training-data augmentation and ensemble technique. We achieve double-digit percentage improvement for databases that are not part of the Spider corpus.",
        "id": 252819055
      },
      {
        "title": "Fast Statistical Parsing of Noun Phrases for Document Indexing",
        "text": "Information Retrieval (IR) is an important application area of Natural Language Processing (NLP) where one encounters the genuine challenge of processing large quantities of unrestricted natural language text. While much effort has been made to apply NLP techniques to IR, very few NLP techniques have been evaluated on a document collection larger than several megabytes. Many NLP techniques are simply not efficient enough, and not robust enough, to handle a large amount of text. This paper proposes a new probabilistic model for noun phrase parsing, and reports on the application of such a parsing technique to enhance document indexing. The effectiveness of using syntactic phrases provided by the parser to supplement single words for indexing is evaluated with a 250 megabytes document collection. The experiment's results show that supplementing single words with syntactic phrases for indexing consistently and significantly improves retrieval performance.",
        "id": 3262098
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_acl",
    "question": "Are there any research papers on methods to compress large-scale language models using task-agnostic knowledge distillation techniques?",
    "positive_ctxs": [
      {
        "title": "TinyBERT: Distilling BERT for Natural Language Understanding",
        "text": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resourcerestricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large \"teacher\" BERT can be effectively transferred to a small \"student\" Tiny-BERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pretraining and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT.TinyBERT 4 1 with 4 layers is empirically effective and achieves more than 96.8% the performance of its teacher BERT BASE on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT 4 is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only ∼28% parameters and ∼31% inference time of them. Moreover, TinyBERT 6 with 6 layers performs on-par with its teacher BERT BASE . * Authors contribute equally. † This work is done when Xiaoqi Jiao is an intern at Huawei Noah's Ark Lab. ‡ Corresponding authors. 1  The code and models are publicly available at https: //github.com/huawei-noah/Pretrained-Language-Model/tree/ master/TinyBERT",
        "id": 202719327
      }
    ],
    "negative_ctxs": [
      {
        "title": "A Frame-based Sentence Representation for Machine Reading Comprehension",
        "text": "Sentence representation (SR) is the most crucial and challenging task in Machine Reading Comprehension (MRC). MRC systems typically only utilize the information contained in the sentence itself, while human beings can leverage their semantic knowledge. To bridge the gap, we proposed a novel Frame-based Sentence Representation (FSR) method, which employs frame semantic knowledge to facilitate sentence modelling. Specifically, different from existing methods that only model lexical units (LUs), Frame Representation Models, which utilize both LUs in frame and Frame-to-Frame (F-to-F) relations, are designed to model frames and sentences with attention schema. Our proposed FSR method is able to integrate multiple-frame semantic information to get much better sentence representations. Our extensive experimental results show that it performs better than state-of-the-art technologies on machine reading comprehension task.",
        "id": 220047836
      },
      {
        "title": "Factuality Detection on the Cheap: Inferring Factuality for Increased Precision in Detecting Negated Events",
        "text": "This paper describes a system for discriminating between factual and non-factual contexts, trained on weakly labeled data by taking advantage of information implicit in annotations of negated events. In addition to evaluating factuality detection in isolation, we also evaluate its impact on a system for event detection. The two components for factuality detection and event detection form part of a system for identifying negative factual events, or counterfacts, with top-ranked results in the *SEM 2012 shared task.",
        "id": 1649240
      },
      {
        "title": "Humanly Certifying Superhuman Classifiers",
        "text": "Estimating the performance of a machine learning system is a longstanding challenge in artificial intelligence research.Today, this challenge is especially relevant given the emergence of systems which appear to increasingly outperform human beings.In some cases, this \"superhuman\" performance is readily demonstrated; for example by defeating legendary human players in traditional two player games.On the other hand, it can be challenging to evaluate classification models that potentially surpass human performance.Indeed, human annotations are often treated as a ground truth, which implicitly assumes the superiority of the human over any models trained on human annotations.In reality, human annotators can make mistakes and be subjective.Evaluating the performance with respect to a genuine oracle may be more objective and reliable, even when querying the oracle is expensive or impossible.In this paper, we first raise the challenge of evaluating the performance of both humans and models with respect to an oracle which is unobserved.We develop a theory for estimating the accuracy compared to the oracle, using only imperfect human annotations for reference.Our analysis provides a simple recipe for detecting and certifying superhuman performance in this setting, which we believe will assist in understanding the stage of current research on classification.We validate the convergence of the bounds and the assumptions of our theory on carefully designed toy experiments with known oracles.Moreover, we demonstrate the utility of our theory by meta-analyzing large-scale natural language processing tasks, for which an oracle does not exist, and show that under our assumptions a number of models from recent years are with high probability superhuman.",
        "id": 237532482
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that tries to interpret how bi-directional RNNs manage to carry out Named Entity Recognition (NER) tasks?",
    "positive_ctxs": [
      {
        "title": "Interpretability Analysis for Named Entity Recognition to Understand System Predictions and How They Can Improve under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics",
        "text": "Named entity recognition systems achieve remarkable performance on domains such as English news. It is natural to ask: What are these models actually learning to achieve this? Are they merely memorizing the names themselves? Or are they capable of interpreting the text and inferring the correct entity type from the linguistic context? We examine these questions by contrasting the performance of several variants of architectures for named entity recognition, with some provided only representations of the context as features. We experiment with GloVebased BiLSTM-CRF as well as BERT. We find that context does influence predictions, but the main factor driving high performance is learning the named tokens themselves. Furthermore, we find that BERT is not always better at recognizing predictive contexts compared to a BiLSTM-CRF model. We enlist human annotators to evaluate the feasibility of inferring entity types from context alone and find that humans are also mostly unable to infer entity types for the majority of examples on which the context-only system made errors. However, there is room for improvement: A system should be able to recognize any named entity in a predictive context correctly Submission Volume 47, Number 1 and our experiments indicate that current systems may be improved by such capability. Our human study also revealed that systems and humans do not always learn the same contextual clues, and context-only systems are sometimes correct even when humans fail to recognize the entity type from the context. Finally, we find that one issue contributing to model errors is the use of \"entangled\" representations that encode both contextual and local token information into a single vector, which can obscure clues. Our results suggest that designing models that explicitly operate over representations of local inputs and context, respectively, may in some cases improve performance. In light of these and related findings, we highlight directions for future work.",
        "id": 215548904
      }
    ],
    "negative_ctxs": [
      {
        "title": "\"",
        "text": "",
        "id": 15360530
      },
      {
        "title": "Learning Lexicon Models from Search Logs for Query Expansion",
        "text": "This paper explores log-based query expansion (QE) models for Web search. Three lexicon models are proposed to bridge the lexical gap between Web documents and user queries. These models are trained on pairs of user queries and titles of clicked documents. Evaluations on a real world data set show that the lexicon models, integrated into a ranker-based QE system, not only significantly improve the document retrieval performance but also outperform two state-of-the-art log-based QE methods.",
        "id": 15849128
      },
      {
        "title": "",
        "text": "",
        "id": 13963354
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "manual_acl",
    "question": "What is a large event-coverage general-domain event argument extraction dataset?",
    "positive_ctxs": [
      {
        "title": "GENEVA: Benchmarking Generalizability for Event Argument Extraction with Hundreds of Event Types and Argument Roles",
        "text": "Recent works in Event Argument Extraction (EAE) have focused on improving model generalizability to cater to new events and domains. However, standard benchmarking datasets like ACE and ERE cover less than 40 event types and 25 entity-centric argument roles. Limited diversity and coverage hinder these datasets from adequately evaluating the generalizability of EAE models. In this paper, we first contribute by creating a large and diverse EAE ontology. This ontology is created by transforming FrameNet, a comprehensive semantic role labeling (SRL) dataset for EAE, by exploiting the similarity between these two tasks. Then, exhaustive human expert annotations are collected to build the ontology, concluding with 115 events and 220 argument roles, with a significant portion of roles not being entities. We utilize this ontology to further introduce GENEVA, a diverse generalizability benchmarking dataset comprising four test suites, aimed at evaluating models' ability to handle limited data and unseen event type generalization. We benchmark six EAE models from various families. The results show that owing to non-entity argument roles, even the best-performing model can only achieve 39% F1 score, indicating how GENEVA provides new challenges for generalization in EAE. Overall, our large and diverse EAE ontology can aid in creating more comprehensive future resources, while GENEVA is a challenging benchmarking dataset encouraging further research for improving generalizability in EAE.",
        "id": 258865260
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "Mey's criticism of the functional approac~ to generative description concerns (1) the formal properties of the system proposed by Sgall et el. (its weak generative power, recursivit$~), and (2) some itl~or!~al questions connected with the mentioned approach.",
        "id": 14918516
      },
      {
        "title": "thecerealkiller at SemEval-2016 Task 4: Deep Learning based System for Classifying Sentiment of Tweets on Two Point Scale",
        "text": "In this paper, we propose a deep learning system for classification of tweets on a two-point scale. Our architecture consists of a multilayered recurrent neural network having gated recurrent units. The network is pre-trained with a weakly labeled dataset of tweets to learn the sentiment specific embeddings. Then it is fine tuned on the given training dataset of the task 4B in SemEval-2016. The network does very little pre-processing for raw tweets and no post-processing at all. The proposed system achieves 3rd rank on the leaderboard of task 4B.",
        "id": 16092889
      },
      {
        "title": "Approches à base de fréquences pour la simplification lexicale",
        "text": "La simplification lexicale consiste à remplacer des mots ou des phrases par leur équivalent plus simple. Dans cet article, nous présentons trois modèles de simplification lexicale, fondés sur différents critères qui font qu'un mot est plus simple à lire et à comprendre qu'un autre. Nous avons testé différentes tailles de contextes autour du mot étudié : absence de contexte avec un modèle fondé sur des fréquences de termes dans un corpus d'anglais simplifié ; quelques mots de contexte au moyen de probabilités à base de n-grammes issus de données du web ; et le contexte étendu avec un modèle fondé sur les fréquences de cooccurrences.ABSTRACTStudying frequency-based approaches to process lexical simplificationLexical simplification aims at replacing words or phrases by simpler equivalents. In this paper, we present three models for lexical simplification, focusing on the criteria that make one word simpler to read and understand than another. We tested different contexts of the considered word : no context, with a model based on word frequencies in a simplified English corpus ; a few words context, with n-grams probabilites on Web data, and an extended context, with a model based on co-occurrence frequencies.MOTS-CLÉS : simplification lexicale, fréquence lexicale, modèle de langue.",
        "id": 37296498
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "manual_iclr",
    "question": "What paper provides generalization bounds for self supervised learning models eg. CLIP",
    "positive_ctxs": [
      {
        "title": "Understanding prompt engineering may not require rethinking generalization",
        "text": "Zero-shot learning in prompted vision-language models, the practice of crafting prompts to build classifiers without an explicit training process, has achieved impressive performance in many settings.This success presents a seemingly surprising observation: these methods suffer relatively little from overfitting, i.e., when a prompt is manually engineered to achieve low error on a given training set (thus rendering the method no longer actually zero-shot), the approach still performs well on held-out test data.In this paper, we show that we can explain such performance well via recourse to classical PAC-Bayes bounds.Specifically, we show that the discrete nature of prompts, combined with a PAC-Bayes prior given by a language model, results in generalization bounds that are remarkably tight by the standards of the literature: for instance, the generalization bound of an ImageNet classifier is often within a few percentage points of the true test error.We demonstrate empirically that this holds for existing handcrafted prompts and prompts generated through simple greedy search.Furthermore, the resulting bound is well-suited for model selection: the models with the best bound typically also have the best test performance.This work thus provides a possible justification for the widespread practice of \"prompt engineering,\" even if it seems that such methods could potentially overfit the training data.",
        "id": 263830433
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 216051496
      },
      {
        "title": "Homophonic Pun Generation with Lexically Constrained Rewriting",
        "text": "Punning is a creative way to make conversation enjoyable and literary writing elegant. In this paper, we focus on the task of generating a pun sentence given a pair of homophones. We first find the constraint words supporting the semantic incongruity for a sentence. Then we rewrite the sentence with explicit positive and negative constraints. Our model achieves the state-of-the-art results in both automatic and human evaluations. We further make an error analysis and discuss the challenges for the computational pun models.",
        "id": 226262275
      },
      {
        "title": "Multi-document Summarization via Budgeted Maximization of Submodular Functions",
        "text": "We treat the text summarization problem as maximizing a submodular function under a budget constraint. We show, both theoretically and empirically, a modified greedy algorithm can efficiently solve the budgeted submodular maximization problem near-optimally, and we derive new approximation bounds in doing so. Experiments on DUC'04 task show that our approach is superior to the bestperforming method from the DUC'04 evaluation on ROUGE-1 scores.",
        "id": 1803710
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_acl",
    "question": "What research has been done on improving named entity recognition tasks by augmenting them with retrieval of external contexts?",
    "positive_ctxs": [
      {
        "title": "Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning",
        "text": "Recent advances in Named EntityRecognition (NER)show that document-level contexts can significantly improve model performance. In many application scenarios, however, such contexts are not available. In this paper, we propose to find external contexts of a sentence by retrieving and selecting a set of semantically relevant texts through a search engine, with the original sentence as the query. We find empirically that the contextual representations computed on the retrieval-based input view, constructed through the concatenation of a sentence and its external contexts, can achieve significantly improved performance compared to the original input view based only on the sentence. Furthermore, we can improve the model performance of both input views by Cooperative Learning, a training method that encourages the two input views to produce similar contextual representations or output label distributions. Experiments show that our approach can achieve new state-of-the-art performance on 8 NER data sets across 5 domains. 1",
        "id": 234337605
      }
    ],
    "negative_ctxs": [
      {
        "title": "ETH-DS3Lab at SemEval-2018 Task 7: Effectively Combining Recurrent and Convolutional Neural Networks for Relation Classification and Extraction",
        "text": "Reliably detecting relevant relations between entities in unstructured text is a valuable resource for knowledge extraction, which is why it has awaken significant interest in the field of Natural Language Processing. In this paper, we present a system for relation classification and extraction based on an ensemble of convolutional and recurrent neural networks that ranked first in 3 out of the 4 Subtasks at Se-mEval 2018 Task 7. We provide detailed explanations and grounds for the design choices behind the most relevant features and analyze their importance.",
        "id": 4696194
      },
      {
        "title": "Reorder and then Parse, Fast and Accurate Discontinuous Constituency Parsing",
        "text": "Discontinuous constituency parsing is still kept developing for its efficiency and accuracy are far behind its continuous counterparts. Motivated by the observation that a discontinuous constituent tree can be simply transformed into a pseudo-continuous one by artificially reordering words in the sentence, we propose a novel reordering method, thereby construct fast and accurate discontinuous constituency parsing systems working in continuous way. Specifically, we model the relative position changes of words as a list of actions. By parsing and performing this actions, the corresponding pseudocontinuous sequence is derived. Discontinuous parse tree can be further inferred via integrating a high-performance pseudo-continuous constituency parser. Our systems are evaluated on three classical discontinuous constituency treebanks, achieving new state-of-the-art on two treebanks and showing a distinct advantage in speed.",
        "id": 256460951
      },
      {
        "title": "Methods to Optimize Wav2Vec with Language Model for Automatic Speech Recognition in Resource Constrained Environment",
        "text": "Automatic Speech Recognition (ASR) on resource constrained environment is a complex task since most of the State-Of-The-Art models are combination of multilayered convolutional neural network (CNN) and Transformer models which itself requires huge resources such as GPU or TPU for training as well as inference. The accuracy as a performance metric of an ASR system depends upon the efficiency of phonemes to word translation of an Acoustic Model and context correction of the Language model. However, inference as a performance metric is also an important aspect, which mostly depends upon the resources. Also, most of the ASR models uses transformer models at its core and one caveat of transformers is that it usually has a finite amount of sequence length it can handle. Either because it uses position encodings or simply because the cost of attention in transformers is actually O(n²) in sequence length, meaning that using very large sequence length explodes in complexity/memory. So you cannot run the system with finite hardware even a very high-end GPU, because if we inference even a one hour long audio with Wav2Vec the system will crash. In this paper, we used some state-of-the-art methods to optimize the Wav2Vec model for better accuracy of predictions in resource constrained systems. In addition, we have performed tests with other SOTA models such as Citrinet and Quartznet for the comparative analysis.",
        "id": 257767721
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "What techniques exist to enhance the few-shot fine-tuning performance in small pre-trained language models?",
    "positive_ctxs": [
      {
        "title": "Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference",
        "text": "Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with \"task descriptions\" in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in lowresource settings by a large margin. 1",
        "id": 210838924
      },
      {
        "title": "Making Pre-trained Language Models Better Few-shot Learners",
        "text": "The recent GPT-3 model(Brown et al., 2020)achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF-better few-shot fine-tuning of language models 1 -a suite of simple and complementary techniques for finetuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning. 2 * The first two authors contributed equally. 1 Alternatively, language models' best friends forever. 2 Our implementation is publicly available at https:// github.com/princeton-nlp/LM-BFF. Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Association for Computational Linguistics (ACL). Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Association for Computational Linguistics (ACL).",
        "id": 229923710
      }
    ],
    "negative_ctxs": [
      {
        "title": "Improving Event Duration Question Answering by Leveraging Existing Temporal Information Extraction Data",
        "text": "Understanding event duration is essential for understanding natural language. However, the amount of training data for tasks like duration question answering, i.e., McTACO, is very limited, suggesting a need for external duration information to improve this task. The duration information can be obtained from existing temporal information extraction tasks, such as UDS-T and TimeBank, where more duration data is available. A straightforward two-stage fine-tuning approach might be less likely to succeed given the discrepancy between the target duration question answering task and the intermediary duration classification task. This paper resolves this discrepancy by automatically recasting an existing event duration classification task from UDS-T to a question answering task similar to the target McTACO. We investigate the transferability of duration information by comparing whether the original UDS-T duration classification or the recast UDS-T duration question answering can be transferred to the target task. Our proposed model achieves a 13% Exact Match score improvement over the baseline on the McTACO duration question answering task, showing that the two-stage fine-tuning approach succeeds when the discrepancy between the target and intermediary tasks are resolved.",
        "id": 250150656
      },
      {
        "title": "Development of the Korean Resource Grammar: Towards Grammar Customization",
        "text": "The Korean Resource Grammar (KRG)is a computational open-source grammar of Korean (Kim and Yang, 2003)  that has been constructed within the DELPH-IN consortium since 2003. This paper reports the second phase of the KRG development that moves from a phenomenabased approach to grammar customization using the LinGO Grammar Matrix. This new phase of development not only improves the parsing efficiency but also adds generation capacity, which is necessary for many NLP applications.",
        "id": 5142531
      },
      {
        "title": "Learning Bilingual Lexicons from Monolingual Corpora",
        "text": "We present a method for learning bilingual translation lexicons from monolingual corpora. Word types in each language are characterized by purely monolingual features, such as context counts and orthographic substrings. Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings. We show that high-precision lexicons can be learned in a variety of language pairs and from a range of corpus types.",
        "id": 7185434
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a study that investigates the implementation of sparsity within attention mechanisms to enhance the performance of models processing extremely lengthy documents?",
    "positive_ctxs": [
      {
        "title": "READTWICE: Reading Very Large Documents with Memories",
        "text": "Knowledge-intensive tasks such as question answering often require assimilating information from different sections of large inputs such as books or article collections. We propose READTWICE 1 , a simple and effective technique that combines several strengths of prior approaches to model long-range dependencies with Transformers. The main idea is to read text in small segments, in parallel, summarizing each segment into a memory table to be used in a second read of the text. We show that the method outperforms models of comparable size on several question answering (QA) datasets and sets a new state of the art on the challenging NarrativeQA task, with questions about entire books.",
        "id": 234334398
      }
    ],
    "negative_ctxs": [
      {
        "title": "Augmenting WordNet-like lexical resources with distributional evidence. An application-oriented perspective\"",
        "text": "The paper deals with the issue of how and to what extent WordNet-like resources provide the necessary information for an assessment of semantic similarity which is useful for practical applications. The general point is made that taxonomical information should be complemented with distributional evidence. The claim is substantiated through experimental a~t8 and an illustration of a word sense disambiguation system (SENSE) capable of using contextually-relevant semantic similarity.",
        "id": 6039355
      },
      {
        "title": "FBK-irst : A Multi-Phase Kernel Based Approach for Drug-Drug Interaction Detection and Classification that Exploits Linguistic Information",
        "text": "This paper presents the multi-phase relation extraction (RE) approach which was used for the DDI Extraction task of SemEval 2013. As a preliminary step, the proposed approach indirectly (and automatically) exploits the scope of negation cues and the semantic roles of involved entities for reducing the skewness in the training data as well as discarding possible negative instances from the test data. Then, a state-of-the-art hybrid kernel is used to train a classifier which is later applied on the instances of the test data not filtered out by the previous step. The official results of the task show that our approach yields an F-score of 0.80 for DDI detection and an F-score of 0.65 for DDI detection and classification. Our system obtained significantly higher results than all the other participating teams in this shared task and has been ranked 1st.",
        "id": 18430390
      },
      {
        "title": "Movie Review Classification Based on a Multiple Classifier *",
        "text": "In this paper, we propose a method to classify movie review documents into positive or negative opinions. There are several approaches to classify documents. The previous studies, however, used only a single classifier for the classification task. We describe a multiple classifier for the review document classification task. The method consists of three classifiers based on SVMs, ME and score calculation. We apply two voting methods and SVMs to the integration process of single classifiers. The integrated methods improved the accuracy as compared with the three single classifiers. The experimental results show the effectiveness of our method.",
        "id": 13949747
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research papers that investigate employing Transformer-based architectures for completing knowledge graphs?",
    "positive_ctxs": [
      {
        "title": "MLMLM: Link Prediction with Mean Likelihood Masked Language Model",
        "text": "Knowledge Bases (KBs) are easy to query, verifiable, and interpretable. They however scale with man-hours and high-quality data. Masked Language Models (MLMs), such as BERT, scale with computing power as well as unstructured raw text data. The knowledge contained within those models is however not directly interpretable. We propose to perform link prediction with MLMs to address both the KBs scalability issues and the MLMs interpretability issues. To do that we introduce MLMLM, Mean Likelihood Masked Language Model, an approach comparing the mean likelihood of generating the different entities to perform link prediction in a tractable manner. We obtain State of the Art (SotA) results on the WN18RR dataset and the best nonentity-embedding based results on the FB15k-237 dataset. We also obtain convincing results on link prediction on previously unseen entities, making MLMLM a suitable approach to introducing new entities to a KB.",
        "id": 221703752
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 237367026
      },
      {
        "title": "Describing Spatial Relationships between Objects in Images in English and French",
        "text": "The context for the work we report here is the automatic description of spatial relationships between pairs of objects in images. We investigate the task of selecting prepositions for such spatial relationships. We describe the two datasets of object pairs and prepositions we have created for English and French, and report results for predicting prepositions for object pairs in both of these languages, using two methods: (a) an existing approach which manually fixes the mapping from geometrical features to prepositions, and (b) a Naive Bayes classifier trained on the English and French datasets. For the latter we use features based on object class labels and geometrical measurements of object bounding boxes. We evaluate the automatically generated prepositions on unseen data in terms of accuracy against the human-selected prepositions.",
        "id": 9560571
      },
      {
        "title": "Catching the Red Priest: Using Historical Editions of Encyclopaedia Britannica to Track the Evolution of Reputations",
        "text": "In this paper, we investigate the feasibility of using the chronology of changes in historical editions of Encyclopaedia Britannica (EB) to track the changes in the landscape of cultural knowledge, and specifically, the rise and fall in reputations of historical figures. We describe the dataprocessing pipeline we developed in order to identify the matching articles about historical figures in Wikipedia, the current electronic edition of Encyclopaedia Britannica (edition 15), and several digitized historical editions, namely, editions 3, 9, 11. We evaluate our results on the tasks of article segmentation and cross-edition matching using a manually annotated subset of 1000 articles from each edition. As a case study for the validity of discovered trends, we use the Wikipedia category of 18th century classical composers. We demonstrate that our data-driven method allows us to identify cases where a historical figure's reputation experiences a drastic fall or a dramatic recovery which would allow scholars to further investigate previously overlooked instances of such change.",
        "id": 14714125
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "manual_iclr",
    "question": "What paper considers sensitive data issue when prompting large language model APIs?",
    "positive_ctxs": [
      {
        "title": "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
        "text": "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under privacy-restricted scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-ofthe-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.Figure 1: Synthetic medical data for illustration. Though rich in domain-specific knowledge, medical data contains sensitive private information. We extract keywords to mitigate privacy concerns.Steven Smith is a 60-year-old man admitted at Auckland Hospital. He was attended by Dr. Edward Jones at Date: 06/01/2008 . He has a past medical history significant for uncontrolled HTN who presents with a non-reducible right inguinal hernia. Patient first noticed a right sided bulge in 3 months prior. Every day it slips out and he has to manually push it back it. He has had to present to the emergency room twice recently when he was unable to push it back it. He was pending an outpatient repair of his right inguinal hernia.What are the assessment and recommendations for this patient?Figure 2: Framework overview. (a) To mitigate privacy leakage, we use a keyword extractor to obtain medical keywords. Clinicians then create several contexts based on these keywords and candidate answers, which the LLM uses to produce privacy-restricted contexts. (b) The generated contexts are used as additional input to enhance SLM medical decision-making capacity.",
        "id": 258832501
      }
    ],
    "negative_ctxs": [
      {
        "title": "FanfictionNLP: A Text Processing Pipeline for Fanfiction",
        "text": "Fanfiction presents an opportunity as a data source for research in NLP, education, and social science. However, answering specific research questions with this data is difficult, since fanfiction contains more diverse writing styles than formal fiction. We present a text processing pipeline for fanfiction, with a focus on identifying text associated with characters. The pipeline includes modules for character identification and coreference, as well as the attribution of quotes and narration to those characters. Additionally, the pipeline contains a novel approach to character coreference that uses knowledge from quote attribution to resolve pronouns within quotes. For each module, we evaluate the effectiveness of various approaches on 10 annotated fanfiction stories. This pipeline outperforms tools developed for formal fiction on the tasks of character coreference and quote attribution.",
        "id": 235097305
      },
      {
        "title": "LEARNING-BASED SUPPORT ESTIMATION IN SUBLINEAR TIME",
        "text": "We consider the problem of estimating the number of distinct elements in a large data set (or, equivalently, the support size of the distribution induced by the data set) from a random sample of its elements. The problem occurs in many applications, including biology, genomics, computer systems and linguistics. A line of research spanning the last decade resulted in algorithms that estimate the support up to ±εn from a sample of size O(log 2 (1/ε) · n/ log n), where n is the data set size. Unfortunately, this bound is known to be tight, limiting further improvements to the complexity of this problem. In this paper we consider estimation algorithms augmented with a machine-learning-based predictor that, given any element, returns an estimation of its frequency. We show that if the predictor is correct up to a constant approximation factor, then the sample complexity can be reduced significantly, to log(1/ε) · n 1−Θ(1/ log(1/ε)) .We evaluate the proposed algorithms on a collection of data sets, using the neuralnetwork based estimators from Hsu et al, ICLR'19 as predictors. Our experiments demonstrate substantial (up to 3x) improvements in the estimation accuracy compared to the state of the art algorithm.",
        "id": 235446639
      },
      {
        "title": "Universal Sentence Encoder for English",
        "text": "We present easy-to-use TensorFlow Hub sentence embedding models having good task transfer performance. Model variants allow for trade-offs between accuracy and compute resources. We report the relationship between model complexity, resources, and transfer performance. Comparisons are made with baselines without transfer learning and to baselines that incorporate word-level transfer. Transfer learning using sentence-level embeddings is shown to outperform models without transfer learning and often those that use only word-level transfer. We show good transfer task performance with minimal training data and obtain encouraging results on word embedding association tests (WEAT) of model bias. † Corresponding authors: {cer, yinfeiy}@google.com 1 We describe our publicly released models. SeeYang et al. (2018)andHenderson et al. (2017)for additional architectural details of models similar to those presented here.2 https://www.tensorflow.org/hub/, Apache 2.0 license, with models available as saved TF graphs. import tensorflow_hub as hub embed = hub.Module(\"https://tfhub.dev/google/\" \"universal-sentence-encoder/2\") embedding = embed([\"Hello World!\"])",
        "id": 53245704
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that examines how well language models work with creole languages, particularly in relation to their effectiveness with Nigerian Pidgin, given its close linguistic relationship with English?",
    "positive_ctxs": [
      {
        "title": "On Language Models for Creoles",
        "text": "Creole languages such as Nigerian Pidgin English and Haitian Creole are under-resourced and largely ignored in the NLP literature. Creoles typically result from the fusion of a foreign language with multiple local languages, and what grammatical and lexical features are transferred to the creole is a complex process (Sessarego, 2020). While creoles are generally stable, the prominence of some features may be much stronger with certain demographics or in some linguistic situations(Winford, 1999;Patrick, 1999). This paper makes several contributions: We collect existing corpora and release models for Haitian Creole, Nigerian Pidgin English, and Singaporean Colloquial English. We evaluate these models on intrinsic and extrinsic tasks. Motivated by the above literature, we compare standard language models with distributionally robust ones and find that, somewhat surprisingly, the standard language models are superior to the distributionally robust ones. We investigate whether this is an effect of overparameterization or relative distributional stability, and find that the difference persists in the absence of over-parameterization, and that drift is limited, confirming the relative stability of creole languages.",
        "id": 237490383
      }
    ],
    "negative_ctxs": [
      {
        "title": "NL Domain Explanations in Knowledge Based MAT",
        "text": "This paper discusses an innovative approach to knowledge based Machine Aided Translation (MAT) where the translator is supported by an user-friendly environment providing linguistic and domain knowledge explanations. Our project aims at integration of a Knowledge Base (KB) in a MAT system and studies the integration principles as well as the internal interface between language and knowledge. The paper presents some related work, rel~)rts the solutions applied in our project and tries to generaiize our evaluation of the selected MAT approach.",
        "id": 774544
      },
      {
        "title": "An Open Web Platform for Rule-Based Speech-to-Sign Translation",
        "text": "We present an open web platform for developing, compiling, and running rulebased speech to sign language translation applications. Speech recognition is performed using the Nuance Recognizer 10.2 toolkit, and signed output, including both manual and non-manual components, is rendered using the JASigning avatar system. The platform is designed to make the component technologies readily accessible to sign language experts who are not necessarily computer scientists. Translation grammars are written in a version of Synchronous Context-Free Grammar adapted to the peculiarities of sign language. All processing is carried out on a remote server, with content uploaded and accessed through a web interface. Initial experiences show that simple translation grammars can be implemented on a time-scale of a few hours to a few days and produce signed output readily comprehensible to Deaf informants. Overall, the platform drastically lowers the barrier to entry for researchers interested in building applications that generate high-quality signed language.",
        "id": 15635164
      },
      {
        "title": "A machine learning approach to the automatic evaluation of machine translation",
        "text": "We present a machine learning approach to evaluating the wellformedness of output of a machine translation system, using classifiers that learn to distinguish human reference translations from machine translations. This approach can be used to evaluate an MT system, tracking improvements over time; to aid in the kind of failure analysis that can help guide system development; and to select among alternative output strings. The method presented is fully automated and independent of source language, target language and domain.",
        "id": 890333
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research that examines how an annotator's individual attributes, like their gender, ethnicity, and political views, influence their judgment of content deemed offensive?",
    "positive_ctxs": [
      {
        "title": "Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection",
        "text": "Warning: this paper discusses and contains content that is offensive or upsetting.The perceived toxicity of language can vary based on someone's identity and beliefs, but this variation is often ignored when collecting toxic language datasets, resulting in dataset and model biases. We seek to understand the who, why, and what behind biases in toxicity annotations. In two online studies with demographically and politically diverse participants, we investigate the effect of annotator identities (who) and beliefs (why), drawing from social psychology research about hate speech, free speech, racist beliefs, political leaning, and more. We disentangle what is annotated as toxic by considering posts with three characteristics: anti-Black language, African American English (AAE) dialect, and vulgarity. Our results show strong associations between annotator identity and beliefs and their ratings of toxicity. Notably, more conservative annotators and those who scored highly on our scale for racist beliefs were less likely to rate anti-Black language as toxic, but more likely to rate AAE as toxic. We additionally present a case study illustrating how a popular toxicity detection system's ratings inherently reflect only specific beliefs and perspectives. Our findings call for contextualizing toxicity labels in social variables, which raises immense implications for toxic language annotation and detection.Text Categories",
        "id": 244117167
      },
      {
        "title": "On Releasing Annotator-Level Labels and Information in Datasets",
        "text": "A common practice in building NLP datasets, especially using crowd-sourced annotations, involves obtaining multiple annotator judgements on the same data instances, which are then flattened to produce a single \"ground truth\" label or score, through majority voting, averaging, or adjudication. While these approaches may be appropriate in certain annotation tasks, such aggregations overlook the socially constructed nature of human perceptions that annotations for relatively more subjective tasks are meant to capture. In particular, systematic disagreements between annotators owing to their socio-cultural backgrounds and/or lived experiences are often obfuscated through such aggregations. In this paper, we empirically demonstrate that label aggregation may introduce representational biases of individual and group perspectives. Based on this finding, we propose a set of recommendations for increased utility and transparency of datasets for downstream use cases.",
        "id": 238634705
      }
    ],
    "negative_ctxs": [
      {
        "title": "Présentation du logiciel Antidote RX",
        "text": "Antidote RX est la sixième édition d'Antidote, un logiciel d'aide à la rédaction développé et commercialisé par la société Druide informatique. Antidote RX comporte un correcteur grammatical avancé, dix dictionnaires de consultation et dix guides linguistiques. Il fonctionne sous les systèmes d'exploitation Windows, Mac OS X et Linux.",
        "id": 232021489
      },
      {
        "title": "Towards Relation Extraction from Speech",
        "text": "Relation extraction has focused on extracting semantic relationships between entities from the unstructured written textual data. However, with the vast and rapidly increasing amounts of spoken data, relation extraction from speech is an important but under-explored problem. In this paper, we propose a new information extraction task, speech relation extraction (SpeechRE). To facilitate further research, we construct the first synthetic training datasets, as well as the first human-spoken test set with native English speakers. We establish strong baseline performance for SpeechRE via two approaches. The pipeline approach connects a pretrained ASR module with a text-based relation extraction module. The end-to-end approach employs a cross-modal encoder-decoder architecture. Our comprehensive experiments reveal the relative strengths and weaknesses of these approaches, and shed light on important future directions in SpeechRE research. We share the source code and datasets on https://github.com/ wutong8023/SpeechRE.",
        "id": 252918407
      },
      {
        "title": "Improving Collocation Extraction for High Frequency Words",
        "text": "The purpose of this paper is to introduce an alternative word association measure aimed at addressing the under-extraction collocations that contain high frequency words. While measures such as MI provide the important contribution of filtering out sheer high frequency of words in the detection of collocations in large corpora, one side effect of this filtering is that it becomes correspondingly difficult for such measures to detect true collocations involving high frequency words. As an alternative, we propose normalizing the MI measure by dividing the frequency of a candidate lexeme by the number of senses of that lexeme. We premise this alternative approach on the one sense per collocation assumption ofYarowsky (1992;1995). Ten verb-noun collocations involving three high frequency verbs (make, take, run) are used to compare the extraction results of traditional MI and the proposed normalized MI. Results show the ranking of these high-frequency verbs as candidate collocates with the target focal nouns is raised by normalizing MI as proposed. Side effects of these improved rankings are discussed, such as increase in false positives resulting from higher recall. It is found that overall rank precision remains quite stable even with the increased recall of normalized MI.",
        "id": 41849556
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you suggest papers that tackle conversational search by using retrieval and conversational response ranking?",
    "positive_ctxs": [
      {
        "title": "Fine-grained Post-training for Improving Retrieval-based Dialogue Systems",
        "text": "Retrieval-based dialogue systems display an outstanding performance when pre-trained language models are used, which includes bidirectional encoder representations from transformers (BERT). During the multi-turn response selection, BERT focuses on training the relationship between the context with multiple utterances and the response. However, this method of training is insufficient when considering the relations between each utterance in the context. This leads to a problem of not completely understanding the context flow that is required to select a response. To address this issue, we propose a new fine-grained post-training method that reflects the characteristics of the multi-turn dialogue. Specifically, the model learns the utterance level interactions by training every short context-response pair in a dialogue session. Furthermore, by using a new training objective, the utterance relevance classification, the model understands the semantic relevance and coherence between the dialogue utterances. Experimental results show that our model achieves new state-of-the-art with significant margins on three benchmark datasets. This suggests that the fine-grained post-training method is highly effective for the response selection task. 1",
        "id": 235097662
      }
    ],
    "negative_ctxs": [
      {
        "title": "A Unified Lexical Processing Framework Based on the Margin Infused Relaxed Algorithm. A Case Study on the Romanian Language",
        "text": "General natural language processing and text-to-speech applications require certain (lexical level) processing steps in order to solve some frequent tasks such as lemmatization, syllabification, lexical stress prediction and phonetic transcription. These steps usually require knowledge of the word's lexical composition (derivative morphology, inflectional affixes, etc.). For known words all applications use lexicons, but there are always out-of-vocabulary (OOV) words that impede the performance of NLP and speech synthesis applications. In such cases, either rule based or data-driven techniques are used to automatically process these OOV words and generate the desired results. In this paper we describe how the above mentioned tasks can be achieved using a Perceptron with the Margin Infused Relaxed Algorithm (MIRA) and sequence labeling.",
        "id": 18383661
      },
      {
        "title": "",
        "text": "",
        "id": 227230352
      },
      {
        "title": "Integrating a Phrase-based SMT Model and a Bilingual Lexicon for Human in Semi-Automatic Acquisition of Technical Term Translation Lexicon",
        "text": "This paper presents an attempt at developing a technique of acquiring translation pairs of technical terms with sufficiently high precision from parallel patent documents. The approach taken in the proposed technique is based on integrating the phrase translation table of a state-of-the-art statistical phrasebased machine translation model, and compositional translation generation based on an existing bilingual lexicon for human use. Our evaluation results clearly show that the agreement between the two individual techniques definitely contribute to improving precision of translation candidates. We then apply the Support Vector Machines (SVMs) to the task of automatically validating translation candidates in the phrase translation table. Experimental evaluation results again show that the SVMs based approach to translation candidates validation can contribute to improving the precision of translation candidates in the phrase translation table.",
        "id": 7184744
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "What research is available on acquiring sentence embeddings through unsupervised approaches, possibly employing contrastive learning methods?",
    "positive_ctxs": [
      {
        "title": "Smoothed Contrastive Learning for Unsupervised Sentence Embedding",
        "text": "Unsupervised contrastive sentence embedding models, e.g., unsupervised SimCSE, use the InfoNCE loss function in training. Theoretically, we expect to use larger batches to get more adequate comparisons among samples and avoid overfitting. However, increasing batch size leads to performance degradation when it exceeds a threshold, which is probably due to the introduction of false-negative pairs through statistical observation. To alleviate this problem, we introduce a simple smoothing strategy upon the InfoNCE loss function, termed Gaussian Smoothed InfoNCE (GS-InfoNCE). In other words, we add random Gaussian noise as an extension to the negative pairs without increasing the batch size. Through experiments on the semantic text similarity tasks, though simple, the proposed smoothing strategy brings improvements to unsupervised SimCSE. Our code are available at",
        "id": 237453212
      }
    ],
    "negative_ctxs": [
      {
        "title": "Machine Translation Supported by Terminological Information",
        "text": "It is well-known that natural language (NL) is highly complex and ambiguous, and designing a system in the sense of 'large scale engineering' rather than in the sense of so-called 'runnable specifications', i.e. computational solutions to pre-selected NL problem areas, which could cope with most complexities of NL, seems not to be feasible in the foreseeable future. Nevertheless, there is a widespread recognition that systems designed for specific purposes are far more likely to be viable. However, in this context the discipline of computational terminology has received little attention in computational linguistics; an unfortunate situation given that natural language processing (NLP) systems seem to be most successful when applied to specialised domains. In this paper we present an approach that integrates an instance of computational terminology into a constraint-based NLP/MT environment. Parts of this research have been carried out in the context of the ET-10/66 project 'Terminology and Extra-linguistic Knowledge' financed by the Commission of the European Communities (CEC). Like in this project we have chosen the subject field telecommunications as the domain of reference, and the text corpus on which the work is based is the Handbook on Satellite Communication of the International Radio Consultative Committee (CCIR); this corpus is an expository type of text. The problems to be solved through our approach, and which are characteristic for sublanguage texts, relate to multiword term identification, domain-specific attachment of prepositional phrases and the disambiguation of lexical ambiguities. The terminology knowledge used in our project for constructing a terminology knowledge base was partly extracted from the information encoded in the EIRETERM term bank, that is designed primarily for human users, and is based on a linguistically motivated statistical analysis of the reference corpus.",
        "id": 18694352
      },
      {
        "title": "",
        "text": "",
        "id": 219306034
      },
      {
        "title": "Detecting Errors in Automatically-Parsed Dependency Relations",
        "text": "We outline different methods to detect errors in automatically-parsed dependency corpora, by comparing so-called dependency rules to their representation in the training data and flagging anomalous ones. By comparing each new rule to every relevant rule from training, we can identify parts of parse trees which are likely erroneous. Even the relatively simple methods of comparison we propose show promise for speeding up the annotation process.",
        "id": 2359904
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you recommend studies that investigate fine-tuning pre-trained language models using weakly supervised learning, especially those employing techniques like contrastive regularization or self-training?",
    "positive_ctxs": [
      {
        "title": "Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach",
        "text": "Fine-tuned pre-trained language models (LMs) have achieved enormous success in many natural language processing (NLP) tasks, but they still require excessive labeled data in the finetuning stage. We study the problem of finetuning pre-trained LMs using only weak supervision, without any labeled data. This problem is challenging because the high capacity of LMs makes them prone to overfitting the noisy labels generated by weak supervision.To address this problem, we develop a contrastive self-training framework, COSINE, to enable fine-tuning LMs with weak supervision. Underpinned by contrastive regularization and confidence-based reweighting, our framework gradually improves model fitting while effectively suppressing error propagation. Experiments on sequence, token, and sentence pair classification tasks show that our model outperforms the strongest baseline by large margins and achieves competitive performance with fully-supervised fine-tuning methods. Our implementation is available on https:// github.com/yueyu1030/COSINE.",
        "id": 222377768
      }
    ],
    "negative_ctxs": [
      {
        "title": "Understanding Information Graphics: A Discourse-Level Problem",
        "text": "Keywords: graphics, understanding, discourse, plan-based modelsInformation graphics that appear in newspapers and magazines generally have a message that the viewer is intended to recognize. This paper argues that understanding such information graphics is a discourse-level problem. In particular, it requires assimilating information from multiple knowledge sources to recognize the intended message of the graphic, just as recognizing intention in text does. Moreover, when an article is composed of text and graphics, the intended message of the information graphic (its discourse intention) must be integrated into the discourse structure of the surrounding text and contributes to the overall discourse intention of the article. This paper describes how we extend plan-based techniques that have been used for understanding traditional discourse to the understanding of information graphics. This work is part of a project to develop an interactive natural language system that provides sight-impaired users with access to information graphics.",
        "id": 17291778
      },
      {
        "title": "#Irony or #Sarcasm- A Quantitative and Qualitative Study Based on Twitter",
        "text": "Current study is with the aim to identify similarities and distinctions between irony and sarcasm by adopting quantitative sentiment analysis as well as qualitative content analysis. The result of quantitative sentiment analysis shows that sarcastic tweets are used with more positive tweets than ironic tweets. The result of content analysis corresponds to the result of quantitative sentiment analysis in identifying the aggressiveness of sarcasm. On the other hand, from content analysis it shows that irony owns two senses. The first sense of irony is equal to aggressive sarcasm with speaker awareness. Thus, tweets of first sense of irony may attack a specific target, and the speaker may tag his/her tweet irony because the tweet itself is ironic. These tweets though tagged as irony are in fact sarcastic tweets. Different from this, the tweets of second sense of irony is tagged to classify an event to be ironic. However, from the distribution in sentiment analysis and examples in content analysis, irony seems to be more broadly used in its second sense.",
        "id": 7967443
      },
      {
        "title": "A Novel Disambiguation Method For Unification-Based Grammars Using Probabilistic Context-Free Approximations",
        "text": "We present a novel disambiguation method for unification-based grammars (UBGs). In contrast to other methods, our approach obviates the need for probability models on the UBG side in that it shifts the responsibility to simpler context-free models, indirectly obtained from the UBG. Our approach has three advantages: (i) training can be effectively done in practice, (ii) parsing and disambiguation of context-free readings requires only cubic time, and (iii) involved probability distributions are mathematically clean. In an experiment for a mid-size UBG, we show that our novel approach is feasible. Using unsupervised training, we achieve 88% accuracy on an exact-match task.",
        "id": 7470225
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Is there a dataset containing question-answer pairs used in psychological counseling available for research?",
    "positive_ctxs": [
      {
        "title": "PsyQA: A Chinese Dataset for Generating Long Counseling Text for Mental Health Support",
        "text": "Great research interests have been attracted to devise AI services that are able to provide mental health support. However, the lack of corpora is a main obstacle to this research, particularly in Chinese language. In this paper, we propose PsyQA, a Chinese dataset of psychological health support in the form of question and answer pair. PsyQA is crawled from a Chinese mental health service platform, and contains 22K questions and 56K long and wellstructured answers. Based on the psychological counseling theories, we annotate a portion of answer texts with typical strategies for providing support, and further present in-depth analysis of both lexical features and strategy patterns in the counseling answers. We also evaluate the performance of generating counseling answers with the generative pretrained models. Results show that utilizing strategies enhances the fluency and helpfulness of generated answers, but there is still a large space for future research.",
        "id": 235313818
      }
    ],
    "negative_ctxs": [
      {
        "title": "Evaluating the Impact of Using a Domain-specific Bilingual Lexicon on the Performance of a Hybrid Machine Translation Approach",
        "text": "This paper describes an Example-Based Machine Translation prototype and presents an evaluation of the impact of using a domainspecific vocabulary on its performance. This prototype is based on a hybrid approach which needs only monolingual texts in the target language and consists to combine translation candidates returned by a cross-language search engine with translation hypotheses provided by a finite-state transducer. The results of this combination are evaluated against a statistical language model of the target language in order to obtain the n-best translations. To measure the performance of this hybrid approach, we achieved several experiments using corpora on two domains from the European Parliament proceedings (Europarl) and the European Medicines Agency documents (Emea). The obtained results show that the proposed approach outperforms the state-of-the-art Statistical Machine Translation system Moses when texts to translate are related to the specialized domain.",
        "id": 16539945
      },
      {
        "title": "The ACL Anthology Reference Corpus: A Reference Dataset for Bibliographic Research in Computational Linguistics",
        "text": "The ACL Anthology is a digital archive of conference and journal papers in natural language processing and computational linguistics. Its primary purpose is to serve as a reference repository of research results, but we believe that it can also be an object of study and a platform for research in its own right. We describe an enriched and standardized reference corpus derived from the ACL Anthology that can be used for research in scholarly document processing. This corpus, which we call the ACL Anthology Reference Corpus (ACL ARC), brings together the recent activities of a number of research groups around the world. Our goal is to make the corpus widely available, and to encourage other researchers to use it as a standard testbed for experiments in both bibliographic and bibliometric research.",
        "id": 174500
      },
      {
        "title": "A Comparison of Approaches for Sentiment Classification on Lithuanian Internet Comments",
        "text": "Despite many methods that effectively solve sentiment classification task for such widely used languages as English, there is no clear answer which methods are the most suitable for the languages that are substantially different. In this paper we attempt to solve Internet comments sentiment classification task for Lithuanian, using two classification approachesknowledge-based and supervised machine learning. We explore an influence of sentiment word dictionaries based on the different parts-of-speech (adjectives, adverbs, nouns, and verbs) for knowledge-based method; different feature types (bag-ofwords, lemmas, word n-grams, character n-grams) for machine learning methods; and pre-processing techniques (emoticons replacement with sentiment words, diacritics replacement, etc.) for both approaches. Despite that supervised machine learning methods (Support Vector Machine and Naïve Bayes Multinomial) significantly outperform proposed knowledge-based method all obtained results are above baseline. The best accuracy 0.679 was achieved with Naïve Bayes Multinomial and token unigrams plus bigrams, when pre-processing involved diacritics replacement.",
        "id": 1190575
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you suggest a study that explores data annotation paradigms that help assuage concerns in lack of annotator expertise when using crowdsourcing?",
    "positive_ctxs": [
      {
        "title": "MULTIVERS: Improving scientific claim verification with weak supervision and full-document context",
        "text": "The scientific claim verification task requires an NLP system to label scientific documents which SUPPORT or REFUTE an input claim, and to select evidentiary sentences (or rationales) justifying each predicted label. In this work, we present MULTIVERS, which predicts a fact-checking label and identifies rationales in a multitask fashion based on a shared encoding of the claim and full document context. This approach accomplishes two key modeling goals. First, it ensures that all relevant contextual information is incorporated into each labeling decision. Second, it enables the model to learn from instances annotated with a document-level fact-checking label, but lacking sentence-level rationales. This allows MULTIVERS to perform weakly-supervised domain adaptation by training on scientific documents labeled using high-precision heuristics. Our approach outperforms two competitive baselines on three scientific claim verification datasets, with particularly strong performance in zero / few-shot domain adaptation experiments. Our code and data are available at https://github.com/ dwadden/multivers. . 2021. Abstract, Rationale, Stance: A Joint Model for Scientific Claim Verification. In EMNLP.",
        "id": 245130931
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 229365710
      },
      {
        "title": "使用關聯法則為主之語言模型於擷取 長距離中文文字關聯性 Association Rule Based Language Models for Discovering Long Distance Dependency in Chinese",
        "text": "2) 在這邊 n-gram 模型 P(d|c)扮演的是可能性量測的角色，透過語言模型機率計算 可以獲得 P(d|c) 的值，而假設所有類別出現的機率是均等，只要能使 P(d|c)最 佳化的類別語言模型，即為此文件 d 為最有可能對應之類別。 2.2 n-gram 模型之建立 語言模型主要的功能是在評估一段文句出現的機率，假設有一文句 S 其長度 為 T 並且是由一段詞序列 W 1 W 2 W 3 …W T 所組成，則 S 出現的機率可以寫成",
        "id": 33418460
      },
      {
        "title": "Détermination et pondération des raffinements d'un terme à partir de son arbre des usages nommés",
        "text": "Grâce à la participation d'un grand nombre de personnes via des jeux accessibles sur le web, nous avons construit un réseau lexical évolutif de grande taille pour le Français. A partir de cette ressource, nous avons abordé la question de la détermination des sens d'usage d'un terme, puis après avoir introduit la notion de similarité entre ces différents usages, nous avons pu obtenir pour un terme son arbre des usages : la racine regroupe tous les usages du terme et une descente dans l'arbre correspond à un raffinement de ces usages. Le nommage des différents noeuds est effectué lors d'une descente en largeur. En simplifiant l'arbre des usages nommés, nous déterminons les différents sens d'un terme, sens que nous introduisons dans le réseau lexical en tant que noeuds de raffinement du terme considéré. Nous terminons par une évaluation empirique des résultats obtenus.Abstract Thanks to the participation of a large number of persons via web-based games, a largesized evolutionary lexical network is available for French. With this resource, we approached the question of the determination of the word usages of a term, and after introducing the notion of similarity between these various word usages, we were able to build for a term its word usage tree: the root groups together all possible usages of this term and a search in the tree corresponds to a refinement of these word usages. The labelling of the various nodes is made during a width-first search. From its labelled word usage tree, we obtain the different meanings of a term, which can be inserted in the lexical network as refinement nodes for this term. Lastly, we present an evaluation of the results we obtain.Mots-clés : réseau lexical, arbre des usages nommés d'un terme, pondération des sens d'un terme Keywords: lexical network, tree of labelled word usages for a term, weighting of the meanings",
        "id": 232021942
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "What research has been conducted on scaling within-document coreference resolution in extended texts?",
    "positive_ctxs": [
      {
        "title": "Scaling Within Document Coreference to Long Texts",
        "text": "State of the art end-to-end coreference resolution models use expensive span representations and antecedent prediction mechanisms. These approaches are expensive both in terms of their memory requirements as well as compute time, and are particularly ill-suited for long documents. In this paper, we propose an approximation to end-to-end models which scales gracefully to documents of any length. Replacing span representations with token representations, we reduce the time/memory complexity via token windows and nearest neighbor sparsification methods for more efficient antecedent prediction. We show our approach's resulting reduction of training and inference time compared to state-of-the-art methods with only a minimal loss in accuracy.",
        "id": 236477874
      }
    ],
    "negative_ctxs": [
      {
        "title": "Towards Automatic Error Analysis of Machine Translation Output",
        "text": "Evaluation and error analysis of machine translation output are important but difficult tasks. In this article, we propose a framework for automatic error analysis and classification based on the identification of actual erroneous words using the algorithms for computation of Word Error Rate (WER) and Position-independent word Error Rate (PER), which is just a very first step towards development of automatic evaluation measures that provide more specific information of certain translation problems. The proposed approach enables the use of various types of linguistic knowledge in order to classify translation errors in many different ways. This work focuses on one possible set-up, namely, on five error categories: inflectional errors, errors due to wrong word order, missing words, extra words, and incorrect lexical choices. For each of the categories, we analyze the contribution of various POS classes. We compared the results of automatic error analysis with the results of human error analysis in order to investigate two possible applications: estimating the contribution of each error type in a given translation output in order to identify the main sources of errors for a given translation system, and comparing different translation outputs using the introduced error categories in order to obtain more information about advantages and disadvantages of different systems and possibilites for improvements, as well as about advantages and disadvantages of applied methods for improvements. We used Arabic-English Newswire and Broadcast News and Chinese-English Newswire outputs created in the framework of the GALE project, several Spanish and English European Parliament outputs generated during the TC-Star project, and three German-English outputs generated in the framework of the fourth Machine Translation Workshop. We show that our results correlate very well with the results of a human error analysis, and that all our metrics except the extra words reflect well the differences between different versions of the same translation system as well as the differences between different translation systems.",
        "id": 8332776
      },
      {
        "title": "Classification of Modal Meaning in Negotiation Dialogues",
        "text": "This paper addresses modality classification for multi-issue bargaining dialogues in order to model human-like negotiation behaviour and to efficiently compute negotiation strategies. We propose a modality annotation and classification scheme comprising semantically distinguishable categories applied reliably by humans and machines. Our classification of modality varieties is based on both the traditional dichotomy of epistemic and root modalities, and on the analysis of the available corpus data. Our modality scheme has been used for annotating human-human dialogues and training SVM-based classifiers. We built predictive models that show accuracies in the range between 73.3 and 82.6%.",
        "id": 4900978
      },
      {
        "title": "Linguistically Motivated Question Classification",
        "text": "In this paper we describe a question interpretation module designed as a part of a Question Answering Dialogue System (QADS) which is used for an interactive quiz application. Question interpretation is achieved in applying a sequence of classification, information extraction, query formalization and query expansion tasks. The process of a question classification is performed based on a domain-specific taxonomy of semantic roles and relations. Our taxonomy was designed in accordance with the real spoken dialogue data. The SVM-based classifier is trained to predict the Expected Answer Type (EAT) with the precision of 82%. In order to retrieve a correct answer, focus word(-s) are extracted to augment the EAT identified by the system. Our hybrid algorithm for the extraction of focus words demonstrates the accuracy of 94.6%. EAT together with focus words are formalized in a query, which is further expanded with the synonyms from WordNet. The expanded query facilitates the search and retrieval of the information that is necessary to generate the system's responses. 1 http://trec.nist.gov/pubs/trec8 2",
        "id": 1559567
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_acl",
    "question": "Where can I find information on self-attentive parsers that have been trained in a few-shot learning setting, including their official code and hyperparameters?",
    "positive_ctxs": [
      {
        "title": "Constituency Parsing with a Self-Attentive Encoder",
        "text": "We demonstrate that replacing an LSTM encoder with a self-attentive architecture can lead to improvements to a state-ofthe-art discriminative constituency parser. The use of attention makes explicit the manner in which information is propagated between different locations in the sentence, which we use to both analyze our model and propose potential improvements. For example, we find that separating positional and content information in the encoder can lead to improved parsing accuracy. Additionally, we evaluate different approaches for lexical representation. Our parser achieves new state-ofthe-art results for single models trained on the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset.",
        "id": 19206893
      }
    ],
    "negative_ctxs": [
      {
        "title": "Discriminative Reasoning for Document-level Relation Extraction",
        "text": "Document-level relation extraction (DocRE) models generally use graph networks to implicitly model the reasoning skill (i.e., pattern recognition, logical reasoning, coreference reasoning, etc.) related to the relation between",
        "id": 235313469
      },
      {
        "title": "Does BERT Learn as Humans Perceive? Understanding Linguistic Styles through Lexica",
        "text": "People convey their intention and attitude through linguistic styles of the text that they write. In this study, we investigate lexicon usages across styles throughout two lenses: human perception and machine word importance, since words differ in the strength of the stylistic cues that they provide. To collect labels of human perception, we curate a new dataset, HUMMINGBIRD, on top of benchmarking style datasets. We have crowd workers highlight the representative words in the text that makes them think the text has the following styles: politeness, sentiment, offensiveness, and five emotion types. We then compare these human word labels with word importance derived from a popular fine-tuned style classifier like BERT. Our results show that the BERT often finds content words not relevant to the target style as important words used in style prediction, but humans do not perceive the same way even though for some styles (e.g., positive sentiment and joy) humanand machine-identified words share significant overlap for some styles.",
        "id": 237433537
      },
      {
        "title": "Cross-lingual Parse Disambiguation based on Semantic Correspondence",
        "text": "We present a system for cross-lingual parse disambiguation, exploiting the assumption that the meaning of a sentence remains unchanged during translation and the fact that different languages have different ambiguities. We simultaneously reduce ambiguity in multiple languages in a fully automatic way. Evaluation shows that the system reliably discards dispreferred parses from the raw parser output, which results in a pre-selection that can speed up manual treebanking.",
        "id": 474999
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_acl",
    "question": "Could you point me to studies that have investigated dialectal variations and normalized text in non-Latin scripts, such as Arabic or Japanese?",
    "positive_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 218974489
      },
      {
        "title": "Multi-dialect Neural Machine Translation and Dialectometry",
        "text": "We present a multi-dialect neural machine translation (NMT) model tailored to Japanese. While the surface forms of Japanese dialects differ from those of standard Japanese, most of the dialects share fundamental properties such as word order, and some also use many of the same phonetic correspondence rules. To take advantage of these properties, we integrate multilingual, syllable-level, and fixed-order translation techniques into a general NMT model. Our experimental results demonstrate that this model can outperform a baseline dialect translation model. In addition, we show that visualizing the dialect embeddings learned by the model can facilitate geographical and typological analyses of dialects.",
        "id": 198918735
      }
    ],
    "negative_ctxs": [
      {
        "title": "Towards an Ontology for Art and Colours",
        "text": "To meet a variety of needs in information modeling, software development and integration as well as knowledge management and reuse, various groups within industry, academia, and government have been developing and deploying sharable and reusable models known as ontologies. Ontologies play an important role in knowledge representation. In this paper, we address the problem of capturing knowledge needed for indexing and retrieving art resources. We describe a case study in which we attempt to construct an ontology for a subset of art. The aim of the present ontology is to build an extensible repository of knowledge and information about artists, their works and materials used in artistic creations. Influenced by the recent interest in colours and colouring materials, mainly shared by French researchers and linguists, an ontology prototype has been developed using Protégé. It allows to organize and catalogue information about artists, art works, colouring materials and related colours.",
        "id": 18043043
      },
      {
        "title": "EmoTag1200 : Understanding the Association between Emojis and Emotions",
        "text": "Given the growing ubiquity of emojis in language, there is a need for methods and resources that shed light on their meaning and communicative role. One conspicuous aspect of emojis is their use to convey affect in ways that may otherwise be non-trivial to achieve. In this paper, we seek to explore the connection between emojis and emotions by means of a new dataset consisting of human-solicited association ratings. We additionally conduct experiments to assess to what extent such associations can be inferred from existing data in an unsupervised manner. Our experiments show that this succeeds when high-quality wordlevel information is available.",
        "id": 226262234
      },
      {
        "title": "Ferryman at SemEval-2020 Task 3: Bert with TFIDF-Weighting for Predicting the Effect of Context in Word Similarity",
        "text": "Word similarity is widely used in machine learning applications like searching engine and recommendation. Measuring the changing meaning of the same word between two different sentences is not only a way to handle complex features in word usage (such as sentence syntax and semantics), but also an important method for different word polysemy modeling. In this paper, we present the methodology proposed by team Ferryman. Our system is based on the Bidirectional Encoder Representations from Transformers (BERT) model combined with term frequency-inverse document frequency (TF-IDF), applying the method on the provided datasets called CoSimLex, which covers four different languages including English, Croatian, Slovene, and Finnish. Our team Ferryman wins the the first position for English task and the second position for Finnish in the subtask 1. * All the corresponding to Yan Wang. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/.",
        "id": 227231766
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you suggest a study that explores improved training methods for dense passage retrieval within open-domain question answering systems?",
    "positive_ctxs": [
      {
        "title": "RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering",
        "text": "In open-domain question answering, dense passage retrieval has become a new paradigm to retrieve relevant passages for finding answers. Typically, the dual-encoder architecture is adopted to learn dense representations of questions and passages for semantic matching. However, it is difficult to effectively train a dual-encoder due to the challenges including the discrepancy between training and inference, the existence of unlabeled positives and limited training data. To address these challenges, we propose an optimized training approach, called RocketQA, to improving dense passage retrieval. We make three major technical contributions in RocketQA, namely crossbatch negatives, denoised hard negatives and data augmentation. The experiment results show that RocketQA significantly outperforms previous state-of-the-art models on both MS-MARCO and Natural Questions. We also conduct extensive experiments to examine the effectiveness of the three strategies in RocketQA. Besides, we demonstrate that the performance of end-to-end QA can be improved based on our RocketQA retriever 1 .",
        "id": 231815627
      }
    ],
    "negative_ctxs": [
      {
        "title": "Clustering Words with the MDL Principle",
        "text": "We address the probhml of automaticMly constructing a thesaurus by clustering words based on corpus data. We view this problem as that of estimating a joint distribution over the (:artesian product of a partition of a set of nouns and a partition of a set of verbs, and propose a learning a.lgorithm based on the Mininmm Description Length (MDL) Principle for such estimation. We empirically compared the performance of our method based on the MDL Principle against the Maximum Likelihood Estimator in word clustering, and found that the former outperforms the latter. ~¢Ve also evaluated the method by conducting pp-attachment disambiguation experiments using an automaticMly constructed thesaurus. Our experimental results indicate that such a thesaurus can be used to improve accuracy in disambiguation.",
        "id": 148
      },
      {
        "title": "THE ROLE OF METAPHORS IN DESCRIPTIONS OF EMOTIONS",
        "text": "Why do we use metaphors? For nearly 2000 years, the most generally accepted answer was that people only use metaphors for rhetorical purposes. Metaphorical language was thought to be merely ornamental --the seasoning of language, exploited for effect by poets and politicians 1, as compared with the cold factual language of the scientist. This view, however, is now no longer accepted (see, for example,Gentner, 1982;Boyd, 1979). It is now assumed, at least by psychologists and linguists, that metaphors, and their close cousins, analogies, are important tools of cognition and communication, providing us with unfamiliar ways of conceptualizing familiar things, and familiar ways of conceptualizing unfamiliar things(Lakoff & Johnson, 1980;Ortony, 1979;Vosniadou & Ortony, in preparation). Yet, what is still assumed, rather than demonstrated, is that nonliteral uses of language are sometimes necessary for accomplishing such goals, rather than merely convenient or elegant ways of doing so. In this paper we present a sort of empirical existence proof that there are some things whose descriptions appear to invoke much more use of metaphorical language than others. This, while not establishing the necessity of metaphors, certainly is a first step.In theory, there are at least three communicative functions that metaphor might serve(Ortony 1975). First, they might allow one to express that which is difficult or impossible to express if one is restricted to literal uses of language. Evidence for this \"inexpressibility\" claim would constitute encouraging support for the necessity-of-metaphors view. A second possible function of metaphors is that they may constitute a particularly compact means of communication. Although conscious experience is continuous in form, the linguistic system we use to talk about it is comprised of discrete elements (lexical items). Unlike more literal forms of language, metaphor may enable us to convey a great deal of information in a succinct manner by obviating the need to isolate the predicates to be expressed into their corresponding lexical representations. Finally, metaphors may help capture the vividness of phenomenal experience. If metaphors convey chunks of information rather than discrete units, they can paint a richer and more detailed picture of our subjective experience than might be expressed by literal language. This we call the \"'vividness\" claim.In this paper we shall concentrate on the first and last of these possible functions. In order to do so, we need to examine a discourse domain for which a prima facie case can be made for supposing that literal language will often be inadequate and which lends itself to variations in vividness. There doubtless are many such domains. The one that we selected was that of internal states, in particular, emotional states. The literature on the linguistic expression of emotions suggests a relatively high incidence of figurative language use (e.g.,Davitz, 1969), providing pragmatic reasons for believing that the context of (linguistic) emotional expression may be a profitable one within which to study metaphor production. Emotional states seemed wellsuited for our purposes because they tend to have an elusive, transient quality that is difficult to describe using literal language, although, of course, they can usually be labeled using literal language. Thus, while it might be easy for a person to label an emotional state as, for example, \"fear,\" it is difficult to provide a literal description of the quality of some particular experience of fear. Furthermore, because emotions vary in intensity, one might expect differential levels of vividness. There seem to be two possible ways in which people might try to communicate the quality of an emotional state. First, a speaker might use literal language to describe the events that triggered the emotional state and hope that the hearer correctly infers how he or she felt. For example, a person might describe the details of being mugged, hoping that a listener would recognize the emotional experience as the type one would have if one were attacked by a mugger. In such a case, the literal description would not describe the quality Of lWinston Churchill once renmrked: \"How infinite is the debt owed to metaphors by politicians who want to speak strongly but are not sure what they are going to say\"!",
        "id": 3903723
      },
      {
        "title": "The Classical Language Toolkit: An NLP Framework for Pre-Modern Languages",
        "text": "This paper announces version 1.0 of the Classical Language Toolkit (CLTK), an NLP framework for pre-modern languages. The vast majority of NLP, its algorithms and software, is created with assumptions particular to living languages, thus neglecting certain important characteristics of largely non-spoken historical languages. Further, scholars of pre-modern languages often have different goals than those of living-language researchers. To fill this void, the CLTK adapts ideas from several leading NLP frameworks to create a novel software architecture that satisfies the unique needs of premodern languages and their researchers. Its centerpiece is a modular processing pipeline that balances the competing demands of algorithmic diversity with pre-configured defaults. The CLTK currently provides pipelines, including models, for almost 20 languages.",
        "id": 237732031
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "manual_iclr",
    "question": "What paper shows that RLAIF can fully replace RLHF to align language models from scratch?",
    "positive_ctxs": [
      {
        "title": "SALMON: SELF-ALIGNMENT WITH PRINCIPLE-FOLLOWING REWARD MODELS",
        "text": "Supervised Fine-Tuning (SFT) on response demonstrations combined with Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful paradigm for aligning LLM-based AI agents. However, a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences. This paper presents a novel approach, namely SALMON (Self-ALignMent with principle-fOllowiNg reward models), to align base language models with minimal human supervision, using only a small set of human-defined principles, yet achieving superior performance. Central to our approach is a principle-following reward model. Trained on synthetic preference data, this model can generate reward scores based on arbitrary human-defined principles. By merely adjusting these principles during the RL training phase, we gain full control over the preferences with the reward model, subsequently influencing the behavior of the RLtrained policies, and eliminating the reliance on the collection of online human preferences. Applying our method to the LLaMA-2-70b base language model, we developed an AI assistant named Dromedary-2. With only 6 exemplars for in-context learning and 31 human-defined principles, Dromedary-2 significantly surpasses the performance of several state-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We have open-sourced the code and model weights to encourage further research into aligning LLMbased AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight.",
        "id": 263831633
      }
    ],
    "negative_ctxs": [
      {
        "title": "Personalized Extractive Summarization Using an Ising Machine Towards Real-time Generation of Efficient and Coherent Dialogue Scenarios",
        "text": "We propose a personalized dialogue scenario generation system which transmits efficient and coherent information with a real-time extractive summarization method optimized by an Ising machine. The summarization problem is formulated as a quadratic unconstraint binary optimization (QUBO) problem, which extracts sentences that maximize the sum of the degree of user's interest in the sentences of documents with the discourse structure of each document and the total utterance time as constraints. To evaluate the proposed method, we constructed a news article corpus with annotations of the discourse structure, users' profiles, and interests in sentences and topics. The experimental results confirmed that a Digital Annealer, which is a simulated annealing-based Ising machine, can solve our QUBO model in a practical time without violating the constraints using this dataset.",
        "id": 241583381
      },
      {
        "title": "PaVeDa -Pavia Verbs Database: Challenges and Perspectives",
        "text": "This paper describes an ongoing endeavor to construct Pavia Verbs Database (PaVeDa) -an open-access typological resource that builds upon previous work on verb argument structure, and in particular the Valency Patterns Leipzig (ValPaL) project(Hartmann et al., 2013). The PaVeDa database features four major innovations as compared to the ValPaL database: (i) it includes data from ancient languages enabling diachronic research; (ii) it expands the language sample to language families that are not represented in the ValPaL; (iii) it is linked to external corpora that are used as sources of usage-based examples of stored patterns; (iv) it introduces a new cross-linguistic layer of annotation for valency patterns which allows for contrastive data visualization.",
        "id": 250390491
      },
      {
        "title": "Contrastive Out-of-Distribution Detection for Pretrained Transformers",
        "text": "Pretrained Transformers achieve remarkable performance when training and test data are from the same distribution. However, in realworld scenarios, the model often faces out-ofdistribution (OOD) instances that can cause severe semantic shift problems at inference time. Therefore, in practice, a reliable model should identify such instances, and then either reject them during inference or pass them over to models that handle another distribution. In this paper, we develop an unsupervised OOD detection method, in which only the indistribution (ID) data are used in training. We propose to fine-tune the Transformers with a contrastive loss, which improves the compactness of representations, such that OOD instances can be better differentiated from ID ones. These OOD instances can then be accurately detected using the Mahalanobis distance in the model's penultimate layer. We experiment with comprehensive settings and achieve near-perfect OOD detection performance, outperforming baselines drastically. We further investigate the rationales behind the improvement, finding that more compact representations through margin-based contrastive learning bring the improvement. We release our code to the community for future research 1 .",
        "id": 233296689
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Can you recommend a dialogue summarization dataset mined from broadcast interviews on the TV or radio?",
    "positive_ctxs": [
      {
        "title": "MEDIASUM: A Large-scale Media Interview Dataset for Dialogue Summarization",
        "text": "This paper introduces MEDIASUM 1 , a largescale media interview dataset consisting of 463.6K transcripts with abstractive summaries. To create this dataset, we collect interview transcripts from NPR and CNN and employ the overview and topic descriptions as summaries. Compared with existing public corpora for dialogue summarization, our dataset is an order of magnitude larger and contains complex multi-party conversations from multiple domains. We conduct statistical analysis to demonstrate the unique positional bias exhibited in the transcripts of televised and radioed interviews. We also show that MEDIASUM can be used in transfer learning to improve a model's performance on other dialogue summarization tasks. * Equal contribution 1 https://github.com/zcgzcgzcg1/ MediaSum/",
        "id": 232185439
      }
    ],
    "negative_ctxs": [
      {
        "title": "Does a Computational Linguist have to be a Linguist?",
        "text": "Invited Speaker AbstractEarly computational linguists supplied much of theoretical basis that the ALPAC report said was needed for research on the practical problem of machine translation. The result of their efforts turned out to be more fundamental in that it provided a general theoretical basis for the study of language use as a process, giving rise eventually to constraint-based grammatical formalisms for syntax, finite-state approaches to morphology and phonology, and a host of models how speakers might assemble sentences, and hearers take them apart. Recently, an entirely new enterprise, based on machine learning and big data, has sprung on the scene and challenged the ALPAC committee's finding that linguistic processing must have a firm basis in linguistic theory. In this talk, I will show that the long-term development of linguistic processing requires linguistic theory, sophisticated statistical manipulation of big data, and a third component which is not linguistic at all. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 2030",
        "id": 43656672
      },
      {
        "title": "État de l'art : l'influence du domaine sur la classification de l'opinion Dis-moi de quoi tu parles, je te dirai ce que tu penses",
        "text": "L'intérêt pour la fouille d'opinion s'est développé en même temps que se sont répandus les blogs, forums et autres plate-formes où les internautes peuvent librement exprimer leur opinion. La très grande quantité de données disponibles oblige à avoir recours à des traitements automatiques de fouille d'opinion. Cependant, la manière dont les gens expriment leur avis change selon ce dont ils parlent. Les distributions des mots utilisés sont différentes d'un domaine à l'autre. Aussi, il est très difficile d'obtenir un classifieur d'opinion fonctionnant sur tous les domaines. De plus, on ne peut appliquer sans adaptation sur un domaine cible un classifieur entraîné sur un domaine source différent. L'objet de cet article est de recenser les moyens de résoudre ce problème difficile.ABSTRACTState of the Art : Influence of Domain on Opinion ClassificationThe interest in opinion mining has grown concurrently with blogs, forums, and others platforms where the internauts can freely write about their opinion on every topic. As the amounts of available data are increasingly huge, the use of automatic methods for opinion mining becomes imperative. However, sentiment is expressed differently in different domains : words distributions can indeed differ significantly. An effective global opinion classifier is therefore hard to develop. Moreover, a classifier trained on a source domain can't be used without adaptation on a target domain. This article aims to describe the state-of-the-art methods used to solve this difficult task. MOTS-CLÉS : État de l'art, Fouille d'opinion, Multi-domaines, Cross-domaines.",
        "id": 45292586
      },
      {
        "title": "Phrase-Based Translation Model for Question Retrieval in Community Question Answer Archives",
        "text": "Community-based question answer (Q&A) has become an important issue due to the popularity of Q&A archives on the web. This paper is concerned with the problem of question retrieval. Question retrieval in Q&A archives aims to find historical questions that are semantically equivalent or relevant to the queried questions. In this paper, we propose a novel phrase-based translation model for question retrieval. Compared to the traditional word-based translation models, the phrasebased translation model is more effective because it captures contextual information in modeling the translation of phrases as a whole, rather than translating single words in isolation. Experiments conducted on real Q&A data demonstrate that our proposed phrasebased translation model significantly outperforms the state-of-the-art word-based translation model.",
        "id": 2726891
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a study that investigates the improvement of knowledge representation learning by incorporating global context into graph attention networks?",
    "positive_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 236477489
      }
    ],
    "negative_ctxs": [
      {
        "title": "Application Adaptive Electronic Dictionary with Intelligent Interface",
        "text": "The paper presents an electronic dictionary that can be adapted to the needs of different NLP applications. It suggests some ways to save on software customisation and acquisition effort through an intelligent developer interface. The emphasis is made on the flexibility of data representation, handling and access speed.",
        "id": 8140941
      },
      {
        "title": "Published as a conference paper at ICLR 2023 LEARNING HIERARCHICAL PROTEIN REPRESENTA- TIONS VIA COMPLETE 3D GRAPH NETWORKS",
        "text": "We consider representation learning for proteins with 3D structures. We build 3D graphs based on protein structures and develop graph networks to learn their representations. Depending on the levels of details that we wish to capture, protein representations can be computed at different levels, e.g., the amino acid, backbone, or all-atom levels. Importantly, there exist hierarchical relations among different levels. In this work, we propose to develop a novel hierarchical graph network, known as ProNet, to capture the relations. Our ProNet is very flexible and can be used to compute protein representations at different levels of granularity. By treating each amino acid as a node in graph modeling as well as harnessing the inherent hierarchies, our ProNet is more effective and efficient than existing methods. We also show that, given a base 3D graph network that is complete, our ProNet representations are also complete at all levels. Experimental results show that ProNet outperforms recent methods on most datasets. In addition, results indicate that different downstream tasks may require representations at different levels. Our code is publicly available as part of the DIG library",
        "id": 257364758
      },
      {
        "title": "Argument Structure and Unaccusativity in the Constraint-based Lexicon",
        "text": "This paper addresses the issue of Split Intransitivity (si) and Unaccusative Mismatches (uMs), proposing a constraint-based approach to si and ums within a recent framework of Head-driven Phrase Structure Grammar. I argue against the widely accepted dichotomous distinction of intransitive verbs, which has been advanced by the Unaccusative Hypothesis[Perlmutter (1978)]. I then propose a quadripartitive distinction of intransitive verbs on the basis of the distribution of subject argument in the semantically motivated argument structure, and show that this quadripartitive distinction allows a better understanding of si and ums. The main idea of this proposal will be summarized as the Quadripartitive Split Intransitivity Hypothesis (Qsm).",
        "id": 17043198
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_acl",
    "question": "Can you direct me to studies that explore techniques like question answering and passage retrieval for mitigating the effects of clickbait headlines?",
    "positive_ctxs": [
      {
        "title": "Clickbait Spoiling via Question Answering and Passage Retrieval",
        "text": "We introduce and study the task of clickbait spoiling: generating a short text that satisfies the curiosity induced by a clickbait post. Clickbait links to a web page and advertises its contents by arousing curiosity instead of providing an informative summary. Our contributions are approaches to classify the type of spoiler needed (i.e., a phrase or a passage), and to generate appropriate spoilers. A large-scale evaluation and error analysis on a new corpus of 5,000 manually spoiled clickbait poststhe Webis Clickbait Spoiling Corpus 2022shows that our spoiler type classifier achieves an accuracy of 80%, while the question answering model DeBERTa-large outperforms all others in generating spoilers for both types.",
        "id": 247593812
      }
    ],
    "negative_ctxs": [
      {
        "title": "Copenhagen at CoNLL-SIGMORPHON 2018: Multilingual Inflection in Context with Explicit Morphosyntactic Decoding",
        "text": "This paper documents the Team Copenhagen system which placed first in the CoNLL-SIGMORPHON 2018 shared task on universal morphological reinflection, Task 2 with an overall accuracy of 49.87. Task 2 focuses on morphological inflection in context: generating an inflected word form, given the lemma of the word and the context it occurs in. Previous SIGMORPHON shared tasks have focused on context-agnostic inflection-the \"inflection in context\" task was introduced this year. We approach this with an encoder-decoder architecture over character sequences with three core innovations, all contributing to an improvement in performance: (1) a wide context window; (2) a multi-task learning approach with the auxiliary task of MSD prediction; (3) training models in a multilingual fashion.",
        "id": 52164624
      },
      {
        "title": "MATAPHORICAL EXTENSION AND LEXICAL MEANING",
        "text": "Metonymy and metaphor reflect an important part of the way people ordinarily conceptualize of themselves, events, and everyday world [1]. We will argue for this position via the lexicalization process of two linguistic items in Chinese: shang and zai, and demonstrate that grammatical meaning develops from lexical meaning by a process of \"generalization or weakening of semantic content,\" which is in fact metaphorical in nature. The purpose of this paper is to ascertain into the nature of metaphorical extension (via metaphor and metonymy) and the creation of lexical meaning as they are seen in the two lexical items mentioned. Though data gathered from corpus, dictionaries and native speaker intuition, we wish to examine the relationship between conversion, metaphor and metonymy, and understand better 1) the driving force for polysemy in Chinese lexicon; 2) the different driving forces, concerning metaphor and metonymy, for prototypical categories and grammaticalization.",
        "id": 16593363
      },
      {
        "title": "A Novel Approach towards Cross Lingual Sentiment Analysis using Transliteration and Character Embedding",
        "text": "Sentiment analysis with deep learning in resource-constrained languages is a challenging task. In this paper, we introduce a novel approach for sentiment analysis in resource-constrained scenarios using character embedding and cross-lingual sentiment analysis with transliteration. We use this method to introduce the novel task of inducing sentiment polarity of words and sentences and aspect term sentiment analysis in the no-resource scenario. We formulate this task by taking a metalingual approach whereby we transliterate data from closely related languages and transform it into a meta language. We also demonstrated the efficacy of using character-level embedding for sentence representation. We experimented with 4 Indian languages -Bengali, Hindi, Tamil, and Telugu, and obtained encouraging results. We also presented new state-of-the-art results on the Hindi sentiment analysis dataset leveraging our metalingual character embeddings.",
        "id": 257767699
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "manual_iclr",
    "question": "What paper proposes breaking down programming problems by predicting the objects that a solution would create?",
    "positive_ctxs": [
      {
        "title": "ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis",
        "text": "When writing programs, people have the ability to tackle a new complex task by decomposing it into smaller and more familiar subtasks. While it is difficult to measure whether neural program synthesis methods have similar capabilities, we can measure whether they compositionally generalize, that is, whether a model that has been trained on the simpler subtasks is subsequently able to solve more complex tasks. In this paper, we characterize several different forms of compositional generalization that are desirable in program synthesis, forming a meta-benchmark which we use to create generalization tasks for two popular datasets, RobustFill and DeepCoder. We then propose ExeDec, a novel decomposition-based synthesis strategy that predicts execution subgoals to solve problems step-by-step informed by program execution at each step. ExeDec has better synthesis performance and greatly improved compositional generalization ability compared to baselines.Preprint. Under review.",
        "id": 260164542
      }
    ],
    "negative_ctxs": [
      {
        "title": "Topological Dependency Trees: A Constraint-Based Account of Linear Precedence",
        "text": "We describe a new framework for dependency grammar, with a modular decomposition of immediate dependency and linear precedence. Our approach distinguishes two orthogonal yet mutually constraining structures: a syntactic dependency tree and a topological dependency tree. The syntax tree is nonprojective and even non-ordered, while the topological tree is projective and partially ordered.",
        "id": 1160899
      },
      {
        "title": "Redundancy: helping semantic disambiguation",
        "text": "Redundancy is a good thing, at least in a learning process. To be a good teacher you must say what you are going to say, say it, then say what you have just said. Well, three times is better than one. To acquire and learn knowledge from text for building a lexical knowledge base, we need to find a source of information that states facts, and repeats them a few times using slightly different sentence structures. A technique is needed for gathering information from that source and identify the redundant information. The extraction of the commonality is an active learning of the knowledge expressed. The proposed research is based on a clustering method developed byBarri~re and Popowich (1996)which performs a gathering of related information about a particular topic. Individual pieces of information are represented via the Conceptual Graph (CG) formalism and the result of the clustering is a large CG embedding all individual graphs. In the present paper, we suggest that the identification of the redundant information within the resulting graph is very useful for disambiguation of the original information at the semantic level.",
        "id": 7861345
      },
      {
        "title": "A Salience-Based Approach to Gesture-Speech Alignment",
        "text": "One of the first steps towards understanding natural multimodal language is aligning gesture and speech, so that the appropriate gestures ground referential pronouns in the speech. This paper presents a novel technique for gesture-speech alignment, inspired by saliencebased approaches to anaphoric pronoun resolution. We use a hybrid between data-driven and knowledge-based mtehods: the basic structure is derived from a set of rules about gesture salience, but the salience weights themselves are learned from a corpus. Our system achieves 95% recall and precision on a corpus of transcriptions of unconstrained multimodal monologues, significantly outperforming a competitive baseline.",
        "id": 311756
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_acl",
    "question": "I'm exploring ways to enhance question answering systems through domain adaptation. Could you point me towards research that specifically focuses on synthetic data generation for this purpose?",
    "positive_ctxs": [
      {
        "title": "End-to-End Synthetic Data Generation for Domain Adaptation of Question Answering Systems",
        "text": "We propose an end-to-end approach for synthetic QA data generation. Our model comprises a single transformer-based encoderdecoder network that is trained end-to-end to generate both answers and questions. In a nutshell, we feed a passage to the encoder and ask the decoder to generate a question and an answer token-by-token. The likelihood produced in the generation process is used as a filtering score, which avoids the need for a separate filtering model. Our generator is trained by finetuning a pretrained LM using maximum likelihood estimation. The experimental results indicate significant improvements in the domain adaptation of QA models outperforming current state-of-the-art methods. * *equal contribution. † Siamak Shakeri is currently with Google. The work was done when he was at AWS AI.",
        "id": 222310116
      }
    ],
    "negative_ctxs": [
      {
        "title": "SESSION 11 -NATURAL LANGUAGE III",
        "text": "",
        "id": 27270383
      },
      {
        "title": "Identifying Appropriate Support for Propositions in Online User Comments",
        "text": "The ability to analyze the adequacy of supporting information is necessary for determining the strength of an argument. 1 This is especially the case for online user comments, which often consist of arguments lacking proper substantiation and reasoning. Thus, we develop a framework for automatically classifying each proposition as UNVERIFIABLE, VERIFIABLE NON-EXPERIENTIAL, or VERIFIABLE EXPE-RIENTIAL 2 , where the appropriate type of support is reason, evidence, and optional evidence, respectively 3 . Once the existing support for propositions are identified, this classification can provide an estimate of how adequately the arguments have been supported. We build a goldstandard dataset of 9,476 sentences and clauses from 1,047 comments submitted to an eRulemaking platform and find that Support Vector Machine (SVM) classifiers trained with n-grams and additional features capturing the verifiability and experientiality exhibit statistically significant improvement over the unigram baseline, achieving a macro-averaged F 1 of 68.99%.",
        "id": 14764893
      },
      {
        "title": "Published as a conference paper at ICLR 2023 EQUIVARIANT HYPERGRAPH DIFFUSION NEURAL OP- ERATORS",
        "text": "Hypergraph neural networks (HNNs) using neural networks to encode hypergraphs provide a promising way to model higher-order relations in data and further solve relevant prediction tasks built upon such higher-order relations. However, higher-order relations in practice contain complex patterns and are often highly irregular. So, it is often challenging to design an HNN that suffices to express those relations while keeping computational efficiency. Inspired by hypergraph diffusion algorithms, this work proposes a new HNN architecture named ED-HNN, which provably approximates any continuous equivariant hypergraph diffusion operators that can model a wide range of higher-order relations. ED-HNN can be implemented efficiently by combining star expansions of hypergraphs with standard message passing neural networks. ED-HNN further shows great superiority in processing heterophilic hypergraphs and constructing deep models. We evaluate ED-HNN for node classification on nine real-world hypergraph datasets. ED-HNN uniformly outperforms the best baselines over these nine datasets and achieves more than 2%↑ in prediction accuracy over four datasets therein. Our",
        "id": 250526398
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_acl",
    "question": "Can you point me to a paper that discussed transformer-based sentence embeddings?",
    "positive_ctxs": [
      {
        "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
        "text": "BERT(Devlin et al., 2018)and RoBERTa (Liu  et al., 2019)  has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods. 1",
        "id": 201646309
      }
    ],
    "negative_ctxs": [
      {
        "title": "PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction",
        "text": "Joint extraction of entities and relations from unstructured texts is a crucial task in information extraction. Recent methods achieve considerable performance but still suffer from some inherent limitations, such as redundancy of relation prediction, poor generalization of span-based extraction and inefficiency. In this paper, we decompose this task into three subtasks, Relation Judgement, Entity Extraction and Subject-object Alignment from a novel perspective and then propose a joint relational triple extraction framework based on Potential Relation and Global Correspondence (PRGC). Specifically, we design a component to predict potential relations, which constrains the following entity extraction to the predicted relation subset rather than all relations; then a relation-specific sequence tagging component is applied to handle the overlapping problem between subjects and objects; finally, a global correspondence component is designed to align the subject and object into a triple with low-complexity. Extensive experiments show that PRGC achieves state-of-the-art performance on public benchmarks with higher efficiency and delivers consistent performance gain on complex scenarios of overlapping triples.",
        "id": 235485451
      },
      {
        "title": "Meeting Decision Tracker: Making Meeting Minutes with De-Contextualized Utterances",
        "text": "Meetings are a universal process to make decisions in business and project collaboration. The capability to automatically itemize the decisions in daily meetings allows for extensive tracking of past discussions. To that end, we developed Meeting Decision Tracker, a prototype system to construct decision items comprising decision utterance detector (DUD) and decision utterance rewriter (DUR). We show that DUR makes a sizable contribution to improving the user experience by dealing with utterance collapse in natural conversation. An introduction video of our system is also available at https://youtu.be/TG1pJJo0Iqo.",
        "id": 253018848
      },
      {
        "title": "UQeResearch: Semantic Textual Similarity Quantification",
        "text": "This paper presents an approach for estimating the Semantic Textual Similarity of full English sentences as specified in Shared Task 2 of SemEval-2015. The semantic similarity of sentence pairs is quantified from three perspectives -structural, syntactical, and semantic. The numerical representations of the derived similarity measures are then applied to train a regression ensemble. Although none of these three sets of measures is able to represent the semantic similarity of two sentences individually, our experimental results show that the combination of these features can precisely assess the semantic similarity of the sentences. In the English subtask our system's best result ranked 35 among 73 system runs with 0.7189 average Pearson correlation over five test sets. This was 0.08 correlation points less than the best submitted run.",
        "id": 8035413
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "What research has been conducted on incorporating visual data into the text summarization process?",
    "positive_ctxs": [
      {
        "title": "Summary-Oriented Vision Modeling for Multimodal Abstractive Summarization",
        "text": "Multimodal abstractive summarization (MAS) aims to produce a concise summary given the multimodal data (text and vision). Existing studies mainly focus on how to effectively use the visual features from the perspective of an article, having achieved impressive success on the high-resource English dataset. However, less attention has been paid to the visual features from the perspective of the summary, which may limit the model performance, especially in the low-and zero-resource scenarios. In this paper, we propose to improve the summary quality through summary-oriented visual features. To this end, we devise two auxiliary tasks including vision to summary task and masked image modeling task. Together with the main summarization task, we optimize the MAS model via the training objectives of all these tasks. By these means, the MAS model can be enhanced by capturing the summaryoriented visual features, thereby yielding more accurate summaries. Experiments on 44 languages, covering mid-high-, low-, and zeroresource scenarios, verify the effectiveness and superiority of the proposed approach, which achieves state-of-the-art performance under all scenarios. Additionally, we will contribute a large-scale multilingual multimodal abstractive summarization (MM-Sum) dataset. 1",
        "id": 254685691
      }
    ],
    "negative_ctxs": [
      {
        "title": "An Empirical Study for Generating Zero Pronoun in Korean based on Cost-based Centering Model",
        "text": "In Korean, in order to generate a coherent text, a redundantly prominent noun should be replaced by a non-zero pronoun or zero pronoun. Otherwise, the text becomes unnatural. Specifically, a redundant noun in Korean is frequently omitted while a redundant noun in English is replaced by a pronoun. This paper proposes a generation algorithm of the zero pronoun, using a Cost-based Centering Model which considers the inference cost. For an objective evaluation of our algorithm, we collected 87 texts from three genres, and manually recovered the omitted elements. Using the collected texts, we verify that our algorithm is well defined to explain the phenomenon of the zero pronoun in Korean. We also show that the proposed approach resolves both the overgeneration of the zero pronoun in Continue and its under-generation in other transitions in terms of Centering.",
        "id": 13133422
      },
      {
        "title": "The European Thesaurus on International Relations and Area Studies - A Multilingual Resource for Indexing, Retrieval, and Translation",
        "text": "The multilingual European Thesaurus on International Relations and Area Studies (European Thesaurus) is a special subject thesaurus for the field of international affairs. It is intended for use in libraries and documentation centres of academic institutions and international organizations. The European Thesaurus was established in a collaborative project involving a number of leading European research institutes on international politics. It integrates the controlled terminologies of several existing thesauri. The European Thesaurus comprises about 8,200 terms and proper names from the 24 subject areas covered by the thesaurus. Because of its multilinguality, the European Thesaurus can not only be used for indexing, retrieval and terminological reference, but serves also as a translation tool for the languages represented. The establishment of cross-concordances to related thesauri extends the range of application of the European Thesaurus even further. They enable the treatment of semantic heterogeneity within subject gateways. The European Thesaurus is available both in a seven-lingual printversion as well as in an eight-lingual online-version. To reflect the changes in terminology the European Thesaurus is regularly being amended and modified. Further languages are going to be included.",
        "id": 23412271
      },
      {
        "title": "Active Image Indexing",
        "text": "Image copy detection and retrieval from large databases leverage two components. First, a neural network maps an image to a vector representation, that is relatively robust to various transformations of the image. Second, an efficient but approximate similarity search algorithm trades scalability (size and speed) against quality of the search, thereby introducing a source of error. This paper improves the robustness of image copy detection with active indexing, that optimizes the interplay of these two components. We reduce the quantization loss of a given image representation by making imperceptible changes to the image before its release. The loss is back-propagated through the deep neural network back to the image, under perceptual constraints. These modifications make the image more retrievable. Our experiments show that the retrieval and copy detection of activated images is significantly improved. For instance, activation improves by +40% the Re-call1@1 on various image transformations, and for several popular indexing structures based on product quantization and locality sensitivity hashing.",
        "id": 252993084
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_acl",
    "question": "Can you recommend some literature that focuses on dependency-based models for relation extraction, especially those that utilize dependency parsing to capture non-local syntactic relations?",
    "positive_ctxs": [
      {
        "title": "N -ary Relation Extraction using Graph State LSTM",
        "text": "Cross-sentence n-ary relation extraction detects relations among n entities across multiple sentences. Typical methods formulate an input as a document graph, integrating various intra-sentential and inter-sentential dependencies. The current state-of-the-art method splits the input graph into two DAGs, adopting a DAG-structured LSTM for each. Though being able to model rich linguistic knowledge by leveraging graph edges, important information can be lost in the splitting procedure. We propose a graph-state LSTM model, which uses a parallel state to model each word, recurrently enriching state values via message passing. Compared with DAG LSTMs, our graph LSTM keeps the original graph structure, and speeds up computation by allowing more parallelization. On a standard benchmark, our model shows the best result in the literature.",
        "id": 52115592
      },
      {
        "title": "Bidirectional Recurrent Convolutional Neural Network for Relation Classification",
        "text": "Relation classification is an important semantic processing task in the field of natural language processing (NLP). In this paper, we present a novel model BRCNN to classify the relation of two entities in a sentence. Some state-of-the-art systems concentrate on modeling the shortest dependency path (SDP) between two entities leveraging convolutional or recurrent neural networks. We further explore how to make full use of the dependency relations information in the SDP, by combining convolutional neural networks and twochannel recurrent neural networks with long short term memory (LSTM) units. We propose a bidirectional architecture to learn relation representations with directional information along the SDP forwards and backwards at the same time, which benefits classifying the direction of relations. Experimental results show that our method outperforms the state-of-theart approaches on the SemEval-2010 Task 8 dataset.",
        "id": 1774259
      },
      {
        "title": "Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Paths",
        "text": "Relation classification is an important research arena in the field of natural language processing (NLP). In this paper, we present SDP-LSTM, a novel neural network to classify the relation of two entities in a sentence. Our neural architecture leverages the shortest dependency path (SDP) between two entities; multichannel recurrent neural networks, with long short term memory (LSTM) units, pick up heterogeneous information along the SDP. Our proposed model has several distinct features: (1) The shortest dependency paths retain most relevant information (to relation classification), while eliminating irrelevant words in the sentence. (2) The multichannel LSTM networks allow effective information integration from heterogeneous sources over the dependency paths.(3) A customized dropout strategy regularizes the neural network to alleviate overfitting. We test our model on the SemEval 2010 relation classification task, and achieve an F 1 -score of 83.7%, higher than competing methods in the literature.",
        "id": 5403702
      },
      {
        "title": "End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures",
        "text": "We present a novel end-to-end neural model to extract entities and relations between them. Our recurrent neural network based model captures both word sequence and dependency tree substructure information by stacking bidirectional treestructured LSTM-RNNs on bidirectional sequential LSTM-RNNs. This allows our model to jointly represent both entities and relations with shared parameters in a single model. We further encourage detection of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sampling. Our model improves over the stateof-the-art feature-based model on end-toend relation extraction, achieving 12.1% and 5.7% relative error reductions in F1score on ACE2005 and ACE2004, respectively. We also show that our LSTM-RNN based model compares favorably to the state-of-the-art CNN based model (in F1-score) on nominal relation classification (SemEval-2010 Task 8). Finally, we present an extensive ablation analysis of several model components.",
        "id": 2476229
      }
    ],
    "negative_ctxs": [
      {
        "title": "Maltilex: A Computational Lexicon for Maltese",
        "text": "The project described in this paper, which is still in the preliminary phase, concerns the design and implementation of a computational lexicon for Maltese, a language very much in current use but so far lacking most of the infrastructure required for NLP. One of the main characteristics of Maltese, a source of many difficulties, is that it is an amalgam of different language types (chiefly Semitic and Romance), as illustrated in the first part of the paper. The latter part of the paper describes our general approach to the problem of constructing the lexicon.",
        "id": 4013259
      },
      {
        "title": "An Application of Latent Semantic Analysis to Word Sense Discrimination for Words with Related and Unrelated Meanings",
        "text": "We present an application of Latent Semantic Analysis to word sense discrimination within a tutor for English vocabulary learning. We attempt to match the meaning of a word in a document with the meaning of the same word in a fill-in-the-blank question. We compare the performance of the Lesk algorithm to Latent Semantic Analysis. We also compare the performance of Latent Semantic Analysis on a set of words with several unrelated meanings and on a set of words having both related and unrelated meanings.",
        "id": 296903
      },
      {
        "title": "The Character-based CRF Segmenter of MSRA&NEU for the 4th Bakeoff",
        "text": "This paper describes the Chinese Word Segmenter for the fourth International Chinese Language Processing Bakeoff. Base on Conditional Random Field (CRF) model, a basic segmenter is designed as a problem of character-based tagging. To further improve the performance of our segmenter, we employ a word-based approach to increase the in-vocabulary (IV) word recall and a post-processing to increase the out-of-vocabulary (OOV) word recall. We participate in the word segmentation closed test on all five corpora and our system achieved four second best and one the fifth in all the five corpora.",
        "id": 18174510
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you recommend articles that explore the role of late interaction in dense retrieval systems and its influence on the performance of information retrieval?",
    "positive_ctxs": [
      {
        "title": "ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction",
        "text": "Neural information retrieval (IR) has greatly advanced search and other knowledgeintensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6-10×.",
        "id": 244799249
      }
    ],
    "negative_ctxs": [
      {
        "title": "Published as a conference paper at ICLR 2022 DENOISING LIKELIHOOD SCORE MATCHING FOR CONDITIONAL SCORE-BASED DATA GENERATION",
        "text": "Many existing conditional score-based data generation methods utilize Bayes' theorem to decompose the gradients of a log posterior density into a mixture of scores. These methods facilitate the training procedure of conditional score models, as a mixture of scores can be separately estimated using a score model and a classifier. However, our analysis indicates that the training objectives for the classifier in these methods may lead to a serious score mismatch issue, which corresponds to the situation that the estimated scores deviate from the true ones. Such an issue causes the samples to be misled by the deviated scores during the diffusion process, resulting in a degraded sampling quality. To resolve it, we formulate a novel training objective, called Denoising Likelihood Score Matching (DLSM) loss, for the classifier to match the gradients of the true log likelihood density. Our experimental evidence shows that the proposed method outperforms the previous methods on both Cifar-10 and Cifar-100 benchmarks noticeably in terms of several key evaluation metrics. We thus conclude that, by adopting DLSM, the conditional scores can be accurately modeled, and the effect of the score mismatch issue is alleviated. * Work done during an internship at Mediatek Inc.",
        "id": 247763065
      },
      {
        "title": "Generalizing Case Frames Using a Thesaurus and the MDL Principle",
        "text": "A new method for automatically acquiring case frame patterns from large corpora is proposed. In particular, the problem of generalizing values of a case frame slot for a verb is viewed as that of estimating a conditional probability distribution over a partition of words, and a new generalization method based on the Minimum Description Length (MDL) principle is proposed. In order to assist with efficiency, the proposed method makes use of an existing thesaurus and restricts its attention to those partitions that are present as \"cuts\" in the thesaurus tree, thus reducing the generalization problem to that of estimating a \"tree cut model\" of the thesaurus tree. An efficient algorithm is given, which provably obtains the optimal tree cut model for the given frequency data of a case slot, in the sense of MDL. Case frame patterns obtained by the method were used to resolve PP-attachment ambiguity. Experimental results indicate that the proposed method improves upon or is at least comparable with existing methods. . In this paper, we propose a new generalization method, belonging to the first of these two categories, which is both theoretically well-motivated and computationally efficient. Specifically, we formalize the problem of generalizing values of a case frame slot for a given verb as that of estimating a conditional probability distribution over a partition of words, and propose a new generalization method based on the Minimum Description Length principle (MDL): a principle of data compression and statistical estimation from information theory. 1 In order to assist with efficiency, our method makes use of an existing thesaurus and restricts its attention on those partitions that are present as \"cuts\" in the thesaurus tree, thus reducing the generalization problem to that of estimating a \"tree cut model\" of the thesaurus tree. We then give an efficient algorithm that provably obtains the optimal tree cut model for the given frequency data of a case slot, in the sense of MDL. In order to test the effectiveness of our method, we conducted PP-attachment disambiguation experiments using the case frame patterns obtained by our method. Our experimental results indicate that the proposed method improves upon or is at least comparable to existing methods.The remainder of this paper is organized as follows: In Section 2, we formalize the problem of generalizing values of a case frame slot as that of estimating a conditional distribution. In Section 3, we describe our MDL-based generalization method. In Section 4, we present our experimental results. We then give some concluding remarks in Section 5.",
        "id": 8497895
      },
      {
        "title": "Using the verifiability of details as a test of deception: A conceptual framework for the automation of the verifiability approach",
        "text": "The Verifiability Approach (VA) is a promising new approach for deception detection. It extends existing verbal credibility assessment tools by asking interviewees to provide statements rich in verifiable detail. Details that i) have been experienced with an identifiable person, ii) have been witnessed by an identifiable person, or iii) have been recorded through technology, are labelled as verifiable. With only minimal modifications of information-gathering interviews this approach has yielded remarkable classification accuracies. Currently, the VA relies on extensive manual annotation by human coders. Aiming to extend the VA's applicability, we present a work in progress on automated VA scoring. We provide a conceptual outline of two automation approaches: one being based on the Linguistic Inquiry and Word Count software and the other on rule-based shallow parsing and named entity recognition. Differences between both approaches and possible future steps for an automated VA are discussed.",
        "id": 12108058
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "What research has been conducted on creating neural network frameworks for parsing text into SQL?",
    "positive_ctxs": [
      {
        "title": "ShadowGNN: Graph Projection Neural Network for Text-to-SQL Parser",
        "text": "Given a database schema, Text-to-SQL aims to translate a natural language question into the corresponding SQL query. Under the setup of cross-domain, traditional semantic parsing models struggle to adapt to unseen database schemas. To improve the model generalization capability for rare and unseen schemas, we propose a new architecture, ShadowGNN, which processes schemas at abstract and semantic levels. By ignoring names of semantic items in databases, abstract schemas are exploited in a well-designed graph projection neural network to obtain delexicalized representation of question and schema. Based on the domain-independent representations, a relation-aware transformer is utilized to further extract logical linking between question and schema. Finally, a SQL decoder with context-free grammar is applied. On the challenging Text-to-SQL benchmark Spider, empirical results show that ShadowGNN outperforms state-of-the-art models. When the annotated data is extremely limited (only 10% training set), ShadowGNN gets over absolute 5% performance gain, which shows its powerful generalization ability. Our implementation will be open-sourced at https://github. com/WowCZ/shadowgnn.",
        "id": 233210172
      }
    ],
    "negative_ctxs": [
      {
        "title": "Proceedings of the 2nd Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal Automated morphological analysis of clinical language samples",
        "text": "Quantitative analysis of clinical language samples is a powerful tool for assessing and screening developmental language impairments, but requires extensive manual transcription, annotation, and calculation, resulting in error-prone results and clinical underutilization. We describe a system that performs automated morphological analysis needed to calculate statistics such as the mean length of utterance in morphemes (MLUM), so that these statistics can be computed directly from orthographic transcripts. Estimates of MLUM computed by this system are closely comparable to those produced by manual annotation. Our system can be used in conjunction with other automated annotation techniques, such as maze detection. This work represents an important first step towards increased automation of language sample analysis, and towards attendant benefits of automation, including clinical greater utilization and reduced variability in care delivery.",
        "id": 919698
      },
      {
        "title": "Qualia Structure and the Compositional Interpretation of Compounds",
        "text": "The analysis of nominal compound constructions has proven to be a recalcitrant problem for linguistic semantics and poses serious challenges for natural language processing systems. We argue for a compositional treatment of compound constructions which limits the need for listing of compounds in the lexicon. We argue that the development of a practical model of compound interpretation crucially depends on issues of lexicon design. The Generative Lexicon (Pustejovsky 1995) provides us with a model of the lexicon which couples sufficiently expressive lexical semantic representations with mechanisms which capture the relationship between those representations and their syntactic expression. In our approach, the qualia structures of the nouns in a compound provide relational structure enabling compositional interpretation of the modification of the head noun by the modifying noun. This brings compound interpretation under the same rubric as other forms of composition in natural language, including argument selection, adjectival modification, and type coercion(Pustejovsky (1991(Pustejovsky ( ,1995, Bouillon 1995). We examine data from both English and Italian and develop analyses for both languages which use phrase structure schemata to account for the connections between lexical semantic representation and syntactic expression. In addition to applications in natural language understanding, machine translation, and generation, the model of compound interpretation developed here can be applied to multi-lingual information extraction tasks.",
        "id": 5170618
      },
      {
        "title": "Diagnosing meaning errors in short answers to reading comprehension questions",
        "text": "A common focus of systems in Intelligent Computer-Assisted Language Learning (ICALL) is to provide immediate feedback to language learners working on exercises. Most of this research has focused on providing feedback on the form of the learner input. Foreign language practice and second language acquisition research, on the other hand, emphasizes the importance of exercises that require the learner to manipulate meaning.",
        "id": 865293
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "What research has been conducted on enhancing conversational generation models using knowledge sourced from the internet?",
    "positive_ctxs": [
      {
        "title": "Internet-Augmented Dialogue Generation",
        "text": "The largest store of continually updating knowledge on our planet can be accessed via internet search. In this work we study giving access to this information to conversational agents. Large language models, even though they store an impressive amount of knowledge within their weights, are known to hallucinate facts when generating dialogue (Shuster et al., 2021); moreover, those facts are frozen in time at the point of model training. In contrast, we propose an approach that learns to generate an internet search query based on the context, and then conditions on the search results to finally generate a response, a method that can employ up-to-the-minute relevant information. We train and evaluate such models on a newly collected dataset of human-human conversations whereby one of the speakers is given access to internet search during knowledge-driven discussions in order to ground their responses. We find that search-query based access of the internet in conversation provides superior performance compared to existing approaches that either use no augmentation or FAISS-based retrieval (Lewis et al., 2020b). . 2020. Exploring the limits of transfer learning with a unified text-to-text transformer.",
        "id": 236034557
      }
    ],
    "negative_ctxs": [
      {
        "title": "SemEval-2016 Task 7: Determining Sentiment Intensity of English and Arabic Phrases",
        "text": "We present a shared task on automatically determining sentiment intensity of a word or a phrase. The words and phrases are taken from three domains: general English, English Twitter, and Arabic Twitter. The phrases include those composed of negators, modals, and degree adverbs as well as phrases formed by words with opposing polarities. For each of the three domains, we assembled the datasets that include multi-word phrases and their constituent words, both manually annotated for real-valued sentiment intensity scores. The three datasets were presented as the test sets for three separate tasks (each focusing on a specific domain). Five teams submitted nine system outputs for the three tasks. All datasets created for this shared task are freely available to the research community.",
        "id": 11994495
      },
      {
        "title": "",
        "text": "",
        "id": 236999865
      },
      {
        "title": "Hate-CLIPper: Multimodal Hateful Meme Classification based on Cross-modal Interaction of CLIP Features",
        "text": "Hateful memes are a growing menace on social media. While the image and its corresponding text in a meme are related, they do not necessarily convey the same meaning when viewed individually. Hence, detecting hateful memes requires careful consideration of both visual and textual information. Multimodal pretraining can be beneficial for this task because it effectively captures the relationship between the image and the text by representing them in a similar feature space. Furthermore, it is essential to model the interactions between the image and text features through intermediate fusion. Most existing methods either employ multimodal pre-training or intermediate fusion, but not both. In this work, we propose the Hate-CLIPper architecture, which explicitly models the cross-modal interactions between the image and text representations obtained using Contrastive Language-Image Pre-training (CLIP) encoders via a feature interaction matrix (FIM). A simple classifier based on the FIM representation is able to achieve state-of-the-art performance on the Hateful Memes Challenge (HMC) dataset with an AUROC of 85.8, which even surpasses the human performance of 82.65. Experiments on other meme datasets such as Propaganda Memes and TamilMemes also demonstrate the generalizability of the proposed approach. Finally, we analyze the interpretability of the FIM representation and show that cross-modal interactions can indeed facilitate the learning of meaningful concepts. The code for this work is available at https://github. com/gokulkarthik/hateclipper.",
        "id": 252846273
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "manual_iclr",
    "question": "What paper evaluated the ability of visual few-shot learning models to do in-context learning?",
    "positive_ctxs": [
      {
        "title": "CONTEXT-AWARE META-LEARNING",
        "text": "Large Language Models like ChatGPT demonstrate a remarkable capacity to learn new concepts during inference without any fine-tuning. However, visual models trained to detect new objects during inference have been unable to replicate this ability, and instead either perform poorly or require meta-training and/or finetuning on similar objects. In this work, we propose a meta-learning algorithm that emulates Large Language Models by learning new visual concepts during inference without fine-tuning. Our approach leverages a frozen pre-trained feature extractor, and analogous to in-context learning, recasts meta-learning as sequence modeling over datapoints with known labels and a test datapoint with an unknown label. On 8 out of 11 meta-learning benchmarks, our approach-without meta-training or fine-tuning-exceeds or matches the state-of-the-art algorithm, P>M>F, which is meta-trained on these benchmarks.Under Review classify the query given an input sequence composed of the support set and query point. This learning paradigm trains the Transformer encoder to extrapolate to new classes in the parameter-space of the model, enabling our approach to learn new visual concepts during inference without fine-tuning. Due to its capacity to learn visual information \"in-context\", we term our approach Context-Aware Meta-Learning (CAML).Our primary contribution is to develop a meta-learning algorithm for universal meta-learning: the capacity to learn any new visual concept during inference without fine-tuning or meta-training on related images. This challenging setting emulates the deployment of LLMs to real-time applications, and strong performance in this setting would unlock new applications of visual meta-learning. Our theoretical analysis shows that an ELMES is the encoding that minimizes the entropy of detecting classes within the support set, and therefore, does not need to be learned. Our empirical analysis highlights the importance of reformulating meta-learning as sequence modeling: considering the support set and query together enables the model to attend to specific visual features of images in the support set to classify the query. Finally, our empirical analysis indicates CAML is a state-of-the-art meta-learning algorithm. On a diverse set of 8 out of 11 meta-learning benchmarks-and without meta-training or fine-tuning-CAML outperforms or matches the performance of P>M>F (Hu et al., 2022), a state-of-the-art meta-learning algorithm that is meta-trained on each benchmark.",
        "id": 264172174
      }
    ],
    "negative_ctxs": [
      {
        "title": "CONSTRUCTIBLE REPRESENTATIONS FOR TWO SEMANTIC RELATIONS",
        "text": "",
        "id": 16451235
      },
      {
        "title": "Extracting MWEs from Italian corpora: A case study for refining the POS-pattern methodology",
        "text": "An established method for MWE extraction is the combined use of previously identified POS-patterns and association measures. However, the selection of such POSpatterns is rarely debated. Focusing on Italian MWEs containing at least one adjective, we set out to explore how candidate POS-patterns listed in relevant literature and lexicographic sources compare with POS sequences exhibited by statistically significant n-grams including an adjective position extracted from a large corpus of Italian. All literature-derived patterns are found-and new meaningful candidate patterns emerge-among the top-ranking trigrams for three association measures. We conclude that a final solid set to be used for MWE extraction will have to be further refined through a combination of association measures as well as manual inspection.",
        "id": 6492462
      },
      {
        "title": "Resource Interoperability for Sustainable Benchmarking: The Case of Events",
        "text": "With the continuous growth of benchmark corpora, which often annotate the same documents, there is a range of opportunities to compare and combine similar and complementary annotations. However, these opportunities are hampered by a wide range of problems that are related to the lack of resource interoperability. In this paper, we illustrate these problems by assessing aspects of interoperability at the document-level across a set of 20 corpora annotated with (aspects of) events. The issues range from applying different document naming conventions, to mismatches in textual content and structural/conceptual differences among annotation schemes. We provide insight into the exact document intersections between the corpora by mapping their document identifiers and perform an empirical analysis of event annotations showing their compatibility and consistency in and across the corpora. This way, we aim to make the community more aware of the challenges and opportunities and to inspire working collaboratively towards interoperable resources.",
        "id": 21697471
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_acl",
    "question": "Can you point me to studies discussing methods for evaluating text generation models on various dimensions? I'm particularly interested in models like T5 and FLAN-T5, and how to assess their performance on summary-level and turn-level tasks.",
    "positive_ctxs": [
      {
        "title": "Towards a Unified Multi-Dimensional Evaluator for Text Generation",
        "text": "Multi-dimensional evaluation is the dominant paradigm for human evaluation in Natural Language Generation (NLG), i.e., evaluating the generated text from multiple explainable dimensions, such as coherence and fluency. However, automatic evaluation in NLG is still dominated by similarity-based metrics, and we lack a reliable framework for a more comprehensive evaluation of advanced models. In this paper, we propose a unified multi-dimensional evaluator UNIEVAL for NLG. We re-frame NLG evaluation as a Boolean Question Answering (QA) task, and by guiding the model with different questions, we can use one evaluator to evaluate from multiple dimensions. Furthermore, thanks to the unified Boolean QA format, we are able to introduce an intermediate learning phase that enables UNIEVAL to incorporate external knowledge from multiple related tasks and gain further improvement. Experiments on three typical NLG tasks show that UNIEVAL correlates substantially better with human judgments than existing metrics. Specifically, compared to the top-performing unified evaluators, UNIEVAL achieves a 23% higher correlation on text summarization, and over 43% on dialogue response generation. Also, UNIEVAL demonstrates a strong zero-shot learning ability for unseen evaluation dimensions and tasks. Source code, data and all pre-trained evaluators are available on our GitHub repository 1 .2023Generated Summary: Harry Kane is nominated for both the PFA player and young player of the season. The Spurs striker has been released from the awards ceremony on Sunday. The Tottenham striker features in a new animation. Reference Summary: Harry Kane has been in superb form for Tottenham this season. The 21-year-old has scored 30 goals in all competitions for Spurs. Kane also made his England debut and scored within two minutes. Document: Harry Kane's celebrations this season have always shown him to be an animated young man . . .Similarity-based EvaluatorsROUGE-1: 0.44 ROUGE-2: 0.25 ROUGE-L: 0.42 BERTScore: 0.24 Single-dimensional Evaluators (predicted by two different evaluators (Deng et al., 2021)) Consistency: 0.87 Relevance: 0.74Unified Evaluator (predicted by BARTScore, and the scoring range is negative infinity to 0)",
        "id": 252873117
      }
    ],
    "negative_ctxs": [
      {
        "title": "EXCOM : Plate-forme d'annotation sémantique de textes multilingues",
        "text": "Nous proposons une plateforme d\"annotation sémantique, appelée « EXCOM ».Basée sur la méthode de l\" « Exploration Contextuelle », elle permet, à travers une diversité de langues, de procéder à des annotations automatiques de segments textuels par l'analyse des formes de surface dans leur contexte. Les textes sont traités selon des « points de vue » discursifs dont les valeurs sont organisées dans une « carte sémantique ». L\"annotation se base sur un ensemble de règles linguistiques, écrites par un analyste, qui permettent d\"identifier les représentations textuelles sous-jacentes aux différentes catégories de la carte. Le système offre, à travers deux types d\"interfaces (développeur ou utilisateur), une chaîne de traitements automatiques de textes qui comprend la segmentation, l\"annotation et d\"autres fonctionnalités de post-traitement. Les documents annotés peuvent être utilisés, par exemple, pour des systèmes de recherche d\"information, de veille, de classification ou de résumé automatique.AbstractWe propose a platform for semantic annotation, called \"EXCOM\". Based on the \"Contextual Exploration\" method, it enables, across a great range of languages, to perform automatic annotations of textual segments by analyzing surface forms in their context. Texts are approached through discursive \"points of view\", of which values are organized into a \"semantic map\". The annotation is based on a set of linguistic rules, manually constructed by an analyst, and that enables to automatically identify the textual representations underlying the different semantic categories of the map. The system provides through two sorts of userfriendly interfaces (analyst or end-user) a complete pipeline of automatic text processing which consists of segmentation, annotation and other post-processing functionalities. Annotated documents can be used, for instance, for information retrieval systems, classification or automatic summarization.",
        "id": 60424971
      },
      {
        "title": "An Unsupervised Dynamic Bayesian Network Approach to Measuring Speech Style Accommodation",
        "text": "Speech style accommodation refers to shifts in style that are used to achieve strategic goals within interactions. Models of stylistic shift that focus on specific features are limited in terms of the contexts to which they can be applied if the goal of the analysis is to model socially motivated speech style accommodation. In this paper, we present an unsupervised Dynamic Bayesian Model that allows us to model stylistic style accommodation in a way that is agnostic to which specific speech style features will shift in a way that resembles socially motivated stylistic variation. This greatly expands the applicability of the model across contexts. Our hypothesis is that stylistic shifts that occur as a result of social processes are likely to display some consistency over time, and if we leverage this insight in our model,we will achieve a model that better captures inherent structure within speech.",
        "id": 15242587
      },
      {
        "title": "Title = {Automatic and Human Evaluation of Local Topic Automatic and Human Evaluation of Local Topic Quality",
        "text": "Topic models are typically evaluated with respect to the global topic distributions that they generate, using metrics such as coherence, but without regard to local (token-level) topic assignments. Token-level assignments are important for downstream tasks such as classification. Recent models, which claim to improve token-level topic assignments, are only validated on global metrics. We elicit human judgments of token-level topic assignments: over a variety of topic model types and parameters, global metrics agree poorly with human assignments. Since human evaluation is expensive we propose automated metrics to evaluate topic models at a local level. Finally, we correlate our proposed metrics with human judgments: an evaluation based on the percent of topic switches correlates most strongly with human judgment of local topic quality. This new metric, which we call consistency, should be adopted alongside global metrics such as topic coherence.",
        "id": 170079259
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "manual_acl",
    "question": "Is there any paper that leverages syntactic rules to explicitly guide text generation?",
    "positive_ctxs": [
      {
        "title": "Explicit Syntactic Guidance for Neural Text Generation",
        "text": "Most existing text generation models follow the sequence-to-sequence paradigm. Generative Grammar suggests that humans generate natural language texts by learning language grammar. We propose a syntax-guided generation schema, which generates the sequence guided by a constituency parse tree in a topdown direction. The decoding process can be decomposed into two parts: (1) predicting the infilling texts for each constituent in the lexicalized syntax context given the source sentence;(2) mapping and expanding each constituent to construct the next-level syntax context. Accordingly, we propose a structural beam search method to find possible syntax structures hierarchically. Experiments on paraphrase generation and machine translation show that the proposed method outperforms autoregressive baselines, while also demonstrating effectiveness in terms of interpretability, controllability, and diversity.",
        "id": 259203682
      }
    ],
    "negative_ctxs": [
      {
        "title": "Predicting Literary Quality How Perspectivist Should We Be?",
        "text": "Approaches in literary quality tend to belong to two main grounds: one sees quality as completely subjective, relying on the idiosyncratic nature of individual perspectives on the perception of beauty; the other is ground-truth inspired, and attempts to find one or two values that predict something like an objective quality: the number of copies sold, for example, or the winning of a prestigious prize. While the first school usually does not try to predict quality at all, the second relies on a single majority vote in one form or another. In this article we discuss the advantages and limitations of these schools of thought and describe a different approach to reader's quality judgments, which moves away from raw majority vote, but does try to create intermediate classes or groups of annotators. Drawing on previous works we describe the benefits and drawbacks of building similar annotation classes. Finally we share early results from a large corpus of literary reviews for an insight into which classes of readers might make most sense when dealing with the appreciation of literary quality.",
        "id": 252624715
      },
      {
        "title": "Curriculum Learning Based on Reward Sparseness for Deep Reinforcement Learning of Task Completion Dialogue Management",
        "text": "Learning from sparse and delayed reward is a central issue in reinforcement learning. In this paper, to tackle reward sparseness problem of task oriented dialogue management, we propose a curriculum based approach on the number of slots of user goals. This curriculum makes it possible to learn dialogue management for sets of user goals with large number of slots. We also propose a dialogue policy based on progressive neural networks whose modules with parameters are appended with previous parameters fixed as the curriculum proceeds, and this policy improves performances over the one with single set of parameters.",
        "id": 53234787
      },
      {
        "title": "Domain-Adaptable Hybrid Generation of RDF Entity Descriptions",
        "text": "RDF ontologies provide structured data on entities in many domains and continue to grow in size and diversity. While they can be useful as a starting point for generating descriptions of entities, they often miss important information about an entity that cannot be captured as simple relations. In addition, generic approaches to generation from RDF cannot capture the unique style and content of specific domains. We describe a framework for hybrid generation of entity descriptions, which combines generation from RDF data with text extracted from a corpus, and extracts unique aspects of the domain from the corpus to create domain-specific generation systems. We show that each component of our approach significantly increases the satisfaction of readers with the text across multiple applications and domains.",
        "id": 29910620
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research that investigates the enhancement of neural passage retrieval through the application of dual encoders and the generation of synthetic questions?",
    "positive_ctxs": [
      {
        "title": "Zero-shot Neural Passage Retrieval via Domain-targeted Synthetic Question Generation",
        "text": "A major obstacle to the wide-spread adoption of neural retrieval models is that they require large supervised training sets to surpass traditional term-based techniques, which are constructed from raw corpora. In this paper, we propose an approach to zero-shot learning for passage retrieval that uses synthetic question generation to close this gap. The question generation system is trained on general domain data, but is applied to documents in the targeted domain. This allows us to create arbitrarily large, yet noisy, question-passage relevance pairs that are domain specific. Furthermore, when this is coupled with a simple hybrid termneural model, first-stage retrieval performance can be improved further. Empirically, we show that this is an effective strategy for building neural passage retrieval models in the absence of large training corpora. Depending on the domain, this technique can even approach the accuracy of supervised models.",
        "id": 231704318
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 219732655
      },
      {
        "title": "Parsing with the Shortest Derivation",
        "text": "Common wisdom has it that tile bias of stochastic grammars in favor of shorter deriwttions of a sentence is hamfful and should be redressed. We show that the common wisdom is wrong for stochastic grammars that use elementary trees instead o1' conlext-l'ree rules, such as Stochastic Tree-Substitution Grammars used by Data-Oriented Parsing models. For such grammars a non-probabilistic metric based on tile shortest derivation outperforms a probabilistic metric on the ATIS and OVIS corpora, while it obtains competitive results on the Wall Street Journal (WSJ) corpus. This paper also contains the first publislmd experiments with DOP on the WSJ.",
        "id": 1051919
      },
      {
        "title": "DIDEC: The Dutch Image Description and Eye-tracking Corpus",
        "text": "We present a corpus of spoken Dutch image descriptions, paired with two sets of eye-tracking data: free viewing, where participants look at images without any particular purpose, and description viewing, where we track eye movements while participants produce spoken descriptions of the images they are viewing. This paper describes the data collection procedure and the corpus itself, and provides an initial analysis of self-corrections in image descriptions. We also present two studies showing the potential of this data. Though these studies mainly serve as an example, we do find two interesting results: (1) the eye-tracking data for the description viewing task is more coherent than for the free-viewing task; (2) variation in image descriptions (also called image specificity; Jas and Parikh, 2015) is only moderately correlated across different languages. Our corpus can be used to gain a deeper understanding of the image description task, particularly how visual attention is correlated with the image description process.Title and Abstract in DutchDIDEC: Een corpus van afbeeldingen met Nederlandstalige beschrijvingen en eye-tracking data Wij presenteren DIDEC, een corpus van foto's met gesproken Nederlandse beschrijvingen en twee verschillende soorten eye-tracking data: ofwel verzameld tijdens het beschrijven van de afbeeldingen, ofwel verzameld terwijl de proefpersonen alleen maar keken naar de afbeeldingen (zonder ze te hoeven beschrijven). Dit artikel beschrijft de dataverzameling, alsook een eerste analyse van de zelf-correcties in de beschrijvingen. Daarnaast beschrijven we twee voorbeeldstudies om aan te geven wat er mogelijk is met DIDEC. Deze studies geven twee interessante resultaten: (1) de eye-tracking data verzameld tijdens het beschrijven van de afbeeldingen is eenduidiger dan de data verzameld tijdens het bekijken van de afbeeldingen; (2) de variatie in de beschrijvingen voor iedere afbeelding (ook wel afbeeldingsspecificiteit genoemd; Jas and Parikh, 2015) is slechts matig gecorreleerd tussen verschillende talen (Duits, Nederlands, Engels). Ons corpus kan gebruikt worden om beter te begrijpen hoe mensen afbeeldingen beschrijven, en in het bijzonder wat de rol is van visuele aandacht op het beschrijvingsproces. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 1. Free viewing: eye-tracking data collected without any concurrent task. 2. Description viewing: eye-tracking data collected simultaneously with the spoken descriptions.",
        "id": 52009933
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that investigates applying combinatorial optimization techniques in unsupervised entity matching?",
    "positive_ctxs": [
      {
        "title": "From Alignment to Assignment: Frustratingly Simple Unsupervised Entity Alignment",
        "text": "Cross-lingual entity alignment (EA) aims to find the equivalent entities between crosslingual KGs (Knowledge Graphs), which is a crucial step for integrating KGs. Recently, many GNN-based EA methods are proposed and show decent performance improvements on several public datasets. However, existing GNN-based EA methods inevitably inherit poor interpretability and low efficiency from neural networks. Motivated by the isomorphic assumption of GNN-based methods, we successfully transform the cross-lingual EA problem into an assignment problem. Based on this re-definition, we propose a frustratingly Simple but Effective Unsupervised entity alignment method (SEU) without neural networks. Extensive experiments have been conducted to show that our proposed unsupervised approach even beats advanced supervised methods across all public datasets while having high efficiency, interpretability, and stability.",
        "id": 237420821
      }
    ],
    "negative_ctxs": [
      {
        "title": "The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 80-85, PREFER: Using a Graph-Based Approach to Generate Paraphrases for Language Learning",
        "text": "Paraphrasing is an important aspect of language competence; however, EFL learners have long had difficulty paraphrasing in their writing owing to their limited language proficiency. Therefore, automatic paraphrase suggestion systems can be useful for writers. In this paper, we present PREFER 1 , a paraphrase reference tool for helping language learners improve their writing skills. In this paper, we attempt to transform the paraphrase generation problem into a graphical problem in which the phrases are treated as nodes and translation similarities as edges. We adopt the PageRank algorithm to rank and filter the paraphrases generated by the pivot-based paraphrase generation method. We manually evaluate the performance of our method and assess the effectiveness of PREFER in language learning. The results show that our method successfully preserves both the semantic meaning and syntactic structure of the query phrase. Moreover, the students' writing performance improve most with the assistance of PREFER.",
        "id": 1326531
      },
      {
        "title": "Box Embeddings: An open-source library for representation learning using geometric structures",
        "text": "A major factor contributing to the success of modern representation learning is the ease of performing various vector operations. Recently, objects with geometric structures (eg. distributions, complex or hyperbolic vectors, or regions such as cones, disks, or boxes) have been explored for their alternative inductive biases and additional representational capacities.",
        "id": 237485374
      },
      {
        "title": "",
        "text": "",
        "id": 218973754
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "manual_iclr",
    "question": "Is there a paper illustrating that pre-trained transformers from LLMs can be used to encode visual information in a wide range of scenarios?",
    "positive_ctxs": [
      {
        "title": "FROZEN TRANSFORMERS IN LANGUAGE MODELS ARE EFFECTIVE VISUAL ENCODER LAYERS",
        "text": "This paper reveals that large language models (LLMs), despite being trained solely on textual data, are surprisingly strong encoders for purely visual tasks in the absence of language.Even more intriguingly, this can be achieved by a simple yet previously overlooked strategy -employing a frozen transformer block from pre-trained LLMs as a constituent encoder layer to directly process visual tokens.Our work pushes the boundaries of leveraging LLMs for computer vision tasks, significantly departing from conventional practices that typically necessitate a multi-modal vision-language setup with associated language prompts, inputs, or outputs.We demonstrate that our approach consistently enhances performance across a diverse range of tasks, encompassing pure 2D and 3D visual recognition tasks (e.g., image and point cloud classification), temporal modeling tasks (e.g., action recognition), non-semantic tasks (e.g., motion forecasting), and multi-modal tasks (e.g., 2D/3D visual question answering and image-text retrieval).Such improvements are a general phenomenon, applicable to various types of LLMs (e.g., LLaMA and OPT) and different LLM transformer blocks.We additionally propose the information filtering hypothesis to explain the effectiveness of pre-trained LLMs in visual encoding -the pre-trained LLM transformer blocks discern informative visual tokens and further amplify their effect.This hypothesis is empirically supported by the observation that the feature activation, after training with LLM transformer blocks, exhibits a stronger focus on relevant regions.We hope that our work inspires new perspectives on utilizing LLMs and deepening our understanding of their underlying mechanisms.Code is available at https://github.com/ziqipang/LM4VisualEncoding.",
        "id": 264306111
      }
    ],
    "negative_ctxs": [
      {
        "title": "Published as a conference paper at ICLR 2022 PRIORGRAD: IMPROVING CONDITIONAL DENOISING DIFFUSION MODELS WITH DATA-DEPENDENT ADAP- TIVE PRIOR",
        "text": "Denoising diffusion probabilistic models have been recently proposed to generate high-quality samples by estimating the gradient of the data density. The framework defines the prior noise as a standard Gaussian distribution, whereas the corresponding data distribution may be more complicated than the standard Gaussian distribution, which potentially introduces inefficiency in denoising the prior noise into the data sample because of the discrepancy between the data and the prior. In this paper, we propose PriorGrad to improve the efficiency of the conditional diffusion model for speech synthesis (for example, a vocoder using a mel-spectrogram as the condition) by applying an adaptive prior derived from the data statistics based on the conditional information. We formulate the training and sampling procedures of PriorGrad and demonstrate the advantages of an adaptive prior through a theoretical analysis. Focusing on the speech synthesis domain, we consider the recently proposed diffusion-based speech generative models based on both the spectral and time domains and show that PriorGrad achieves faster convergence and inference with superior performance, leading to an improved perceptual quality and robustness to a smaller network capacity, and thereby demonstrating the efficiency of a data-dependent adaptive prior. * Work done during an internship at Microsoft Research Asia † Corresponding Authors Published as a conference paper at ICLR 2022 However, although the diffusion-based speech synthesis models have achieved high-quality speech audio generation, they exhibit potential inefficiency, which may necessitate advanced strategies. For example, the model suffers from a significantly slow convergence during training, and a prohibitively large training computation time is required to learn the approximate reverse diffusion process. We investigate the diffuion-based models and observe the discrepancy between the real data distribution and the choice of the prior. Existing diffusion-based models define a standard Gaussian as the prior distribution and design a non-parametric diffusion process that procedurally destroys the signal into the prior noise. The deep neural network is trained to approximate the reverse diffusion process by estimating the gradient of the data density. Although applying the standard Gaussian as the prior is simple without any assumptions on the target data, it also introduces inefficiency. For example, in time-domain waveform data, the signal has extremely high variability between different segments such as voiced and unvoiced parts. Jointly modeling the voiced and unvoiced segments with the same standard Gaussian prior may be difficult for the model to cover all modes of the data, leading to training inefficiencies and potentially spurious diffusion trajectories.Given the previous reasoning, we assessed the following question: For a conditional diffusion-based model, can we formulate a more informative prior without incorporating additional computational or parameter complexity? To investigate this, we propose a simple yet effective method, called PriorGrad, that uses adaptive noise by directly computing the mean and variance for the forward diffusion process prior, based on the conditional information. Specifically, using a conditional speech synthesis model, we propose structuring the prior distribution based on the conditional data, such as a mel-spectrogram for the vocoder  Kong et al., 2021)  and a phoneme for the acoustic model(Jeong et al., 2021). By computing the statistics from the conditional data at the frame level (vocoder) or phoneme-level (acoustic model) granularity and mapping them as the mean and variance of the Gaussian prior, we can structure the noise that is similar to the target data distribution at an instance level, easing the burden of learning the reverse diffusion process.We implemented PriorGrad based on the recently proposed diffusion-based speech generative models (Kong et al., 2021;Jeong et al., 2021), and conducted experiments on the LJSpeech (Ito & Johnson, 2017) dataset. The experimental results demonstrate the benefits of Prior-Grad, such as a significantly faster model convergence during training, improved perceptual quality, and an improved tolerance to a reduction in network capacity. Our contributions are as follows:",
        "id": 247011539
      },
      {
        "title": "LINGUISTIC ANALYSIS OF NATURAL LANGUAGE COMMUNICATION WITH COMPUTERS",
        "text": "",
        "id": 1010309
      },
      {
        "title": "Standard and Nonstandard Lexicon in Aviation English: A Corpus Linguistic Study",
        "text": "This study aims at investigating the lexical items in Aviation Phraseology that has both standard and nonstandard meanings when Pilot and Air Traffic Controller (ATC) use them in radiotelephony.",
        "id": 52140006
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "manual_acl",
    "question": "How to faithfully and explicitly measure the helpfulness of human explanations to language models during finetuning and inference?",
    "positive_ctxs": [
      {
        "title": "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations",
        "text": "Human-annotated labels and explanations are critical for training explainable NLP models. However, unlike human-annotated labels whose quality is easier to calibrate (e.g., with a majority vote), human-crafted free-form explanations can be quite subjective. Before blindly using them as ground truth to train ML models, a vital question needs to be asked: How do we evaluate a human-annotated explanation's quality? In this paper, we build on the view that the quality of a human-annotated explanation can be measured based on its helpfulness (or impairment) to the ML models' performance for the desired NLP tasks for which the annotations were collected. In comparison to the commonly used Simulatability score, we define a new metric that can take into consideration of the helpfulness of an explanation for model performance at both fine-tuning and inference. With the help of a unified dataset format, we evaluated the proposed metric on five datasets (e.g., e-SNLI) against two model architectures (T5 and BART), and the results show that our proposed metric can objectively evaluate the quality of human-annotated explanations, while Simulatability falls short.",
        "id": 258546862
      }
    ],
    "negative_ctxs": [
      {
        "title": "CIC-FBK Approach to Native Language Identification",
        "text": "We present the CIC-FBK system, which took part in the Native Language Identification (NLI) Shared Task 2017. Our approach combines features commonly used in previous NLI research, i.e., word n-grams, lemma n-grams, part-of-speech n-grams, and function words, with recently introduced character n-grams from misspelled words, and features that are novel in this task, such as typed character n-grams, and syntactic n-grams of words and of syntactic relation tags. We use log-entropy weighting scheme and perform classification using the Support Vector Machines (SVM) algorithm. Our system achieved 0.8808 macro-averaged F1-score and shared the 1 st rank in the NLI Shared Task 2017 scoring.",
        "id": 1615409
      },
      {
        "title": "The Norwegian Dependency Treebank",
        "text": "The Norwegian Dependency Treebank is a new syntactic treebank for Norwegian Bokmål and Nynorsk with manual syntactic and morphological annotation, developed at the National Library of Norway in collaboration with the University of Oslo. It is the first publically available treebank for Norwegian. This paper presents the core principles behind the syntactic annotation and how these principles were employed in certain specific cases. We then present the selection of texts and distribution between genres, as well as the annotation process and an evaluation of the inter-annotator agreement. Finally, we present the first results of data-driven dependency parsing of Norwegian, contrasting four state-of-the-art dependency parsers trained on the treebank. The consistency and the parsability of this treebank is shown to be comparable to other large treebank initiatives.",
        "id": 7770967
      },
      {
        "title": "SINotas: the Evaluation of a NLG Application",
        "text": "SINotas is a data-to-text NLG application intended to produce short textual reports on students' academic performance from a database conveying their grades, weekly attendance rates and related academic information. Although developed primarily as a testbed for Portuguese Natural Language Generation, SINotas generates reports of interest to both students keen to learn how their professors would describe their efforts, and to the professors themselves, who may benefit from an at-a-glance view of the student's performance. In a traditional machine learning approach, SINotas uses a data-text aligned corpus as training data for decision-tree induction. The current system comprises a series of classifiers that implement major Document Planning subtasks (namely, data interpretation, content selection, within-and between-sentence structuring), and a small surface realisation grammar of Brazilian Portuguese. In this paper we focus on the evaluation work of the system, applying a number of intrinsic and user-based evaluation metrics to a collection of text reports generated from real application data.",
        "id": 1016733
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Have any research papers investigated the creation of datasets through model-generated data for annotators to identify hallucinations in the results, specifically for developing diagnostic evaluation datasets?",
    "positive_ctxs": [
      {
        "title": "Q 2 : Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering",
        "text": "Neural knowledge-grounded generative models for dialogue often produce content that is factually inconsistent with the knowledge they rely on, making them unreliable and limiting their applicability. Inspired by recent work on evaluating factual consistency in abstractive summarization, we propose an automatic evaluation metric for factual consistency in knowledge-grounded dialogue using automatic question generation and question answering. Our metric, denoted Q 2 , compares answer spans using natural language inference (NLI), instead of token-based matching as done in previous work. To foster proper evaluation, we curate a novel dataset of dialogue system outputs for the Wizard-of-Wikipedia dataset, manually annotated for factual consistency. We perform a thorough meta-evaluation of Q 2 against other metrics using this dataset and two others, where it consistently shows higher correlation with human judgements.",
        "id": 233289483
      }
    ],
    "negative_ctxs": [
      {
        "title": "Annotating Educational Questions for Student Response Analysis",
        "text": "Questions play an important role in the educational domain, representing the main form of interaction between instructors and students. In this paper, we introduce the first taxonomy and annotated educational corpus of questions that aims to help with the analysis of student responses. The dataset can be employed in approaches that classify questions based on the expected answer types. This can be an important component in applications that require prior knowledge about the desired answer to a given question, such as educational and question answering systems. To demonstrate the applicability and the effectiveness of the data within approaches to classify questions based on expected answer types, we performed extensive experiments on our dataset using a neural network with word embeddings as features. The approach achieved a weighted F1-score of 0.511, overcoming the baseline by 12%. This demonstrates that our corpus can be effectively integrated in simple approaches that classify questions based on the response type.",
        "id": 21730476
      },
      {
        "title": "Statistical Models for Unsupervised Prepositional Phrase Attachment",
        "text": "We present several unsupervised statistical models for the prepositional phrase attachment task that approach the accuracy of the best supervised methods for this task. Our unsupervised approach uses a heuristic based on attachment proximity and trains from raw text that is annotated with only part-of-speech tags and morphological base forms, as opposed to attachment information. It is therefore less resource-intensive and more portable than previous corpus-based algorithm proposed for this task.We present results for prepositional phrase attachment in both English and Spanish.",
        "id": 2751975
      },
      {
        "title": "Speech Technology for Everyone: Automatic Speech Recognition for Non-Native English with Transfer Learning",
        "text": "To address the performance gap of English ASR models on L2 English speakers, we evaluate fine-tuning of pretrained wav2vec 2.0 models (Baevski et al., 2020;Xu et al., 2021)on L2-ARCTIC, a non-native English speech corpus(Zhao et al., 2018)under different training settings. We compare (a) models trained with a combination of diverse accents to ones trained with only specific accents and (b) results from different single-accent models. Our experiments demonstrate the promise of developing ASR models for non-native English speakers, even with small amounts of L2 training data and even without a language model. Our models also excel in the zero-shot setting where we train on multiple L2 datasets and test on a blind L2 test set.",
        "id": 238259710
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "What techniques exist for incorporating context in detecting emotions within dialogues by leveraging pre-trained language models?",
    "positive_ctxs": [
      {
        "title": "Context-Dependent Embedding Utterance Representations for Emotion Recognition in Conversations",
        "text": "Emotion Recognition in Conversations(ERC)has been gaining increasing importance as conversational agents become more and more common. Recognizing emotions is key for effective communication, being a crucial component in the development of effective and empathetic conversational agents. Knowledge and understanding of the conversational context are extremely valuable for identifying the emotions of the interlocutor. We thus approach Emotion Recognition in Conversations leveraging the conversational context, i.e., taking into attention previous conversational turns. The usual approach to model the conversational context has been to produce context-independent representations of each utterance and subsequently perform contextual modeling of these. Here we propose context-dependent embedding representations of each utterance by leveraging the contextual representational power of pretrained transformer language models. In our approach, we feed the conversational context appended to the utterance to be classified as input to the RoBERTa encoder, to which we append a simple classification module, thus discarding the need to deal with context after obtaining the embeddings since these constitute already an efficient representation of such context. We also investigate how the number of introduced conversational turns influences our model performance. The effectiveness of our approach is validated on the open-domain DailyDialog dataset and on the task-oriented EmoWOZ dataset.",
        "id": 258179794
      }
    ],
    "negative_ctxs": [
      {
        "title": "A-Team / Martin-Luther-Universität Halle-Wittenberg@CLSciSumm 20",
        "text": "This document demonstrates our groups approach to the CL-SciSumm shared task 2020(Chandrasekaran et al., 2020). There are three tasks in CL-SciSumm 2020. In Task 1a, we apply a Siamese neural network to identify the spans of text in the reference paper best reflecting a citation. In Task 1b, we use a SVM to classify the facet of a citation.",
        "id": 226283866
      },
      {
        "title": "A Framework for Identity Resolution and Merging for Multi-source Information Extraction",
        "text": "In the context of ontology-based information extraction, identity resolution is the process of deciding whether an instance extracted from text refers to a known entity in the target domain (e.g. the ontology). We present an ontology-based framework for identity resolution which can be customised to different application domains and extraction tasks. Rules for identify resolution, which compute similarities between target and source entities based on class information and instance properties and values, can be defined for each class in the ontology. We present a case study of the application of the framework to the problem of multi-source job vacancy extraction.",
        "id": 6353943
      },
      {
        "title": "An Interface for Rapid Natural Language Processing Development in UIMA",
        "text": "This demonstration presents the Annotation Librarian, an application programming interface that supports rapid development of natural language processing (NLP) projects built in Apache Unstructured Information Management Architecture (UIMA). The flexibility of UIMA to support all types of unstructured data -images, audio, and text -increases the complexity of some of the most common NLP development tasks. The Annotation Librarian interface handles these common functions and allows the creation and management of annotations by mirroring Java methods used to manipulate Strings. The familiar syntax and NLP-centric design allows developers to adopt and rapidly develop NLP algorithms in UIMA. The general functionality of the interface is described in relation to the use cases that necessitated its creation.",
        "id": 7525432
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_acl",
    "question": "Are there any studies that explore post-hoc techniques for hallucination detection at both the token- and sentence-level in neural sequence generation tasks?",
    "positive_ctxs": [
      {
        "title": "Detecting Hallucinated Content in Conditional Neural Sequence Generation",
        "text": "Neural sequence models can generate highly fluent sentences, but recent studies have also shown that they are also prone to hallucinate additional content not supported by the input. These variety of fluent but wrong outputs are particularly problematic, as it will not be possible for users to tell they are being presented incorrect content. To detect these errors, we propose a task to predict whether each token in the output sequence is hallucinated (not contained in the input) and collect new manually annotated evaluation sets for this task. We also introduce a method for learning to detect hallucinations using pretrained language models fine tuned on synthetic data that includes automatically inserted hallucinations Experiments on machine translation (MT) and abstractive summarization demonstrate that our proposed approach consistently outperforms strong baselines on all benchmark datasets. We further demonstrate how to use the token-level hallucination labels to define a fine-grained loss over the target sequence in low-resource MT and achieve significant improvements over strong baseline methods.We also apply our method to word-level quality estimation for MT and show its effectiveness in both supervised and unsupervised settings 1 .",
        "id": 226254579
      },
      {
        "title": "Evaluating the Factual Consistency of Abstractive Text Summarization",
        "text": "The most common metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and generated summaries. Training data is generated by applying a series of rule-based transformations to the sentences of source documents. The factual consistency model is then trained jointly for three tasks: 1) predict whether each summary sentence is factually consistent or not, 2) in either case, extract a span in the source document to support this consistency prediction, 3) for each summary sentence that is deemed inconsistent, extract the inconsistent span from it. Transferring this model to summaries generated by several neural models reveals that this highly scalable approach outperforms previous models, including those trained with strong supervision using datasets from related domains, such as natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency. We also release a manually annotated dataset for factual consistency verification, code for training data generation, and trained model weights at https://github.com/salesforce/factCC.",
        "id": 204976362
      }
    ],
    "negative_ctxs": [
      {
        "title": "Mining the Web for Discourse Markers",
        "text": "This paper proposes a methodology for obtaining sentences containing discourse markers from the World Wide Web. The proposed methodology is particularly suitable for collecting large numbers of discourse marker tokens. It relies on the automatic identification of discourse markers, and we show that this can be done with an accuracy within 9% of that of human performance. We also show that the distribution of discourse markers on the web correlates highly with those in a conventional balanced corpus.",
        "id": 16720635
      },
      {
        "title": "Behavior Analysis of NLI Models: Uncovering the Influence of Three Factors on Robustness",
        "text": "Natural Language Inference is a challenging task that has received substantial attention, and state-of-the-art models now achieve impressive test set performance in the form of accuracy scores. Here, we go beyond this single evaluation metric to examine robustness to semantically-valid alterations to the input data. We identify three factorsinsensitivity, polarity and unseen pairs -and compare their impact on three SNLI models under a variety of conditions. Our results demonstrate a number of strengths and weaknesses in the models' ability to generalise to new in-domain instances. In particular, while strong performance is possible on unseen hypernyms, unseen antonyms are more challenging for all the models. More generally, the models suffer from an insensitivity to certain small but semantically significant alterations, and are also often influenced by simple statistical correlations between words and training labels. Overall, we show that evaluations of NLI models can benefit from studying the influence of factors intrinsic to the models or found in the dataset used.",
        "id": 21677829
      },
      {
        "title": "A Text-based Method for Detection and Filtering of Commercial Segments in Broadcast News",
        "text": "Story segmentation is an important problem in multimedia indexing and retrieval and includes detection of commercials as one of its component problems. Commercials appear regularly in television data and are usually treated as noise. Hence, filtering of commercials is an important task. This paper presents a system that detects and filters commercials from broadcast news data. While previous work in the area relies largely on features from audio, video and captions, the system described in this paper uses just closed caption text to perform this task. An evaluation of this system is also presented which shows comparable performance with other methods.",
        "id": 2041910
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a study that investigates incorporating a fact memory component into neural networks to improve language modeling activities without requiring retraining or fine-tuning?",
    "positive_ctxs": [
      {
        "title": "Adaptable and Interpretable Neural Memory Over Symbolic Knowledge",
        "text": "Past research has demonstrated that large neural language models (LMs) encode surprising amounts of factual information: however, augmenting or modifying this information requires modifying a corpus and retraining, which is computationally expensive. To address this problem, we develop a neural LM that includes an interpretable neuro-symbolic KB in the form of a \"fact memory\". Each element of the fact memory is formed from a triple of vectors, where each vector corresponds to a KB entity or relation. Our LM improves performance on knowledge-intensive question-answering tasks, sometimes dramatically, including a 27 point increase in one setting of WebQuestionsSP over a state-of-the-art open-book model, despite using 5% of the parameters. Most interestingly, we demonstrate that the model can be modified, without any re-training, by updating the fact memory.",
        "id": 235097242
      }
    ],
    "negative_ctxs": [
      {
        "title": "MixKMeans: Clustering Question-Answer Archives",
        "text": "Community-driven Question Answering (CQA) systems that crowdsource experiential information in the form of questions and answers and have accumulated valuable reusable knowledge.Clustering of QA datasets from CQA systems provides a means of organizing the content to ease tasks such as manual curation and tagging. In this paper, we present a clustering method that exploits the two-part question-answer structure in QA datasets to improve clustering quality. Our method, MixKMeans, composes question and answer space similarities in a way that the space on which the match is higher is allowed to dominate. This construction is motivated by our observation that semantic similarity between question-answer data (QAs) could get localized in either space. We empirically evaluate our method on a variety of real-world labeled datasets. Our results indicate that our method significantly outperforms stateof-the-art clustering methods for the task of clustering question-answer archives.",
        "id": 16906583
      },
      {
        "title": "DATA TYPES IN COMPUTATIONAL PHONOLOGY",
        "text": "This paper exanfines certain aspects of phonological structure from the viewpoint of ahstract data types, Our imnlediate goal is to find a format for l)honological representation which will be reasonably f,'fithful to the concerns of theoreti: cal phonology while I)eing rigorous enough to a(Irail a computational interl)retation. The longer term goal is to incorporate such representations into all appropriate general framework for llatnral language processing, i 1 4 9",
        "id": 6803114
      },
      {
        "title": "Senti-LSSVM: Sentiment-Oriented Multi-Relation Extraction with Latent Structural SVM",
        "text": "Extracting instances of sentiment-oriented relations from user-generated web documents is important for online marketing analysis. Unlike previous work, we formulate this extraction task as a structured prediction problem and design the corresponding inference as an integer linear program. Our latent structural SVM based model can learn from training corpora that do not contain explicit annotations of sentiment-bearing expressions, and it can simultaneously recognize instances of both binary (polarity) and ternary (comparative) relations with regard to entity mentions of interest. The empirical evaluation shows that our approach significantly outperforms stateof-the-art systems across domains (cameras and movies) and across genres (reviews and forum posts). The gold standard corpus that we built will also be a valuable resource for the community.",
        "id": 2825268
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "manual_acl",
    "question": "Is there a paper exploring the curse of multilinguality for similar languages?",
    "positive_ctxs": [
      {
        "title": "Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages",
        "text": "The NLP community has mainly focused on scaling Large Language Models (LLMs) vertically, i.e., making them better for about 100 languages. We instead scale LLMs horizontally: we create, through continued pretraining, Glot500-m, an LLM that covers 511 predominantly low-resource languages. An important part of this effort is to collect and clean Glot500-c, a corpus that covers these 511 languages and allows us to train Glot500-m. We evaluate Glot500-m on five diverse tasks across these languages. We observe large improvements for both high-resource and low-resource languages compared to an XLM-R baseline. Our analysis shows that no single factor explains the quality of multilingual LLM representations. Rather, a combination of factors determines quality including corpus size, script, \"help\" from related languages and the total capacity of the model. Our work addresses an important goal of NLP research: we should not limit NLP to a small fraction of the world's languages and instead strive to support as many languages as possible to bring the benefits of NLP technology to all languages and cultures. Code, data and models are available at https",
        "id": 258832427
      }
    ],
    "negative_ctxs": [
      {
        "title": "Effective Use of Transformer Networks for Entity Tracking",
        "text": "Tracking entities in procedural language requires understanding the transformations arising from actions on entities as well as those entities' interactions. While self-attention-based pre-trained language encoders like GPT and BERT have been successfully applied across a range of natural language understanding tasks, their ability to handle the nuances of procedural texts is still untested. In this paper, we explore the use of pre-trained transformer networks for entity tracking tasks in procedural text. First, we test standard lightweight approaches for prediction with pre-trained transformers, and find that these approaches underperform even simple baselines. We show that much stronger results can be attained by restructuring the input to guide the transformer model to focus on a particular entity. Second, we assess the degree to which transformer networks capture the process dynamics, investigating such factors as merged entities and oblique entity references. On two different tasks, ingredient detection in recipes and QA over scientific processes, we achieve state-ofthe-art results, but our models still largely attend to shallow context clues and do not form complex representations of intermediate entity or process state. 1 760Seq. of Steps water mixture sugarRoots absorb water from soil.M O OThe water flows to the leaf.M O OLight from the sun and CO 2 enter the leaf.E O OLight, water, and CO 2 combine into mixture.",
        "id": 202539208
      },
      {
        "title": "A Qualitative Comparison of Scientific and Journalistic Texts from the Perspective of Extracting Definitions",
        "text": "In this paper we highlight a selection of features of scientific text which distinguish it from news stories. We argue that features such as structure, selective use of past tense, voice and stylistic conventions can affect question answering in the scientific domain. We demonstrate this through qualitative observations made while working on retrieving definitions to terms related to salmon fish. to authors submitting papers to the journal Aquaculture (Elsevier Author Guide, 2003) specify the following required sections: Abstract,",
        "id": 15424937
      },
      {
        "title": "",
        "text": "",
        "id": 220330818
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "manual_acl",
    "question": "Is there any paper that proposes a set of criteria to comprehensively evaluate generated conversations?",
    "positive_ctxs": [
      {
        "title": "Modeling What-to-ask and How-to-ask for Answer-unaware Conversational Question Generation",
        "text": "Conversational Question Generation (CQG) is a critical task for machines to assist humans in fulfilling their information needs through conversations. The task is generally cast into two different settings: answer-aware and answerunaware. While the former facilitates the models by exposing the expected answer, the latter is more realistic and receiving growing attentions recently. What-to-ask and how-to-ask are the two main challenges in the answerunaware setting. To address the first challenge, existing methods mainly select sequential sentences in context as the rationales. We argue that the conversation generated using such naive heuristics may not be natural enough as in reality, the interlocutors often talk about the relevant contents that are not necessarily sequential in context. Additionally, previous methods decide the type of question (boolean/span-based) to be generated implicitly. Modeling the question type explicitly is crucial in this (answer-unaware) setting, as the answer which hints the models to generate a boolean or span-based question, is unavailable. To this end, we present SG-CQG, a two-stage CQG framework. For the what-to-ask stage, a sentence is selected as the rationale from a semantic graph that we construct, and extract the answer span from it. For the how-to-ask stage, a classifier determines the target answer type of the question via two explicit control signals before generating and filtering. In addition, we propose Conv-Distinct, a novel evaluation metric for CQG, to evaluate the diversity of the generated conversation from a context. Compared with the existing answer-unaware CQG models, the proposed SG-CQG achieves stateof-the-art performance.",
        "id": 258547050
      }
    ],
    "negative_ctxs": [
      {
        "title": "Improved Relation Classification by Deep Recurrent Neural Networks with Data Augmentation",
        "text": "Nowadays, neural networks play an important role in the task of relation classification. By designing different neural architectures, researchers have improved the performance to a large extent in comparison with traditional methods. However, existing neural networks for relation classification are usually of shallow architectures (e.g., one-layer convolutional neural networks or recurrent networks). They may fail to explore the potential representation space in different abstraction levels. In this paper, we propose deep recurrent neural networks (DRNNs) for relation classification to tackle this challenge. Further, we propose a data augmentation method by leveraging the directionality of relations. We evaluated our DRNNs on the SemEval-2010 Task 8, and achieve an F 1 -score of 86.1%, outperforming previous state-of-the-art recorded results. 1",
        "id": 7437060
      },
      {
        "title": "Cross-Cultural Transfer Learning for Text Classification",
        "text": "Large training datasets are required to achieve competitive performance in most natural language tasks. The acquisition process for these datasets is labor intensive, expensive, and time consuming. This process is also prone to human errors. In this work, we show that crosscultural differences can be harnessed for natural language text classification. We present a transfer-learning framework that leverages widely-available unaligned bilingual corpora for classification tasks, using no task-specific data. Our empirical evaluation on two tasks -formality classification and sarcasm detection -shows that the cross-cultural difference between German and American English, as manifested in product review text, can be applied to achieve good performance for formality classification, while the difference between Japanese and American English can be applied to achieve good performance for sarcasm detection -both without any task-specific labeled data.",
        "id": 202785347
      },
      {
        "title": "Effective Approach to Develop a Sentiment Annotator For Legal Domain in a Low Resource Setting",
        "text": "Analyzing the sentiments of legal opinions available in Legal Opinion Texts can facilitate several use cases such as legal judgement prediction, contradictory statements identification and party-based sentiment analysis. However, the task of developing a legal domain specific sentiment annotator is challenging due to resource constraints such as lack of domain specific labelled data and domain expertise. In this study, we propose novel techniques that can be used to develop a sentiment annotator for the legal domain while minimizing the need for manual annotations of data.",
        "id": 226226405
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "manual_acl",
    "question": "Is there a dataset that allows to perform aspect-based sentiment classification on French news?",
    "positive_ctxs": [
      {
        "title": "MAD-TSC: A Multilingual Aligned News Dataset for Target-dependent Sentiment Classification",
        "text": "Target-dependent sentiment classification (TSC) enables a fine-grained automatic analysis of sentiments expressed in texts. Sentiment expression varies depending on the domain, and it is necessary to create domain-specific datasets.While socially important, TSC in the news domain remains relatively understudied. We introduce MAD-TSC, the first multilingual aligned dataset designed for TSC in news. MAD-TSC differs substantially from existing resources. First, it includes aligned examples in eight languages to facilitate a comparison of performance for individual languages, and a direct comparison of human and machine translation. Second, the dataset is sampled from a diversified parallel news corpus, and is diversified in terms of news sources and geographic spread of entities. Finally, MAD-TSC is more challenging than existing datasets because its samples are more complex. We exemplify the use of MAD-TSC with comprehensive monolingual and multilingual experiments. The latter shows that machine translations can successfully replace manual ones, and that performance for all included languages can match that of English by automatically translating test examples.",
        "id": 259370666
      }
    ],
    "negative_ctxs": [
      {
        "title": "NLP-CIC-WFU at SocialDisNER: Disease Mention Extraction in Spanish Tweets Using Transfer Learning and Search by Propagation",
        "text": "Named entity recognition (e.g., disease mention extraction) is one of the most relevant tasks for data mining in the medical field. Although it is a well-known challenge, the bulk of the efforts to tackle this task have been made using clinical texts commonly written in English. In this work, we present our contribution to the SocialDisNER competition, which consists of a transfer learning approach to extracting disease mentions in a corpus from Twitter written in Spanish. We fine-tuned a model based on mBERT and applied post-processing using regular expressions to propagate the entities identified by the model and enhance disease mention extraction. Our system achieved a competitive strict F1 of 0.851 on the testing data set.",
        "id": 252819441
      },
      {
        "title": "A Unified Syntax-aware Framework for Semantic Role Labeling",
        "text": "Semantic role labeling (SRL) aims to recognize the predicate-argument structure of a sentence. Syntactic information has been paid a great attention over the role of enhancing SRL. However, the latest advance shows that syntax would not be so important for SRL with the emerging much smaller gap between syntax-aware and syntax-agnostic SRL. To comprehensively explore the role of syntax for SRL task, we extend existing models and propose a unified framework to investigate more effective and more diverse ways of incorporating syntax into sequential neural networks. Exploring the effect of syntactic input quality on SRL performance, we confirm that high-quality syntactic parse could still effectively enhance syntactically-driven SRL. Using empirically optimized integration strategy, we even enlarge the gap between syntax-aware and syntax-agnostic SRL. Our framework achieves state-of-the-art results on CoNLL-2009 benchmarks both for English and Chinese, substantially outperforming all previous models.",
        "id": 53081355
      },
      {
        "title": "Analyse morphologique non supervisée en domaine biomédical Application à la recherche d'information",
        "text": "Dans le domaine biomédical, utiliser des termes spécialisés est essentiel pour accéder à l'information. Cependant, dans beaucoup de langues, ces termes sont des constructions morphologiques complexes qui compliquent cet accès à l'information. Dans cet article, nous nous intéressons à l'identification des composants morphologiques de ces termes et à leur utilisation pour une tâche de recherche d'information (RI). Nous proposons différentes approches reposant sur un alignement automatique avec une langue pivot particulière, le japonais, et sur un apprentissage par analogie permettant de produire des analyses morphologiques fines des termes d'une langue donnée. Ces analyses morphologiques sont ensuite utilisées pour améliorer l'indexation de documents biomédicaux. Les expériences rapportées montrent la validité de cette approche avec des gains en MAP de plus de 10 % par rapport à un système de RI standard.ABSTRACT. In the biomedical field, using of specialized terms is key to access information. However, in most Indo-European languages, these terms are complex morphological structures. The presented work aims at identifying the various meaningful components of these terms and use them to improve biomedical Information Retrieval (IR). We present different approaches combining automatic alignments with a pivot language, Japanese, and analogical learning that allows an accurate morphological analysis of terms. These morphological analysis are used to improve the indexing of medical documents. The experiments reported in this paper show the validity of this approach with a 10% MAP improvement over a standard IR system. MOTS-CLÉS : morphologie, terminologie biomédicale, alignement, apprentissage par analogie, indexation morphosémantique, recherche d'information biomédicale.",
        "id": 260165148
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research that investigates methods for fine-tuning generative language models with a focus on parameter efficiency to reduce computational demands?",
    "positive_ctxs": [
      {
        "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
        "text": "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were \"virtual tokens\". We apply prefix-tuning to GPT-2 for table-totext generation and to BART for summarization. We show that by modifying only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. 2020. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. Amodei. 2020. Language models are few-shot learners.",
        "id": 230433941
      }
    ],
    "negative_ctxs": [
      {
        "title": "Representing Clinical Notes for Adverse Drug Event Detection",
        "text": "Electronic health records have emerged as a promising source of information for pharmacovigilance. Adverse drug events are, however, known to be heavily underreported, which makes it important to develop capabilities to detect such information automatically in clinical text.While machine learning offers possible solutions, it remains unclear how best to represent clinical notes in a manner conducive to learning high-performing predictive models. Here, 42 representations are explored in an empirical investigation using 27 real, clinical datasets, indicating that combining local and global (distributed) representations of words and named entities yields higher accuracy than using either in isolation. Subsequent analyses highlight the relative importance of various named entity classes for predicting adverse drug events.",
        "id": 6304413
      },
      {
        "title": "SpeechRecorder -a Universal Platform Independent Multi-Channel Audio Recording Software",
        "text": "SpeechRecorder is a platform independent audio recording software for speech corpus recordings. It is implemented in Java in a clean object-oriented design and adheres to established technology standards and document interchange formats. SpeechRecorder allows Unicode text and multimedia prompts, it supports audio recordings via more than two channels, and it features multiple configurable screens. Recording sessions are defined by recording scripts written in XML. The recording scripts can be executed manually by the experimenter, or automatically for unsupervised recordings; progress through the script can be sequential or randomized. SpeechRecorder is based on URLs to access local and network resources and thus allows recordings via the WWW.",
        "id": 15486073
      },
      {
        "title": "",
        "text": "",
        "id": 248780088
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "manual_acl",
    "question": "Which papers were among the first to explore the task of targeted training data extraction?",
    "positive_ctxs": [
      {
        "title": "ETHICIST: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation",
        "text": "Large pre-trained language models achieve impressive results across many tasks. However, recent works point out that pre-trained language models may memorize a considerable fraction of their training data, leading to the privacy risk of information leakage. In this paper, we propose a method named ETHICIST for targeted training data Extraction THrough loss smoothed soft prompting and calIbrated ConfIdence eSTimation, investigating how to recover the suffix in the training data when given a prefix. To elicit memorization in the attacked model, we tune soft prompt embeddings while keeping the model fixed. We further propose a smoothing loss that smooths the loss distribution of the suffix tokens to make it easier to sample the correct suffix. In order to select the most probable suffix from a collection of sampled suffixes and estimate the prediction confidence, we propose a calibrated confidence estimation method, which normalizes the confidence of the generated suffixes with a local estimation. We show that ETHICIST significantly improves the extraction performance on a recently proposed public benchmark. We also investigate several factors influencing the data extraction performance, including decoding strategy, model scale, prefix length, and suffix length. Our code is available at https://github.com/ thu-coai/Targeted-Data-Extraction.",
        "id": 259370520
      }
    ],
    "negative_ctxs": [
      {
        "title": "Choosing the best machine translation system to translate a sentence by using only source-language information",
        "text": "This paper describes a novel approach aimed to identify a priori which subset of machine translation (MT) systems among a known set will produce the most reliable translations for a given source-language (SL) sentence. We aim to select this subset of MT systems by using only information extracted from the SL sentence to be translated, and without access to the inner workings of the MT systems being used. A system able to select in advance, without translating, that subset of MT systems will allow multi-engine MT systems to save computing resources and focus on the combination of the output of the best MT systems. The selection of the best MT systems is done by extracting a set of features from each SL sentence and then using maximum entropy classifiers trained over a set of parallel sentences. Preliminary experiments on two European language pairs show a small, non-statistical significant improvement.",
        "id": 16402907
      },
      {
        "title": "World Knowledge for Reading Comprehension: Rare Entity Prediction with Hierarchical LSTMs Using External Descriptions",
        "text": "Humans interpret texts with respect to some background information, or world knowledge, and we would like to develop automatic reading comprehension systems that can do the same. In this paper, we introduce a task and several models to drive progress towards this goal. In particular, we propose the task of rare entity prediction: given a web document with several entities removed, models are tasked with predicting the correct missing entities conditioned on the document context and the lexical resources. This task is challenging due to the diversity of language styles and the extremely large number of rare entities. We propose two recurrent neural network architectures which make use of external knowledge in the form of entity descriptions. Our experiments show that our hierarchical LSTM model performs significantly better at the rare entity prediction task than those that do not make use of external resources.",
        "id": 2215426
      },
      {
        "title": "LSOIE: A Large-Scale Dataset for Supervised Open Information Extraction",
        "text": "Open Information Extraction (OIE) systems seek to compress the factual propositions of a sentence into a series of n-ary tuples. These tuples are useful for downstream tasks in natural language processing like knowledge base creation, textual entailment, and natural language understanding. However, current OIE datasets are limited in both size and diversity. We introduce a new dataset by converting the QA-SRL 2.0 dataset to a large-scale OIE dataset (LSOIE). Our LSOIE dataset is 20 times larger than the next largest human-annotated OIE dataset. We construct and evaluate several benchmark OIE models on LSOIE, providing baselines for future improvements on the task. Our LSOIE data, models, and code are made publicly available. 1",
        "id": 231719135
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that includes an online community dataset used to examine machine learning models for hate speech identification?",
    "positive_ctxs": [
      {
        "title": "Hate Towards the Political Opponent: A Twitter Corpus Study of the 2020 US Elections on the Basis of Offensive Speech and Stance Detection",
        "text": "The 2020 US Elections have been, more than ever before, characterized by social media campaigns and mutual accusations. We investigate in this paper if this manifests also in online communication of the supporters of the candidates Biden and Trump, by uttering hateful and offensive communication. We formulate an annotation task, in which we join the tasks of hateful/offensive speech detection and stance detection, and annotate 3000 Tweets from the campaign period, if they express a particular stance towards a candidate. Next to the established classes of favorable and against, we add mixed and neutral stances and also annotate if a candidate is mentioned without an opinion expression. Further, we annotate if the tweet is written in an offensive style. This enables us to analyze if supporters of Joe Biden and the Democratic Party communicate differently than supporters of Donald Trump and the Republican Party. A BERT baseline classifier shows that the detection if somebody is a supporter of a candidate can be performed with high quality (.89 F 1 for Trump and .91 F 1 for Biden), while the detection that somebody expresses to be against a candidate is more challenging (.79 F 1 and .64 F 1 , respectively). The automatic detection of hate/offensive speech remains challenging (with .53 F 1 ). Our corpus is publicly available and constitutes a novel resource for computational modelling of offensive language under consideration of stances.",
        "id": 232092599
      }
    ],
    "negative_ctxs": [
      {
        "title": "PortageLive: Delivering Machine Translation Technology via Virtualization",
        "text": "We describe and report initial results on using virtual machines as a vehicle to deploy machine translation technology to the marketplace. Virtual machines can bridge the gap between the computing infrastructure typically used in research environments and commodity PCs typical of office environments. A key component is the compact representation of the underlying databases and models in tightly packed tries, which allows us to run state-ofthe art translation technology on regular office PCs.",
        "id": 35789919
      },
      {
        "title": "Propagation of emotions, arousal and polarity in WordNet using Heterogeneous Structured Synset Embeddings",
        "text": "In this paper we present a novel method for emotive propagation in a wordnet based on a large emotive seed. We introduce a sense-level emotive lexicon annotated with polarity, arousal and emotions. The data were annotated as a part of a large study involving over 20,000 participants. A total of 30,000 lexical units in Polish WordNet were described with metadata, each unit received about 50 annotations concerning polarity, arousal and 8 basic emotions, marked on a multilevel scale. We present a preliminary approach to propagating emotive metadata to unlabeled lexical units based on the distribution of manual annotations using logistic regression and description of mixed synset embeddings based on our Heterogeneous Structured Synset Embeddings.",
        "id": 232021736
      },
      {
        "title": "",
        "text": "",
        "id": 208128503
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_acl",
    "question": "Can you list some publications that discuss the evaluation metrics used in semantic role labeling tasks, specifically focusing on precision and evaluation scripts used in shared tasks like SemEval?",
    "positive_ctxs": [
      {
        "title": "SemEval-2007 Task 17: English Lexical Sample, SRL and All Words",
        "text": "This paper describes our experience in preparing the data and evaluating the results for three subtasks of SemEval-2007 Task-17 -Lexical Sample, Semantic Role Labeling (SRL) and All-Words respectively. We tabulate and analyze the results of participating systems.",
        "id": 17553490
      },
      {
        "title": "SENSEVAL-3 TASK Automatic Labeling of Semantic Roles",
        "text": "The SENSEVAL-3 task to perform automatic labeling of semantic roles was designed to encourage research into and use of the FrameNet dataset. The task was based on the considerable expansion of the FrameNet data since the baseline study of automatic labeling of semantic roles by Gildea and Jurafsky. The FrameNet data provide an extensive body of \"gold standard\" data that can be used in lexical semantics research, as the basis for its further exploitation in NLP applications. Eight teams participated in the task, with a total of 20 runs. Discussions among participants during development of the task and the scoring of their runs contributed to a successful task. Participants used a wide variety of techniques, investigating many aspects of the FrameNet data. They achieved results showing considerable improvements from Gildea and Jurafsky's baseline study. Importantly, their efforts have contributed considerably to making the complex FrameNet dataset more accessible. They have amply demonstrated that FrameNet is a substantial lexical resource that will permit extensive further research and exploitation in NLP applications in the future.",
        "id": 561429
      },
      {
        "title": "Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling",
        "text": "In this paper we describe the CoNLL-2005 shared task on Semantic Role Labeling. We introduce the specification and goals of the task, describe the data sets and evaluation methods, and present a general overview of the 19 systems that have contributed to the task, providing a comparative description and results.",
        "id": 16509032
      }
    ],
    "negative_ctxs": [
      {
        "title": "GEOMETRY OF POLYSEMY",
        "text": "Vector representations of words have heralded a transformational approach to classical problems in NLP; the most popular example is word2vec. However, a single vector does not suffice to model the polysemous nature of many (frequent) words, i.e., words with multiple meanings. In this paper, we propose a three-fold approach for unsupervised polysemy modeling: (a) context representations, (b) sense induction and disambiguation and (c) lexeme (as a word and sense pair) representations. A key feature of our work is the finding that a sentence containing a target word is well represented by a low rank subspace, instead of a point in a vector space. We then show that the subspaces associated with a particular sense of the target word tend to intersect over a line (one-dimensional subspace), which we use to disambiguate senses using a clustering algorithm that harnesses the Grassmannian geometry of the representations. The disambiguation algorithm, which we call K-Grassmeans, leads to a procedure to label the different senses of the target word in the corpus -yielding lexeme vector representations, all in an unsupervised manner starting from a large (Wikipedia) corpus in English. Apart from several prototypical target (word,sense) examples and a host of empirical studies to intuit and justify the various geometric representations, we validate our algorithms on standard sense induction and disambiguation datasets and present new state-of-the-art results.",
        "id": 18114929
      },
      {
        "title": "Standoff Coordination for Multi-Tool Annotation in a Dialogue Corpus",
        "text": "The LUNA corpus is a multi-lingual, multidomain spoken dialogue corpus currently under development that will be used to develop a robust natural spoken language understanding toolkit for multilingual dialogue services. The LUNA corpus will be annotated at multiple levels to include annotations of syntactic, semantic, and discourse information; specialized annotation tools will be used for the annotation at each of these levels. In order to synchronize these multiple layers of annotation, the PAULA standoff exchange format will be used. In this paper, we present the corpus and its PAULA-based architecture. 1",
        "id": 6618630
      },
      {
        "title": "PANDORA Talks: Personality and Demographics on Reddit",
        "text": "Personality and demographics are important variables in social sciences and computational sociolinguistics. However, datasets with both personality and demographic labels are scarce. To address this, we present PANDORA, the first dataset of Reddit comments of 10k users partially labeled with three personality models and demographics (age, gender, and location), including 1.6k users labeled with the wellestablished Big 5 personality model. We showcase the usefulness of this dataset on three experiments, where we leverage the more readily available data from other personality models to predict the Big 5 traits, analyze gender classification biases arising from psychodemographic variables, and carry out a confirmatory and exploratory analysis based on psychological theories. Finally, we present benchmark prediction models for all personality and demographic variables.",
        "id": 215547988
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research that introduces a metric for assessing Text-to-Image synthesis, emphasizing the semantic congruence between the text and the produced image rather than solely the visual quality?",
    "positive_ctxs": [
      {
        "title": "CLIPScore: A Reference-free Evaluation Metric for Image Captioning",
        "text": "Image captioning has conventionally relied on reference-based automatic evaluations, where machine captions are compared against captions written by humans. This is in contrast to the reference-free manner in which humans assess caption quality.In this paper, we report the surprising empirical finding that CLIP (Radford et al., 2021), a cross-modal model pretrained on 400M im-age+caption pairs from the web, can be used for robust automatic evaluation of image captioning without the need for references. Experiments spanning several corpora demonstrate that our new reference-free metric, CLIPScore, achieves the highest correlation with human judgements, outperforming existing reference-based metrics like CIDEr and SPICE. Information gain experiments demonstrate that CLIPScore, with its tight focus on image-text compatibility, is complementary to existing reference-based metrics that emphasize text-text similarities. Thus, we also present a reference-augmented version, RefCLIPScore, which achieves even higher correlation. Beyond literal description tasks, several case studies reveal domains where CLIPScore performs well (clip-art images, alt-text rating), but also where it is relatively weaker in comparison to reference-based metrics, e.g., news captions that require richer contextual knowledge.",
        "id": 233296711
      }
    ],
    "negative_ctxs": [
      {
        "title": "Summ-it++: an enriched version of the Summ-it corpus",
        "text": "This paper presents Summ-it++, an enriched version the Summ-it corpus. In this new version, the corpus has received new semantic layers, named entity categories and relations between named entities, adding to the previous coreference annotation. In addition, we change the original Summ-it format to SemEval.",
        "id": 9775710
      },
      {
        "title": "Improving Summaries by Revising Them",
        "text": "This paper describes a program which revises a draft text by aggregating together descriptions of discourse entities, in addition to deleting extraneous information. In contrast to knowledgerich sentence aggregation approaches explored in the past, this approach exploits statistical parsing and robust coreference detection. In an evaluation involving revision of topic-related summaries using informativeness measures from the TIPSTER SUMMAC evaluation, the results show gains in informativeness without compromising readability.",
        "id": 2601442
      },
      {
        "title": "",
        "text": "",
        "id": 207911293
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research that investigates how to use contrastive learning for improving logical reasoning over text?",
    "positive_ctxs": [
      {
        "title": "MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning",
        "text": "Logical reasoning is of vital importance to natural language understanding. Previous studies either employ graph-based models to incorporate prior knowledge about logical relations, or introduce symbolic logic into neural models through data augmentation. These methods, however, heavily depend on annotated training data, and thus suffer from overfitting and poor generalization problems due to the dataset sparsity. To address these two problems, in this paper, we propose MERIt, a MEta-path guided contrastive learning method for logical ReasonIng of text, to perform selfsupervised pre-training on abundant unlabeled text data. Two novel strategies serve as indispensable components of our method. In particular, a strategy based on meta-path is devised to discover the logical structure in natural texts, followed by a counterfactual data augmentation strategy to eliminate the information shortcut induced by pre-training. The experimental results on two challenging logical reasoning benchmarks, i.e., ReClor and LogiQA, demonstrate that our method outperforms the SOTA baselines with significant improvements. 1 * Corresponding author: Yangyang Guo and Liqiang Nie. 1 Our code and pre-trained models are available at https: //github.com/SparkJiao/MERIt. 2  We refer the term logical reasoning to the task itself in the remaining of this paper.",
        "id": 247187518
      }
    ],
    "negative_ctxs": [
      {
        "title": "Introducing QuBERT: A Large Monolingual Corpus and BERT Model for Southern Quechua",
        "text": "The lack of resources for languages in the Americas has proven to be a problem for the creation of digital systems such as machine translation, search engines, chat bots, and more. The scarceness of digital resources for a language causes a higher impact on populations where the language is spoken by millions of people. We introduce the first official large combined corpus for deep learning of an indigenous South American low-resource language spoken by millions called Quechua. Specifically, our curated corpus is created from text gathered from the southern region of Peru where a dialect of Quechua is spoken that has not traditionally been used for digital systems as a target dialect in the past. In order to make our work repeatable by others, we also offer a public, pre-trained, BERT model called Qu-BERT which is the largest linguistic model ever trained for any Quechua type, not just the southern region dialect. We furthermore test our corpus and its corresponding BERT model on two major tasks: (1) named-entity recognition (NER) and (2) part-of-speech (POS) tagging by using state-of-the-art techniques where we achieve results comparable to other work on higher-resource languages. In this article, we describe the methodology, challenges, and results from the creation of QuBERT which is on on par with other state-of-the-art multilingual models for natural language processing achieving between 71 and 74% F1 score on NER and 84-87% on POS tasks. . 2020. Multilingual denoising pretraining for neural machine translation. Trans. Assoc.",
        "id": 250391068
      },
      {
        "title": "Sense Extension Functions in Lexical Semantics",
        "text": "A b stra ctRepresenting polysemy in an economical way is an issue of major impor tance within lexical semantics. Polysemy is found both within single lexi cal entries, and systematically in some lexical classes with common semantic properties. Prepositions in various languages are generally considered highly polysemic in an unpredictable way. The latter participate in what can be called systematic polysemy. This work is highly inspired by work as different as Pustejovsky [Pus91], Copestake and Briscoe [CB95], and Lakoff [Lak94].1 will sketch a framework or the fundamentals of a formalism in which important polysemic properties can be described. The interpretational se mantics is built as typed lambda-calculus. This choice is not essential to the formalism, which might be extended to situation-theoretical notation and in terpretation. Currently, situation-theoretical issues are not discussed within the framework.It is briefly outlined how the lexical semantics as construed in this paper can be implemented in a typed feature structure formalism compatible to HPSG [PS94]. Accounts of various aspects of prepositional semantics are given in this formalism, with special emphasis on the Danish preposition rued.",
        "id": 18156564
      },
      {
        "title": "Joint Learning for Coreference Resolution with Markov Logic",
        "text": "Pairwise coreference resolution models must merge pairwise coreference decisions to generate final outputs. Traditional merging methods adopt different strategies such as the bestfirst method and enforcing the transitivity constraint, but most of these methods are used independently of the pairwise learning methods as an isolated inference procedure at the end. We propose a joint learning model which combines pairwise classification and mention clustering with Markov logic. Experimental results show that our joint learning system outperforms independent learning systems. Our system gives a better performance than all the learning-based systems from the CoNLL-2011 shared task on the same dataset. Compared with the best system from CoNLL-2011, which employs a rule-based method, our system shows competitive performance.",
        "id": 17786494
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "manual_acl",
    "question": "Which family of model generally perform the best for the event conceptualization task",
    "positive_ctxs": [
      {
        "title": "CAT: A Contextualized Conceptualization and Instantiation Framework for Commonsense Reasoning",
        "text": "Commonsense reasoning, aiming at endowing machines with a human-like ability to make situational presumptions, is extremely challenging to generalize. For someone who barely knows about meditation, while is knowledgeable about singing, he can still infer that meditation makes people relaxed from the existing knowledge that singing makes people relaxed by first conceptualizing singing as a relaxing event and then instantiating that event to meditation. This process, known as conceptual induction and deduction, is fundamental to commonsense reasoning while lacking both labeled data and methodologies to enhance commonsense modeling. To fill such a research gap, we propose CAT (Contextualized ConceptuAlization and InsTantiation), a semi-supervised learning framework that integrates event conceptualization and instantiation to conceptualize commonsense knowledge bases at scale. Extensive experiments show that our framework achieves state-of-the-art performances on two conceptualization tasks, and the acquired abstract commonsense knowledge can significantly improve commonsense inference modeling. Our code, data, and fine-tuned models are publicly available at https://github.com/HKUST-KnowComp/CAT. * Equal Contribution PersonX watches football game, as a result, PersonX will: feel relaxed PersonX plays with his dog, as a result, PersonX will: be happy and relaxed PersonX [observe] as a result, PersonX will: feel relaxed PersonX [relaxing event] as a result, PersonX will: feel relaxed Conceptualization (watches football game → relaxing event) Instantiation (relaxing event → plays with his dog) Abstract Knowledge Aided Reasoning Conceptualization (watches football game → observe) Wrong Abstract Knowledge",
        "id": 258557145
      }
    ],
    "negative_ctxs": [
      {
        "title": "Fast Unsupervised Dependency Parsing with Arc-Standard Transitions",
        "text": "Unsupervised dependency parsing is one of the most challenging tasks in natural languages processing. The task involves finding the best possible dependency trees from raw sentences without getting any aid from annotated data. In this paper, we illustrate that by applying a supervised incremental parsing model to unsupervised parsing; parsing with a linear time complexity will be faster than the other methods. With only 15 training iterations with linear time complexity, we gain results comparable to those of other state of the art methods. By employing two simple universal linguistic rules inspired from the classical dependency grammar, we improve the results in some languages and get the state of the art results. We also test our model on a part of the ongoing Persian dependency treebank. This work is the first work done on the Persian language.",
        "id": 426641
      },
      {
        "title": "More Than Words: Collocation Retokenization for Latent Dirichlet Allocation Models",
        "text": "Traditionally, Latent Dirichlet Allocation (LDA) ingests words in a collection of documents to discover their latent topics using word-document co-occurrences. Previous studies show that representing bigrams collocations in the input can improve topic coherence in English. However, it is unclear how to achieve the best results for languages without marked word boundaries such as Chinese and Thai. Here, we explore the use of retokenization based on chi-squared measures, tstatistics, and raw frequency to merge frequent token ngrams into collocations when preparing input to the LDA model. Based on the goodness of fit and the coherence metric, we show that topics trained with merged tokens result in topic keys that are clearer, more coherent, and more effective at distinguishing topics than those of unmerged models.",
        "id": 237277900
      },
      {
        "title": "Incremental Neural Lexical Coherence Modeling",
        "text": "Pretrained language models, neural models pretrained on massive amounts of data, have established the state of the art in a range of NLP tasks. They are based on a modern machine-learning technique, the Transformer which relates all items simultaneously to capture semantic relations in sequences. However, it differs from what humans do. Humans read sentences one-by-one, incrementally. Can neural models benefit by interpreting texts incrementally as humans do? We investigate this question in coherence modeling. We propose a coherence model which interprets sentences incrementally to capture lexical relations between them. We compare the state of the art in each task, simple neural models relying on a pretrained language model, and our model in two downstream tasks. Our findings suggest that interpreting texts incrementally as humans could be useful to design more advanced models. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/.",
        "id": 227231546
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "What are the latest developments in conversational agents that integrate external knowledge sources and employ diverse tactics to offer emotional support during interactions?",
    "positive_ctxs": [
      {
        "title": "MISC: A MIxed Strategy-Aware Model Integrating COMET for Emotional Support Conversation",
        "text": "Applying existing methods to emotional support conversation-which provides valuable assistance to people who are in need-has two major limitations: (a) they generally employ a conversation-level emotion label, which is too coarse-grained to capture user's instant mental state; (b) most of them focus on expressing empathy in the response(s) rather than gradually reducing user's distress. To address the problems, we propose a novel model MISC, which firstly infers the user's fine-grained emotional status, and then responds skillfully using a mixture of strategy. Experimental results on the benchmark dataset demonstrate the effectiveness of our method and reveal the benefits of fine-grained emotion understanding as well as mixed-up strategy modeling. Our code and data could be found in https: //github.com/morecry/MISC.",
        "id": 247748640
      }
    ],
    "negative_ctxs": [
      {
        "title": "On the difficulty of a distributional semantics of spoken language",
        "text": "In the domain of unsupervised learning most work on speech has focused on discovering low-level constructs such as phoneme inventories or word-like units. In contrast, for written language, where there is a large body of work on unsupervised induction of semantic representations of words, whole sentences and longer texts. In this study we examine the challenges of adapting these approaches from written to spoken language. We conjecture that unsupervised learning of the semantics of spoken language becomes feasible if we abstract from the surface variability. We simulate this setting with a dataset of utterances spoken by a realistic but uniform synthetic voice. We evaluate two simple unsupervised models which, to varying degrees of success, learn semantic representations of speech fragments. Finally we present inconclusive results on human speech, and discuss the challenges inherent in learning distributional semantic representations on unrestricted natural spoken language.",
        "id": 4314399
      },
      {
        "title": "The Anatomy of a Systemic Choice",
        "text": "Scope and Purpose 1Choice is one of the most prominent organizing concepts in systemic linguistics. Languages are described in terms of the choices available to the speaker and the relationships of those choices to each other and to the language produced. This paper addresses the problems of characterizing processes of choosing in a systemic framework and creating a corresponding notation. Focus on notation is necessary at this point since development of notation must to some extent precede development of corresponding content. Although these developments are part of an investigation of computer text production, their significance is not confined to that enterprise; they are as significant in linguistics as in computer science. This paper focuses on the perspective of choice as a speaker's action. We hope that by exploring the nature of systemic choices, several kinds of interests in language will be served:Interest in Grammar as Language Description-Describing choices can yield a richer understanding of the alternatives offered, and how one system of choices differs from another.",
        "id": 9972666
      },
      {
        "title": "FastClass: A Time-Efficient Approach to Weakly-Supervised Text Classification",
        "text": "Weakly-supervised text classification aims to train a classifier using only class descriptions and unlabeled data. Recent research shows that keyword-driven methods can achieve state-ofthe-art performance on various tasks. However, these methods not only rely on carefullycrafted class descriptions to obtain classspecific keywords but also require substantial amount of unlabeled data and takes a long time to train. This paper proposes FastClass, an efficient weakly-supervised classification approach. It uses dense text representation to retrieve class-relevant documents from external unlabeled corpus and selects an optimal subset to train a classifier. Compared to keyworddriven methods, our approach is less reliant on initial class descriptions as it no longer needs to expand each class description into a set of class-specific keywords. Experiments on a wide range of classification tasks show that the proposed approach frequently outperforms keyword-driven models in terms of classification accuracy and often enjoys orders-ofmagnitude faster training speed. * Corresponding authors.",
        "id": 254564126
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Have any research papers collected feedback from real users who were using LLMs for scientific writing?",
    "positive_ctxs": [
      {
        "title": "Sparks: Inspiration for Science Writing using Language Models",
        "text": "Large-scale language models are rapidly improving, performing well on a variety of tasks with little to no customization. In this work we investigate how language models can support science writing, a challenging writing task that is both open-ended and highly constrained. We present a system for generating \"sparks\", sentences related to a scientific concept intended to inspire writers. We run a user study with 13 STEM graduate students and find three main use cases of sparks-inspiration, translation, and perspective-each of which correlates with a unique interaction pattern. We also find that while participants were more likely to select higher quality sparks, the overall quality of sparks seen by a given participant did not correlate with their satisfaction with the tool. 1",
        "id": 239009871
      }
    ],
    "negative_ctxs": [
      {
        "title": "Identifying Bad Semantic Neighbors for Improving Distributional Thesauri",
        "text": "Distributional thesauri are now widely used in a large number of Natural Language Processing tasks. However, they are far from containing only interesting semantic relations. As a consequence, improving such thesaurus is an important issue that is mainly tackled indirectly through the improvement of semantic similarity measures. In this article, we propose a more direct approach focusing on the identification of the neighbors of a thesaurus entry that are not semantically linked to this entry. This identification relies on a discriminative classifier trained from unsupervised selected examples for building a distributional model of the entry in texts. Its bad neighbors are found by applying this classifier to a representative set of occurrences of each of these neighbors. We evaluate the interest of this method for a large set of English nouns with various frequencies.",
        "id": 1669441
      },
      {
        "title": "Multi-view Models for Political Ideology Detection of News Articles",
        "text": "A news article's title, content and link structure often reveal its political ideology. However, most existing works on automatic political ideology detection only leverage textual cues. Drawing inspiration from recent advances in neural inference, we propose a novel attention based multi-view model to leverage cues from all of the above views to identify the ideology evinced by a news article. Our model draws on advances in representation learning in natural language processing and network science to capture cues from both textual content and the network structure of news articles. We empirically evaluate our model against a battery of baselines and show that our model outperforms state of the art by 10 percentage points F1 score.",
        "id": 52183735
      },
      {
        "title": "Towards Convenient Bi-Directional Grammar Formalisms",
        "text": "P. Newmzrn",
        "id": 5127127
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you suggest a study that explores the idea of using prompts to fine-tune language models, potentially providing an alternative viewpoint to standard optimization and regularization methods?",
    "positive_ctxs": [
      {
        "title": "Making Pre-trained Language Models Better Few-shot Learners",
        "text": "The recent GPT-3 model(Brown et al., 2020)achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF-better few-shot fine-tuning of language models 1 -a suite of simple and complementary techniques for finetuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning. 2 * The first two authors contributed equally. 1 Alternatively, language models' best friends forever. 2 Our implementation is publicly available at https:// github.com/princeton-nlp/LM-BFF. Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Association for Computational Linguistics (ACL). Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Association for Computational Linguistics (ACL).",
        "id": 229923710
      }
    ],
    "negative_ctxs": [
      {
        "title": "When does deep multi-task learning work for loosely related document classification tasks? When does deep multi-task learning work for loosely related document classification tasks?",
        "text": "This work aims to contribute to our understanding of when multi-task learning through parameter sharing in deep neural networks leads to improvements over single-task learning. We focus on the setting of learning from loosely related tasks, for which no theoretical guarantees exist. We therefore approach the question empirically, studying which properties of datasets and single-task learning characteristics correlate with improvements from multi-task learning. We are the first to study this in a text classification setting and across more than 500 different task pairs.",
        "id": 53581688
      },
      {
        "title": "Event Schema Induction with a Probabilistic Entity-Driven Model",
        "text": "Event schema induction is the task of learning high-level representations of complex events (e.g., a bombing) and their entity roles (e.g., perpetrator and victim) from unlabeled text. Event schemas have important connections to early NLP research on frames and scripts, as well as modern applications like template extraction. Recent research suggests event schemas can be learned from raw text. Inspired by a pipelined learner based on named entity coreference, this paper presents the first generative model for schema induction that integrates coreference chains into learning. Our generative model is conceptually simpler than the pipelined approach and requires far less training data. It also provides an interesting contrast with a recent HMM-based model. We evaluate on a common dataset for template schema extraction. Our generative model matches the pipeline's performance, and outperforms the HMM by 7 F1 points (20%).",
        "id": 6341459
      },
      {
        "title": "Temporal Ranking for Fresh Information Retrieval",
        "text": "In business, the retrieval of up-to-date, or fresh, information is very important. It is difficult for conventional search engines based on a centralized architecture to retrieve fresh information, because they take a long time to collect documents via Web robots. In contrast to a centralized architecture, a search engine based on a distributed architecture does not need to collect documents, because each site makes an index independently. As a result, distributed search engines can be used to retrieve fresh information. However, fast indexing alone is not enough to retrieve fresh information, as support for temporal information based retrieval is also required. In this paper, we describe temporal information retrieval in distributed search engines. In particular, we propose an implementation of temporal ranking.",
        "id": 7209972
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "manual_acl",
    "question": "What are some methods for solving the class-incremetal continual learning problems?",
    "positive_ctxs": [
      {
        "title": "Rehearsal-free Continual Language Learning via Efficient Parameter Isolation",
        "text": "We study the problem of defying catastrophic forgetting when learning a series of language processing tasks. Compared with previous methods, we emphasize the importance of not caching history tasks' data, which makes the problem more challenging. Our proposed method applies the parameter isolation strategy. For each task, it allocates a small portion of private parameters and learns them with a shared pre-trained model. To load correct parameters at testing time, we introduce a simple yet effective non-parametric method. Experiments on continual language learning benchmarks show that our method is significantly better than all existing no-data-cache methods, and is comparable (or even better) than those using historical data 1 . . 2021a. Continual learning for text classification with information disentanglement based regularization. In",
        "id": 259370817
      }
    ],
    "negative_ctxs": [
      {
        "title": "The preliminary study of robust speech feature extraction based on maximizing the accuracy of states in deep acoustic models",
        "text": "",
        "id": 233029488
      },
      {
        "title": "Talk to Papers: Bringing Neural Question Answering to Academic Search",
        "text": "We introduce Talk to Papers 1 , which exploits the recent open-domain question answering (QA) techniques to improve the current experience of academic search. It's designed to enable researchers to use natural language queries to find precise answers and extract insights from a massive amount of academic papers. We present a large improvement over classic search engine baseline on several standard QA datasets, and provide the community a collaborative data collection tool to curate the first natural language processing research QA dataset via a community effort. *",
        "id": 214802635
      },
      {
        "title": "",
        "text": "",
        "id": 218692901
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "What papers discuss the effect of false negatives among hard negatives in dense retriever training?",
    "positive_ctxs": [
      {
        "title": "Debiased Contrastive Learning of Unsupervised Sentence Representations",
        "text": "Recently, contrastive learning has been shown to be effective in improving pre-trained language models (PLM) to derive high-quality sentence representations. It aims to pull close positive examples to enhance the alignment while push apart irrelevant negatives for the uniformity of the whole representation space. However, previous works mostly adopt in-batch negatives or sample from training data at random. Such a way may cause the sampling bias that improper negatives (e.g., false negatives and anisotropy representations) are used to learn sentence representations, which will hurt the uniformity of the representation space. To address it, we present a new framework DCLR (Debiased Contrastive Learning of unsupervised sentence Representations) to alleviate the influence of these improper negatives. In DCLR, we design an instance weighting method to punish false negatives and generate noise-based negatives to guarantee the uniformity of the representation space. Experiments on seven semantic textual similarity tasks show that our approach is more effective than competitive baselines. Our code and data are publicly available at the link: https: //github.com/RUCAIBox/DCLR.",
        "id": 248496439
      }
    ],
    "negative_ctxs": [
      {
        "title": "Friendly Topic Assistant for Transformer Based Abstractive Summarization",
        "text": "Abstractive document summarization is a comprehensive task including document understanding and summary generation, in which area Transformer-based models have achieved the state-of-the-art performance. Compared with Transformers, topic models are better at learning explicit document semantics, and hence could be integrated into Transformers to further boost their performance. To this end, we rearrange and explore the semantics learned by a topic model, and then propose a topic assistant (TA) including three modules. TA is compatible with various Transformerbased models and user-friendly since i) TA is a plug-and-play model that does not break any structure of the original Transformer network, making users easily fine-tune Transformer+TA based on a well pre-trained model; ii) TA only introduces a small number of extra parameters. Experimental results on three datasets demonstrate that TA is able to improve the performance of several Transformer-based models.",
        "id": 226262313
      },
      {
        "title": "",
        "text": "",
        "id": 164936286
      },
      {
        "title": "A Translation Model For Languages of Acceding Countries",
        "text": "The paper proposes a model for translation between syntactically similar languages of acceding countries. This model is based on the presupposition that the translation of related languages should exploit the relatedness by using as simple methods and tools as possible. In the first part the paper discusses the properties of some \"new\" languages, the second part describes a simple translation model which has already been tested on several pairs of syntactically similar languages..",
        "id": 16165854
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that examines the difficulties in employing weakly labeled datasets for named entity recognition and offers techniques to mitigate the associated data noise?",
    "positive_ctxs": [
      {
        "title": "Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data",
        "text": "Weak supervision has shown promising results in many natural language processing tasks, such as Named Entity Recognition (NER). Existing work mainly focuses on learning deep NER models only with weak supervision, i.e., without any human annotation, and shows that by merely using weakly labeled data, one can achieve good performance, though still underperforms fully supervised NER with manually/strongly labeled data. In this paper, we consider a more practical scenario, where we have both a small amount of strongly labeled data and a large amount of weakly labeled data. Unfortunately, we observe that weakly labeled data does not necessarily improve, or even deteriorate the model performance (due to the extensive noise in the weak labels) when we train deep NER models over a simple or weighted combination of the strongly labeled and weakly labeled data. To address this issue, we propose a new multi-stage computational framework -NEEDLE with three essential ingredients: (1) weak label completion, (2) noise-aware loss function, and (3) final finetuning over the strongly labeled data. Through experiments on E-commerce query NER and Biomedical NER, we demonstrate that NEE-DLE can effectively suppress the noise of the weak labels and outperforms existing methods. In particular, we achieve new SOTA F1-scores on 3 Biomedical NER datasets: BC5CDRchem 93.74, BC5CDR-disease 90.69, NCBIdisease 92.28.Gamal Crichton, Sampo Pyysalo, Billy Chiu, and AnnaKorhonen. 2017. A neural network multi-task learning approach to biomedical named entity recognition. BMC bioinformatics, 18(1):368.",
        "id": 235446386
      }
    ],
    "negative_ctxs": [
      {
        "title": "Building Graph Representations of Deep Vector Embeddings",
        "text": "Patterns stored within pre-trained deep neural networks compose large and powerful descriptive languages that can be used for many different purposes. Typically, deep network representations are implemented within vector embedding spaces, which enables the use of traditional machine learning algorithms on top of them. In this short paper we propose the construction of a graph embedding space instead, introducing a methodology to transform the knowledge coded within a deep convolutional network into a topological space (i.e., a network). We outline how such graph can hold data instances, data features, relations between instances and features, and relations among features. Finally, we introduce some preliminary experiments to illustrate how the resultant graph embedding space can be exploited through graph analytics algorithms.",
        "id": 5213431
      },
      {
        "title": "Multi 2 OIE: Multilingual Open Information Extraction Based on Multi-Head Attention with BERT",
        "text": "In this paper, we propose Multi 2 OIE, which performs open information extraction (open IE) by combining BERT (Devlin et al., 2019)   with multi-head attention blocks(Vaswani et al., 2017). Our model is a sequence-labeling system with an efficient and effective argument extraction method. We use a query, key, and value setting inspired by the Multimodal Transformer (Tsai et al., 2019) to replace the previously used bidirectional long short-term memory architecture with multihead attention. Multi 2 OIE outperforms existing sequence-labeling systems with high computational efficiency on two benchmark evaluation datasets, Re-OIE2016 and CaRB. Additionally, we apply the proposed method to multilingual open IE using multilingual BERT. Experimental results on new benchmark datasets introduced for two languages (Spanish and Portuguese) demonstrate that our model outperforms other multilingual systems without training data for the target languages.",
        "id": 221761515
      },
      {
        "title": "Text Simplification as Tree Labeling",
        "text": "We present a new, structured approach to text simplification using conditional random fields over top-down traversals of dependency graphs that jointly predicts possible compressions and paraphrases. Our model reaches readability scores comparable to word-based compression approaches across a range of metrics and human judgements while maintaining more of the important information.",
        "id": 16305240
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research that analyses prompt tuning as a method to improve the generalizability of pre-trained models while avoiding catastrophic forgetting?",
    "positive_ctxs": [
      {
        "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
        "text": "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were \"virtual tokens\". We apply prefix-tuning to GPT-2 for table-totext generation and to BART for summarization. We show that by modifying only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. 2020. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. Amodei. 2020. Language models are few-shot learners.",
        "id": 230433941
      },
      {
        "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
        "text": "In this work, we explore \"prompt tuning,\" a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient \"prompt ensembling.\" We release code and model checkpoints to reproduce our experiments. 1ReferencesRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,  Danilo Giampiccolo, Bernardo Magnini, and Idan  Szpektor. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognising textual entailment, volume 6, pages 6-4. Venice. . 2019a. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
        "id": 233296808
      }
    ],
    "negative_ctxs": [
      {
        "title": "Recherche cross-modale pour répondre à des questions visuelles",
        "text": "Répondre à des questions visuelles à propos d'entités nommées (KVQAE) est une tâche difficile qui demande de rechercher des informations dans une base de connaissances multimodale.Nous étudions ici comment traiter cette tâche avec une recherche cross-modale et sa combinaison avec une recherche mono-modale, en se focalisant sur le modèle CLIP, un modèle multimodal entraîné sur des images appareillées à leur légende textuelle.Nos résultats démontrent la supériorité de la recherche cross-modale, mais aussi la complémentarité des deux, qui peuvent être combinées facilement.Nous étudions également différentes manières d'ajuster CLIP et trouvons que l'optimisation cross-modale est la meilleure solution, étant en adéquation avec son pré-entraînement.Notre méthode surpasse les approches précédentes, tout en étant plus simple et moins coûteuse.Ces gains de performance sont étudiés intrinsèquement selon la pertinence des résultats de la recherche et extrinsèquement selon l'exactitude de la réponse extraite par un module externe.Nous discutons des différences entre ces métriques et de ses implications pour l'évaluation de la KVQAE.",
        "id": 264038819
      },
      {
        "title": "Investigation Into Using the Unicode Standard for Primitives of Unified Han Characters",
        "text": "The Unicode standard identifies and provides representation of the vast majority of known characters used in today's writing systems. Many of these characters belong to the unified Han series, which encapsulates characters from writing systems used in languages such as Chinese, Japanese and Korean languages. These pictographic characters are often made up of smaller primitives, either other characters or more simplified pictography. This paper presents research findings of how the Unicode standard currently represents the primitives used in 4134 of the most common Han characters.PACLIC 28! 131 character was somewhat similar in meaning across most data sets.For each entry, the primitives were then defined and described relative to their position. Character positions were broken up into four main directions: top (t), bottom (b), left (l), right (r), to describe where primitives belong visually within a parent character.",
        "id": 20149764
      },
      {
        "title": "Published as a conference paper at ICLR 2023 SQA3D: SITUATED QUESTION ANSWERING IN 3D SCENES",
        "text": "DescriptionSitting at the edge of the bed and facing the couch. Question q : Can I go straight to the coffee table in front of me? Scene context : 3D scan, egocentric video, birdeye view (BEV) picture, etc. Answer : No Location (optional): t t+1ABSTRACT We propose a new task to benchmark scene understanding of embodied agents: Situated Question Answering in 3D Scenes (SQA3D). Given a scene context (e.g., 3D scan), SQA3D requires the tested agent to first understand its situation (position, orientation, etc.) in the 3D scene as described by text, then reason about its surrounding environment and answer a question under that situation. Based upon 650 scenes from ScanNet, we provide a dataset centered around 6.8k unique situations, along with 20.4k descriptions and 33.4k diverse reasoning questions for these situations. These questions examine a wide spectrum of reasoning capabilities for an intelligent agent, ranging from spatial relation comprehension to commonsense understanding, navigation, and multi-hop reasoning. SQA3D imposes a significant challenge to current multi-modal especially 3D reasoning models. We evaluate various state-of-the-art approaches and find that the best one only achieves an overall score of 47.20%, while amateur human participants can reach 90.06%. We believe SQA3D could facilitate future embodied AI research with stronger situation understanding and reasoning capabilities. Code and data are released at sqa3d.github.io.",
        "id": 252907411
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a study that investigates how integrating model quantization with knowledge distillation?",
    "positive_ctxs": [
      {
        "title": "BinaryBERT: Pushing the Limit of BERT Quantization",
        "text": "The rapid development of large pre-trained language models has greatly increased the demand for model compression techniques, among which quantization is a popular solution. In this paper, we propose Binary-BERT, which pushes BERT quantization to the limit by weight binarization. We find that a binary BERT is hard to be trained directly than a ternary counterpart due to its complex and irregular loss landscape. Therefore, we propose ternary weight splitting, which initializes BinaryBERT by equivalently splitting from a half-sized ternary network. The binary model thus inherits the good performance of the ternary one, and can be further enhanced by fine-tuning the new architecture after splitting. Empirical results show that our Binary-BERT has only a slight performance drop compared with the full-precision model while being 24× smaller, achieving the state-of-the-art compression results on the GLUE and SQuAD benchmarks. (a) Full-precision Model. (b) Ternary Model. (c) Binary Model. (d) All Together.",
        "id": 229923538
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 219305397
      },
      {
        "title": "Sentence Ordering with Manifold-based Classification in Multi-Document Summarization",
        "text": "In this paper, we propose a sentence ordering algorithm using a semi-supervised sentence classification and historical ordering strategy. The classification is based on the manifold structure underlying sentences, addressing the problem of limited labeled data. The historical ordering helps to ensure topic continuity and avoid topic bias. Experiments demonstrate that the method is effective.",
        "id": 583930
      },
      {
        "title": "Reduce Catastrophic Forgetting of Dense Retrieval Training with Teleportation Negatives",
        "text": "In this paper, we investigate the instability in the standard dense retrieval training, which iterates between model training and hard negative selection using the being-trained model. We show the catastrophic forgetting phenomena behind the training instability, where models learn and forget different negative groups during training iterations. We then propose ANCE-Tele, which accumulates momentum negatives from past iterations and approximates future iterations using lookahead negatives, as \"teleportations\" along the time axis to smooth the learning process. On web search and OpenQA, ANCE-Tele outperforms previous state-of-theart systems of similar size, eliminates the dependency on sparse retrieval negatives, and is competitive among systems using significantly more (50x) parameters. Our analysis demonstrates that teleportation negatives reduce catastrophic forgetting and improve convergence speed for dense retrieval training. The source code of this paper is available at https: //github.com/OpenMatch/ANCE-Tele.",
        "id": 253237545
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a study that explores how language models are not robust to the surface form editing when testing commonsense knowledge?",
    "positive_ctxs": [
      {
        "title": "Did the Cat Drink the Coffee? Challenging Transformers with Generalized Event Knowledge",
        "text": "Prior research has explored the ability of computational models to predict a word semantic fit with a given predicate. While much work has been devoted to modeling the typicality relation between verbs and arguments in isolation, in this paper we take a broader perspective by assessing whether and to what extent computational approaches have access to the information about the typicality of entire events and situations described in language (Generalized Event Knowledge).",
        "id": 236318140
      }
    ],
    "negative_ctxs": [
      {
        "title": "Learning Logical Structures of Paragraphs in Legal Articles",
        "text": "This paper presents a new task, learning logical structures of paragraphs in legal articles, which is studied in research on Legal Engineering (Katayama, 2007). The goals of this task are recognizing logical parts of law sentences in a paragraph, and then grouping related logical parts into some logical structures of formulas, which describe logical relations between logical parts. We present a two-phase framework to learn logical structures of paragraphs in legal articles. In the first phase, we model the problem of recognizing logical parts in law sentences as a multi-layer sequence learning problem, and present a CRF-based model to recognize them. In the second phase, we propose a graph-based method to group logical parts into logical structures. We consider the problem of finding a subset of complete sub-graphs in a weighted-edge complete graph, where each node corresponds to a logical part, and a complete sub-graph corresponds to a logical structure. We also present an integer linear programming formulation for this optimization problem. Our models achieve 74.37% in recognizing logical parts, 79.59% in recognizing logical structures, and 55.73% in the whole task on the Japanese National Pension Law corpus.",
        "id": 17666550
      },
      {
        "title": "OpenNMT System Description for WNMT 2018: 800 words/sec on a single-core CPU",
        "text": "We present a system description of the OpenNMT Neural Machine Translation entry for the WNMT 2018 evaluation. In this work, we developed a heavily optimized NMT inference model targeting a high-performance CPU system. The final system uses a combination of four techniques, all of them leading to significant speed-ups in combination: (a) sequence distillation, (b) architecture modifications, (c) pre-computation, particularly of vocabulary, and (d) CPU targeted quantization. This work achieves the fastest performance of the shared task, and led to the development of new features that have been integrated to OpenNMT and made available to the community.",
        "id": 51793903
      },
      {
        "title": "",
        "text": "",
        "id": 241583236
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that investigates how neural language models' forecasts correlate with human linguistic processing, especially in terms of syntactic surprisal?",
    "positive_ctxs": [
      {
        "title": "Syntactic Surprisal From Neural Models Predicts, But Underestimates, Human Processing Difficulty From Syntactic Ambiguities",
        "text": "Humans exhibit garden path effects: When reading sentences that are temporarily structurally ambiguous, they slow down when the structure is disambiguated in favor of the less preferred alternative. Surprisal theory(Hale, 2001;Levy, 2008), a prominent explanation of this finding, proposes that these slowdowns are due to the unpredictability of each of the words that occur in these sentences. Challenging this hypothesis, van Schijndel and Linzen (2021) find that estimates of the cost of word predictability derived from language models severely underestimate the magnitude of human garden path effects. In this work, we consider whether this underestimation is due to the fact that humans weight syntactic factors in their predictions more highly than language models do. We propose a method for estimating syntactic predictability from a language model, allowing us to weigh the cost of lexical and syntactic predictability independently. We find that treating syntactic predictability independently from lexical predictability indeed results in larger estimates of garden path. At the same time, even when syntactic predictability is independently weighted, surprisal still greatly underestimate the magnitude of human garden path effects. Our results support the hypothesis that predictability is not the only factor responsible for the processing cost associated with garden path sentences.",
        "id": 253098758
      }
    ],
    "negative_ctxs": [
      {
        "title": "Baseline Models for Pronoun Prediction and Pronoun-Aware Translation",
        "text": "This paper presents baseline models for the cross-lingual pronoun prediction task and the pronoun-focused translation task at Dis-coMT 2015. We present simple yet effective classifiers for the former and discuss the impact of various contextual features on the prediction performance. In the translation task we rely on the document-level decoder Docent and a cross-sentence target language-model over selected words based on the parts-of-speech of the aligned source language words.",
        "id": 14151586
      },
      {
        "title": "Embedding Learning Through Multilingual Concept Induction",
        "text": "We present a new method for estimating vector space representations of words: embedding learning by concept induction. We test this method on a highly parallel corpus and learn semantic representations of words in 1259 different languages in a single common space. An extensive experimental evaluation on crosslingual word similarity and sentiment analysis indicates that concept-based multilingual embedding learning performs better than previous approaches.",
        "id": 46885888
      },
      {
        "title": "One Year of Contender: What Have We Learned about Assessing and Tuning Industrial Spoken Dialog Systems?",
        "text": "A lot. Since inception of Contender, a machine learning method tailored for computerassisted decision making in industrial spoken dialog systems, it was rolled out in over 200 instances throughout our applications processing nearly 40 million calls. The net effect of this data-driven method is a significantly increased system performance gaining about 100,000 additional automated calls every month.",
        "id": 15978451
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "manual_acl",
    "question": "Give me a paper proposing to circumvent a single-truth target in training generative language models.",
    "positive_ctxs": [
      {
        "title": "Soft Alignment Objectives for Robust Adaptation of Language Generation",
        "text": "Domain adaptation allows generative language models to address specific flaws caused by the domain shift of their application. However, the traditional adaptation by further training on indomain data rapidly weakens the model's ability to generalize to other domains, making the openended deployments of the adapted models prone to errors. This work introduces novel training objectives built upon a semantic similarity of the predicted tokens to the reference.Our results show that (1) avoiding the common assumption of a single correct prediction by constructing the training target from tokens' semantic similarity can largely mitigate catastrophic forgetting of adaptation, while (2) preserving the adaptation in-domain quality, (3) with negligible additions to compute costs. In the broader context, the objectives grounded in a continuous token similarity pioneer the exploration of the middle ground between the efficient but naïve exact-match token-level objectives and expressive but computationally-and resourceintensive sequential objectives. . 2020. A mixed learning objective for neural machine translation. In . 2020. Exploring the limits of transfer learning with a unified text-to-text transformer.",
        "id": 258947837
      }
    ],
    "negative_ctxs": [
      {
        "title": "Do Sentence Embeddings Capture Discourse Properties of Sentences from Scientific Abstracts ?",
        "text": "We introduce four tasks designed to determine which sentence encoders best capture discourse properties of sentences from scientific abstracts, namely coherence between clauses of a sentence, and discourse relations within sentences. We show that even if contextual encoders such as BERT or SciBERT encodes the coherence in discourse units, they do not help to predict three discourse relations commonly used in scientific abstracts. We discuss what these results underline, namely that these discourse relations are based on particular phrasing that allow non-contextual encoders to perform well.",
        "id": 226283881
      },
      {
        "title": "Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces",
        "text": "In this paper we consider semantic spaces consisting of objects from some particular domain (e.g. IMDB movie reviews). Various authors have observed that such semantic spaces often model salient features (e.g. how scary a movie is) as directions. These feature directions allow us to rank objects according to how much they have the corresponding feature, and can thus play an important role in interpretable classifiers, recommendation systems, or entity-oriented search engines, among others. Methods for learning semantic spaces, however, are mostly aimed at modelling similarity. In this paper, we argue that there is an inherent trade-off between capturing similarity and faithfully modelling features as directions. Following this observation, we propose a simple method to fine-tune existing semantic spaces, with the aim of improving the quality of their feature directions. Crucially, our method is fully unsupervised, requiring only a bag-of-words representation of the objects as input.1 https://github.com/ThomasAger/ Modelling-Salient-Features-as-Directions-in-Fine-Tuned-Semantic-Spaces",
        "id": 53103683
      },
      {
        "title": "",
        "text": "",
        "id": 226283537
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_nonacl",
    "question": "What research should I consult regarding the application of continuous vector prompts in language models instead of the conventional discrete token-level prompts?",
    "positive_ctxs": [
      {
        "title": "P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks",
        "text": "Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning(Li and Liang, 2021;Qin and Eisner, 2021)optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research. 1",
        "id": 248780177
      },
      {
        "title": "Learning How to Ask: Querying LMs with Mixtures of Soft Prompts",
        "text": "Natural-language prompts have recently been used to coax pretrained language models into performing other AI tasks, using a fill-in-theblank paradigm (Petroni et al., 2019) or a few-shot extrapolation paradigm (Brown et al.,  2020). For example, language models retain factual knowledge from their training corpora that can be extracted by asking them to \"fill in the blank\" in a sentential prompt. However, where does this prompt come from? We explore the idea of learning prompts by gradient descent-either fine-tuning prompts taken from previous work, or starting from random initialization. Our prompts consist of \"soft words,\" i.e., continuous vectors that are not necessarily word type embeddings from the language model. Furthermore, for each task, we optimize a mixture of prompts, learning which prompts are most effective and how to ensemble them. Across multiple English LMs and tasks, our approach hugely outperforms previous methods, showing that the implicit factual knowledge in language models was previously underestimated. Moreover, this knowledge is cheap to elicit: random initialization is nearly as good as informed initialization.",
        "id": 233231453
      }
    ],
    "negative_ctxs": [
      {
        "title": "Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA",
        "text": "The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pretrained transformer models in a bi-encoderbased dense passage retriever (DPR), concluding that the language-agnostic approach is superior. Additionally, the study investigates the impact of query rewriting techniques using large language models, such as ChatGPT, on multilingual, document-grounded questionanswering systems. The experiments conducted demonstrate that, for the examples examined, query rewriting does not enhance performance compared to the original queries. This failure is due to topic switching in final dialogue turns and irrelevant topics being considered for query rewriting.",
        "id": 259290499
      },
      {
        "title": "",
        "text": "",
        "id": 215768182
      },
      {
        "title": "Parameter Space Noise for Exploration",
        "text": "Deep reinforcement learning (RL) methods generally engage in exploratory behavior through noise injection in the action space. An alternative is to add noise directly to the agent's parameters, which can lead to more consistent exploration and a richer set of behaviors. Methods such as evolutionary strategies use parameter perturbations, but discard all temporal structure in the process and require significantly more samples. Combining parameter noise with traditional RL methods allows to combine the best of both worlds. We demonstrate that both off-and on-policy methods benefit from this approach through experimental comparison of DQN, DDPG, and TRPO on high-dimensional discrete action environments as well as continuous control tasks. Our results show that RL with parameter noise learns more efficiently than traditional RL with action space noise and evolutionary strategies individually.",
        "id": 2971655
      }
    ]
  },
  {
    "specificity": 0,
    "query_set": "inline_acl",
    "question": "What are some techniques or tools used in machine learning for matching and grounding annotated data to an existing knowledge base, particularly using sentence embedding-based cosine similarity or clustering algorithms?",
    "positive_ctxs": [
      {
        "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
        "text": "BERT(Devlin et al., 2018)and RoBERTa (Liu  et al., 2019)  has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods. 1",
        "id": 201646309
      }
    ],
    "negative_ctxs": [
      {
        "title": "Subword-based Compact Reconstruction of Word Embeddings",
        "text": "The idea of subword-based word embeddings has been proposed in the literature, mainly for solving the out-of-vocabulary (OOV) word problem observed in standard word-based word embeddings. In this paper, we propose a method of reconstructing pre-trained word embeddings using subword information that can effectively represent a large number of subword embeddings in a considerably small fixed space. The key techniques of our method are twofold: memory-shared embeddings and a variant of the key-value-query self-attention mechanism. Our experiments show that our reconstructed subword-based embeddings can successfully imitate well-trained word embeddings in a small fixed space while preventing quality degradation across several linguistic benchmark datasets, and can simultaneously predict effective embeddings of OOV words. We also demonstrate the effectiveness of our reconstruction method when we apply them to downstream tasks 1 .",
        "id": 174799852
      },
      {
        "title": "Selective-LAMA: Selective Prediction for Confidence-Aware Evaluation of Language Models",
        "text": "Recent studies have suggested that neural language models learn and store a large amount of facts and commonsense knowledge from training data. The ability of language models to restore such knowledge is often evaluated via zero-shot cloze-style QA tasks. However, such evaluations rely only on prediction accuracy without punishing the systems for their mistakes, e.g., simply guessing or hallucinating likely answers. Selective prediction is a more informative evaluation framework that takes the confidence of predictions into account. Under the selective prediction setting, a model is evaluated not only by the number of correct predictions, but also by the ability to filter out dubious predictions by estimating the confidence of individual predictions. Such confidence-aware evaluation is crucial for determining whether to trust zero-shot predictions of language models. In this paper, we apply the selective prediction setting to an existing benchmark, LAMA probe, and conduct extensive experiments with recent neural language models and different confidence functions. We empirically show that our Selective-LAMA evaluation is more robust to the effect of simple guesses than the conventional accuracy-based evaluation. Our evaluation reveals the importance of the choice of confidence functions by showing that simply relying on token probabilities is not always the best choice. Further analysis shows that various confidence functions exhibit different preferences over predicted tokens for a given context.",
        "id": 258378336
      },
      {
        "title": "THE KANERVA MACHINE: A GENERATIVE DISTRIBUTED MEMORY",
        "text": "We present an end-to-end trained memory system that quickly adapts to new data and generates samples like them. Inspired by Kanerva's sparse distributed memory, it has a robust distributed reading and writing mechanism. The memory is analytically tractable, which enables optimal on-line compression via a Bayesian update-rule. We formulate it as a hierarchical conditional generative model, where memory provides a rich data-dependent prior distribution. Consequently, the top-down memory and bottom-up perception are combined to produce the code representing an observation. Empirically, we demonstrate that the adaptive memory significantly improves generative models trained on both the Omniglot and CIFAR datasets. Compared with the Differentiable Neural Computer (DNC) and its variants, our memory model has greater capacity and is significantly easier to train.",
        "id": 3524564
      }
    ]
  }
]