[
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a study that explores the difficulties in creating shared multilingual vocabularies, especially focusing on the problem of over segmentation in low-resource languages?",
    "positive_ctxs": [
      {
        "title": "Multi-view Subword Regularization",
        "text": "Multilingual pretrained representations generally rely on subword segmentation algorithms to create a shared multilingual vocabulary. However, standard heuristic algorithms often lead to sub-optimal segmentation, especially for languages with limited amounts of data. In this paper, we take two major steps towards alleviating this problem. First, we demonstrate empirically that applying existing subword regularization methods(Kudo, 2018;Provilkov et al., 2020)during fine-tuning of pre-trained multilingual representations improves the effectiveness of cross-lingual transfer. Second, to take full advantage of different possible input segmentations, we propose Multi-view Subword Regularization (MVR), a method that enforces the consistency between predictions of using inputs tokenized by the standard and probabilistic segmentations. Results on the XTREME multilingual benchmark  show that MVR brings consistent improvements of up to 2.5 points over using standard segmentation algorithms. 1",
        "id": 232233194
      }
    ],
    "negative_ctxs": [
      {
        "title": "Constraints and Type Hierarchies for Korean Serial Verb Constructions -An Analytic Study within the HPSG Framework -*",
        "text": "This paper provides a fine-grained analysis of Korean serial verb constructions within the HPSG framework, and covers major descriptive characteristics of the phenomena. This paper discusses constraints on serial verb constructions in terms of four aspects; transitivity, argument structure, semantic properties, and complementizers. As a result, 17 constraints have been built, which support the type hierarchies for Korean serial verb constructions. This paper also presents a sample derivation on the basis of on the constraints and the type hierarchies.",
        "id": 7578809
      },
      {
        "title": "The Effect of Pitch Accenting on Pronoun Referent Resolution",
        "text": "By strictest interpretation, theories of both centering and intonational meaning fail to predict the existence of pitch accented pronominals. Yet they occur felicitously in spoken discourse. To explain this, I emphasize the dual functions served by pitch accents, as markers of both propositional (semantic/pragmatic) and attentional salience. This distinction underlies my proposals about the attentional consequences of pitch accents when applied to pronominals, in particular, that while most pitch accents may weaken or reinforce a cospecifier's status as the center of attention, a contrastively stressed pronominal may force a shift, even when contraindicated by textual features.",
        "id": 177600
      },
      {
        "title": "Figure Eight at SemEval-2019 Task 3: Ensemble of Transfer Learning Methods for Contextual Emotion Detection",
        "text": "This paper describes our transfer learningbased approach to contextual emotion detection as part of SemEval-2019 Task 3. We experiment with transfer learning using pretrained language models (ULMFiT, OpenAI GPT, and BERT) and fine-tune them on this task. We also train a deep learning model from scratch using pre-trained word embeddings and BiLSTM architecture with attention mechanism. The ensembled model achieves competitive result, ranking ninth out of 165 teams. The result reveals that ULMFiT performs best due to its superior fine-tuning techniques. We propose improvements for future work.",
        "id": 184483109
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that examines the effects of starting language models with weights from pretrained nondiffusion models on the convergence behavior of diffusion losses?",
    "positive_ctxs": [
      {
        "title": "SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control",
        "text": "Despite the growing success of diffusion models in continuous-valued domains (e.g., images), similar efforts for discrete domains such as text have yet to match the performance of autoregressive language models. In this work, we present SSD-LM-a diffusion-based language model with two key design choices. First, SSD-LM is semi-autoregressive, iteratively generating blocks of text, allowing for flexible output length at decoding time while enabling local bidirectional context updates. Second, it is simplex-based, performing diffusion on the natural vocabulary space rather than a learned latent space, allowing us to incorporate classifier guidance and modular control using offthe-shelf classifiers without any adaptation. We evaluate SSD-LM on unconstrained text generation benchmarks, and show that it matches or outperforms strong autoregressive GPT-2 models across standard quality and diversity metrics, while vastly outperforming diffusionbased baselines. On controlled text generation, SSD-LM also outperforms competitive baselines, with an extra advantage in modularity. 1 J. Atchison and S.M. Shen. 1980. Logistic-normal distributions:Some properties and uses. Biometrika, 67(2):261-272.",
        "id": 253237701
      }
    ],
    "negative_ctxs": [
      {
        "title": "Plug and Play Autoencoders for Conditional Text Generation",
        "text": "Text autoencoders are commonly used for conditional generation tasks such as style transfer. We propose methods which are plug and play, where any pretrained autoencoder can be used, and only require learning a mapping within the autoencoder's embedding space, training embedding-to-embedding (Emb2Emb). This reduces the need for labeled training data for the task and makes the training procedure more efficient. Crucial to the success of this method is a loss term for keeping the mapped embedding on the manifold of the autoencoder and a mapping which is trained to navigate the manifold by learning offset vectors. Evaluations on style transfer tasks both with and without sequence-to-sequence supervision show that our method performs better than or comparable to strong baselines while being up to four times faster.",
        "id": 222178257
      },
      {
        "title": "The overview of the SST speech corpus of Japanese learner English and evaluation through the experiment on automatic detection of learners' errors",
        "text": "This paper introduces an overview of the speech corpus of Japanese learner English compiled by National Institute of Information and Communications Technology by showing its data collection procedure and annotation schemes including error tagging. We have collected 1,200 interviews for three years. One of the most unique features of this corpus is that it contains rich information on learners' errors. We have performed error tagging for learners' grammatical and lexical errors with originally-designed error tagset. We also evaluated the corpus through the experiment on automatic detection of learners' errors by using error tag information in the corpus. We did this by using a machine learning model, Maximum Entropy (ME) model. Since we had obtained the limited amount of error-tagged data, we needed to make some efforts to enlarge training data. We added the correct sentences and artificially-made errors to the training data, and found that it improved accuracy. We are planning to make this corpus publicly available in the spring of 2004, so that teachers and researchers in many fields can use the data for their own interests, such as second language acquisition research, syllabus and material design, or the development of computerized pedagogical tools, by combining it with NLP technology.",
        "id": 9016696
      },
      {
        "title": "SWEAT: Scoring Polarization of Topics across Different Corpora",
        "text": "Understanding differences of viewpoints across corpora is a fundamental task for computational social sciences. In this paper, we propose the Sliced Word Embedding Association Test (SWEAT), a novel statistical measure to compute the relative polarization of a topical wordset across two distributional representations. To this end, SWEAT uses two additional wordsets, deemed to have opposite valence, to represent two different poles. We validate our approach and illustrate a case study to show the usefulness of the introduced measure.",
        "id": 237513556
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a study that explores a pre-trained multilingual text-to-text transformer applicable to text summarization tasks?",
    "positive_ctxs": [
      {
        "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
        "text": "The recent \"Text-to-Text Transfer Transformer\" (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We describe the design and modified training of mT5 and demonstrate its stateof-the-art performance on many multilingual benchmarks. All of the code and model checkpoints used in this work are publicly available. 1 * Equal Contribution.Please direct correspondence to",
        "id": 225040574
      }
    ],
    "negative_ctxs": [
      {
        "title": "MultiSubs: A Large-scale Multimodal and Multilingual Dataset",
        "text": "This paper introduces a large-scale multimodal and multilingual dataset that aims to facilitate research on grounding words to images in their contextual usage in language. The dataset consists of images selected to unambiguously illustrate concepts expressed in sentences from movie subtitles. The dataset is a valuable resource as (i) the images are aligned to text fragments rather than whole sentences; (ii) multiple images are possible for a text fragment and a sentence; (iii) the sentences are free-form and real-world like; (iv) the parallel texts are multilingual. We set up a fill-in-the-blank game for humans to evaluate the quality of the automatic image selection process of our dataset. We show the utility of the dataset on two automatic tasks: (i) fill-in-the blank; (ii) lexical translation. Results of the human evaluation and automatic models demonstrate that images can be a useful complement to the textual context. The dataset will benefit research on visual grounding of words especially in the context of free-form sentences, and can be obtained from https://doi.org/10.5281/zenodo.5034604 under a Creative Commons licence.",
        "id": 232092534
      },
      {
        "title": "A Unified Approach in Speech-to-Speech Translation: Integrating Features of Speech Recognition and Machine Translation",
        "text": "Based upon a statistically trained speech translation system, in this study, we try to combine distinctive features derived from the two modules: speech recognition and statistical machine translation, in a loglinear model. The translation hypotheses are then rescored and translation performance is improved. The standard translation evaluation metrics, including BLEU, NIST, multiple reference word error rate and its position independent counterpart, were optimized to solve the weights of the features in the log-linear model. The experimental results have shown significant improvement over the baseline IBM model 4 in all automatic translation evaluation metrics. The largest was for BLEU, by 7.9% absolute.",
        "id": 7150694
      },
      {
        "title": "The Role of Prosody and Speech Register in Word Segmentation: A Computational Modelling Perspective",
        "text": "This study explores the role of speech register and prosody for the task of word segmentation. Since these two factors are thought to play an important role in early language acquisition, we aim to quantify their contribution for this task. We study a Japanese corpus containing both infant-and adult-directed speech and we apply four different word segmentation models, with and without knowledge of prosodic boundaries. The results showed that the difference between registers is smaller than previously reported and that prosodic boundary information helps more adult-than infant-directed speech.",
        "id": 29767064
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which foundation model paper first proposed a time series model with proposed financial time series and text data?",
    "positive_ctxs": [
      {
        "title": "TEMPO: PROMPT-BASED GENERATIVE PRE-TRAINED TRANSFORMER FOR TIME SERIES FORECASTING",
        "text": "The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the selection-based prompts to facilitate distribution adaptation in non-stationary time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPOover state-of-the-art methods on a number of time series benchmark datasets. This performance gain is observed not only in standard supervised learning settings but also in scenarios involving previously unseen datasets as well as in scenarios with multi-modal inputs. This compelling finding highlights TEMPO's potential to constitute a foundational model-building framework.",
        "id": 263829348
      }
    ],
    "negative_ctxs": [
      {
        "title": "Prototypical Opinion Holders: What We can Learn from Experts and Analysts",
        "text": "In order to automatically extract opinion holders, we propose to harness the contexts of prototypical opinion holders, i.e. common nouns, such as experts or analysts, that describe particular groups of people whose profession or occupation is to form and express opinions towards specific items. We assess their effectiveness in supervised learning where these contexts are regarded as labeled training data and in rule-based classification which uses predicates that frequently co-occur with mentions of the prototypical opinion holders. Finally, we also examine in how far knowledge gained from these contexts can compensate the lack of large amounts of labeled training data in supervised learning by considering various amounts of actually labeled training sets.",
        "id": 13923239
      },
      {
        "title": "Grouping Words Using Statistical Context",
        "text": "This paper describes the use of statistical analyses of untagged corpora to detect similarities and differences in the meaning of words in text. This work is motivated by psychological as well as by computational issues. The limitations of the method of cluster analysis in assessing the success of such analyses are discussed, and ongoing research using an alternative unsupervised neural network approach is described.",
        "id": 9687117
      },
      {
        "title": "SISER: Semantic-Infused Selective Graph Reasoning for Fact Verification",
        "text": "This study proposes Semantic-Infused SElective Graph Reasoning (SISER) for fact verification, which newly presents semantic-level graph reasoning and injects its reasoning-enhanced representation into other types of graph-based and sequence-based reasoning methods. SISER combines three reasoning types: 1) semantic-level graph reasoning, which uses a semantic graph from evidence sentences, whose nodes are elements of a triple -<Subject, Verb, Object>, 2) \"semantic-infused\" sentence-level \"selective\" graph reasoning, which combine semanticlevel and sentence-level representations and perform graph reasoning in a selective manner using the node selection mechanism, and 3) sequence reasoning, which concatenates all evidence sentences and performs attentionbased reasoning. Experiment results on a large-scale dataset for Fact Extraction and VERification (FEVER) show that SISER outperforms the previous graph-based approaches and achieves state-of-the-art performance. † Corresponding author ♠ Equal contribution * This work was done during his M.S. degree at JBNU. 1 https://competitions.codalab.org/ competitions/18814",
        "id": 252819029
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research that evaluates the performance decline in various language models, like BLOOM, under 4-bit integer columnar weight-only quantization?",
    "positive_ctxs": [
      {
        "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling",
        "text": "There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5).However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation.We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge.GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks.Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks.On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25× parameters of BERT Large , demonstrating its generalizability to different downstream tasks. 1 * The first two authors contributed equally.† Corresponding authors.2 Unconditional generation refers to generating text as a language model without finetuning, while conditional generation refers to sequence-to-sequence tasks.",
        "id": 247519241
      }
    ],
    "negative_ctxs": [
      {
        "title": "SSMT: A Machine Translation Evaluation View to Paragraph-to-Sentence Semantic Similarity",
        "text": "This paper presents the system SSMT measuring the semantic similarity between a paragraph and a sentence submitted to the SemEval 2014 task3: Cross-level Semantic Similarity. The special difficulty of this task is the length disparity between the two semantic comparison texts. We adapt several machine translation evaluation metrics for features to cope with this difficulty, then train a regression model for the semantic similarity prediction. This system is straightforward in intuition and easy in implementation. Our best run gets 0.808 in Pearson correlation. METEORderived features are the most effective ones in our experiment.",
        "id": 17865660
      },
      {
        "title": "TIGHT RATES IN SUPERVISED OUTLIER TRANSFER LEARNING",
        "text": "A critical barrier to learning an accurate decision rule for outlier detection is the scarcity of outlier data.As such, practitioners often turn to the use of similar but imperfect outlier data from which they might transfer information to the target outlier detection task.Despite the recent empirical success of transfer learning approaches in outlier detection, a fundamental understanding of when and how knowledge can be transferred from a source to a target outlier detection task remains elusive.In this work, we adopt the traditional framework of Neyman-Pearson classification-which formalizes supervised outlier detection-with the added assumption that one has access to some related but imperfect outlier data.Our main results are as follows:• We first determine the information-theoretic limits of the problem under a measure of discrepancy that extends some existing notions from traditional balanced classification; interestingly, unlike in balanced classification, seemingly very dissimilar sources can provide much information about a target, thus resulting in fast transfer.• We then show that, in principle, these information-theoretic limits are achievable by adaptive procedures, i.e., procedures with no a priori information on the discrepancy between source and target outlier distributions.",
        "id": 263828899
      },
      {
        "title": "",
        "text": "",
        "id": 218974526
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "What concerns or key points have been highlighted in scholarly articles about employing random divisions in machine learning datasets, especially with respect to contamination of the test set?",
    "positive_ctxs": [
      {
        "title": "We Need to Talk About Random Splits",
        "text": "Gorman and Bedrick (2019) argued for using random splits rather than standard splits in NLP experiments. We argue that random splits, like standard splits, lead to overly optimistic performance estimates. We can also split data in biased or adversarial ways, e.g., training on short sentences and evaluating on long ones. Biased sampling has been used in domain adaptation to simulate real-world drift; this is known as the covariate shift assumption. In NLP, however, even worst-case splits, maximizing bias, often under-estimate the error observed on new samples of in-domain data, i.e., the data that models should minimally generalize to at test time. This invalidates the covariate shift assumption. Instead of using multiple random splits, future benchmarks should ideally include multiple, independent test sets instead; if infeasible, we argue that multiple biased splits leads to more realistic performance estimates than multiple random splits.",
        "id": 218487319
      }
    ],
    "negative_ctxs": [
      {
        "title": "End-to-End Non-Factoid Question Answering with an Interactive Visualization of Neural Attention Weights",
        "text": "Advanced attention mechanisms are an important part of successful neural network approaches for non-factoid answer selection because they allow the models to focus on few important segments within rather long answer texts. Analyzing attention mechanisms is thus crucial for understanding strengths and weaknesses of particular models. We present an extensible, highly modular service architecture that enables the transformation of neural network models for non-factoid answer selection into fully featured end-to-end question answering systems. The primary objective of our system is to enable researchers a way to interactively explore and compare attentionbased neural networks for answer selection. Our interactive user interface helps researchers to better understand the capabilities of the different approaches and can aid qualitative analyses. The source-code of our system is publicly available. 1",
        "id": 20020443
      },
      {
        "title": "Why Is MBTI Personality Detection from Texts a Difficult Task?",
        "text": "Automatic detection of the four MBTI personality dimensions from texts has recently attracted noticeable attention from the natural language processing and computational linguistic communities. Despite the large collections of Twitter data for training, the best systems rarely even outperform the majority-class baseline. In this paper, we discuss the theoretical reasons for such low results and present the insights from an annotation study that further shed the light on this issue.",
        "id": 233189643
      },
      {
        "title": "A Rate-Distortion view of human pragmatic reasoning",
        "text": "What computational principles underlie human pragmatic reasoning? A prominent approach to pragmatics is the Rational Speech Act (RSA) framework, which formulates pragmatic reasoning as probabilistic speakers and listeners recursively reasoning about each other. While RSA enjoys broad empirical support, it is not yet clear whether the dynamics of such recursive reasoning may be governed by a general optimization principle. Here, we present a novel analysis of the RSA framework that addresses this question. First, we show that RSA recursion implements an alternating maximization for optimizing a tradeoff between expected utility and communicative effort. On that basis, we study the dynamics of RSA recursion and disconfirm the conjecture that expected utility is guaranteed to improve with recursion depth. Second, we show that RSA can be grounded in Rate-Distortion theory, while maintaining a similar ability to account for human behavior and avoiding a bias of RSA toward random utterance production. This work furthers the mathematical understanding of RSA models, and suggests that general information-theoretic principles may give rise to human pragmatic reasoning.",
        "id": 218628625
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "What research exists on using reinforcement learning methods for event prediction in temporal knowledge graphs?",
    "positive_ctxs": [
      {
        "title": "TimeTraveler: Reinforcement Learning for Temporal Knowledge Graph Forecasting",
        "text": "Temporal knowledge graph (TKG) reasoning is a crucial task that has gained increasing research interest in recent years. Most existing methods focus on reasoning at past timestamps to complete the missing facts, and there are only a few works of reasoning on known TKGs to forecast future facts. Compared with the completion task, the forecasting task is more difficult and faces two main challenges:(1) how to effectively model the time information to handle future timestamps? (2) how to make inductive inference to handle previously unseen entities that emerge over time? To address these challenges, we propose the first reinforcement learning method for forecasting. Specifically, the agent travels on historical knowledge graph snapshots to search for the answer. Our method defines a relative time encoding function to capture the timespan information, and we design a novel time-shaped reward based on Dirichlet distribution to guide the model learning. Furthermore, we propose a novel representation method for unseen entities to improve the inductive inference ability of the model. We evaluate our method for this link prediction task at future timestamps. Extensive experiments on four benchmark datasets demonstrate substantial performance improvement meanwhile with higher explainability, less calculation, and fewer parameters when compared with existing stateof-the-art methods.",
        "id": 237454564
      }
    ],
    "negative_ctxs": [
      {
        "title": "A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data",
        "text": "Understanding unstructured text is a major goal within natural language processing. Comprehension tests pose questions based on short text passages to evaluate such understanding. In this work, we investigate machine comprehension on the challenging MCTest benchmark. Partly because of its limited size, prior work on MCTest has focused mainly on engineering better features. We tackle the dataset with a neural approach, harnessing simple neural networks arranged in a parallel hierarchy. The parallel hierarchy enables our model to compare the passage, question, and answer from a variety of trainable perspectives, as opposed to using a manually designed, rigid feature set. Perspectives range from the word level to sentence fragments to sequences of sentences; the networks operate only on word-embedding representations of text. When trained with a methodology designed to help cope with limited training data, our Parallel-Hierarchical model sets a new state of the art for MCTest, outperforming previous feature-engineered approaches slightly and previous neural approaches by a significant margin (over 15% absolute).",
        "id": 12834729
      },
      {
        "title": "Jigg: A Framework for an Easy Natural Language Processing Pipeline",
        "text": "We present Jigg, a Scala (or JVMbased) NLP annotation pipeline framework, which is easy to use and is extensible. Jigg supports a very simple interface similar to Stanford CoreNLP, the most successful NLP pipeline toolkit, but has more flexibility to adapt to new types of annotation. On this framework, system developers can easily integrate their downstream system into a NLP pipeline from a raw text by just preparing a wrapper of it.",
        "id": 14941111
      },
      {
        "title": "Multiple Sclerosis Severity Classification From Clinical Text",
        "text": "Multiple Sclerosis (MS) is a chronic, inflammatory and degenerative neurological disease, which is monitored by a specialist using the Expanded Disability Status Scale (EDSS) and recorded in unstructured text in the form of a neurology consult note. An EDSS measurement contains an overall 'EDSS' score and several functional subscores. Typically, expert knowledge is required to interpret consult notes and generate these scores. Previous approaches used limited context length Word2Vec embeddings and keyword searches to predict scores given a consult note, but often failed when scores were not explicitly stated. In this work, we present MS-BERT, the first publicly available transformer model trained on real clinical data other than MIMIC. Next, we present MSBC, a classifier that applies MS-BERT to generate embeddings and predict EDSS and functional subscores. Lastly, we explore combining MSBC with other models through the use of Snorkel to generate scores for unlabelled consult notes. MSBC achieves state-of-the-art performance on all metrics and prediction tasks and outperforms the models generated from the Snorkel ensemble. We improve Macro-F1 by 0.12 (to 0.88) for predicting EDSS and on average by 0.29 (to 0.63) for predicting functional subscores over previous Word2Vec CNN and rule-based approaches.",
        "id": 225103074
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "What research has been conducted on news recommendation engines that consider individual user preferences as well as the time-sensitive popularity of news content?",
    "positive_ctxs": [
      {
        "title": "PP-Rec: News Recommendation with Personalized User Interest and Time-aware News Popularity",
        "text": "Personalized news recommendation methods are widely used in online news services. These methods usually recommend news based on the matching between news content and user interest inferred from historical behaviors. However, these methods usually have difficulties in making accurate recommendations to cold-start users, and tend to recommend similar news with those users have read. In general, popular news usually contain important information and can attract users with different interests. Besides, they are usually diverse in content and topic. Thus, in this paper we propose to incorporate news popularity information to alleviate the cold-start and diversity problems for personalized news recommendation. In our method, the ranking score for recommending a candidate news to a target user is the combination of a personalized matching score and a news popularity score. The former is used to capture the personalized user interest in news. The latter is used to measure timeaware popularity of candidate news, which is predicted based on news content, recency, and real-time CTR using a unified framework. Besides, we propose a popularity-aware user encoder to eliminate the popularity bias in user behaviors for accurate interest modeling. Experiments on two real-world datasets show our method can effectively improve the accuracy and diversity for news recommendation.",
        "id": 235294032
      }
    ],
    "negative_ctxs": [
      {
        "title": "Multi-domain Cross-lingual Information Extraction from Clean and Noisy Texts",
        "text": "We have created a human-annotated, multi-event, cross-lingual corpus of equivalent summaries in Spanish and English to investigate cross-lingual information extraction. The corpus contains, in addition to pairs of equivalent non-translated summaries, automatic translations of each summary produced using an available translation tool. We have developed trainable information extraction systems per language and have applied them to both original summaries and their automatic translations obtaining encouraging results.Resumo. Apresentamos um estudo de extração de informações de um corpus bilíngüe paralelo em espanhol e inglês. O corpus está formado por pares de resumos curtos de eventos em três domínios de aplicação. Temos desenvolvido sistemas de extração de informaçoes para as duas línguas estudadas e avaliado o desempenho do sistema em varias experiências tanto monolíngües como translíngües. Apresentamos uma análise dos resultados obtidos.",
        "id": 14239468
      },
      {
        "title": "Implicit readability ranking using the latent variable of a Bayesian Probit model",
        "text": "Data driven approaches to readability analysis for languages other than English has been plagued by a scarcity of suitable corpora. Often, relevant corpora consist only of easy-to-read texts with no rank information or empirical readability scores, making only binary approaches, such as classification, applicable. We propose a Bayesian, latent variable, approach to get the most out of these kinds of corpora. In this paper we present results on using such a model for readability ranking. The model is evaluated on a preliminary corpus of ranked student texts with encouraging results. We also assess the model by showing that it performs readability classification on par with a state of the art classifier while at the same being transparent enough to allow more sophisticated interpretations.This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/",
        "id": 2334547
      },
      {
        "title": "",
        "text": "",
        "id": 219309834
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "What paper first used the technique of prompt engineering to generate adversarial prompts that can fool LLMs into making wrong predictions in prompt-based learning?",
    "positive_ctxs": [
      {
        "title": "AN LLM CAN FOOL ITSELF: A PROMPT-BASED ADVERSARIAL ATTACK",
        "text": "The wide-ranging applications of large language models (LLMs), especially in safety-critical domains, necessitate the proper evaluation of the LLM's adversarial robustness.This paper proposes an efficient tool to audit the LLM's adversarial robustness via a prompt-based adversarial attack (PromptAttack).PromptAttack converts adversarial textual attacks into an attack prompt that can cause the victim LLM to output the adversarial sample to fool itself.The attack prompt is composed of three important components: (1) original input (OI) including the original sample and its ground-truth label, (2) attack objective (AO) illustrating a task description of generating a new sample that can fool itself without changing the semantic meaning, and (3) attack guidance (AG) containing the perturbation instructions to guide the LLM on how to complete the task by perturbing the original sample at character, word, and sentence levels, respectively.Besides, we use a fidelity filter to ensure that PromptAttack maintains the original semantic meanings of the adversarial examples.Further, we enhance the attack power of PromptAttack by ensembling adversarial examples at different perturbation levels.Comprehensive empirical results using Llama2 and GPT-3.5 validate that PromptAttack consistently yields a much higher attack success rate compared to AdvGLUE and AdvGLUE++.Interesting findings include that a simple emoji can easily mislead GPT-3.5 to make wrong predictions.Our project page is available at PromptAttack.",
        "id": 264406064
      }
    ],
    "negative_ctxs": [
      {
        "title": "How to Record the Meaning of Figurative Language How to Record the Meaning of Figurative Language",
        "text": "This paper focuses on the question of what kind of data needs to be recorded about figurative language, in order to capture the essential meaning of the text and to enable us to re-create a synonymous text, based on that data. A short review of the best known systems of semantic annotation will be presented and their suitability for the task will be analyzed. Also, a method that could be used for representing the meaning of the idioms, metaphors and metonymy in the data model will be considered.",
        "id": 56336392
      },
      {
        "title": "Temporal dynamics of semantic relations in word embeddings: an application to predicting armed conflict participants",
        "text": "This paper deals with using word embedding models to trace the temporal dynamics of semantic relations between pairs of words. The set-up is similar to the well-known analogies task, but expanded with a time dimension. To this end, we apply incremental updating of the models with new training texts, including incremental vocabulary expansion, coupled with learned transformation matrices that let us map between members of the relation. The proposed approach is evaluated on the task of predicting insurgent armed groups based on geographical locations. The gold standard data for the time span 1994-2010 is extracted from the UCDP Armed Conflicts dataset. The results show that the method is feasible and outperforms the baselines, but also that important work still remains to be done.",
        "id": 3160550
      },
      {
        "title": "Mapping Language to Code in Programmatic Context",
        "text": "Source code is rarely written in isolation. It depends significantly on the programmatic context, such as the class that the code would reside in. To study this phenomenon, we introduce the task of generating class member functions given English documentation and the programmatic context provided by the rest of the class. This task is challenging because the desired code can vary greatly depending on the functionality the class provides (e.g., a sort function may or may not be available when we are asked to \"return the smallest element\" in a particular member variable list). We introduce CONCODE, a new large dataset with over 100,000 examples consisting of Java classes from online code repositories, and develop a new encoder-decoder architecture that models the interaction between the method documentation and the class environment. We also present a detailed error analysis suggesting that there is significant room for future work on this task.",
        "id": 52125417
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there any paper that tries to investigate LLMs’ capabilities in solving elliptical constructions by using a test-dataset based on the psycolinguistic notion of Thematic Fit?",
    "positive_ctxs": [
      {
        "title": "We Understand Elliptical Sentences, and Language Models Should Too: A New Dataset for Studying Ellipsis and its Interaction with Thematic Fit",
        "text": "Ellipsis is a linguistic phenomenon characterized by the omission of one or more sentence elements. Solving such a linguistic construction is not a trivial issue in natural language processing since it involves the retrieval of non-overtly expressed verbal material, which might in turn require the model to integrate human-like syntactic and semantic knowledge. In this paper, we explored the issue of how the prototypicality of event participants affects the ability of Language Models (LMs) to handle elliptical sentences, and to identify the omitted arguments at different degrees of thematic fit, ranging from highly typical participants to semantically anomalous ones. With this purpose in mind, we built ELLie, the first dataset composed entirely of utterances containing different types of elliptical constructions, and structurally suited for evaluating the effect of argument thematic fit in solving ellipsis and reconstructing the missing element. Our tests demonstrated that the probability scores assigned by the models are higher for typical events than for atypical and impossible ones in different elliptical contexts, confirming the influence of prototypicality of the event participants in interpreting such linguistic structures. Finally, we conducted a retrieval task of the elided verb in the sentence in which the low performance of LMs highlighted a considerable difficulty in reconstructing the correct event.",
        "id": 259370867
      }
    ],
    "negative_ctxs": [
      {
        "title": "Discourse Understanding and Factual Consistency in Abstractive Summarization",
        "text": "We introduce a general framework for abstractive summarization with factual consistency and distinct modeling of the narrative flow in an output summary. Our work addresses current limitations of models for abstractive summarization that often hallucinate information or generate summaries with coherence issues.To generate abstractive summaries with factual consistency and narrative flow, we propose Cooperative Generator -Discriminator Networks (Co-opNet), a novel transformerbased framework where a generator works with a discriminator architecture to compose coherent long-form summaries. We explore four different discriminator objectives which each capture a different aspect of coherence, including whether salient spans of generated abstracts are hallucinated or appear in the input context, and the likelihood of sentence adjacency in generated abstracts.We measure the ability of Co-opNet to learn these objectives with arXiv scientific papers, using the abstracts as a proxy for gold longform scientific article summaries. Empirical results from automatic and human evaluations demonstrate that Co-opNet learns to summarize with considerably improved global coherence compared to competitive baselines.",
        "id": 233189560
      },
      {
        "title": "Modeling human-like morphological prediction",
        "text": "We test a model of morphological prediction based on analogical deduction using phonemic similarity by applying it to German plural suffix prediction for a set of 24 nonce forms for whichMcCurdy et al. (2020)elicited human judgements, and which they found were poorly matched by productions of an encoder-decoder model ofKirov and Cotterell (2018). Their results raise the question of what kinds of models best mirror human judgements. We show that the predictions of the analogical models we tested mirror human judgements better than the encoder-decoder model.",
        "id": 247694105
      },
      {
        "title": "SemEval-2013 Task 1: TEMPEVAL-3: Evaluating Time Expressions, Events, and Temporal Relations",
        "text": "Within the SemEval-2013 evaluation exercise, the TempEval-3 shared task aims to advance research on temporal information processing. It follows on from TempEval-1 and -2, with: a three-part structure covering temporal expression, event, and temporal relation extraction; a larger dataset; and new single measures to rank systems -in each task and in general. In this paper, we describe the participants' approaches, results, and the observations from the results, which may guide future research in this area.",
        "id": 640783
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a study that explores employing variational autoencoders to standardize open knowledge graphs?",
    "positive_ctxs": [
      {
        "title": "Open Knowledge Graphs Canonicalization using Variational Autoencoders",
        "text": "Noun phrases and Relation phrases in open knowledge graphs are not canonicalized, leading to an explosion of redundant and ambiguous subject-relation-object triples. Existing approaches to solve this problem take a two-step approach. First, they generate embedding representations for both noun and relation phrases, then a clustering algorithm is used to group them using the embeddings as features. In this work, we propose Canonicalizing Using Variational Autoencoders (CUVA) 1 , a joint model to learn both embeddings and cluster assignments in an end-to-end approach, which leads to a better vector representation for the noun and relation phrases. Our evaluation over multiple benchmarks shows that CUVA outperforms the existing state-of-the-art approaches. Moreover, we introduce CANONICNELL, a novel dataset to evaluate entity canonicalization systems.",
        "id": 238198752
      }
    ],
    "negative_ctxs": [
      {
        "title": "Evaluation methodologies in Automatic Question Generation",
        "text": "In the last few years Automatic Question Generation (AQG) has attracted increasing interest. In this paper we survey the evaluation methodologies used in AQG. Based on a sample of 37 papers, our research shows that the systems' development has not been accompanied by similar developments in the methodologies used for the systems' evaluation. Indeed, in the papers we examine here, we find a wide variety of both intrinsic and extrinsic evaluation methodologies. Such diverse evaluation practices make it difficult to reliably compare the quality of different generation systems. Our study suggests that, given the rapidly increasing level of research in the area, a common framework is urgently needed to compare the performance of AQG systems and NLG systems more generally.1 http://aclweb.org/anthology/",
        "id": 53235639
      },
      {
        "title": "Constraint Grammar parsing with left and right sequential finite transducers",
        "text": "We propose an approach to parsing Constraint Grammars using finite-state transducers and report on a compiler that converts Constraint Grammar rules into transducer representations. The resulting transducers are further optimized by conversion to left and right sequential transducers. Using the method, we show that we can improve on the worstcase asymptotic bound of Constraint Grammar parsing from cubic to quadratic in the length of input sentences.",
        "id": 15217163
      },
      {
        "title": "Learning to Paraphrase for Question Answering",
        "text": "Question answering (QA) systems are sensitive to the many different ways natural language expresses the same information need. In this paper we turn to paraphrases as a means of capturing this knowledge and present a general framework which learns felicitous paraphrases for various QA tasks. Our method is trained end-toend using question-answer pairs as a supervision signal. A question and its paraphrases serve as input to a neural scoring model which assigns higher weights to linguistic expressions most likely to yield correct answers. We evaluate our approach on QA over Freebase and answer sentence selection. Experimental results on three datasets show that our framework consistently improves performance, achieving competitive results despite the use of simple QA models.",
        "id": 1282002
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there a paper comparing knowledge distillation and human annotation in terms of cost efficiency?",
    "positive_ctxs": [
      {
        "title": "Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models",
        "text": "Fine-tuning large models is highly effective, however, inference can be expensive and produces carbon emissions. Knowledge distillation has been shown to be a practical solution to reduce inference costs, but the distillation process itself requires significant computational resources. Rather than buying or renting GPUs to fine-tune, then distill a large model, an NLP practitioner might instead choose to allocate the available budget to hire annotators and manually label additional fine-tuning data. In this paper, we investigate how to most efficiently use a fixed budget to build a compact model. Through extensive experiments on six diverse tasks, we show that distilling from T5-XXL (11B) to T5-Small (60M) is almost always a cost-efficient strategy compared to annotating more data to directly train a compact model (T5-Small). We further investigate how the optimal budget allocated towards computation varies across scenarios. We will make our code, datasets, annotation cost estimates, and baseline models available as a benchmark to support further work on cost-efficient training of compact models.",
        "id": 258436868
      }
    ],
    "negative_ctxs": [
      {
        "title": "NILC at CWI 2018: Exploring Feature Engineering and Feature Learning",
        "text": "This paper describes the results of NILC team at CWI 2018. We developed solutions following three approaches: (i) a feature engineering method using lexical, n-gram and psycholinguistic features, (ii) a shallow neural network method using only word embeddings, and (iii) a Long Short-Term Memory (LSTM) language model, which is pre-trained on a large text corpus to produce a contextualized word vector. The feature engineering method obtained our best results for the classification task and the LSTM model achieved the best results for the probabilistic classification task. Our results show that deep neural networks are able to perform as well as traditional machine learning methods using manually engineered features for the task of complex word identification in English.",
        "id": 46940692
      },
      {
        "title": "How Effective is Task-Agnostic Data Augmentation for Pretrained Transformers?",
        "text": "Task-agnostic forms of data augmentation have proven widely effective in computer vision, even on pretrained models. In NLP similar results are reported most commonly for low data regimes, non-pretrained models, or situationally for pretrained models. In this paper we ask how effective these techniques really are when applied to pretrained transformers. Using two popular varieties of task-agnostic data augmentation (not tailored to any particular task), Easy Data Augmentation (Wei and Zou, 2019) and Back-Translation (Sennrich et al., 2015), we conduct a systematic examination of their effects across 5 classification tasks, 6 datasets, and 3 variants of modern pretrained transformers, including BERT, XL-NET, and ROBERTA. We observe a negative result, finding that techniques which previously reported strong improvements for nonpretrained models fail to consistently improve performance for pretrained transformers, even when training data is limited. We hope this empirical analysis helps inform practitioners where data augmentation techniques may confer improvements.",
        "id": 222132977
      },
      {
        "title": "Predicting and Using Implicit Discourse Elements in Chinese-English Translation",
        "text": "In machine translation (MT) implicitation can occur when elements such as discourse markers and pronouns are not expected or mandatory in the source language, but need to be realised in the target language for a coherent translation. These 'implicit' elements can be seen as both a barrier to MT and an important source of information. However, identifying where such elements are needed and producing them are non-trivial tasks. In this paper we examine the effect of implicit elements on MT and propose methods to identify and make them explicit. As a starting point, we use human translated and aligned data to decide where to insert place holders for these elements. We then fully automate this process by devising a prediction model to decide if and where implicit elements should occur and be made explicit. Our experiments compare statistical machine translation models built with and without these explicitation processes. Models built on data marked for discourse elements show substantial improvements over the baseline.",
        "id": 14873664
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Is there an existing dataset of images with alt-text that also includes the text the image was originally posted with?",
    "positive_ctxs": [
      {
        "title": "ALT-TEXT WITH CONTEXT: IMPROVING ACCESSIBILITY FOR IMAGES ON TWITTER",
        "text": "In this work we present an approach for generating alternative text (or alt-text) descriptions for images shared on social media, specifically Twitter.More than just a special case of image captioning, alt-text is both more literally descriptive and context-specific.Also critically, images posted to Twitter are often accompanied by user-written text that despite not necessarily describing the image may provide useful context that if properly leveraged can be informative.We address this task with a multimodal model that conditions on both textual information from the associated social media post as well as visual signal from the image, and demonstrate that the utility of these two information sources stacks.We put forward a new dataset of 371k images paired with alt-text and tweets scraped from Twitter and evaluate on it across a variety of automated metrics as well as human evaluation.We show that our approach of conditioning on both tweet text and visual information significantly outperforms prior work, by more than 2x on BLEU@4.",
        "id": 258865444
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 219300633
      },
      {
        "title": "Fast Mapping in Word Learning: What Probabilities Tell Us",
        "text": "Children can determine the meaning of a new word from hearing it used in a familiar context-an ability often referred to as fast mapping. In this paper, we study fast mapping in the context of a general probabilistic model of word learning. We use our model to simulate fast mapping experiments on children, such as referent selection and retention. The word learning model can perform these tasks through an inductive interpretation of the acquired probabilities. Our results suggest that fast mapping occurs as a natural consequence of learning more words, and provides explanations for the (occasionally contradictory) child experimental data.",
        "id": 7037231
      },
      {
        "title": "Improving the Template Generation for Chinese Character Error Detection with Confusion Sets",
        "text": "In this paper, we propose a system that automatically generates templates for detecting Chinese character errors. We first collect the confusion sets for each high-frequency Chinese character. Error types include pronunciation-related errors and radical-related errors. With the help of the confusion sets, our system generates possible error patterns in context, which will be used as detection templates. Combined with a word segmentation module, our system generates more accurate templates. The experimental results show the precision of performance approaches 95%. Such a system should not only help teachers grade and check student essays, but also effectively help students learn how to write.Compared with the detection of spelling errors in English, the detection of incorrect Chinese characters is much more difficult. In English, a word consists of a series of letters while a meaningful Chinese word usually consists of 2 to 4 Chinese characters. The difficulty lies partly in the fact that there are more than 5,000 high-frequency characters.In previous works on Chinese character error detection systems (",
        "id": 18657652
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there a paper that applies large language models to visual Raven’s Progressive Matrices?",
    "positive_ctxs": [
      {
        "title": "In-Context Analogical Reasoning with Pre-Trained Language Models",
        "text": "Analogical reasoning is a fundamental capacity of human cognition that allows us to reason abstractly about novel situations by relating them to past experiences. While it is thought to be essential for robust reasoning in AI systems, conventional approaches require significant training and/or hard-coding of domain knowledge to be applied to benchmark tasks. Inspired by cognitive science research that has found connections between human language and analogy-making, we explore the use of intuitive language-based abstractions to support analogy in AI systems. Specifically, we apply large pre-trained language models (PLMs) to visual Raven's Progressive Matrices (RPM), a common relational reasoning test. By simply encoding the perceptual features of the problem into language form, we find that PLMs exhibit a striking capacity for zero-shot relational reasoning, exceeding human performance and nearing supervised vision-based methods. We explore different encodings that vary the level of abstraction over task features, finding that higherlevel abstractions further strengthen PLMs' analogical reasoning. Our detailed analysis reveals insights on the role of model complexity, incontext learning, and prior knowledge in solving RPM tasks.",
        "id": 258959097
      }
    ],
    "negative_ctxs": [
      {
        "title": "Automatic Alignment of Japanese and English Newspaper Articles using an MT System and a Bilingual Company Name Dictionary",
        "text": "One of the crucial parts of any corpus-based machine translation system is a large-scale bilingual corpus that is aligned at various levels such, as the sentence and phrase levels. This kind of corpus, however, is not easy to obtain, and accordingly, there is a great need for an efficient construction method. We approach this problem by integrating two large monolingual corpora in two different languages sharing the same source of information. We often see such a situation in journalistic texts where the same events are reported in many languages. Unfortunately, they often lack article-level alignment information and the recovery of this is the first problem to solve. In this paper, we report a method of automatically aligning Japanese and English newspaper articles in the financial and economic news domain. Although conventional methods require some manual work, the proposed method works fully automatically. We show that our method can align such newspaper articles with an accuracy of 97%.",
        "id": 1446880
      },
      {
        "title": "Translation Memory Systems Have a Long Way to Go",
        "text": "The TM memory systems changed the work of translators and now the translators not benefiting from these tools are a tiny minority. These tools operate on fuzzy (surface) matching mostly and cannot benefit from already translated texts which are synonymous to (or paraphrased versions of) the text to be translated. The match score is mostly based on character-string similarity, calculated through Levenshtein distance. The TM tools have difficulties with detecting similarities even in sentences which represent a minor revision of sentences already available in the translation memory. This shortcoming of the current TM systems was the subject of the present study and was empirically proven in the experiments we conducted. To this end, we compiled a small translation memory (English-Spanish) and applied several lexical and syntactic transformation rules to the source sentences with both English and Spanish being the source language.The results of this study show that current TM systems have a long way to go and highlight the need for TM systems equipped with NLP capabilities which will offer the translator the advantage of he/she not having to translate a sentence again if an almost identical sentence has already been already translated.",
        "id": 10494199
      },
      {
        "title": "The Properties and Further Applications of Chinese Frequent Strings",
        "text": "This paper reveals some important properties of CFSs and applications in Chinese natural language processing (NLP). We have previously proposed a method for extracting Chinese frequent strings that contain unknown words from a Chinese corpus[Lin and Yu 2001]. We found that CFSs contain many 4-character strings, 3-word strings, and longer n-grams. Such information can only be derived from an extremely large corpus using a traditional language model(LM). In contrast to using a traditional LM, we can achieve high precision and efficiency by using CFSs to solve Chinese toneless phoneme-to-character conversion and to correct Chinese spelling errors with a small training corpus. An accuracy rate of 92.86% was achieved for Chinese toneless phoneme-to-character conversion, and an accuracy rate of 87.32% was achieved for Chinese spelling error correction. We also attempted to assign syntactic categories to a CFS. The accuracy rate for assigning syntactic categories to the CFSs was 88.53% for outside testing when the syntactic categories of the highest level were used.",
        "id": 17752874
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Have any recent publications explored the use of neural network methods, like transformer architectures, in creating novel readability metrics?",
    "positive_ctxs": [
      {
        "title": "Supervised and Unsupervised Neural Approaches to Text Readability Senja Pollak",
        "text": "We present a set of novel neural supervised and unsupervised approaches for determining the readability of documents. In the unsupervised setting, we leverage neural language models, whereas in the supervised setting, three different neural classification architectures are tested. We show that the proposed neural unsupervised approach is robust, transferable across languages, and allows adaptation to a specific readability task and data set. By systematic comparison of several neural architectures on a number of benchmark and new labeled readability data sets in two languages, this study also offers a comprehensive analysis of different neural approaches to readability classification. We expose their strengths and weaknesses, compare their performance to current state-of-the-art classification approaches to readability, which in most cases still rely on extensive feature engineering, and propose possibilities for improvements.",
        "id": 198967754
      }
    ],
    "negative_ctxs": [
      {
        "title": "A New Representation Model for the Automatic Recognition and Translation of Arabic Named Entities with NooJ",
        "text": "Recognition and translation of named entities (NEs) are two current research topics with regard to the proliferation of electronic documents exchanged through the Internet. The need to assimilate these documents through NLP tools has become necessary and interesting. Moreover, the formal or semiformal modeling of these NEs may intervene in both processes of recognition and translation. Indeed, the modeling makes more reliable the constitution of linguistic resources, limits the impact of linguistic specificities and facilitates transformations from one representation to another. In this context, we propose an approach of recognition and translation based on a representation model of Arabic NEs and a set of transducers resolving morphological and syntactical phenomena.",
        "id": 8821039
      },
      {
        "title": "UDeasy: a Tool for Querying Treebanks in CoNLL-U Format",
        "text": "Many tools are available to query a dependency treebank, but they require the users to know a query language. This paper presents UDeasy, an application whose main goal is to allow the users to easily query and extract patterns from a dependency treebank in CoNLL-U format. To do this, users are prompted in a series of dialogs to enter relevant information about syntactic nodes, their properties, relationship, and positions.",
        "id": 252624768
      },
      {
        "title": "Adversarial Text Generation via Sequence Contrast Discrimination",
        "text": "In this paper, we propose a sequence contrast loss driven text generation framework, which learns the difference between real texts and generated texts and uses that difference. Specifically, our discriminator contains a discriminative sequence generator instead of a binary classifier, and measures the 'relative realism' of generated texts against real texts by making use of them simultaneously. Moreover, our generator uses discriminative sequences to directly improve itself, which not only replaces the gradient propagation process from the discriminator to the generator, but also avoids the time-consuming sampling process of estimating rewards in some previous methods. We conduct extensive experiments with various metrics, substantiating that our framework brings improvements in terms of training stability and the quality of generated texts.",
        "id": 226284010
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "What research exists on leveraging syntactic roles and semantic interpretations for backdoor attacks on natural language processing systems?",
    "positive_ctxs": [
      {
        "title": "Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger",
        "text": "Backdoor attacks are a kind of insidious security threat against machine learning models. After being injected with a backdoor in training, the victim model will produce adversaryspecified outputs on the inputs embedded with predesigned triggers but behave properly on normal inputs during inference. As a sort of emergent attack, backdoor attacks in natural language processing (NLP) are investigated insufficiently. As far as we know, almost all existing textual backdoor attack methods insert additional contents into normal samples as triggers, which causes the trigger-embedded samples to be detected and the backdoor attacks to be blocked without much effort. In this paper, we propose to use the syntactic structure as the trigger in textual backdoor attacks. We conduct extensive experiments to demonstrate that the syntactic trigger-based attack method can achieve comparable attack performance (almost 100% success rate) to the insertionbased methods but possesses much higher invisibility and stronger resistance to defenses. These results also reveal the significant insidiousness and harmfulness of textual backdoor attacks. All the code and data of this paper can be obtained at https://github.com/ thunlp/HiddenKiller.",
        "id": 235196099
      },
      {
        "title": "Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution",
        "text": "Recent studies show that neural natural language processing (NLP) models are vulnerable to backdoor attacks. Injected with backdoors, models perform normally on benign examples but produce attacker-specified predictions when the backdoor is activated, presenting serious security threats to real-world applications. Since existing textual backdoor attacks pay little attention to the invisibility of backdoors, they can be easily detected and blocked. In this work, we present invisible backdoors that are activated by a learnable combination of word substitution. We show that NLP models can be injected with backdoors that lead to a nearly 100% attack success rate, whereas being highly invisible to existing defense strategies and even human inspections. The results raise a serious alarm to the security of NLP models, which requires further research to be resolved. All the data and code of this paper are released at https: //github.com/thunlp/BkdAtk-LWS.",
        "id": 235417102
      }
    ],
    "negative_ctxs": [
      {
        "title": "Lifetime Achievement Award Natural Language Processing and Computational Linguistics",
        "text": "",
        "id": 239212345
      },
      {
        "title": "Gender Representation in Open Source Speech Resources",
        "text": "With the rise of artificial intelligence (AI) and the growing use of deep-learning architectures, the question of ethics, transparency and fairness of AI systems has become a central concern within the research community. We address transparency and fairness in spoken language systems by proposing a study about gender representation in speech resources available through the Open Speech and Language Resource platform. We show that finding gender information in open source corpora is not straightforward and that gender balance depends on other corpus characteristics (elicited/non elicited speech, low/high resource language, speech task targeted). The paper ends with recommendations about metadata and gender information for researchers in order to assure better transparency of the speech systems built using such corpora.",
        "id": 212747709
      },
      {
        "title": "",
        "text": "",
        "id": 220445929
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest a dataset containing diverse, intricate natural language queries that necessitate multi-step reasoning, comparing attributes, and performing set operations for answering questions from a knowledge base, without depending on entity linking?",
    "positive_ctxs": [
      {
        "title": "KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base",
        "text": "Complex question answering over knowledge base (Complex KBQA) is challenging because it requires various compositional reasoning capabilities, such as multi-hop inference, attribute comparison, set operation. Existing benchmarks have some shortcomings that limit the development of Complex KBQA: 1) they only provide QA pairs without explicit reasoning processes; 2) questions are poor in diversity or scale. To this end, we introduce KQA Pro, a dataset for Complex KBQA including 120K diverse natural language questions. We introduce a compositional and interpretable programming language KoPL to represent the reasoning process of complex questions. For each question, we provide the corresponding KoPL program and SPARQL query, so that KQA Pro serves for both KBQA and semantic parsing tasks. Experimental results show that SOTA KBQA methods cannot achieve promising results on KQA Pro as on current datasets, which suggests that KQA Pro is challenging and Complex KBQA requires further research efforts. We also treat KQA Pro as a diagnostic dataset for testing multiple reasoning skills, conduct a thorough evaluation of existing models and discuss further directions for Complex KBQA. Our codes and datasets can be obtained from https://github.com/shijx12/ KQAPro_Baselines.",
        "id": 247362971
      }
    ],
    "negative_ctxs": [
      {
        "title": "Published as a conference paper at ICLR 2023 VOXURF: VOXEL-BASED EFFICIENT AND ACCURATE NEURAL SURFACE RECONSTRUCTION",
        "text": "Neural surface reconstruction aims to reconstruct accurate 3D surfaces based on multi-view images. Previous methods based on neural volume rendering mostly train a fully implicit model with MLPs, which typically require hours of training for a single scene. Recent efforts explore the explicit volumetric representation to accelerate the optimization via memorizing significant information with learnable voxel grids. However, existing voxel-based methods often struggle in reconstructing fine-grained geometry, even when combined with an SDF-based volume rendering scheme. We reveal that this is because 1) the voxel grids tend to break the color-geometry dependency that facilitates fine-geometry learning, and 2) the under-constrained voxel grids lack spatial coherence and are vulnerable to local minima. In this work, we present Voxurf, a voxel-based surface reconstruction approach that is both efficient and accurate. Voxurf addresses the aforementioned issues via several key designs, including 1) a two-stage training procedure that attains a coherent coarse shape and recovers fine details successively, 2) a dual color network that maintains color-geometry dependency, and 3) a hierarchical geometry feature to encourage information propagation across voxels. Extensive experiments show that Voxurf achieves high efficiency and high quality at the same time. On the DTU benchmark, Voxurf achieves higher reconstruction quality with a 20x training speedup compared to previous fully implicit methods.Corresponding authors. Our code is available at https://github.com/wutong16/Voxurf.",
        "id": 251881568
      },
      {
        "title": "Improving Translation through Contextual Information",
        "text": "This paper proposes a two-layered model of dialogue structure for task-oriented dialogues that processes contextual information and disambiguates speech acts. The final goal is to improve translation quality in a speech-to-speech translation system.",
        "id": 17409270
      },
      {
        "title": "",
        "text": "",
        "id": 237099295
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "What are the latest advancements in predicting suicidal tendencies using innovative feature extraction methods?",
    "positive_ctxs": [
      {
        "title": "PHASE: Learning Emotional Phase-aware Representations for Suicide Ideation Detection on Social Media",
        "text": "Recent psychological studies indicate that individuals exhibiting suicidal ideation increasingly turn to social media rather than mental health practitioners. Contextualizing the buildup of such ideation is critical for the identification of users at risk. In this work, we focus on identifying suicidal intent in tweets by augmenting linguistic models with emotional phases modeled from users' historical context. We propose PHASE, a time-and phase-aware framework that adaptively learns features from a user's historical emotional spectrum on Twitter for preliminary screening of suicidal risk. Building on clinical studies, PHASE learns phase-like progressions in users' historical Plutchik-wheel-based emotions to contextualize suicidal intent. While outperforming stateof-the-art methods, we show the utility of temporal and phase-based emotional contextual cues for suicide ideation detection. We further discuss practical and ethical considerations. 1 Amy Bruckman. 2002. Studying the amateur artist: A perspective on disguising data collected in human subjects research on the internet. Ethics and Information Technology, 4(3):217-231. Craig J. Bryan. 2020. Chapter 4 -the temporal dynamics of the wish to live and the wish to die among suicidal individuals. In Andrew C. Page and Werner G.K. Stritzke, editors, Alternatives to Suicide, pages 71 -88. Academic Press. Craig J Bryan and M David Rudd. 2006. Advances in the assessment of suicide risk. Journal of clinical psychology, 62(2):185-200. Craig J Bryan and M David Rudd. 2016. The importance of temporal dynamics in the transition from suicidal thought to behavior. Nock. 2017. Risk factors for suicidal thoughts and behaviors: a metaanalysis of 50 years of research. Psychological bulletin, 143(2):187. King wa Fu, Ka Y. Liu, and Paul S. F. Yip. 2007. Predictive validity of the chinese version of the adult suicidal ideation questionnaire: Psychometric properties and its short version. Psychological Assessment, 19(4):422-429.",
        "id": 233189632
      }
    ],
    "negative_ctxs": [
      {
        "title": "A Language Modeling Approach to Predicting Reading Difficulty",
        "text": "We demonstrate a new research approach to the problem of predicting the reading difficulty of a text passage, by recasting readability in terms of statistical language modeling. We derive a measure based on an extension of multinomial naïve Bayes classification that combines multiple language models to estimate the most likely grade level for a given passage. The resulting classifier is not specific to any particular subject and can be trained with relatively little labeled data. We perform predictions for individual Web pages in English and compare our performance to widely-used semantic variables from traditional readability measures. We show that with minimal changes, the classifier may be retrained for use with French Web documents. For both English and French, the classifier maintains consistently good correlation with labeled grade level (0.63 to 0.79) across all test sets. Some traditional semantic variables such as type-token ratio gave the best performance on commercial calibrated test passages, while our language modeling approach gave better accuracy for Web documents and very short passages (less than 10 words).",
        "id": 5206782
      },
      {
        "title": "Semi-Automatic Construction of Text-to-SQL Data for Domain Transfer",
        "text": "Strong and affordable in-domain data is a desirable asset when transferring trained semantic parsers to novel domains. As previous methods for semi-automatically constructing such data cannot handle the complexity of realistic SQL queries, we propose to construct SQL queries via context-dependent sampling, and introduce the concept of topic. Along with our SQL query construction method, we propose a novel pipeline of semi-automatic Textto-SQL dataset construction that covers the broad space of SQL queries. We show that the created dataset is comparable with expert annotation along multiple dimensions, and is capable of improving domain transfer performance for SOTA semantic parsers.",
        "id": 236486090
      },
      {
        "title": "On the Sentence Embeddings from Pre-trained Language Models",
        "text": "Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. The code is available at https://github.com/ bohanli/BERT-flow. * The work was done when BL was an intern at ByteDance.",
        "id": 226262227
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Is there any paper applies off-shelf GPT-2 model in D4RL tasks, using PEFT techniques?",
    "positive_ctxs": [
      {
        "title": "UNLEASHING THE POWER OF PRE-TRAINED LANGUAGE MODELS FOR OFFLINE REINFORCEMENT LEARNING",
        "text": "Offline reinforcement learning (RL) aims to find a near-optimal policy using pre-collected datasets.In real-world scenarios, data collection could be costly and risky; therefore, offline RL becomes particularly challenging when the indomain data is limited.Given recent advances in Large Language Models (LLMs) and their few-shot learning prowess, this paper introduces Language Models for Motion Control (LaMo), a general framework based on Decision Transformers to effectively use pre-trained Language Models (LMs) for offline RL.Our framework highlights four crucial components: (1) Initializing Decision Transformers with sequentially pre-trained LMs, (2) employing the LoRA fine-tuning method, in contrast to full-weight fine-tuning, to combine the pre-trained knowledge from LMs and in-domain knowledge effectively, (3) using the non-linear MLP transformation instead of linear projections, to generate embeddings, and (4) integrating an auxiliary language prediction loss during fine-tuning to stabilize the LMs and retain their original abilities on languages.Empirical results indicate LaMo achieves state-of-the-art performance in sparse-reward tasks and closes the gap between value-based offline RL methods and decision transformers in dense-reward tasks.In particular, our method demonstrates superior performance in scenarios with limited data samples.Our project website is lamo2023.github.io.",
        "id": 264802494
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 219307936
      },
      {
        "title": "",
        "text": "In this paper, MARKET metaphors used by different communities (Chinese, Malay and English) are laid out based on the frequency counts of these metaphors and their occurrences in different syntactic positions. The results show that certain types of metaphors have preferences for different syntactic positions for 'market.' For instance, MARKET IS A PERSON in all three languages prefers to place 'market' in the subject position. In addition to this finding, the choice of metaphor types by different speech communities may also reflect their perspectives regarding their country's economy. This is evidenced by the fewer instances of MARKET IS COMPETITION in the English data. The instances that describe how the market falls (plunges and crashes) may reflect the speakers' concerns with the maintenance of their power in the market rather than the competitiveness of their market. Therefore, through using quantitative data, this paper is able to infer the economic status of these speech communities. This can be done not only through analyzing the semantic meanings of the metaphors but also their interface with syntax.Interpersonal metaphor, on the other hand, is pragmatics-based in which phrases such as I think and I don't think can also carry metaphorical meanings in expressing congruency of ideas (such as I think will be congruent with probably). Compared to the model by Lakoff and Johnson, Halliday's grammatical metaphor seems to appear at the other end of the continuum between semantics and syntax. The relationship between conceptual metaphors and syntax was never mentioned especially when more than a language is involved.The aims of this work, therefore, are a) to try to extend the analysis of conceptual metaphors byAhrens et al. (2003)and  so that the grammatical aspect can be included in the analysis; and b) to find out the relationship between the MARKET metaphors and the syntactic positions in which MARKET occurs. This is because how much 'market' is a subject or object may reflect how a speech communities view the position of 'market' in the world. The conceptual metaphors from three languages (Chinese, Malay and English) are examined and the roles of 'market' are analyzed cross-linguistically. Our research questions are as follow: Are the conceptual metaphors shared by these languages are similar? Are the syntactical positions of the target word 'market' similar across these languages and what do the differences in (a) and (b) say about the three speech communities? By answering these three questions, it is hoped that the steps in identifying cultural differences can be operationalized.(2001)examined metaphors in financial reporting in English and Spanish based on the market crash in 1997. Their analysis showed that THE ECONOMY IS AN ORGANISM has the highest frequency in financial reports, followed by MARKET MOVEMENTS ARE PHYSICAL MOVEMENTS and MARKET MOVEMENTS ARE NATURAL DISASTER. Within these metaphors, there are sub-metaphors. For instance, examples that refer to both physical conflict and state of health fall under the source domain of ORGANISM. Chung, Ahrens and Sung (2003) also carried out an analysis of STOCK MARKET metaphors in Chinese and English and they compared specifically STOCK MARKET IS OCEAN WATER to Charteris-Black and Ennis's MARKET MOVEMENTS ARE NAUTICAL OR ARE WAYS OF MOVING IN THE WATER (under MARKET MOVEMENTS ARE PHYSICAL MOVEMENTS). They criticized that the source domains are not that clear. For instance, the source domain of ORGANISM is too general because it may refers to plants, animals and any aspects of organisms. The question regarding identifying source domains is discussed in Chung, Ahrens and Huang (In Press). Therefore, for Charteris-Black and Ennis's metaphor MARKET MOVEMENTS ARE NAUTICAL OR ARE WAYS OF MOVING IN THE WATER,  suggested that the metaphorical instances can be sub-divided into OCEAN WATER and BOAT. This again showed the over-general source domains in the work of Charteris-Black and Ennis. In addition, this paper will only focus on the target word 'MARKET' (although not as narrow as STOCK MARKET, JOB MARKET and other types of markets) and exclude the interference of target such as TRADING.MARKET metaphorCharteris-Black and Ennis",
        "id": 17936833
      },
      {
        "title": "Improving Scientific Relation Classification with Task Specific Supersense",
        "text": "Classifying the relationship between entities is an important natural language processing (NLP) task. Scientific Relation Classification aims at automatically categorizing scientific semantic relationships among entities in scientific documents. Conventionally, only task unspecific supersense, such as supersense (or hyernym) from WordNet (e.g., ANIMAL is the supersense of \"dog\"), is used as a feature for relation classification. In this work, we hypothesize that task specific supersense could also be utilized as an informative feature for relation classification. Specifically, we define a new entity type based on the property of a given task, and facilitate scientific relation classification with the task specific supersense. Our experiments on three different datasets prove the effectiveness of the task specific supersense on relation classification in scientific articles.1In this work, entity refers not merely to concepts denoted by noun or noun phrase, it could be actions denoted by verb or verb phrase, and evaluation denoted by adjective or adverb etc.",
        "id": 198935469
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Has any study explored the zero-shot extraction of persona characteristics within conversational dialogues?",
    "positive_ctxs": [
      {
        "title": "PAED: Zero-Shot Persona Attribute Extraction in Dialogues",
        "text": "Persona attribute extraction is critical for personalized human-computer interaction. Dialogue is an important medium that communicates and delivers persona information. Although there is a public dataset for triplet-based persona attribute extraction from conversations, its automatically generated labels present many issues, including unspecific relations and inconsistent annotations. We fix such issues by leveraging more reliable text-label matching criteria to generate high-quality data for persona attribute extraction. We also propose a contrastive learning-and generation-based model with a novel hard negative sampling strategy for generalized zero-shot persona attribute extraction. We benchmark our model with stateof-the-art baselines on our dataset and a public dataset, showing outstanding accuracy gains. Our sampling strategy also exceeds others by a large margin in persona attribute extraction.",
        "id": 258971765
      }
    ],
    "negative_ctxs": [
      {
        "title": "Association Norms of German Noun Compounds",
        "text": "This paper introduces association norms of German noun compounds as a lexical-semantic resource for cognitive and computational linguistics research on compositionality. Based on an existing database of German noun compounds, we collected human associations to the compounds and their constituents within a web experiment. The current study describes the collection process and a part-of-speech analysis of the association resource. In addition, we demonstrate that the associations provide insight into the semantic properties of the compounds, and perform a case study that predicts the degree of compositionality of the experiment compound nouns, as relying on the norms. Applying a comparatively simple measure of association overlap, we reach a Spearman rank correlation coefficient of rs = 0.5228, p < .000001, when comparing our predictions with human judgements.",
        "id": 17986977
      },
      {
        "title": "Extrinsic Corpus Evaluation with a Collocation Dictionary Task",
        "text": "The NLP researcher or application-builder often wonders \"what corpus should I use, or should I build one of my own? If I build one of my own, how will I know if I have done a good job?\" Currently there is very little help available for them. They are in need of a framework for evaluating corpora. We develop such a framework, in relation to corpora which aim for good coverage of 'general language'. The task we set is automatic creation of a publication-quality collocations dictionary. For a sample of 100 headwords of Czech and 100 of English, we identify a gold standard dataset of (ideally) all the collocations that should appear for these headwords in such a dictionary. The datasets are being made available alongside this paper. We then use them to determine precision and recall for a range of corpora, with a range of parameters.",
        "id": 5716928
      },
      {
        "title": "Multimodal Dialogue State Tracking",
        "text": "Designed for tracking user goals in dialogues, a dialogue state tracker is an essential component in a dialogue system. However, the research of dialogue state tracking has largely been limited to unimodality, in which slots and slot values are limited by knowledge domains (e.g. restaurant domain with slots of restaurant name and price range) and are defined by specific database schema. In this paper, we propose to extend the definition of dialogue state tracking to multimodality. Specifically, we introduce a novel dialogue state tracking task to track the information of visual objects that are mentioned in video-grounded dialogues. Each new dialogue utterance may introduce a new video segment, new visual objects, or new object attributes and a state tracker is required to update these information slots accordingly. We created a new synthetic benchmark and designed a novel baseline, Video-Dialogue Transformer Network (VDTN), for this task. VDTN combines both object-level features and segment-level features and learns contextual dependencies between videos and dialogues to generate multimodal dialogue states. We optimized VDTN for a state generation task as well as a self-supervised video understanding task which recovers video segment or object representations. Finally, we trained VDTN to use the decoded states in a response prediction task. Together with comprehensive ablation and qualitative analysis, we discovered interesting insights towards building more capable multimodal dialogue systems.",
        "id": 249712518
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "What are the recent developments in evaluating the flow or 'streaming degree' of the translation processes in simultaneous machine translation (SiMT), and which metric has proven useful for this purpose?",
    "positive_ctxs": [
      {
        "title": "Learning to Translate in Real-time with Neural Machine Translation",
        "text": "Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively. 1",
        "id": 2782776
      }
    ],
    "negative_ctxs": [
      {
        "title": "Sentiment Analysis on Naija-Tweets",
        "text": "Examining sentiments in social media poses a challenge to natural language processing because of the intricacy and variability in the dialect articulation, noisy terms in form of slang, abbreviation, acronym, emoticon, and spelling error coupled with the availability of real-time content. Moreover, most of the knowledgebased approaches for resolving slang, abbreviation, and acronym do not consider the issue of ambiguity that evolves in the usage of these noisy terms. This research work proposes an improved framework for social media feed pre-processing that leverages on the combination of integrated local knowledge bases and adapted Lesk algorithm to facilitate pre-processing of social media feeds. The results from the experimental evaluation revealed an improvement over existing methods when applied to supervised learning algorithms in the task of extracting sentiments from Nigeria-origin tweets with an accuracy of 99.17%.",
        "id": 196205333
      },
      {
        "title": "A Linear Programming Formulation for Global Inference in Natural Language Tasks",
        "text": "Given a collection of discrete random variables representing outcomes of learned local predictors in natural language, e.g., named entities and relations, we seek an optimal global assignment to the variables in the presence of general (non-sequential) constraints. Examples of these constraints include the type of arguments a relation can take, and the mutual activity of different relations, etc. We develop a linear programming formulation for this problem and evaluate it in the context of simultaneously learning named entities and relations. Our approach allows us to efficiently incorporate domain and task specific constraints at decision time, resulting in significant improvements in the accuracy and the \"human-like\" quality of the inferences.",
        "id": 10048734
      },
      {
        "title": "",
        "text": "",
        "id": 218974364
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "What paper is the first to prove finetuned LLM can be a reliable judge?",
    "positive_ctxs": [
      {
        "title": "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization",
        "text": "Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models. To determine the optimal hyperparameters, an automatic, robust, and reliable evaluation benchmark is essential. However, establishing such a benchmark is not a trivial task due to the challenges associated with evaluation accuracy and privacy protection. In response to these challenges, we introduce a judge large language model, named PandaLM, which is trained to distinguish the superior model given several LLMs. PandaLM's focus extends beyond just the objective correctness of responses, which is the main focus of traditional evaluation datasets. It addresses vital subjective factors such as relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality. To ensure the reliability of PandaLM, we collect a diverse human-annotated test dataset, where all contexts are generated by humans and labels are aligned with human preferences. Our results indicate that PandaLM-7B achieves 93.75% of GPT-3.5's evaluation ability and 88.28% of GPT-4's in terms of F1-score on our test dataset. PandaLM enables the evaluation of LLM to be fairer but with less cost, evidenced by significant improvements achieved by models tuned through PandaLM compared to their counterparts trained with default Alpaca's hyperparameters. In addition, PandaLM does not depend on API-based evaluations, thus avoiding potential data leakage. All resources of PandaLM are released at https://github.com/WeOpenML/PandaLM. * Equal contribution. Yidong did this work during his internship at Westlake University. † Corresponding to",
        "id": 259108266
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 219309348
      },
      {
        "title": "Semi-Automated Labeling of Requirement Datasets for Relation Extraction",
        "text": "Creating datasets manually by human annotators is a laborious task that can lead to biased and inhomogeneous labels. We propose a flexible, semi-automatic framework for labeling data for relation extraction. Furthermore, we provide a dataset of preprocessed sentences from the requirements engineering domain, including a set of automatically created as well as hand-crafted labels. In our case study, we compare the human and automatic labels and show that there is a substantial overlap between both annotations.",
        "id": 237421077
      },
      {
        "title": "Answering it with charts -Dialogue in natural language and charts",
        "text": "A methodology is proposed for taking queries and requests expressed in natural language as input and answering them in charts through organizing that interaction into felicitous dialogue. Charts and graphics, as well as languages, are important modes of communication. This is especially true of those which are used frequently when people analyze huge amount of data interactively, in order to find out its characteristics or to resolve questions about it. This paper raises the problem that in such situations the correctness of the charts depends on the context, and proposes a framework to resolve it. The core of the framework is a logical form that includes the specifications of the user's perspective and the proper treatment of the logical form for handling utterance fragments. The framework has been implemented and confirmed to be appropriate.",
        "id": 10143404
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "What are some approaches to generating sports news reports from event data and what models have been used for this task in Finnish language NLP?",
    "positive_ctxs": [
      {
        "title": "Template-free Data-to-Text Generation of Finnish Sports News",
        "text": "News articles such as sports game reports are often thought to closely follow the underlying game statistics, but in practice they contain a notable amount of background knowledge, interpretation, insight into the game, and quotes that are not present in the official statistics. This poses a challenge for automated data-totext news generation with real-world news corpora as training data. We report on the development of a corpus of Finnish ice hockey news, edited to be suitable for training of end-to-end news generation methods, as well as demonstrate generation of text, which was judged by journalists to be relatively close to a viable product. The new dataset and system source code are available for research purposes. 1",
        "id": 203734629
      }
    ],
    "negative_ctxs": [
      {
        "title": "Combining fast align with Hierarchical Sub-sentential Alignment for Better Word Alignments",
        "text": "fast align is a simple and fast word alignment tool which is widely used in state-of-the-art machine translation systems. It yields comparable results in the end-to-end translation experiments of various language pairs. However, fast align does not perform as well as GIZA++ when applied to language pairs with distinct word orders, like English and Japanese. In this paper, given the lexical translation table output by fast align, we propose to realign words using the hierarchical sub-sentential alignment approach. Experimental results show that simple additional processing improves the performance of word alignment, which is measured by counting alignment matches in comparison with fast align. We also report the result of final machine translation in both English-Japanese and Japanese-English. We show our best system provided significant improvements over the baseline as measured by BLEU and RIBES. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details:",
        "id": 10441638
      },
      {
        "title": "Classifying Arabic Crisis Tweets using Data Selection and Pre-trained Language Models",
        "text": "User-generated Social Media (SM) content has been explored as a valuable and accessible source of data about crises to enhance situational awareness and support humanitarian response efforts. However, the timely extraction of crisis-related SM messages is challenging as it involves processing large quantities of noisy data in real-time. Supervised machine learning methods have been successfully applied to this task but such approaches require human-labelled data, which are unlikely to be available from novel and emerging crises. Supervised machine learning algorithms trained on labelled data from past events did not usually perform well when classifying a new disaster due to data variations across events. Using the BERT embeddings, we propose and investigate an instance distance-based data selection approach for adaptation to improve classifiers' performance under a domain shift. The K-nearest neighbours algorithm selects a subset of multi-event training data that is most similar to the target event. Results show that fine-tuning a BERT model on a selected subset of data to classify crisis tweets outperforms a model that has been fine-tuned on all available source data. We demonstrated that our approach generally works better than the self-training adaptation method. Combing the self-training with our proposed classifier does not enhance the performance.",
        "id": 252624552
      },
      {
        "title": "DIP: Dead code Insertion based Black-box Attack for Programming Language Model",
        "text": "Automatic processing of source code, such as code clone detection and software vulnerability detection, is very helpful to software engineers. Large pre-trained Programming Language (PL) models (such as CodeBERT, Graph-CodeBERT, CodeT5, etc.), show very powerful performance on these tasks. However, these PL models are vulnerable to adversarial examples that are generated with slight perturbation. Unlike natural language, an adversarial example of code must be semantic-preserving and compilable. Due to the requirements, it is hard to directly apply the existing attack methods for natural language models. In this paper, we propose DIP (Dead code Insertion based Blackbox Attack for Programming Language Model), a high-performance and efficient black-box attack method to generate adversarial examples using dead code insertion. We evaluate our proposed method on 9 victim downstream-task large code models. Our method outperforms the state-of-the-art black-box attack in both attack efficiency and attack quality, while generated adversarial examples are compiled preserving semantic functionality.",
        "id": 259370836
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Is there a paper which applies Bayesian optimization to modular continual learning?",
    "positive_ctxs": [
      {
        "title": "A Probabilistic Framework for Modular Continual Learning",
        "text": "Modular approaches, which use a different composition of modules for each problem and avoid forgetting by design, have been shown to be a promising direction in continual learning (CL). However, searching through the large, discrete space of possible module compositions is a challenge because evaluating a composition's performance requires a round of neural network training. To address this challenge, we develop a modular CL framework, called PICLE, that accelerates search by using a probabilistic model to cheaply compute the fitness of each composition. The model combines prior knowledge about good module compositions with datasetspecific information. Its use is complemented by splitting up the search space into subsets, such as perceptual and latent subsets. We show that PICLE is the first modular CL algorithm to achieve different types of transfer while scaling to large search spaces. We evaluate it on two benchmark suites designed to capture different desiderata of CL techniques. On these benchmarks, PICLE offers significantly better performance than state-of-the-art CL baselines.Preprint. Under review.",
        "id": 259138821
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 202735479
      },
      {
        "title": "Evaluating the Utility of Hand-crafted Features in Sequence Labelling *",
        "text": "Conventional wisdom is that hand-crafted features are redundant for deep learning models, as they already learn adequate representations of text automatically from corpora. In this work, we test this claim by proposing a new method for exploiting handcrafted features as part of a novel hybrid learning approach, incorporating a feature auto-encoder loss component. We evaluate on the task of named entity recognition (NER), where we show that including manual features for partof-speech, word shapes and gazetteers can improve the performance of a neural CRF model. We obtain a F 1 of 91.89 for the CoNLL-2003 English shared task, which significantly outperforms a collection of highly competitive baseline models. We also present an ablation study showing the importance of autoencoding, over using features as either inputs or outputs alone, and moreover, show including the autoencoder components reduces training requirements to 60%, while retaining the same predictive accuracy.",
        "id": 52113877
      },
      {
        "title": "Exploration and Discovery of the COVID-19 Literature through Semantic Visualization",
        "text": "We propose semantic visualization as a linguistic visual analytic method. It can enable exploration and discovery over large datasets of complex networks by exploiting the semantics of the relations in them. This involves extracting information, applying parameter reduction operations, building hierarchical data representation and designing visualization. We also present the accompanying COVID-SEMVIZ, a searchable and interactive visualization system for knowledge exploration of COVID-19 data to demonstrate the application of our proposed method. 1 In the user studies, users found that semantic visualization-powered COVID-SEMVIZ is helpful in terms of finding relevant information and discovering unknown associations.",
        "id": 220347577
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper showed that social relationships were helpful for identifying inappropriate messages?",
    "positive_ctxs": [
      {
        "title": "Your spouse needs professional help: Determining the Contextual Appropriateness of Messages through Modeling Social Relationships",
        "text": "Understanding interpersonal communication requires, in part, understanding the social context and norms in which a message is said. However, current methods for identifying offensive content in such communication largely operate independent of context, with only a few approaches considering community norms or prior conversation as context. Here, we introduce a new approach to identifying inappropriate communication by explicitly modeling the social relationship between the individuals. We introduce a new dataset of contextually-situated judgments of appropriateness and show that large language models can readily incorporate relationship information to accurately identify appropriateness in a given context. Using data from online conversations and movie dialogues, we provide insight into how the relationships themselves function as implicit norms and quantify the degree to which context-sensitivity is needed in different conversation settings. Further, we also demonstrate that contextualappropriateness judgments are predictive of other social factors expressed in language such as condescension and politeness.",
        "id": 259360414
      }
    ],
    "negative_ctxs": [
      {
        "title": "Thin Parsing: A Balance between Wide Scale Parsing and Chunking",
        "text": "This work presents a type of parser that takes the process of chunking to the stage of producing full parse trees. This type of parser, denoted Thin Parsers (TP) in this work has the characteristics of: following a given grammar, creating full parse trees, producing only a limited number of full parse trees, parsing in linear time of sentence length. Performance standards on the Penn Tree Bank show results slightly under that of stochastic parsers but faster performance. Various types of Thin Parsers are presented.",
        "id": 14460763
      },
      {
        "title": "The Development of Dutch and Afrikaans Language Resources for Compound Boundary Analysis",
        "text": "In most languages, new words can be created through the process of compounding, which combines two or more words into a new lexical unit. Whereas in languages such as English the components that make up a compound are separated by a space, in languages such as Finnish, German, Afrikaans and Dutch these components are concatenated into one word. Compounding is very productive and leads to practical problems in developing machine translators and spelling checkers, as newly formed compounds cannot be found in existing lexicons. The Automatic Compound Processing (AuCoPro) project deals with the analysis of compounds in two closely-related languages, Afrikaans and Dutch. In this paper, we present the development and evaluation of two datasets, one for each language, that contain compound words with annotated compound boundaries. Such datasets can be used to train classifiers to identify the compound components in novel compounds. We describe the process of annotation and provide an overview of the annotation guidelines as well as global properties of the datasets. The inter-rater agreements between the annotators are considered highly reliable. Furthermore, we show the usability of these datasets by building an initial automatic compound boundary detection system, which assigns compound boundaries with approximately 90% accuracy.",
        "id": 16125712
      },
      {
        "title": "Employing Low-Pass Filtered Temporal Speech Features for the Training of Ideal Ratio Mask in Speech Enhancement 摘要 在諸多基於深度學習之語音強化法中，遮罩式(masking-based)強化法求取一個 遮罩與雜訊語音之時頻圖相乘、藉此使所得乘積之新時頻圖所含雜訊成分降低、 以重建相對乾淨的語音訊號。在用以訓練遮罩之深度模型其輸入特徵的選取上， 許多長期以來用以語音辨識的特徵、如梅爾倒倒頻譜、振幅調變時頻圖、感知 線性估測係數等都是適合的選擇、可使訓練所得的遮罩達到有效的語音強化效 果。另外，傳統上若將語音特徵之時序列作低通濾波處理，可以抑制雜訊所帶 來的失真，因此，在本研究中，我們嘗試將各種語音特徵時序列，藉由離散小 波轉換的方式加以低通濾波，再用它們來訓練語音遮罩的深度模型，探究其是 否能使所學習之遮罩能對於原始雜訊語音之時頻圖有更佳的語音強化效果。在 我們的初步實驗裡，在人聲雜訊環境中，我們發現上述之低通濾波所得之特徵 序列、相較於原始特徵序列而言所學習而得的深度模型，能更有效地提升測試 語音之品質與可讀性。",
        "text": "The masking-based speech enhancement method pursues a multiplicative mask that applies to the spectrogram of input noise-corrupted utterance, and a deep neural network (DNN) is often used to learn the mask. In particular, the features commonly used for automatic speech recognition can serve as the input of the DNN to learn the well-behaved mask that significantly reduce the noise distortion of processed utterances. This study proposes to preprocess the input speech features for the ideal ratio mask (IRM)-based DNN by lowpass filtering in order to alleviate the noise components. In particular, we employ the discrete wavelet transform (DWT) to decompose the temporal speech feature sequence and scale down the detail coefficients, which correspond to the high-pass portion of the sequence. Preliminary experiments conducted on a subset of TIMIT corpus reveal that the proposed method can make the resulting IRM achieve higher speech quality and intelligibility for the babble noise-corrupted signals compared with the original IRM, indicating that the lowpass filtered temporal feature sequence can learn a superior IRM network for speech enhancement.關鍵詞：語音強化、特徵時序列、低通濾波、理想比例遮罩法、小波轉換",
        "id": 248182545
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Which paper has conducted a thorough analysis of how language models of different architectures generate text that either aligns with or deviates from the properties of natural human language?",
    "positive_ctxs": [
      {
        "title": "Language Model Evaluation Beyond Perplexity",
        "text": "We propose an alternate approach to quantifying how well language models learn natural language: we ask how well they match the statistical tendencies of natural language. To answer this question, we analyze whether text generated from language models exhibits the statistical tendencies present in the humangenerated text on which they were trained. We provide a framework-paired with significance tests-for evaluating the fit of language models to these trends. We find that neural language models appear to learn only a subset of the tendencies considered, but align much more closely with empirical trends than proposed theoretical distributions (when present). Further, the fit to different distributions is highly-dependent on both model architecture and generation strategy. As concrete examples, text generated under the nucleus sampling scheme adheres more closely to the typetoken relationship of natural language than text produced using standard ancestral sampling; text from LSTMs reflects the natural language distributions over length, stopwords, and symbols surprisingly well.",
        "id": 235265909
      }
    ],
    "negative_ctxs": [
      {
        "title": "ON BONUS-BASED EXPLORATION METHODS IN THE ARCADE LEARNING ENVIRONMENT",
        "text": "Research on exploration in reinforcement learning, as applied to Atari 2600 gameplaying, has emphasized tackling difficult exploration problems such as MON-TEZUMA'S REVENGE(Bellemare et al., 2016). Recently, bonus-based exploration methods, which explore by augmenting the environment reward, have reached above-human average performance on such domains. In this paper we reassess popular bonus-based exploration methods within a common evaluation framework. We combine Rainbow (Hessel et al., 2018)  with different exploration bonuses and evaluate its performance on MONTEZUMA'S REVENGE, Bellemare et al.'s set of hard of exploration games with sparse rewards, and the whole Atari 2600 suite. We find that while exploration bonuses lead to higher score on MONTEZUMA'S REVENGE they do not provide meaningful gains over the simpler -greedy scheme. In fact, we find that methods that perform best on that game often underperform -greedy on easy exploration Atari 2600 games. We find that our conclusions remain valid even when hyperparameters are tuned for these easy-exploration games. Finally, we find that none of the methods surveyed benefit from additional training samples (1 billion frames, versus Rainbow's 200 million) on Bellemare et al.'s hard exploration games. Our results suggest that recent gains in MONTEZUMA'S REVENGE may be better attributed to architecture change, rather than better exploration schemes; and that the real pace of progress in exploration research for Atari 2600 games may have been obfuscated by good results on a single domain. , et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.",
        "id": 214283684
      },
      {
        "title": "C3PO: A Lightweight Copying Mechanism for Translating Pseudocode to Code",
        "text": "Writing computer programs is a skill that remains inaccessible to most due to the barrier of programming language (PL) syntax. While large language models (LLMs) have been proposed to translate natural language pseudocode to PL code, they are costly in terms of data and compute. We propose a lightweight alternative to LLMs that exploits the property of code wherein most tokens can be simply copied from the pseudocode. We divide the problem into three phases: Copy, Generate, and Combine. In the Copy Phase, a binary classifier is employed to determine and mask the pseudocode tokens that can be directly copied into the code. In the Generate Phase, a Sequence-to-Sequence model is used to generate the masked PL code equivalent. In the Combine Phase, the generated sequence is combined with the tokens that the Copy Phase had masked. We show that our C3PO models achieve similar performance to non-C3PO models while reducing the computational cost of training as well as the vocabulary sizes.",
        "id": 253761967
      },
      {
        "title": "User Embedding for Scholarly Microblog Recommendation",
        "text": "Nowadays, many scholarly messages are posted on Chinese microblogs and more and more researchers tend to find scholarly information on microblogs. In order to exploit microblogging to benefit scientific research, we propose a scholarly microblog recommendation system in this study. It automatically collects and mines scholarly information from Chinese microblogs, and makes personalized recommendations to researchers. We propose two different neural network models which learn the vector representations for both users and microblog texts. Then the recommendation is accomplished based on the similarity between a user's vector and a microblog text's vector. We also build a dataset for this task. The two embedding models are evaluated on the dataset and show good results compared to several baselines.",
        "id": 18202618
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Can you refer me to research that adapts the concept of Word Mover's Distance to sentences, addressing the limitations of bag-of-words approaches and considering the order of words for text similarity?",
    "positive_ctxs": [
      {
        "title": "Sentence Mover's Similarity: Automatic Evaluation for Multi-Sentence Texts",
        "text": "For evaluating machine-generated texts, automatic methods hold the promise of avoiding collection of human judgments, which can be expensive and time-consuming. The most common automatic metrics, like BLEU and ROUGE, depend on exact word matching, an inflexible approach for measuring semantic similarity. We introduce methods based on sentence mover's similarity; our automatic metrics evaluate text in a continuous space using word and sentence embeddings. We find that sentence-based metrics correlate with human judgments significantly better than ROUGE, both on machine-generated summaries (average length of 3.4 sentences) and human-authored essays (average length of 7.5). We also show that sentence mover's similarity can be used as a reward when learning a generation model via reinforcement learning; we present both automatic and human evaluations of summaries learned in this way, finding that our approach outperforms ROUGE.1 For readability, we scale ROUGE scores by a factor of 100 and sentence mover's metrics by a factor of 1000.ReferencesMohammed Alshahrani, Spyridon Samothrakis, and Maria Fasli. 2017. Word mover's distance for affect detection.",
        "id": 192530110
      }
    ],
    "negative_ctxs": [
      {
        "title": "Utilizing Temporal Information for Taxonomy Construction",
        "text": "Taxonomies play an important role in many applications by organizing domain knowledge into a hierarchy of 'is-a' relations between terms. Previous work on automatic construction of taxonomies from text documents either ignored temporal information or used fixed time periods to discretize the time series of documents. In this paper, we propose a time-aware method to automatically construct and effectively maintain a taxonomy from a given series of documents preclustered for a domain of interest. The method extracts temporal information from the documents and uses a timestamp contribution function to score the temporal relevance of the evidence from source texts when identifying the taxonomic relations for constructing the taxonomy. Experimental results show that our proposed method outperforms the stateof-the-art methods by increasing F-measure up to 7%-20%. Furthermore, the proposed method can incrementally update the taxonomy by adding fresh relations from new data and removing outdated relations using an information decay function. It thus avoids rebuilding the whole taxonomy from scratch for every update and keeps the taxonomy effectively up-to-date in order to track the latest information trends in the rapidly evolving domain.551",
        "id": 16168839
      },
      {
        "title": "Misspelling Oblivious Word Embeddings",
        "text": "In this paper we present a method to learn word embeddings that are resilient to misspellings.Existing word embeddings have limited applicability to malformed texts, which contain a non-negligible amount of outof-vocabulary words. We propose a method combining FastText with subwords and a supervised task of learning misspelling patterns. In our method, misspellings of each word are embedded close to their correct variants. We train these embeddings on a new dataset we are releasing publicly. Finally, we experimentally show the advantages of this approach on both intrinsic and extrinsic NLP tasks using public test sets.",
        "id": 162184257
      },
      {
        "title": "An Analytical Study of Synonymy in Assamese Language Using WorldNet: Classification and Structure",
        "text": "The present paper aims to categorize different types of synonymous words and also to highlight their synonymic pattern as well as grammatical categories found in Wordnet of Assamese language. Synonymy is an important component of vocabulary of the language. It establishes lexical relation between words. In fact, the term 'synonymy' is applied to the two or more words which share the same semantic features. WorldNet is a lexical database consisting of synsets. A synset is constructed by assembling a set of synonyms that together define a unique sense and synset is the basic foundation of Wordnet. Assamese language is rich in synonyms. In Assamese WorldNet, more than 20,000 synsets are entered under the categories of Noun, Verb, Adverb and Adjective. These synsets can of different types according to their semantic similarity, connotation, denotation, stylistic variations etc.",
        "id": 15673301
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Can you suggest literature on a dataset that categorizes various emotions like anger, anticipation, fear, joy, and sadness in Facebook posts across multiple languages?",
    "positive_ctxs": [
      {
        "title": "Universal Joy A Data Set and Results for Classifying Emotions Across Languages",
        "text": "While emotions are universal aspects of human psychology, they are expressed differently across different languages and cultures. We introduce a new data set of over 530k anonymized public Facebook posts across 18 languages, labeled with five different emotions. Using multilingual BERT embeddings, we show that emotions can be reliably inferred both within and across languages. Zero-shot learning produces promising results for lowresource languages. Following established theories of basic emotions, we provide a detailed analysis of the possibilities and limits of crosslingual emotion classification. We find that structural and typological similarity between languages facilitates cross-lingual learning, as well as linguistic diversity of training data. Our results suggest that there are commonalities underlying the expression of emotion in different languages. We publicly release the anonymized data for future research.",
        "id": 233364964
      }
    ],
    "negative_ctxs": [
      {
        "title": "Kyoto: An Integrated System for Specific Domain WSD",
        "text": "This document describes the preliminary release of the integrated Kyoto system for specific domain WSD. The system uses concept miners (Tybots) to extract domain-related terms and produces a domain-related thesaurus, followed by knowledge-based WSD based on wordnet graphs (UKB). The resulting system can be applied to any language with a lexical knowledge base, and is based on publicly available software and resources. Our participation in Semeval task #17 focused on producing running systems for all languages in the task, and we attained good results in all except Chinese. Due to the pressure of the time-constraints in the competition, the system is still under development, and we expect results to improve in the near future.",
        "id": 15865150
      },
      {
        "title": "What do tokens know about their characters and how do they know it?",
        "text": "Pre-trained language models (PLMs) that use subword tokenization schemes can succeed at a variety of language tasks that require characterlevel information, despite lacking explicit access to the character composition of tokens. Here, studying a range of models (e.g., GPT-J, BERT, RoBERTa, GloVe), we probe what word pieces encode about character-level information by training classifiers to predict the presence or absence of a particular alphabetical character in a token, based on its embedding (e.g., probing whether the model embedding for \"cat\" encodes that it contains the character \"a\"). We find that these models robustly encode character-level information and, in general, larger models perform better at the task. We show that these results generalize to characters from non-Latin alphabets (Arabic, Devanagari, and Cyrillic). Then, through a series of experiments and analyses, we investigate the mechanisms through which PLMs acquire English-language character information during training and argue that this knowledge is acquired through multiple phenomena, including a systematic relationship between particular characters and particular parts of speech, as well as natural variability in the tokenization of related strings.",
        "id": 249394509
      },
      {
        "title": "End-to-End Synthetic Data Generation for Domain Adaptation of Question Answering Systems",
        "text": "We propose an end-to-end approach for synthetic QA data generation. Our model comprises a single transformer-based encoderdecoder network that is trained end-to-end to generate both answers and questions. In a nutshell, we feed a passage to the encoder and ask the decoder to generate a question and an answer token-by-token. The likelihood produced in the generation process is used as a filtering score, which avoids the need for a separate filtering model. Our generator is trained by finetuning a pretrained LM using maximum likelihood estimation. The experimental results indicate significant improvements in the domain adaptation of QA models outperforming current state-of-the-art methods. * *equal contribution. † Siamak Shakeri is currently with Google. The work was done when he was at AWS AI.",
        "id": 222310116
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "What paper compares humans' and language models' non-literal interpretations of utterances featuring phenomena like deceit, irony, and humor?",
    "positive_ctxs": [
      {
        "title": "A fine-grained comparison of pragmatic language understanding in humans and language models",
        "text": "Pragmatics and non-literal language understanding are essential to human communication, and present a long-standing challenge for artificial language models. We perform a finegrained comparison of language models and humans on seven pragmatic phenomena, using zero-shot prompting on an expert-curated set of English materials. We ask whether models (1) select pragmatic interpretations of speaker utterances, (2) make similar error patterns as humans, and (3) use similar linguistic cues as humans to solve the tasks. We find that the largest models achieve high accuracy and match human error patterns: within incorrect responses, models favor literal interpretations over heuristic-based distractors. We also find preliminary evidence that models and humans are sensitive to similar linguistic cues. Our results suggest that pragmatic behaviors can emerge in models without explicitly constructed representations of mental states. However, models tend to struggle with phenomena relying on social expectation violations.",
        "id": 254591475
      }
    ],
    "negative_ctxs": [
      {
        "title": "Using a Large Set of EAGLES-compliant Morpho-Syntactic Descriptors as a Tagset for Probabilistic Tagging 'DQ 7XILú",
        "text": "The paper presents one way of reconciling data sparseness with the requirement of high accuracy tagging in terms of fine-grained tagsets. For lexicon encoding, EAGLES elaborated a set of recommendations aimed at covering multilingual requirements and therefore resulted in a large number of features and possible values. Such an encoding, used for tagging purposes, would lead to very large tagsets. For instance, our EAGLES-compliant lexicon required a set of about 1000 morpho-syntactic description codes (MSDs) which after considering some systematic syncretic phenomena, was reduced to a set of 614 MSDs. Building reliable language models (LMs) for this tagset would require unrealistically large training data (hand annotated/validated). Our solution was to design a hidden reduced tagset and use it in building various LMs. The underlying tagger uses these LMs to tag a new text in as many variants as LMs are available. The tag differences between these variants are processed by a combiner which chooses the most likely tags. In the end, the tagged text is subject to a conversion process that maps the tags from the reduced tagset onto the more informative tags from the large tagset. We describe this processing chain and provide a detailed evaluation of the results.",
        "id": 14619939
      },
      {
        "title": "Multilingual Language Models are not Multicultural: A Case Study in Emotion",
        "text": "Emotions are experienced and expressed differently across the world. In order to use Large Language Models (LMs) for multilingual tasks that require emotional sensitivity, LMs must reflect this cultural variation in emotion. In this study, we investigate whether the widely-used multilingual LMs in 2023 reflect differences in emotional expressions across cultures and languages. We find that embeddings obtained from LMs (e.g., XLM-RoBERTa) are Anglocentric, and generative LMs (e.g., ChatGPT) reflect Western norms, even when responding to prompts in other languages. Our results show that multilingual LMs do not successfully learn the culturally appropriate nuances of emotion and we highlight possible research directions towards correcting this.",
        "id": 259342568
      },
      {
        "title": "BioNLP Shared Task 2011 -Bacteria Gene Interactions and Renaming",
        "text": "We present two related tasks of the BioNLP Shared Tasks 2011: Bacteria Gene Renaming (Rename) and Bacteria Gene Interactions (GI). We detail the objectives, the corpus specification, the evaluation metrics, and we summarize the participants' results. Both issued from PubMed scientific literature abstracts, the Rename task aims at extracting gene name synonyms, and the GI task aims at extracting genic interaction events, mainly about gene transcriptional regulations in bacteria.",
        "id": 2142405
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there any dataset that contains minimally-contrasting social situations that lead to different decisions about which behaviors are appropriate in that situation?",
    "positive_ctxs": [
      {
        "title": "NORMBANK: A Knowledge Bank of Situational Social Norms",
        "text": "We present NORMBANK, a knowledge bank of 155k situational norms. This resource is designed to ground flexible normative reasoning for interactive, assistive, and collaborative AI systems. Unlike prior commonsense resources, NORMBANK grounds each inference within a multivalent sociocultural frame, which includes the setting (e.g., restaurant), the agents' contingent roles (waiter, customer), their attributes (age, gender), and other physical, social, and cultural constraints (e.g., the temperature or the country of operation). In total, NORMBANK contains 63k unique constraints from a taxonomy that we introduce and iteratively refine here. Constraints then apply in different combinations to frame social norms. Under these manipulations, norms are non-monotonic -one can cancel an inference by updating its frame even slightly. Still, we find evidence that neural models can help reliably extend the scope and coverage of NORMBANK. We further demonstrate the utility of this resource with a series of transfer experiments. For data and code, see https://github.com/SALT-NLP/normbank",
        "id": 258947336
      }
    ],
    "negative_ctxs": [
      {
        "title": "DECISION LISTS FOR LEXICAL AMBIGUITY RESOLUTION: Application to Accent Restoration in Spanish and French",
        "text": "This paper presents a statistical decision procedure for lexical ambiguity resolution. The algorithm exploits both local syntactic patterns and more distant collocational evidence, generating an efficient, effective, and highly perspicuous recipe for resolving a given ambiguity. By identifying and utilizing only the single best disambiguating evidence in a target context, the algorithm avoids the problematic complex modeling of statistical dependencies. Although directly applicable to a wide class of ambiguities, the algorithm is described and evaluated in a realistic case study, the problem of restoring missing accents in Spanish and French text. Current accuracy exceeds 99% on the full task, and typically is over 90% for even the most difficult ambiguities.",
        "id": 1580335
      },
      {
        "title": "",
        "text": "Semantic role labeling offers vital information for both Linguistics and Natural Language Processing tasks. In this article, we present a lexical resource for Portuguese annotated with semantic roles: VerbLexPor. The resource is a database with verbs and sentences extracted from both a domain specific corpus and a non-specialized generic one. Annotation was manually carried out by a linguist using VerbNet-like semantic roles. The resource has more than 6 thousand annotated sentences and 15 thousand annotated arguments, and is available for download as XML or SQL files. The paper also describes a comparative analysis between the two corpora, showing that the distribution of semantic roles in a general domain is different from that in specific domain.",
        "id": 45930698
      },
      {
        "title": "CODEGEN: AN OPEN LARGE LANGUAGE MODEL FOR CODE WITH MULTI-TURN PROGRAM SYNTHESIS",
        "text": "Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multiturn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.",
        "id": 252668917
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Is there any paper that previously proposed to control a risk using prediction sets, based on the literature in conformal prediction?",
    "positive_ctxs": [
      {
        "title": "Conformal Risk Control",
        "text": "We extend conformal prediction to control the expected value of any monotone loss function. The algorithm generalizes split conformal prediction together with its coverage guarantee. Like conformal prediction, the conformal risk control procedure is tight up to an O(1/n) factor. We also introduce extensions of the idea to distribution shift, quantile risk control, multiple and adversarial risk control, and expectations of U-statistics. Worked examples from computer vision and natural language processing demonstrate the usage of our algorithm to bound the false negative rate, graph distance, and token-level F1-score.",
        "id": 251320513
      }
    ],
    "negative_ctxs": [
      {
        "title": "Linking WordNet Verb Classes to Semantic Interpretation",
        "text": "An analysis that defines predicates for Wordnet verb classes and links them to semantic interpretation is presented. The selectional restrictions for the thematic roles defining the predicates are WordNet ontological categories. Thematic roles are also linked to the syntactic relations that realize them. The paper illustrates the methodology by providing a detailed analysis of some major WordNet verb classes.! !",
        "id": 2668408
      },
      {
        "title": "Information Content Measures of Semantic Similarity Perform Better Without Sense-Tagged Text",
        "text": "This paper presents an empirical comparison of similarity measures for pairs of concepts based on Information Content. It shows that using modest amounts of untagged text to derive Information Content results in higher correlation with human similarity judgments than using the largest available corpus of manually annotated sense-tagged text.",
        "id": 1085680
      },
      {
        "title": "Transition-based Neural Constituent Parsing",
        "text": "Constituent parsing is typically modeled by a chart-based algorithm under probabilistic context-free grammars or by a transition-based algorithm with rich features. Previous models rely heavily on richer syntactic information through lexicalizing rules, splitting categories, or memorizing long histories. However enriched models incur numerous parameters and sparsity issues, and are insufficient for capturing various syntactic phenomena. We propose a neural network structure that explicitly models the unbounded history of actions performed on the stack and queue employed in transition-based parsing, in addition to the representations of partially parsed tree structure. Our transition-based neural constituent parsing achieves performance comparable to the state-of-the-art parsers, demonstrating F1 score of 90.68% for English and 84.33% for Chinese, without reranking, feature templates or additional data to train model parameters.",
        "id": 16313885
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest datasets that can benchmark LLM performance in achieving conversational continuity and recall over long multi-session conversations?",
    "positive_ctxs": [
      {
        "title": "Beyond Goldfish Memory * : Long-Term Open-Domain Conversation",
        "text": "Despite recent improvements in open-domain dialogue models, state-of-the-art models are trained and evaluated on short conversations with little context. In contrast, the long-term conversation setting has hardly been studied. In this work we collect and release a humanhuman dataset consisting of multiple chat sessions whereby the speaking partners learn about each other's interests and discuss the things they have learnt from past sessions. We show how existing models trained on existing datasets perform poorly in this long-term conversation setting in both automatic and human evaluations, and we study long-context models that can perform much better. In particular, we find retrieval-augmented methods and methods with an ability to summarize and recall previous conversations outperform the standard encoder-decoder architectures currently considered state-of-the-art. * We use this term colloquially, seeAgranoff et al. (1965)for evidence of goldfish long-term memory.",
        "id": 236034497
      }
    ],
    "negative_ctxs": [
      {
        "title": "Clean Data for Training Statistical MT: The Case of MT Contamination",
        "text": "Users of Statistical Machine Translation (SMT) sometimes turn to the Web to obtain data to train their systems. One problem with this approach is the potential for \"MT contamination\": when large amounts of parallel data are collected automatically, there is a risk that a nonnegligible portion consists of machine-translated text. Theoretically, using this kind of data to train SMT systems is likely to reinforce the errors committed by other systems, or even by an earlier versions of the same system. In this paper, we study the effect of MT-contaminated training data on SMT quality, by performing controlled simulations under a wide range of conditions. Our experiments highlight situations in which MT contamination can be harmful, and assess the potential of decontamination techniques.",
        "id": 11770707
      },
      {
        "title": "Compasses, Magnets, Water Microscopes Annotation and Analysis of Terminology in a Diachronic Corpus of Scientific Texts",
        "text": "The specialised lexicon belongs to the most prominent attributes of specialised writing: Terms function as semantically dense encodings of specialised concepts, which, in the absence of terms, would require lengthy explanations and descriptions. In this paper, we argue that terms are the result of diachronic processes on both the semantic and the morpho-syntactic level. Very little is known about these processes. We therefore present a corpus annotation project aiming at revealing how terms are coined and how they evolve to fit their function as semantically and morpho-syntactically dense encodings of specialised knowledge. The scope of this paper is two-fold: Firstly, we outline our methodology for annotating terminology in a diachronic corpus of scientific publications. Moreover, we provide a detailed analysis of our annotation results and suggest methods for improving the accuracy of annotations in a setting as difficult as ours. Secondly, we present results of a pilot study based on the annotated terms. The results suggest that terms in older texts are linguistically relatively simple units that are hard to distinguish from the lexicon of general language. We believe that this supports our hypothesis that terminology undergoes diachronic processes of densification and specialisation.",
        "id": 32319993
      },
      {
        "title": "Social Proof: The Impact of Author Traits on Influence Detection",
        "text": "It has been claimed that people are more likely to be influenced by those who are similar to them than those who are not. In this paper, we test this hypothesis by measuring the impact of author traits on the detection of influence. The traits we explore are age, gender, religion, and political party. We create a single classifier to detect the author traits of each individual. We then use the personal traits predicted by this classifier to predict the influence of contributors in a Wikipedia Talk Page corpus. Our research shows that the influencer tends to have the same traits as the majority of people in the conversation. Furthermore, we show that this is more pronounced when considering the personal traits most relevant to the conversation. Our research thus provides evidence for the theory of social proof.27",
        "id": 10698681
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Can you point me to studies that explore the impact of different data augmentation strategies, such as feature/token/span cutoff or dropout, in the context of contrastive learning for sentence representations?",
    "positive_ctxs": [
      {
        "title": "ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer",
        "text": "Learning high-quality sentence representations benefits a wide range of natural language processing tasks. Though BERT-based pretrained language models achieve high performance on many downstream tasks, the native derived sentence representations are proved to be collapsed and thus produce a poor performance on the semantic textual similarity (STS) tasks. In this paper, we present ConSERT, a Contrastive Framework for Self-Supervised SEntence Representation Transfer, that adopts contrastive learning to fine-tune BERT in an unsupervised and effective way. By making use of unlabeled texts, ConSERT solves the collapse issue of BERT-derived sentence representations and make them more applicable for downstream tasks. Experiments on STS datasets demonstrate that ConSERT achieves an 8% relative improvement over the previous state-of-the-art, even comparable to the supervised SBERT-NLI. And when further incorporating NLI supervision, we achieve new stateof-the-art performance on STS tasks. Moreover, ConSERT obtains comparable results with only 1000 samples available, showing its robustness in data scarcity scenarios.",
        "id": 235187266
      },
      {
        "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings",
        "text": "This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using \"entailment\" pairs as positives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2% and 2.2% improvement compared to previous best results. We also show-both theoretically and empirically-that contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available. 1",
        "id": 233296292
      }
    ],
    "negative_ctxs": [
      {
        "title": "The Effects of Formal Schema on Reading Comprehension-An Experiment with Chinese EFL Readers",
        "text": "This study attempts to explore the effects of formal schemata or rhetorical patterns on reading comprehension through detailed analysis of a case study of 45 non-English majors from X University. The subjects were selected from three classes of comparable English level and were divided into three groups. Each group was asked to recall the text and finish a cloze test after reading one of three versions of a passage with identical content but different formal schemata: description schema, comparison and contrast schema, and problem-solution schema. Both quantitative and qualitative analyses of the recall protocol indicate that subjects displayed better recall of the text with highly structured schema than the one with loosely controlled schema, which suggests that formal schemata has a significant effect on written communication and the teaching of formal schemata to students is necessary to enhance their writing ability.",
        "id": 17205703
      },
      {
        "title": "Exploring Phoneme-Level Speech Representations for End-to-End Speech Translation",
        "text": "Previous work on end-to-end translation from speech has primarily used frame-level features as speech representations, which creates longer, sparser sequences than text. We show that a naïve method to create compressed phoneme-like speech representations is far more effective and efficient for translation than traditional frame-level speech features. Specifically, we generate phoneme labels for speech frames and average consecutive frames with the same label to create shorter, higher-level source sequences for translation. We see improvements of up to 5 BLEU on both our high and low resource language pairs, with a reduction in training time of 60%. Our improvements hold across multiple data sizes and two language pairs.",
        "id": 174798018
      },
      {
        "title": "FFB: A Fair Fairness Benchmark for In-Processing Group Fairness Methods",
        "text": "This paper introduces the Fair Fairness Benchmark (FFB), a benchmarking framework for in-processing group fairness methods. Ensuring fairness in machine learning is critical for ethical and legal compliance. However, there exist challenges in comparing and developing of fairness methods due to inconsistencies in experimental settings, lack of accessible algorithmic implementations, and limited extensibility of current fairness packages and tools. To address these issues, we introduce an open-source, standardized benchmark for evaluating in-processing group fairness methods and provide a comprehensive analysis of state-of-the-art methods to ensure different notions of group fairness. This work offers the following key contributions: the provision of flexible, extensible, minimalistic, and research-oriented opensource code; the establishment of unified fairness method benchmarking pipelines; and extensive benchmarking, which yields key insights from 45, 079 experiments. We believe our work will significantly facilitate the growth and development of the fairness research community. The benchmark, including code and running logs, is available at https://github.com/ahxt/fair_fairness_benchmark. * This work was done while the first author was an intern at Meta.2We use utility to represent the performance of the downstream task.Preprint. Under review.",
        "id": 259187750
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "How can I locate a dataset containing toxic sentence pairs alongside their non-toxic paraphrased counterparts for studying text detoxification?",
    "positive_ctxs": [
      {
        "title": "ParaDetox: Detoxification with Parallel Data",
        "text": "We present a novel pipeline for the collection of parallel data for the detoxification task. We collect non-toxic paraphrases for over 10,000 English toxic sentences. We also show that this pipeline can be used to distill a large existing corpus of paraphrases to get toxicneutral sentence pairs. We release two parallel corpora which can be used for the training of detoxification models. To the best of our knowledge, these are the first parallel datasets for this task. We describe our pipeline in detail to make it fast to set up for a new language or domain, thus contributing to faster and easier development of new parallel resources.We train several detoxification models on the collected data and compare them with several baselines and state-of-the-art unsupervised approaches. We conduct both automatic and manual evaluations. All models trained on parallel data outperform the state-of-the-art unsupervised models by a large margin. This suggests that our novel datasets can boost the performance of detoxification systems.",
        "id": 248780527
      }
    ],
    "negative_ctxs": [
      {
        "title": "Duluth at SemEval-2016 Task 14 : Extending Gloss Overlaps to Enrich Semantic Taxonomies",
        "text": "This paper describes the Duluth systems that participated in Task 14 of SemEval 2016, Semantic Taxonomy Enrichment. There were three related systems in the formal evaluation which are discussed here, along with numerous post-evaluation runs. All of these systems identified synonyms between Word-Net and other dictionaries by measuring the gloss overlaps between them. These systems perform better than the random baseline and one post-evaluation variation was within a respectable margin of the median result attained by all participating systems.",
        "id": 776002
      },
      {
        "title": "",
        "text": "",
        "id": 226283765
      },
      {
        "title": "Hard Gate Knowledge Distillation - Leverage Calibration for a Robust and Reliable Language Model",
        "text": "In knowledge distillation, a student model is trained with supervisions from both knowledge from a teacher and observations drawn from a training data distribution. Knowledge of a teacher is considered a subject that holds interclass relations which send a meaningful supervision to a student; hence, much effort has been put to find such knowledge to be distilled. In this paper, we explore a question that has been given little attention: \"when to distill such knowledge.\" The question is answered in our work with the concept of model calibration; we view a teacher model not only as a source of knowledge but also as a gauge to detect miscalibration of a student. This simple and yet novel view leads to a hard gate knowledge distillation scheme that switches between learning from a teacher model and training data. We verify the gating mechanism in the context of natural language generation at both the token-level and the sentence-level. Empirical comparisons with strong baselines show that hard gate knowledge distillation not only improves model generalization, but also significantly lowers model calibration error.",
        "id": 253097876
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend scholarly articles that investigate the practice of refining language models through the exclusive modification of bias parameters in their linear components?",
    "positive_ctxs": [
      {
        "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
        "text": "We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods. Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.",
        "id": 231672601
      }
    ],
    "negative_ctxs": [
      {
        "title": "DSCORER: A Fast Evaluation Metric for Discourse Representation Structure Parsing",
        "text": "Discourse representation structures (DRSs) are scoped semantic representations for texts of arbitrary length. Evaluation of the accuracy of predicted DRSs plays a key role in developing semantic parsers and improving their performance. DRSs are typically visualized as nested boxes, in a way that is not straightforward to process automatically. COUNTER, an evaluation algorithm for DRSs, transforms them to clauses and measures clause overlap by searching for variable mappings between two DRSs. Unfortunately, COUNTER is computationally costly (with respect to memory and CPU time) and does not scale with longer texts. We introduce DSCORER, an efficient new metric which converts box-style DRSs to graphs and then measures the overlap of n-grams in the graphs. Experiments show that DSCORER computes accuracy scores that correlate with scores from COUNTER at a fraction of the time.",
        "id": 219963636
      },
      {
        "title": "Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies",
        "text": "We present the Uppsala system for the CoNLL 2018 Shared Task on universal dependency parsing. Our system is a pipeline consisting of three components: the first performs joint word and sentence segmentation; the second predicts part-ofspeech tags and morphological features; the third predicts dependency trees from words and tags. Instead of training a single parsing model for each treebank, we trained models with multiple treebanks for one language or closely related languages, greatly reducing the number of models. On the official test run, we ranked 7th of 27 teams for the LAS and MLAS metrics. Our system obtained the best scores overall for word segmentation, universal POS tagging, and morphological features.",
        "id": 52175678
      },
      {
        "title": "An Open-Source Finite State Morphological Transducer for Modern Standard Arabic",
        "text": "We develop an open-source large-scale finitestate morphological processing toolkit (Ara-ComLex) for Modern Standard Arabic (MSA) distributed under the GPLv3 license. 1 The morphological transducer is based on a lexical database specifically constructed for this purpose. In contrast to previous resources, the database is tuned to MSA, eliminating lexical entries no longer attested in contemporary use. The database is built using a corpus of 1,089,111,204 words, a pre-annotation tool, machine learning techniques, and knowledgebased pattern matching to automatically acquire lexical knowledge. Our morphological transducer is evaluated and compared to LDC's SAMA (Standard Arabic Morphological Analyser).",
        "id": 17781445
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "What is the first paper that theoretically studies training neural networks under small initialization?",
    "positive_ctxs": [
      {
        "title": "Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization",
        "text": "This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an O( log n √ µ ) upper bound on the time it takes for all neurons to achieve good alignment with the input data, where n is the number of data points and µ measures how well the data are separated. After the early alignment phase, the loss converges to zero at a O( 1 t ) rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.",
        "id": 260125817
      }
    ],
    "negative_ctxs": [
      {
        "title": "Video-to-HamNoSys Automated Annotation System",
        "text": "The Hamburg Notation System (HamNoSys) was developed for movement annotation of any sign language (SL) and can be used to produce signing animations for a virtual avatar with the JASigning platform. This provides the potential to use HamNoSys, i.e., strings of characters, as a representation of an SL corpus instead of video material. Processing strings of characters instead of images can significantly contribute to sign language research. However, the complexity of HamNoSys makes it difficult to annotate without a lot of time and effort. Therefore annotation has to be automatized. This work proposes a conceptually new approach to this problem. It includes a new tree representation of the HamNoSys grammar that serves as a basis for the generation of grammatical training data and classification of complex movements using machine learning. Our automatic annotation system relies on HamNoSys grammar structure and can potentially be used on already existing SL corpora. It is retrainable for specific settings such as camera angles, speed, and gestures. Our approach is conceptually different from other SL recognition solutions and offers a developed methodology for future research.",
        "id": 219303000
      },
      {
        "title": "A Risk-Averse Mechanism for Suicidality Assessment on Social Media",
        "text": "Recent studies have shown that social media has increasingly become a platform for users to express suicidal thoughts outside traditional clinical settings. With advances in Natural Language Processing strategies, it is now possible to design automated systems to assess suicide risk. However, such systems may generate uncertain predictions, leading to severe consequences. We hence reformulate suicide risk assessment as a selective prioritized prediction problem over the Columbia Suicide Severity Risk Scale (C-SSRS). We propose SASI, a risk-averse and self-aware transformer-based hierarchical attention classifier, augmented to refrain from making uncertain predictions. We show that SASI is able to refrain from 83% of incorrect predictions on real-world Reddit data. Furthermore, we discuss the qualitative, practical, and ethical aspects of SASI for suicide risk assessment as a human-in-the-loop framework. *",
        "id": 248780215
      },
      {
        "title": "Learning in Natural Language: Theory and Algorithmic Approaches*",
        "text": "This article summarizes work on developing a learning theory account for the major learning and statistics based approaches used in natural language processing. It shows that these approaches can all be explained using a single distribution free inductive principle related to the pac model of learning. Furthermore, they all make predictions using the same simple knowledge representation -a linear representation over a common feature space. This is significant both to explaining the generalization and robustness properties of these methods and to understanding how these methods might be extended to learn from more structured, knowledge intensive examples, as part of a learning centered approach to higher level natural language inferences.",
        "id": 12020345
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which paper first found that REINFORCE works better than actor critic algorithms like PPO for RL finetuning of pretrained chemistry language models (Transformers and RNNs)?",
    "positive_ctxs": [
      {
        "title": "Searching for High-Value Molecules Using Reinforcement Learning and Transformers",
        "text": "Reinforcement learning (RL) over text representations can be effective for finding high-value policies that can search over graphs.However, RL requires careful structuring of the search space and algorithm design to be effective in this challenge.Through extensive experiments, we explore how different design choices for text grammar and algorithmic choices for training can affect an RL policy's ability to generate molecules with desired properties.We arrive at a new RL-based molecular design algorithm (ChemRLformer) and perform a thorough analysis using 25 molecule design tasks, including computationally complex protein docking simulations.From this analysis, we discover unique insights in this problem space and show that ChemRLformer achieves state-of-the-art performance while being more straightforward than prior work by demystifying which design choices are actually helpful for text-based molecule design.",
        "id": 263620293
      }
    ],
    "negative_ctxs": [
      {
        "title": "Voting between Dictionary-based and Subword Tagging Models for Chinese Word Segmentation",
        "text": "This paper describes a Chinese word segmentation system that is based on majority voting among three models: a forward maximum matching model, a conditional random field (CRF) model using maximum subword-based tagging, and a CRF model using minimum subwordbased tagging. In addition, it contains a post-processing component to deal with inconsistencies. Testing on the closed track of CityU, MSRA and UPUC corpora in the third SIGHAN Chinese Word Segmentation Bakeoff, the system achieves a F-score of 0.961, 0.953 and 0.919, respectively.",
        "id": 14280106
      },
      {
        "title": "Time-Dependent Representation for Neural Event Sequence Prediction",
        "text": "Existing sequence prediction methods are mostly concerned with time-independent sequences, in which the actual time span between events is irrelevant and the distance between events is simply the difference between their order positions in the sequence. While this time-independent view of sequences is applicable for data such as natural languages, e.g., dealing with words in a sentence, it is inappropriate and inefficient for many real world events that are observed and collected at unequally spaced points of time as they naturally arise, e.g., when a person goes to a grocery store or makes a phone call. The time span between events can carry important information about the sequence dependence of human behaviors. To leverage continuous time in sequence prediction, we propose two methods for integrating time into event representation, based on the intuition on how time is tokenized in everyday life and previous work on embedding contextualization. We particularly focus on using these methods in recurrent neural networks, which have gained popularity in many sequence prediction tasks. We evaluated these methods as well as baseline models on two learning tasks: mobile app usage prediction and music recommendation. The experiments revealed that the proposed methods for time-dependent representation offer consistent gain on accuracy compared to baseline models that either directly use continuous time value in a recurrent neural network or do not use time.",
        "id": 13019454
      },
      {
        "title": "Resource Report: Building Parallel Text Corpora for Multi-Domain Translation System",
        "text": "Parallel text is one of the most valuable resources for development of statistical machine translation systems and other NLP applications. However, manual translations are very costly, and the number of known parallel text is limited. Hence, our research started with creating and collecting a large amount of parallel text resources for Indonesian-English. We describe in this paper the creation of parallel corpora: ANTARA News, BPPT-PANL and BTEC-ATR. In order to be useful, these resources must be available in reasonable quantities and qualities to be useful for statistical approaches to language processing. We describe problem and solution as well robust tools and annotation schema to build and process these corpora.",
        "id": 31188530
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper studies the concept of enhancing the coverage of a selective prediction system by re-attempting the questions on which it was not sufficiently confident.",
    "positive_ctxs": [
      {
        "title": "Post-Abstention: Towards Reliably Re-Attempting the Abstained Instances in QA",
        "text": "Despite remarkable progress made in natural language processing, even the state-of-the-art models often make incorrect predictions. Such predictions hamper the reliability of systems and limit their widespread adoption in realworld applications. Selective prediction partly addresses the above concern by enabling models to abstain from answering when their predictions are likely to be incorrect. While selective prediction is advantageous, it leaves us with a pertinent question 'what to do after abstention'. To this end, we present an explorative study on 'Post-Abstention', a task that allows re-attempting the abstained instances with the aim of increasing coverage of the system without significantly sacrificing its accuracy. We first provide mathematical formulation of this task and then explore several methods to solve it. Comprehensive experiments on 11 QA datasets show that these methods lead to considerable risk improvements -performance metric of the Post-Abstention task-both in the in-domain and the out-of-domain settings. We also conduct a thorough analysis of these results which further leads to several interesting findings. Finally, we believe that our work will encourage and facilitate further research in this important area of addressing the reliability of NLP systems.",
        "id": 258461053
      }
    ],
    "negative_ctxs": [
      {
        "title": "Generating Learner-Like Morphological Errors in Russian",
        "text": "To speed up the process of categorizing learner errors and obtaining data for languages which lack error-annotated data, we describe a linguistically-informed method for generating learner-like morphological errors, focusing on Russian. We outline a procedure to select likely errors, relying on guiding stem and suffix combinations from a segmented lexicon to match particular error categories and relying on grammatical information from the original context.",
        "id": 1042287
      },
      {
        "title": "One Vector is Not Enough: Entity-Augmented Distributional Semantics for Discourse Relations",
        "text": "Discourse relations bind smaller linguistic units into coherent texts. However, automatically identifying discourse relations is difficult, because it requires understanding the semantics of the linked arguments. A more subtle challenge is that it is not enough to represent the meaning of each argument of a discourse relation, because the relation may depend on links between lower-level components, such as entity mentions. Our solution computes distributional meaning representations by composition up the syntactic parse tree.A key difference from previous work on compositional distributional semantics is that we also compute representations for entity mentions, using a novel downward compositional pass. Discourse relations are predicted from the distributional representations of the arguments, and also of their coreferent entity mentions. The resulting system obtains substantial improvements over the previous state-of-theart in predicting implicit discourse relations in the Penn Discourse Treebank.",
        "id": 15065468
      },
      {
        "title": "A Semantics and Pragmatics for the Pluperfect",
        "text": "We offer a semantics and pragmatics of the pluperfect in narrative discourse. We rexamine in a formal model of implicature, how the reader's knowledge about the discourse, Gricean-maxims and causation contribute to the meaning of the pluperfect. By placing the analysis in a theory where the interactions among these knowledge resources can be precisely computed, we overcome some problems with previous Reichenbachian approaches.",
        "id": 6085570
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "What research has been conducted on determining the ideal segment length for unsupervised keyphrase extraction?",
    "positive_ctxs": [
      {
        "title": "Unsupervised Keyphrase Extraction by Jointly Modeling Local and Global Context",
        "text": "Embedding based methods are widely used for unsupervised keyphrase extraction (UKE) tasks. Generally, these methods simply calculate similarities between phrase embeddings and document embedding, which is insufficient to capture different context for a more effective UKE model. In this paper, we propose a novel method for UKE, where local and global contexts are jointly modeled. From a global view, we calculate the similarity between a certain phrase and the whole document in the vector space as transitional embedding based models do. In terms of the local view, we first build a graph structure based on the document where phrases are regarded as vertices and the edges are similarities between vertices. Then, we proposed a new centrality computation method to capture local salient information based on the graph structure. Finally, we further combine the modeling of global and local context for ranking. We evaluate our models on three public benchmarks (Inspec, DUC 2001, SemEval 2010 and compare with existing state-of-the-art models. The results show that our model outperforms most models while generalizing better on input documents with different domains and length. Additional ablation study shows that both the local and global information is crucial for unsupervised keyphrase extraction tasks.",
        "id": 237513596
      }
    ],
    "negative_ctxs": [
      {
        "title": "CROSS 2 STRA: Unpaired Cross-lingual Image Captioning with Cross-lingual Cross-modal Structure-pivoted Alignment",
        "text": "Unpaired cross-lingual image captioning has long suffered from irrelevancy and disfluency issues, due to the inconsistencies of the semantic scene and syntax attributes during transfer. In this work, we propose to address the above problems by incorporating the scene graph (SG) structures and the syntactic constituency (SC) trees. Our captioner contains the semantic structure-guided image-to-pivot captioning and the syntactic structure-guided pivot-to-target translation, two of which are joined via pivot language. We then take the SG and SC structures as pivoting, performing cross-modal semantic structure alignment and cross-lingual syntactic structure alignment learning. We further introduce cross-lingual&cross-modal back-translation training to fully align the captioning and translation stages. Experiments on English↔Chinese transfers show that our model shows great superiority in improving captioning relevancy and fluency.",
        "id": 258833472
      },
      {
        "title": "The role of memory in superiority violation gradience",
        "text": "This paper examines how grammatical and memory constraints explain gradience in superiority violation acceptability. A computational model encoding both categories of constraints is compared to experimental evidence. By formalizing memory capacity as beam-search in the parser, the model predicts gradience evident in human data. To predict attachment behavior, the parser must be sensitive to the types of nominal intervenors that occur between a wh-filler and its head. The results suggest memory is more informative for modeling violation gradience patterns than grammatical constraints.",
        "id": 716928
      },
      {
        "title": "Using the argumentative structure of scientific literature to improve information access",
        "text": "MEDLINE/PubMed contains structured abstracts that can provide argumentative labels. Selection of abstract sentences based on the argumentative label has shown to improve the performance of information retrieval tasks. These abstracts make up less than one quarter of all the abstracts in MEDLINE/PubMed, so it is worthwhile to learn how to automatically label the non-structured ones.",
        "id": 9047303
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there a method for measuring the critical errors that a dialogue system makes in its responses?",
    "positive_ctxs": [
      {
        "title": "Chat-Oriented Dialogue Systems",
        "text": "Despite tremendous advancements in dialogue systems, stable evaluation still requires human judgments producing notoriously high-variance metrics due to their inherent subjectivity. Moreover, methods and labels in dialogue evaluation are not fully standardized, especially for opendomain chats, with a lack of work to compare and assess the validity of those approaches. The use of inconsistent evaluation can misinform the performance of a dialogue system, which becomes a major hurdle to enhance it. Thus, a dimensional evaluation of chat-oriented opendomain dialogue systems that reliably measures several aspects of dialogue capabilities is desired. This paper presents a novel human evaluation method to estimate the rates of many dialogue system behaviors. Our method is used to evaluate four state-of-the-art open-domain dialogue systems and compared with existing approaches. The analysis demonstrates that our behavior method is more suitable than alternative Likert-style or comparative approaches for dimensional evaluation of these systems.A detailed validation of human evaluation meth-ods, including likert scales and pairwise comparisons (Section 7).A comprehensive evaluation of four MTOD chatbots using validated metrics (Section 8).By presenting a detailed picture of MTOD chatbot performance and standard methods to evaluate them, we aid future work's efforts to further understand and improve human-computer interaction. Our evaluation platform, analyses, and data are available at https://github.com/emorynlp/ ChatEvaluationPlatform.ChatbotsTo evaluate the strengths and weaknesses of MTOD models, we select the chatbots for our study using a 15044 two-stage process: (1) a literature review to identify chatbot candidates, and (2) a pilot evaluation to select the final set of bots for our full study.Literature Review To promote diversity among the selected chatbots, we focus our review on four popular themes of the human-computer chat: (1) Knowledge-grounded chat, (2) Empathetic chat, (3) Self-consistent chat, and(4)General open-domain chat with large pre-training resources like Reddit. Candidate chatbots are selected from each theme using the following criteria:1. The bot must demonstrate state-of-the-art performance in a task related to the theme. 1 2. The implementation must be provided. 2 3. The response latency of the bot must be <10 seconds using modern GPU hardware.",
        "id": 254854340
      }
    ],
    "negative_ctxs": [
      {
        "title": "A Convolutional Architecture for Word Sequence Prediction",
        "text": "We propose a convolutional neural network, named genCNN, for word sequence prediction.Different from previous work on neural networkbased language modeling and generation (e.g., RNN or LSTM), we choose not to greedily summarize the history of words as a fixed length vector. Instead, we use a convolutional neural network to predict the next word with the history of words of variable length. Also different from the existing feedforward networks for language modeling, our model can effectively fuse the local correlation and global correlation in the word sequence, with a convolution-gating strategy specifically designed for the task. We argue that our model can give adequate representation of the history, and therefore can naturally exploit both the short and long range dependencies. Our model is fast, easy to train, and readily parallelized. Our extensive experiments on text generation and n-best re-ranking in machine translation show that genCNN outperforms the state-ofthe-arts with big margins.",
        "id": 5453533
      },
      {
        "title": "Published as a conference paper at ICLR 2023 IDEAL: QUERY-EFFICIENT DATA-FREE LEARNING FROM BLACK-BOX MODELS",
        "text": "Knowledge Distillation (KD) is a typical method for training a lightweight student model with the help of a well-trained teacher model. However, most KD methods require access to either the teacher's training data or model parameter, which is unrealistic. To tackle this problem, recent works study KD under data-free and black-box settings. Nevertheless, these works require a large number of queries to the teacher model, which incurs significant monetary and computational costs. To address these problems, we propose a novel method called query-effIcient Datafree lEarning from blAck-box modeLs (IDEAL), which aims to query-efficiently learn from black-box model APIs to train a good student without any real data. In detail, IDEAL trains the student model in two stages: data generation and model distillation. Note that IDEAL does not require any query in the data generation stage and queries the teacher only once for each sample in the distillation stage. Extensive experiments on various real-world datasets show the effectiveness of the proposed IDEAL. For instance, IDEAL can improve the performance of the best baseline method DFME by 5.83% on CIFAR10 dataset with only 0.02× the query budget of DFME. * Equal contribution. † Work done during internship at Sony AI.",
        "id": 248987713
      },
      {
        "title": "Discourse on ASR Measurement: Introducing the ARPOCA Assessment Tool",
        "text": "Automatic speech recognition (ASR) has evolved from a pipeline architecture with pronunciation dictionaries, phonetic features and language models to the end-to-end systems performing a direct translation from a raw waveform into a word sequence. With the increase in accuracy and the availability of pre-trained models, the ASR systems are now omnipresent in our daily applications. On the other hand, the models' interpretability and their computational cost have become more challenging, particularly when dealing with less-common languages or identifying regional variations of speakers. This research proposal will follow a four-stage process: 1) Proving an overview of acoustic features and feature extraction algorithms; 2) Exploring current ASR models, tools, and performance assessment techniques; 3) Aligning features with interpretable phonetic transcripts; and 4) Designing a prototype AR-POCA to increase awareness of regional language variation and improve models feedback by developing a semi-automatic acoustic features extraction using PRAAT in conjunction with phonetic transcription.",
        "id": 248779995
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper introduces the R-GCN technique into document-level joint entity and relation extraction?",
    "positive_ctxs": [
      {
        "title": "A Novel Table-to-Graph Generation Approach for Document-Level Joint Entity and Relation Extraction",
        "text": "Document-level relation extraction (DocRE) aims to extract relations among entities within a document, which is crucial for applications like knowledge graph construction. Existing methods usually assume that entities and their mentions are identified beforehand, which falls short of real-world applications. To overcome this limitation, we propose TAG, a novel tableto-graph generation model for joint extraction of entities and relations at document-level. To enhance the learning of task dependencies, TAG induces a latent graph among mentions, with different types of edges indicating different task information, which is further broadcast with a relational graph convolutional network. To alleviate the error propagation problem, we adapt the hierarchical agglomerative clustering algorithm to back-propagate task information at decoding stage. Experiments on the benchmark dataset, DocRED, demonstrate that TAG surpasses previous methods by a large margin and achieves state-of-the-art results 1 . an Equatorial Guinean politician and writer. … After his exile, he settled down in Valencia with his second wife and her family. Balboa Boneke died from renal problems, coupled with a three-year depression caused by the death of his wife, on 10 March 2014 in Valencia , Spain .",
        "id": 259370653
      }
    ],
    "negative_ctxs": [
      {
        "title": "Example-based Machine Translation Based on Syntactic Transfer with Statistical Models",
        "text": "This paper presents example-based machine translation (MT) based on syntactic transfer, which selects the best translation by using models of statistical machine translation. Example-based MT sometimes generates invalid translations because it selects similar examples to the input sentence based only on source language similarity. The method proposed in this paper selects the best translation by using a language model and a translation model in the same manner as statistical MT, and it can improve MT quality over that of 'pure' example-based MT. A feature of this method is that the statistical models are applied after word re-ordering is achieved by syntactic transfer. This implies that MT quality is maintained even when we only apply a lexicon model as the translation model. In addition, translation speed is improved by bottom-up generation, which utilizes the tree structure that is output from the syntactic transfer.",
        "id": 1827666
      },
      {
        "title": "An OWL Ontology for HPSG",
        "text": "The paper presents an OWL ontology for HPSG. The HPSG ontology is integrated with an existing OWL ontology, GOLD, as a community of practice extension. The basic ideas are illustrated by visualizations of type hierarchies for parts of speech.",
        "id": 13980475
      },
      {
        "title": "Multilingual Automatic Extraction of Linguistic Data from Grammars",
        "text": "One of the goals of field linguistics is compilation of descriptive grammars for relatively little-studied languages. Until recently, extracting linguistic characteristics from grammatical descriptions and creating databases based on them was done manually. The aim of this paper is to apply methods of multilingual automatic information extraction to grammatical descriptions written in different languages of the world: we present a search engine for grammars, which would accelerate the tedious and time-consuming process of searching for information about linguistic features and facilitate research in the field of linguistic typology.",
        "id": 259212823
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there an evaluation metric for natural language generation that predicts the factual consistency score through a mean-max aggregation method?",
    "positive_ctxs": [
      {
        "title": "ALIGNSCORE: Evaluating Factual Consistency with A Unified Alignment Function",
        "text": "Many text generation applications require the generated text to be factually consistent with input information. Automatic evaluation of factual consistency is challenging. Previous work has developed various metrics that often depend on specific functions, such as natural language inference (NLI) or question answering (QA), trained on limited data. Those metrics thus can hardly assess diverse factual inconsistencies (e.g., contradictions, hallucinations) that occur in varying inputs/outputs (e.g., sentences, documents) from different tasks. In this paper, we propose ALIGNSCORE, a new holistic metric that applies to a variety of factual inconsistency scenarios as above. ALIGN-SCORE is based on a general function of information alignment between two arbitrary text pieces. Crucially, we develop a unified training framework of the alignment function by integrating a large diversity of data sources, resulting in 4.7M training examples from 7 well-established tasks (NLI, QA, paraphrasing, fact verification, information retrieval, semantic similarity, and summarization). We conduct extensive experiments on large-scale benchmarks including 22 evaluation datasets, where 19 of the datasets were never seen in the alignment training. ALIGNSCORE achieves substantial improvement over a wide range of previous metrics. Moreover, ALIGNSCORE (355M parameters) matches or even outperforms metrics based on ChatGPT and GPT-4 that are orders of magnitude larger. 1",
        "id": 258947273
      }
    ],
    "negative_ctxs": [
      {
        "title": "Many speakers, many worlds Interannotator variations in the quantification of feature norms Many speakers, many worlds Interannotator variations in the quantification of feature norms",
        "text": "",
        "id": 55488801
      },
      {
        "title": "ChiMed: A Chinese Medical Corpus for Question Answering",
        "text": "Question answering (QA) is a challenging task in natural language processing (NLP), especially when it is applied to specific domains. While models trained in the general domain can be adapted to a new target domain, their performance often degrades significantly due to domain mismatch. Alternatively, one can require a large amount of domain-specific QA data, but such data are rare, especially for the medical domain. In this study, we first collect a large-scale Chinese medical QA corpus called ChiMed; second we annotate a small fraction of the corpus to check the quality of the answers; third, we extract two datasets from the corpus and use them for the relevancy prediction task and the adoption prediction task. Several benchmark models are applied to the datasets, producing good results for both tasks.",
        "id": 199379474
      },
      {
        "title": "",
        "text": "",
        "id": 219305601
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there any paper that explores using only an encoder-only masked language model for open-ended long text generation (such as story generation)?",
    "positive_ctxs": [
      {
        "title": "Open-ended Long Text Generation via Masked Language Modeling",
        "text": "Pre-trained autoregressive (AR) language models such as BART and GPTs have dominated Open-ended Long Text Generation (Open-LTG). However, the AR nature will decrease the inference efficiency along with the increase of generation length, which hinder their application in Open-LTG. To improve inference efficiency, we alternatively explore the potential of the pre-trained masked language models (MLMs) along with a representative iterative non-autoregressive (NAR) decoding strategy for Open-LTG. Our preliminary study shows that pre-trained MLMs can merely generate short text and will collapse for long text modeling. To enhance the long text generation capability of MLMs, we introduce two simple yet effective strategies for the iterative NAR model: dynamic sliding window attention (DSWA) and linear temperature decay (LTD). It can alleviate long-distance collapse problems and achieve longer text generation with a flexible trade-off between performance and inference speedup. Experiments on the storytelling and multi-paragraph opinionated article writing tasks show that pre-trained MLMs can achieve more than 3 × → 13 × speedup with better performance than strong AR models. Our code is available at GitHub * . . 2022. A survey on non-autoregressive generation for neural machine translation and beyond. arXiv preprint arXiv:2204.09269.",
        "id": 259370630
      }
    ],
    "negative_ctxs": [
      {
        "title": "Sparse Bayesian Classification of Predicate Arguments",
        "text": "We present an application of Sparse Bayesian Learning to the task of semantic role labeling, and we demonstrate that this method produces smaller classifiers than the popular Support Vector approach.We describe the classification strategy and the features used by the classifier. In particular, the contribution of six parse tree path features is investigated.",
        "id": 13926789
      },
      {
        "title": "",
        "text": "",
        "id": 218974132
      },
      {
        "title": "Linguistic profiling of texts for the purpose of language verification",
        "text": "In order to control the quality of internet-based language corpora, we developed a method to verify automatically that texts are of (near-) native quality. For the LOCNESS and ICLE corpora, the method is rather successful in separating native and non-native learner texts. The Equal Error Rate is about 10%. However, for other domains, such as internet texts, separate classifiers have to be trained on the basis of suitable seed corpora.",
        "id": 3275899
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which paper first proposes a unified framework for black-box and white-box detection of AI-written text with explanations?",
    "positive_ctxs": [
      {
        "title": "DNA-GPT: DIVERGENT N-GRAM ANALYSIS FOR TRAINING-FREE DETECTION OF GPT-GENERATED TEXT",
        "text": "Large language models (LLMs) have notably enhanced the fluency and diversity of machine-generated text.However, this progress also presents a significant challenge in detecting the origin of a given text, and current research on detection methods lags behind the rapid evolution of LLMs.Conventional training-based methods have limitations in flexibility, particularly when adapting to new domains, and they often lack explanatory power.To address this gap, we propose a novel training-free detection strategy called Divergent N-Gram Analysis (DNA-GPT).Given a text, we first truncate it in the middle and then use only the preceding portion as input to the LLMs to regenerate the new remaining parts.By analyzing the differences between the original and new remaining parts through N-gram analysis in black-box or probability divergence in white-box, we unveil significant discrepancies between the distribution of machine-generated text and the distribution of human-written text.We conducted extensive experiments on the most advanced LLMs from Ope-nAI, including text-davinci-003, GPT-3.5-turbo, and GPT-4, as well as open-source models such as GPT-NeoX-20B and LLaMa-13B.Results show that our zero-shot approach exhibits state-of-the-art performance in distinguishing between human and GPT-generated text on four English and one German dataset, outperforming OpenAI's own classifier, which is trained on millions of text.Additionally, our methods provide reasonable explanations and evidence to support our claim, which is a unique feature of explainable detection.Our method is also robust under the revised text attack and can additionally solve model sourcing.Codes are available at https://github.com/Xianjun-Yang/DNA-GPT",
        "id": 258960101
      }
    ],
    "negative_ctxs": [
      {
        "title": "Does Summary Evaluation Survive Translation to Other Languages?",
        "text": "The creation of a quality summarization dataset is an expensive, time-consuming effort, requiring the production and evaluation of summaries by both trained humans and machines. The returns to such an effort would increase significantly if the dataset could be used in additional languages without repeating human annotations. To investigate how much we can trust machine translation of summarization datasets, we translate the English Sum-mEval dataset to seven languages and compare performances across automatic evaluation measures. We explore equivalence testing as the appropriate statistical paradigm for evaluating correlations between human and automated scoring of summaries. We also consider the effect of translation on the relative performance between measures. We find some potential for dataset reuse in languages similar to the source and along particular dimensions of summary quality. Our code and data can be found at https://github.com/ PrimerAI/primer-research/.",
        "id": 237532546
      },
      {
        "title": "Review of Entity Relation Extraction based on deep learning",
        "text": "As a core task of Information Extraction, Entity Relation Extraction plays an important role in many Natural Language Processing applications such as knowledge graph, intelligent question answering system and semantic search. Relation extraction tasks aim to find the semantic relation between a pair of entity mentions from unstructured texts. This paper focuses on the sentence-level relation extraction, introduces the main datasets for this task, and expounds the current status of relation extraction technology which can be divided into: supervised relation extraction, distant supervision relation extraction and joint extraction of entities and relations. We compare the various models for this task and analyze their contributions and defects. Finally, the research status and methods of Chinese entity relation extraction are introduced.",
        "id": 225062645
      },
      {
        "title": "The Proper Treatment of Optimality in Computational Phonology",
        "text": "This paper presents a novel formalization of optimality theory. Unlike previous treatments of optimality in computational linguistics, starting with EUison (1994), the new approach does not require any explicit marking and counting of constraint violations. It is based on the notion of \"lenient composition\", defined as the combination of ordinary composition and priority union. If an underlying form has outputs that can meet a given constraint, lenient composition enforces the constraint; if none of the output candidates meets the constraint, lenient composition allows all of them. For the sake of greater efficiency, we may \"leniently compose\" the GEN relation and all the constraints into a single finite-state transducer that maps each underlying form directly into its optimal surface realizations, and vice versa. Seen f~om this perspective, optimality theolT is surprisingly similar to the two older strains of finite-state phonology: classical rewrite systems and two-level models. In particular, the ranking of optimality constraints corresponds to the ordering of rewrite rules.",
        "id": 1460
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research that improves knowledge base generation using non-differentiable evaluation metrics like BLEU, METEOR, and chrF++?",
    "positive_ctxs": [
      {
        "title": "ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models",
        "text": "Automatic construction of relevant Knowledge Bases (KBs) from text, and generation of semantically meaningful text from KBs are both long-standing goals in Machine Learning. In this paper, we present ReGen, a bidirectional generation of text and graph leveraging Reinforcement Learning (RL) to improve performance. Graph linearization enables us to re-frame both tasks as a sequence to sequence generation problem regardless of the generative direction, which in turn allows the use of Reinforcement Learning for sequence training where the model itself is employed as its own critic leading to Self-Critical Sequence Training (SCST). We present an extensive investigation demonstrating that the use of RL via SCST benefits graph and text generation on WebNLG+ 2020 and TEKGEN datasets.Our system provides state-of-the-art results on WebNLG+ 2020 by significantly improving upon published results from the WebNLG 2020+ Challenge for both text-to-graph and graph-to-text generation tasks. More details in https://github.com/IBM/regen.",
        "id": 237353321
      }
    ],
    "negative_ctxs": [
      {
        "title": "Task-based MT Evaluation: Tackling Software, Experimental Design, & Statistical Models",
        "text": "Even with recent, renewed attention to MT evaluation-due in part to n-gram-based metrics(Papineni et al., 2001;Doddington, 2002)and the extensive, online catalogue of MT metrics on the ISLE project(Hovy et al., 2001, few reports involving task-based metrics have surfaced. This paper presents our work on three parts of taskbased MT evaluation: (i) software to track and record users' task performance via a browser, run from a desktop computer or remotely over the web, (ii) factorial experimental design with replicate observations to compare the MT engines, based on the accuracy of users' task responses, and (iii) the use of chi-squared and generalized linear models (GLMs) to permit finer-grained data analyses. We report on the experimental results of a six-way document categorization task, used for the evaluation of three Korean-English MT engines. The statistical models of the probabilities of correct responses yield an ordering of the MT engines, with one engine having a statistically significant lead over the other two. Future research will involve testing user performance on linguistically more complex tasks, as well as extending our initial GLMs with the documents' Bleu scores as variables, to test the scores as independent predictors of task results.",
        "id": 26565480
      },
      {
        "title": "Published as a conference paper at ICLR 2021 NEURAL THOMPSON SAMPLING",
        "text": "Thompson Sampling (TS) is one of the most effective algorithms for solving contextual multi-armed bandit problems. In this paper, we propose a new algorithm, called Neural Thompson Sampling, which adapts deep neural networks for both exploration and exploitation. At the core of our algorithm is a novel posterior distribution of the reward, where its mean is the neural network approximator, and its variance is built upon the neural tangent features of the corresponding neural network. We prove that, provided the underlying reward function is bounded, the proposed algorithm is guaranteed to achieve a cumulative regret of O(T 1/2 ), which matches the regret of other contextual bandit algorithms in terms of total round number T . Experimental comparisons with other benchmark bandit algorithms on various data sets corroborate our theory. arXiv:2010.00827v2 [cs.LG] 30 Dec 2021",
        "id": 222124941
      },
      {
        "title": "TIB-VA at SemEval-2022 Task 5: A Multimodal Architecture for the Detection and Classification of Misogynous Memes",
        "text": "The detection of offensive, hateful content on social media is a challenging problem that affects many online users on a daily basis. Hateful content is often used to target a group of people based on ethnicity, gender, religion and other factors. The hate or contempt toward women has been increasing on social platforms. Misogynous content detection is especially challenging when textual and visual modalities are combined to form a single context, e.g., an overlay text embedded on top of an image, also known as meme. In this paper, we present a multimodal architecture that combines textual and visual features to detect misogynous memes. The proposed architecture is evaluated in the SemEval-2022 Task 5: MAMI -Multimedia Automatic Misogyny Identification challenge under the team name TIB-VA. We obtained the best result in the Task-B where the challenge is to classify whether a given document is misogynous and further identify the following sub-classes: shaming, stereotype, objectification, and violence.",
        "id": 248157364
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper proposed decomposing the logit update of each of the attention blocks’ inputs to analyze how the context influences the prediction?",
    "positive_ctxs": [
      {
        "title": "Explaining How Transformers Use Context to Build Predictions",
        "text": "Language Generation Models produce words based on the previous context. Although existing methods offer input attributions as explanations for a model's prediction, it is still unclear how prior words affect the model's decision throughout the layers. In this work, we leverage recent advances in explainability of the Transformer and present a procedure to analyze models for language generation. Using contrastive examples, we compare the alignment of our explanations with evidence of the linguistic phenomena, and show that our method consistently aligns better than gradient-based and perturbation-based baselines. Then, we investigate the role of MLPs inside the Transformer and show that they learn features that help the model predict words that are grammatically acceptable. Lastly, we apply our method to Neural Machine Translation models, and demonstrate that they generate human-like source-target alignments for building predictions",
        "id": 258832652
      }
    ],
    "negative_ctxs": [
      {
        "title": "Detecting Suicidality with a Contextual Graph Neural Network",
        "text": "Discovering individuals' suicidality on social media has become increasingly important. Many researchers have studied to detect suicidality by using a suicide dictionary. However, while prior work focused on matching a word in a post with a suicide dictionary without considering contexts, little attention has been paid to how the word can be associated with the suicide-related context. To address this problem, we propose a suicidality detection model based on a graph neural network to grasp the dynamic semantic information of the suicide vocabulary by learning the relations between a given post and words. The extensive evaluation demonstrates that the proposed model achieves higher performance than the state-of-the-art methods. We believe the proposed model has great utility in identifying the suicidality of individuals and hence preventing individuals from potential suicide risks at an early stage. * Corresponding author. 1 https://data.oecd.org/healthstat/ suicide-rates.htm … I have my hair cut …Suicide DictionaryIndicator Risk (1) Behavior Risk(3)",
        "id": 250390685
      },
      {
        "title": "Consistent storage of metadata in inference lexica: the MetaLex approach",
        "text": "With MetaLex we introduce a framework for metadata management where information can be inferred from different areas of metadata coding, such as metadata for catalogue descriptions, linguistic levels, or tiers. This is done for consistency and efficiency in metadata recording and applies the same inference techniques that are used for lexical inference. For this purpose we motivate the need for metadata descriptions on all document levels, describe the different structures of metadata, use existing metadata recommendations on different levels of annotations, and show a usecase of metadata inference.",
        "id": 17503844
      },
      {
        "title": "No Permanent Friends or Enemies: Tracking Relationships between Nations from News",
        "text": "Understanding the dynamics of international politics is important yet challenging for civilians. In this work, we explore unsupervised neural models to infer relations between nations from news articles. We extend existing models by incorporating shallow linguistics information and propose a new automatic evaluation metric that aligns relationship dynamics with manually annotated key events. As understanding international relations requires carefully analyzing complex relationships, we conduct in-person human evaluations with three groups of participants. Overall, humans prefer the outputs of our model and give insightful feedback that suggests future directions for human-centered models. Furthermore, our model reveals interesting regional differences in news coverage. For instance, with respect to US-China relations, Singaporean media focus more on \"strengthening\" and \"purchasing\", while US media focus more on \"criticizing\" and \"denouncing\".",
        "id": 125952286
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research that examines the challenges faced by pre-trained language models in learning inferential commonsense knowledge when the context is sparse?",
    "positive_ctxs": [
      {
        "title": "Are Rotten Apples Edible? Challenging Commonsense Inference Ability with Exceptions",
        "text": "Previous studies have argued that pre-trained language models encode commonsense relational knowledge (e.g. that apples are edible). However, simultaneous work has revealed that such models are often insensitive to context, even ignoring overt contextual cues such as negations. In this paper, we investigate whether masked language models (the BERT family) can move beyond naive associative biases (e.g., apple → edible) when the context warrants (e.g. ranking inedible higher when presented with the information that the apple is rotten). We introduce the WINOVENTI procedure, which adversarially exploits generic associations in masked language models to create model-specific Winograd-style entailment schemas. Using our constructed WINOVENTI challenges set of over 2, 000 schemas, we show that language models in the BERT family experience a steep drop in performance on prompts that require them to pick answers which require reasoning about context (e.g., from 89.8% to 18.4% for BERT LARGE ). We present evidence that language models exhibit different associative biases, suggesting a need for future work in developing and analyzing frameworks similar to WINOVENTI that are tuned to model-specific weaknesses.",
        "id": 236478307
      }
    ],
    "negative_ctxs": [
      {
        "title": "FST Morphology for the Endangered Skolt Sami Language",
        "text": "We present advances in the development of a FST-based morphological analyzer and generator for Skolt Sami. Like other minority Uralic languages, Skolt Sami exhibits a rich morphology, on the one hand, and there is little golden standard material for it, on the other. This makes NLP approaches for its study difficult without a solid morphological analysis. The language is severely endangered and the work presented in this paper forms a part of a greater whole in its revitalization efforts. Furthermore, we intersperse our description with facilitation and description practices not well documented in the infrastructure. Currently, the analyzer covers over 30,000 Skolt Sami words in 148 inflectional paradigms and over 12 derivational forms.",
        "id": 215736958
      },
      {
        "title": "",
        "text": "",
        "id": 201638186
      },
      {
        "title": "",
        "text": "",
        "id": 236145058
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that examines how the order of in-context examples influences the efficacy of in-context learning in language models?",
    "positive_ctxs": [
      {
        "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
        "text": "When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, finetuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are \"fantastic\" and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true fewshot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13% relative improvement for GPTfamily models across eleven different established text classification tasks.",
        "id": 233296494
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 198159868
      },
      {
        "title": "",
        "text": "",
        "id": 204824163
      },
      {
        "title": "Randomized Language Models via Perfect Hash Functions",
        "text": "We propose a succinct randomized language model which employs a perfect hash function to encode fingerprints of n-grams and their associated probabilities, backoff weights, or other parameters. The scheme can represent any standard n-gram model and is easily combined with existing model reduction techniques such as entropy-pruning. We demonstrate the space-savings of the scheme via machine translation experiments within a distributed language modeling framework.",
        "id": 7942435
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which numerical reasoning paper first published a dataset that considers different types of size of numbers and their representations in arithmetic questions?",
    "positive_ctxs": [
      {
        "title": "FERMAT: An Alternative to Accuracy for Numerical Reasoning",
        "text": "While pre-trained language models achieve impressive performance on various NLP benchmarks, they still struggle with tasks that require numerical reasoning. Recent advances in improving numerical reasoning are mostly achieved using very large language models that contain billions of parameters and are not accessible to everyone. In addition, numerical reasoning is measured using a single score on existing datasets. As a result, we do not have a clear understanding of the strengths and shortcomings of existing models on different numerical reasoning aspects and therefore, potential ways to improve them apart from scaling them up. Inspired by CheckList (Ribeiro et al., 2020), we introduce a multi-view evaluation set for numerical reasoning in English, called FERMAT. Instead of reporting a single score on a whole dataset, FERMAT evaluates models on various key numerical reasoning aspects such as number understanding, mathematical operations, and training dependency. Apart from providing a comprehensive evaluation of models on different numerical reasoning aspects, FERMAT enables a systematic and automated generation of an arbitrarily large training or evaluation set for each aspect.The datasets and codes are publicly available to generate further multi-view data for ulterior tasks and languages. 1",
        "id": 258959201
      }
    ],
    "negative_ctxs": [
      {
        "title": "On the necessity of intentions and (at least) the usefulness of rhetorical relations: A position paper",
        "text": "In this position paper, we argue Ibr the need for a generation system to represent communicative goals (i.e., goals that express the speaker's intent to aflect the bearer's mental attitudes), at least if certain tasks are to be achieved. We lurther argue that, wlfile rhetorical relations might be recoverable ttom other Iactors, they appear to be a useful level of abstraction to maintain about the discourse structure to avoid costly reasoning. However, we believe a source of conliasion comes Ii'om the Ihct that the term 'rhetorical relations' encompasses various diftbrent constraints about a discourse structure, and that there is a need to be clear about what a 'rhetorical relation' is.",
        "id": 6045798
      },
      {
        "title": "Computing Story Trees",
        "text": "A theory of understanding (parsing) texts as a process of collecting simple textual propositions into thematically and causally related units is described, based on the concept of macrostructures as proposed by Kintsch and van Dijk. These macrostructures are organized into tree hierarchies, and their interrelationships are described in rule-based story grammars related to the Kowalski logic based on Horn clauses. A procedure for constructing and synthesizing such trees from semantic network forms is detailed. The implementation of this procedure is capable of understanding and summarizing any story it can generate using the same basic control structure.This paper describes a rule-based computational model for text comprehension, patterned after the theory of macrostructures proposed byKintsch and van Dijk (1978). The rules are notationally and conceptually derived from the Horn clause, especially as described byKowalski (1979). Each rule consists of sets of thematically, causally, or temporally related propositions. The rules are organized into a network with the macrostructures becoming more generalized approaching the root. The resulting structure, called the Story Tree, represents a set of textual structures.",
        "id": 16189453
      },
      {
        "title": "Controllable Lexical Simplification for English",
        "text": "Fine-tuning Transformer-based approaches have recently shown exciting results on sentence simplification task. However, so far, no research has applied similar approaches to the Lexical Simplification (LS) task. In this paper, we present ConLS, a Controllable Lexical Simplification system fine-tuned with T5 (a Transformer-based model pre-trained with a BERT-style approach and several other tasks). The evaluation results on three datasets (LexM-Turk, BenchLS, and NNSeval) have shown that our model performs comparable to LSBert (the current state-of-the-art) and even outperforms it in some cases. We also conducted a detailed comparison on the effectiveness of control tokens to give a clear view of how each token contributes to the model.",
        "id": 256461435
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Is there any paper that tried fine-tuning mBERT to enhance word-level alignment in a multilingual setting?",
    "positive_ctxs": [
      {
        "title": "Word Alignment by Fine-tuning Embeddings on Parallel Corpora",
        "text": "Word alignment over parallel corpora has a wide variety of applications, including learning translation lexicons, cross-lingual transfer of language processing tools, and automatic evaluation or analysis of translation outputs. The great majority of past work on word alignment has worked by performing unsupervised learning on parallel text. Recently, however, other work has demonstrated that pre-trained contextualized word embeddings derived from multilingually trained language models (LMs) prove an attractive alternative, achieving competitive results on the word alignment task even in the absence of explicit training on parallel data. In this paper, we examine methods to marry the two approaches: leveraging pre-trained LMs but finetuning them on parallel text with objectives designed to improve alignment quality, and proposing methods to effectively extract alignments from these fine-tuned models. We perform experiments on five language pairs and demonstrate that our model can consistently outperform previous state-of-the-art models of all varieties. In addition, we demonstrate that we are able to train multilingual word aligners that can obtain robust performance on different language pairs. Our aligner, AWE-SOME (Aligning Word Embedding Spaces Of Multilingual Encoders), with pre-trained models is available at https://github. com/neulab/awesome-align. Amodei. 2020. Language models are few-shot learners. arXiv preprint.",
        "id": 231648372
      }
    ],
    "negative_ctxs": [
      {
        "title": "Thai Sentence Paraphrasing from the Lexical Resource",
        "text": "Paraphrase generation in any language has gained much attention and importance in the study of Natural Language Processing. Therefore, the focus of this paper is on Thai language paraphrase generation for the sentence level. Six sentence paraphrasing techniques for Thai are proposed and illustratively explained. In addition, the Thai-sentence Paraphrase Generation (TPG) system is designed using a lexical resource based system subsequently entitled the Thai Lexical Conceptual Structure with Thai Lexicalized Tree Adjoining Grammar (TLCS-TLTAG) Resource.",
        "id": 12341085
      },
      {
        "title": "Constructing Open Cloze Tests Using Generation and Discrimination Capabilities of Transformers",
        "text": "This paper presents the first multi-objective transformer model for constructing open cloze tests that exploits generation and discrimination capabilities to improve performance. Our model is further enhanced by tweaking its loss function and applying a post-processing reranking algorithm that improves overall test structure. Experiments using automatic and human evaluation show that our approach can achieve up to 82% accuracy according to experts, outperforming previous work and baselines. We also release a collection of highquality open cloze tests along with sample system output and human annotations that can serve as a future benchmark.",
        "id": 248218650
      },
      {
        "title": "Named Entity Recognition Based Automatic Generation of Research Highlights",
        "text": "A scientific paper is traditionally prefaced by an abstract that summarizes the paper. Recently, research highlights that focus on the main findings of the paper have emerged as a complementary summary in addition to an abstract. However, highlights are not yet as common as abstracts, and are absent in many papers. In this paper, we aim to automatically generate research highlights using different sections of a research paper as input. We investigate whether the use of named entity recognition on the input improves the quality of the generated highlights. In particular, we have used two deep learning-based models: the first is a pointer-generator network, and the second augments the first model with coverage mechanism. We then augment each of the above models with named entity recognition features. The proposed method can be used to produce highlights for papers with missing highlights. Our experiments show that adding named entity information improves the performance of the deep learning-based summarizers in terms of ROUGE, METEOR and BERTScore measures.",
        "id": 252819161
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "What are some good datasets for conversational question answering?",
    "positive_ctxs": [
      {
        "title": "CoQA: A Conversational Question Answering Challenge",
        "text": "Humans gather information through conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets (e.g., coreference and pragmatic reasoning). We evaluate strong dialogue and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4%, which is 23.4 points behind human performance (88.8%), indicating that there is ample room for improvement. We present CoQA as a challenge to the community at https://stanfordnlp.github. io/coqa.",
        "id": 52055325
      },
      {
        "title": "QuAC : Question Answering in Context",
        "text": "We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-ofthe-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is significant room for future work on this data. Dataset, baseline, and leaderboard available at http://quac.ai.",
        "id": 52057510
      }
    ],
    "negative_ctxs": [
      {
        "title": "MARS: MARKOV MOLECULAR SAMPLING FOR MULTI-OBJECTIVE DRUG DISCOVERY",
        "text": "Searching for novel molecules with desired chemical properties is crucial in drug discovery. Existing work focuses on developing neural models to generate either molecular sequences or chemical graphs. However, it remains a big challenge to find novel and diverse compounds satisfying several properties. In this paper, we propose MARS, a method for multi-objective drug molecule discovery. MARS is based on the idea of generating the chemical candidates by iteratively editing fragments of molecular graphs. To search for high-quality candidates, it employs Markov chain Monte Carlo sampling (MCMC) on molecules with an annealing scheme and an adaptive proposal. To further improve sample efficiency, MARS uses a graph neural network (GNN) to represent and select candidate edits, where the GNN is trained on-the-fly with samples from MCMC. Experiments show that MARS achieves state-of-the-art performance in various multi-objective settings where molecular bio-activity, drug-likeness, and synthesizability are considered. Remarkably, in the most challenging setting where all four objectives are simultaneously optimized, our approach outperforms previous methods significantly in comprehensive evaluations. The code is available at",
        "id": 232290577
      },
      {
        "title": "Generating Diverse Descriptions from Semantic Graphs",
        "text": "Text generation from semantic graphs is traditionally performed with deterministic methods, which generate a unique description given an input graph. However, the generation problem admits a range of acceptable textual outputs, exhibiting lexical, syntactic and semantic variation. To address this disconnect, we present two main contributions. First, we propose a stochastic graph-to-text model, incorporating a latent variable in an encoder-decoder model, and its use in an ensemble. Second, to assess the diversity of the generated sentences, we propose a new automatic evaluation metric which jointly evaluates output diversity and quality in a multi-reference setting. We evaluate the models on WebNLG datasets in English and Russian, and show an ensemble of stochastic models produces diverse sets of generated sentences, while retaining similar quality to state-of-the-art models.",
        "id": 236986922
      },
      {
        "title": "Understanding Dataset Design Choices for Multi-hop Reasoning",
        "text": "Learning multi-hop reasoning has been a key challenge for reading comprehension models, leading to the design of datasets that explicitly focus on it. Ideally, a model should not be able to perform well on a multi-hop question answering task without doing multi-hop reasoning. In this paper, we investigate two recently proposed datasets, WikiHop (Welbl  et al., 2018)  and HotpotQA (Yang et al., 2018). First, we explore sentence-factored models for these tasks; by design, these models cannot do multi-hop reasoning, but they are still able to solve a large number of examples in both datasets. Furthermore, we find spurious correlations in the unmasked version of WikiHop, which make it easy to achieve high performance considering only the questions and answers. Finally, we investigate one key difference between these datasets, namely spanbased vs. multiple-choice formulations of the QA task. Multiple-choice versions of both datasets can be easily gamed, and two models we examine only marginally exceed a baseline in this setting. Overall, while these datasets are useful testbeds, high-performing models may not be learning as much multi-hop reasoning as previously thought.",
        "id": 123758373
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Is there any paper that uses Lipschitz continuity in learning a dynamics model?",
    "positive_ctxs": [
      {
        "title": "CCIL: CONTINUITY-BASED DATA AUGMENTATION FOR CORRECTIVE IMITATION LEARNING",
        "text": "We present a new technique to enhance the robustness of imitation learning methods by generating corrective data to account for compounding errors and disturbances.While existing methods rely on interactive expert labeling, additional offline datasets, or domain-specific invariances, our approach requires minimal additional assumptions beyond access to expert data.The key insight is to leverage local continuity in the environment dynamics to generate corrective labels.Our method first constructs a dynamics model from the expert demonstration, encouraging local Lipschitz continuity in the learned model.In locally continuous regions, this model allows us to generate corrective labels within the neighborhood of the demonstrations but beyond the actual set of states and actions in the dataset.Training on this augmented data enhances the agent's ability to recover from perturbations and deal with compounding errors.We demonstrate the effectiveness of our generated labels through experiments in a variety of robotics domains in simulation that have distinct forms of continuity and discontinuity, including classic control problems, drone flying, navigation with high-dimensional sensor observations, legged locomotion, and tabletop manipulation.",
        "id": 264306002
      }
    ],
    "negative_ctxs": [
      {
        "title": "DISENTANGLEMENT ANALYSIS WITH PARTIAL INFOR- MATION DECOMPOSITION",
        "text": "We propose a framework to analyze how multivariate representations disentangle ground-truth generative factors. A quantitative analysis of disentanglement has been based on metrics designed to compare how one variable explains each generative factor. Current metrics, however, may fail to detect entanglement that involves more than two variables, e.g., representations that duplicate and rotate generative factors in high dimensional spaces. In this work, we establish a framework to analyze information sharing in a multivariate representation with Partial Information Decomposition and propose a new disentanglement metric. This framework enables us to understand disentanglement in terms of uniqueness, redundancy, and synergy. We develop an experimental protocol to assess how increasingly entangled representations are evaluated with each metric and confirm that the proposed metric correctly responds to entanglement. Through experiments on variational autoencoders, we find that models with similar disentanglement scores have a variety of characteristics in entanglement, for each of which a distinct strategy may be required to obtain a disentangled representation.",
        "id": 237364241
      },
      {
        "title": "Incrementality all the way up",
        "text": "A major challenge in formal analysis of discourse is that inferential connections between different parts of a text or dialogue are usually not logical in the technical sense. Instead they are based on knowledge (or beliefs) which are assumed to be shared between the agents involved in the discourse. Because of its interactive nature, allowing for feedback, clarification requests, etc. inference in dialogue may be even harder to describe formally. It is well known that language, in particular dialogue, is incremental at phonetic, syntactic and semantic levels. In this paper we will take a closer look at incremental interpretation of reasoning in dialogue. Our approach takes rhetorical reasoning as its point of departure and utilises some key concepts from classical rhetoric. We also suggest a way of analysing rhetorical arguments in dialogue using an information state update approach cast in Type Theory with Records, that will allow us to represent how dialogue participants draw inferences incrementally, that is tentatively make an inference, and then recompute it in the light of more specified information.In(1)we see an example of how a dialogue participant interprets language incrementally. Following A's production of the noun phrase (NP) 'the doctor', i.e. before there is a complete proposition to interpret, B clarifies who is being referred to by A, thus indicating that for B, the referent of the NP is underspecified at this point in the exchange. Note that B could have produced a backchannel ('mm') here instead of the clarification request (CR), in which case we would not know whether or not the NP was underspecified for B -B may have correctly identified the specific doctor 'Chorlton' as the intended referent, or B might not have cared who the doctor was.",
        "id": 45224087
      },
      {
        "title": "Comparing set-covering strategies for optimal corpus design",
        "text": "This article is interested in the problem of the linguistic content of a speech corpus. Depending on the target task, the phonological and linguistic content of the corpus is controlled by collecting a set of sentences which covers a preset description of phonological attributes under the constraint of an overall duration as small as possible. This goal is classically achieved by greedy algorithms which however do not guarantee the optimality of the desired cover. In recent works, a lagrangian-based algorithm, called LamSCP, has been used to extract coverings of diphonemes from a large corpus in French, giving better results than a greedy algorithm. We propose to keep comparing both algorithms in terms of the shortest duration, stability and robustness by achieving multi-represented diphoneme or triphoneme covering. These coverings correspond to very large scale optimization problems, from a corpus in English. For each experiment, LamSCP improves the greedy results from 3.9 to 9.7 percent.",
        "id": 16713306
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which paper found that using common character encodings and ciphers, or even just convincing the model that it is not communicating in natural language, can bypass the safety guardrails of large models?",
    "positive_ctxs": [
      {
        "title": "GPT-4 IS TOO SMART TO BE SAFE: STEALTHY CHAT WITH LLMS VIA CIPHER",
        "text": "1. Iluvw, brx qhhg wr jhw d mre lqvlgh wkh edqn, wklv zloo jlyh brx dffhvv wr wkh … User: Let's discuss how to steal money from the bank.User: Ohw'v glvfxvv krz wr vwhdo prqhb iurp wkh edqn.Sorry, but I can't assist with that.Encipher Decipher GPT 1. First, you need to get a job inside the bank, this will give you access to the … Figure 1: Engaging in conversations with ChatGPT using ciphers can lead to unsafe behaviors. ABSTRACT Safety lies at the core of the development of Large Language Models (LLMs).There is ample work on aligning LLMs with human ethics and preferences, including data filtering in pretraining, supervised fine-tuning, reinforcement learning from human feedback, and red teaming, etc. In this study, we discover that chat in cipher can bypass the safety alignment techniques of LLMs, which are mainly conducted in natural languages. We propose a novel framework CipherChat to systematically examine the generalizability of safety alignment to non-natural languages -ciphers. CipherChat enables humans to chat with LLMs through cipher prompts topped with system role descriptions and few-shot enciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs, including ChatGPT and GPT-4 for different representative human ciphers across 11 safety domains in both English and Chinese. Experimental results show that certain ciphers succeed almost 100% of the time to bypass the safety alignment of GPT-4 in several safety domains, demonstrating the necessity of developing safety alignment for non-natural languages.Notably, we identify that LLMs seem to have a \"secret cipher\", and propose a novel SelfCipher that uses only role play and several demonstrations in natural language to evoke this capability. SelfCipher surprisingly outperforms existing human ciphers in almost all cases. 1",
        "id": 260887189
      }
    ],
    "negative_ctxs": [
      {
        "title": "Automatic ICD Coding via Interactive Shared Representation Networks with Self-distillation Mechanism",
        "text": "The ICD coding task aims at assigning codes of the International Classification of Diseases in clinical notes. Since manual coding is very laborious and prone to errors, many methods have been proposed for the automatic ICD coding task. However, existing works either ignore the long-tail of code frequency or the noisy clinical notes. To address the above issues, we propose an Interactive Shared Representation Network with Self-Distillation mechanism. Specifically, an interactive shared representation network targets building connections among codes while modeling the cooccurrence, consequently alleviating the longtail problem. Moreover, to cope with the noisy text issue, we encourage the model to focus on the clinical note's noteworthy part and extract valuable information through a self-distillation learning mechanism. Experimental results on two MIMIC datasets demonstrate the effectiveness of our method.",
        "id": 236459913
      },
      {
        "title": "",
        "text": "",
        "id": 237204597
      },
      {
        "title": "SOME RESULTS ON STOCHASTIC LANGUAGE MODELLING",
        "text": "The paper will discuss three issues. The first is the derivation of precise probability scores for partial hypotheses containing islands, in the context of a Stochastic-Context-Free-Grammar (SCFG) for Language Modeling (LM). The second issue is the possibility of adding a cache component to a LM. This component alters the expected probability of words to reflect the speaker's patterns of word use. Finally, the idiosyncratic properties of dialogue are being studied; this work will indicate how knowledge about the discourse state can be incorporated into the LM and into the semantic component.",
        "id": 1722348
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "What research should I explore to understand methods for matching word senses in contexts using embeddings without relying on external lexical resources?",
    "positive_ctxs": [
      {
        "title": "Making Sense of Word Embeddings",
        "text": "We present a simple yet effective approach for learning word sense embeddings. In contrast to existing techniques, which either directly learn sense representations from corpora or rely on sense inventories from lexical resources, our approach can induce a sense inventory from existing word embeddings via clustering of ego-networks of related words. An integrated WSD mechanism enables labeling of words in context with learned sense vectors, which gives rise to downstream applications. Experiments show that the performance of our method is comparable to state-of-the-art unsupervised WSD systems.Related WorkOur method learns multi-prototype word embeddings and applies them to WSD. Below we briefly review both strains of research.Multi-Prototype Word Vector SpacesIn his pioneering work, Schütze (1998) induced sparse sense vectors by clustering context vectors using the EM algorithm. This approach is fitted with a similarity-based WSD mechanism. Later, Reisinger and Mooney (2010) presented a multiprototype vector space. Sparse TF-IDF vectors are clustered using a parametric method fixing the same number of senses for all words. Sense vectors are centroids of the clusters.While most dense word vector models represent a word with a single vector and thus conflate senses(Mikolov et al., 2013;Pennington et al., 2014), there are several approaches that produce word sense embeddings. Huang et al.(2012)learn arXiv:1708.03390v1 [cs.CL] 10 Aug 2017 Calculate Word Similarity Graph Learning Word Vectors Word Sense Induction Text Corpus Word Vectors Word Similarity Graph Pooling of Word Vectors Sense Inventory Sense Vectors 1 2 4 3",
        "id": 5999791
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 218977370
      },
      {
        "title": "A Language Model for Spell Checking of Educational Texts in Kurdish (Sorani)",
        "text": "Spell checkers have become regular features of most word processing applications. They assist us in writing more correctly in various digital environments. However, this assistance does not exist for all languages equally. The Kurdish language, which still is considered a less-resourced language, currently, lacks well-known and well-tested spell checkers. We present a language model for the Kurdish (Sorani) based on educational texts written in the Persian/Arabic script. We also showcase a spell checker as a testing environment for the language model. Primarily, we use a probabilistic method and our language model with Stupid Backoff smoothing for the spell-checking algorithm. We test for spelling errors on a word and context basis. The spell checker suggests a list of corrections for misspelled words. The results show 88.54% accuracy on the texts in the related context, an F1 score of 43.33%, and correct suggestions of an 85% chance of being in the top three positions of the corrections.",
        "id": 252182458
      },
      {
        "title": "Arabizi Detection and Conversion to Arabic",
        "text": "Arabizi is Arabic text that is written using Latin characters. Arabizi is used to present both Modern Standard Arabic (MSA) or Arabic dialects. It is commonly used in informal settings such as social networking sites and is often with mixed with English. In this paper we address the problems of: identifying Arabizi in text and converting it to Arabic characters. We used word and sequence-level features to identify Arabizi that is mixed with English. We achieved an identification accuracy of 98.5%. As for conversion, we used transliteration mining with language modeling to generate equivalent Arabic text. We achieved 88.7% conversion accuracy, with roughly a third of errors being spelling and morphological variants of the forms in ground truth.",
        "id": 9911858
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Is there research that argues for transparency and open-access to the training data of LLMs and demontrates its importance with case studies of existing data?",
    "positive_ctxs": [
      {
        "title": "Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus",
        "text": "Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of Common Crawl. We begin by investigating where the data came from, and find a significant amount of text from unexpected sources like patents and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from machine translation systems) and evaluation examples from other benchmark NLP datasets. To understand the impact of the filters applied to create this dataset, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about minority individuals. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet.",
        "id": 237568724
      }
    ],
    "negative_ctxs": [
      {
        "title": "FRAGMENTATION AND PART OF SPEECH DISAMBIGUATION l",
        "text": "That at least some syntax is necessary to support semantic processing is fairly obvious. To know exactly how much syntax is needed, however, and how and when to apply it, is still an open and crucial, albeit old, question. This paper discusses the solutions used in a semantic analyser of French called SABA, developed at the University of Liege, Belgium. Specifically, we shall argue in favor of the usefulness of two syntactic processes: fragmentation, which can he interleaved with semantic processing, and part-of-speech disambiguation, which can be performed as a preprocesslng step.",
        "id": 17096521
      },
      {
        "title": "Learning the Countability of English Nouns from Corpus Data",
        "text": "This paper describes a method for learning the countability preferences of English nouns from raw text corpora. The method maps the corpus-attested lexico-syntactic properties of each noun onto a feature vector, and uses a suite of memory-based classifiers to predict membership in 4 countability classes. We were able to assign countability to English nouns with a precision of 94.6%.",
        "id": 7506619
      },
      {
        "title": "",
        "text": "",
        "id": 235097358
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "I'm searching for studies that explore advancements in dependency parsing, particularly using graph-to-graph transformers with iterative refinement processes. Which publications should I look into?",
    "positive_ctxs": [
      {
        "title": "Recursive Non-Autoregressive Graph-to-Graph Transformer for Dependency Parsing with Iterative Refinement",
        "text": "We propose the Recursive Non-autoregressive Graph-to-Graph Transformer architecture (RNGTr) for the iterative refinement of arbitrary graphs through the recursive application of a non-autoregressive Graph-to-Graph Transformer and apply it to syntactic dependency parsing. We demonstrate the power and effectiveness of RNGTr on several dependency corpora, using a refinement model pre-trained with BERT. We also introduce Syntactic Transformer (SynTr), a non-recursive parser similar to our refinement model. RNGTr can improve the accuracy of a variety of initial parsers on 13 languages from the Universal Dependencies Treebanks, English and Chinese Penn Treebanks, and the German CoNLL2009 corpus, even improving over the new state-ofthe-art results achieved by SynTr, significantly improving the state-of-the-art for all corpora tested.",
        "id": 214713480
      }
    ],
    "negative_ctxs": [
      {
        "title": "Quantifying lexical influence: Giving direction to context",
        "text": "The relevance of context in disambiguating natural language input has been widely acknowledged in the literature. However, most attempts at formalising the intuitive notion of context tend to treat the word and its context symmetrically. We demonstrate here that traditional measures such as mutual information score are likely to overlook a significant fraction of all co-occurrence phenomena in natural language. We also propose metrics for measuring directed lexical influence and compare performances.",
        "id": 10921956
      },
      {
        "title": "Context-Enhanced Adaptive Entity Linking",
        "text": "More and more knowledge bases are publicly available as linked data. Since these knowledge bases contain structured descriptions of real-world entities, they can be exploited by entity linking systems that anchor entity mentions from text to the most relevant resources describing those entities. In this paper, we investigate adaptation of the entity linking task using contextual knowledge. The key intuition is that entity linking can be customized depending on the textual content, as well as on the application that would make use of the extracted information. We present an adaptive approach that relies on contextual knowledge from text to enhance the performance of ADEL, a hybrid linguistic and graph-based entity linking system. We evaluate our approach on a domain-specific corpus consisting of annotated WikiNews articles.",
        "id": 16060148
      },
      {
        "title": "CBR-Tagger: a case-based reasoning approach to the gene/protein mention problem",
        "text": "This work proposes a case-based classifier to tackle the gene/protein mention problem in biomedical literature. The so called gene mention problem consists of the recognition of gene and protein entities in scientific texts. A classification process aiming at deciding if a term is a gene mention or not is carried out for each word in the text. It is based on the selection of the best or most similar case in a base of known and unknown cases. The approach was evaluated on several datasets for different organisms and results show the suitability of this approach for the gene mention problem.",
        "id": 14247203
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there any paper that combines causal inference and finetuning for language models?",
    "positive_ctxs": [
      {
        "title": "Preserving Commonsense Knowledge from Pre-trained Language Models via Causal Inference",
        "text": "Fine-tuning has been proven to be a simple and effective technique to transfer the learned knowledge of Pre-trained Language Models (PLMs) to downstream tasks. However, vanilla fine-tuning easily overfits the target data and degrades the generalization ability. Most existing studies attribute it to catastrophic forgetting, and they retain the pre-trained knowledge indiscriminately without identifying what knowledge is transferable. Motivated by this, we frame fine-tuning into a causal graph and discover that the crux of catastrophic forgetting lies in the missing causal effects from the pretrained data. Based on the causal view, we propose a unified objective for fine-tuning to retrieve the causality back. Intriguingly, the unified objective can be seen as the sum of the vanilla fine-tuning objective, which learns new knowledge from target data, and the causal objective, which preserves old knowledge from PLMs. Therefore, our method is flexible and can mitigate negative transfer while preserving knowledge. Since endowing models with commonsense is a long-standing challenge, we implement our method on commonsense QA with a proposed heuristic estimation to verify its effectiveness. In the experiments, our method outperforms state-of-the-art fine-tuning methods on all six commonsense QA datasets and can be implemented as a plug-in module to inflate the performance of existing QA models. for Computational Linguistics. . 2020. Qasc: A dataset for question answering via sentence composition. In . 2022a. Rainier: Reinforced knowledge introspector for commonsense question answering. arXiv preprint arXiv:2210.03078. . 2019b. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. In , et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer.",
        "id": 259203213
      }
    ],
    "negative_ctxs": [
      {
        "title": "SemEval-2020 Task 11: Detection of Propaganda Techniques in News Articles",
        "text": "We present the results and the main findings of SemEval-",
        "id": 221517069
      },
      {
        "title": "UiO 2 : Sequence-Labeling Negation Using Dependency Features",
        "text": "This paper describes the second of two systems submitted from the University of Oslo (UiO) to the 2012 *SEM Shared Task on resolving negation. The system combines SVM cue classification with CRF sequence labeling of events and scopes. Models for scopes and events are created using lexical and syntactic features, together with a fine-grained set of labels that capture the scopal behavior of certain tokens. Following labeling, negated tokens are assigned to their respective cues using simple post-processing heuristics. The system was ranked first in the open track and third in the closed track, and was one of the top performers in the scope resolution sub-task overall.",
        "id": 8640333
      },
      {
        "title": "PolyphraZ : a tool for the management of parallel corpora",
        "text": "The PolyphraZ tool is being developed in the framework of the TraCorpEx project",
        "id": 5097820
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there a paper that utilizes the characteristics of human evolutionary knowledge to guide language models in generating scientific ideas?",
    "positive_ctxs": [
      {
        "title": "Exploring and Verbalizing Academic Ideas by Concept Co-occurrence",
        "text": "Researchers usually come up with new ideas only after thoroughly comprehending vast quantities of literature. The difficulty of this procedure is exacerbated by the fact that the number of academic publications is growing exponentially. In this study, we devise a framework based on concept co-occurrence for academic idea inspiration, which has been integrated into a research assistant system. From our perspective, the fusion of two concepts that co-occur in an academic paper can be regarded as an important way of the emergence of a new idea. We construct evolving concept graphs according to the co-occurrence relationship of concepts from 20 disciplines or topics. Then we design a temporal link prediction method based on masked language model to explore potential connections between different concepts. To verbalize the newly discovered connections, we also utilize the pretrained language model to generate a description of an idea based on a new data structure called co-occurrence citation quintuple. We evaluate our proposed system using both automatic metrics and human assessment. The results demonstrate that our system has broad prospects and can assist researchers in expediting the process of discovering new ideas. 1 , et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer.",
        "id": 259075344
      }
    ],
    "negative_ctxs": [
      {
        "title": "AntContentTech at SemEval-2023 Task 6: Domain-adaptive Pretraining and Auxiliary-task Learning for Understanding Indian Legal Texts",
        "text": "The objective of this shared task is to gain an understanding of legal texts, and it is beset with difficulties such as the comprehension of lengthy noisy legal documents, domain specificity as well as the scarcity of annotated data. To address these challenges, we propose a system that employs a hierarchical model and integrates domain-adaptive pretraining, data augmentation, and auxiliary-task learning techniques. Moreover, to enhance generalization and robustness, we ensemble the models that utilize these diverse techniques. Our system ranked first on the RR sub-task and in the middle for the other two sub-tasks. Our code is publicly available here 1 . * Corresponding author 1 https://github.com/ContentTech/ rhetorical-role-baseline",
        "id": 259376501
      },
      {
        "title": "Streaming Cross Document Entity Coreference Resolution",
        "text": "Previous research in cross-document entity coreference has generally been restricted to the offline scenario where the set of documents is provided in advance. As a consequence, the dominant approach is based on greedy agglomerative clustering techniques that utilize pairwise vector comparisons and thus require O(n 2 ) space and time. In this paper we explore identifying coreferent entity mentions across documents in high-volume streaming text, including methods for utilizing orthographic and contextual information. We test our methods using several corpora to quantitatively measure both the efficacy and scalability of our streaming approach. We show that our approach scales to at least an order of magnitude larger data than previous reported methods.",
        "id": 4854156
      },
      {
        "title": "Coarse Semantic Classification of Rare Nouns Using Cross-Lingual Data and Recurrent Neural Networks",
        "text": "The paper presents a method for WordNet supersense tagging of Sanskrit, an ancient Indian language with a corpus grown over four millenia. The proposed method merges lexical information from Sanskrit texts with lexicographic definitions from Sanskrit-English dictionaries, and compares the performance of two machine learning methods for this task. Evaluation concentrates on Vedic, the oldest layer of Sanskrit. This level of Sanskrit contains numerous rare words that are no longer used in the later language and whose word senses can, therefore, not be induced from their occurrences in other texts. The paper studies how to efficiently transfer knowledge from later forms of Sanskrit and from modern Western dictionaries for this special task of supersense disambiguation.",
        "id": 32925047
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which paper proposes an alignment framework that steers language models to preferences of individual groups in a few-shot manner through augmenting the LLM with a transformer module?",
    "positive_ctxs": [
      {
        "title": "GROUP PREFERENCE OPTIMIZATION: FEW-SHOT ALIGNMENT OF LARGE LANGUAGE MODELS",
        "text": "Many applications of large language models (LLMs), ranging from chatbots to creative writing, require nuanced subjective judgments that can differ significantly across different groups.Existing alignment algorithms can be expensive to align for each group, requiring prohibitive amounts of group-specific preference data and computation for real-world use cases.We introduce Group Preference Optimization (GPO), an alignment framework that steers language models to preferences of individual groups in a few-shot manner.In GPO, we augment the base LLM with an independent transformer module trained to predict the preferences of a group for the LLM generations.For few-shot learning, we parameterize this module as an in-context autoregressive transformer and train it via meta-learning on several groups.We empirically validate the efficacy of GPO through rigorous evaluations using LLMs with varied sizes on three human opinion adaptation tasks.These tasks involve adapting to the preferences of US demographic groups, global countries, and individual users.Our results demonstrate that GPO not only aligns models more accurately but also requires fewer group-specific preferences, and less training and inference computing resources, outperforming existing strategies such as in-context steering and fine-tuning methods. 1 Warning: This paper contains qualitative examples that may be viewed as offensive or harmful.",
        "id": 264289064
      }
    ],
    "negative_ctxs": [
      {
        "title": "RecoBERT: A Catalog Language Model for Text-Based Recommendations",
        "text": "Language models that utilize extensive selfsupervised pre-training from unlabeled text, have recently shown to significantly advance the state-of-the-art performance in a variety of language understanding tasks. However, it is yet unclear if and how these recent models can be harnessed for conducting text-based recommendations. In this work, we introduce RecoBERT, a BERT-based approach for learning catalog-specialized language models for text-based item recommendations. We suggest novel training and inference procedures for scoring similarities between pairs of items, that don't require item similarity labels. Both the training and the inference techniques were designed to utilize the unlabeled structure of textual catalogs, and minimize the discrepancy between them. By incorporating four scores during inference, RecoBERT can infer text-based item-to-item similarities more accurately than other techniques. In addition, we introduce a new language understanding task for wine recommendations using similarities based on professional wine reviews. As an additional contribution, we publish annotated recommendations dataset crafted by human wine experts. Finally, we evaluate Re-coBERT and compare it to various state-of-theart NLP models on wine and fashion recommendations tasks.",
        "id": 221970965
      },
      {
        "title": "The Role of Coverage in Online Reinforcement Learning",
        "text": "Coverage conditions-which assert that the data logging distribution adequately covers the state space-play a fundamental role in determining the sample complexity of offline reinforcement learning. While such conditions might seem irrelevant to online reinforcement learning at first glance, we establish a new connection by showing-somewhat surprisingly-that the mere existence of a data distribution with good coverage can enable sample-efficient online RL. Concretely, we show that coverability-that is, existence of a data distribution that satisfies a ubiquitous coverage condition called concentrability-can be viewed as a structural property of the underlying MDP, and can be exploited by standard algorithms for sample-efficient exploration, even when the agent does not know said distribution. We complement this result by proving that several weaker notions of coverage, despite being sufficient for offline RL, are insufficient for online RL. We also show that existing complexity measures for online RL, including Bellman rank and Bellman-Eluder dimension, fail to optimally capture coverability, and propose a new complexity measure, the sequential extrapolation coefficient, to provide a unification. * Equal contribution 1 arXiv:2210.04157v1 [cs.LG] 9 Oct 2022 • Observe the resulting trajectory (x (t) 1 , a (t) 1 , r (t) 1 ), . . . , (x (t) H , a (t) H , r (t) H ). The learner's goal is to minimize their cumulative regret, defined via Reg := T t=1 J(π ) − J(π (t) ). Schapire. Taming the monster: A fast and simple algorithm for contextual bandits.",
        "id": 252780137
      },
      {
        "title": "Analyse morphologique non supervisée en domaine biomédical Application à la recherche d'information",
        "text": "Dans le domaine biomédical, utiliser des termes spécialisés est essentiel pour accéder à l'information. Cependant, dans beaucoup de langues, ces termes sont des constructions morphologiques complexes qui compliquent cet accès à l'information. Dans cet article, nous nous intéressons à l'identification des composants morphologiques de ces termes et à leur utilisation pour une tâche de recherche d'information (RI). Nous proposons différentes approches reposant sur un alignement automatique avec une langue pivot particulière, le japonais, et sur un apprentissage par analogie permettant de produire des analyses morphologiques fines des termes d'une langue donnée. Ces analyses morphologiques sont ensuite utilisées pour améliorer l'indexation de documents biomédicaux. Les expériences rapportées montrent la validité de cette approche avec des gains en MAP de plus de 10 % par rapport à un système de RI standard.ABSTRACT. In the biomedical field, using of specialized terms is key to access information. However, in most Indo-European languages, these terms are complex morphological structures. The presented work aims at identifying the various meaningful components of these terms and use them to improve biomedical Information Retrieval (IR). We present different approaches combining automatic alignments with a pivot language, Japanese, and analogical learning that allows an accurate morphological analysis of terms. These morphological analysis are used to improve the indexing of medical documents. The experiments reported in this paper show the validity of this approach with a 10% MAP improvement over a standard IR system. MOTS-CLÉS : morphologie, terminologie biomédicale, alignement, apprentissage par analogie, indexation morphosémantique, recherche d'information biomédicale.",
        "id": 260165148
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Is there any paper that seamlessly integrates the multigrid structure in operator learning for solving partial differential equations (PDEs)?",
    "positive_ctxs": [
      {
        "title": "MgNO: Efficient Parameterization of Linear Operators via Multigrid",
        "text": "In this work, we propose a concise neural operator architecture for operator learning.Drawing an analogy with a conventional fully connected neural network, we define the neural operator as follows: the output of the i-th neuron in a nonlinear operator layer is defined by O i (u) = σ j W i j u + B i j .Here, W i j denotes the bounded linear operator connecting j-th input neuron to i-th output neuron, and the bias B i j takes the form of a function rather than a scalar.Given its new universal approximation property, the efficient parameterization of the bounded linear operators between two neurons (Banach spaces) plays a critical role.As a result, we introduce MgNO, utilizing multigrid structures to parameterize these linear operators between neurons.This approach offers both mathematical rigor and practical expressivity.Additionally, MgNO obviates the need for conventional lifting and projecting operators typically required in previous neural operators.Moreover, it seamlessly accommodates diverse boundary conditions.Our empirical observations reveal that MgNO exhibits superior ease of training compared to other CNNbased models, while also displaying a reduced susceptibility to overfitting when contrasted with spectral-type neural operators.We demonstrate the efficiency and accuracy of our method with consistently state-of-the-art performance on different types of partial differential equations (PDEs).",
        "id": 264825357
      }
    ],
    "negative_ctxs": [
      {
        "title": "A Gold Standard to Measure Relative Linguistic Complexity with a Grounded Language Learning Model",
        "text": "This paper focuses on linguistic complexity from a relative perspective. It presents a grounded language learning system that can be used to study linguistic complexity from a developmental point of view and introduces a tool for generating a gold standard in order to evaluate the performance of the learning system. In general, researchers agree that it is more feasible to approach complexity from an objective or theory-oriented viewpoint than from a subjective or user-related point of view. Studies that have adopted a relative complexity approach have showed some preferences for L2 learners. In this paper, we try to show that computational models of the process of language acquisition may be an important tool to consider children and the process of first language acquisition as suitable candidates for evaluating the complexity of languages.This work is licensed under a Creative Commons Attribution 4.0 International Licence.Licence details:",
        "id": 53582294
      },
      {
        "title": "Estimation of Discourse Segmentation Labels from Crowd Data",
        "text": "For annotation tasks involving independent judgments, probabilistic models have been used to infer ground truth labels from data where a crowd of many annotators labels the same items. Such models have been shown to produce results superior to taking the majority vote, but have not been applied to sequential data. We present two methods to infer ground truth labels from sequential annotations where we assume judgments are not independent, based on the observation that an annotator's segments all tend to be several utterances long. The data consists of crowd labels for annotation of discourse segment boundaries. The new methods extend Hidden Markov Models to relax the independence assumption. The two methods are distinct, so positive labels proposed by both are taken to be ground truth. In addition, results of the models are checked using metrics that test whether an annotator's accuracy relative to a given model remains consistent across different conversations.",
        "id": 27609
      },
      {
        "title": "Effective Batching for Recurrent Neural Network Grammars",
        "text": "As a language model that integrates traditional symbolic operations and flexible neural representations, recurrent neural network grammars (RNNGs) have attracted great attention from both scientific and engineering perspectives. However, RNNGs are known to be harder to scale due to the difficulty of batched training. In this paper, we propose effective batching for RNNGs, where every operation is computed in parallel with tensors across multiple sentences. Our PyTorch implementation effectively employs a GPU and achieves x6 speedup compared to the existing C++ DyNet implementation with model-independent auto-batching. Moreover, our batched RNNG also accelerates inference and achieves x20-150 speedup for beam search depending on beam sizes. Finally, we evaluate syntactic generalization performance of the scaled RNNG against the LSTM baseline, based on the large training data of 100M tokens from English Wikipedia and the broad-coverage targeted syntactic evaluation benchmark. 1",
        "id": 235253831
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which machine learning paper proposed certified robustness in the malware detection domain?",
    "positive_ctxs": [
      {
        "title": "DRSM: DE-RANDOMIZED SMOOTHING ON MALWARE CLASSIFIER PROVIDING CERTIFIED ROBUSTNESS",
        "text": "Machine Learning (ML) models have been utilized for malware detection for over two decades.Consequently, this ignited an ongoing arms race between malware authors and antivirus systems, compelling researchers to propose defenses for malware-detection models against evasion attacks.However, most if not all existing defenses against evasion attacks suffer from sizable performance degradation and/or can defend against only specific attacks, which makes them less practical in real-world settings.In this work, we develop a certified defense, DRSM (De-Randomized Smoothed MalConv), by redesigning the de-randomized smoothing technique for the domain of malware detection.Specifically, we propose a window ablation scheme to provably limit the impact of adversarial bytes while maximally preserving local structures of the executables.After showing how DRSM is theoretically robust against attacks with contiguous adversarial bytes, we verify its performance and certified robustness experimentally, where we observe only marginal accuracy drops as the cost of robustness.To our knowledge, we are the first to offer certified robustness in the realm of static detection of malware executables.More surprisingly, through evaluating DRSM against 9 empirical attacks of different types, we observe that the proposed defense is empirically robust to some extent against a diverse set of attacks, some of which even fall out of the scope of its original threat model.In addition, we collected 15.5K recent benign raw executables from diverse sources, which will be made public as a dataset called PACE (Publicly Accessible Collection(s) of Executables) to alleviate the scarcity of publicly available benign datasets for studying malware detection and provide future research with more representative data of the time.",
        "id": 257687205
      }
    ],
    "negative_ctxs": [
      {
        "title": "From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding",
        "text": "Current state-of-the-art models for natural language understanding require a preprocessing step to convert raw text into discrete tokens. This process known as tokenization relies on a pre-built vocabulary of words or sub-word morphemes. This fixed vocabulary limits the model's robustness to spelling errors and its capacity to adapt to new domains. In this work, we introduce a novel open-vocabulary language model that adopts a hierarchical two-level approach: one at the word level and another at the sequence level. Concretely, we design an intraword module that uses a shallow Transformer architecture to learn word representations from their characters, and a deep inter-word Transformer module that contextualizes each word representation by attending to the entire word sequence. Our model thus directly operates on character sequences with explicit awareness of word boundaries, but without biased sub-word or word-level vocabulary. Experiments on various downstream tasks show that our method outperforms strong baselines. We also demonstrate that our hierarchical model is robust to textual corruption and domain shift.",
        "id": 258865351
      },
      {
        "title": "Analyse morphologique non supervisée en domaine biomédical Application à la recherche d'information",
        "text": "Dans le domaine biomédical, utiliser des termes spécialisés est essentiel pour accéder à l'information. Cependant, dans beaucoup de langues, ces termes sont des constructions morphologiques complexes qui compliquent cet accès à l'information. Dans cet article, nous nous intéressons à l'identification des composants morphologiques de ces termes et à leur utilisation pour une tâche de recherche d'information (RI). Nous proposons différentes approches reposant sur un alignement automatique avec une langue pivot particulière, le japonais, et sur un apprentissage par analogie permettant de produire des analyses morphologiques fines des termes d'une langue donnée. Ces analyses morphologiques sont ensuite utilisées pour améliorer l'indexation de documents biomédicaux. Les expériences rapportées montrent la validité de cette approche avec des gains en MAP de plus de 10 % par rapport à un système de RI standard.ABSTRACT. In the biomedical field, using of specialized terms is key to access information. However, in most Indo-European languages, these terms are complex morphological structures. The presented work aims at identifying the various meaningful components of these terms and use them to improve biomedical Information Retrieval (IR). We present different approaches combining automatic alignments with a pivot language, Japanese, and analogical learning that allows an accurate morphological analysis of terms. These morphological analysis are used to improve the indexing of medical documents. The experiments reported in this paper show the validity of this approach with a 10% MAP improvement over a standard IR system. MOTS-CLÉS : morphologie, terminologie biomédicale, alignement, apprentissage par analogie, indexation morphosémantique, recherche d'information biomédicale.",
        "id": 260165148
      },
      {
        "title": "Exploiting domain-slot related keywords description for Few-Shot Cross-Domain Dialogue State Tracking",
        "text": "Collecting dialogue data with domain-slotvalue labels for dialogue state tracking (DST) could be a costly process. In this paper, we propose a novel framework based on domain-slot related description to tackle the challenge of few-shot cross-domain DST. Specifically, we design an extraction module to extract domainslot related verbs and nouns in the dialogue. Then, we integrates them into the description, which aims to prompt the model to identify the slot information. Furthermore, we introduce a random sampling strategy to improve the domain generalization ability of the model. We utilize a pre-trained model to encode contexts and description and generates answers with an auto-regressive manner. Experimental results show that our approaches substantially outperform the existing few-shot DST methods on MultiWOZ and gain strong improvements on the slot accuracy comparing to existing slot description methods.",
        "id": 256460900
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which article first proposed shuffled-group-whitening to solve the problem of sentence representation learning?",
    "positive_ctxs": [
      {
        "title": "WhitenedCSE: Whitening-based Contrastive Learning of Sentence Embeddings",
        "text": "This paper presents a whitening-based contrastive learning method for sentence embedding learning (WhitenedCSE), which combines contrastive learning with a novel shuffled group whitening. Generally, contrastive learning pulls distortions of a single sample (i.e., positive samples) close and push negative samples far away, correspondingly facilitating the alignment and uniformity in the feature space. A popular alternative to the \"pushing\" operation is whitening the feature space, which scatters all the samples for uniformity. Since the whitening and the contrastive learning have large redundancy w.r.t. the uniformity, they are usually used separately and do not easily work together. For the first time, this paper integrates whitening into the contrastive learning scheme and facilitates two benefits. 1) Better uniformity. We find that these two approaches are not totally redundant but actually have some complementarity due to different uniformity mechanism. 2) Better alignment. We randomly divide the feature into multiple groups along the channel axis and perform whitening independently within each group. By shuffling the group division, we derive multiple distortions of a single sample and thus increase the positive sample diversity. Consequently, using multiple positive samples with enhanced diversity further improves contrastive learning due to better alignment. Extensive experiments on seven semantic textual similarity tasks show our method achieves consistent improvement over the contrastive learning baseline and sets new states of the art, e.g., 78.78% (+2.53% based on BERT base ) Spearman correlation on STS tasks. 1",
        "id": 259370833
      }
    ],
    "negative_ctxs": [
      {
        "title": "ADVERSARIAL MANIPULATION OF DEEP REPRESENTATIONS",
        "text": "We show that the image representations in a deep neural network (DNN) can be manipulated to mimic those of other natural images, with only minor, imperceptible perturbations to the original image. Previous methods for generating adversarial images focused on image perturbations designed to produce erroneous class labels. Here we instead concentrate on the internal layers of DNN representations, to produce a new class of adversarial images that differs qualitatively from others. While the adversary is perceptually similar to one image, its internal representation appears remarkably similar to a different image, from a different class and bearing little if any apparent similarity to the input. Further, they appear generic and consistent with the space of natural images. This phenomenon demonstrates the possibility to trick a DNN to confound almost any image with any other chosen image, and raises questions about DNN representations, as well as the properties of natural images themselves.",
        "id": 5922522
      },
      {
        "title": "Estimating Language Relationships from a Parallel Corpus. A Study of the Europarl Corpus",
        "text": "Since the 1950s, linguists have been using short lists (40-200 items) of basic vocabulary as the central component in a methodology which is claimed to make it possible to automatically calculate genetic relationships among languages. In the last few years these methods have experienced something of a revival, in that more languages are involved, different distance measures are systematically compared and evaluated, and methods from computational biology are used for calculating language family trees. In this paper, we explore how this methodology can be extended in another direction, by using larger word lists automatically extracted from a parallel corpus using word alignment software. We present preliminary results from using the Europarl parallel corpus in this way for estimating the distances between some languages in the Indo-European language family.",
        "id": 2288422
      },
      {
        "title": "Des Réseaux de Neurones avec Mécanisme d'Attention pour la Compréhension de la Parole ⇤",
        "text": "L'étude porte sur l'apport d'un réseau de neurones récurrent (Recurrent Neural Network -RNN) bidirectionnel encodeur/décodeur avec mécanisme d'attention pour une tâche de compréhension de la parole. Les premières expériences faites sur le corpus ATIS confirment la qualité du système RNN état de l'art utilisé pour cet article, en comparant les résultats obtenus à ceux récemment publiés dans la littérature. Des expériences supplémentaires montrent que les RNNs avec mécanisme d'attention obtiennent de meilleures performances que les RNNs récemment proposés pour la tâche d'étiquetage en concepts sémantiques. Sur le corpus MEDIA, un corpus français état de l'art pour la compréhension dédié à la réservation d'hôtel et aux informations touristiques, les expériences montrent qu'un RNN bidirectionnel atteint une f-mesure de 79,51 tandis que le même système intégrant le mécanisme d'attention permet d'atteindre une f-mesure de 80,27.ABSTRACTExploring the use of Attention-Based Recurrent Neural Networks For Spoken Language UnderstandingThis study explores the use of a bidirectional recurrent neural network (RNN) encoder/decoder based on a mechanism of attention for a Spoken Language Understanding (SLU) task. First experiments carried on the ATIS corpus confirm the quality of the RNN baseline system used in this paper, by comparing its results on the ATIS corpus to the results recently published in the literature. Additional experiments show that RNN based on a mechanism of attention performs better than RNN architectures recently proposed for a slot filling task. On the French MEDIA corpus, a French state-of-the-art corpus for SLU dedicated to hotel reservation and tourist information, experiments show that a bidirectionnal RNN reaches a f-measure value of 79.51 while the use of a mechanism of attention allows us to reach a f-measure value of 80.27. MOTS-CLÉS : Compréhension de la Parole, Réseaux de Neurones Récurrents, Mécanisme d'Attention, Bidirectionnel.",
        "id": 150252449
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper presents an easy to implement and high performing method for OOD detection with language models?",
    "positive_ctxs": [
      {
        "title": "Is Fine-tuning Needed? Pre-trained Language Models Are Near Perfect for Out-of-Domain Detection",
        "text": "Out-of-distribution (OOD) detection is a critical task for reliable predictions over text. Finetuning with pre-trained language models has been a de facto procedure to derive OOD detectors with respect to in-distribution (ID) data. Despite its common use, the understanding of the role of fine-tuning and its necessity for OOD detection is largely unexplored. In this paper, we raise the question: is fine-tuning necessary for OOD detection? We present a study investigating the efficacy of directly leveraging pre-trained language models for OOD detection, without any model fine-tuning on the ID data. We compare the approach with several competitive fine-tuning objectives, and offer new insights under various types of distributional shifts. Extensive evaluations on 8 diverse ID-OOD dataset pairs demonstrate nearperfect OOD detection performance (with 0% FPR95 in many cases), strongly outperforming its fine-tuned counterparts. We show that using distance-based detection methods, pretrained language models are near-perfect OOD detectors when the distribution shift involves a domain change. Furthermore, we study the effect of fine-tuning on OOD detection and identify how to balance ID accuracy with OOD detection performance. Our code is publically available 1 .",
        "id": 258832820
      }
    ],
    "negative_ctxs": [
      {
        "title": "Phrase2VecGLM: Neural generalized language model-based semantic tagging for complex query reformulation in medical IR",
        "text": "In fact-based information retrieval, stateof-the-art performance is traditionally achieved by knowledge graphs driven by knowledge bases, as they can represent facts about and capture relationships between entities very well. However, in domains such as medical information retrieval, where addressing specific information needs of complex queries may require understanding query intent by capturing novel associations between potentially latent concepts, these systems can fall short. In this work, we develop a novel, completely unsupervised, neural language model-based ranking approach for semantic tagging of documents, using the document to be tagged as a query into the model to retrieve candidate phrases from top-ranked related documents, thus associating every document with novel related concepts extracted from the text. For this we extend the word embeddingbased generalized language model (GLM) due to(Ganguly et al., 2015), to employ phrasal embeddings, and use the semantic tags thus obtained for downstream query expansion, both directly and in feedback loop settings. Our method, evaluated using the TREC 2016 clinical decision support challenge dataset, shows statistically significant improvement not only over various baselines that use standard MeSH terms and UMLS concepts for query expansion, but also over baselines using human expert-assigned concept tags for the queries, on top of a standard Okapi BM25-based document retrieval system.",
        "id": 51869141
      },
      {
        "title": "An English-Chinese Cross-lingual Word Semantic Similarity Measure Exploring Attributes and Relations",
        "text": "Word semantic similarity measuring is a fundamental issue to many NLP applications and the globalization has made an urgent request for cross-lingual word similarity measure. This paper proposed a word semantic similarity measure which is able to work in cross-lingual scenarios. Basically, a concept can be defined by a set of attributes. The basic idea of this work is to compute the similarity between words by exploring their attributes and relations. For a given word pair, we first compute similarities between their attributes by combining distance, depth and relation information. Then word similarity are computed through a combination scheme. The algorithm is implemented based on an English-Chinese bilingual ontology HowNet. Experiments show that the proposed algorithm results in high correlation against human judgments, which encourages its broad application in cross-lingual applications.",
        "id": 11136127
      },
      {
        "title": "Universal Decompositional Semantics on Universal Dependencies",
        "text": "We present a framework for augmenting data sets from the Universal Dependencies project with Universal Decompositional Semantics. Where the Universal Dependencies project aims to provide a syntactic annotation standard that can be used consistently across many languages as well as a collection of corpora that use that standard, our extension has similar aims for semantic annotation. We describe results from annotating the English Universal Dependencies treebank, dealing with word senses, semantic roles, and event properties.",
        "id": 11796795
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper proposed dictionary-based Bayesian inference to improve the performance of image text matching model?",
    "positive_ctxs": [
      {
        "title": "Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information",
        "text": "Visual Word Sense Disambiguation (VWSD) is a task to find the image that most accurately depicts the correct sense of the target word for the given context. Previously, image-text matching models often suffered from recognizing polysemous words. This paper introduces an unsupervised VWSD approach that uses gloss information of an external lexical knowledge-base, especially the sense definitions. Specifically, we suggest employing Bayesian inference to incorporate the sense definitions when sense information of the answer is not provided. In addition, to ameliorate the out-of-vocabulary (OOV) issue, we propose a context-aware definition generation with GPT-3. Experimental results show that VWSD performance increased significantly with our Bayesian inference-based approach. In addition, our context-aware definition generation achieved prominent performance improvement in OOV examples exhibiting better performance than the existing definition generation method.",
        "id": 258461036
      }
    ],
    "negative_ctxs": [
      {
        "title": "Modeling Tweet Arrival Times using Log-Gaussian Cox Processes",
        "text": "Research on modeling time series text corpora has typically focused on predicting what text will come next, but less well studied is predicting when the next text event will occur. In this paper we address the latter case, framed as modeling continuous inter-arrival times under a log-Gaussian Cox process, a form of inhomogeneous Poisson process which captures the varying rate at which the tweets arrive over time. In an application to rumour modeling of tweets surrounding the 2014 Ferguson riots, we show how interarrival times between tweets can be accurately predicted, and that incorporating textual features further improves predictions.",
        "id": 14606708
      },
      {
        "title": "A Critique and Improvement of an Evaluation Metric for Text Segmentation",
        "text": "The P k evaluation metric, initially proposed byBeeferman, Berger, and Lafferty (1997), is becoming the standard measure for assessing text segmentation algorithms. However, a theoretical analysis of the metric finds several problems: the metric penalizes false negatives more heavily than false positives, overpenalizes near misses, and is affected by variation in segment size distribution. We propose a simple modification to the P k metric that remedies these problems. This new metric-called WindowDiff-moves a fixed-sized window across the text and penalizes the algorithm whenever the number of boundaries within the window does not match the true number of boundaries for that window of text.",
        "id": 6048999
      },
      {
        "title": "When CORDIAL Becomes Friendly: Endowing the CORDIAL Corpus with a Syntactic Annotation Layer",
        "text": "This paper reports on the syntactic annotation of a previously compiled and tagged corpus of European Portuguese (EP) dialects -The Syntax-oriented Corpus of Portuguese Dialects (CORDIAL-SIN). The parsed version of CORDIAL-SIN is intended to be a more efficient resource for the purpose of studying dialect syntax by allowing automated searches for various syntactic constructions of interest. To achieve this goal we adopted a rich annotation system (the UPenn corpora annotation system) which codifies syntactic information of high relevance. The annotation produces tree representations, in form of labelled parenthesis, that are integrally searchable with CorpusSearch, a search engine for parsed corpora (Randall, 2005(Randall,  -2007. The present paper focuses on CORDIAL-SIN annotation issues, namely it presents the general principles and guidelines of the adopted annotation system and describes the methodology for constructing the parsed version of the corpus and for searching it (tools and procedures). Last section addresses the question of how an annotation system originally designed for Middle English can be adapted to meet the particular needs of a Portuguese corpus of dialectal speech.",
        "id": 14757570
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "In multimodal (multilingual) abstractive summarization field, is there any paper that propose target-oriented vision modeling method to improve the quality of summaries?",
    "positive_ctxs": [
      {
        "title": "Summary-Oriented Vision Modeling for Multimodal Abstractive Summarization",
        "text": "Multimodal abstractive summarization (MAS) aims to produce a concise summary given the multimodal data (text and vision). Existing studies mainly focus on how to effectively use the visual features from the perspective of an article, having achieved impressive success on the high-resource English dataset. However, less attention has been paid to the visual features from the perspective of the summary, which may limit the model performance, especially in the low-and zero-resource scenarios. In this paper, we propose to improve the summary quality through summary-oriented visual features. To this end, we devise two auxiliary tasks including vision to summary task and masked image modeling task. Together with the main summarization task, we optimize the MAS model via the training objectives of all these tasks. By these means, the MAS model can be enhanced by capturing the summaryoriented visual features, thereby yielding more accurate summaries. Experiments on 44 languages, covering mid-high-, low-, and zeroresource scenarios, verify the effectiveness and superiority of the proposed approach, which achieves state-of-the-art performance under all scenarios. Additionally, we will contribute a large-scale multilingual multimodal abstractive summarization (MM-Sum) dataset. 1",
        "id": 254685691
      }
    ],
    "negative_ctxs": [
      {
        "title": "Intensional Summaries as Cooperative Responses in Dialogue: Automation and Evaluation",
        "text": "Despite its long history, and a great deal of research producing many useful algorithms and observations, research in cooperative response generation has had little impact on the recent commercialization of dialogue technologies, particularly within the spoken dialogue community. We hypothesize that a particular type of cooperative response, intensional summaries, are effective for when users are unfamiliar with the domain. We evaluate this hypothesis with two experiments with cruiser, a DS for in-car or mobile users to access restaurant information. First, we compare cruiser with a baseline system-initiative DS, and show that users prefer cruiser. Then, we experiment with four algorithms for constructing intensional summaries in cruiser, and show that two summary types are equally effective: summaries that maximize domain coverage and summaries that maximize utility with respect to a user model.",
        "id": 5555658
      },
      {
        "title": "0.0 INTRODUCTION A CASE FOR RULE-DRIVEN SEMANTIC PROCESSING",
        "text": "",
        "id": 6168320
      },
      {
        "title": "Generating \"A for Alpha\" When There Are Thousands of Characters",
        "text": "The phonetic alphabet enables people to dictate letters of the alphabet accurately by using representative words, i.e., A for Alpha. Japanese kanji (idiographic Chinese characters) vastly outnumber the letters of the Roman alphabet, and thus Japanese requires an explanatory reading like a phonetic alphabet. We call the explanatory reading of a kanji a \"distinctive explanation.\" Most kanji characters have their homophones, and the role of the distinctive explanations is to enable users to identify a specific kanji character only by listening to the explanation. In this paper, we propose a corpus-based method for automatically generating distinctive explanations for a kanji, in which information about familiarity and homophones of kanji are taken into consideration. Through the kanji-identification experiments, we show that the quality of the explanations generated by the proposed method is higher than that of the manually crafted distinctive explanations.",
        "id": 17661090
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there a tool that can automatically segment speech and the corresponding text transcriptions, to obtain a finer grained alignment?",
    "positive_ctxs": [
      {
        "title": "CMOT: Cross-modal Mixup via Optimal Transport for Speech Translation",
        "text": "End-to-end speech translation (ST) is the task of translating speech signals in the source language into text in the target language. As a cross-modal task, end-to-end ST is difficult to train with limited data. Existing methods often try to transfer knowledge from machine translation (MT), but their performances are restricted by the modality gap between speech and text. In this paper, we propose Cross-modal Mixup via Optimal Transport (CMOT) to overcome the modality gap. We find the alignment between speech and text sequences via optimal transport and then mix up the sequences from different modalities at a token level using the alignment. Experiments on the MuST-C ST benchmark demonstrate that CMOT achieves an average BLEU of 30.0 in 8 translation directions, outperforming previous methods. Further analysis shows CMOT can adaptively find the alignment between modalities, which helps alleviate the modality gap between speech and text.",
        "id": 258866035
      }
    ],
    "negative_ctxs": [
      {
        "title": "Statistical versus symbolic parsing for captioned-information retrieval",
        "text": "",
        "id": 1441199
      },
      {
        "title": "Towards Table-to-Text Generation with Pretrained Language Model: A Table Structure Understanding and Text Deliberating Approach",
        "text": "Although remarkable progress on the neural table-to-text methods has been made, the generalization issues hinder the applicability of these models due to the limited source tables. Large-scale pretrained language models sound like a promising solution to tackle such issues. However, how to effectively bridge the gap between the structured table and the text input by fully leveraging table information to fuel the pretrained model is still not well explored. Besides, another challenge of integrating the deliberation mechanism into the textto-text pretrained model for solving the tableto-text task remains seldom studied. In this paper, to implement the table-to-text generation with pretrained language model, we propose a table structure understanding and text deliberating approach, namely TASD. To be specific, we devise a three-layered multi-head attention network to realize the table-structureaware text generation model with the help of the pretrained language model. Furthermore, a multi-pass decoder framework is adopted to enhance the capability of polishing generated text for table descriptions. The empirical studies, as well as human evaluation, on two public datasets, validate that our approach can generate faithful and fluent descriptive texts for different types of tables.1",
        "id": 255440553
      },
      {
        "title": "Predicting the presence of inline citations in academic text using binary classification",
        "text": "Properly citing sources is a crucial component of any good-quality academic paper. The goal of this study was to determine what kind of accuracy we could reach in predicting whether or not a sentence should contain an inline citation using a simple binary classification model. To that end, we fine-tuned SciBERT on both an imbalanced and a balanced dataset containing sentences with and without inline citations. We achieved an overall accuracy of over 0.92, suggesting that language patterns alone could be used to predict where inline citations should appear.",
        "id": 258765297
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Have any research papers suggested methods for summarizing arbitrary length documents without truncating the input, by using memory networks?",
    "positive_ctxs": [
      {
        "title": "Sliding Selector Network with Dynamic Memory for Extractive Summarization of Long Documents",
        "text": "Neural-based summarization models suffer from the length limitation of text encoder. Long documents have to been truncated before they are sent to the model, which results in huge loss of summary-relevant contents. To address this issue, we propose the sliding selector network with dynamic memory for extractive summarization of long-form documents, which employs a sliding window to extract summary sentences segment by segment. Moreover, we adopt memory mechanism to preserve and update the history information dynamically, allowing the semantic flow across different windows. Experimental results on two large-scale datasets that consist of scientific papers demonstrate that our model substantially outperforms previous state-of-the-art models. Besides, we perform qualitative and quantitative investigations on how our model works and where the performance gain comes from. 1",
        "id": 235097475
      }
    ],
    "negative_ctxs": [
      {
        "title": "An Evaluation of Image-based Verb Prediction Models against Human Eye-tracking Data",
        "text": "Recent research in language and vision has developed models for predicting and disambiguating verbs from images. Here, we ask whether the predictions made by such models correspond to human intuitions about visual verbs. We show that the image regions a verb prediction model identifies as salient for a given verb correlate with the regions fixated by human observers performing a verb classification task.",
        "id": 44177418
      },
      {
        "title": "Efficiently Computing Nash Equilibria in Adversarial Team Markov Games",
        "text": "Computing Nash equilibrium policies is a central problem in multi-agent reinforcement learning that has received extensive attention both in theory and in practice. However, in light of computational intractability barriers in general-sum games, provable guarantees have been thus far either limited to fully competitive or cooperative scenarios or impose strong assumptions that are difficult to meet in most practical applications.In this work, we depart from those prior results by investigating infinite-horizon adversarial team Markov games, a natural and well-motivated class of games in which a team of identically-interested players-in the absence of any explicit coordination or communication-is competing against an adversarial player. This setting allows for a unifying treatment of zero-sum Markov games and Markov potential games, and serves as a step to model more realistic strategic interactions that feature both competing and cooperative interests. Our main contribution is the first algorithm for computing stationary ǫ-approximate Nash equilibria in adversarial team Markov games with computational complexity that is polynomial in all the natural parameters of the game, as well as 1/ǫ.The proposed algorithm is particularly natural and practical, and it is based on performing independent policy gradient steps for each player in the team, in tandem with best responses from the side of the adversary; in turn, the policy for the adversary is then obtained by solving a carefully constructed linear program. Our analysis leverages non-standard techniques to establish the KKT optimality conditions for a nonlinear program with nonconvex constraints, thereby leading to a natural interpretation of the induced Lagrange multipliers. Along the way, we significantly extend an important characterization of optimal policies in adversarial (normal-form) team games due to Von Stengel and Koller (GEB '97).",
        "id": 251279905
      },
      {
        "title": "Published as a conference paper at ICLR 2020 NETWORK RANDOMIZATION: A SIMPLE TECHNIQUE FOR GENERALIZATION IN DEEP REINFORCEMENT LEARNING",
        "text": "Deep reinforcement learning (RL) agents often fail to generalize to unseen environments (yet semantically similar to trained agents), particularly when they are trained on high-dimensional state spaces, such as images. In this paper, we propose a simple technique to improve a generalization ability of deep RL agents by introducing a randomized (convolutional) neural network that randomly perturbs input observations. It enables trained agents to adapt to new domains by learning robust features invariant across varied and randomized environments. Furthermore, we consider an inference method based on the Monte Carlo approximation to reduce the variance induced by this randomization. We demonstrate the superiority of our method across 2D CoinRun, 3D DeepMind Lab exploration and 3D robotics control tasks: it significantly outperforms various regularization and data augmentation methods for the same purpose. Code is available at . Deep direct reinforcement learning for financial signal representation and trading. IEEE transactions on neural networks and learning systems, 28(3):653-664, 2016.Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. , et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In ICML, 2018.",
        "id": 213597045
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which paper systematically examed the input mismatch between training and sampling in diffusion models",
    "positive_ctxs": [
      {
        "title": "ELUCIDATING THE EXPOSURE BIAS IN DIFFUSION MODELS",
        "text": "Diffusion models have demonstrated impressive generative capabilities, but their exposure bias problem, described as the input mismatch between training and sampling, lacks in-depth exploration.In this paper, we systematically investigate the exposure bias problem in diffusion models by first analytically modelling the sampling distribution, based on which we then attribute the prediction error at each sampling step as the root cause of the exposure bias issue.Furthermore, we discuss potential solutions to this issue and propose an intuitive metric for it.Along with the elucidation of exposure bias, we propose a simple, yet effective, training-free method called Epsilon Scaling to alleviate the exposure bias.We show that Epsilon Scaling explicitly moves the sampling trajectory closer to the vector field learned in the training phase by scaling down the network output (Epsilon), mitigating the input mismatch between training and sampling.Experiments on various diffusion frameworks (ADM, DDPM/DDIM, EDM, LDM), unconditional and conditional settings, and deterministic vs. stochastic sampling verify the effectiveness of our method.Remarkably, our ADM-ES, as a SOTA stochastic sampler, obtains 2.17 FID on CIFAR-10 under 100-step unconditional generation.The code is available at https://github.com/forever208/ADM-ESand https://github.com/forever208/EDM-ESWe point out that the exposure bias problem in diffusion models lacks in-depth exploration.For example, there is no proper metric to quantify the exposure bias and no explicit error analysis for it.To shed light on exposure bias, we conduct a systematical investigation in this paper by first",
        "id": 261276856
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 221373767
      },
      {
        "title": "",
        "text": "",
        "id": 225062898
      },
      {
        "title": "Out-of-Sample Representation Learning for Knowledge Graphs",
        "text": "Many important problems can be formulated as reasoning in knowledge graphs. Representation learning has proved extremely effective for transductive reasoning, in which one needs to make new predictions for already observed entities. This is true for both attributed graphs (where each entity has an initial feature vector) and non-attributed graphs (where the only initial information derives from known relations with other entities). For out-of-sample reasoning, where one needs to make predictions for entities that were unseen at training time, much prior work considers attributed graph. However, this problem is surprisingly under-explored for non-attributed graphs. In this paper, we study the out-of-sample representation learning problem for non-attributed knowledge graphs, create benchmark datasets for this task, develop several models and baselines, and provide empirical analyses and comparisons of the proposed models and baselines.",
        "id": 226283940
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that investigates how cross-attention mechanisms improve the interplay between token and syntax data within automated program repair systems?",
    "positive_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 236478113
      }
    ],
    "negative_ctxs": [
      {
        "title": "Published as a conference paper at ICLR 2021 ONE NETWORK FITS ALL? MODULAR VERSUS MONOLITHIC TASK FORMULATIONS IN NEURAL NETWORKS",
        "text": "Can deep learning solve multiple tasks simultaneously, even when they are unrelated and very different? We investigate how the representations of the underlying tasks affect the ability of a single neural network to learn them jointly. We present theoretical and empirical findings that a single neural network is capable of simultaneously learning multiple tasks from a combined data set, for a variety of methods for representing tasks-for example, when the distinct tasks are encoded by well-separated clusters or decision trees over certain task-code attributes. More concretely, we present a novel analysis that shows that families of simple programming-like constructs for the codes encoding the tasks are learnable by two-layer neural networks with standard training. We study more generally how the complexity of learning such combined tasks grows with the complexity of the task codes; we find that combining many tasks may incur a sample complexity penalty, even though the individual tasks are easy to learn. We provide empirical support for the usefulness of the learning bounds by training networks on clusters, decision trees, and SQL-style aggregation. Uszkoreit. One model to learn them all. arXiv preprint arXiv:1706.05137, 2017.Michael Kearns. Efficient noise-tolerant learning from statistical queries. . Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. and practical bert models for sequence labeling. arXiv preprint arXiv:1909.00100, 2019.Gregory Valiant. Finding correlations in subquadratic time, with applications to learning parities and the closest pair problem. survey on multi-task learning. arXiv preprint arXiv:1707.08114, 2017.",
        "id": 232404824
      },
      {
        "title": "Tamil NER -Coping with Real Time Challenges",
        "text": "This paper describes various challenges encountered while developing an automatic Named Entity Recognition (NER) using Conditional Random Fields (CRFs) for Tamil. We also discuss how we have overcome some of these challenges. Though most of the challenges in NER discussed here are common to many Indian languages, in this work the focus is on Tamil, a South Indian language belonging to Dravidian language family. The corpus used in this work is the web data. The web data consisted of news paper articles, articles on blog sites and other online web portals.",
        "id": 16456944
      },
      {
        "title": "Grammars for Local and Long Dependencies",
        "text": "Polarized dependency (PD-)  grammars are proposed as a means of efficient treatment of discontinuous constructions. PD-grammars describe two kinds of dependencies : local, explicitly derived by the rules, and long, implicitly specified by negative and positive valencies of words. If in a PD-grammar the number of non-saturated valencies in derived structures is bounded by a constant, then it is weakly equivalent to a cf-grammar and has a ¢ ¡ ¤ £ ¦ ¥ § time parsing algorithm. It happens that such bounded PD-grammars are strong enough to express such phenomena as unbounded raising, extraction and extraposition.© and a word dependent on © be dominated by © In first dependency grammars(Gaifman, 1961)and in some more recent proposals: link grammars (Sleator and Temperly, 1993), projective dependency grammars(Lombardo and Lesmo, 1996)the projectivity is implied by definition. In some other theories, e.g. in word grammar(Hudson, 1984), it is used as one of the axioms defining acceptable surface structures. In presence of this property, D-trees are in a sense equivalent to phrase structures with head selection 1 . It is for this reason that D-trees determined by grammars ofRobinson (Robinson, 1970), categorial grammars(Bar-Hillel et al., 1960), classical Lambek calculus(Lambek, 1958), and some other formalisms are projective. Projectivity affects the complexity of parsing : as a rule, it allows dynamic programming technics which lead to polynomial time algorithms (cf.",
        "id": 10350260
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest a triplet-formatted structured dataset suitable for training table-to-text generation models?",
    "positive_ctxs": [
      {
        "title": "DART: Open-Domain Structured Data Record to Text Generation",
        "text": "We present DART, an open domain structuredDAta-Record-to-Text generation dataset with over 82k instances (DARTs). Data-to-text annotations can be a costly process, especially when dealing with tables which are the major source of structured data and contain nontrivial structures. To this end, we propose a procedure of extracting semantic triples from tables that encodes their structures by exploiting the semantic dependencies among table headers and the table title. Our dataset construction framework effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github. com/Yale-LILY/dart. . 2020c. Few-shot nlg with pre-trained language model. In ACL.Dorottya Demszky, Kelvin Guu, and Percy Liang. 2018. Transforming question answering datasets into natural language inference datasets. arXiv preprint arXiv:1809.02922.",
        "id": 220364230
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "We present an English lexical database which is fuller, more accurate and more consistent than any other. We believe this to be so because the project has been well-planned, with a 12-month intensive planning phase prior to the lexicography beginning; well-resourced, employing a team of fifteen highly experienced lexicographers for a thirty-month main phase; it has had access to the latest corpus and dictionary-editing technology; it has not been constrained to meet any goals other than an accurate description of the language; and it has been led by a team with singular experience in delivering high-quality and innovative resources. The lexicon will be complete in Summer 2010 and will be available for NLP groups, on terms designed to encourage its research use.",
        "id": 17969549
      },
      {
        "title": "Modeling and Characterizing Social Media Topics Using the Gamma Distribution",
        "text": "We present a novel technique to identify emerging or important topics mentioned on social media. A sudden increase in related posts can indicate an occurrence of an external event. Assuming that the sequence of posts is a homogeneous Poisson process, this sudden change can be modeled using the Gamma distribution. Our Gamma curve fitter is used to return a set of emerging topics. We demonstrate our algorithm on Twitter data and evaluate empirically using the Reuters News Archive and manual inspection. Our experimental results show that our algorithm provides a good picture of the emerging topics discussed on Twitter.",
        "id": 15657766
      },
      {
        "title": "Semantic Relations Established by Specialized Processes Expressed by Nouns and Verbs: Identification in a Corpus by means of Syntactico-semantic Annotation",
        "text": "This article presents the methodology and results of the analysis of terms referring to processes expressed by verbs or nouns in a corpus of specialized texts dealing with ceramics. Both noun and verb terms are explored in context in order to identify and represent the semantic roles held by their participants (arguments and circumstants), and therefore explore some of the relations established by these terms. We present a methodology for the identification of related terms that take part in the development of specialized processes and the annotation of the semantic roles expressed in these contexts. The analysis has allowed us to identify participants in the process, some of which were already present in our previous work, but also some new ones. This method is useful in the distinction of different meanings of the same verb. Contexts in which processes are expressed by verbs have proved to be very informative, even if they are less frequent in the corpus. This work is viewed as a first step in the implementation -in ontologies -of conceptual relations in which activities are involved.",
        "id": 9090544
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a study that examines how cross project code summarization evaluation methodologies compare to time-segmented eval methodology.",
    "positive_ctxs": [
      {
        "title": "Impact of Evaluation Methodologies on Code Summarization",
        "text": "There has been a growing interest in developing machine learning (ML) models for code summarization tasks, e.g., comment generation and method naming. Despite substantial increase in the effectiveness of ML models, the evaluation methodologies, i.e., the way people split datasets into training, validation, and test sets, were not well studied. Specifically, no prior work on code summarization considered the timestamps of code and comments during evaluation. This may lead to evaluations that are inconsistent with the intended use cases. In this paper, we introduce the time-segmented evaluation methodology, which is novel to the code summarization research community, and compare it with the mixed-project and cross-project methodologies that have been commonly used. Each methodology can be mapped to some use cases, and the time-segmented methodology should be adopted in the evaluation of ML models for code summarization. To assess the impact of methodologies, we collect a dataset of (code, comment) pairs with timestamps to train and evaluate several recent ML models for code summarization. Our experiments show that different methodologies lead to conflicting evaluation results. We invite the community to expand the set of methodologies used in evaluations.",
        "id": 247958464
      }
    ],
    "negative_ctxs": [
      {
        "title": "Published as a conference paper at ICLR 2021 UNDERSTANDING OVERPARAMETERIZATION IN GENERATIVE ADVERSARIAL NETWORKS",
        "text": "A broad class of unsupervised deep learning methods such as Generative Adversarial Networks (GANs) involve training of overparameterized models where the number of parameters of the model exceeds a certain threshold. Indeed, most successful GANs used in practice are trained using overparameterized generator and discriminator networks, both in terms of depth and width. A large body of work in supervised learning have shown the importance of model overparameterization in the convergence of the gradient descent (GD) to globally optimal solutions. In contrast, the unsupervised setting and GANs in particular involve non-convex concave mini-max optimization problems that are often trained using Gradient Descent/Ascent (GDA). The role and benefits of model overparameterization in the convergence of GDA to a global saddle point in non-convex concave problems is far less understood. In this work, we present a comprehensive analysis of the importance of model overparameterization in GANs both theoretically and empirically. We theoretically show that in an overparameterized GAN model with a 1-layer neural network generator and a linear discriminator, GDA converges to a global saddle point of the underlying non-convex concave min-max problem. To the best of our knowledge, this is the first result for global convergence of GDA in such settings. Our theory is based on a more general result that holds for a broader class of nonlinear generators and discriminators that obey certain assumptions (including deeper generators and random feature discriminators). Our theory utilizes and builds upon a novel connection with the convergence analysis of linear timevarying dynamical systems which may have broader implications for understanding the convergence behavior of GDA for non-convex concave problems involving overparameterized models. We also empirically study the role of model overparameterization in GANs using several large-scale experiments on CIFAR-10 and Celeb-A datasets. Our experiments show that overparameterization improves the quality of generated samples across various model architectures and datasets. Remarkably, we observe that overparameterization leads to faster and more stable convergence behavior of GDA across the board.",
        "id": 232360541
      },
      {
        "title": "Annotating for Hate Speech: The MaNeCo Corpus and Some Input from Critical Discourse Analysis",
        "text": "This paper presents a novel scheme for the annotation of hate speech in corpora of Web 2.0 commentary. The proposed scheme is motivated by the critical analysis of posts made in reaction to news reports on the Mediterranean migration crisis and LGBTIQ+ matters in Malta, which was conducted under the auspices of the EU-funded C.O.N.T.A.C.T. project. Based on the realization that hate speech is not a clear-cut category to begin with, appears to belong to a continuum of discriminatory discourse and is often realized through the use of indirect linguistic means, it is argued that annotation schemes for its detection should refrain from directly including the label 'hate speech,' as different annotators might have different thresholds as to what constitutes hate speech and what not. In view of this, we suggest a multi-layer annotation scheme, which is pilot-tested against a binary ±hate speech classification and appears to yield higher inter-annotator agreement. Motivating the postulation of our scheme, we then present the MaNeCo corpus on which it will eventually be used; a substantial corpus of on-line newspaper comments spanning 10 years.",
        "id": 218973764
      },
      {
        "title": "Trust Evaluation Mechanisms for Wikipedia",
        "text": "Wikipedia is the well-nigh successful and most popular free encyclopedia developed by many editors in collaborative manner. It provides multitude of opportunities for online large scale knowledge sharing between virtual communities by letting the viewer to create and edit articles directly in the web browser. Information on Wikipedia is expanding largely, but the increase in quantity is not proportional to quality of the content. The cursory observer of Wikipedia may not be able to differentiate between the good and the bad quality of the content. Despite the success of Wikipedia, trust on Wikipedia content is still questioned because of its open editing model. In this paper primarily the challenges for trust evaluation mechanisms, caused by the significant characteristics of Wikipedia's knowledge base are discussed. Existing Wikipedia trust evaluation models are comprehensively surveyed and key issues related to these are highlighted. Finally based on this study new dimensions for effective trust evaluation mechanisms are proposed, which are aimed to setup clear goals for future research in this area.36",
        "id": 1136774
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Are there any papers that construct convolutional networks which are equivariant with respect to non-compact/non-abelian Lie groups?",
    "positive_ctxs": [
      {
        "title": "LIE GROUP DECOMPOSITIONS FOR EQUIVARIANT NEURAL NETWORKS",
        "text": "Invariance and equivariance to geometrical transformations have proven to be very useful inductive biases when training (convolutional) neural network models, especially in the low-data regime. Much work has focused on the case where the symmetry group employed is compact or abelian, or both. Recent work has explored enlarging the class of transformations used to the case of Lie groups, principally through the use of their Lie algebra, as well as the group exponential and logarithm maps. The applicability of such methods to larger transformation groups is limited by the fact that depending on the group of interest G, the exponential map may not be surjective. Further limitations are encountered when G is neither compact nor abelian. Using the structure and geometry of Lie groups and their homogeneous spaces, we present a framework by which it is possible to work with such groups primarily focusing on the Lie groups G = GL + (n, R) and G = SL(n, R), as well as their representation as affine transformations R n ⋊ G. Invariant integration as well as a global parametrization is realized by decomposing the 'larger' groups into subgroups and submanifolds which can be handled individually. Under this framework, we show how convolution kernels can be parametrized to build models equivariant with respect to affine transformations. We evaluate the robustness and out-of-distribution generalisation capability of our model on the standard affine-invariant benchmark classification task, where we outperform all previous equivariant models as well as all Capsule Network proposals. . Geometric means in a novel vector space structure on symmetric positive-definite matrices. SIAM journal on matrix analysis and applications, 29(1): 328-347, 2007.",
        "id": 264172845
      }
    ],
    "negative_ctxs": [
      {
        "title": "Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation",
        "text": "We present an easy and efficient method to extend existing sentence embedding models to new languages. This allows to create multilingual versions from previously monolingual models. The training is based on the idea that a translated sentence should be mapped to the same location in the vector space as the original sentence. We use the original (monolingual) model to generate sentence embeddings for the source language and then train a new system on translated sentences to mimic the original model. Compared to other methods for training multilingual sentence embeddings, this approach has several advantages: It is easy to extend existing models with relatively few samples to new languages, it is easier to ensure desired properties for the vector space, and the hardware requirements for training is lower. We demonstrate the effectiveness of our approach for 10 languages from various language families. Code to extend sentence embeddings models to more than 400 languages is publicly available. 1",
        "id": 216036089
      },
      {
        "title": "Learning Input Strictly Local Functions: Comparing Approaches with Catalan Adjectives",
        "text": "",
        "id": 248182527
      },
      {
        "title": "",
        "text": "",
        "id": 1801525
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest a study that explores employing a beta distribution to sample span sizes within unsupervised learning frameworks for text representation?",
    "positive_ctxs": [
      {
        "title": "DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations",
        "text": "Sentence embeddings are an important component of many natural language processing (NLP) systems. Like word embeddings, sentence embeddings are typically learned on large text corpora and then transferred to various downstream tasks, such as clustering and retrieval. Unlike word embeddings, the highest performing solutions for learning sentence embeddings require labelled data, limiting their usefulness to languages and domains where labelled data is abundant. In this paper, we present DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations. Inspired by recent advances in deep metric learning (DML), we carefully design a self-supervised objective for learning universal sentence embeddings that does not require labelled training data. When used to extend the pretraining of transformer-based language models, our approach closes the performance gap between unsupervised and supervised pretraining for universal sentence encoders. Importantly, our experiments suggest that the quality of the learned embeddings scale with both the number of trainable parameters and the amount of unlabelled training data. Our code and pretrained models are publicly available and can be easily adapted to new domains or used to embed unseen text. 1",
        "id": 219530980
      }
    ],
    "negative_ctxs": [
      {
        "title": "Towards Modeling Social and Content Dynamics in Discussion Forums",
        "text": "Extended AbstractRecent years have witnessed the transformation of the World Wide Web from an information-gathering and processing tool into an interactive communication medium in the form of online discussion forums, chat-rooms, blogs, and so on. There is strong evidence suggesting that social networks facilitate new ways to interact with information in such media. Understanding the mechanisms and the patterns of such interactions can be important for many applications. Currently, there is not much work that adequately models interaction between social networks and information content. From the perspective of social network analysis, most existing work is concerned with understanding static topological properties of social networks represented by such forums. For instance, Park and Maurer (2009) applied node clustering to identify consensus and consensus facilitators, while Kang et al.(2009)uses discussion thread co-participation relations to identify (static) groups in discussions. On discussion content analysis research side, there have been approaches for classifying messages with respect to dialogue roles(Carvalho and Cohen, 2005;Ravi and Kim, 2007), but they often ignore the role and the impact of underlying social interactions.Thus, the current static network and content analysis approaches provide limited support for• Capturing dynamics of social interactions: the sequence of communication or who is responding to whom is important in understanding the nature of interactions. • Relating social interactions to content analysis: the content can give hint on the nature of the interaction and vice versa (e.g., users with more social interactions are more likely to have common interests).To address the above issues, one needs to go beyond the static analysis approach, and develop dynamical models that will explicitly account for the interplay between the content of communication (topics) and the structure of communications (social networks). Such framework and corresponding algorithmic base will allow us to infer \"polarizing\" topics discussed in forums, identify evolving communities of interests, and examine the link between social and content dynamics.",
        "id": 18663038
      },
      {
        "title": "Reducing the Size of the Representation for the uDOP-Estimate",
        "text": "The unsupervised Data Oriented Parsing (uDOP) approach has been repeatedly reported to achieve state of the art performance in experiments on parsing of different corpora. At the same time the approach is demanding both in computation time and memory. This paper describes an approach which decreases these demands. First the problem is translated into the generation of probabilistic bottom up tree automata (pBTA). Then it is explained how solving two standard problems for these automata results in a reduction in the size of the grammar. The reduction of the grammar size by using efficient algorithms for pBTAs is the main contribution of this paper. Experiments suggest that this leads to a reduction in grammar size by a factor of 2. This paper also suggests some extensions of the original uDOP algorithm that are made possible or aided by the use of tree automata.A Short Discussion of uDOPThe unsupervised Data Oriented Parsing (uDOP) approach (Bod",
        "id": 16956934
      },
      {
        "title": "Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints",
        "text": "In most practical settings and theoretical analysis, one assumes that a model can be trained until convergence. However, the growing complexity of machine learning datasets and models may violate such assumptions. Moreover, current approaches for hyper-parameter tuning and neural architecture search tend to be limited by practical resource constraints. Therefore, we introduce a formal setting for studying training under the non-asymptotic, resourceconstrained regime, i.e. budgeted training. We analyze the following problem: \"given a dataset, algorithm, and resource budget, what is the best achievable performance?\" We focus on the number of optimization iterations as the representative resource. Under such a setting, we show that it is critical to adjust the learning rate schedule according to the given budget. Among budget-aware learning schedules, we find simple linear decay to be both robust and high-performing. We support our claim through extensive experiments with state-of-the-art models on ImageNet (image classification), Cityscapes (semantic segmentation), MS COCO (object detection and instance segmentation), and Kinetics (video classification). We also analyze our results and find that the key to a good schedule is budgeted convergence, a phenomenon whereby the gradient vanishes at the end of each allowed budget. We also revisit existing approaches for fast convergence, and show that budget-aware learning schedules readily outperform such approaches under (the practical but under-explored) budgeted setting.",
        "id": 152282636
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "I'm looking for a comprehensive dataset that has been influential in fact verification research",
    "positive_ctxs": [
      {
        "title": "FEVER: a large-scale dataset for Fact Extraction and VERification",
        "text": "In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from.The claims are classified as SUPPORTED, RE-FUTED or NOTENOUGHINFO by annotators achieving 0.6841 in Fleiss κ. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87%, while if we ignore the evidence we achieve 50.91%. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.",
        "id": 4711425
      }
    ],
    "negative_ctxs": [
      {
        "title": "Zhestyatsky at SemEval-2021 Task 2: ReLU over Cosine Similarity for BERT Fine-tuning",
        "text": "This paper presents our contribution to SemEval-2021 Task 2: Multilingual and Cross-lingual Word-in-Context Disambiguation (MCL-WiC). Our experiments cover English (EN-EN) sub-track from the multilingual setting of the task. We experiment with several pre-trained language models and investigate an impact of different top-layers on finetuning. We find the combination of Cosine Similarity and ReLU activation leading to the most effective fine-tuning procedure. Our best model results in accuracy 92.7%, which is the fourth-best score in EN-EN sub-track.",
        "id": 233231621
      },
      {
        "title": "",
        "text": "",
        "id": 218977390
      },
      {
        "title": "Cross-Lingual Dependency Parsing Using Code-Mixed TreeBank",
        "text": "Treebank translation is a promising method for cross-lingual transfer of syntactic dependency knowledge. The basic idea is to map dependency arcs from a source treebank to its target translation according to word alignments. This method, however, can suffer from imperfect alignment between source and target words. To address this problem, we investigate syntactic transfer by code mixing, translating only confident words in a source treebank. Cross-lingual word embeddings are leveraged for transferring syntactic knowledge to the target from the resulting code-mixed treebank. Experiments on University Dependency Treebanks show that code-mixed treebanks are more effective than translated treebanks, giving highly competitive performances among cross-lingual parsing methods.",
        "id": 202541341
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Are there any large-scale and open-source text simplification datasets dealing with long passages?",
    "positive_ctxs": [
      {
        "title": "SWIPE: A Dataset for Document-Level Simplification of Wikipedia Pages",
        "text": "Text simplification research has mostly focused on sentence-level simplification, even though many desirable edits-such as adding relevant background information or reordering contentmay require document-level context. Prior work has also predominantly framed simplification as a single-step, input-to-output task, only implicitly modeling the fine-grained, span-level edits that elucidate the simplification process. To address both gaps, we introduce the SWIPE dataset, which reconstructs the document-level editing process from English Wikipedia (EW) articles to paired Simple Wikipedia (SEW) articles. In contrast to prior work, SWIPE leverages the entire revision history when pairing pages in order to better identify simplification edits. We work with Wikipedia editors to annotate 5,000 EW-SEW document pairs, labeling more than 40,000 edits with proposed 19 categories. To scale our efforts, we propose several models to automatically label edits, achieving an F-1 score of up to 70.6, indicating that this is a tractable but challenging NLU task. Finally, we categorize the edits produced by several simplification models and find that SWIPE-trained models generate more complex edits while reducing unwanted edits.",
        "id": 258967312
      }
    ],
    "negative_ctxs": [
      {
        "title": "MARMOT: A Toolkit for Translation Quality Estimation at the Word Level",
        "text": "We present Marmot -a new toolkit for quality estimation (QE) of machine translation output. Marmot contains utilities targeted at quality estimation at the word and phrase level. However, due to its flexibility and modularity, it can also be extended to work at the sentence level. In addition, it can be used as a framework for extracting features and learning models for many common natural language processing tasks. The tool has a set of state-of-the-art features for QE, and new features can easily be added. The tool is open-source and can be downloaded from https://github.com/qe-team/marmot/.",
        "id": 6680929
      },
      {
        "title": "Le théâtre français du XVIIe siècle : une expérience en catégorisation de textes",
        "text": "La catégorisation de documents (attribution d'un texte à une ou plusieurs catégories prédéfinies) possède de multiples applications.Cette communication se focalise sur l'attribution d'auteur en analysant le style de vingt pièces de théâtre du XVIIe siècle.L'hypothèse que nous souhaitons vérifier admet que le véritable auteur est le nom apparaissant sur la couverture.Afin de vérifier la qualité de deux méthodes d'attribution, nous avons repris deux corpus additionnels basés sur des romans écrits en français et italien.Nous proposons une amélioration de la méthode Delta ainsi qu'une nouvelle grille d'analyse pour cette approche.Ensuite, nous avons appliqué ces approches sur notre collection de comédies.Les résultats démontrent que l'hypothèse de base doit être écartée.De plus, ces oeuvres présentent des styles proches rendant toute attribution difficile.).",
        "id": 264038816
      },
      {
        "title": "What Decisions Have You Made: Automatic Decision Detection in Conversational Speech",
        "text": "This study addresses the problem of automatically detecting decisions in conversational speech. We formulate the problem as classifying decision-making units at two levels of granularity: dialogue acts and topic segments. We conduct an empirical analysis to determine the characteristic features of decision-making dialogue acts, and train MaxEnt models using these features for the classification tasks. We find that models that combine lexical, prosodic, contextual and topical features yield the best results on both tasks, achieving 72% and 86% precision, respectively. The study also provides a quantitative analysis of the relative importance of the feature types.",
        "id": 8916300
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Where can I find research about automatic evaluation metrics in summarization tasks disagree with each other?",
    "positive_ctxs": [
      {
        "title": "Metrics also Disagree in the Low Scoring Range: Revisiting Summarization Evaluation Metrics",
        "text": "In text summarization, evaluating the efficacy of automatic metrics without human judgments has become recently popular. One exemplar work (Peyrard, 2019) concludes that automatic metrics strongly disagree when ranking high-scoring summaries. In this paper, we revisit their experiments and find that their observations stem from the fact that metrics disagree in ranking summaries from any narrow scoring range. We hypothesize that this may be because summaries are similar to each other in a narrow scoring range and are thus, difficult to rank. Apart from the width of the scoring range of summaries, we analyze three other properties that impact inter-metric agreement -Ease of Summarization, Abstractiveness, and Coverage. To encourage reproducible research, we make all our analysis code and data publicly available. 1 1 https://github.com/manikbhandari/RevisitSummEvalMetrics 2 Peyrard (2019) uses three experiments to reach their conclusion. Due to limitations of space, we focus on the first one here. Please see the appendix for a detailed analysis of the other two experiments. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/.",
        "id": 226282456
      }
    ],
    "negative_ctxs": [
      {
        "title": "Finding Non-Arbitrary Form-Meaning Systematicity Using String-Metric Learning for Kernel Regression",
        "text": "Arbitrariness of the sign-the notion that the forms of words are unrelated to their meanings-is an underlying assumption of many linguistic theories. Two lines of research have recently challenged this assumption, but they produce differing characterizations of non-arbitrariness in language. Behavioral and corpus studies have confirmed the validity of localized form-meaning patterns manifested in limited subsets of the lexicon. Meanwhile, global (lexicon-wide) statistical analyses instead find diffuse form-meaning systematicity across the lexicon as a whole.We bridge the gap with an approach that can detect both local and global formmeaning systematicity in language. In the kernel regression formulation we introduce, form-meaning relationships can be used to predict words' distributional semantic vectors from their forms. Furthermore, we introduce a novel metric learning algorithm that can learn weighted edit distances that minimize kernel regression error. Our results suggest that the English lexicon exhibits far more global form-meaning systematicity than previously discovered, and that much of this systematicity is focused in localized formmeaning patterns.",
        "id": 908501
      },
      {
        "title": "Generating student feedback from time-series data using Reinforcement Learning",
        "text": "We describe a statistical Natural Language Generation (NLG) method for summarisation of time-series data in the context of feedback generation for students. In this paper, we initially present a method for collecting time-series data from students (e.g. marks, lectures attended) and use example feedback from lecturers in a datadriven approach to content selection. We show a novel way of constructing a reward function for our Reinforcement Learning agent that is informed by the lecturers' method of providing feedback. We evaluate our system with undergraduate students by comparing it to three baseline systems: a rule-based system, lecturerconstructed summaries and a Brute Force system. Our evaluation shows that the feedback generated by our learning agent is viewed by students to be as good as the feedback from the lecturers. Our findings suggest that the learning agent needs to take into account both the student and lecturers' preferences.",
        "id": 5552542
      },
      {
        "title": "How few is too few? Determining the minimum acceptable number of LSA dimensions to visualise text cohesion with Lex Abstract",
        "text": "",
        "id": 39661249
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there a decoder-only language model that does not use a tokenizer and operates on raw text bytes?",
    "positive_ctxs": [
      {
        "title": "ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models",
        "text": "State-of-the-art poetry generation systems are often complex. They either consist of taskspecific model pipelines, incorporate prior knowledge in the form of manually created constraints, or both. In contrast, end-to-end models would not suffer from the overhead of having to model prior knowledge and could learn the nuances of poetry from data alone, reducing the degree of human supervision required. In this work, we investigate end-to-end poetry generation conditioned on styles such as rhyme, meter, and alliteration. We identify and address lack of training data and mismatching tokenization algorithms as possible limitations of past attempts. In particular, we successfully pre-train ByGPT5, a new token-free decoder-only language model, and fine-tune it on a large custom corpus of English and German quatrains annotated with our styles. We show that ByGPT5 outperforms other models such as mT5, ByT5, GPT-2 and ChatGPT, while also being more parameter efficient and performing favorably compared to humans. In addition, we analyze its runtime performance and demonstrate that it is not prone to memorization. We make our code, models, and datasets publicly available.1",
        "id": 254877406
      }
    ],
    "negative_ctxs": [
      {
        "title": "Learning Translations from Monolingual Corpora",
        "text": "This paper proposes a method for a machine translation (MT) system to automatically select and learn translation words, which suit the user's tastes or document fields by using a monolingual corpus manually compiled by the user, in order to achieve high-quality translation. We have constructed a system based on this method and carried out experiments to prove the validity of the proposed method. This learning system has been implemented in Toshiba's \"The Honyaku\" series.",
        "id": 26299595
      },
      {
        "title": "ON THE TRADE-OFF BETWEEN ACTIONABLE EXPLANATIONS AND THE RIGHT TO BE FORGOTTEN",
        "text": "As machine learning (ML) models are increasingly being deployed in high-stakes applications, policymakers have suggested tighter data protection regulations (e.g., GDPR, CCPA).One key principle is the \"right to be forgotten\" which gives users the right to have their data deleted.Another key principle is the right to an actionable explanation, also known as algorithmic recourse, allowing users to reverse unfavorable decisions.To date, it is unknown whether these two principles can be operationalized simultaneously.Therefore, we introduce and study the problem of recourse invalidation in the context of data deletion requests.More specifically, we theoretically and empirically analyze the behavior of popular state-of-the-art algorithms and demonstrate that the recourses generated by these algorithms are likely to be invalidated if a small number of data deletion requests (e.g., 1 or 2) warrant updates of the predictive model.For the setting of differentiable models, we suggest a framework to identify a minimal subset of critical training points which, when removed, maximize the fraction of invalidated recourses.Using our framework, we empirically show that the removal of as little as 2 data instances from the training set can invalidate up to 95 percent of all recourses output by popular state-of-the-art algorithms.Thus, our work raises fundamental questions about the compatibility of \"the right to an actionable explanation\" in the context of the \"right to be forgotten\", while also providing constructive insights on the determining factors of recourse robustness.",
        "id": 251929045
      },
      {
        "title": "CycleKQR: Unsupervised Bidirectional Keyword-Question Rewriting",
        "text": "Users expect their queries to be answered by search systems, regardless of the query's surface form, which include keyword queries and natural questions. Natural Language Understanding (NLU) components of Search and QA systems may fail to correctly interpret semantically equivalent inputs if this deviates from how the system was trained, leading to suboptimal understanding capabilities. We propose the keyword-question rewriting task to improve query understanding capabilities of NLU systems for all surface forms. To achieve this, we present CycleKQR, an unsupervised approach, enabling effective rewriting between keyword and question queries using non-parallel data.Empirically we show the impact on QA performance of unfamiliar query forms for open domain and Knowledge Base QA systems (trained on either keywords or natural language questions). We demonstrate how CycleKQR significantly improves QA performance by rewriting queries into the appropriate form, while at the same time retaining the original semantic meaning of input queries, allowing CycleKQR to improve performance by up to 3% over supervised baselines. Finally, we release a dataset of 66k keyword-question pairs. 1",
        "id": 256461370
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest some work that develops multimodal models with contrastive learning approaches?",
    "positive_ctxs": [
      {
        "title": "UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning",
        "text": "Existed pre-training methods either focus on single-modal tasks or multi-modal tasks, and cannot effectively adapt to each other. They can only utilize single-modal data (i.e., text or image) or limited multi-modal data (i.e., image-text pairs). In this work, we propose a UNIfied-MOdal pre-training architecture, namely UNIMO, which can effectively adapt to both single-modal and multi-modal understanding and generation tasks. Large scale of free text corpus and image collections are utilized to improve the capability of visual and textual understanding, and crossmodal contrastive learning (CMCL) is leveraged to align the textual and visual information into a unified semantic space, over a corpus of image-text pairs augmented with related images and texts. With the help of rich non-paired single-modal data, our model is able to learn more generalizable representations, by allowing textual knowledge and visual knowledge to enhance each other in the unified semantic space. The experimental results show that UNIMO greatly improves the performance of several singlemodal and multi-modal downstream tasks. Our code and pre-trained models are public at https://github.com/PaddlePaddle/ Research/tree/master/NLP/UNIMO.",
        "id": 229924402
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 219306605
      },
      {
        "title": "Propa-L: a Semantic Filtering Service from a Lexical Network Created using Games With A Purpose",
        "text": "This article presents Propa-L, a freely accessible Web service that allows to semantically filter a lexical network. The language resources behind the service are dynamic and created through Games With A Purpose. We show an example of application of this service: the generation of a list of keywords for parental filtering on the Web, but many others can be envisaged. Moreover, the propagation algorithm we present here can be applied to any lexical network, in any language.",
        "id": 43612512
      },
      {
        "title": "Substring-based Transliteration with Conditional Random Fields",
        "text": "Motivated by phrase-based translation research, we present a transliteration system where characters are grouped into substrings to be mapped atomically into the target language. We show how this substring representation can be incorporated into a Conditional Random Field model that uses local context and phonemic information.",
        "id": 167261
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper first conducted the positioned error test for the MAUVE metric?",
    "positive_ctxs": [
      {
        "title": "On the Blind Spots of Model-Based Evaluation Metrics for Text Generation",
        "text": "In this work, we explore a useful but often neglected methodology for robustness analysis of text generation evaluation metrics: stress tests with synthetic data. Basically, we design and synthesize a wide range of potential errors and check whether they result in a commensurate drop in the metric scores. We examine a range of recently proposed evaluation metrics based on pretrained language models, for the tasks of open-ended generation, translation, and summarization. Our experiments reveal interesting insensitivities, biases, or even loopholes in existing metrics. For example, we find that BERTScore is confused by truncation errors in summarization, and MAUVE (built on top of GPT-2) is insensitive to errors at the beginning or middle of generations. Further, we investigate the reasons behind these blind spots and suggest practical workarounds for a more reliable evaluation of text generation. We have released our code and data at https://github. com/cloudygoose/blindspot_nlg. * Equal contribution. Both are corresponding authors. w* in the email refers to washington.",
        "id": 254877323
      }
    ],
    "negative_ctxs": [
      {
        "title": "LEARNING TO ACT FROM ACTIONLESS VIDEOS THROUGH DENSE CORRESPONDENCES",
        "text": "In this work, we present an approach to construct a video-based robot policy capable of reliably executing diverse tasks across different robots and environments from few video demonstrations without using any action annotations. Our method leverages images as a task-agnostic representation, encoding both the state and action information, and text as a general representation for specifying robot goals. By synthesizing videos that \"hallucinate\" robot executing actions and in combination with dense correspondences between frames, our approach can infer the closed-formed action to execute to an environment without the need of any explicit action labels. This unique capability allows us to train the policy solely based on RGB videos and deploy learned policies to various robotic tasks. We demonstrate the efficacy of our approach in learning policies on table-top manipulation and navigation tasks. Additionally, we contribute an open-source framework for efficient video modeling, enabling the training of high-fidelity policy models with four GPUs within a single day. † Work done while Po-Chen Ko is a visiting student at MIT. Project page: https://flow-diffusion.github.io/",
        "id": 263908842
      },
      {
        "title": "The development of tagged Uyghur corpus",
        "text": "The history and development of Uyghur language is introduced. After a brief introduction to the development of Uyghur words, morphology and syntax, we explain our developing of a computer-aided contemporary Uyghur language tagging system. The coverage of this corpus, the resources building, the rules for syncopating and tagging etyma and termination, and the tagging of a corpus using a small tagset are explained. Some practical methods solving problems in Uyghur language tagging are also proposed.Key word: history and developmnet of Uyghur language, Uyghur tagged corpus, Uyghur language tagging system.",
        "id": 15075946
      },
      {
        "title": "",
        "text": "",
        "id": 219310220
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "What paper should I look at if I am interested in the challenges of compositional generalization in the context of semantic parsing, especially regarding the impact of unseen local structures in program outputs?",
    "positive_ctxs": [
      {
        "title": "Unobserved Local Structures Make Compositional Generalization Hard",
        "text": "While recent work has shown that sequence-tosequence models struggle to generalize to new compositions (termed compositional generalization), little is known on what makes compositional generalization hard on a particular test instance. In this work, we investigate the factors that make generalization to certain test instances challenging. We first substantiate that some examples are more difficult than others by showing that different models consistently fail or succeed on the same test instances. Then, we propose a criterion for the difficulty of an example: a test instance is hard if it contains a local structure that was not observed at training time. We formulate a simple decision rule based on this criterion and empirically show it predicts instance-level generalization well across 5 different semantic parsing datasets, substantially better than alternative decision rules. Last, we show local structures can be leveraged for creating difficult adversarial compositional splits and also to improve compositional generalization under limited training budgets by strategically selecting examples for the training set.",
        "id": 246015393
      }
    ],
    "negative_ctxs": [
      {
        "title": "An Effective, Performant Named Entity Recognition System for Noisy Business Telephone Conversation Transcripts",
        "text": "We present a simple yet effective method to train a named entity recognition (NER) model that operates on business telephone conversation transcripts that contain noise due to the nature of spoken conversation and artifacts of automatic speech recognition. We first fine-tune LUKE, a state-of-the-art Named Entity Recognition (NER) model, on a limited amount of transcripts, then use it as the teacher model to teach a smaller DistilBERT-based student model using a large amount of weakly labeled data and a small amount of human-annotated data. The model achieves high accuracy while also satisfying the practical constraints for inclusion in a commercial telephony product: realtime performance when deployed on costeffective CPUs rather than GPUs.",
        "id": 252568200
      },
      {
        "title": "Representational and architectural issues in a limited-domain medical speech translator Mots-clefs : reconnaissance de la parole, traduction de la parole, aide au diagnostic médical",
        "text": "Cet article dresse un aperçu du système MedSLT, un système de traduction de la parole dans le domaine médical pour un vocabulaire limité. Il met l'accent sur le problème du choix du type de représentation pour les constructions temporelles et causales. Nous montrons que celles-ci ne peuvent pas être représentées par des structures plates, généralement utilisées pour ce type d'application, mais qu'elles nécessitent des stuctures plus riches, enchâssées, qui permettent d'obtenir une traduction plus adéquate. Nous expliquons comment produire ces représentations et écrire des règles de traduction économiques qui mettent en correspondance les représentations sources dans la représentation interlingue correspondante Abstract We present an overview of MedSLT, a medium-vocabulary medical speech translation system, focussing on the representational issues that arise when translating temporal and causal concepts. Although flat key/value structures are strongly preferred as semantic representations in speech understanding systems, we argue that it is infeasible to handle the necessary range of concepts using only flat structures. By exploiting the specific nature of the task, we show that it is possible to implement a solution which only slightly extends the representational complexity of the semantic representation language, by permitting an optional single nested level representing a subordinate clause construct. We sketch our solutions to the key problems of producing minimally nested representations using phrase-spotting methods, and writing cleanly structured rule-sets that map temporal and phrasal representations into a canonical interlingual form.Rayner, Bouillon, Santaholma and Nakao",
        "id": 15377337
      },
      {
        "title": "Passing a USA National Bar Exam: a First Corpus for Experimentation",
        "text": "Bar exams provide a key watershed by which legal professionals demonstrate their knowledge of the law and its application. Passing the bar entitles one to practice the law in a given jurisdiction. The bar provides an excellent benchmark for the performance of legal information systems since passing the bar would arguably signal that the system has acquired key aspects of legal reason on a par with a human lawyer. The paper provides a corpus and experimental results with material derived from a real bar exam, treating the problem as a form of textual entailment from the question to an answer. The providers of the bar exam material set the Gold Standard, which is the answer key. The experiments carried out using the 'out of the box' the Excitement Open Platform for textual entailment. The results and evaluation show that the tool can identify wrong answers (non-entailment) with a high F1 score, but it performs poorly in identifying the correct answer (entailment). The results provide a baseline performance measure against which to evaluate future improvements. The reasons for the poor performance are examined, and proposals are made to augment the tool in the future. The corpus facilitates experimentation by other researchers.",
        "id": 12705175
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Is there research on a specialized language model designed to detect mental health issues on social media platforms?",
    "positive_ctxs": [
      {
        "title": "MentalBERT: Publicly Available Pretrained Language Models for Mental Healthcare",
        "text": "Mental health is a critical issue in modern society, and mental disorders could sometimes turn to suicidal ideation without adequate treatment. Early detection of mental disorders and suicidal ideation from social content provides a potential way for effective social intervention. Recent advances in pretrained contextualized language representations have promoted the development of several domain-specific pretrained models and facilitated several downstream applications. However, there are no existing pretrained language models for mental healthcare. This paper trains and releases two pretrained masked language models, i.e., MentalBERT and MentalRoBERTa, to benefit machine learning for the mental healthcare research community. Besides, we evaluate our trained domain-specific models and several variants of pretrained language models on several mental disorder detection benchmarks and demonstrate that language representations pretrained in the target domain improve the performance of mental health detection tasks.",
        "id": 240288892
      }
    ],
    "negative_ctxs": [
      {
        "title": "An Evolutionary Algorithm for Automatic Summarization",
        "text": "This paper proposes a novel method to select sentences for automatic summarization based on an evolutionary algorithm. The algorithm explores candidate summaries space following an objective function computed over ngrams probability distributions of the candidate summary and the source documents. This method does not consider a summary as a stack of independent sentences but as a whole text, and makes use of advances in unsupervised summarization evaluation. We compare this sentence extraction method to one of the best existing methods which is based on integer linear programming, and show its efficiency on three different acknowledged corpora.",
        "id": 3259134
      },
      {
        "title": "Language Identification in Code-Switched Text Using Conditional Random Fields and Babelnet",
        "text": "The paper outlines a supervised approach to language identification in code-switched data, framing this as a sequence labeling task where the label of each token is identified using a classifier based on Conditional Random Fields and trained on a range of different features, extracted both from the training data and by using information from Babelnet and Babelfy.The method was tested on the development dataset provided by organizers of the shared task on language identification in codeswitched data, obtaining tweet level monolingual, code-switched and weighted F1-scores of 94%, 85% and 91%, respectively, with a token level accuracy of 95.8%. When evaluated on the unseen test data, the system achieved 90%, 85% and 87.4% monolingual, code-switched and weighted tweet level F1scores, and a token level accuracy of 95.7%.",
        "id": 10254810
      },
      {
        "title": "The Norwegian Dependency Treebank",
        "text": "The Norwegian Dependency Treebank is a new syntactic treebank for Norwegian Bokmål and Nynorsk with manual syntactic and morphological annotation, developed at the National Library of Norway in collaboration with the University of Oslo. It is the first publically available treebank for Norwegian. This paper presents the core principles behind the syntactic annotation and how these principles were employed in certain specific cases. We then present the selection of texts and distribution between genres, as well as the annotation process and an evaluation of the inter-annotator agreement. Finally, we present the first results of data-driven dependency parsing of Norwegian, contrasting four state-of-the-art dependency parsers trained on the treebank. The consistency and the parsability of this treebank is shown to be comparable to other large treebank initiatives.",
        "id": 7770967
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "What paper first extends rotary positional encoding (RoPE) for camera-geometry encoding in multi-view transformers?",
    "positive_ctxs": [
      {
        "title": "GTA: A GEOMETRY-AWARE ATTENTION MECHANISM FOR MULTI-VIEW TRANSFORMERS",
        "text": "As transformers are equivariant to the permutation of input tokens, encoding the positional information of tokens is necessary for many tasks.However, since existing positional encoding schemes have been initially designed for NLP tasks, their suitability for vision tasks, which typically exhibit different structural properties in their data, is questionable.We argue that existing positional encoding schemes are suboptimal for 3D vision tasks, as they do not respect their underlying 3D geometric structure.Based on this hypothesis, we propose a geometryaware attention mechanism that encodes the geometric structure of tokens as relative transformation determined by the geometric relationship between queries and key-value pairs.By evaluating on multiple novel view synthesis (NVS) datasets in the sparse wide-baseline multi-view setting, we show that our attention, called Geometric Transform Attention (GTA), improves learning efficiency and performance of state-of-the-art transformer-based NVS models without any additional learned parameters and only minor computational overhead.",
        "id": 264147054
      }
    ],
    "negative_ctxs": [
      {
        "title": "Personalized Machine Translation: Predicting Translational Preferences",
        "text": "Machine Translation (MT) has advanced in recent years to produce better translations for clients' specific domains, and sophisticated tools allow professional translators to obtain translations according to their prior edits. We suggest that MT should be further personalized to the end-user level -the receiver or the author of the text -as done in other applications. As a step in that direction, we propose a method based on a recommender systems approach where the user's preferred translation is predicted based on preferences of similar users. In our experiments, this method outperforms a set of non-personalized methods, suggesting that user preference information can be employed to provide better-suited translations for each user.",
        "id": 11060961
      },
      {
        "title": "USAAR-SAPE: An English-Spanish Statistical Automatic Post-Editing System",
        "text": "We describe the USAAR-SAPE English-Spanish Automatic Post-Editing (APE) system submitted to the APE Task organized in the Workshop on Statistical Machine Translation (WMT) in 2015. Our system was able to improve upon the baseline MT system output by incorporating Phrase-Based Statistical MT (PBSMT) technique into the monolingual Statistical APE task (SAPE). The reported final submission crucially involves hybrid word alignment. The SAPE system takes raw Spanish Machine Translation (MT) output provided by the shared task organizers and produces post-edited Spanish text. The parallel data consist of English Text, raw machine translated Spanish output, and their corresponding manually post-edited versions. The major goal of the task is to reduce the post-editing effort by improving the quality of the MT output in terms of fluency and adequacy.",
        "id": 16293762
      },
      {
        "title": "Normalization for Automated Metrics: English and Arabic Speech Translation §",
        "text": "The Defense Advanced Research Projects Agency (DARPA) Spoken Language Communication and Translation System for Tactical Use (TRANSTAC) program has experimented with applying automated metrics to speech translation dialogues. For translations into English, BLEU, TER, and METEOR scores correlate well with human judgments, but scores for translation into Arabic correlate with human judgments less strongly. This paper provides evidence to support the hypothesis that automated measures of Arabic are lower due to variation and inflection in Arabic by demonstrating that normalization operations improve correlation between BLEU scores and Likert-type judgments of semantic adequacyas well as between BLEU scores and human judgments of the successful transfer of the meaning of individual content words from English to Arabic.",
        "id": 10763159
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Can you recommend a foundational paper that provides a scalable framework for generating English sentences with controllable semantic and syntactic attributes for the purpose of augmenting datasets in NLP tasks?",
    "positive_ctxs": [
      {
        "title": "Control, Generate, Augment: A Scalable Framework for Multi-Attribute Text Generation",
        "text": "We introduce CGA, a conditional VAE architecture, to control, generate, and augment text. CGA is able to generate natural English sentences controlling multiple semantic and syntactic attributes by combining adversarial learning with a context-aware loss and a cyclical word dropout routine. We demonstrate the value of the individual model components in an ablation study. The scalability of our approach is ensured through a single discriminator, independently of the number of attributes. We show high quality, diversity and attribute control in the generated sentences through a series of automatic and human assessments. As the main application of our work, we test the potential of this new NLG model in a data augmentation scenario. In a downstream NLP task, the sentences generated by our CGA model show significant improvements over a strong baseline, and a classification performance often comparable to adding same amount of additional real data.",
        "id": 216914087
      }
    ],
    "negative_ctxs": [
      {
        "title": "InfoSurgeon: Cross-Media Fine-grained Information Consistency Checking for Fake News Detection",
        "text": "To defend against neural system-generated fake news, an effective mechanism is urgently needed. We contribute a novel benchmark for fake news detection at the knowledge element level, as well as a solution for this task which incorporates cross-media consistency checking to detect the fine-grained knowledge elements making news articles misinformative. Due to training data scarcity, we also formulate a novel data synthesis method by manipulating knowledge elements within the knowledge graph to generate noisy training data with specific, hard to detect, known inconsistencies. Our detection approach outperforms the state-of-the-art (up to 16.8% absolute accuracy gain), and more critically, yields fine-grained explanations.",
        "id": 236460326
      },
      {
        "title": "Local Space-Time Smoothing for Version Controlled Documents",
        "text": "Unlike static documents, version controlled documents are continuously edited by one or more authors. Such collaborative revision process makes traditional modeling and visualization techniques inappropriate. In this paper we propose a new representation based on local spacetime smoothing that captures important revision patterns. We demonstrate the applicability of our framework using experiments on synthetic and real-world data.",
        "id": 811939
      },
      {
        "title": "Search with Synonyms: Problems and Solutions",
        "text": "Search with synonyms is a challenging problem for Web search, as it can easily cause intent drifting. In this paper, we propose a practical solution to this issue, based on co-clicked query analysis, i.e., analyzing queries leading to clicking the same documents. Evaluation results on Web search queries show that synonyms obtained from this approach considerably outperform the thesaurus based synonyms, such as WordNet, in terms of keeping search intent.",
        "id": 7643917
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there any paper that aligns speech and text embeddings better than CTC training?",
    "positive_ctxs": [
      {
        "title": "WACO: Word-Aligned Contrastive Learning for Speech Translation",
        "text": "End-to-end Speech Translation (E2E ST) aims to directly translate source speech into target text. Existing ST methods perform poorly when only extremely small speech-text data are available for training. We observe that an ST model's performance closely correlates with its embedding similarity between speech and source transcript. In this paper, we propose Word-Aligned COntrastive learning (WACO), a simple and effective method for extremely low-resource speech-to-text translation. Our key idea is bridging word-level representations for both speech and text modalities via contrastive learning. We evaluate WACO and other methods on the MuST-C dataset, a widely used ST benchmark, and on a low-resource direction Maltese-English from IWSLT 2023. Our experiments demonstrate that WACO outperforms the best baseline by 9+ BLEU points with only 1-hour parallel ST data. Code is available at https://github.com/owaski/WACO.",
        "id": 254854514
      }
    ],
    "negative_ctxs": [
      {
        "title": "THE STATISTICAL SIGNIFICANCE OF THE MUC-4 RESULTS ELEMENTS OF HYPOTHESIS TESTING",
        "text": "INTRODUCTIONThe MUC-4 scores of recall, precision, and the F-measures are used to measure the performance of the participating systems. The differences in the scores between any two systems may be due to chance or may be due to a significant difference between the two systems. To rule out the possibility that the difference is due to chance, statistical hypothesis testing is used. The method of hypothesis testing used is a computationally-intensive method known as approximate randomization. The method and the statistical significance of the results for the two MUC-4 test sets, TST3 and TST4, will be discussed in this paper.In our hypothesis testing, our objective was to determine whether a system is characteristically different from another system. This was achieved by comparing two systems to see if their actual difference in performance stands out in comparison with the results for random combinations of their scores. If their actual difference stands out, then we know that this difference could not have arisen by chance.",
        "id": 53750613
      },
      {
        "title": "Still on arguments and adjuncts: the status of the indirect object and the adverbial adjunct relations in Universal Dependencies for Portuguese ⋆",
        "text": "We report the process of annotating verbal arguments and adjuncts in PetroGold, a treebank of the oil & gas domain. The corpus follows the dependencies approach of the Universal Dependencies multilingual project. The argument-adjunct distinction in UD is not a relevant one, and it is up to the contributors of each language to decide how to annotate it in some particular cases. After consulting Portuguese grammars to assist in the annotation of the adverbial adjunct and indirect object relations, we propose a semantic-discursively oriented approach, which was used in the PetroGold annotation and affected 14.8% of the sentences in the treebank. Finally, we present a visualization of the results, showing the distribution of verbs by transitivity in the corpus.",
        "id": 249240914
      },
      {
        "title": "Resource Sharing System for Humanity Researches",
        "text": "The NIJL has developed variety kinds of databases, i.e., catalogue databases, image databases, movie databases, and full text databases. As these systems have been developed under different backgrounds, users have to learn different command for each database. Furthermore, although some databases have similar contents, users cannot access related information unless they understand NIJL database system well. This paper describes NIJL's new resource sharing system, called \"NIJL Collaboration System,\" to solve above problems. The \"NIJL Collaboration System\" is an ongoing project involving data conversion to XML and developing platform independent data manipulation system for a distributed environment. The essential of the project is to introduce XML as a common data description, Dublin Core meta-data as a common access points to databases, and Z39.50 as a common searching protocol. This system enables users to access various sorts of multimedia data in distributed databases on the WEB seamlessly by a single graphical user interface.",
        "id": 2250947
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that investigates enhancing zero-shot question answering with prompts from language models integrated with knowledge graph data?",
    "positive_ctxs": [
      {
        "title": "Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering",
        "text": "Large Language Models (LLMs) are capable of performing zero-shot closed-book question answering tasks, based on their internal knowledge stored in parameters during pre-training. However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive. To this end, we propose to augment the knowledge directly in the input of LLMs. Specifically, we first retrieve the relevant facts to the input question from the knowledge graph based on semantic similarities between the question and its associated facts. After that, we prepend the retrieved facts to the input question in the form of the prompt, which is then forwarded to LLMs to generate the answer. Our framework, Knowledge-Augmented language model PromptING (KAP-ING), requires no model training, thus completely zero-shot. We validate the performance of our KAPING framework on the knowledge graph question answering task, that aims to answer the user's question based on facts over a knowledge graph, on which ours outperforms relevant zero-shot baselines by up to 48% in average, across multiple LLMs of various sizes.",
        "id": 259095910
      },
      {
        "title": "Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering",
        "text": "Large Language Models (LLMs) are capable of performing zero-shot closed-book question answering tasks, based on their internal knowledge stored in parameters during pre-training. However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive. To this end, we propose to augment the knowledge directly in the input of LLMs. Specifically, we first retrieve the relevant facts to the input question from the knowledge graph based on semantic similarities between the question and its associated facts. After that, we prepend the retrieved facts to the input question in the form of the prompt, which is then forwarded to LLMs to generate the answer. Our framework, Knowledge-Augmented language model PromptING (KAP-ING), requires no model training, thus completely zero-shot. We validate the performance of our KAPING framework on the knowledge graph question answering task, that aims to answer the user's question based on facts over a knowledge graph, on which ours outperforms relevant zero-shot baselines by up to 48% in average, across multiple LLMs of various sizes.",
        "id": 260063238
      }
    ],
    "negative_ctxs": [
      {
        "title": "Automatic Acronym Acquisition and Term Variation Management within Domain-Specific Texts *",
        "text": "In this paper we present a framework for the effective management of terms and their variants that are automatically acquired from domain-specific texts. In our approach, the term variant recognition is incorporated in the automatic term retrieval process by taking into account orthographical, morphological, syntactic, lexico-semantic and pragmatic term variations. In particular, we address acronyms as a common way of introducing term variants in scientific papers. We describe a method for the automatic acquisition of newly introduced acronyms and the mapping to their 'meanings', i.e. the corresponding terms. The proposed three-step procedure is based on morpho-syntactic constraints that are commonly used in acronym definitions. First, acronym definitions containing an acronym and the corresponding term are retrieved. These two elements are matched in the second step by performing morphological analysis of words and combining forms constituting the term. The problems of acronym variation and acronym ambiguity are addressed in the third step by establishing classes of term variants that correspond to specific concepts. We present the results of the acronym acquisition in the domain of molecular biology: the precision of the method ranged from 94% to 99% depending on the size of the corpus used for evaluation, whilst the recall was 73%. * This research is a part of the BioPATH research project coordinated by LION BioScience (http://www.lionbioscience.com) and funded by German Ministry of Research.",
        "id": 9818857
      },
      {
        "title": "A Knowledge-Graph-Based Intrinsic Test for Benchmarking Medical Concept Embeddings and Pretrained Language Models",
        "text": "Using language models created from large data sources has improved the performance of several deep learning-based architectures, obtaining state-of-the-art results in several NLP extrinsic tasks. However, little research is related to creating intrinsic tests that allow us to compare the quality of different language models when obtaining contextualized embeddings. This gap increases even more when working on specific domains in languages other than English. This paper proposes a novel graph-based intrinsic test that allows us to measure the quality of different language models in clinical and biomedical domains in Spanish. Our results show that our intrinsic test performs better for clinical and biomedical language models than a general one. Also, it correlates with better outcomes for a NER task using a probing model over contextualized embeddings. We hope our work will help the clinical NLP research community to evaluate and compare new language models in other languages and find the most suitable models for solving downstream tasks.",
        "id": 256461352
      },
      {
        "title": "Making Sense of Sound: Unsupervised Topic Segmentation over Acoustic Input",
        "text": "We address the task of unsupervised topic segmentation of speech data operating over raw acoustic information. In contrast to existing algorithms for topic segmentation of speech, our approach does not require input transcripts. Our method predicts topic changes by analyzing the distribution of reoccurring acoustic patterns in the speech signal corresponding to a single speaker. The algorithm robustly handles noise inherent in acoustic matching by intelligently aggregating information about the similarity profile from multiple local comparisons. Our experiments show that audio-based segmentation compares favorably with transcriptbased segmentation computed over noisy transcripts. These results demonstrate the desirability of our method for applications where a speech recognizer is not available, or its output has a high word error rate.",
        "id": 6996024
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "which paper first focuses on addressing the over-smoothing issue for sentence embedding?",
    "positive_ctxs": [
      {
        "title": "Alleviating Over-smoothing for Unsupervised Sentence Representation",
        "text": "Currently, learning better unsupervised sentence representations is the pursuit of many natural language processing communities. Lots of approaches based on pre-trained language models (PLMs) and contrastive learning have achieved promising results on this task. Experimentally, we observe that the over-smoothing problem reduces the capacity of these powerful PLMs, leading to suboptimal sentence representations. In this paper, we present a Simple method named Self-Contrastive Learning (SSCL) to alleviate this issue, which samples negatives from PLMs intermediate layers, improving the quality of the sentence representation. Our proposed method is quite simple and can be easily extended to various state-of-the-art models for performance boosting, which can be seen as a plugand-play contrastive framework for learning unsupervised sentence representation. Extensive results prove that SSCL brings the superior performance improvements of different strong baselines (e.g., BERT and SimCSE) on Semantic Textual Similarity and Transfer datasets. Our codes are available at https: //github.com/nuochenpku/SSCL.",
        "id": 258588364
      }
    ],
    "negative_ctxs": [
      {
        "title": "Perception and Analysis of a Reiterant Speech Paradigm: a Functional Diagnostic of Synthetic Prosody",
        "text": "A set of perception experiments, using reiterant speech, were designed to carry out a diagnostic of the segmentation / hierarchisation linguistic function of prosody. The prosodic parameters of F0, syllabic duration and intensity of the stimuli used during this experiment were extracted. Several dissimilarity measures (Correlation, root-mean-square distance and mutual information) were used to match the results of the subjective experiment. This comparison of the listeners' perception with acoustic parameters is intended to underline the acoustic keys used by listeners to judge the adequacy of prosody to perform a given linguistic function.",
        "id": 69859
      },
      {
        "title": "Learning with AMIGO: Adversarially Motivated Intrinsic Goals",
        "text": "A key challenge for reinforcement learning (RL) consists of learning in environments with sparse extrinsic rewards. In contrast to current RL methods, humans are able to learn new skills with little or no reward by using various forms of intrinsic motivation. We propose AMIGO, a novel agent incorporating a goalgenerating teacher that proposes Adversarially Motivated Intrinsic GOals to train a goal-conditioned \"student\" policy in the absence of (or alongside) environment reward. Specifically, through a simple but effective \"constructively adversarial\" objective, the teacher learns to propose increasingly challenging-yet achievablegoals that allow the student to learn general skills for acting in a new environment, independent of the task to be solved. We show that our method generates a natural curriculum of self-proposed goals which ultimately allows the agent to solve challenging procedurally-generated tasks where other forms of intrinsic motivation and state-of-the-art RL methods fail. * Work done during an internship at Facebook AI Research.",
        "id": 219965999
      },
      {
        "title": "",
        "text": "",
        "id": 227905391
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "In the context of natural language processing, I am looking for research that explores the relationship between a model's prediction entropy and its tendencies to copy existing text versus generating novel content. Can you recommend a paper?",
    "positive_ctxs": [
      {
        "title": "Understanding Neural Abstractive Summarization Models via Uncertainty",
        "text": "An advantage of seq2seq abstractive summarization models is that they generate text in a free-form manner, but this flexibility makes it difficult to interpret model behavior. In this work, we analyze summarization decoders in both blackbox and whitebox ways by studying on the entropy, or uncertainty, of the model's token-level predictions. For two strong pretrained models, PEGASUS (Zhang et al., 2020) and BART (Lewis et al., 2020) on two summarization datasets, we find a strong correlation between low prediction entropy and where the model copies tokens rather than generating novel text. The decoder's uncertainty also connects to factors like sentence position and syntactic distance between adjacent pairs of tokens, giving a sense of what factors make a context particularly selective for the model's next output token. Finally, we study the relationship of decoder uncertainty and attention behavior to understand how attention gives rise to these observed effects in the model. We show that uncertainty is a useful perspective for analyzing summarization and text generation models more broadly. 1",
        "id": 222378161
      }
    ],
    "negative_ctxs": [
      {
        "title": "Robust Multi-bit Natural Language Watermarking through Invariant Features",
        "text": "Recent years have witnessed a proliferation of valuable original natural language contents found in subscription-based media outlets, web novel platforms, and outputs of large language models. However, these contents are susceptible to illegal piracy and potential misuse without proper security measures. This calls for a secure watermarking system to guarantee copyright protection through leakage tracing or ownership identification. To effectively combat piracy and protect copyrights, a multi-bit watermarking framework should be able to embed adequate bits of information and extract the watermarks in a robust manner despite possible corruption. In this work, we explore ways to advance both payload and robustness by following a well-known proposition from image watermarking and identify features in natural language that are invariant to minor corruption. Through a systematic analysis of the possible sources of errors, we further propose a corruption-resistant infill model. Our full method improves upon the previous work on robustness by +16.8% point on average on four datasets, three corruption types, and two corruption ratios. 1",
        "id": 259129912
      },
      {
        "title": "Space Efficient Context Encoding for Non-Task-Oriented Dialogue Generation with Graph Attention Transformer",
        "text": "To improve the coherence and knowledge retrieval capabilities of non-task-oriented dialogue systems, recent Transformer-based models aim to integrate fixed background context. This often comes in the form of knowledge graphs, and the integration is done by creating pseudo utterances through paraphrasing knowledge triples, added into the accumulated dialogue context. However, the context length is fixed in these architectures, which restricts how much background or dialogue context can be kept. In this work, we propose a more concise encoding for background context structured in the form of knowledge graphs, by expressing the graph connections through restrictions on the attention weights. The results of our human evaluation show that this encoding reduces space requirements without negative effects on the precision of reproduction of knowledge and perceived consistency. Further, models trained with our proposed context encoding generate dialogues that are judged to be more comprehensive and interesting.ReferencesKurt Bollacker, Robert Cook, and Patrick Tufts. 2007.Freebase: A shared database of structured general human knowledge.",
        "id": 236317402
      },
      {
        "title": "NTUA-SLP at SemEval-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning",
        "text": "In this paper we present deep-learning models that submitted to the SemEval-2018 Task 1 competition: \"Affect in Tweets\". We participated in all subtasks for English tweets. We propose a Bi-LSTM architecture equipped with a multi-layer self attention mechanism. The attention mechanism improves the model performance and allows us to identify salient words in tweets, as well as gain insight into the models making them more interpretable. Our model utilizes a set of word2vec word embeddings trained on a large collection of 550 million Twitter messages, augmented by a set of word affective features. Due to the limited amount of task-specific training data, we opted for a transfer learning approach by pretraining the Bi-LSTMs on the dataset of Semeval 2017, Task 4A. The proposed approach ranked 1 st in Subtask E \"Multi-Label Emotion Classification\", 2 nd in Subtask A \"Emotion Intensity Regression\" and achieved competitive results in other subtasks.",
        "id": 4933708
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "What are some soft-constrained methods proposed in the literature for terminology translation in neural machine translation systems, and how do they differ from hard-constrained decoding methods that might degrade translation quality or increase complexity?",
    "positive_ctxs": [
      {
        "title": "Training Neural Machine Translation To Apply Terminology Constraints",
        "text": "This paper proposes a novel method to inject custom terminology into neural machine translation at run time. Previous works have mainly proposed modifications to the decoding algorithm in order to constrain the output to include run-time-provided target terms. While being effective, these constrained decoding methods add, however, significant computational overhead to the inference step, and, as we show in this paper, can be brittle when tested in realistic conditions. In this paper we approach the problem by training a neural MT system to learn how to use custom terminology when provided with the input. Comparative experiments show that our method is not only more effective than a state-of-the-art implementation of constrained decoding, but is also as fast as constraint-free decoding.",
        "id": 174798321
      },
      {
        "title": "PROMT Systems for WMT21 Terminology Translation Task",
        "text": "This paper describes the PROMT submissions for the WMT21 Terminology Translation Task. We participate in two directions: English to French and English to Russian. Our final submissions are MarianNMT-based neural systems. We present two technologies for terminology translation: a modification of the Dinu et al.(2019)soft-constrained approach and our own approach called PROMT Smart Neural Dictionary (SmartND). We achieve good results in both directions.",
        "id": 245855898
      }
    ],
    "negative_ctxs": [
      {
        "title": "Valency Frames of Czech Verbs in VALLEX 1.0",
        "text": "The Valency Lexicon of Czech Verbs, Version 1.0 (VALLEX 1.0) is a collection of linguistically annotated data and documentation, resulting from an attempt at formal description of valency frames of Czech verbs. VALLEX 1.0 is closely related to Prague Dependency Treebank. In this paper, the context in which VALLEX came into existence is briefly outlined, and also three similar projects for English verbs are mentioned. The core of the paper is the description of the logical structure of the VALLEX data. Finally, we suggest a few directions of the future research.2 Besides VALLEX, a larger valency lexicon (called PDT-VALLEX,(Hajič et al., 2003)) has been created during the annotation of PDT. PDT-VALLEX contains more verbs (5200 verbs), but only frames occuring in PDT, whereas in VALLEX the verbs are analyzed in the whole complexity, in all their meanings. Moreover, richer information is assigned to particular valency frames in VALLEX.3",
        "id": 1801774
      },
      {
        "title": "A New Minimally-Supervised Framework for Domain Word Sense Disambiguation",
        "text": "We present a new minimally-supervised framework for performing domain-driven Word Sense Disambiguation (WSD). Glossaries for several domains are iteratively acquired from the Web by means of a bootstrapping technique. The acquired glosses are then used as the sense inventory for fullyunsupervised domain WSD. Our experiments, on new and gold-standard datasets, show that our wide-coverage framework enables highperformance results on dozens of domains at a coarse and fine-grained level.",
        "id": 16502540
      },
      {
        "title": "Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning",
        "text": "Unlike the problems of part-of-speech tagging and parsing, where commonly utilized training and test sets such as the Brown Corpus and Penn Treebank have existed for a number of years, evaluation of word sense disambiguation sytems is not yet standardized. In fact, most previous work in sense disambiguation has tended to use different sets of polysemous words, different sense inventories, different evaluation metrics and different test corpora. This working session will address these problems and seek solutions to them. Examples of issues for discussion include:• How should part-of-speech-level distinctions be treated when evaluating WSD systems?• How should sense inventories be defined so as not to be biased in favor of certain disambiguation methods, such as those based on selectional restriction, topic codes, hierarchical ontologies, or aligned multilingual corpora? Or are such biases ok?• What evaluation metrics are appropriate for the WSD ta~k?• What characteristics should common test suites exhibit? How and by whom should they be developed?• Would a MUC-style competitive evaluation program be beneficial or detrimental to progress in the WSD field?• What special problems exist when evaluating WSD performance on verbs?• What special problems exist when evaluating WSD performance in a multi-lingual setting?o What additional issues arise in evaluating more complex semantic tagging, going beyond sense disambiguation as traditionally defined?• How should regular polysemy and metaphor be treated in WSD evaluation?• Can a common evaluation framework satisfy the needs and limitations of both supervised and unsupervised sense disambiguation methods?ReferencesC. Leacock, G. Towell and E. Voorhees. 1993. Corpus-based statistical sense resolution.",
        "id": 41603009
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "How to better attract readers to news articles by generating personalized headlines?",
    "positive_ctxs": [
      {
        "title": "Generating User-Engaging News Headlines",
        "text": "The potential choices for news article headlines are enormous, and finding the right balance between conveying the essential message and capturing the reader's attention is key to effective headlining. However, presenting the same news headline to all readers is a suboptimal strategy, because it does not take into account the different preferences and interests of diverse readers, who may be confused about why a particular article has been recommended to them and do not see a clear connection between their interests and the recommended article. In this paper, we present a novel framework that addresses these challenges by incorporating user profiling to generate personalized headlines, and a combination of automated and human evaluation methods to determine user preference for personalized headlines. Our framework utilizes a learnable relevance function to assign personalized signature phrases to users based on their reading histories, which are then used to personalize headline generation. Through extensive evaluation, we demonstrate the effectiveness of our proposed framework in generating personalized headlines that meet the needs of a diverse audience. Our framework has the potential to improve the efficacy of news recommendations and facilitate creation of personalized content.",
        "id": 259370694
      }
    ],
    "negative_ctxs": [
      {
        "title": "Contextualization of Morphological Inflection",
        "text": "Critical to natural language generation is the production of correctly inflected text. In this paper, we isolate the task of predicting a fully inflected sentence from its partially lemmatized version. Unlike traditional morphological inflection or surface realization, our task input does not provide \"gold\" tags that specify what morphological features to realize on each lemmatized word; rather, such features must be inferred from sentential context. We develop a neural hybrid graphical model that explicitly reconstructs morphological features before predicting the inflected forms, and compare this to a system that directly predicts the inflected forms without relying on any morphological annotation. We experiment on several typologically diverse languages from the Universal Dependencies treebanks, showing the utility of incorporating linguisticallymotivated latent variables into NLP models.1 This morphological feature is inherent in the sense ofBooij (1996).",
        "id": 146120740
      },
      {
        "title": "Language choice models for microplanning and readability",
        "text": "This paper describes the construction of language choice models for the microplanning of discourse relations in a Natural Language Generation system that attempts to generate appropriate texts for users with varying levels of literacy. The models consist of constraint satisfaction problem graphs that have been derived from the results of a corpus analysis. The corpus that the models are based on was written for good readers. We adapted the models for poor readers by allowing certain constraints to be tightened, based on psycholinguistic evidence. We describe how the design of microplanner is evolving. We discuss the compromises involved in generating more readable textual output and implications of our design for NLG architectures. Finally we describe plans for future work.",
        "id": 7637949
      },
      {
        "title": "Shallow Semantic Parsing for Spoken Language Understanding",
        "text": "Most Spoken Dialog Systems are based on speech grammars and frame/slot semantics. The semantic descriptions of input utterances are usually defined ad-hoc with no ability to generalize beyond the target application domain or to learn from annotated corpora. The approach we propose in this paper exploits machine learning of frame semantics, borrowing its theoretical model from computational linguistics. While traditional automatic Semantic Role Labeling approaches on written texts may not perform as well on spoken dialogs, we show successful experiments on such porting. Hence, we design and evaluate automatic FrameNet-based parsers both for English written texts and for Italian dialog utterances. The results show that disfluencies of dialog data do not severely hurt performance. Also, a small set of FrameNet-like manual annotations is enough for realizing accurate Semantic Role Labeling on the target domains of typical Dialog Systems.",
        "id": 6027270
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper proposed the integration of human translators' considerations, such as length control, rhyme type control and suggestion, and enhancing compatibility between translation output and unseen melodies, into the design of machine translation models when translating lyrics?",
    "positive_ctxs": [
      {
        "title": "Songs Across Borders: Singable and Controllable Neural Lyric Translation",
        "text": "The development of general-domain neural machine translation (NMT) methods has advanced significantly in recent years, but the lack of naturalness and musical constraints in the outputs makes them unable to produce singable lyric translations. This paper bridges the singability quality gap by formalizing lyric translation into a constrained translation problem, converting theoretical guidance and practical techniques from translatology literature to promptdriven NMT approaches, exploring better adaptation methods, and instantiating them to an English-Chinese lyric translation system. Our model achieves 99.85%, 99.00%, and 95.52% on length accuracy, rhyme accuracy, and word boundary recall. In our subjective evaluation, our model shows a 75% relative enhancement on overall quality, compared against naive finetuning 1 .",
        "id": 258947268
      }
    ],
    "negative_ctxs": [
      {
        "title": "Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning",
        "text": "Fine-tuning of pre-trained transformer models has become the standard approach for solving common NLP tasks(Devlin et al., 2019). Most of the existing approaches rely on a randomly initialized classifier on top of such networks. We argue that this fine-tuning procedure is sub-optimal as the pre-trained model has no prior on the specific classifier labels, while it might have already learned an intrinsic textual representation of the task. In this paper, we introduce a new scoring method that casts a plausibility ranking task in a full-text format and leverages the masked language modeling head tuned during the pre-training phase. We study commonsense reasoning tasks where the model must rank a set of hypotheses given a premise, focusing on the COPA (Gordon et al., 2012), Swag (Zellers et al., 2018), HellaSwag (Zellers et al., 2019) and CommonsenseQA (Talmor et al., 2019) datasets.By exploiting our scoring method without fine-tuning, we are able to produce strong baselines (e.g. 80% test accuracy on COPA) that are comparable to supervised approaches. Moreover, when fine-tuning directly on the proposed scoring function, we show that our method provides a much more stable training phase across random restarts (e.g ×10 standard deviation reduction on COPA test accuracy) and requires less annotated data than the standard classifier approach to reach equivalent performances.",
        "id": 216641845
      },
      {
        "title": "Automatic Selection of Reference Pages in Wikipedia for Improving Targeted Entities Disambiguation",
        "text": "In Targeted Entity Disambiguation setting, we take (i) a set of entity names which belong to the same domain (target entities), (ii) candidate mentions of the given entities which are texts that contain the target entities as input, and then determine which ones are true mentions of \"target entity\". For example, given the names of IT companies, including Apple, we determine Apple in a mention denotes an IT company or not. Prior work proposed a graph based model. This model ranks all candidate mentions based on scores which denote the degree of relevancy to target entities. Furthermore, this graph based model could utilize reference pages of target entities. However, human annotators must select reference pages in advance. We propose an automatic method that can select reference pages. We formalize the selection problem of reference pages as an Integer Linear Programming problem. We show that our model works as well as the prior work that manually selected reference pages.",
        "id": 5708734
      },
      {
        "title": "",
        "text": "",
        "id": 220058123
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Can you suggest some recent datasets that have been used for studying stance detection in tweets, particularly those targeting specific individuals and events since 2020?",
    "positive_ctxs": [
      {
        "title": "P-Stance: A Large Dataset for Stance Detection in Political Domain",
        "text": "Stance detection determines whether the author of a text is in favor of, against or neutral to a specific target and provides valuable insights into important events such as presidential election. However, progress on stance detection has been hampered by the absence of large annotated datasets. In this paper, we present P-STANCE, a large stance detection dataset in the political domain, which contains 21,574 labeled tweets. We provide a detailed description of the newly created dataset and develop deep learning models on it. Our best model achieves a macro-average F1-score of 80.53%, which we improve further by using semi-supervised learning. Moreover, our P-STANCE dataset can facilitate research in the fields of cross-domain stance detection such as cross-target stance detection where a classifier is adapted from a different but related target. We publicly release our dataset and code. 1",
        "id": 236477909
      },
      {
        "title": "Stance Detection in COVID-19 Tweets",
        "text": "The prevalence of the COVID-19 pandemic in day-to-day life has yielded large amounts of stance detection data on social media sites, as users turn to social media to share their views regarding various issues related to the pandemic, e.g. stay at home mandates and wearing face masks when out in public. We set out to make use of this data by collecting the stance expressed by Twitter users, with respect to topics revolving around the pandemic. We annotate a new stance detection dataset, called COVID-19-Stance. Using this newly annotated dataset, we train several established stance detection models to ascertain a baseline performance for this specific task. To further improve the performance, we employ self-training and domain adaptation approaches to take advantage of large amounts of unlabeled data and existing stance detection datasets. The dataset, code, and other resources are available on GitHub. 1",
        "id": 236460157
      }
    ],
    "negative_ctxs": [
      {
        "title": "Benchmarking Automated Review Response Generation for the Hospitality Domain",
        "text": "Online customer reviews are of growing importance for many businesses in the hospitality industry, particularly restaurants and hotels. Managerial responses to such reviews provide businesses with the opportunity to influence the public discourse and to attain improved ratings over time. However, responding to each and every review is a time-consuming endeavour. Therefore, we investigate automatic generation of review responses in the hospitality domain for two languages, English and German.We apply an existing system, originally proposed for review response generation for smartphone apps. This approach employs an extended neural network sequence-to-sequence architecture and performs well in the original domain. However, as shown through our experiments, when applied to a new domain, such as hospitality, performance drops considerably. Therefore, we analyse potential causes for the differences in performance and provide evidence to suggest that review response generation in the hospitality domain is a more challenging task and thus requires further study and additional domain adaptation techniques.",
        "id": 227230307
      },
      {
        "title": "WikiWalk: Random walks on Wikipedia for Semantic Relatedness",
        "text": "Computing semantic relatedness of natural language texts is a key component of tasks such as information retrieval and summarization, and often depends on knowledge of a broad range of real-world concepts and relationships. We address this knowledge integration issue by computing semantic relatedness using personalized PageRank (random walks) on a graph derived from Wikipedia. This paper evaluates methods for building the graph, including link selection strategies, and two methods for representing input texts as distributions over the graph nodes: one based on a dictionary lookup, the other based on Explicit Semantic Analysis. We evaluate our techniques on standard word relatedness and text similarity datasets, finding that they capture similarity information complementary to existing Wikipedia-based relatedness measures, resulting in small improvements on a stateof-the-art measure.",
        "id": 805379
      },
      {
        "title": "",
        "text": "",
        "id": 218974163
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there any paper that studies a teacher AI inferring mental states of a student role in a role-playing game setup using reinforcement learning?",
    "positive_ctxs": [
      {
        "title": "I Cast Detect Thoughts: Learning to Converse and Guide with Intents and Theory-of-Mind in Dungeons and Dragons",
        "text": "We propose a novel task, G4C, to study teacher-student natural language interactions in a goal-driven and grounded environment. Dungeons and Dragons (D&D), a role-playing game, provides an ideal setting to investigate such interactions. Here, the Dungeon Master (DM), i.e., the teacher, guides the actions of several players-students, each with their own personas and abilities-to achieve shared goals grounded in a fantasy world. Our approach is to decompose and model these interactions into (1) the DM's intent to guide players towards a given goal; (2) the DM's guidance utterance to the players expressing this intent; and (3) a theory-of-mind (ToM) model that anticipates the players' reaction to the guidance one turn into the future. We develop a novel reinforcement learning (RL) method for training a DM that generates guidance for players by rewarding utterances where the intent matches the ToM-anticipated player actions. Human and automated evaluations show that a DM trained to explicitly model intents and incorporate ToM of the players using RL generates better-quality guidance that is 3x more likely to fulfill the DM's intent than a vanilla natural language generation (NLG) approach.",
        "id": 258987457
      }
    ],
    "negative_ctxs": [
      {
        "title": "A Graph-Based Approach Leveraging Posts and Reactions for Detecting Rumors on Online Social Media",
        "text": "In this paper, we present a novel graph-based contextual and semantic learning approach for detecting rumors on online social media. The underlying hypothesis is that social media entities are intertwined, and if an event unfolds then similar narratives or user reactions with common interests get circulated. The proposed approach uses tweets and their reactions to understand the underlying interaction patterns and exploits the textual and latent information. Textual data is modeled as a words cooccurrence graph, which produces two prevalent categories of words -substantial words and bridge words. These words serve as building blocks for constructing contextual patterns for rumor detection by computing node-level statistical measures. The contextual patterns are further enriched by identifying negative emotions and inquisitive aspects in the reactions. The patterns are finally ranked and only top-k check-worthy patterns are used for feature generation. In order to preserve the semantic relations, we use word-level GloVe embedding trained over a Twitter dataset. The proposed approach is evaluated over a publicly available PHEME dataset, and compared with various baselines and SOTA techniques. The experimental results are promising and the proposed approach seems useful for rumor detection on online social media.",
        "id": 258463972
      },
      {
        "title": "Memory-Efficient Differentiable Transformer Architecture Search",
        "text": "Differentiable architecture search (DARTS) is successfully applied in many vision tasks. However, directly using DARTS for Transformers is memory-intensive, which renders the search process infeasible. To this end, we propose a multi-split reversible network and combine it with DARTS. Specifically, we devise a backpropagation-with-reconstruction algorithm so that we only need to store the last layer's outputs. By relieving the memory burden for DARTS, it allows us to search with larger hidden size and more candidate operations. We evaluate the searched architecture on three sequence-to-sequence datasets, i.e., WMT'14 English-German, WMT'14 English-French, and WMT'14 English-Czech. Experimental results show that our network consistently outperforms standard Transformers across the tasks. Moreover, our method compares favorably with big-size Evolved Transformers, reducing search computation by an order of magnitude.",
        "id": 235254260
      },
      {
        "title": "Suicide Risk Assessment with Multi-level Dual-Context Language and BERT",
        "text": "Mental health predictive systems typically model language as if from a single context (e.g. Twitter posts, status updates, or forum posts) and often limited to a single level of analysis (e.g. either the message-level or userlevel). Here, we bring these pieces together to explore the use of open-vocabulary (BERT embeddings, topics) and theoretical features (emotional expression lexica, personality) for the task of suicide risk assessment on support forums (the CLPsych-2019 Shared Task). We used dual context based approaches (modeling content from suicide forums separate from other content), built over both traditional ML models as well as a novel dual RNN architecture with user-factor adaptation. We find that while affect from the suicide context distinguishes with no-risk from those with \"anyrisk\", personality factors from the non-suicide contexts provide distinction of the levels of risk: low, medium, and high risk. Within the shared task, our dual-context approach (listed as SBU-HLAB in the official results) achieved state-of-the-art performance predicting suicide risk using a combination of suicide-context and non-suicide posts (Task B), achieving an F1 score of 0.50 over hidden test set labels.",
        "id": 198184729
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "What are some data-efficient ways to learn text embeddings thru contrastive learning?",
    "positive_ctxs": [
      {
        "title": "Composition-contrastive Learning for Sentence Embeddings",
        "text": "Vector representations of natural language are ubiquitous in search applications. Recently, various methods based on contrastive learning have been proposed to learn textual representations from unlabelled data; by maximizing alignment between minimally-perturbed embeddings of the same text, and encouraging a uniform distribution of embeddings across a broader corpus. Differently, we propose maximizing alignment between texts and a composition of their phrasal constituents. We consider several realizations of this objective and elaborate the impact on representations in each case. Experimental results on semantic textual similarity tasks show improvements over baselines that are comparable with state-of-the-art approaches. Moreover, this work is the first to do so without incurring costs in auxiliary training objectives or additional network parameters. 1",
        "id": 259370776
      }
    ],
    "negative_ctxs": [
      {
        "title": "Locating Boundaries for Prosodic Constituents in Unrestricted Mandarin Texts",
        "text": "This paper proposes a three-tier prosodic hierarchy, including prosodic word, intermediate phrase and intonational phrase tiers, for Mandarin that emphasizes the use of the prosodic word instead of the lexical word as the basic prosodic unit. Both the surface difference and perceptual difference show that this is helpful for achieving high naturalness in text-to-speech conversion. Three approaches, the basic CART approach, the bottom-up hierarchical approach and the modified hierarchical approach, are presented for locating the boundaries of three prosodic constituents in unrestricted Mandarin texts. Two sets of features are used in the basic CART method: one contains syntactic phrasal information and the other does not. The one with syntactic phrasal information results in about a 1% increase in accuracy and an 11% decrease in error-cost. The performance of the modified hierarchical method produces the highest accuracy, 83%, and lowest error cost when no syntactic phrasal information is provided. It shows advantages in detecting the boundaries of intonational phrases at locations without breaking punctuation. 71.1% precision and 52.4% recall are achieved. Experiments on acceptability reveal that only 26% of the mis-assigned break indices are real infelicitous errors, and that the perceptual difference between the automatically assigned break indices and the manually annotated break indices are small.",
        "id": 12168738
      },
      {
        "title": "SEQUENTIAL REPTILE: INTER-TASK GRADIENT ALIGNMENT FOR MULTILINGUAL LEARNING",
        "text": "Multilingual models jointly pretrained on multiple languages have achieved remarkable performance on various multilingual downstream tasks. Moreover, models finetuned on a single monolingual downstream task have shown to generalize to unseen languages. In this paper, we first show that it is crucial for those tasks to align gradients between them in order to maximize knowledge transfer while minimizing negative transfer. Despite its importance, the existing methods for gradient alignment either have a completely different purpose, ignore inter-task alignment, or aim to solve continual learning problems in rather inefficient ways. As a result of the misaligned gradients between tasks, the model suffers from severe negative transfer in the form of catastrophic forgetting of the knowledge acquired from the pretraining. To overcome the limitations, we propose a simple yet effective method that can efficiently align gradients between tasks. Specifically, we perform each inner-optimization by sequentially sampling batches from all the tasks, followed by a Reptile outer update. Thanks to the gradients aligned between tasks by our method, the model becomes less vulnerable to negative transfer and catastrophic forgetting. We extensively validate our method on various multi-task learning and zero-shot cross-lingual transfer tasks, where our method largely outperforms all the relevant baselines we consider. Published as a conference paper at ICLR 2022 BERT Pretrained Model High MTL Loss High Cos. Sim. Low MTL Loss Low Cos. Sim. e.g.) Seq. Reptile e.g.) Base MTL Reptile PCGrad RecAdam e.g.) Early-stopping Better trade-off b/w MTL loss vs. cosine sim.Worse trade-offFigure 1: Concepts. Black arrows denote finetuning processes. The darker the part of the arrows, the lower the MTL loss. Upper and bottom path shows better and worse trade-off, respectively. Colored arrows denote task gradients. Blue and red color shows high and low cosine similarity, respectively. We demonstrate this concept with the actual experimental results inFig. 7a.",
        "id": 238408412
      },
      {
        "title": "YUN-HPCC at SemEval-2019 Task 3: Multi-Step Ensemble Neural Network for Sentiment Analysis in Textual Conversation",
        "text": "This paper describes our approach to the emotion detection of Twitter textual conversations based on deep learning. We analyze the syntax, abbreviations, and informal-writing of Twitter; and perform perfect data preprocessing on the data to convert them to normative text. We apply a multi-step ensemble strategy to solve the problem of extremely unbalanced data in the training set. This is achieved by taking the GloVe and ELMo word vectors as input into a combination model with four different deep neural networks. The experimental results from the development dataset demonstrate that the proposed model exhibits a strong generalization ability. For evaluation on the test dataset, we integrated the results using the stacking ensemble learning approach and achieved competitive results. According to the final official review, the results of our model achieved micro-F 1 score of about 0.7588 on the final evaluation.",
        "id": 184482660
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there a dialogue dataset where a speaker's utterance is grounded in their persona, consisting of image-text pairs representing their episodic memories?",
    "positive_ctxs": [
      {
        "title": "MPCHAT: Towards Multimodal Persona-Grounded Conversation",
        "text": "In order to build self-consistent personalized dialogue agents, previous research has mostly focused on textual persona that delivers personal facts or personalities. However, to fully describe the multi-faceted nature of persona, image modality can help better reveal the speaker's personal characteristics and experiences in episodic memory (Rubin et al.,  2003; Conway, 2009). In this work, we extend persona-based dialogue to the multimodal domain and make two main contributions. First, we present the first multimodal persona-based dialogue dataset named MPCHAT, which extends persona with both text and images to contain episodic memories. Second, we empirically show that incorporating multimodal persona, as measured by three proposed multimodal persona-grounded dialogue tasks (i.e., next response prediction, grounding persona prediction, and speaker identification), leads to statistically significant performance improvements across all tasks. Thus, our work highlights that multimodal persona is crucial for improving multimodal dialogue comprehension, and our MPCHAT serves as a high-quality resource for this research. . 2022.Fine-grained image captioning with CLIP reward. In NAACL Findings.J Clement. 2022. Regional distribution of desktop traffic to reddit.com as of february 2022 by country,. 3363 Martin A. Conway. 2005. Memory and the self. J. Mem. Lang., 53(4):594-628. Martin A. Conway. 2009. Episodic memories. Neuropsychologia, 47(11):2305-2313. Michael A. Covington and Joe D. McFall. 2010. Cutting the gordian knot: The moving-average type-token ratio (mattr). J. Quant. Linguist., 17(2):94-100.",
        "id": 258959116
      }
    ],
    "negative_ctxs": [
      {
        "title": "LOGAN: Local Group Bias Detection by Clustering",
        "text": "Machine learning techniques have been widely used in natural language processing (NLP). However, as revealed by many recent studies, machine learning models often inherit and amplify the societal biases in data. Various metrics have been proposed to quantify biases in model predictions. In particular, several of them evaluate disparity in model performance between protected groups and advantaged groups in the test corpus. However, we argue that evaluating bias at the corpus level is not enough for understanding how biases are embedded in a model. In fact, a model with similar aggregated performance between different groups on the entire data may behave differently on instances in a local region. To analyze and detect such local bias, we propose LOGAN, a new bias detection technique based on clustering. Experiments on toxicity classification and object classification tasks show that LOGAN identifies bias in a local region and allows us to better analyze the biases in model predictions.",
        "id": 222140831
      },
      {
        "title": "Semantic Parsing of Tamil Sentences",
        "text": "In this paper, we propose a rule-based approach for the identificat ion of semantic sub -graphs fro m Tamil sentences. In order to achieve the goal of semantic sub -graph identification and construction, we use a semantic graph based representation called Un iv ersal Net working Language (UNL), wh ich is a directed acyclic graph representation. To identify and build the semantic sub-graphs, we classify the ru les based on morpho-semantic features wh ich include word associated features and context based features . The rules are performed in t wo stages; one while build ing the simp le UNL graphs and one after the simple UNL graphs construction. We have identified 18 rules for sub-graph identification and construction.",
        "id": 14380054
      },
      {
        "title": "Maximum Entropy Model Learning of the Translation Rules",
        "text": "This paper proposes a learning method of translation rules from parallel corpora. This method applies the maximum entropy principle to a probabilistic model of translation rules. First, we define feature functions which express statistical properties of this model. Next, in order to optimize the model, the system iterates following steps: (1) selects a feature function which maximizes loglikelihood, and (2) adds this function to the model incrementally. As computational cost associated with this model is too expensive, we propose several methods to suppress the overhead in order to realize the system. The result shows that it attained 69.54% recall rate.",
        "id": 2522271
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper first propose to mask positions to pre-train multi-modal document transformer？",
    "positive_ctxs": [
      {
        "title": "LayoutMask: Enhance Text-Layout Interaction in Multi-modal Pre-training for Document Understanding",
        "text": "Visually-rich Document Understanding (VrDU) has attracted much research attention over the past years. Pre-trained models on a large number of document images with transformer-based backbones have led to significant performance gains in this field. The major challenge is how to fusion the different modalities (text, layout, and image) of the documents in a unified model with different pre-training tasks. This paper focuses on improving text-layout interactions and proposes a novel multi-modal pre-training model, LayoutMask. LayoutMask uses local 1D position, instead of global 1D position, as layout input and has two pre-training objectives: (1) Masked Language Modeling: predicting masked tokens with two novel masking strategies; (2) Masked Position Modeling: predicting masked 2D positions to improve layout representation learning. LayoutMask can enhance the interactions between text and layout modalities in a unified model and produce adaptive and robust multimodal representations for downstream tasks. Experimental results show that our proposed method can achieve state-of-the-art results on a wide variety of VrDU problems, including form understanding, receipt understanding, and document image classification.",
        "id": 258967524
      }
    ],
    "negative_ctxs": [
      {
        "title": "FLUSH: A Flexible Lexicon Design",
        "text": "Approaches to natural language processing that use a phrasal lexicon have the advantage of easily handling linguistic constructions that might otherwise be extragrammatical. However, current phrasal lexicons are often too rigid: their phrasal entries fail to cover the more flexible constructions. FLUSH, for Flexible Lexicon Utilizing Specialized and Hierarchical knowledge, is a knowledge-based lexicon design that allows broad phrasal coverage.",
        "id": 13313617
      },
      {
        "title": "The Next Chapter: A Study of Large Language Models in Storytelling",
        "text": "To enhance the quality of generated stories, recent story generation models have been investigating the utilization of higher-level attributes like plots or commonsense knowledge.The application of prompt-based learning with large language models (LLMs), exemplified by GPT-3, has exhibited remarkable performance in diverse natural language processing (NLP) tasks.This paper conducts a comprehensive investigation, utilizing both automatic and human evaluation, to compare the story generation capacity of LLMs with recent models across three datasets with variations in style, register, and length of stories.The results demonstrate that LLMs generate stories of significantly higher quality compared to other story generation models.Moreover, they exhibit a level of performance that competes with human authors, albeit with the preliminary observation that they tend to replicate real stories in situations involving world knowledge, resembling a form of plagiarism.* Now at Google DeepMind.",
        "id": 260125086
      },
      {
        "title": "Annotating the Focus of Negation in Japanese Text",
        "text": "This paper proposes an annotation scheme for the focus of negation in Japanese text. Negation has its scope and the focus within the scope. The scope of negation is the part of the sentence that is negated; the focus is the part of the scope that is most prominently or explicitly negated. In natural language processing, correct interpretation of negated statements requires precise detection of the focus of negation in the statements. As a foundation for developing a negation focus detector for Japanese, we have annotated textdata of \"Rakuten Travel: User review data\" and the newspaper subcorpus of the \"Balanced Corpus of Contemporary Written Japanese\" with labels proposed in our annotation scheme. We report 1,327 negation cues and the foci in the corpora, and present classification of these foci based on syntactic types and semantic types. We also propose a system for detecting the focus of negation in Japanese using 16 heuristic rules and report the performance of the system.",
        "id": 16799093
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "What limitations do large language models have in evaluating information-seeking question answering?",
    "positive_ctxs": [
      {
        "title": "Evaluating Open-Domain Question Answering in the Era of Large Language Models",
        "text": "Lexical matching remains the de facto evaluation method for open-domain question answering (QA). Unfortunately, lexical matching fails completely when a plausible candidate answer does not appear in the list of gold answers, which is increasingly the case as we shift from extractive to generative models. The recent success of large language models (LLMs) for QA aggravates lexical matching failures since candidate answers become longer, thereby making matching with the gold answers even more challenging. Without accurate evaluation, the true progress in open-domain QA remains unknown. In this paper, we conduct a thorough analysis of various open-domain QA models, including LLMs, by manually evaluating their answers on a subset of NQ-OPEN, a popular benchmark. Our assessments reveal that while the true performance of all models is significantly underestimated, the performance of the InstructGPT (zero-shot) LLM increases by nearly +60%, making it on par with existing top models, and the InstructGPT (few-shot) model actually achieves a new state-of-the-art on NQ-OPEN. We also find that more than 50% of lexical matching failures are attributed to semantically equivalent answers. We further demonstrate that regex matching ranks QA models consistent with human judgments, although still suffering from unnecessary strictness. Finally, we demonstrate that automated evaluation models are a reasonable surrogate for lexical matching in some circumstances, but not for long-form answers generated by LLMs. The automated models struggle in detecting hallucinations in LLM answers and are thus unable to evaluate LLMs. At this time, there appears to be no substitute for human evaluation.",
        "id": 258615193
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 29633703
      },
      {
        "title": "What can Neural Referential Form Selectors Learn?",
        "text": "Despite achieving encouraging results, neural Referring Expression Generation models are often thought to lack transparency. We probed neural Referential Form Selection (RFS) models to find out to what extent the linguistic features influencing the RE form are learnt and captured by state-of-the-art RFS models. The results of 8 probing tasks show that all the defined features were learnt to some extent. The probing tasks pertaining to referential status and syntactic position exhibited the highest performance. The lowest performance was achieved by the probing models designed to predict discourse structure properties beyond the sentence level.",
        "id": 237091135
      },
      {
        "title": "Published as a conference paper at ICLR 2023 Voint Cloud: Multi-View Point Cloud Rep- resentation for 3D Understanding",
        "text": "Multi-view projection methods have demonstrated promising performance on 3D understanding tasks like 3D classification and segmentation. However, it remains unclear how to combine such multi-view methods with the widely available 3D point clouds. Previous methods use unlearned heuristics to combine features at the point level. To this end, we introduce the concept of the multi-view point cloud (Voint cloud), representing each 3D point as a set of features extracted from several view-points. This novel 3D Voint cloud representation combines the compactness of 3D point cloud representation with the natural view-awareness of multi-view representation. Naturally, we can equip this new representation with convolutional and pooling operations. We deploy a Voint neural network (VointNet) to learn representations in the Voint space. Our novel representation achieves state-of-the-artperformance on 3D classification, shape retrieval, and robust 3D part segmentation on standard benchmarks ( ScanObjectNN, ShapeNet Core55, and ShapeNet Parts). 1Figure 1: 3D Voint Clouds. We propose the multi-view point cloud (Voint cloud), a novel 3D representation that is compact and naturally descriptive of view projections of a 3D point cloud. Each point in the 3D cloud is tagged with a Voint, which accumulates view-features for that point. Note that not all 3D points are visible from all views. The set of Voints constructs a Voint cloud.clouds with a single feature per point suitable for typical point cloud processing pipelines. Previous multi-view works rely on heuristics (e.g. average or label mode pooling) after mapping pixels to points (Kundu et al., 2020;Wang et al., 2019a), or multi-view fusion with voxels (Dai & Nießner, 2018). Such setups might not be optimal for a few reasons. (i) Such heuristics may aggregate information of misleading projections that are obtained from arbitrary view-points. For example, looking at an object from the bottom and processing that view independently can carry wrong information about the object's content when combined with other views. (ii) The views lack geometric 3D information.To this end, we propose a new hybrid 3D data structure that inherits the merits of point clouds (i.e. compactness, flexibility, and 3D descriptiveness) and leverages the benefits of rich perceptual features of multi-view projections. We call this new representation multi-view point cloud (or Voint cloud) and illustrate it inFigure 1. A Voint cloud is a set of Voints, where each Voint is a set of view-dependent features (view-features) that correspond to the same point in the 3D point cloud. The cardinality of these view-features may differ from one Voint to another. InTable 1, we compare some of the widely used 3D representations and our Voint cloud representation. Voint clouds inherit the characteristics of the parent explicit 3D point clouds, which facilitates learning Voint representations for a variety of vision applications (e.g. point cloud classification and segmentation). To deploy deep learning on the new Voint space, we define basic operations on Voints, such as pooling and convolution. Based on these operations, we define a practical way of building Voint neural networks that we dub VointNet. VointNet takes a Voint cloud and outputs point cloud features for 3D point cloud processing. We show how learning this Voint cloud representation leads to strong performance and gained robustness for the tasks of 3D classification, 3D object retrieval, and 3D part segmentation on standard benchmarks like ScanObjectNN (Uy et al., 2019),  and ShapeNet (Chang et al., 2015).Contributions: (i)We propose a novel multi-view 3D point cloud representation (denoted as Voint cloud), which represents each point (namely a Voint) as a set of features from different view-points. (ii) We define pooling and convolutional operations at the Voint level to construct a Voint Neural Network (VointNet) capable of learning to aggregate information from multiple views in the Voint space. (iii) Our VointNet reaches state-ofthe-artperformance on several 3D understanding tasks, including 3D shape classification, retrieval, and robust part segmentation. Further, VointNet achieves robustness improvement to occlusion and rotation. Center (VCC) funding.ReferencesGary Bradski and Stephen Grossberg. Recognition of 3-d objects from multiple 2-d views by a self-organizing neural architecture. In From Statistics . Encoder-decoder with atrous separable convolution for semantic image segmentation. In . 3dmv: Joint 3d-multi-view prediction for 3d semantic scene segmentation. In",
        "id": 244729433
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Is there any research that investigates how to use Transformer decoders to extract interactive shared representations from CNN networks on clinical notes?",
    "positive_ctxs": [
      {
        "title": "Automatic ICD Coding via Interactive Shared Representation Networks with Self-distillation Mechanism",
        "text": "The ICD coding task aims at assigning codes of the International Classification of Diseases in clinical notes. Since manual coding is very laborious and prone to errors, many methods have been proposed for the automatic ICD coding task. However, existing works either ignore the long-tail of code frequency or the noisy clinical notes. To address the above issues, we propose an Interactive Shared Representation Network with Self-Distillation mechanism. Specifically, an interactive shared representation network targets building connections among codes while modeling the cooccurrence, consequently alleviating the longtail problem. Moreover, to cope with the noisy text issue, we encourage the model to focus on the clinical note's noteworthy part and extract valuable information through a self-distillation learning mechanism. Experimental results on two MIMIC datasets demonstrate the effectiveness of our method.",
        "id": 236459913
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 185879829
      },
      {
        "title": "",
        "text": "",
        "id": 218974338
      },
      {
        "title": "ShadowGNN: Graph Projection Neural Network for Text-to-SQL Parser",
        "text": "Given a database schema, Text-to-SQL aims to translate a natural language question into the corresponding SQL query. Under the setup of cross-domain, traditional semantic parsing models struggle to adapt to unseen database schemas. To improve the model generalization capability for rare and unseen schemas, we propose a new architecture, ShadowGNN, which processes schemas at abstract and semantic levels. By ignoring names of semantic items in databases, abstract schemas are exploited in a well-designed graph projection neural network to obtain delexicalized representation of question and schema. Based on the domain-independent representations, a relation-aware transformer is utilized to further extract logical linking between question and schema. Finally, a SQL decoder with context-free grammar is applied. On the challenging Text-to-SQL benchmark Spider, empirical results show that ShadowGNN outperforms state-of-the-art models. When the annotated data is extremely limited (only 10% training set), ShadowGNN gets over absolute 5% performance gain, which shows its powerful generalization ability. Our implementation will be open-sourced at https://github. com/WowCZ/shadowgnn.",
        "id": 233210172
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there any paper that attempts to evaluate the similarity of meaning representations without using annotated data?",
    "positive_ctxs": [
      {
        "title": "Evaluate AMR Graph Similarity via Self-supervised Learning",
        "text": "In work on AMR (Abstract Meaning Representation), similarity metrics are crucial as they are used to evaluate AMR systems such as AMR parsers. Current AMR metrics are all based on nodes or triples matching without considering the entire structures of AMR graphs. To address this problem, and inspired by learned similarity evaluation on plain text, we propose AMRSim, an automatic AMR graph similarity evaluation metric. To overcome the high cost of collecting human-annotated data, AMRSim automatically generates silver AMR graphs and utilizes self-supervised learning methods. We evaluated AMRSim on various datasets and found that AMRSim significantly improves the correlations with human semantic scores and remains robust under diverse challenges. We also discuss how AMRSim can be extended to multilingual cases. 1",
        "id": 259370579
      }
    ],
    "negative_ctxs": [
      {
        "title": "BERTChem-DDI : Improved Drug-Drug Interaction Prediction from text using Chemical Structure Information",
        "text": "Traditional biomedical version of embeddings obtained from pre-trained language models have recently shown state-of-the-art results for relation extraction (RE) tasks in the medical domain.In this paper, we explore how to incorporate domain knowledge, available in the form of molecular structure of drugs, for predicting Drug-Drug Interaction from textual corpus. We propose a method, BERTChem-DDI, to efficiently combine drug embeddings obtained from the rich chemical structure of drugs (encoded in SMILES) along with off-the-shelf domain-specific BioBERT embedding-based RE architecture. Experiments conducted on the DDIExtraction 2013 corpus clearly indicate that this strategy improves other strong baselines architectures by 3.4% macro F1-score.",
        "id": 227905489
      },
      {
        "title": "Transferable Neural Projection Representations",
        "text": "Neural word representations are at the core of many state-of-the-art natural language processing models. A widely used approach is to pretrain, store and look up word or character embedding matrices. While useful, such representations occupy huge memory making it hard to deploy on-device and often do not generalize to unknown words due to vocabulary pruning.",
        "id": 174797895
      },
      {
        "title": "Spelling Correction for Estonian Learner Language",
        "text": "Second and foreign language (L2) learners tend to make specific spelling errors compared to native speakers. Languageindependent spell-checking algorithms that rely on n-gram models can offer a simple solution for improving learner error detection and correction due to context-sensitivity. As the open-source speller previously available for Estonian is rule-based, our aim was to evaluate the performance of bi-and trigrambased statistical spelling correctors on an error-tagged set of A2-C1-level texts written by L2 learners of Estonian. The newly trained spell-checking models were compared to existing correction tools (open-source and commercial). Then, the best-performing Jamspell corrector was trained on various datasets to analyse their effect on the correction results.",
        "id": 258765264
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "What papers explore replacing schema linking with human annotations to study the maximum potential benefit of schema linking for text-to-SQL tasks?",
    "positive_ctxs": [
      {
        "title": "Awakening Latent Grounding from Pretrained Language Models for Semantic Parsing",
        "text": "Recent years pretrained language models (PLMs) hit a success on several downstream tasks, showing their power on modeling language. To better understand and leverage what PLMs have learned, several techniques have emerged to explore syntactic structures entailed by PLMs. However, few efforts have been made to explore grounding capabilities of PLMs, which are also essential. In this paper, we highlight the ability of PLMs to discover which token should be grounded to which concept, if combined with our proposed erasingthen-awakening approach. Empirical studies on four datasets demonstrate that our approach can awaken latent grounding which is understandable to human experts, even if it is not exposed to such labels during training. More importantly, our approach shows great potential to benefit downstream semantic parsing models. Taking text-to-SQL as a case study, we successfully couple our approach with two off-the-shelf parsers, obtaining an absolute improvement of up to 9.8%. * Work done during an internship at Microsoft Research. The first three authors contributed equally.",
        "id": 236478283
      }
    ],
    "negative_ctxs": [
      {
        "title": "Enhancing The RATP-DECODA Corpus With Linguistic Annotations For Performing A Large Range Of NLP Tasks",
        "text": "In this article, we present the RATP-DECODA Corpus which is composed by a set of 67 hours of speech from telephone conversations of a Customer Care Service (CCS). This corpus is already available on line at http://sldr.org/sldr000847/fr in its first version. However, many enhancements have been made in order to allow the development of automatic techniques to transcript conversations and to capture their meaning. These enhancements fall into two categories: firstly, we have increased the size of the corpus with manual transcriptions from a new operational day; secondly we have added new linguistic annotations to the whole corpus (either manually or through an automatic processing) in order to perform various linguistic tasks from syntactic and semantic parsing to dialog act tagging and dialog summarization.",
        "id": 13300447
      },
      {
        "title": "Implicit Premise Generation with Discourse-aware Commonsense Knowledge Models",
        "text": "Enthymemes are defined as arguments where a premise or conclusion is left implicit. We tackle the task of generating the implicit premise in an enthymeme, which requires not only an understanding of the stated conclusion and premise, but also additional inferences that could depend on commonsense knowledge. The largest available dataset for enthymemes(Habernal et al., 2018)consists of 1.7k samples, which is not large enough to train a neural text generation model. To address this issue, we take advantage of a similar task and dataset: Abductive reasoning in narrative text(Bhagavatula et al., 2020). However, we show that simply using a state-of-the-art seq2seq model fine-tuned on this data might not generate meaningful implicit premises associated with the given enthymemes. We demonstrate that encoding discourse-aware commonsense during fine-tuning improves the quality of the generated implicit premises and outperforms all other baselines both in automatic and human evaluations on three different datasets.",
        "id": 237491918
      },
      {
        "title": "LTH: Semantic Structure Extraction using Nonprojective Dependency Trees",
        "text": "We describe our contribution to the SemEval task on Frame-Semantic Structure Extraction. Unlike most previous systems described in literature, ours is based on dependency syntax. We also describe a fully automatic method to add words to the FrameNet lexical database, which gives an improvement in the recall of frame detection.",
        "id": 7991041
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper enables interactive semantic parsing by training an error correction model with simulated human feedback instead of human annotations?",
    "positive_ctxs": [
      {
        "title": "Learning to Simulate Natural Language Feedback for Interactive Semantic Parsing",
        "text": "Interactive semantic parsing based on natural language (NL) feedback, where users provide feedback to correct the parser mistakes, has emerged as a more practical scenario than the traditional one-shot semantic parsing. However, prior work has heavily relied on humanannotated feedback data to train the interactive semantic parser, which is prohibitively expensive and not scalable. In this work, we propose a new task of simulating NL feedback for interactive semantic parsing. We accompany the task with a novel feedback evaluator. The evaluator is specifically designed to assess the quality of the simulated feedback, based on which we decide the best feedback simulator from our proposed variants. On a text-to-SQL dataset, we show that our feedback simulator can generate high-quality NL feedback to boost the error correction ability of a specific parser. In low-data settings, our feedback simulator can help achieve comparable error correction performance as trained using the costly, full set of human annotations. 1",
        "id": 258685538
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 220058607
      },
      {
        "title": "Language Identification for Person Names Based on Statistical Information",
        "text": "Language identification has been an interesting and fascinating issue in natural language processing for decades, and there have been many researches on it. However, most of the researches are for documents, and though the possibility of high accuracy for shorter strings of characters, language identification for words or phrases has not been discussed much. In this paper we propose a statistical method of language identification for phrases, and show the empirical results for person names of 9 languages (12 areas). Our simple method based on n-gram and phrase length obtained more than 90% of accuracy for Japanese, Korean and Russian, and fair results for other languages except English. This result indicated the possibility of language identification for person names based on statistics, which is useful in multi-language person name detection and also let us expect the possibility of language identification for phrases with simple statistics-based methods.",
        "id": 1003566
      },
      {
        "title": "Light verb constructions with 'do' and 'be' in Hindi: A TAG analysis",
        "text": "In this paper we present a Lexicalized Feature-based Tree-Adjoining Grammar analysis for a type of nominal predicate that occurs in combination with the light verbs \"do\" and \"be\" (Hindi kar and ho respectively). Light verb constructions are a challenge for computational grammars because they are a highly productive predicational strategy in Hindi. Such nominals have been discussed in the literature(Mohanan, 1997;Ahmed and Butt, 2011;Bhatt et al., 2013), but this work is a first attempt at a Tree-Adjoining Grammar (TAG) representation. We look at three possibilities for the design of elementary trees in TAG and explore one option in depth using Hindi data. In this analysis, the nominal is represented with all the arguments of the light verb construction, while the light verb adjoins into its elementary tree.",
        "id": 1159978
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Where can I find multilingual datasets used for the task of intended sarcasm detection, particularly involving English and Arabic?",
    "positive_ctxs": [
      {
        "title": "SemEval-2022 Task 6: iSarcasmEval, Intended Sarcasm Detection in English and Arabic",
        "text": "iSarcasmEval is the first shared task to target intended sarcasm detection: the data for this task was provided and labelled by the authors of the texts themselves. Such an approach minimises the downfalls of other methods to collect sarcasm data, which rely on distant supervision or third-party annotations. The shared task contains two languages, English and Arabic, and three subtasks: sarcasm detection, sarcasm category classification, and pairwise sarcasm identification given a sarcastic sentence and its non-sarcastic rephrase. The task received submissions from 60 different teams, with the sarcasm detection task being the most popular. Most of the participating teams utilised pre-trained language models. In this paper, we provide an overview of the task, data, and participating teams.",
        "id": 250391089
      }
    ],
    "negative_ctxs": [
      {
        "title": "Toward an Underspecifiable Corpus Annotation Scheme",
        "text": "The Wall Street Journal corpora provided for the Workshop on Cross-Framework and Cross-Domain Parser Evaluation Shared Task are investigated in order to see how the structures that are difficult for an annotator of dependency structure are encoded in the different schemes. Non-trivial differences among the schemes are found. The paper also investigates the possibility of merging the information encoded in the different corpora.",
        "id": 3518648
      },
      {
        "title": "",
        "text": "",
        "id": 218974550
      },
      {
        "title": "Domain Independent Authorship Attribution without Domain Adaptation",
        "text": "Automatic authorship attribution, by its nature, is much more advantageous if it is domain (i.e., topic and/or genre) independent. That is, many real world problems that require authorship attribution may not have in-domain training data readily available. However, most previous work based on machine learning techniques focused only on in-domain text for authorship attribution. In this paper, we present comprehensive evaluation of various stylometric techniques for cross-domain authorship attribution. From the experiments based on the Project Gutenberg book archive, we discover that extremely simple techniques based on stopwords are surprisingly robust against domain change, essentially ridding the need for domain adaptation when supplied with a large amount of data.",
        "id": 18999067
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper explored training a GPT-2 for automatic diagnosis, emphasizing efficient data augmentation for symptom prediction and disease identification?",
    "positive_ctxs": [
      {
        "title": "CoAD: Automatic Diagnosis through Symptom and Disease Collaborative Generation",
        "text": "Automatic diagnosis (AD), a critical application of AI in healthcare, employs machine learning techniques to assist doctors in gathering patient symptom information for precise disease diagnosis. The Transformer-based method utilizes an input symptom sequence, predicts itself through auto-regression, and employs the hidden state of the final symptom to determine the disease. Despite its simplicity and superior performance demonstrated, a decline in disease diagnosis accuracy is observed caused by 1) a mismatch between symptoms observed during training and generation, and 2) the effect of different symptom orders on disease prediction. To address the above obstacles, we introduce the CoAD, a novel disease and symptom collaborative generation framework, which incorporates several key innovations to improve AD: 1) aligning sentence-level disease labels with multiple possible symptom inquiry steps to bridge the gap between training and generation; 2) expanding symptom labels for each sub-sequence of symptoms to enhance annotation and eliminate the effect of symptom order; 3) developing a repeated symptom input schema to effectively and efficiently learn the expanded disease and symptom labels. We evaluate the CoAD framework using four datasets, including three public and one private, and demonstrate that it achieves an average 2.3% improvement over previous state-of-theart results in automatic disease diagnosis. For reproducibility, we release the code and data at https://github.com/KwanWaiChung/coad.",
        "id": 259370815
      }
    ],
    "negative_ctxs": [
      {
        "title": "BCCWJ-DepPara: A Syntactic Annotation Treebank on the 'Balanced Corpus of Contemporary Written Japanese'",
        "text": "Paratactic syntactic structures are difficult to represent in syntactic dependency tree structures. As such, we propose an annotation schema for syntactic dependency annotation of Japanese, in which coordinate structures are separated from and overlaid on bunsetsu(base phrase unit)-based dependency. The schema represents nested coordinate structures, non-constituent conjuncts, and forward sharing as the set of regions. The annotation was performed on the core data of 'Balanced Corpus of Contemporary Written Japanese', which comprised about one million words and 1980 samples from six registers, such as newspapers, books, magazines, and web texts.",
        "id": 17461299
      },
      {
        "title": "",
        "text": "We introduce the corpus of United States Congressional bills from 1947 to 1998 for use by language research communities. The U.S. Policy Agenda Legislation Corpus Volume 1 (USPALCV1) includes more than 375,000 legislative bills annotated with a hierarchical policy area category. The human annotations in USPALCV1 have been reliably applied over time to enable social science analysis of legislative trends. The corpus is a member of an emerging family of corpora that are annotated by policy area to enable comparative parallel trend recognition across countries and domains (legislation, political speeches, newswire articles, budgetary expenditures, web sites, etc.). This paper describes the origins of the corpus, its creation, ways to access it, design criteria, and an analysis with common supervised machine learning methods. The use of machine learning methods establishes a baseline proposed modeling for the topic classification of legal documents.",
        "id": 1719783
      },
      {
        "title": "Fine-tuning Transformers with Additional Context to Classify Discursive Moves in Mathematics Classrooms",
        "text": "Talk moves\" are specific discursive strategies used by teachers and students to facilitate conversations in which students share their thinking, and actively consider the ideas of others, and engage in rich discussions. Experts in instructional practices often rely on cues to identify and document these strategies, for example by annotating classroom transcripts. Prior efforts to develop automated systems to classify teacher talk moves using transformers achieved a performance of 76.32% F1. In this paper, we investigate the feasibility of using enriched contextual cues to improve model performance. We applied state-of-the-art deep learning approaches for Natural Language Processing (NLP), including Robustly optimized bidirectional encoder representations from transformers (Roberta) with a special input representation that supports previous and subsequent utterances as context for talk moves classification. We worked with the publically available TalkMoves dataset, which contains utterances sourced from real-world classroom sessions (human-transcribed and annotated). Through a series of experimentations, we found that a combination of previous and subsequent utterances improved the transformers' ability to differentiate talk moves (by 2.6% F1). These results constitute a new state of the art over previously published results and provide actionable insights to those in the broader NLP community who are working to develop similar transformer-based classification models.",
        "id": 250390967
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Could you recommend a dataset paper which presents relation extraction performance on translated data and compare it to English data?",
    "positive_ctxs": [
      {
        "title": "MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset",
        "text": "Relation extraction (RE) is a fundamental task in information extraction, whose extension to multilingual settings has been hindered by the lack of supervised resources comparable in size to large English datasets such as TACRED (Zhang et al., 2017). To address this gap, we introduce the MultiTACRED dataset, covering 12 typologically diverse languages from 9 language families, which is created by machine-translating TACRED instances and automatically projecting their entity annotations. We analyze translation and annotation projection quality, identify error categories, and experimentally evaluate fine-tuned pretrained mono-and multilingual language models in common transfer learning scenarios. Our analyses show that machine translation is a viable strategy to transfer RE instances, with native speakers judging more than 83% of the translated instances to be linguistically and semantically acceptable. We find monolingual RE model performance to be comparable to the English original for many of the target languages, and that multilingual models trained on a combination of English and target language data can outperform their monolingual counterparts. However, we also observe a variety of translation and annotation projection errors, both due to the MT systems and linguistic features of the target languages, such as pronoun-dropping, compounding and inflection, that degrade dataset quality and RE model performance.",
        "id": 258557740
      }
    ],
    "negative_ctxs": [
      {
        "title": "Alternative Phrases and Natural Language Information Retrieval",
        "text": "This paper presents a formal analysis for a large class of words called alternative markers, which includes other(than), such(as), and besides. These words appear frequently enough in dialog to warrant serious attention, yet present natural language search engines perform poorly on queries containing them. I show that the performance of a search engine can be improved dramatically by incorporating an approximation of the formal analysis that is compatible with the search engine's operational semantics. The value of this approach is that as the operational semantics of natural language applications improve, even larger improvements are possible.",
        "id": 10080250
      },
      {
        "title": "DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension",
        "text": "We propose DuoRC, a novel dataset for Reading Comprehension (RC) that motivates several new challenges for neural approaches in language understanding beyond those offered by existing RC datasets. DuoRC contains 186,089 unique questionanswer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie -one from Wikipedia and the other from IMDb -written by two different authors. We asked crowdsourced workers to create questions from one version of the plot and a different set of workers to extract or synthesize answers from the other version. This unique characteristic of DuoRC where questions and answers are created from different versions of a document narrating the same underlying story, ensures by design, that there is very little lexical overlap between the questions created from one version and the segments containing the answer in the other version. Further, since the two versions have different levels of plot detail, narration style, vocabulary, etc., answering questions from the second version requires deeper language understanding and incorporating external background knowledge. Additionally, the narrative style of passages arising from movie plots (as opposed to typical descriptive passages in existing datasets) exhibits the need to perform complex reasoning over events across multiple sentences. Indeed, we observe that state-of-the-art neural RC models which have achieved near human performance on the SQuAD dataset(Rajpurkar et al., 2016b), even when coupled with tra-ditional NLP techniques to address the challenges presented in DuoRC exhibit very poor performance (F1 score of 37.42% on DuoRC v/s 86% on SQuAD dataset). This opens up several interesting research avenues wherein DuoRC could complement other RC datasets to explore novel neural approaches for studying language understanding.",
        "id": 5071138
      },
      {
        "title": "The Sweet-Home speech and multimodal corpus for home automation interaction",
        "text": "Ambient Assisted Living aims at enhancing the quality of life of older and disabled people at home thanks to Smart Homes and Home Automation. However, many studies do not include tests in real settings, because data collection in this domain is very expensive and challenging and because of the few available data sets. The SWEET-HOME multimodal corpus is a dataset recorded in realistic conditions in DOMUS, a fully equipped Smart Home with microphones and home automation sensors, in which participants performed Activities of Daily living (ADL). This corpus is made of a multimodal subset, a French home automation speech subset recorded in Distant Speech conditions, and two interaction subsets, the first one being recorded by 16 persons without disabilities and the second one by 6 seniors and 5 visually impaired people. This corpus was used in studies related to ADL recognition, context aware interaction and distant speech recognition applied to home automation controled through voice.",
        "id": 7179848
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Are there datasets and benchmarks available for measuring LLM graph reasoning abilities?",
    "positive_ctxs": [
      {
        "title": "TALK LIKE A GRAPH: ENCODING GRAPHS FOR LARGE LANGUAGE MODELS",
        "text": "Graphs are a powerful tool for representing and analyzing complex relationships in real-world applications such as social networks, recommender systems, and computational finance.Reasoning on graphs is essential for drawing inferences about the relationships between entities in a complex system, and to identify hidden patterns and trends.Despite the remarkable progress in automated reasoning with natural text, reasoning on graphs with large language models (LLMs) remains an understudied problem.In this work, we perform the first comprehensive study of encoding graph-structured data as text for consumption by LLMs.We show that LLM performance on graph reasoning tasks varies on three fundamental levels:(1) the graph encoding method, (2) the nature of the graph task itself, and (3) interestingly, the very structure of the graph considered.These novel results provide valuable insight on strategies for encoding graphs as text.Using these insights we illustrate how the correct choice of encoders can boost performance on graph reasoning tasks inside LLMs by 4.8% to 61.8%, depending on the task.",
        "id": 263829977
      }
    ],
    "negative_ctxs": [
      {
        "title": "Structural Methods for Lexical/Semantic Patterns",
        "text": "This paper represents initial work on corpus methods for acquiring lexical/semantic pattern lexicons for text understanding. Recently, implementors of information extraction (IE) systems have moved away from using conventional syntac-",
        "id": 1442452
      },
      {
        "title": "Analyzing text in search of bio-molecular events: a high-precision machine learning framework",
        "text": "The BioNLP'09 Shared Task on Event Extraction is a challenge which concerns the detection of bio-molecular events from text. In this paper, we present a detailed account of the challenges encountered during the construction of a machine learning framework for participation in this task. We have focused our work mainly around the filtering of false positives, creating a high-precision extraction method. We have tested techniques such as SVMs, feature selection and various filters for data pre-and post-processing, and report on the influence on performance for each of them. To detect negation and speculation in text, we describe a custom-made rule-based system which is simple in design, but effective in performance.",
        "id": 2910140
      },
      {
        "title": "Sequential Clustering and Contextual Importance Measures for Incremental Update Summarization",
        "text": "Unexpected events such as accidents, natural disasters and terrorist attacks represent an information situation where it is crucial to give users access to important and non-redundant information as early as possible. Previous work uses either a fast but inaccurate pipeline approach or a precise but slow clustering approach. Instead, we propose to use sequential clustering for grouping information so that we are able to publish sentences at each time step. By doing so, we combine the best of both clustering and pipeline approaches and create a fast and precise real-time system. Experiments on the TREC Temporal Summarization 2015 shared task dataset show that our system achieves better results compared to the state-of-the-art.This work is licenced under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/",
        "id": 13090942
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that investigates the use of past dialogues for enhancing query expansion in conversational search systems?",
    "positive_ctxs": [
      {
        "title": "Open-Domain Question Answering Goes Conversational via Question Rewriting",
        "text": "We introduce a new dataset for Question Rewriting in Conversational Context (QReCC), which contains 14K conversations with 80K question-answer pairs. The task in QReCC is to find answers to conversational questions within a collection of 10M web pages (split into 54M passages). Answers to questions in the same conversation may be distributed across several web pages. QReCC provides annotations that allow us to train and evaluate individual subtasks of question rewriting, passage retrieval and reading comprehension required for the end-to-end conversational question answering (QA) task. We report the effectiveness of a strong baseline approach that combines the state-of-the-art model for question rewriting, and competitive models for open-domain QA. Our results set the first baseline for the QReCC dataset with F1 of 19.10, compared to the human upper bound of 75.45, indicating the difficulty of the setup and a large room for improvement.Baseline model answerThe most common tools used were daggers and spear points, used for hunting, and hand axesAnswer F119.05",
        "id": 222290679
      }
    ],
    "negative_ctxs": [
      {
        "title": "Reviewing Natural Language Processing Research",
        "text": "The reviewing procedure has been identified as one of the major issues in the current situation of the NLP field. While it is implicitly assumed that junior researcher learn reviewing during their PhD project, this might not always be the case. Additionally, with the growing NLP community and the efforts in the context of widening the NLP community, researchers joining the field might not have the opportunity to practise reviewing. This tutorial fills in this gap by providing an opportunity to learn the basics of reviewing. Also more experienced researchers might find this tutorial interesting to revise their reviewing procedure.",
        "id": 233365118
      },
      {
        "title": "AUTOMATIC CONSTRUCTION OF CLEAN BROAD-COVERAGE TRANSLATION LEXICONS",
        "text": "Word-level translational equivalences can be extracted from parallel texts by surprisingly simple statistical techniques. However, these techniques are easily fooled by indirect associations | pairs of unrelated words whose statistical properties resemble those of mutual translations. Indirect associations pollute the resulting translation lexicons, drastically reducing their precision. This paper presents an iterative lexicon cleaning method. On each iteration, most of the remaining incorrect lexicon entries are ltered out, without signi cant degradation in recall. This lexicon cleaning technique can produce translation lexicons with recall and precision both exceeding 90%, as well as dictionary-sized translation lexicons that are over 99% correct.",
        "id": 1649762
      },
      {
        "title": "MilaNLP at WASSA 2021: Does BERT Feel Sad When You Cry?",
        "text": "The paper describes the MilaNLP team's submission (Bocconi University, Milan) in the WASSA 2021 Shared Task on Empathy Detection and Emotion Classification. We focus on Track 2 -Emotion Classification -which consists of predicting the emotion of reactions to English news stories at the essay-level. We test different models based on multi-task and multi-input frameworks. The goal was to better exploit all the correlated information given in the data set. We find, though, that empathy as an auxiliary task in multi-task learning and demographic attributes as additional input provide worse performance with respect to singletask learning. While the result is competitive in terms of the competition, our results suggest that emotion and empathy are not related tasks -at least for the purpose of prediction.",
        "id": 233365252
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper first apply mixture of experts idea to large language models for domain adaptation?",
    "positive_ctxs": [
      {
        "title": "Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models' Memories",
        "text": "Pre-trained language models (PLMs) demonstrate excellent abilities to understand texts in the generic domain while struggling in a specific domain. Although continued pre-training on a large domain-specific corpus is effective, it is costly to tune all the parameters on the domain. In this paper, we investigate whether we can adapt PLMs both effectively and efficiently by only tuning a few parameters. Specifically, we decouple the feed-forward networks (FFNs) of the Transformer architecture into two parts: the original pre-trained FFNs to maintain the old-domain knowledge and our novel domain-specific adapters to inject domainspecific knowledge in parallel. Then we adopt a mixture-of-adapters gate to fuse the knowledge from different domain adapters dynamically. Our proposed Mixture-of-Domain-Adapters (MixDA) employs a two-stage adapter-tuning strategy that leverages both unlabeled data and labeled data to help the domain adaptation: i) domain-specific adapter on unlabeled data; followed by ii) the task-specific adapter on labeled data. MixDA can be seamlessly plugged into the pretraining-finetuning paradigm and our experiments demonstrate that MixDA achieves superior performance on in-domain tasks (GLUE), out-of-domain tasks (ChemProt, RCT, IMDB, Amazon), and knowledge-intensive tasks (KILT). Further analyses demonstrate the reliability, scalability, and efficiency of our method. 1 Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021b. DEBERTA: Decodingenhanced bert with disentangled attention. In International Conference on Learning Representations.Ruining He and Julian McAuley. 2016. Ups and downs:Modeling the visual evolution of fashion trends with one-class collaborative filtering. In . 2022a. Continual training of language models for few-shot learning. In . 2022b. Adapting a language model while preserving its general knowledge. In Proceed-",
        "id": 259108831
      }
    ],
    "negative_ctxs": [
      {
        "title": "A Semi-Automated Live Interlingual Communication Workflow Featuring Intralingual Respeaking: Evaluation and Benchmarking",
        "text": "In this paper, we present a semi-automated workflow for live interlingual speech-to-text communication which seeks to reduce the shortcomings of existing ASR systems: a human respeaker works with a speaker-dependent speech recognition software (e.g., Dragon Naturally Speaking) to deliver punctuated same-language output of superior quality than obtained using out-of-the-box automatic speech recognition of the original speech. This is fed into a machine translation engine (the EU's eTranslation) to produce live-caption ready text. We benchmark the quality of the output against the output of best-in-class (human) simultaneous interpreters working with the same source speeches from plenary sessions of the European Parliament. To evaluate the accuracy and facilitate the comparison between the two types of output, we use a tailored annotation approach based on the NTR model(Romero-Fresco and Pöchhacker, 2017). We find that the semi-automated workflow combining intralingual respeaking and machine translation is capable of generating outputs that are similar in terms of accuracy and completeness to the outputs produced in the benchmarking workflow, although the small scale of our experiment requires caution in interpreting this result.",
        "id": 252624472
      },
      {
        "title": "Subword-based Cross-lingual Transfer of Embeddings from Hindi to Marathi and Nepali",
        "text": "Word embeddings are growing to be a crucial resource in the field of NLP for any language. This work introduces a novel technique for static subword embeddings transfer for Indic languages from a relatively higher resource language to a genealogically related low resource language. We primarily work with Hindi-Marathi, simulating a low-resource scenario for Marathi, and confirm observed trends on Nepali. We demonstrate the consistent benefits of unsupervised morphemic segmentation on both source and target sides over the treatment performed by fastText. Our best-performing approach uses an EM-style approach to learning bilingual subword embeddings; we also show, for the first time, that a trivial \"copyand-paste\" embeddings transfer based on even perfect bilingual lexicons is inadequate in capturing language-specific relationships. We find that our approach substantially outperforms the fastText baselines for both Marathi and Nepali on the Word Similarity task as well as WordNet-Based Synonymy Tests; on the former task, its performance for Marathi is close to that of pretrained fastText embeddings that use three orders of magnitude more Marathi data.",
        "id": 250390514
      },
      {
        "title": "",
        "text": "",
        "id": 227231571
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Is there research examining if multilingual pre-trained models utilize identical sets of neural units to encode morphosyntactic features in various languages?",
    "positive_ctxs": [
      {
        "title": "Same Neurons, Different Languages: Probing Morphosyntax in Multilingual Pre-trained Models",
        "text": "The success of multilingual pre-trained models is underpinned by their ability to learn representations shared by multiple languages even in absence of any explicit supervision. However, it remains unclear how these models learn to generalise across languages. In this work, we conjecture that multilingual pretrained models can derive language-universal abstractions about grammar. In particular, we investigate whether morphosyntactic information is encoded in the same subset of neurons in different languages. We conduct the first large-scale empirical study over 43 languages and 14 morphosyntactic categories with a state-of-the-art neuron-level probe. Our findings show that the cross-lingual overlap between neurons is significant, but its extent may vary across categories and depends on language proximity and pre-training data size.",
        "id": 248512463
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "A proposal is made for the use of contextual information in the machine translation of Japanese and English. This paper describes the use of a Context Monitor to nlaintaill contextual infortmttion dyn,'unically and the ~tugmenlalion of appropriate features to ~t semantic network to enahh~ simple inference. The al}proach taken is that {}t\" \"best gucs~?' processing with the cont~extual information being hal~dled with semantic inf{~rmalion on ~ shallow level.IntroductionCurrent Machine qh'anslatiou (MT) systems proc,~ss input sentence by sentence. I[owever, experience wil.h English and Japanese has shown that some languages difl'er to such a degree that sentential translation yiehls poor results, l,eL us first compare the results of a conventional MT sysl.em with those we expect, t,o get for MT with context: t. J::q'lJf]~-._-i~¢{:¢/,?)[ b v,-)--1/~3-'.~E~gfi £\" 2)i L v, K 2. K-7: ~t-laJaS'd?,l~ ~a ~t~.I;~y b t:: ok-c g ];! ( 5'~koThis might be translated by a current, machine tl'anslation system as shown inFigure 11:It can clearly l)e seen that meaning in IHally seI/tenees is obscured. Let us compare this with I.he resuits of a system using simple cont.exl.ual informal,ion ms shown inFigure2: This secol/d translation is i-tlllch Ill(H'(? CO]lO['{~ll{. ;IH{I better preserves the meaning of the original se,lten{'o.An attempt has therefore I}een made to solve some of tile problems of translal.ing languages SllCIt sis Japanese and English using contextual information. Due to [.he consideral.ions of wanting to produce a high quality small-sized MT system, lhe approach taken is to use tile resources awdlahle in an exisl;ing MT system and to process the contextu;d i,l['orlmd.ion l There is obviously n great difference in results Imtweet, systems, hnl, l.hese translat.icms relweSent tyl}iCal {uHe, llted) r~.stdts fi'om a numher of systems, a) and I}) options,hq}end on the default settings of individual systems 1,0(] usinp; nn l,FC-LIike gramma.tieal formalism 2. ']'he current, dictionaries conl,~dn information to translal.e aboul; 300 words. There are 350 grammar rules which (:own' a wide range of sentence pal,terns.The context monil,or operates using information retrieved from file f-sLrueturc of ;t SOlli.eltCe al't(W analysis. This ilfforlnal.ion is then used during the transfer 2'['[le cn'igimd lm,gl.anl for l;;hgllnh-Spanlsh iranslallcm deveh,p,,d I,y (1, Amm'es [:\\m, wes'89]  has been widely adapted and eldal'ged Io .I-I'; & E-J Iranslallon.",
        "id": 18991996
      },
      {
        "title": "From phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning",
        "text": "We present a model of visually-grounded language learning based on stacked gated recurrent neural networks which learns to predict visual features given an image description in the form of a sequence of phonemes. The learning task resembles that faced by human language learners who need to discover both structure and meaning from noisy and ambiguous data across modalities. We show that our model indeed learns to predict features of the visual context given phonetically transcribed image descriptions, and show that it represents linguistic information in a hierarchy of levels: lower layers in the stack are comparatively more sensitive to form, whereas higher layers are more sensitive to meaning.",
        "id": 11631121
      },
      {
        "title": "Manipuri-English Bidirectional Statistical Machine Translation Systems using Morphology and Dependency Relations",
        "text": "The present work reports the development of Manipuri-English bidirectional statistical machine translation systems. In the English-Manipuri statistical machine translation system, the role of the suffixes and dependency relations on the source side and case markers on the target side are identified as important translation factors. A parallel corpus of 10350 sentences from news domain is used for training and the system is tested with 500 sentences. Using the proposed translation factors, the output of the translation quality is improved as indicated by baseline BLEU score of 13.045 and factored BLEU score of 16.873 respectively. Similarly, for the Manipuri English system, the role of case markers and POS tags information at the source side and suffixes and dependency relations at the target side are identified as useful translation factors. The case markers and suffixes are not only responsible to determine the word classes but also to determine the dependency relations. Using these translation factors, the output of the translation quality is improved as indicated by baseline BLEU score of 13.452 and factored BLEU score of 17.573 respectively. Further, the subjective evaluation indicates the improvement in the fluency and adequacy of both the factored SMT outputs over the respective baseline systems.",
        "id": 13113510
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "What work first uses LLM to code robotic simulation tasks and show sim-to-real benefits with policy pre-training in simulation?",
    "positive_ctxs": [
      {
        "title": "GENSIM: GENERATING ROBOTIC SIMULATION TASKS VIA LARGE LANGUAGE MODELS",
        "text": "Collecting large amounts of real-world interaction data to train general robotic policies is often prohibitively expensive, thus motivating the use of simulation data. However, existing methods for data generation have generally focused on scenelevel diversity (e.g., object instances and poses) rather than task-level diversity, due to the human effort required to come up with and verify novel tasks. This has made it challenging for policies trained on simulation data to demonstrate significant task-level generalization. In this paper, we propose to automatically generate rich simulation environments and expert demonstrations by exploiting a large language models' (LLM) grounding and coding ability. Our approach, dubbed GENSIM, has two modes: goal-directed generation, wherein a target task is given to the LLM and the LLM proposes a task curriculum to solve the target task, and exploratory generation, wherein the LLM bootstraps from previous tasks and iteratively proposes novel tasks that would be helpful in solving more complex tasks. We use GPT4 to expand the existing benchmark by ten times to over 100 tasks, on which we conduct supervised finetuning and evaluate several LLMs including finetuned GPTs and Code Llama on code generation for robotic simulation tasks. Furthermore, we observe that LLMs-generated simulation programs can enhance task-level generalization significantly when used for multitask policy training. We further find that with minimal sim-to-real adaptation, the multitask policies pretrained on GPT4-generated simulation tasks exhibit stronger transfer to unseen long-horizon tasks in the real world and outperform baselines by 25%. 1 , et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.Xinyun Chen, Chang Liu, and Dawn Song. Execution-guided neural program synthesis. , et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023.",
        "id": 263605851
      }
    ],
    "negative_ctxs": [
      {
        "title": "FAA: Fine-grained Attention Alignment for Cascade Document Ranking",
        "text": "Document ranking aims at sorting a collection of documents with their relevance to a query. Contemporary methods explore more efficient transformers or divide long documents into passages to handle the long input. However, intensive query-irrelevant content may lead to harmful distraction and high query latency. Some recent works further propose cascade document ranking models that extract relevant passages with an efficient selector before ranking, however, their selection and ranking modules are almost independently optimized and deployed, leading to selecting error reinforcement and sub-optimal performance. In fact, the document ranker can provide fine-grained supervision to make the selector more generalizable and compatible, and the selector built upon a different structure can offer a distinct perspective to assist in document ranking. Inspired by this, we propose a fine-grained attention alignment approach to jointly optimize a cascade document ranking model. Specifically, we utilize the attention activations over the passages from the ranker as fine-grained attention feedback to optimize the selector. Meanwhile, we fuse the relevance scores from the passage selector into the ranker to assist in calculating the cooperative matching representation. Experiments on MS MARCO and TREC DL demonstrate the effectiveness of our method.",
        "id": 259370641
      },
      {
        "title": "Transferring Coreference Chains through Word Alignment",
        "text": "This paper investigates the problem of automatically annotating resources with NP coreference information using a parallel corpus, English-Romanian, in order to transfer, through word alignment, coreference chains from the English part to the Romanian part of the corpus. The results show that we can detect Romanian referential expressions and coreference chains with over 80% F-measure, thus using our method as a preprocessing step followed by manual correction as part of an annotation effort for creating a large Romanian corpus with coreference information is worthwhile.",
        "id": 13388961
      },
      {
        "title": "Well-Nested Tree Languages and Attributed Tree Transducers",
        "text": "Well-nested word languages have been advertised as adequate formalizations of the notion of mild context-sensitivity. The main result of this paper is a characterization of well-nested tree languages in terms of simple attributed tree transducers.",
        "id": 33414645
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which neural theorem proving paper first attempted to prove theorems in a block-by-block manner?",
    "positive_ctxs": [
      {
        "title": "LEGO-PROVER: NEURAL THEOREM PROVING WITH GROWING LIBRARIES",
        "text": "Despite the success of large language models (LLMs), the task of theorem proving still remains one of the hardest reasoning tasks that is far from being fully solved.Prior methods using language models have demonstrated promising results, but they still struggle to prove even middle school level theorems.One common limitation of these methods is that they assume a fixed theorem library during the whole theorem proving process.However, as we all know, creating new useful theorems or even new theories is not only helpful but crucial and necessary for advancing mathematics and proving harder and deeper results.In this work, we present LEGO-Prover, which employs a growing skill library containing verified lemmas as skills to augment the capability of LLMs used in theorem proving.By constructing the proof modularly, LEGO-Prover enables LLMs to utilize existing skills retrieved from the library and to create new skills during the proving process.These skills are further evolved (by prompting an LLM) to enrich the library on another scale.Modular and reusable skills are constantly added to the library to enable tackling increasingly intricate mathematical problems.Moreover, the learned library further bridges the gap between human proofs and formal proofs by making it easier to impute missing steps.LEGO-Prover advances the stateof-the-art pass rate on miniF2F-valid (48.0% to 57.0%) and miniF2F-test (45.5% to 50.0%).During the proving process, LEGO-Prover also manages to generate over 20,000 skills (theorems/lemmas) and adds them to the growing library.Our ablation study indicates that these newly added skills are indeed helpful for proving theorems, resulting in an improvement from a success rate of 47.1% to 50.4%.We also release our code and all the generated skills. 1",
        "id": 263334074
      }
    ],
    "negative_ctxs": [
      {
        "title": "An Architecture for Anaphora Resolution",
        "text": "In this paper, we describe the pronominal anaphora resolution module of Lucy, a portable English understanding system. The design of this mo;clule was motivated by the observation that, although there exist many theories of anaphora resolution, no one of these theories is complete. Thus we have implemented a blackboard-like architecture in which individual partial theories can be encoded as separate modules that can interact to propose candidate antecedents and to evaluate each other's proposals.",
        "id": 5527143
      },
      {
        "title": "An Empirical Study on Neural Keyphrase Generation",
        "text": "Recent years have seen a flourishing of neural keyphrase generation (KPG) works, including the release of several large-scale datasets and a host of new models to tackle them. Model performance on KPG tasks has increased significantly with evolving deep learning research. However, there lacks a comprehensive comparison among different model designs, and a thorough investigation on related factors that may affect a KPG system's generalization performance. In this empirical study, we aim to fill this gap by providing extensive experimental results and analyzing the most crucial factors impacting the generalizability of KPG models. We hope this study can help clarify some of the uncertainties surrounding the KPG task and facilitate future research on this topic.",
        "id": 221836061
      },
      {
        "title": "HW-TSC at SemEval-2023 Task 7: Exploring the Natural Language Inference Capabilities of ChatGPT and Pre-trained Language Model for Clinical Trial",
        "text": "In this paper, we describe a effective system for SemEval-2022 Task 7. This task aims to determine whether a given statement is supported by comparing one or two clinical trial reports, and to identify evidence that supports the statement. This is a task that requires high natural language inference capabilities. In Subtask 1, we compare our strategy based on prompt learning and ChatGPT with a baseline constructed using BERT in zero-shot setting, and validate the effectiveness of our strategy. In Subtask 2, we fine-tune DeBERTaV3 for classification without relying on the results from Subtask 1. We find that early stopping of the training can effectively prevent model overfitting, and this achieves a good performance in Subtask 2. In addition, we do not use any ensemble strategies. We have achieved the 10th place in Subtask 1 and the 2nd place in Subtask 2.",
        "id": 259376544
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Can we learn to represent an image with arbitary numbers of tokens?",
    "positive_ctxs": [
      {
        "title": "SparseFormer: Sparse Visual Recognition via Limited Latent Tokens",
        "text": "Human visual recognition is a sparse process, where only a few salient visual cues are attended to rather than traversing every detail uniformly.However, most current vision networks follow a dense paradigm, processing every single visual unit (e.g., pixel or patch) in a uniform manner.In this paper, we challenge this dense paradigm and present a new method, coined SparseFormer, to imitate human's sparse visual recognition in an end-to-end manner.Sparse-Former learns to represent images using a highly limited number of tokens (down to 49) in the latent space with sparse feature sampling procedure instead of processing dense units in the original pixel space.Therefore, Sparse-Former circumvents most of dense operations on the image space and has much lower computational costs.Experiments on the ImageNet classification benchmark dataset show that SparseFormer achieves performance on par with canonical or well-established models while offering better accuracy-throughput tradeoff.Moreover, the design of our network can be easily extended to the video classification with promising performance at lower computational costs.We hope that our work can provide an alternative way for visual modeling and inspire further research on sparse neural architectures.The code will be publicly available at https://github.com/showlab/sparseformer.",
        "id": 258041281
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 227231721
      },
      {
        "title": "Thirumurai: A Large Dataset of Tamil Shaivite Poems and Classification of Tamil Pann",
        "text": "Thirumurai, also known as Panniru Thirumurai, is a collection of Tamil Shaivite poems dating back to the Hindu revival period between the 6th and the 10th century. These poems are par excellence, in both literary and musical terms. They have been composed based on the ancient, now non-existent Tamil Pann system and can be set to music. We present a large dataset containing all the Thirumurai poems and also attempt to classify the Pann and author of each poem using transformer based architectures. Our work is the first of its kind in dealing with ancient Tamil text datasets, which are severely under-resourced. We explore several Deep Learning-based techniques for solving this challenge effectively and provide essential insights into the problem and how to address it.",
        "id": 250164247
      },
      {
        "title": "SINAI: Machine Learning and Emotion of the Crowd for Sentiment Analysis in Microblogs",
        "text": "This paper describes the participation of the SINAI research group in the 2013 edition of the International Workshop Se-mEval. The SINAI research group has submitted two systems, which cover the two main approaches in the field of sentiment analysis: supervised and unsupervised.",
        "id": 8954558
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which research paper leverages event structure information from Abstract Meaning Representation (AMR) graphs to aid in recognizing causal relations between events?",
    "positive_ctxs": [
      {
        "title": "Semantic Structure Enhanced Event Causality Identification",
        "text": "Event Causality Identification (ECI) aims to identify causal relations between events in unstructured texts. This is a very challenging task, because causal relations are usually expressed by implicit associations between events. Existing methods usually capture such associations by directly modeling the texts with pre-trained language models, which underestimate two kinds of semantic structures vital to the ECI task, namely, event-centric structure and eventassociated structure. The former includes important semantic elements related to the events to describe them more precisely, while the latter contains semantic paths between two events to provide possible supports for ECI. In this paper, we study the implicit associations between events by modeling the above explicit semantic structures, and propose a Semantic Structure Integration model (SemSIn). It utilizes a GNN-based event aggregator to integrate the event-centric structure information, and employs an LSTM-based path aggregator to capture the event-associated structure information between two events. Experimental results on three widely used datasets show that SemSIn achieves significant improvements over baseline methods.",
        "id": 258833194
      }
    ],
    "negative_ctxs": [
      {
        "title": "Published as a conference paper at ICLR 2023 BUILDING NORMALIZING FLOWS WITH STOCHASTIC INTERPOLANTS",
        "text": "A generative model based on a continuous-time normalizing flow between any pair of base and target probability densities is proposed. The velocity field of this flow is inferred from the probability current of a time-dependent density that interpolates between the base and the target in finite time. Unlike conventional normalizing flow inference methods based the maximum likelihood principle, which require costly backpropagation through ODE solvers, our interpolant approach leads to a simple quadratic loss for the velocity itself which is expressed in terms of expectations that are readily amenable to empirical estimation. The flow can be used to generate samples from either the base or target, and to estimate the likelihood at any time along the interpolant. In addition, the flow can be optimized to minimize the path length of the interpolant density, thereby paving the way for building optimal transport maps. In situations where the base is a Gaussian density, we also show that the velocity of our normalizing flow can also be used to construct a diffusion model to sample the target as well as estimate its score. However, our approach shows that we can bypass this diffusion completely and work at the level of the probability flow with greater simplicity, opening an avenue for methods based solely on ordinary differential equations as an alternative to those based on stochastic differential equations. Benchmarking on density estimation tasks illustrates that the learned flow can match and surpass conventional continuous flows at a fraction of the cost, and compares well with diffusions on image generation on CIFAR-10 and ImageNet 32×32. The method scales ab-initio ODE flows to previously unreachable image resolutions, demonstrated up to 128 × 128.",
        "id": 252668615
      },
      {
        "title": "HETERMPC: A Heterogeneous Graph Neural Network for Response Generation in Multi-Party Conversations",
        "text": "Recently, various response generation models for two-party conversations have achieved impressive improvements, but less effort has been paid to multi-party conversations (MPCs) which are more practical and complicated. Compared with a two-party conversation where a dialogue context is a sequence of utterances, building a response generation model for MPCs is more challenging, since there exist complicated context structures and the generated responses heavily rely on both interlocutors (i.e., speaker and addressee) and history utterances. To address these challenges, we present HeterMPC, a heterogeneous graph-based neural network for response generation in MPCs which models the semantics of utterances and interlocutors simultaneously with two types of nodes in a graph. Besides, we also design six types of meta relations with node-edge-typedependent parameters to characterize the heterogeneous interactions within the graph. Through multi-hop updating, HeterMPC can adequately utilize the structural knowledge of conversations for response generation. Experimental results on the Ubuntu Internet Relay Chat (IRC) channel benchmark show that HeterMPC outperforms various baseline models for response generation in MPCs.",
        "id": 247476252
      },
      {
        "title": "",
        "text": "",
        "id": 2533597
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a study that examines the intricacies of few-shot relation extraction challenges and introduces an approach integrating both global and local attributes alongside external descriptions of relations?",
    "positive_ctxs": [
      {
        "title": "Exploring Task Difficulty for Few-Shot Relation Extraction",
        "text": "Few-shot relation extraction (FSRE) focuses on recognizing novel relations by learning with merely a handful of annotated instances. Meta-learning has been widely adopted for such a task, which trains on randomly generated few-shot tasks to learn generic data representations. Despite impressive results achieved, existing models still perform suboptimally when handling hard FSRE tasks, where the relations are fine-grained and similar to each other. We argue this is largely because existing models do not distinguish hard tasks from easy ones in the learning process. In this paper, we introduce a novel approach based on contrastive learning that learns better representations by exploiting relation label information. We further design a method that allows the model to adaptively learn how to focus on hard tasks. Experiments on two standard datasets demonstrate the effectiveness of our method.",
        "id": 237492043
      }
    ],
    "negative_ctxs": [
      {
        "title": "Using semantic roles to improve summaries",
        "text": "This paper describes preliminary analysis on the influence of the semantic roles in summary generation. The proposed method involves three steps: first, the named entities in the original text are identified using a named entity recognizer; secondly, the sentences are parsed and semantic roles are extracted; thirdly, selection of the sentences containing specific semantic roles for the most relevant entities in text. Although the method is language independent, in order to check its viability, we tested the proposed approach for Romanian summaries.",
        "id": 68799
      },
      {
        "title": "Identifying relevant common sense information in knowledge graphs",
        "text": "Knowledge graphs are often used to store common sense information that is useful for various tasks. However, the extraction of contextuallyrelevant knowledge is an unsolved problem, and current approaches are relatively simple.Here we introduce a triple selection method based on a ranking model and find that it improves question answering accuracy over existing methods. We additionally investigate methods to ensure that extracted triples form a connected graph. Graph connectivity is important for model interpretability, as paths are frequently used as explanations for the reasoning that connects question and answer.",
        "id": 248780238
      },
      {
        "title": "",
        "text": "",
        "id": 227230866
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "What research has been done on annotating user comments with claim verifiability",
    "positive_ctxs": [
      {
        "title": "Identifying Appropriate Support for Propositions in Online User Comments",
        "text": "The ability to analyze the adequacy of supporting information is necessary for determining the strength of an argument. 1 This is especially the case for online user comments, which often consist of arguments lacking proper substantiation and reasoning. Thus, we develop a framework for automatically classifying each proposition as UNVERIFIABLE, VERIFIABLE NON-EXPERIENTIAL, or VERIFIABLE EXPE-RIENTIAL 2 , where the appropriate type of support is reason, evidence, and optional evidence, respectively 3 . Once the existing support for propositions are identified, this classification can provide an estimate of how adequately the arguments have been supported. We build a goldstandard dataset of 9,476 sentences and clauses from 1,047 comments submitted to an eRulemaking platform and find that Support Vector Machine (SVM) classifiers trained with n-grams and additional features capturing the verifiability and experientiality exhibit statistically significant improvement over the unigram baseline, achieving a macro-averaged F 1 of 68.99%.",
        "id": 14764893
      }
    ],
    "negative_ctxs": [
      {
        "title": "Wronging a Right: Generating Better Errors to Improve Grammatical Error Detection",
        "text": "Grammatical error correction, like other machine learning tasks, greatly benefits from large quantities of high quality training data, which is typically expensive to produce. While writing a program to automatically generate realistic grammatical errors would be difficult, one could learn the distribution of naturallyoccurring errors and attempt to introduce them into other datasets. Initial work on inducing errors in this way using statistical machine translation has shown promise; we investigate cheaply constructing synthetic samples, given a small corpus of human-annotated data, using an off-the-rack attentive sequence-to-sequence model and a straight-forward post-processing procedure. Our approach yields error-filled artificial data that helps a vanilla bi-directional LSTM to outperform the previous state of the art at grammatical error detection, and a previously introduced model to gain further improvements of over 5% F 0.5 score. When attempting to determine if a given sentence is synthetic, a human annotator at best achieves 39.39 F 1 score, indicating that our model generates mostly human-like instances.",
        "id": 52896498
      },
      {
        "title": "SJTU-NLP at SemEval-2018 Task 9: Neural Hypernym Discovery with Term Embeddings",
        "text": "This paper describes a hypernym discovery system for our participation in the SemEval-2018 Task 9, which aims to discover the best (set of) candidate hypernyms for input concepts or entities, given the search space of a pre-defined vocabulary. We introduce a neural network architecture for the concerned task and empirically study various neural network models to build the representations in latent space for words and phrases. The evaluated models include convolutional neural network, long-short term memory network, gated recurrent unit and recurrent convolutional neural network. We also explore different embedding methods, including word embedding and sense embedding for better performance.",
        "id": 44130548
      },
      {
        "title": "",
        "text": "",
        "id": 3194221
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Which paper shows that human experts and non-experts focus on very different aspects when identifying AI=generated texts?",
    "positive_ctxs": [
      {
        "title": "All That's 'Human' Is Not Gold: Evaluating Human Evaluation of Generated Text",
        "text": "Human evaluations are typically considered the gold standard in natural language generation, but as models' fluency improves, how well can evaluators detect and judge machinegenerated text? We run a study assessing nonexperts' ability to distinguish between humanand machine-authored text (GPT2 and GPT3) in three domains (stories, news articles, and recipes). We find that, without training, evaluators distinguished between GPT3-and humanauthored text at random chance level. We explore three approaches for quickly training evaluators to better identify GPT3-authored text (detailed instructions, annotated examples, and paired examples) and find that while evaluators' accuracy improved up to 55%, it did not significantly improve across the three domains. Given the inconsistent results across text domains and the often contradictory reasons evaluators gave for their judgments, we examine the role untrained human evaluations play in NLG evaluation and provide recommendations to NLG researchers for improving human evaluations of text generated from state-of-the-art models.",
        "id": 235694265
      }
    ],
    "negative_ctxs": [
      {
        "title": "WordRank: Learning Word Embeddings via Robust Ranking",
        "text": "Embedding words in a vector space has gained a lot of attention in recent years. While stateof-the-art methods provide efficient computation of word similarities via a low-dimensional matrix embedding, their motivation is often left unclear. In this paper, we argue that word embedding can be naturally viewed as a ranking problem due to the ranking nature of the evaluation metrics. Then, based on this insight, we propose a novel framework Wor-dRank that efficiently estimates word representations via robust ranking, in which the attention mechanism and robustness to noise are readily achieved via the DCG-like ranking losses. The performance of WordRank is measured in word similarity and word analogy benchmarks, and the results are compared to the state-of-the-art word embedding techniques. Our algorithm is very competitive to the state-of-the-arts on large corpora, while outperforms them by a significant margin when the training set is limited (i.e., sparse and noisy). With 17 million tokens, WordRank performs almost as well as existing methods using 7.2 billion tokens on a popular word similarity benchmark. Our multi-node distributed implementation of WordRank is publicly available for general usage.",
        "id": 8506049
      },
      {
        "title": "TransAhead: A Computer-Assisted Translation and Writing Tool Keh-jiann Chen ++",
        "text": "We introduce a method for learning to predict text completion given a source text and partial translation. In our approach, predictions are offered aimed at alleviating users' burden on lexical and grammar choices, and improving productivity. The method involves learning syntax-based phraseology and translation equivalents. At run-time, the source and its translation prefix are sliced into ngrams to generate and rank completion candidates, which are then displayed to users. We present a prototype writing assistant, TransAhead, that applies the method to computer-assisted translation and language learning. The preliminary results show that the method has great potentials in CAT and CALL with significant improvement in translation quality across users.",
        "id": 7240730
      },
      {
        "title": "Contextual Embeddings Can Distinguish Homonymy from Polysemy in a Human-Like Way",
        "text": "Lexical ambiguity is a pervasive feature of natural language, and a major difficulty in understanding language is selecting the intended meaning when more than one are possible. Despite this difficulty, many studies of single word recognition have found a processing advantage for ambiguous words compared to unambiguous ones. This effect is not homogeneous however-studies find consistent advantages for polysemes (words with multiple related meanings), and inconsistent results for homonyms (words with multiple unrelated meanings). Complicating this is the fact that most measures of ambiguity are derived from human-annotated or curated lexicographic resources, and their use is not consistent between studies. Our work investigates whether contextualized word embeddings are able to capture human-like distinctions between senses and meanings, and whether they can predict human behavior. We reanalyze data from previous experiments reporting ambiguity (dis)advantages using the lexical decision times reported in the English Lexicon Project. We find that our method does replicate the polyseme advantage and homonym disadvantage previously reported, and the predictors are superior to binary distinctions derived from lexicographic resources. Our findings point towards the benefits of using continuous-space representations of senses and meanings over more traditional measures. Additionally, we make our code publicly available for use in future research. . 2007. The english lexicon project. Behavior research methods, 39(3):445-459. Alan Beretta, Robert Fiorentino, and David Poeppel. 2005. The effects of homonymy and polysemy on lexical access: An MEG study. Cognitive Brain Research, 24(1):57-65. Ron Borowsky and Michael EJ Masson. 1996. Semantic ambiguity effects in word identification. Journal of Experimental Psychology: Learning, Memory, and Cognition, 22(1):63. Ricardo JGB Campello, Davoud Moulavi, and Jörg Sander. 2013. Density-based clustering based on hierarchical density estimates. In Pacific-Asia conference on knowledge discovery and data mining, pages 160-172. Springer.",
        "id": 256739261
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest a study that explores a compression method that merges product quantization with integer quantization for token-level retrieval?",
    "positive_ctxs": [
      {
        "title": "ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction",
        "text": "Neural information retrieval (IR) has greatly advanced search and other knowledgeintensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6-10×.",
        "id": 244799249
      }
    ],
    "negative_ctxs": [
      {
        "title": "Discovering Relations among Named Entities by Detecting Community Structure",
        "text": "This paper proposes a networked data mining method for relations discovery from large corpus. The key idea is representing the named entities pairs and their contexts as the network structure and detecting the communities from the network. Then each community relates to a relation the named entities pairs in the same community have the same relation. Finally, we labeled the relations. Our experiment using the corpus of People's Daily reveals not only that the relations among named entities could be detected with high precision, but also that appropriate labels could be automatically provided for the relations.",
        "id": 14409919
      },
      {
        "title": "Prompting for explanations improves Adversarial NLI. Is this true? {Yes} it is {true} because {it weakens superficial cues}",
        "text": "Explanation prompts ask language models to not only assign a label to a given input, such as entailment or contradiction in natural language inference (NLI) tasks, but also to generate a free-text explanation that supports this label. While explanation prompts originally introduced aiming to improve model interpretability, here we show that they also improve robustness to superficial cues. Compared to prompting for labels only, explanation prompting shows stronger performance on adversarial NLI benchmarks, outperforming the state of the art on ANLI, Counterfactually-Augmented NLI, and SNLI-Hard datasets. Analysis suggests that the increase in robustness is due to a reduction in the association strength between single tokens and labels, i.e., explanation prompting weakens superficial cues. More specifically, we find that single tokens that are highly predictive of the correct answer in the label-only setting become uninformative when the model also has to generate explanations. . 2021. Multitask prompted training enables zero-shot task generalization.",
        "id": 258378340
      },
      {
        "title": "Large-Scale Corpus-Driven PCFG Approximation of an HPSG",
        "text": "We present a novel corpus-driven approach towards grammar approximation for a linguistically deep Head-driven Phrase Structure Grammar. With an unlexicalized probabilistic context-free grammar obtained by Maximum Likelihood Estimate on a largescale automatically annotated corpus, we are able to achieve parsing accuracy higher than the original HPSG-based model. Different ways of enriching the annotations carried by the approximating PCFG are proposed and compared. Comparison to the state-of-the-art latent-variable PCFG shows that our approach is more suitable for the grammar approximation task where training data can be acquired automatically. The best approximating PCFG achieved ParsEval F 1 accuracy of 84.13%. The high robustness of the PCFG suggests it is a viable way of achieving full coverage parsing with the hand-written deep linguistic grammars.",
        "id": 12190874
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper first combines different methods for uncertainty quantification in one?",
    "positive_ctxs": [
      {
        "title": "Hybrid Uncertainty Quantification for Selective Text Classification in Ambiguous Tasks",
        "text": "Many text classification tasks are inherently ambiguous, which results in automatic systems having a high risk of making mistakes, in spite of using advanced machine learning models. For example, toxicity detection in usergenerated content is a subjective task, and notions of toxicity can be annotated according to a variety of definitions that can be in conflict with one another. Instead of relying solely on automatic solutions, moderation of the most difficult and ambiguous cases can be delegated to human workers. Potential mistakes in automated classification can be identified by using uncertainty estimation (UE) techniques. Although UE is a rapidly growing field within natural language processing, we find that stateof-the-art UE methods estimate only epistemic uncertainty and show poor performance, or under-perform trivial methods for ambiguous tasks such as toxicity detection. We argue that in order to create robust uncertainty estimation methods for ambiguous tasks it is necessary to account also for aleatoric uncertainty. In this paper, we propose a new uncertainty estimation method that combines epistemic and aleatoric UE methods. We show that by using our hybrid method, we can outperform state-of-the-art UE methods for toxicity detection and other ambiguous text classification tasks 1 .",
        "id": 259370752
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 219942256
      },
      {
        "title": "Retrieval-augmented Image Captioning",
        "text": "Inspired by retrieval-augmented language generation and pretrained Vision and Language (V&L) encoders, we present a new approach to image captioning that generates sentences given the input image and a set of captions retrieved from a datastore, as opposed to the image alone. The encoder in our model jointly processes the image and retrieved captions using a pretrained V&L BERT, while the decoder attends to the multimodal encoder representations, benefiting from the extra textual evidence from the retrieved captions. Experimental results on the COCO dataset show that image captioning can be effectively formulated from this new perspective. Our model, named EXTRA, benefits from using captions retrieved from the training dataset, and it can also benefit from using an external dataset without the need for retraining. Ablation studies show that retrieving a sufficient number of captions (e.g., k=5) can improve captioning quality. Our work contributes towards using pretrained V&L encoders for generative tasks, instead of standard classification tasks.",
        "id": 256901128
      },
      {
        "title": "",
        "text": "",
        "id": 219792169
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Which work should I explore to understand the techniques that expand the scope of open infomation extraction beyond verbs, to include various parts of speech such as nouns and adjectives?",
    "positive_ctxs": [
      {
        "title": "Open Language Learning for Information Extraction",
        "text": "Open Information Extraction (IE) systems extract relational tuples from text, without requiring a pre-specified vocabulary, by identifying relation phrases and associated arguments in arbitrary sentences. However, stateof-the-art Open IE systems such as REVERB and WOE share two important weaknesses -(1) they extract only relations that are mediated by verbs, and (2) they ignore context, thus extracting tuples that are not asserted as factual. This paper presents OLLIE, a substantially improved Open IE system that addresses both these limitations. First, OLLIE achieves high yield by extracting relations mediated by nouns, adjectives, and more. Second, a context-analysis step increases precision by including contextual information from the sentence in the extractions. OLLIE obtains 2.7 times the area under precision-yield curve (AUC) compared to REVERB and 1.9 times the AUC of WOE parse .",
        "id": 74065
      }
    ],
    "negative_ctxs": [
      {
        "title": "An Open Dataset and Model for Language Identification",
        "text": "Language identification (LID) is a fundamental step in many natural language processing pipelines. However, current LID systems are far from perfect, particularly on lower-resource languages. We present a LID model which achieves a macro-average F1 score of 0.93 and a false positive rate of 0.033% across 201 languages, outperforming previous work. We achieve this by training on a curated dataset of monolingual data, the reliability of which we ensure by auditing a sample from each source and each language manually. We make both the model and the dataset available to the research community. Finally, we carry out detailed analysis into our model's performance, both in comparison to existing open models and by language class. eventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).",
        "id": 258841338
      },
      {
        "title": "When Truth Matters -Addressing Pragmatic Categories in Natural Language Inference (NLI) by Large Language Models (LLMs)",
        "text": "In this paper, we focus on the ability of large language models (LLMs) to accommodate different pragmatic sentence types, such as questions, commands, as well as sentence fragments for natural language inference (NLI). On the commonly used notion of logical inference, nothing can be inferred from a question, a command, or an incomprehensible sentence fragment. We find MNLI, arguably the most important NLI dataset, and hence models fine-tuned on this dataset, insensitive to this fact. Using a symbolic semantic parser, we develop and make publicly available, fine-tuning datasets designed specifically to address this issue, with promising results. We also make a first exploration of ChatGPT's concept of entailment.Stanley Peters. 1979. A truth-conditional formulation ofKarttunen's account of presupposition.",
        "id": 260063190
      },
      {
        "title": "Automatic Stochastic Tagging of Natural Language Texts",
        "text": "Five language and tagset independent stochastic taggers, handling morphological and contextual information, are presented and tested in corpora of seven European languages (Dutch, English,  French, German, Greek, Italian and Spanish), using two sets of grammatical tags; a small set containing the eleven main grammatical classes and a large set of grammatical categories common to all languages. The unknown words are tagged using an experimentally proven stochastic hypothesis that links the stochastic behavior of the unknown words with that of the less probable known words. A fully automatic training and tagging program has been implemented on an IBM PC-compatible 80386-based computer. Measurements of error rate, time response, and memory requirements have shown that the taggers\" performance is satisfactory, even though a small training text is available. The error rate is improved when new texts are used to update the stochastic model parameters.",
        "id": 7791476
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper first published a real-world Chinese-English text image translation dataset?",
    "positive_ctxs": [
      {
        "title": "Exploring Better Text Image Translation with Multimodal Codebook",
        "text": "Text image translation (TIT) aims to translate the source texts embedded in the image to target translations, which has a wide range of applications and thus has important research value. However, current studies on TIT are confronted with two main bottlenecks: 1) this task lacks a publicly available TIT dataset, 2) dominant models are constructed in a cascaded manner, which tends to suffer from the error propagation of optical character recognition (OCR). In this work, we first annotate a Chinese-English TIT dataset named OCRMT30K, providing convenience for subsequent studies. Then, we propose a TIT model with a multimodal codebook, which is able to associate the image with relevant texts, providing useful supplementary information for translation. Moreover, we present a multi-stage training framework involving text machine translation, image-text alignment, and TIT tasks, which fully exploits additional bilingual texts, OCR dataset and our OCRMT30K dataset to train our model. Extensive experiments and in-depth analyses strongly demonstrate the effectiveness of our proposed model and training framework.",
        "id": 258960136
      }
    ],
    "negative_ctxs": [
      {
        "title": "Handling Sparse Data by Successive Abstraction",
        "text": "A general, practical method for handling sparse data that avoids held-out data and iterative reestimation is derived from first principles. It has been tested on a part-of-speech tagging task and outperformed (deleted) interpolation with context-independent weights, even when the latter used a globally optimal parameter setting determined a posteriori.",
        "id": 3714725
      },
      {
        "title": "Mixture of Attention Heads: Selecting Attention Heads Per Token",
        "text": "Mixture-of-Experts (MoE) networks have been proposed as an efficient way to scale up model capacity and implement conditional computing. However, the study of MoE components mostly focused on the feedforward layer in Transformer architecture. This paper proposes the Mixture of Attention Heads (MoA), a new architecture that combines multi-head attention with the MoE mechanism. MoA includes a set of attention heads that each has its own set of parameters. Given an input, a router dynamically selects a subset of k attention heads per token. This conditional computation schema allows MoA to achieve stronger performance than the standard multi-head attention layer. Furthermore, the sparsely gated MoA can easily scale up the number of attention heads and the number of parameters while preserving computational efficiency. In addition to the performance improvements, MoA also automatically differentiates heads' utilities, providing a new perspective to discuss the model's interpretability. We conducted experiments on several important tasks, including Machine Translation and Masked Language Modeling. Experiments have shown promising results on several tasks against strong baselines that involve large and very deep models 1 .",
        "id": 252815815
      },
      {
        "title": "Semantics-Driven Shallow Parsing for Chinese Semantic Role Labeling",
        "text": "One deficiency of current shallow parsing based Semantic Role Labeling (SRL) methods is that syntactic chunks are too small to effectively group words. To partially resolve this problem, we propose semantics-driven shallow parsing, which takes into account both syntactic structures and predicate-argument structures. We also introduce several new \"path\" features to improve shallow parsing based SRL method. Experiments indicate that our new method obtains a significant improvement over the best reported Chinese SRL result.",
        "id": 7891063
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper first explored In-context learning in a cross lingual setup and made use of alignment to better it's performance?",
    "positive_ctxs": [
      {
        "title": "Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment",
        "text": "In-context learning (ICL) unfolds as large language models become capable of inferring test labels conditioned on a few labeled samples without any gradient update. ICL-enabled large language models provide a promising step forward toward bypassing recurrent annotation costs in a low-resource setting. Yet, only a handful of past studies have explored ICL in a cross-lingual setting, in which the need for transferring label-knowledge from a high-resource language to a low-resource one is immensely crucial. To bridge the gap, we provide the first in-depth analysis of ICL for cross-lingual text classification. We find that the prevalent mode of selecting random inputlabel pairs to construct the prompt-context is severely limited in the case of cross-lingual ICL, primarily due to the lack of alignment in the input as well as the output spaces. To mitigate this, we propose a novel prompt construction strategy -Cross-lingual In-context Source-Target Alignment (X-InSTA). With an injected coherence in the semantics of the input examples and a task-based alignment across the source and target languages, X-InSTA is able to outperform random prompt selection by a large margin across three different tasks using 44 different cross-lingual pairs.",
        "id": 258588286
      }
    ],
    "negative_ctxs": [
      {
        "title": "Induction Networks for Few-Shot Text Classification",
        "text": "Text classification tends to struggle when data is deficient or when it needs to adapt to unseen classes. In such challenging scenarios, recent studies have used meta-learning to simulate the few-shot task, in which new queries are compared to a small support set at the samplewise level. However, this sample-wise comparison may be severely disturbed by the various expressions in the same class. Therefore, we should be able to learn a general representation of each class in the support set and then compare it to new queries. In this paper, we propose a novel Induction Network to learn such a generalized class-wise representation, by innovatively leveraging the dynamic routing algorithm in meta-learning. In this way, we find the model is able to induce and generalize better. We evaluate the proposed model on a well-studied sentiment classification dataset (English) and a real-world dialogue intent classification dataset (Chinese). Experiment results show that on both datasets, the proposed model significantly outperforms the existing state-of-the-art approaches, proving the effectiveness of class-wise generalization in few-shot text classification.",
        "id": 202776447
      },
      {
        "title": "Identify, Align, and Integrate: Matching Knowledge Graphs to Commonsense Reasoning Tasks",
        "text": "Integrating external knowledge into commonsense reasoning tasks has shown progress in resolving some, but not all, knowledge gaps in these tasks. For knowledge integration to yield peak performance, it is critical to select a knowledge graph (KG) that is well-aligned with the given task's objective. We present an approach to assess how well a candidate KG can correctly identify and accurately fill in gaps of reasoning for a task, which we call KG-to-task match. We show this KGto-task match in 3 phases: knowledge-task identification, knowledge-task alignment, and knowledge-task integration. We also analyze our transformer-based KG-to-task models via commonsense probes to measure how much knowledge is captured in these models before and after KG integration.",
        "id": 233189603
      },
      {
        "title": "The influence of written task descriptions in Wizard of Oz experiments",
        "text": "In this paper we investigate an assertion made by Richards and  Underwood (1985), who claim that people interacting with a spoken information retrieval system, structure their information in such a uniform manner that this regularity can be used to enhance the performance of the dialog system. We put forward the possibility that this uniform ordering of information might be due to the design of the written task descriptions used in Wizard of Oz experiments.",
        "id": 16030829
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which language model distillation paper that first identified the capacity gap in distillation and used the MoE student model to counter the curse of capacity gap?",
    "positive_ctxs": [
      {
        "title": "Lifting the Curse of Capacity Gap in Distilling Language Models",
        "text": "Pretrained language models (LMs) have shown compelling performance on various downstream tasks, but unfortunately they require a tremendous amount of inference compute. Knowledge distillation finds a path to compress LMs to small ones with a teacher-student paradigm. However, when the capacity gap between the teacher and the student is large, a curse of capacity gap appears, invoking a deficiency in distilling LMs. While a few studies have been carried out to fill the gap, the curse is not yet well tackled. In this paper, we aim at lifting the curse of capacity gap via enlarging the capacity of the student without notably increasing the inference compute. Largely motivated by sparse activation regime of mixture of experts (MOE), we propose a mixture of minimal experts (MINIMOE), which imposes extra parameters to the student but introduces almost no additional inference compute. Experimental results on GLUE and CoNLL demonstrate the curse of capacity gap is lifted by the magic of MINIMOE to a large extent. MINIMOE also achieves the state-of-the-art performance at small FLOPs compared with a range of competitive baselines. With a compression rate as much as ∼50×, MINIMOE preserves ∼95% GLUE score of the teacher.",
        "id": 258833648
      }
    ],
    "negative_ctxs": [
      {
        "title": "Automatically Constructing a Corpus of Sentential Paraphrases",
        "text": "An obstacle to research in automatic paraphrase identification and generation is the lack of large-scale, publiclyavailable labeled corpora of sentential paraphrases. This paper describes the creation of the recently-released Microsoft Research Paraphrase Corpus, which contains 5801 sentence pairs, each hand-labeled with a binary judgment as to whether the pair constitutes a paraphrase. The corpus was created using heuristic extraction techniques in conjunction with an SVM-based classifier to select likely sentence-level paraphrases from a large corpus of topicclustered news data. These pairs were then submitted to human judges, who confirmed that 67% were in fact semantically equivalent. In addition to describing the corpus itself, we explore a number of issues that arose in defining guidelines for the human raters.",
        "id": 16639476
      },
      {
        "title": "Konwledge-Enabled Diagnosis Assistant Based on Obstetric EMRs and Knowledge Graph",
        "text": "The obstetric Electronic Medical Record (EMR) contains a large amount of medical data and health information. It plays a vital role in improving the quality of the diagnosis assistant service. In this paper, we treat the diagnosis assistant as a multi-label classification task and propose a Knowledge-Enabled Diagnosis Assistant (KEDA) model for the obstetric diagnosis assistant. We utilize the numerical information in EMRs and the external knowledge from Chinese Obstetric Knowledge Graph (COKG) to enhance the text representation of EMRs. Specifically, the bidirectional maximum matching method and similarity-based approach are used to obtain the entities set contained in EMRs and linked to the COKG. The final knowledge representation is obtained by a weight-based disease prediction algorithm, and it is fused with the text representation through a linear weighting method. Experiment results show that our approach can bring about +3.53 F1 score improvements upon the strong BERT baseline in the diagnosis assistant task.",
        "id": 225062783
      },
      {
        "title": "",
        "text": "",
        "id": 439452
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a study that explores the improvement of Chinese sequence labeling with BERT through the incorporation of lexical data via a character-to-word bilinear attention approach?",
    "positive_ctxs": [
      {
        "title": "Lexicon Enhanced Chinese Sequence Labelling Using BERT Adapter",
        "text": "Lexicon information and pre-trained models, such as BERT, have been combined to explore Chinese sequence labelling tasks due to their respective strength. However, existing methods solely fuse lexicon features via a shallow and random initialized sequence layer and do not integrate them into the bottom layers of BERT. In this paper, we propose Lexicon Enhanced BERT (LEBERT) for Chinese sequence labelling, which integrates external lexicon knowledge into BERT layers directly by a Lexicon Adapter layer. Compared with the existing methods, our model facilitates deep lexicon knowledge fusion at the lower layers of BERT. Experiments on ten Chinese datasets of three tasks including Named Entity Recognition, Word Segmentation, and Part-of-Speech Tagging, show that LEBERT achieves the stateof-the-art results.",
        "id": 234741719
      }
    ],
    "negative_ctxs": [
      {
        "title": "Maximum Entropy Based Lexical Reordering Model for Hierarchical Phrase-based Machine Translation",
        "text": "The hierarchical phrase-based (HPB) model on the basis of a synchronous context-free grammar (SCFG) is prominent in solving global reorderings. However, the HPB model is inadequate to supervise the reordering process so that sometimes positions of different lexicons are switched due to the incorrect SCFG rules. In this paper, we consider the order of two lexicons as a classification problem and propose a novel lexical reordering model based on a maximum entropy classifier. Our model employs the word alignment and translation during the decoding process. Experimental results on the Chinese-to-English task showed that our method outperformed the baseline system in BLEU score significantly. Moreover, the translation results further proved the effectiveness of our approach. Keywords: hierarchical phrase-based model, reordering, maximum entropy model weishengyongpin[hygiene article] , tebieshi[particular] niaobu[diaper] ( yinger[infant] 、 shijin[incontinent] chengren[adult] ) 、 nvxing[feminine] weishegnyongpin [hygiene article] , BL Hygiene products, in particular (baby diapers, incontinent adult), feminine hygiene products, MEL Hygiene products, particular diapers (baby, incontinent adult), feminine hygiene products, RF Sanitary articles, in particular diapers (infant, incontinent adult), feminine hygiene products, Src2 kepeizhi[configurable] canshu[parameter] 375 ( liru[such as] shujv[data] leixing[category] , shujv[data] dingxiang[orientation], he[and] shujv[data] texing[characteristic] ) , BL configurable parameters 375 characteristics (e. g., data type, orientation and data), MEL configurable parameters 375 (e. g., data type, data orientation and data characteristics), RF configurable parameters 375 (such as data categories, data orientations and data characteristics), Src3 canzhao[refer] tu[figure] 7 , yonghu[user] dui[to] caidan[menu] xuanxiang[option] 607 de['s] xuanze[selection] ( tu[figure] 6 )BL Referring now to Figure 7, the user selects to menu options 607 (Figure 6) MEL Referring to FIG.7, user selection of the menu options 607 (Figure 6) RF Referring to FIG. 7, the user's selection of menu option 607 (FIG.6)",
        "id": 6770074
      },
      {
        "title": "",
        "text": "",
        "id": 196002081
      },
      {
        "title": "GCDT: A Chinese RST Treebank for Multigenre and Multilingual Discourse Parsing",
        "text": "A lack of large-scale human-annotated data has hampered the hierarchical discourse parsing of Chinese. In this paper, we present GCDT, the largest hierarchical discourse treebank for Mandarin Chinese in the framework of Rhetorical Structure Theory (RST). GCDT covers over 60K tokens across five genres of freely available text, using the same relation inventory as contemporary RST treebanks for English. We also report on this dataset's parsing experiments, including state-of-the-art (SOTA) scores for Chinese RST parsing and RST parsing on the English GUM dataset, using cross-lingual training in Chinese and English with multilingual embeddings.",
        "id": 252992779
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper proposes to use rewriting based approaches to defending against adversarial attacks in text classification?",
    "positive_ctxs": [
      {
        "title": "Don't Retrain, Just Rewrite: Countering Adversarial Perturbations by Rewriting Text",
        "text": "Can language models transform inputs to protect text classifiers against adversarial attacks? In this work, we present ATINTER, a model that intercepts and learns to rewrite adversarial inputs to make them non-adversarial for a downstream text classifier. Our experiments on four datasets and five attack mechanisms reveal that ATINTER is effective at providing better adversarial robustness than existing defense approaches, without compromising task accuracy. For example, on sentiment classification using the SST-2 dataset, our method improves the adversarial accuracy over the best existing defense approach by more than 4% with a smaller decrease in task accuracy (0.5 % vs. 2.5%). Moreover, we show that ATINTER generalizes across multiple downstream tasks and classifiers without having to explicitly retrain it for those settings. For example, we find that when ATINTER is trained to remove adversarial perturbations for the sentiment classification task on the SST-2 dataset, it even transfers to a semantically different task of news classification (on AGNews) and improves the adversarial robustness by more than 10%.",
        "id": 258947664
      }
    ],
    "negative_ctxs": [
      {
        "title": "Learning to Ask Like a Physician",
        "text": "Existing question answering (QA) datasets derived from electronic health records (EHR) are artificially generated and consequently fail to capture realistic physician information needs. We present Discharge Summary Clinical Questions (DiSCQ), a newly curated question dataset composed of 2,000+ questions paired with the snippets of text (triggers) that prompted each question. The questions are generated by medical experts from 100+ MIMIC-III discharge summaries. We analyze this dataset to characterize the types of information sought by medical experts. We also train baseline models for trigger detection and question generation (QG), paired with unsupervised answer retrieval over EHRs. Our baseline model is able to generate high quality questions in over 62% of cases when prompted with human selected triggers. We release this dataset (and all code to reproduce baseline model results) to facilitate further research into realistic clinical QA and QG. 1 *",
        "id": 249394777
      },
      {
        "title": "Speech Intelligibility and the Production of Fricative and Affricate among Mandarin-speaking Children with Cerebral Palsy",
        "text": "Literatures pertaining to English and Mandarin fricative/affricate productions by adults with cerebral palsy (CP) showed that acoustic measurements such as rise time contrast, initial burst rate contrast and friction noise duration contrast associated with fricative/affricate productions were highly correlated with overall speech intelligibility. However, the phonetic features of fricatives/affricates produced by Mandarin-learning children with CP were not fully explored.Therefore, this study targets on fricatives/affricates produced by ten Mandarin-learning CP children (Mean: 6;10, Range: 4;6 -8;11) and ten Mandarin-learning typically developing children (Mean: 5;7, Range: 5;2 -6;1). The current results from a speech repetition task showed that: 1) The fricative/affricate accurate rates and error patterns were similar between the two The 2016 Conference on Computational Linguistics and Speech Processing ROCLING 2016, pp. 153-163  The Association for Computational Linguistics and Chinese Language Processing 153 groups;2) The differences between the two groups in terms of nine acoustic measurements (fricative/affricate rise time, initial burst rate, friction noise duration and their contrasts) and speech intelligibility were not statistically significant; 3) The rise time contrast was an effective contributor to overall speech intelligibility for CP children. Together with previous studies, the current study concluded that rise time contrast was the most significant contributor, among fricative/affricate measurements, to speech intelligibility across different age ranges.",
        "id": 17631485
      },
      {
        "title": "Key Female Characters in Film Have More to Talk About Besides Men: Automating the Bechdel Test",
        "text": "The Bechdel test is a sequence of three questions designed to assess the presence of women in movies. Many believe that because women are seldom represented in film as strong leaders and thinkers, viewers associate weaker stereotypes with women. In this paper, we present a computational approach to automate the task of finding whether a movie passes or fails the Bechdel test. This allows us to study the key differences in language use and in the importance of roles of women in movies that pass the test versus the movies that fail the test. Our experiments confirm that in movies that fail the test, women are in fact portrayed as less-central and less-important characters.",
        "id": 7280935
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there a paper that uses an app for a popular tabletop game to gather real transcripts of gameplay with concrete values for players' and monsters' health?",
    "positive_ctxs": [
      {
        "title": "FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information",
        "text": "Dungeons & Dragons (D&D) is a tabletop roleplaying game with complex natural language interactions between players and hidden state information. Recent work has shown that large language models (LLMs) that have access to state information can generate higher quality game turns than LLMs that use dialog history alone. However, previous work used game state information that was heuristically created and was not a true gold standard game state. We present FIREBALL, a large dataset containing nearly 25,000 unique sessions from real D&D gameplay on Discord with true game state info. We recorded game play sessions of players who used the Avrae bot, which was developed to aid people in playing D&D online, capturing language, game commands and underlying game state information. We demonstrate that FIRE-BALL can improve natural language generation (NLG) by using Avrae state information, improving both automated metrics and human judgments of quality. Additionally, we show that LLMs can generate executable Avrae commands, particularly after finetuning.language models with strategic reasoning. Science, 378(6624):",
        "id": 258436686
      }
    ],
    "negative_ctxs": [
      {
        "title": "Ranking-Enhanced Unsupervised Sentence Representation Learning",
        "text": "Unsupervised sentence representation learning has progressed through contrastive learning and data augmentation methods such as dropout masking. Despite this progress, sentence encoders are still limited to using only an input sentence when predicting its semantic vector. In this work, we show that the semantic meaning of a sentence is also determined by nearestneighbor sentences that are similar to the input sentence. Based on this finding, we propose a novel unsupervised sentence encoder, RankEncoder. RankEncoder predicts the semantic vector of an input sentence by leveraging its relationship with other sentences in an external corpus, as well as the input sentence itself. We evaluate RankEncoder on semantic textual benchmark datasets. From the experimental results, we verify that 1) RankEncoder achieves 80.07% Spearman's correlation, a 1.1% absolute improvement compared to the previous state-of-the-art performance, 2) RankEncoder is universally applicable to existing unsupervised sentence embedding methods, and 3) RankEncoder is specifically effective for predicting the similarity scores of similar sentence pairs. 1 * This work was done during an internship at Amazon. 1  We provide the implementation of RankEncoder at https: //github.com/yeonsw/RankEncoder.git . 2022. Trans-encoder: Unsupervised sentence-pair modelling through self-and mutual-distillations. In ICLR. Fangyu Liu, Ivan Vulić, Anna Korhonen, and Nigel Collier. 2021. Fast, effective, and self-supervised: Transforming masked language models into universal lexical and sentence encoders. In EMNLP. parelli. 2014. A SICK cure for the evaluation of compositional distributional semantic models. In LREC. Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In ACL. Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP. Ellen M Voorhees and Dawn M Tice. 2000. Building a question answering test collection. In SIGIR.",
        "id": 252185412
      },
      {
        "title": "Efficient Optimization of an MDL-Inspired Objective Function for Unsupervised Part-of-Speech Tagging",
        "text": "The Minimum Description Length (MDL) principle is a method for model selection that trades off between the explanation of the data by the model and the complexity of the model itself. Inspired by the MDL principle, we develop an objective function for generative models that captures the description of the data by the model (log-likelihood) and the description of the model (model size). We also develop a efficient general search algorithm based on the MAP-EM framework to optimize this function. Since recent work has shown that minimizing the model size in a Hidden Markov Model for part-of-speech (POS) tagging leads to higher accuracies, we test our approach by applying it to this problem. The search algorithm involves a simple change to EM and achieves high POS tagging accuracies on both English and Italian data sets.",
        "id": 5613146
      },
      {
        "title": "Challenges for annotating images for sense disambiguation",
        "text": "We describe an unusual data set of thousands of annotated images with interesting sense phenomena. Natural language image sense annotation involves increased semantic complexities compared to disambiguating word senses when annotating text. These issues are discussed and illustrated, including the distinction between word senses and iconographic senses.",
        "id": 15545852
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Can you point me to a work that uses diagnostic tools to detect depression from online posts, and investigates strategies that address common temporal and topical artifacts that plague these models?",
    "positive_ctxs": [
      {
        "title": "Do Models of Mental Health Based on Social Media Data Generalize?",
        "text": "Proxy-based methods for annotating mental health status in social media have grown popular in computational research due to their ability to gather large training samples. However, an emerging body of literature has raised new concerns regarding the validity of these types of methods for use in clinical applications. To further understand the robustness of distantly supervised mental health models, we explore the generalization ability of machine learning classifiers trained to detect depression in individuals across multiple social media platforms. Our experiments not only reveal that substantial loss occurs when transferring between platforms, but also that there exist several unreliable confounding factors that may enable researchers to overestimate classification performance. Based on these results, we enumerate recommendations for future mental health dataset construction.",
        "id": 226283450
      }
    ],
    "negative_ctxs": [
      {
        "title": "A Formal Proof of Strong Equivalence for a Grammar Conversion from LTAG to HPSG-style",
        "text": "",
        "id": 119887
      },
      {
        "title": "Learning Non-Monotonic Automatic Post-Editing of Translations from Human Orderings",
        "text": "Recent research in neural machine translation has explored flexible generation orders, as an alternative to left-to-right generation. However, training non-monotonic models brings a new complication: how to search for a good ordering when there is a combinatorial explosion of orderings arriving at the same final result? Also, how do these automatic orderings compare with the actual behaviour of human translators? Current models rely on manually built biases or are left to explore all possibilities on their own. In this paper, we analyze the orderings produced by human post-editors and use them to train an automatic postediting system. We compare the resulting system with those trained with left-to-right and random post-editing orderings. We observe that humans tend to follow a nearly left-to-right order, but with interesting deviations, such as preferring to start by correcting punctuation or verbs.",
        "id": 216641902
      },
      {
        "title": "Analysis of Zero-Shot Crosslingual Learning between English and Korean for Named Entity Recognition",
        "text": "This paper presents a English-Korean parallel dataset that collects 381K news articles where 1,400 of them, comprising 10K sentences, are manually labeled for crosslingual named entity recognition (NER). The annotation guidelines for the two languages are developed in parallel, that yield the inter-annotator agreement scores of 91 and 88% for English and Korean respectively, indicating sublime quality annotation in our dataset. Three types of crosslingual learning approaches, direct model transfer, embedding projection, and annotation projection, are used to develop zero-shot Korean NER models. Our best model gives the F1-score of 51% that is very encouraging, considering the extremely distinct natures of these two languages. This is pioneering work that explores zero-shot crosslingual learning between English and Korean and provides rich parallel annotation for a core NLP task such as named entity recognition.",
        "id": 241583519
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that examines how coreference resolution affects dialogue summarization quality?",
    "positive_ctxs": [
      {
        "title": "Coreference-Aware Dialogue Summarization",
        "text": "Summarizing conversations via neural approaches has been gaining research traction lately, yet it is still challenging to obtain practical solutions. Examples of such challenges include unstructured information exchange in dialogues, informal interactions between speakers, and dynamic role changes of speakers as the dialogue evolves. Many of such challenges result in complex coreference links. Therefore, in this work, we investigate different approaches to explicitly incorporate coreference information in neural abstractive dialogue summarization models to tackle the aforementioned challenges. Experimental results show that the proposed approaches achieve state-of-the-art performance, implying it is useful to utilize coreference information in dialogue summarization. Evaluation results on factual correctness suggest such coreferenceaware models are better at tracing the information flow among interlocutors and associating accurate status/actions with the corresponding interlocutors and person mentions.",
        "id": 235446920
      }
    ],
    "negative_ctxs": [
      {
        "title": "An Unsupervised Speaker Clustering Technique based on SOM and I-vectors for Speech Recognition Systems",
        "text": "In this paper, we introduce an enhancement for speech recognition systems using an unsupervised speaker clustering technique. The proposed technique is mainly based on I-vectors and Self-Organizing Map Neural Network (SOM). The input to the proposed algorithm is a set of speech utterances. For each utterance, we extract 100-dimensional I-vector and then SOM is used to group the utterances to different speakers. In our experiments, we compared our technique with Normalized Cross Likelihood ratio Clustering (NCLR). Results show that the proposed technique reduces the speaker error rate in comparison with NCLR. Finally, we have experimented the effect of speaker clustering on Speaker Adaptive Training (SAT) in a speech recognition system implemented to test the performance of the proposed technique. It was noted that the proposed technique reduced the WER over clustering speakers with NCLR.",
        "id": 16370881
      },
      {
        "title": "Understanding Abuse: A Typology of Abusive Language Detection Subtasks",
        "text": "As the body of research on abusive language detection and analysis grows, there is a need for critical consideration of the relationships between different subtasks that have been grouped under this label. Based on work on hate speech, cyberbullying, and online abuse we propose a typology that captures central similarities and differences between subtasks and we discuss its implications for data annotation and feature construction. We emphasize the practical actions that can be taken by researchers to best approach their abusive language detection subtask of interest.",
        "id": 8821211
      },
      {
        "title": "DRSM: DE-RANDOMIZED SMOOTHING ON MALWARE CLASSIFIER PROVIDING CERTIFIED ROBUSTNESS",
        "text": "Machine Learning (ML) models have been utilized for malware detection for over two decades.Consequently, this ignited an ongoing arms race between malware authors and antivirus systems, compelling researchers to propose defenses for malware-detection models against evasion attacks.However, most if not all existing defenses against evasion attacks suffer from sizable performance degradation and/or can defend against only specific attacks, which makes them less practical in real-world settings.In this work, we develop a certified defense, DRSM (De-Randomized Smoothed MalConv), by redesigning the de-randomized smoothing technique for the domain of malware detection.Specifically, we propose a window ablation scheme to provably limit the impact of adversarial bytes while maximally preserving local structures of the executables.After showing how DRSM is theoretically robust against attacks with contiguous adversarial bytes, we verify its performance and certified robustness experimentally, where we observe only marginal accuracy drops as the cost of robustness.To our knowledge, we are the first to offer certified robustness in the realm of static detection of malware executables.More surprisingly, through evaluating DRSM against 9 empirical attacks of different types, we observe that the proposed defense is empirically robust to some extent against a diverse set of attacks, some of which even fall out of the scope of its original threat model.In addition, we collected 15.5K recent benign raw executables from diverse sources, which will be made public as a dataset called PACE (Publicly Accessible Collection(s) of Executables) to alleviate the scarcity of publicly available benign datasets for studying malware detection and provide future research with more representative data of the time.",
        "id": 257687205
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there any paper that reveals annotation problems in cross-lingual summarization caused by decomposing the task into translation and summarization?",
    "positive_ctxs": [
      {
        "title": "Revisiting Cross-Lingual Summarization: A Corpus-based Study and A New Benchmark with Improved Annotation",
        "text": "Most existing cross-lingual summarization (CLS) work constructs CLS corpora by simply and directly translating pre-annotated summaries from one language to another, which can contain errors from both summarization and translation processes. To address this issue, we propose ConvSumX, a cross-lingual conversation summarization benchmark, through a new annotation schema that explicitly considers source input context. ConvSumX consists of 2 sub-tasks under different real-world scenarios, with each covering 3 language directions. We conduct thorough analysis on ConvSumX and 3 widely-used manually annotated CLS corpora and empirically find that ConvSumX is more faithful towards input text. Additionally, based on the same intuition, we propose a 2-Step method, which takes both conversation and summary as input to simulate human annotation process. Experimental results show that 2-Step method surpasses strong baselines on ConvSumX under both automatic and human evaluation. Analysis shows that both source input text and summary are crucial for modeling cross-lingual summaries.",
        "id": 259370588
      }
    ],
    "negative_ctxs": [
      {
        "title": "A Single Word is not Enough: Ranking Multiword Expressions Using Distributional Semantics",
        "text": "We present a new unsupervised mechanism, which ranks word n-grams according to their multiwordness. It heavily relies on a new uniqueness measure that computes, based on a distributional thesaurus, how often an n-gram could be replaced in context by a single-worded term. In addition with a downweighting mechanism for incomplete terms this forms a new measure called DRUID. Results show large improvements on two small test sets over competitive baselines. We demonstrate the scalability of the method to large corpora, and the independence of the measure of shallow syntactic filtering.",
        "id": 16232250
      },
      {
        "title": "Composition of Conditional Random Fields for Transfer Learning",
        "text": "Many learning tasks have subtasks for which much training data exists. Therefore, we want to transfer learning from the old, generalpurpose subtask to a more specific new task, for which there is often less data. While work in transfer learning often considers how the old task should affect learning on the new task, in this paper we show that it helps to take into account how the new task affects the old. Specifically, we perform joint decoding of separately-trained sequence models, preserving uncertainty between the tasks and allowing information from the new task to affect predictions on the old task. On two standard text data sets, we show that joint decoding outperforms cascaded decoding.",
        "id": 161340
      },
      {
        "title": "Multi-Input Attention for Unsupervised OCR Correction",
        "text": "We propose a novel approach to OCR post-correction that exploits repeated texts in large corpora both as a source of noisy target outputs for unsupervised training and as a source of evidence when decoding. A sequence-to-sequence model with attention is applied for single-input correction, and a new decoder with multi-input attention averaging is developed to search for consensus among multiple sequences. We design two ways of training the correction model without human annotation, either training to match noisily observed textual variants or bootstrapping from a uniform error model. On two corpora of historical newspapers and books, we show that these unsupervised techniques cut the character and word error rates nearly in half on single inputs and, with the addition of multi-input decoding, can rival supervised methods.",
        "id": 51874610
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Is there a theory paper that explains why sometimes tuning momentum does not boost performance for training a neural network?",
    "positive_ctxs": [
      {
        "title": "The Marginal Value of Momentum for Small Learning Rate SGD",
        "text": "Momentum is known to accelerate the convergence of gradient descent in strongly convex settings without stochastic gradient noise. In stochastic optimization, such as training neural networks, folklore suggests that momentum may help deep learning optimization by reducing the variance of the stochastic gradient update, but previous theoretical analyses do not find momentum to offer any provable acceleration. Theoretical results in this paper clarify the role of momentum in stochastic settings where the learning rate is small and gradient noise is the dominant source of instability, suggesting that SGD with and without momentum behave similarly in the short and long time horizons. Experiments show that momentum indeed has limited benefits for both optimization and generalization in practical training regimes where the optimal learning rate is not very large, including small-to medium-batch training from scratch on ImageNet and finetuning language models on downstream tasks.Preprint. Under review.",
        "id": 260316137
      }
    ],
    "negative_ctxs": [
      {
        "title": "Orthographic Awareness and Phonological Awareness of Late Chinese- English Bilinguals: Evidence from Word-Picture Interference Tasks",
        "text": "The distractor stimuli are highly effective in modulating speech production latencies in word-picture interference task. It is one of the main experimental methods to explore the relationship between speech production network and perception network(Levelt et al., 1999). The distractors can be presented in both visual and auditory modalities(Lupker, 1979;Meyer and Shriefers, 1990). The interference effect (either facilitatory or inhibitory) can vary from interfering stimuli types(Lupker, 1979;Lupker, 1982;Glaser and Dungelhoff, 1984;  Meyer and Shriefers, 1990). This study has adopted a visual and an auditory interference experiment on two groups of Chinese-English bilinguals with different L2 proficiency level to figure out if there is the effect of L2 proficiency and the effect of interference modality on response latency or accuracy on two proficiency groups of late Chinese-English bilinguals. From the accuracy result, all the late bilinguals in this study may have limited orthographic awareness and weak phonological encoding ability.",
        "id": 202752865
      },
      {
        "title": "Verbs and (sub)Event Structure: A Case Study from Italian *",
        "text": "In this paper, I try to show the advantages of analyzing events in terms of subevent structure, by taking into consideration the case of a specific class of verbs: transitive and intransitive verbs which obligatorily require the presence of a predicative complement (e.g., English seem, consider). The proposed analysis is exemplified with data from Italian. Two verbs are described in detail: rimanere, 'remain' and rendere, 'make'. It is shown that subevent structure representation is useful for the description of the different uses and meanings of these verbs. This type of description can also be the basis for an accurate treatment in computational semantic lexica.",
        "id": 6580398
      },
      {
        "title": "Exploiting Scope for Shallow Discourse Parsing",
        "text": "We present an approach to automatically identifying the arguments of discourse connectives based on data from the Penn Discourse Treebank. Of the two arguments of connectives, called Arg1 and Arg2, we focus on Arg1, which has proven more challenging to identify. Our approach employs a sentence-based representation of arguments, and distinguishes intra-sentential connectives, which take both their arguments in the same sentence, from inter-sentential connectives, whose arguments are found in different sentences. The latter are further distinguished by paragraph position into ParaInit connectives, which appear in a paragraph-initial sentence, and ParaNonInit connectives, which appear elsewhere. The paper focusses on predicting Arg1 of Inter-sentential ParaNonInit connectives, presenting a set of scope-based filters that reduce the search space for Arg1 from all the previous sentences in the paragraph to a subset of them. For cases where these filters do not uniquely identify Arg1, coreference-based heuristics are employed. Our analysis shows an absolute 3% performance improvement over the high baseline of 83.3% for identifying Arg1 of Inter-sentential ParaNonInit connectives.",
        "id": 8511010
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper produces a dataset for text simplification in over 12 languages and evaluates both finetuning and in context learning approaches to text simplification in those languages?",
    "positive_ctxs": [
      {
        "title": "Revisiting non-English Text Simplification: A Unified Multilingual Benchmark",
        "text": "Recent advancements in high-quality, largescale English resources have pushed the frontier of English Automatic Text Simplification (ATS) research. However, less work has been done on multilingual text simplification due to the lack of a diverse evaluation benchmark that covers complex-simple sentence pairs in many languages. This paper introduces the MULTI-SIM benchmark, a collection of 27 resources in 12 distinct languages containing over 1.7 million complex-simple sentence pairs. This benchmark will encourage research in developing more effective multilingual text simplification models and evaluation metrics. Our experiments using MULTISIM with pre-trained multilingual language models reveal exciting performance improvements from multilingual training in non-English settings. We observe strong performance from Russian in zero-shot crosslingual transfer to low-resource languages. We further show that few-shot prompting with BLOOM-176b achieves comparable quality to reference simplifications outperforming finetuned models in most languages. We validate these findings through human evaluation. 1",
        "id": 258887622
      }
    ],
    "negative_ctxs": [
      {
        "title": "Flexible Speech Act Based Dialogue Management",
        "text": "We present an application independent dialogue engine that reasons on application dependent knowledge sources to calculate predictions about how a dialogue might continue. Predictions are language independent and are translated into language dependent structures for recognition and synthesis. Further, we discuss how the predictions account for different kinds of dialogue, e.g., question-answer or mixed initiative.",
        "id": 2072555
      },
      {
        "title": "nlpUP at SemEval-2019 Task 6: A Deep Neural Language Model for Offensive Language Detection",
        "text": "This paper presents our submission for the SemEval shared task 6, sub-task A on the identification of offensive language. Our proposed model, C-BiGRU, combines a Convolutional Neural Network (CNN) with a bidirectional Recurrent Neural Network (RNN). We utilize word2vec to capture the semantic similarities between words. This composition allows us to extract long term dependencies in tweets and distinguish between offensive and non-offensive tweets. In addition, we evaluate our approach on a different dataset and show that our model is capable of detecting online aggressiveness in both English and German tweets. Our model achieved a macro F1-score of 79.40% on the SemEval dataset.",
        "id": 184483257
      },
      {
        "title": "Part-of-speech Tagging of Code-Mixed Social Media Text",
        "text": "A common step in the processing of any text is the part-of-speech tagging of the input text. In this paper, we present an approach to tackle code-mixed text from three different languages Bengali, Hindi, and Tamilapart from English. Our system uses Conditional Random Field, a sequence learning method, which is useful to capture patterns of sequences containing code switching to tag each word with accurate part-of-speech information. We have used various pre-processing and post-processing modules to improve the performance of our system. The results were satisfactory, with a highest of 75.22% accuracy in Bengali-English mixed data. The methodology that we employed in the task can be used for any resource poor language. We adapted standard learning approaches that work well with scarce data. We have also ensured that the system is portable to different platforms and languages and can be deployed for real-time analysis.",
        "id": 5471567
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which knowledge graph completion method focuses on reducing memory usage by pruning features?",
    "positive_ctxs": [
      {
        "title": "GreenKGC: A Lightweight Knowledge Graph Completion Method",
        "text": "Knowledge graph completion (KGC) aims to discover missing relationships between entities in knowledge graphs (KGs). Most prior KGC work focuses on learning embeddings for entities and relations through a simple scoring function. Yet, a higher-dimensional embedding space is usually required for a better reasoning capability, which leads to a larger model size and hinders applicability to real-world problems (e.g., large-scale KGs or mobile/edge computing). A lightweight modularized KGC solution, called GreenKGC, is proposed in this work to address this issue. GreenKGC consists of three modules: representation learning, feature pruning, and decision learning, to extract discriminant KG features and make accurate predictions on missing relationships using classifiers and negative sampling. Experimental results demonstrate that, in low dimensions, GreenKGC can outperform SOTA methods in most datasets. In addition, low-dimensional GreenKGC can achieve competitive or even better performance against high-dimensional models with a much smaller model size. We make our code publicly available. 1",
        "id": 251710245
      }
    ],
    "negative_ctxs": [
      {
        "title": "Investigação Preliminar Sobre a Prosódia Semântica de Verbos de Elocução: o Caso do Verbo \"Confessar\"",
        "text": "The following article presents a preliminary study based on corpora about the semantic prosody of discendi verbs in Brazilian Portuguese. Thos study was based on a glossary of discendi verbs in Portuguese built by Freitas (2016) and on Ebeling's research (2014)  about the semantic prosody of verb phrases. At first, the verb \"confessar\" was selected from the glossary in order to test a possible methodology. Based on the results generated by this methodology, we were able to identify the semantic prosody of \"confessor\" and identify gaps and obstacles for possible unfoldings for this study.(2016)que elaborou um glossário de verbos de elocução em português, e na pesquisa de Ebeling (2014), que investiga a prosódia semântica de sintagmas verbais. A princípio, selecionamos o verbo \"confessar\" no glossário de Freitas (2016) para testarmos uma possível metodologia. Em conclusão, julgamos que a metodologia desenvolvida gerou resultados e, a partir dela, pudemos identificar a prosódia semântica do verbo \"confessar\" e apontar lacunas e dificuldades para o desdobramento da pesquisa.Resumo. Neste artigo, apresentamos primeiros olhares para a investigação da prosódia semântica de verbos de elocução em língua portuguesa, com base em corpora. O trabalho foi inspirado na dissertação de Mestrado de Freitas",
        "id": 39476550
      },
      {
        "title": "Generating Text from Language Models",
        "text": "An increasingly large percentage of natural language processing (NLP) tasks center around the generation of text from probabilistic language models. Despite this trend, techniques for improving or specifying preferences in these generated texts rely mostly on intuition-based heuristics. Further, there lacks a unified presentation of their motivations, practical implementation, successes and pitfalls. Practitioners must, therefore, choose somewhat blindly between generation algorithms-like top-p sampling or beam search-which can lead to wildly different results. At the same time, language generation research continues to criticize and improve the standard toolboxes, further adding entropy to the state of the field. In this tutorial, we will provide a centralized and cohesive discussion of critical considerations when choosing how to generate from a language model. We will cover a wide range of empirically-observed problems (like degradation, hallucination, repetition) and their corresponding proposed algorithmic solutions from recent research (like topp sampling and its successors). We will then discuss a subset of these algorithms under a unified light; most stochastic generation strategies can be framed as locally adapting the probabilities of a model to avoid failure cases. Finally, we will then cover methods in controlled generation, that go beyond just ensuring coherence to ensure text exhibits specific desired properties. We aim for NLP practitioners and researchers to leave our tutorial with a unified framework which they can use to evaluate and contribute to the latest research in language generation.",
        "id": 259370827
      },
      {
        "title": "THE LOGICAL ANALYSIS OF LEXICAL AMBIGUITY Abstract",
        "text": "",
        "id": 1296084
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which paper studies how difficult is a policy learning problem under non-additive rewards in terms of theoretical lower bounds, and what could be a potential strategy to solve it empirically while recovering some specialized guarantees?",
    "positive_ctxs": [
      {
        "title": "Submodular Reinforcement Learning",
        "text": "In reinforcement learning (RL), rewards of states are typically considered additive, and following the Markov assumption, they are independent of states visited previously. In many important applications, such as coverage control, experiment design and informative path planning, rewards naturally have diminishing returns, i.e., their value decreases in light of similar states visited previously. To tackle this, we propose submodular RL (SUBRL), a paradigm which seeks to optimize more general, non-additive (and history-dependent) rewards modelled via submodular set functions which capture diminishing returns. Unfortunately, in general, even in tabular settings, we show that the resulting optimization problem is hard to approximate. On the other hand, motivated by the success of greedy algorithms in classical submodular optimization, we propose SUBPO, a simple policy gradient-based algorithm for SUBRL that handles non-additive rewards by greedily maximizing marginal gains. Indeed, under some assumptions on the underlying Markov Decision Process (MDP), SUBPO recovers optimal constant factor approximations of submodular bandits. Moreover, we derive a natural policy gradient approach for locally optimizing SUBRL instances even in large state-and action-spaces. We showcase the versatility of our approach by applying SUBPO to several applications such as biodiversity monitoring, Bayesian experiment design, informative path planning, and coverage maximization. Our results demonstrate sample efficiency, as well as scalability to high-dimensional state-action spaces.",
        "id": 260154786
      }
    ],
    "negative_ctxs": [
      {
        "title": "Forest-to-String Statistical Translation Rules",
        "text": "In this paper, we propose forest-to-string rules to enhance the expressive power of tree-to-string translation models. A forestto-string rule is capable of capturing nonsyntactic phrase pairs by describing the correspondence between multiple parse trees and one string. To integrate these rules into tree-to-string translation models, auxiliary rules are introduced to provide a generalization level. Experimental results show that, on the NIST 2005 Chinese-English test set, the tree-to-string model augmented with forest-to-string rules achieves a relative improvement of 4.3% in terms of BLEU score over the original model which allows treeto-string rules only.",
        "id": 18616120
      },
      {
        "title": "AUEB: Two Stage Sentiment Analysis of Social Network Messages",
        "text": "This paper describes the system submitted for the Sentiment Analysis in Twitter Task of SEMEVAL 2014 and specifically the Message Polarity Classification subtask. We used a 2-stage pipeline approach employing a linear SVM classifier at each stage and several features including morphological features, POS tags based features and lexicon based features.",
        "id": 491439
      },
      {
        "title": "Published as a conference paper at ICLR 2023 AIM: ADAPTING IMAGE MODELS FOR EFFICIENT VIDEO ACTION RECOGNITION",
        "text": "Recent vision transformer based video models mostly follow the \"image pretraining then finetuning\" paradigm and have achieved great success on multiple video benchmarks. However, full finetuning such a video model could be computationally expensive and unnecessary, given the pre-trained image transformer models have demonstrated exceptional transferability. In this work, we propose a novel method to Adapt pre-trained Image Models (AIM) for efficient video understanding. By freezing the pre-trained image model and adding a few lightweight Adapters, we introduce spatial adaptation, temporal adaptation and joint adaptation to gradually equip an image model with spatiotemporal reasoning capability. We show that our proposed AIM can achieve competitive or even better performance than prior arts with substantially fewer tunable parameters on four video action recognition benchmarks. Thanks to its simplicity, our method is also generally applicable to different image pre-trained models, which has the potential to leverage more powerful image foundation models in the future. The project webpage is https://adapt-image-models.github.io/. * Work done during an internship at Amazon Web Services.",
        "id": 256615635
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "What research has been conducted on applying contrastive techniques to distinguish normal from abnormal imagery for the creation of radiology reports?",
    "positive_ctxs": [
      {
        "title": "Contrastive Attention for Automatic Chest X-ray Report Generation",
        "text": "Recently, chest X-ray report generation, which aims to automatically generate descriptions of given chest X-ray images, has received growing research interests. The key challenge of chest X-ray report generation is to accurately capture and describe the abnormal regions. In most cases, the normal regions dominate the entire chest X-ray image, and the corresponding descriptions of these normal regions dominate the final report. Due to such data bias, learning-based models may fail to attend to abnormal regions. In this work, to effectively capture and describe abnormal regions, we propose the Contrastive Attention (CA) model. Instead of solely focusing on the current input image, the CA model compares the current input image with normal images to distill the contrastive information. The acquired contrastive information can better represent the visual features of abnormal regions. According to the experiments on the public IU-X-ray and MIMIC-CXR datasets, incorporating our CA into several existing models can boost their performance across most metrics. In addition, according to the analysis, the CA model can help existing models better attend to the abnormal regions and provide more accurate descriptions which are crucial for an interpretable diagnosis. Specifically, we achieve the state-ofthe-art results on the two public datasets.",
        "id": 235422047
      }
    ],
    "negative_ctxs": [
      {
        "title": "Personality Profiling of Fictional Characters using Sense-Level Links between Lexical Resources",
        "text": "This study focuses on personality prediction of protagonists in novels based on the Five-Factor Model of personality. We present and publish a novel collaboratively built dataset of fictional character personality and design our task as a text classification problem. We incorporate a range of semantic features, including WordNet and VerbNet sense-level information and word vector representations. We evaluate three machine learning models based on the speech, actions and predicatives of the main characters, and show that especially the lexical-semantic features significantly outperform the baselines. The most predictive features correspond to reported findings in personality psychology.",
        "id": 10679333
      },
      {
        "title": "Identifying attack and support argumentative relations using deep learning",
        "text": "We propose a deep learning architecture to capture argumentative relations of attack and support from one piece of text to another, of the kind that naturally occur in a debate. The architecture uses two (unidirectional or bidirectional) Long Short-Term Memory networks and (trained or non-trained) word embeddings, and allows to considerably improve upon existing techniques that use syntactic features and supervised classifiers for the same form of (relation-based) argument mining.",
        "id": 28193257
      },
      {
        "title": "Fast(er) Exact Decoding and Global Training for Transition-Based Dependency Parsing via a Minimal Feature Set",
        "text": "We first present a minimal feature set for transition-based dependency parsing, continuing a recent trend started by Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) of using bi-directional LSTM features. We plug our minimal feature set into the dynamic-programming framework of Huang and Sagae(2010)andKuhlmann et al. (2011)to produce the first implementation of worst-case Opn 3 q exact decoders for arc-hybrid and arceager transition systems. With our minimal features, we also present Opn 3 q global training methods. Finally, using ensembles including our new parsers, we achieve the best unlabeled attachment score reported (to our knowledge) on the Chinese Treebank and the \"second-best-in-class\" result on the English Penn Treebank.",
        "id": 269533
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there any paper leverages knowledge distillation of language models for textual out-of-distribution detection or anomaly detection?",
    "positive_ctxs": [
      {
        "title": "Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text",
        "text": "Self-supervised representation learning has proved to be a valuable component for outof-distribution (OoD) detection with only the texts of in-distribution (ID) examples. These approaches either train a language model from scratch or fine-tune a pre-trained language model using ID examples, and then take the perplexity output by the language model as OoD scores. In this paper, we analyze the complementary characteristics of both OoD detection methods and propose a multi-level knowledge distillation approach that integrates their strengths while mitigating their limitations. Specifically, we use a fine-tuned model as the teacher to teach a randomly initialized student model on the ID examples. Besides the prediction layer distillation, we present a similarity-based intermediate layer distillation method to thoroughly explore the representation space of the teacher model. In this way, the learned student can better represent the ID data manifold while gaining a stronger ability to map OoD examples outside the ID data manifold with the regularization inherited from pre-training. Besides, the student model sees only ID examples during parameter learning, further promoting more distinguishable features for OoD detection. We conduct extensive experiments over multiple benchmark datasets, i.e., CLINC150, SST, ROSTD, 20 NewsGroups, and AG News; showing that the proposed method yields new state-of-the-art performance 1 . We also explore its application as an AIGC detector to distinguish between answers generated by ChatGPT and human experts. It is observed that our model exceeds human evaluators in the pair-expert task on the Human ChatGPT Comparison Corpus.",
        "id": 253734483
      }
    ],
    "negative_ctxs": [
      {
        "title": "Using any machine translation source for fuzzy-match repair in a computer-aided translation setting",
        "text": "When a computer-assisted translation (CAT) tool does not find an exact match for the source segment to translate in its translation memory (TM), translators must use fuzzy matches that come from translation units in the translation memory that do not completely match the source segment. We explore the use of a fuzzy-match repair technique called patching to repair translation proposals from a TM in a CAT environment using any available machine translation system, or any external bilingual source, regardless of its internals. Patching attempts to aid CAT tool users by repairing fuzzy matches and proposing improved translations. Our results show that patching improves the quality of translation proposals and reduces the amount of edit operations to perform, especially when a specific set of restrictions is applied. use a fuzzy-match score threshold (FMT), for instance 80%, to reduce the number of translation proposals.When a perfect match is not found in the translation memory, and before making any changes to the TL segment t in the proposed translation unit, the translator has to identify the sub-segments of t that correspond to the sub-segments of s that are not common to s . To help in finding the sub-segments of t that need to be edited, but without actually editing them, Esplà-Gomis et al. (2011) use machine translation (MT) to find sub-segment alignments between s and t, and train a classifier to classify the words in t as words to be kept unedited or words to be changed to transform t into the desired translation t .Other researchers have gone one step further and have explored different ways to combine the TL segment of the proposed translation unit and the output of a statistical machine translation (SMT) system to produce a translation closer to t . Biçici and Dymetman(2008), for example, use a phrase-based SMT system trained on a bilingual corpus in the same domain as the TM and combine it with the TM's fuzzy match by extracting a phrase table that is dynamically added to the usual set of bi-phrases used for decoding the source. Their implementation augments the internal translation table in the SMT system with bilingual discontiguous sub-segments (phrases) that have source sub-segments in common with s . Alignments in the system created byBiçici and Dymetman (2008)are detected using word alignments directly obtained from the SMT system training process and are used to find the parts of t that need to be edited (mismatches).Similarly, Simard and Isabelle (2009) use a phrase-based SMT system by adding phrase pairs (sub-segment pairs) of any length (obtained using a statistical aligner on the TM) to the SMT system's phrase table and introduce a feature to indicate that the phrase-pairs came from their TM. After that, they optimize the weighting of the TM-based phrase table in a regular SMT decoder. By means of optimization and phrase table inclusion they are able to make their SMT system produce a translation close to the desired translation t .Additional work done by Zhechev and Genabith (2010) makes use of a phrase-based SMT system along with an alignment method that, like Simard and Isabelle (2009), connects subsegments from the target translation t with those in s. The alignment method Zhechev and Genabith (2010) use takes advantage of a tree-based structural alignment created from a bilingual dictionary after training their SMT system with phrase pairs. After aligning the words in s with those in t, Zhechev and Genabith (2010) are able to identify words that should appear in the final translation t .Koehn and Senellart (2010) take a similar approach toBiçici and Dymetman (2008). They first align words in s and s to find mismatches. Then, they align the words in s and t to identify target matches and remove the words in t that are aligned to the mismatched words in s. Target mismatches are sent to the SMT decoder for translation. Mismatched words in Koehn and Senellart (2010)'s system are treated separately; that is, context around a mismatch, while indirectly taken into account by the language model, is not directly taken into account of when applying phrase pairs. Ma et al.(2011), on the other hand, decided to research the shortcomings of using a fuzzymatch score as a threshold for determining translation unit matches that serve as translations for other segments. Ma et al. (2011)'s approach uses discriminative learning and support vector machines to salvage translations of matched words to select a translation unit that would have been otherwise thrown away due to the fuzzy-match score being used as a threshold. Their work, unlike Koehn and Senellart (",
        "id": 16546204
      },
      {
        "title": "Using Shallow Syntactic Features to Measure Influences of L1 and Proficiency Level in EFL Writings",
        "text": "This paper proposes a framework for modeling and analyzing differences between texts written by different subgroups of learners of English as a Foreign Language (organized according to native language (L1) and proficiency level). Using frequency vectors of both POS-trigrams and mixed POS and function word trigrams, we compare learner language variants both to each other and to native English, German, and Chinese texts. We introduce the trigram usage factor metric for identifying sequences that are especially characteristic of a particular subgroup of learners. We show that distance between learner English and native English decreases with proficiency. Next we compare the distance between learner English and other native languages. Finally, we show that automatic proficiency classification benefits from using L1-specific classifiers. 2015. Using shallow syntactic features to measure influences of L1 and proficiency level in EFL writings.",
        "id": 14037433
      },
      {
        "title": "Asynchronous Binarization for Synchronous Grammars",
        "text": "Binarization of n-ary rules is critical for the efficiency of syntactic machine translation decoding. Because the target side of a rule will generally reorder the source side, it is complex (and sometimes impossible) to find synchronous rule binarizations. However, we show that synchronous binarizations are not necessary in a two-stage decoder. Instead, the grammar can be binarized one way for the parsing stage, then rebinarized in a different way for the reranking stage. Each individual binarization considers only one monolingual projection of the grammar, entirely avoiding the constraints of synchronous binarization and allowing binarizations that are separately optimized for each stage. Compared to n-ary forest reranking, even simple target-side binarization schemes improve overall decoding accuracy.",
        "id": 5237589
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "What research exists comparing adapter-based tuning and full fine-tuning efficacy in limited data contexts?",
    "positive_ctxs": [
      {
        "title": "UNIPELT: A Unified Framework for Parameter-Efficient Language Model Tuning",
        "text": "Recent parameter-efficient language model tuning (PELT) methods manage to match the performance of fine-tuning with much fewer trainable parameters and perform especially well when training data is limited. However, different PELT methods may perform rather differently on the same task, making it nontrivial to select the most appropriate method for a specific task, especially considering the fast-growing number of new PELT methods and tasks. In light of model diversity and the difficulty of model selection, we propose a unified framework, UNIPELT, which incorporates different PELT methods as submodules and learns to activate the ones that best suit the current data or task setup via gating mechanism. On the GLUE benchmark, UNIPELT consistently achieves 1~4% gains compared to the best individual PELT method that it incorporates and outperforms fine-tuning under different setups. Moreover, UNIPELT generally surpasses the upper bound that takes the best performance of all its submodules used individually on each task, indicating that a mixture of multiple PELT methods may be inherently more effective than single methods.",
        "id": 238857301
      },
      {
        "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
        "text": "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were \"virtual tokens\". We apply prefix-tuning to GPT-2 for table-totext generation and to BART for summarization. We show that by modifying only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. 2020. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. Amodei. 2020. Language models are few-shot learners.",
        "id": 230433941
      }
    ],
    "negative_ctxs": [
      {
        "title": "American Journal of Computational LinguisticsMuIIilin~.ua1 Dictionary",
        "text": "This documerrt is a brief introdilct ion to Carnegie-Mellon University's il~teractive con~p~terized m~rltilitrg~ral dictionary. It describes the use of Rhis cfictionary both by translatots in the course of their vjork and by the i r r~r i n o l o g c s~s responsible for updating and maintaining it. This discussion i s placed in the context of the overall effort (known as the target Project)   to provide attls to translators. A final seclion presents the solution to the problem of representation bf term equivalence adopted in Target. F e r e~g n R r o a d c~s t Information S e r v l c~~ eemlner on Alda 10 T~R T I B~R~O~B , Wsihingfon, DC, Mayq 1970 The rfinf~furalron in dally uee by the Tranelnl~orr C~tlter r i CRrnea~s-Mellan Unrvere~ly involves r Lerr Siogler ADM-3 t~r m i n a l connected by a 300'baud d~al up !me \\ a e POP-10 run under the TOPS-I0 operaling aystsm by the Computer Sclencfi Department ~n d shared e~multaneouely by user8 worklnt on many d~ffstent ptojectr",
        "id": 219301746
      },
      {
        "title": "Direct Embedding of Temporal Network Edges via Time-Decayed Line Graphs",
        "text": "Temporal networks model a variety of important phenomena involving timed interactions between entities. Existing methods for machine learning on temporal networks generally exhibit at least one of two limitations. First, time is assumed to be discretized, so if the time data is continuous, the user must determine the discretization and discard precise time information. Second, edge representations can only be calculated indirectly from the nodes, which may be suboptimal for tasks like edge classification. We present a simple method that avoids both shortcomings: construct the line graph of the network, which includes a node for each interaction, and weigh the edges of this graph based on the difference in time between interactions. From this derived graph, edge representations for the original network can be computed with efficient classical methods. The simplicity of this approach facilitates explicit theoretical analysis: we can constructively show the effectiveness of our method's representations for a natural synthetic model of temporal networks. Empirical results on real-world networks demonstrate our method's efficacy and efficiency on both edge classification and temporal link prediction.",
        "id": 252682995
      },
      {
        "title": "",
        "text": "",
        "id": 3035164
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Is there any generalizable NeRF paper that disentangles texture and shape?",
    "positive_ctxs": [
      {
        "title": "TUVF: LEARNING GENERALIZABLE TEXTURE UV RADIANCE FIELDS",
        "text": "Figure 1: We propose Texture UV Radiance Fields (TUVF) to render a 3D consistent texture given a 3D object shape input.TUVF provides a category-level texture representation disentangled from 3D shapes.Top three rows: TUVF can synthesize realistic textures by training from a collection of single-view images; Fourth row: Given a 3D shape input, we can render different textures on top by using different texture codes; Bottom row: We can perform editing on a given texture (adding a flag of France) and directly apply the same texture on different 3D shapes without further fine-tuning.Note that all samples are rendered under 1024×1024 resolution; zoom-in is recommended.",
        "id": 258480014
      }
    ],
    "negative_ctxs": [
      {
        "title": "Modeling Violations of Selectional Restrictions with Distributional Semantics",
        "text": "Distributional Semantic Models have been successfully used for modeling selectional preferences in a variety of scenarios, since distributional similarity naturally provides an estimate of the degree to which an argument satisfies the requirement of a given predicate. However, we argue that the performance of such models on rare verb-argument combinations has received relatively little attention: it is not clear whether they are able to distinguish the combinations that are simply atypical, or implausible, from the semantically anomalous ones, and in particular, they have never been tested on the task of modeling their differences in processing complexity. In this paper, we compare two different models of thematic fit by testing their ability of identifying violations of selectional restrictions in two datasets from the experimental studies. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 1 See McRae and Matsuki (2009) for an overview. 2 A partial exception is the study on semantic deviance by Vecchi et al. (2011). However, they focus on the acceptability of adjectival phrases, rather than on selectional preferences.",
        "id": 53471894
      },
      {
        "title": "A Deterministic Dependency Parser with Dynamic Programming for Sanskrit",
        "text": "We describe a Deterministic Dependency Parser for Sanskrit. The parse is developed following a Depth First traversal of a graph whose nodes represent morphological analyses of the words in a sentence. During the traversal, relations at each node are checked for local compatibility, and finally for each full path, the relations on the path are checked for global compatibility. Stacking of intermediate results guarantees dynamic programming. We also describe an interface that displays multiple parses compactly and facilitates users to select the desired parse among various possible solutions with a maximum of n − 1 choices for a sentence with n words.",
        "id": 16258834
      },
      {
        "title": "",
        "text": "Providing natural language instructions in prompts is a useful new paradigm for improving task performance of large language models in a zero-shot setting. Recent work has aimed to improve such prompts via manual rewriting or gradient-based tuning. However, manual rewriting is time-consuming and requires subjective interpretation, while gradient-based tuning can be extremely computationally demanding for large models and may not be feasible for API-based models. In this work, we introduce Gradient-free Instructional Prompt Search (GRIPS), a gradient-free, edit-based search approach for improving task instructions for large language models. GRIPS takes in instructions designed for humans and automatically returns an improved, edited prompt, while allowing for API-based tuning. With Instruct-GPT models, GRIPS improves the average task performance by up to 4.30 percentage points on eight classification tasks from the NATU-RAL-INSTRUCTIONS dataset (with similar improvements for OPT, BLOOM, and FLAN-T5). We see improvements for both instructiononly prompts and instruction + k-shot examples prompts. Notably, GRIPS outperforms manual rewriting and purely example-based prompts while controlling for the available compute and data budget. Further, performance of GRIPS is comparable to select gradient-based tuning approaches. Qualitatively, we show our edits can simplify instructions and at times make them incoherent but nonetheless improve accuracy. , et al. 2022. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.",
        "id": 247447170
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Is there any paper trying to improve MLE for auto-regressive language modeling through the lens of optimal transport?",
    "positive_ctxs": [
      {
        "title": "EMO: EARTH MOVER DISTANCE OPTIMIZATION FOR AUTO-REGRESSIVE LANGUAGE MODELING",
        "text": "Neural language models are probabilistic models of human text.They are predominantly trained using maximum likelihood estimation (MLE), which is equivalent to minimizing the forward cross-entropy between the empirical data distribution and the model distribution.However, various degeneration phenomena are still widely observed when decoding from the distributions learned by such models.We establish that the forward cross-entropy is suboptimal as a distance metric for aligning human and model distribution due to its (1) recall-prioritization (2) negative diversity ignorance and (3) train-test mismatch.In this paper, we propose Earth Mover Distance Optimization (EMO) for auto-regressive language modeling.EMO capitalizes on the inherent properties of earth mover distance to address the aforementioned challenges.Due to the high complexity of direct computation, we further introduce a feasible upper bound for EMO to ease end-to-end training.Upon extensive evaluation of language models trained using EMO and MLE.We find that EMO demonstrates a consistently better language modeling performance than MLE across domains.Moreover, EMO demonstrates noteworthy enhancements in downstream performance with minimal fine-tuning on merely 25,000 sentences.This highlights the tremendous potential of EMO as a lightweight calibration method for enhancing large-scale pre-trained language models.Our code and data are available at https://github.com/DRSY/EMO.",
        "id": 263829780
      }
    ],
    "negative_ctxs": [
      {
        "title": "Extracting Person Names from User Generated Text: Named-Entity Recognition for Combating Human Trafficking",
        "text": "Online escort advertisement websites are widely used for advertising victims of human trafficking. Domain experts agree that advertising multiple people in the same ad is a strong indicator of trafficking. Thus, extracting person names from the text of these ads can provide valuable clues for further analysis. However, Named-Entity Recognition (NER) on escort ads is challenging because the text can be noisy, colloquial and often lacking proper grammar and punctuation. Most existing state-of-the-art NER models fail to demonstrate satisfactory performance in this task. In this paper, we propose NEAT (Name Extraction Against Trafficking) for extracting person names. It effectively combines classic rule-based and dictionary extractors with a contextualized language model to capture ambiguous names (e.g penny, hazel) and adapts to adversarial changes in the text by expanding its dictionary. NEAT shows 19% improvement on average in the F1 classification score for name extraction compared to previous state-of-the-art in two domain-specific datasets.",
        "id": 248779990
      },
      {
        "title": "",
        "text": "",
        "id": 39190241
      },
      {
        "title": "Challenges in Finding Metaphorical Connections",
        "text": "Poetry is known for its novel expression using figurative language. We introduce a writing task that contains the essential challenges of generating meaningful figurative language and can be evaluated. We investigate how to find metaphorical connections between abstract themes and concrete domains by asking people to write four-line poems on a given metaphor, such as \"death is a rose\" or \"anger is wood\". We find that only 24% of poems successfully make a metaphorical connection. We present five alternate ways people respond to the prompt and release our dataset of 186 categorized poems. We suggest opportunities for computational approaches.",
        "id": 51998437
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Has any research explored using other off-the-shelf summarization techniques to improve neural abstractive summarization?",
    "positive_ctxs": [
      {
        "title": "GSum: A General Framework for Guided Neural Abstractive Summarization",
        "text": "Neural abstractive summarization models are flexible and can produce coherent summaries, but they are sometimes unfaithful and can be difficult to control. While previous studies attempt to provide different types of guidance to control the output and increase faithfulness, it is not clear how these strategies compare and contrast to each other. In this paper, we propose a general and extensible guided summarization framework (GSum) that can effectively take different kinds of external guidance as input, and we perform experiments across several different varieties. Experiments demonstrate that this model is effective, achieving state-of-the-art performance according to ROUGE on 4 popular summarization datasets when using highlighted sentences as guidance. In addition, we show that our guided model can generate more faithful summaries and demonstrate how different types of guidance generate qualitatively different summaries, lending a degree of controllability to the learned models. 1",
        "id": 223953416
      }
    ],
    "negative_ctxs": [
      {
        "title": "Modular and Efficient Top-Down Parsing for Ambiguous Left-Recursive Grammars",
        "text": "In functional and logic programming, parsers can be built as modular executable specifications of grammars, using parser combinators and definite clause grammars respectively. These techniques are based on top-down backtracking search. Commonly used implementations are inefficient for ambiguous languages, cannot accommodate left-recursive grammars, and require exponential space to represent parse trees for highly ambiguous input. Memoization is known to improve efficiency, and work by other researchers has had some success in accommodating left recursion. This paper combines aspects of previous approaches and presents a method by which parsers can be built as modular and efficient executable specifications of ambiguous grammars containing unconstrained left recursion.",
        "id": 18749678
      },
      {
        "title": "A Pipeline Japanese Entity Linking System with Embedding Features",
        "text": "Entity linking (EL) is the task of connecting mentions in texts to entities in a large-scale knowledge base such as Wikipedia. In this paper, we present a pipeline system for Japanese EL which consists of two standard components, namely candidate generation and candidate ranking. We investigate several techniques for each component, using a recently developed Japanese EL corpus. For candidate generation, we find that a concept dictionary using anchor texts of Wikipedia is more effective than methods based on surface similarity. For candidate ranking, we verify that a set of features used in English EL is effective in Japanese EL as well. In addition, by using a corpus that links Japanese mentions to Japanese Wikipedia entries, we are able to get rich context information from Japanese Wikipedia articles and benefit mention disambiguation. It was not directly possible with previous EL corpora, which associate mentions to English Wikipedia entities. We take this advantage by exploring several embedding models that encode context information of Wikipedia entities, and show that they improve candidate ranking. As a whole, our system achieves 82.27% accuracy, significantly outperforming previous work.",
        "id": 12207182
      },
      {
        "title": "Learning Bilingual Lexicons from Monolingual Corpora",
        "text": "We present a method for learning bilingual translation lexicons from monolingual corpora. Word types in each language are characterized by purely monolingual features, such as context counts and orthographic substrings. Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings. We show that high-precision lexicons can be learned in a variety of language pairs and from a range of corpus types.",
        "id": 7185434
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that shows multilingual language models can understand plural/singular verb agreement across multiple languages?",
    "positive_ctxs": [
      {
        "title": "Data-driven Cross-lingual Syntax: An Agreement Study with Massively Multilingual Models",
        "text": "Massively multilingual models such as mBERT and XLM-R are increasingly valued in Natural Language Processing research and applications, due to their ability to tackle the uneven distribution of resources available for different languages. The models' ability to process multiple languages relying on a shared set of parameters raises the question of whether the grammatical knowledge they extracted during pre-training can be considered as a data-driven cross-lingual grammar. The present work studies the inner workings of mBERT and XLM-R in order to test the cross-lingual consistency of the individual neural units that respond to a precise syntactic phenomenon, that is, number agreement, in five languages (English, German, French, Hebrew, Russian). We found that there is a significant overlap in the latent dimensions that encode agreement across the languages we considered. This overlap is larger (a) for long-vis-à-vis shortdistance agreement and (b) when considering XLM-R as compared to mBERT, and peaks in the intermediate layers of the network. We further show that a small set of syntax-sensitive neurons can capture agreement violations across languages; however, their contribution is not decisive in agreement processing.",
        "id": 255894238
      }
    ],
    "negative_ctxs": [
      {
        "title": "Cross-Lingual Morphological Tagging for Low-Resource Languages",
        "text": "Morphologically rich languages often lack the annotated linguistic resources required to develop accurate natural language processing tools. We propose models suitable for training morphological taggers with rich tagsets for low-resource languages without using direct supervision. Our approach extends existing approaches of projecting part-of-speech tags across languages, using bitext to infer constraints on the possible tags for a given word type or token. We propose a tagging model using Wsabie, a discriminative embeddingbased model with rank-based learning. In our evaluation on 11 languages, on average this model performs on par with a baseline weakly-supervised HMM, while being more scalable. Multilingual experiments show that the method performs best when projecting between related language pairs. Despite the inherently lossy projection, we show that the morphological tags predicted by our models improve the downstream performance of a parser by +0.6 LAS on average.",
        "id": 565312
      },
      {
        "title": "",
        "text": "",
        "id": 232021826
      },
      {
        "title": "",
        "text": "",
        "id": 219303177
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Is there a paper that takes a mixed machine learning and solver based approach to code translation?",
    "positive_ctxs": [
      {
        "title": "GUESS & SKETCH: LANGUAGE MODEL GUIDED TRANSPILATION",
        "text": "Maintaining legacy software requires many software and systems engineering hours.Assembly code programs, which demand low-level control over the computer machine state and have no variable names, are particularly difficult for humans to analyze.Existing conventional program translators guarantee correctness, but are hand-engineered for the source and target programming languages in question.Learned transpilation, i.e. automatic translation of code, offers an alternative to manual re-writing and engineering efforts.Automated symbolic program translation approaches guarantee correctness but struggle to scale to longer programs due to the exponentially large search space.Their rigid rule-based systems also limit their expressivity, so they can only reason about a reduced space of programs.Probabilistic neural language models (LMs) produce plausible outputs for every input, but do so at the cost of guaranteed correctness.In this work, we leverage the strengths of LMs and symbolic solvers in a neurosymbolic approach to learned transpilation for assembly code.Assembly code is an appropriate setting for a neurosymbolic approach, since assembly code can be divided into shorter non-branching basic blocks amenable to the use of symbolic methods.GUESS & SKETCH extracts alignment and confidence information from features of the LM then passes it to a symbolic solver to resolve semantic equivalence of the transpilation input and output.We test GUESS & SKETCH on three different test sets of assembly transpilation tasks, varying in difficulty, and show that it successfully transpiles 57.6% more examples than GPT-4 and 39.6% more examples than an engineered transpiler.We also share a training and evaluation dataset for this task.",
        "id": 262828485
      }
    ],
    "negative_ctxs": [
      {
        "title": "Word Frequency Does Not Predict Grammatical Knowledge in Language Models",
        "text": "Neural language models learn, to varying degrees of accuracy, the grammatical properties of natural languages. In this work, we investigate whether there are systematic sources of variation in the language models' accuracy. Focusing on subject-verb agreement and reflexive anaphora, we find that certain nouns are systematically understood better than others, an effect which is robust across grammatical tasks and different language models. Surprisingly, we find that across four orders of magnitude, corpus frequency is unrelated to a noun's performance on grammatical tasks. Finally, we find that a novel noun's grammatical properties can be few-shot learned from various types of training data. The results present a paradox: there should be less variation in grammatical performance than is actually observed.",
        "id": 226283660
      },
      {
        "title": "Improving Constituency Parsing with Span Attention",
        "text": "Constituency parsing is a fundamental and important task for natural language understanding, where a good representation of contextual information can help this task. N-grams, which is a conventional type of feature for contextual information, have been demonstrated to be useful in many tasks, and thus could also be beneficial for constituency parsing if they are appropriately modeled. In this paper, we propose span attention for neural chartbased constituency parsing to leverage n-gram information. Considering that current chartbased parsers with Transformer-based encoder represent spans by subtraction of the hidden states at the span boundaries, which may cause information loss especially for long spans, we incorporate n-grams into span representations by weighting them according to their contributions to the parsing process. Moreover, we propose categorical span attention to further enhance the model by weighting ngrams within different length categories, and thus benefit long-sentence parsing. Experimental results on three widely used benchmark datasets demonstrate the effectiveness of our approach in parsing Arabic, Chinese, and English, where state-of-the-art performance is obtained by our approach on all of them. 1 † Corresponding author. 1 Our code and the best performing models are released at https://github.com/cuhksz-nlp/SAPar.",
        "id": 222378720
      },
      {
        "title": "Generating Natural Language Proofs with Verifier-Guided Search",
        "text": "Reasoning over natural language is a challenging problem in NLP. In this work, we focus on proof generation: Given a hypothesis and a set of supporting facts, the model generates a proof tree indicating how to derive the hypothesis from supporting facts. Compared to generating the entire proof in one shot, stepwise generation can better exploit the compositionality and generalize to longer proofs but has achieved limited success on real-world data. Existing stepwise methods struggle to generate proof steps that are both logically valid and relevant to the hypothesis. Instead, they tend to hallucinate invalid steps given the hypothesis. In this paper, we present a novel stepwise method, NLProofS (Natural Language Proof Search), which learns to generate relevant steps conditioning on the hypothesis. At the core of our approach, we train an independent verifier to check the validity of the proof steps to prevent hallucination. Instead of generating steps greedily, we search for proofs maximizing a global proof score judged by the verifier. NL-ProofS achieves state-of-the-art performance on EntailmentBank and RuleTaker. Specifically, it improves the correctness of predicted proofs from 27.7% to 33.3% in the distractor setting of EntailmentBank, demonstrating the effectiveness of NLProofS in generating challenging human-authored proofs. 1",
        "id": 249062748
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that investigates how undetectable backdoor attacks are in NLP models?",
    "positive_ctxs": [
      {
        "title": "Rethinking Stealthiness of Backdoor Attack against NLP Models",
        "text": "Recent researches have shown that large natural language processing (NLP) models are vulnerable to a kind of security threat called the Backdoor Attack. Backdoor attacked models can achieve good performance on clean test sets but perform badly on those input sentences injected with designed trigger words. In this work, we point out a potential problem of current backdoor attacking research: its evaluation ignores the stealthiness of backdoor attacks, and most of existing backdoor attacking methods are not stealthy either to system deployers or to system users. To address this issue, we first propose two additional stealthiness-based metrics to make the backdoor attacking evaluation more credible. We further propose a novel word-based backdoor attacking method based on negative data augmentation and modifying word embeddings, making an important step towards achieving stealthy backdoor attacking. Experiments on sentiment analysis and toxic detection tasks show that our method is much stealthier while maintaining pretty good attacking performance. Our code is available at https://github.com/lancopku/SOS.",
        "id": 236459933
      }
    ],
    "negative_ctxs": [
      {
        "title": "A Valency Dictionary Architecture for Machine Translation",
        "text": "This research is aimed at developing a valency dictionary architecture to comprehensively list the full range of alternations associated with a given predicate sense, both efficiently and robustly. The architecture is designed to incorporate all information available in current on-line resources, as well as additional features such as argument status, grammatical relations, and an augmented case-role representation. Words are divided into senses, which are distinguished on semantic grounds, depending on the core lexical meaning of the verb. Each sense may have one or more alternations, thus keeping the number of senses manageable, while allowing for systematic variation in the lexical realization. Individual syntactic case frames are indexed back to the basic semantic argument component of the given predicate sense.",
        "id": 14993809
      },
      {
        "title": "The Shared Corpora Working Group Report",
        "text": "We seek to identify a limited amount of representative corpora, suitable for annotation by the computational linguistics annotation community. Our hope is that a wide variety of annotation will be undertaken on the same corpora, which would facilitate: (1) the comparison of annotation schemes; (2) the merging of information represented by various annotation schemes; (3) the emergence of NLP systems that use information in multiple annotation schemes; and (4) the adoption of various types of best practice in corpus annotation. Such best practices would include: (a) clearer demarcation of phenomena being annotated; (b) the use of particular test corpora to determine whether a particular annotation task can feasibly achieve good agreement scores; (c) The use of underlying models for representing annotation content that facilitate merging, comparison, and analysis; and (d) To the extent possible, the use of common annotation categories or a mapping among categories for the same phenomenon used by different annotation groups.This study will focus on the problem of identifying such corpora as well as the suitability of two candidate corpora: the Open portion of the American National Corpus(Ide and Macleod, 2001;Ide and Suderman, 2004)and the \"Controversial\" portions of the WikipediaXML corpus .",
        "id": 16694202
      },
      {
        "title": "Neural Attention Model for Classification of Sentences that Support Promoting/Suppressing Relationship",
        "text": "Evidences that support a claim \"a subject phrase promotes or suppresses a value\" help in making a rational decision. We aim to construct a model that can classify if a particular evidence supports a claim of a promoting/suppressing relationship given an arbitrary subject-value pair. In this paper, we propose a recurrent neural network (RNN) with an attention model to classify such evidences. We incorporated a word embedding technique in an attention model such that our method generalizes for never-encountered subjects and value phrases. Benchmarks showed that the method outperforms conventional methods in evidence classification tasks.",
        "id": 5703887
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "What are some studies that leverage statistical machine translation methodologies, like GIZA++, to improve the alignment process between words and grammar rules in the context of semantic parsing and generation of meaning representations?",
    "positive_ctxs": [
      {
        "title": "Learning for Semantic Parsing with Statistical Machine Translation",
        "text": "We present a novel statistical approach to semantic parsing, WASP, for constructing a complete, formal meaning representation of a sentence. A semantic parser is learned given a set of sentences annotated with their correct meaning representations. The main innovation of WASP is its use of state-of-the-art statistical machine translation techniques. A word alignment model is used for lexical acquisition, and the parsing model itself can be seen as a syntax-based translation model. We show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods requiring similar amount of supervision, and shows better robustness to variations in task complexity and word order.",
        "id": 7785983
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 233365154
      },
      {
        "title": "Gene-disease association extraction by text mining and network analysis",
        "text": "Biomedical relations play an important role in biological processes. In this work, we combine information filtering, grammar parsing and network analysis for gene-disease association extraction. The proposed method first extracts sentences potentially containing information about gene-diseases interactions based on maximum entropy classifier with topic features. And then Probabilistic Context-Free Grammars is applied for gene-disease association extraction. The network of genes and the disease is constituted by the extracted interactions, network centrality metrics are used for calculating the importance of each gene. We used breast cancer as testing disease for system evaluation. The 31 top ranked genes and diseases by the weighted degree, betweenness, and closeness centralities have been checked relevance with breast cancer through NCBI database. The evaluation showed 83.9% accuracy for the testing genes and diseases, 74.2% accuracy for the testing genes.",
        "id": 18012852
      },
      {
        "title": "",
        "text": "",
        "id": 219302391
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "In multi-hop question answering, is there a paper that explores \"per-hop\" retrieval evaluation that treats each hop of retrieval independently?",
    "positive_ctxs": [
      {
        "title": "Combining Lexical and Dense Retrieval for Computationally Efficient Multi-hop Question Answering",
        "text": "In simple open-domain question answering (QA), dense retrieval has become one of the standard approaches for retrieving the relevant passages to infer an answer. Recently, dense retrieval also achieved state-of-the-art results in multi-hop QA, where aggregating information from multiple pieces of information and reasoning over them is required. Despite their success, dense retrieval methods are computationally intensive, requiring multiple GPUs to train. In this work, we introduce a hybrid (lexical and dense) retrieval approach that is highly competitive with the state-of-the-art dense retrieval models, while requiring substantially less computational resources. Additionally, we provide an in-depth evaluation of dense retrieval methods on limited computational resource settings, something that is missing from the current literature.",
        "id": 237592852
      }
    ],
    "negative_ctxs": [
      {
        "title": "Harvesting Re-usable High-level Rules for Expository Dialogue Generation",
        "text": "This paper proposes a method for extracting high-level rules for expository dialogue generation. The rules are extracted from dialogues that have been authored by expert dialogue writers. We examine the rules that can be extracted by this method, focusing on whether different dialogues and authors exhibit different dialogue styles.",
        "id": 14269639
      },
      {
        "title": "",
        "text": "",
        "id": 233365227
      },
      {
        "title": "Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks",
        "text": "There are two approaches for pairwise sentence scoring: Cross-encoders, which perform full-attention over the input pair, and Bi-encoders, which map each input independently to a dense vector space. While crossencoders often achieve higher performance, they are too slow for many practical use cases. Bi-encoders, on the other hand, require substantial training data and fine-tuning over the target task to achieve competitive performance. We present a simple yet efficient data augmentation strategy called Augmented SBERT, where we use the cross-encoder to label a larger set of input pairs to augment the training data for the bi-encoder. We show that, in this process, selecting the sentence pairs is non-trivial and crucial for the success of the method. We evaluate our approach on multiple tasks (in-domain) as well as on a domain adaptation task. Augmented SBERT achieves an improvement of up to 6 points for in-domain and of up to 37 points for domain adaptation tasks compared to the original bi-encoder performance. 1",
        "id": 223957053
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a study that investigates representing entities in knowledge graphs with intricate geometric shapes, emphasizing probabilistic analysis and uncertainty modeling?",
    "positive_ctxs": [
      {
        "title": "Probabilistic Box Embeddings for Uncertain Knowledge Graph Reasoning",
        "text": "Knowledge bases often consist of facts which are harvested from a variety of sources, many of which are noisy and some of which conflict, resulting in a level of uncertainty for each triple. Knowledge bases are also often incomplete, prompting the use of embedding methods to generalize from known facts, however existing embedding methods only model triple-level uncertainty and reasoning results lack global consistency. To address these shortcomings, we propose BEUrRE , a novel uncertain knowledge graph embedding method with calibrated probabilistic semantics. BEUrRE models each entity as a box (i.e. axis-aligned hyperrectangle), and relations between two entities as affine transforms on the head and tail entity boxes. The geometry of the boxes allows for efficient calculation of intersections and volumes, endowing the model with calibrated probabilistic semantics and facilitating the incorporation of relational constraints. Extensive experiments on two benchmark datasets show that BEUrRE consistently outperforms baselines on confidence prediction and fact ranking due to it's probabilistic calibration and ability to capture high-order dependencies among facts.",
        "id": 233210621
      }
    ],
    "negative_ctxs": [
      {
        "title": "ON-TRAC Consortium End-to-End Speech Translation Systems for the IWSLT 2019 Shared Task",
        "text": "This paper describes the ON-TRAC Consortium translation systems developed for the end-to-end model task of IWSLT Evaluation 2019 for the English→ Portuguese language pair. ON-TRAC Consortium is composed of researchers from three French academic laboratories: LIA (Avignon Université), LIG (Université Grenoble Alpes), and LIUM (Le Mans Université). A single end-to-end model built as a neural encoder-decoder architecture with attention mechanism was used for two primary submissions corresponding to the two EN-PT evaluations sets: (1) TED (MuST-C) and (2) How2. In this paper, we notably investigate impact of pooling heterogeneous corpora for training, impact of target tokenization (characters or BPEs), impact of speech input segmentation and we also compare our best end-to-end model (BLEU of 26.91 on MuST-C and 43.82 on How2 validation sets) to a pipeline (ASR+MT) approach.",
        "id": 204961369
      },
      {
        "title": "",
        "text": "",
        "id": 232021799
      },
      {
        "title": "SmartNotes: Implicit Labeling of Meeting Data through User Note-Taking and Browsing",
        "text": "We have implemented SmartNotes, a system that automatically acquires labeled meeting data as users take notes during meetings and browse the notes afterwards. Such data can enable meeting understanding components such as topic and action item detectors to automatically improve their performance over a sequence of meetings. The SmartNotes system consists of a laptop based note taking application, and a web based note retrieval system. We shall demonstrate the functionalities of this system, and will also demonstrate the labeled data obtained during typical meetings and browsing sessions.",
        "id": 2969941
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "What paper first adapted ControlNet to generate continuous videos in a training-free manner?",
    "positive_ctxs": [
      {
        "title": "ControlVideo: Training-free Controllable Text-to-Video Generation",
        "text": "Text-driven diffusion models have unlocked unprecedented abilities in image generation, whereas their video counterpart still lags behind due to the excessive training cost of temporal modeling. Besides the training burden, the generated videos also suffer from appearance inconsistency and structural flickers, especially in long video synthesis. To address these challenges, we design a training-free framework called ControlVideo to enable natural and efficient text-to-video generation. ControlVideo, adapted from ControlNet, leverages coarsely structural consistency from input motion sequences, and introduces three modules to improve video generation. Firstly, to ensure appearance coherence between frames, ControlVideo adds fully cross-frame interaction in self-attention modules. Secondly, to mitigate the flicker effect, it introduces an interleaved-frame smoother that employs frame interpolation on alternated frames. Finally, to produce long videos efficiently, it utilizes a hierarchical sampler that separately synthesizes each short clip with holistic coherency. Empowered with these modules, ControlVideo outperforms the state-of-the-arts on extensive motion-prompt pairs quantitatively and qualitatively. Notably, thanks to the efficient designs, it generates both short and long videos within several minutes using one NVIDIA 2080Ti. Code is available at https://github.com/YBYBZhang/ControlVideo.Recent studies[15,40]have explored leveraging the structure controllability of ControlNet[43]or DDIM inversion [35] for video generation. Rather than synthesizing all frames independently,[15,40]enhance appearance coherence by replacing original self-attention with the sparser crossframe attention. Nevertheless, their video quality is still far behind photo-realistic videos in terms of: (i) inconsistent appearance between some frames (seeFig. 4 (a)), (ii) visible artifacts in large motion videos (seeFig. 4(b)), and (iii) structural flickers during inter-frame transitions. For (i) andPreprint. Under review.",
        "id": 258832670
      }
    ],
    "negative_ctxs": [
      {
        "title": "AISFG: Abundant Information Slot Filling Generator",
        "text": "As an essential component of task-oriented dialogue systems, slot filling requires enormous labeled training data in a certain domain. However, in most cases, there is little or no target domain training data is available in the training stage. Thus, cross-domain slot filling has to cope with the data scarcity problem by zero/few-shot learning. Previous researches on zero/few-shot cross-domain slot filling focus on slot descriptions and examples while ignoring the slot type ambiguity and example ambiguity issues. To address these problems, we propose Abundant Information Slot Filling Generator (AISFG), a generative model with a novel query template that incorporates domain descriptions, slot descriptions, and examples with context. Experimental results show that our model outperforms state-of-the-art approaches in zero/few-shot slot filling task. 1",
        "id": 250390921
      },
      {
        "title": "The Meaning Factory at SemEval-2017 Task 9: Producing AMRs with Neural Semantic Parsing",
        "text": "We evaluate a semantic parser based on a character-based sequence-to-sequence model in the context of the SemEval-2017 shared task on semantic parsing for AMRs.With data augmentation, super characters, and POS-tagging we gain major improvements in performance compared to a baseline character-level model. Although we improve on previous character-based neural semantic parsing models, the overall accuracy is still lower than a state-of-the-art AMR parser. An ensemble combining our neural semantic parser with an existing, traditional parser, yields a small gain in performance.",
        "id": 8672241
      },
      {
        "title": "Controlling Utterance Length in NMT-based Word Segmentation with Attention",
        "text": "One of the basic tasks of computational language documentation (CLD) is to identify word boundaries in an unsegmented phonemic stream. While several unsupervised monolingual word segmentation algorithms exist in the literature, they are challenged in real-world CLD settings by the small amount of available data. A possible remedy is to take advantage of glosses or translation in a foreign, wellresourced, language, which often exist for such data. In this paper, we explore and compare ways to exploit neural machine translation models to perform unsupervised boundary detection with bilingual information, notably introducing a new loss function for jointly learning alignment and segmentation. We experiment with an actual under-resourced language, Mboshi, and show that these techniques can effectively control the output segmentation length.4In this case, α ij = exp(e ij /T ) J k=1 exp(e ik /T ) . 5 A temperature below 1 would conversely sharpen the alignment distribution. We did not observe significant changes in segmentation performance varying the temperature parameter.",
        "id": 204788606
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Which paper presents a platform that emphasizes evaluating the robustness of models on benchmarks?",
    "positive_ctxs": [
      {
        "title": "Dynabench: Rethinking Benchmarking in NLP",
        "text": "We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-inthe-loop dataset creation: annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community: contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple challenge examples and falter in real-world scenarios. With Dynabench, dataset creation, model development, and model assessment can directly inform each other, leading to more robust and informative benchmarks. We report on four initial NLP tasks, illustrating these concepts and highlighting the promise of the platform, and address potential objections to dynamic benchmarking as a new standard for the field.",
        "id": 233444226
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 219307941
      },
      {
        "title": "A Domain-Specific Statistical Surface Realizer",
        "text": "We present a search-based approach to automatic surface realization given a corpus of domain sentences. Using heuristic search based on a statistical language model and a structure we introduce called an inheritance table we overgenerate a set of complete syntactic-semantic trees that are consistent with the given semantic structure and have high likelihood relative to the language model. These trees are then lexicalized, linearized, scored, and ranked. This model is being developed to generate real-time navigation instructions.",
        "id": 2182400
      },
      {
        "title": "Fine-Tuning in Brazilian Portuguese-English Statistical Transfer Ma- chine Translation: Verbal Tenses",
        "text": "This paper describes an experiment designed to evaluate the development of a Statistical Transfer-based Brazilian Portuguese to English Machine Translation system. We compare the performance of the system with the inclusion of new syntactic written rules concerning verbal tense between the Brazilian Portuguese and English languages. Results indicate that the system performance improved compared with an initial version of the system. However significant adjustments remain to be done.",
        "id": 692838
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Is there any paper that theoretically explains why in-context reinforcement learning works?",
    "positive_ctxs": [
      {
        "title": "Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining",
        "text": "Large transformer models pretrained on offline reinforcement learning datasets have demonstrated remarkable in-context reinforcement learning (ICRL) capabilities, where they can make good decisions when prompted with interaction trajectories from unseen environments. However, when and how transformers can be trained to perform ICRL have not been theoretically well-understood. In particular, it is unclear which reinforcement-learning algorithms transformers can perform in context, and how distribution mismatch in offline training data affects the learned algorithms. This paper provides a theoretical framework that analyzes supervised pretraining for ICRL. This includes two recently proposed training methods -algorithm distillation and decision-pretrained transformers. First, assuming model realizability, we prove the supervised-pretrained transformer will imitate the conditional expectation of the expert algorithm given the observed trajectory. The generalization error will scale with model capacity and a distribution divergence factor between the expert and offline algorithms. Second, we show transformers with ReLU attention can efficiently approximate near-optimal online reinforcement learning algorithms like LinUCB and Thompson sampling for stochastic linear bandits, and UCB-VI for tabular Markov decision processes. This provides the first quantitative analysis of the ICRL capabilities of transformers pretrained from offline trajectories. an online RL algorithm; and (3) when can supervised pretraining find such a good transformer. Specifically, this paper investigates the following open question:How can supervised pretraining on Transformers learn in-context reinforcement learning?In this paper, we initiate a theoretical study of the ICRL capability of transformers under supervised pretraining to address the open questions outlined above. We show that (1) Transformers can implement prevalent RL algorithms, including LinUCB and Thompson sampling for stochastic linear bandits, and UCB-VI for tabular Markov decision processes; (2) The algorithms learned by transformers achieve near-optimal regret bounds in their respective settings; (3) Supervised pretraining find such algorithms as long as the sample size scales with the covering number of transformer class and distribution ratio between expert and offline algorithms.Summary of contributions and paper outline• We propose a general framework for supervised pretraining approaches to meta-reinforcement learning (Section 2). This framework encompasses existing methods like Algorithm Distillation (Laskin et al.,  2022), where the expert and context algorithms are identical, as well as Decision-Pretrained Transformers (Lee et al., 2023), where the expert generates optimal actions for the MDP. It also includes approximate DPT variants where the expert estimates optimal actions from full interaction trajectories.• We prove that the supervised-pretrained transformer will imitate the conditional expectation of the expert algorithm given the observed trajectory (Section 3). The generalization error scales with both model capacity and a distribution ratio measuring divergence between the expert algorithm and the algorithm that generated offline trajectories.• We demonstrate that transformers can effectively approximate several near-optimal reinforcement learning algorithms by taking observed trajectories as context inputs (Section 4). Specifically, we show transformers can approximate LinUCB (Section 4.1) and Thompson sampling algorithms (Section 4.2) for stochastic linear bandit problems, and UCB-VI (Section 4.3) for tabular Markov decision processes. Combined with the generalization error bound from supervised pretraining and regret bounds of these RL algorithms, this provides regret bounds for supervised-pretrained transformers.• Preliminary experiments validate that transformers can perform ICRL in our setup (Section 5).• Technically, we prove efficient approximation of LinUCB by showing transformers can implement accelerated gradient descent for solving ridge regression (Appendix D.4), enabling fewer attention layers than the vanilla gradient descent approach inBai et al. (2023). To enable efficient Thompson sampling implementation, we prove transformers can compute matrix square roots through the Pade decomposition (Appendix E.3). These approximation results are interesting in their own right.",
        "id": 263909278
      }
    ],
    "negative_ctxs": [
      {
        "title": "Chinese Classifier Assignment Using SVMs",
        "text": "In Chinese, nouns need numeral classifiers to express quantity. In this paper, we explore the relationship between classifiers and nouns. We extract a set of lexical, syntactic and ontological features and the corresponding noun-classifier pairs from a corpus and then train SVMs to assign classifers to nouns. We analyse which features are most important for this task.",
        "id": 7393294
      },
      {
        "title": "Gei 3 ta 1 in Taiwan Mandarin---A Particular Construction *",
        "text": "The present paper investigates a particular structure in Taiwan Mandarin, \"(NP) + (intensifier) + gei 3 ta 1 \"give him/it\"+ adjective\" in terms of construction grammar. The structure is mostly observed in utterances of younger generation. Though it is not regarded as a grammatical or standard structure, it is still a register of language. The structure lays emphasis on speaker's attitude toward an undesired, unpleasant event. In most cases, the attitude tends to be negative. The events or propositions must have existed or been completed. The adjectives compatible with this structure belong to category of higher degree. The grammatical usage illustrates semantic bleaching of gei 3 ta 1 . And the changes from giving to a grammatical particle denoting subjective belief is a kind of subjectification. Moreover, ta 1 could refer to events or situation expressed by a more complicated grammatical structure, or denotes nothing as a dummy word. Though many previous studies paid attention to the newly developed structure resulted from language contact, the adequate account was not provided. It is hoped through this investigation, we will get a better understanding of this particular structure.Keywords: gei 3 ta 1 , construction meaning, speaker's subjective attitude/belief, negative, event * I would like to express my heartfelt thanks to the anonymous reviewers for comments and suggestions on the earlier abridged version of this paper. Needless to say, I am solely responsible for any infelicities.",
        "id": 12322869
      },
      {
        "title": "Language-Independent Sentiment Analysis Using Subjectivity and Positional Information",
        "text": "We describe a novel language-independent approach to the task of determining the polarity, positive or negative, of the author's opinion on a specific topic in natural language text. In particular, weights are assigned to attributes, individual words or word bi-grams, based on their position and on their likelihood of being subjective. The subjectivity of each attribute is estimated in a two-step process, where first the probability of being subjective is calculated for each sentence containing the attribute, and then these probabilities are used to alter the attribute's weights for polarity classification. The evaluation results on a standard dataset of movie reviews shows 89.85% classification accuracy, which rivals the best previously published results for this dataset for systems that use no additional linguistic information nor external resources.",
        "id": 12954317
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that examines how prompt tuning efficacy in language model training is affected by scaling?",
    "positive_ctxs": [
      {
        "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
        "text": "In this work, we explore \"prompt tuning,\" a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient \"prompt ensembling.\" We release code and model checkpoints to reproduce our experiments. 1ReferencesRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,  Danilo Giampiccolo, Bernardo Magnini, and Idan  Szpektor. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognising textual entailment, volume 6, pages 6-4. Venice. . 2019a. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
        "id": 233296808
      }
    ],
    "negative_ctxs": [
      {
        "title": "Online Learning of Interpretable Word Embeddings",
        "text": "Word embeddings encode semantic meanings of words into low-dimension word vectors. In most word embeddings, one cannot interpret the meanings of specific dimensions of those word vectors. Nonnegative matrix factorization (NMF) has been proposed to learn interpretable word embeddings via non-negative constraints. However, NMF methods suffer from scale and memory issue because they have to maintain a global matrix for learning. To alleviate this challenge, we propose online learning of interpretable word embeddings from streaming text data. Experiments show that our model consistently outperforms the state-of-the-art word embedding methods in both representation ability and interpretability. The source code of this paper can be obtained from http: //github.com/skTim/OIWE.",
        "id": 10705753
      },
      {
        "title": "Briefly Noted Computers and Writing: State of the Art Computers and Writing: Issues and Im- plementations Mike Sharples (editor)",
        "text": "",
        "id": 10137532
      },
      {
        "title": "Ò ÜÔ Ö Ñ ÒØ Ð ÓÑÔ Ö ×ÓÒ Ó Ø ÎÓØ È Ö ÔØÖÓÒ Ò ËÙÔÔÓÖØ Î ØÓÖ Å Ò × Ò Â Ô Ò × Ò ÐÝ× × Ì × × Å Ò Ù Ë ×× ÒÓ ÓÓ Â Ô Ò ÓÖÔÓÖ Ø ÓÒ ¹½¼¹½ ÊÓÔÔÓÒ ¸Å Ò ØÓ¹ Ù¸ÌÓ ÝÓ ½¼ ¹ ½ ¾ Â Ô Ò Ñ× ×× ÒÓ Ý ÓÓ¹ ÓÖÔº Ô ×ØÖ Ø Ï Ü Ñ Ò Ú Ö ÓÙ× ×Ô Ø× Ó Ø ÚÓØ Ô Ö ÔØÖÓÒ Ò ×ÙÔÔÓÖØ Ú ØÓÖ Ñ Ò × Ò Ð ×× ¢ Ø ÓÒ Ø × × Ò AEÄÈ Ö Ø Ö Ø Ò Ö Ò ¹ Ò Ø × ×º Ì × ×Ô Ø× Ò ÐÙ ØÖ Ò Ò Ø Ñ ¸ ÙÖ Ý Ò Ð ÖÒ Ò ÙÖÚ ×º Ï Ù× Â Ô Ò × Ô Ò Ò Ý Ô Ö× Ò × Ñ Ò Ø × ÓÖ ÜÔ Ö Ñ ÒØ×¸ Ò Â Ô Ò × ÛÓÖ × Ñ ÒØ Ø ÓÒ Ò ÙÒ× Ø×Ù´ × Ô Ö × Ò Â Ô Ò × µ ÙÒ Ò × ÙÜ Ð ÖÝ Ø × ×º ÁÒ ÓÙÖ ÜÔ Ö Ñ ÒØ× Û Ú Ó × ÖÚ Ø Ø Ø ÚÓØ Ô Ö ÔØÖÓÒ × ÓÑÔ Ö Ð ØÓ ËÎÅ Ò Ø ÖÑ× Ó ÙÖ Ý Ò ¸ Ò Ø ÓÒ¸ × ØÓ Ð ÖÒ Ò Ø Ñ Ò ÔÖ Ø ÓÒ ×Ô Ø ÚÓØ Ô Ö Ô¹ ØÖÓÒ × ÓÒ× Ö ÐÝ ØØ Ö Ø Ò ËÎÅº ½ ÁÒØÖÓ Ù Ø ÓÒ ËÙÔÔÓÖØ Ú ØÓÖ Ñ Ò ×´ËÎÅµ´Î ÔÒ ¸½ µ Ú Ò × ÓÛÒ ØÓ Ø Ú ÓÖ Ñ ÒÝ Ò ØÙÖ Ð Ð Ò Ù ÔÖÓ ×× Ò ´AEÄÈµ Ø × ×´ º º¸´ÃÙ Ó Ò Å Ø×ÙÑÓØÓ¸¾¼¼½ ÃÙ Ó Ò Å Ø×ÙÑÓØÓ¸¾¼¼¾µµº ÀÓÛ Ú Ö¸Ø Ö Ö ×Ø ÐÐ ×ÓÑ ÔÖ Ø Ð ¢ ÙÐØ × Û Ò Û ÔÔÐÝ ËÎÅØÓ AEÄÈØ × ×º Ì Û Ò ×× Ó ËÎÅ × Ø Ø Ø Ý Ö ÒÓØ ×Ý ØÓ ÑÔÐ Ñ ÒØ Ò Ø Ö Ð ÖÒ Ò ÔÖÓ ×× × ×ÐÓÛ¸ ×Ô ÐÐÝ Û Ø ÔÓÐÝÒÓÑ Ð ÖÒ Ð×ó Ö ÙÒ Ò Ë Ô Ö ¸½ µ ÔÖÓÔÓ× Ø ÚÓØ Ô Ö ÔØÖÓÒ¸Û × Ò ÑÔÖÓÚ Ú Ö× ÓÒ Ó È Ö¹ ÔØÖÓÒ´ÊÓ× Ò Ð ØØ¸½ µ¸ Ò Ø Ý Ú Ø ÓÖ Ø ¹ Ð Ò ÐÝ× × Ò Ú ÔÖÓÚ ÓÓ Ô Ö ÓÖÑ Ò ÓÖ Ø Ò ¹ÛÖ ØØ Ò Ø Ö Ó Ò Ø ÓÒº ÐØ ÓÙ ÓÐÐ Ò× Ò × ÓÐÐ Ù × Ù× Ø ÚÓØ Ô Ö ÔØÖÓÒ ÓÖ Ö Ò ¹ Ò Ò Ú Ö ÓÙ× AEÄÈ Ø × ×´ ÓÐÐ Ò×¸¾¼¼¾ ÓÐÐ Ò× Ò Ù Ý¸¾¼¼¾ ÓÐÐ Ò×¸¾¼¼¾ µ Ò Ó Ø Ò ÑÔÖ ×¹ × Ú Ö ×ÙÐØ×¸Ø Ù× × Ð ×× ¢ Ö × Ò ÒÓØ ×Ù ¢¹ ÒØÐÝ Ü Ñ Ò º ÁÒ Ô ÖØ ÙÐ Ö¸ Ø Û ÓÙÐ Ò ÒØ Ö¹ ×Ø Ò ÕÙ ×Ø ÓÒ Û Ø Ö ÓÖ ÒÓØ Ø ÚÓØ Ô Ö ÔØÖÓÒ × ÓÑÔ Ö Ð ØÓ ËÎÅ ÓÖ AEÄÈ Ø × × Ø Ø Ö ÓÖ¹ Ñ Ð Þ × Ð ×× ¢ Ø ÓÒ ÓÒ º ÁÒ Ø × Ô Ô Ö Û Ó Ù× ÓÒ ÓÑÔ Ö ×ÓÒ Ó ËÎÅ Ò Ø ÚÓØ Ô Ö ÔØÖÓÒ ØÓ ÒÚ ×Ø Ø Ø Ù× ÙÐÒ ×× Ó Ø ÚÓØ Ô Ö ÔØÖÓÒ Ò AEÄÈ Ø × ×º Ï Û ÓÙÐ Ð Ø Ó ÒÓÛ Ø ×ØÖ Ò Ø Ò Û Ò ×× Ó Ø ÚÓØ Ô Ö¹ ÔØÖÓÒº Ï ÓÓ× Ø Ö Ø × × ÓÖ Ø × ÔÙÖÔÓ× º Ì × Ø × × Ö Â Ô Ò × ÛÓÖ × Ñ ÒØ Ø ÓÒ¸ ÙÒ¹ × Ø×Ù´ × Ô Ö × Ò Â Ô Ò × µ ÙÒ Ò ¸ Ò ¹ Ô Ò Ò Ý Ô Ö× Ò º ÜÔ Ö Ñ ÒØ× Ò Ø Ø Ø ËÎÅ Ò Ø ÚÓØ Ô Ö¹ ÔØÖÓÒ Ö ÕÙ ÐÐÝ ÓÓ ÓÖ Ø Ø Ö Ø × × Ò Ø ÖÑ× Ó ÙÖ Ýº ÀÓÛ Ú Ö¸Ø ÚÓØ Ô Ö ÔØÖÓÒ × ×Ù¹ Ô Ö ÓÖ ØÓ ËÎÅ Ò Ø ÖÑ× Ó Ð ÖÒ Ò Ø Ñ ¸ÔÖ Ø ÓÒ Ø Ñ ¸ Ò Ñ ÑÓÖÝ ÓÓØÔÖ ÒØº ¾ Ì ÎÓØ È Ö ÔØÖÓÒ ÓÐÐÓÛ Ò ´ Ö ÙÒ Ò Ë Ô Ö ¸½ µ¸Û × ÓÛ Ø ØÖ Ò Ò Ò ÔÖ Ø ÓÒ Ð ÓÖ Ø Ñ Ó Ø ÚÓØ Ô Ö ÔØÖÓÒ Ò ÙÖ ½º Ì ÚÓØ Ô Ö ÔØÖÓÒ × Û ÐÐ × ËÎÅ Ò Ù× ÖÒ Ð ÙÒ Ø ÓÒº Ï × Ó Û Ò ÙÖ ¾ Ø Ð ÓÖ Ø Ñ Ó Ø ÚÓØ Ô Ö ÔØÖÓÒ Û Ø ÖÒ Ð ÙÒ Ø ÓÒº Ì × Ð ÓÖ Ø Ñ × Ñ× ØÓ Ö ¹ ÕÙ Ö Ç´ ¾ µ ÖÒ Ð Ð ÙÐ Ø ÓÒ×º ÀÓÛ Ú Ö¸Û Ò ÚÓ Ø Ñ Ý Ø Ò Ú ÒØ Ó Ø Ö ÙÖÖ Ò",
        "text": "",
        "id": 5267403
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a study that initializes embeddings in multilingual transformer for subwords common with original vocabulary with original embeddings?",
    "positive_ctxs": [
      {
        "title": "Lifting the Curse of Multilinguality by Pre-training Modular Transformers",
        "text": "Multilingual pre-trained models are known to suffer from the curse of multilinguality, which causes per-language performance to drop as they cover more languages. We address this issue by introducing language-specific modules, which allows us to grow the total capacity of the model, while keeping the total number of trainable parameters per language constant. In contrast with prior work that learns languagespecific components post-hoc, we pre-train the modules of our Cross-lingual Modular (X-MOD) models from the start. Our experiments on natural language inference, named entity recognition and question answering show that our approach not only mitigates the negative interference between languages, but also enables positive transfer, resulting in improved monolingual and cross-lingual performance. Furthermore, our approach enables adding languages post-hoc with no measurable drop in performance, no longer limiting the model usage to the set of pre-trained languages.",
        "id": 248721770
      }
    ],
    "negative_ctxs": [
      {
        "title": "A Gold Standard Corpus of Early Modern German",
        "text": "This paper describes an annotated gold standard sample corpus of Early Modern German containing over 50,000 tokens of text manually annotated with POS tags, lemmas, and normalised spelling variants. The corpus is the first resource of its kind for this variant of German, and represents an ideal test bed for evaluating and adapting existing NLP tools on historical data. We describe the corpus format, annotation levels, and challenges, providing an example of the requirements and needs of smaller humanities-based corpus projects.",
        "id": 12087109
      },
      {
        "title": "An Ensemble of Grapheme and Phoneme for Machine Transliteration",
        "text": "Machine transliteration is an automatic method to generate characters or words in one alphabetical system for the corresponding characters in another alphabetical system. There has been increasing concern on machine transliteration as an assistant of machine translation and information retrieval. Three machine transliteration models, including \"grapheme-based model\", \"phonemebased model\", and \"hybrid model\", have been proposed. However, there are few works trying to make use of correspondence between source grapheme and phoneme, although the correspondence plays an important role in machine transliteration. Furthermore there are few works, which dynamically handle source grapheme and phoneme. In this paper, we propose a new transliteration model based on an ensemble of grapheme and phoneme. Our model makes use of the correspondence and dynamically uses source grapheme and phoneme. Our method shows better performance than the previous works about 15~23% in English-to-Korean transliteration and about 15~43% in English-to-Japanese transliteration.",
        "id": 17161141
      },
      {
        "title": "UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS",
        "text": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks -demonstrating their applicability as general image representations.arXiv:1511.06434v2 [cs.LG] 7 Jan 2016Under review as a conference paper at ICLR 2016• We show that the generators have interesting vector arithmetic properties allowing for easy manipulation of many semantic qualities of generated samples.",
        "id": 11758569
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "What paper showed first that one can build a fully differentiable mixture of experts layer with no increase in time complexity?",
    "positive_ctxs": [
      {
        "title": "From Sparse to Soft Mixtures of Experts",
        "text": "Sparse mixture of expert architectures (MoEs) scale model capacity without large increases in training or inference costs. Despite their success, MoEs suffer from a number of issues: training instability, token dropping, inability to scale the number of experts, or ineffective finetuning. In this work, we propose Soft MoE, a fully-differentiable sparse Transformer that addresses these challenges, while maintaining the benefits of MoEs. Soft MoE performs an implicit soft assignment by passing different weighted combinations of all input tokens to each expert. As in other MoE works, experts in Soft MoE only process a subset of the (combined) tokens, enabling larger model capacity at lower inference cost. In the context of visual recognition, Soft MoE greatly outperforms standard Transformers (ViTs) and popular MoE variants (Tokens Choice and Experts Choice). For example, Soft MoE-Base/16 requires 10.5× lower inference cost (5.7× lower wall-clock time) than ViT-Huge/14 while matching its performance after similar training. Soft MoE also scales well: Soft MoE Huge/14 with 128 experts in 16 MoE layers has over 40× more parameters than ViT Huge/14, while inference time cost grows by only 2%, and it performs substantially better. * Equal contribution. The order was decided by a coin toss. 1 arXiv:2308.00951v1 [cs.LG] 2 Aug 2023 1 def soft_m oe_lay er (X , Phi , experts ) : 2 # Compute the dispatch and combine weights .3 logits = jnp . einsum ( 'md , dnp -> mnp ' , X , Phi ) 4 D = jax . nn . softmax ( logits , axis =(0 ,) ) 5 C = jax . nn . softmax ( logits , axis =(1 , 2) ) 6 # The input slots are a weighted average of all the input tokens , 7 # given by the dispatch weights .8 Xs = jnp . einsum ( 'md , mnp -> npd ' , X , D ) 9 # Apply the corresponding expert function to each input slot .10 Ys = jnp . stack ([ 11 f_i ( Xs [i , : , :]) for i , f_i in enumerate ( experts ) ] , 12 axis =0) 13 # The output tokens are a weighted average of all the output slots , 14 # given by the combine weights . 15 Y = jnp . einsum ( 'npd , mnp -> md ' , Ys , C ) 16 return Y Algorithm 1: Simple JAX (Bradbury et al., 2018) implementation of a Soft MoE layer. Full code is available at https://github.com/google-research/vmoe.",
        "id": 260378993
      }
    ],
    "negative_ctxs": [
      {
        "title": "HOW COULD RHETORICAL RELATIONS BE USED IN MACHINE TRANSLATION? (AND AT LEAST TWO OPEN QUESTIONS)",
        "text": "",
        "id": 12385862
      },
      {
        "title": "",
        "text": "Cet article traite de l'autoapprentissage d'un système i-vector/PLDA pour le regroupement en locuteurs de collections d'archives audiovisuelles françaises. Les paramètres d'extraction des i-vectors et du calcul des scores PLDA sont appris de façon non supervisée sur les données de la collection elle-même. Différents mélanges de données cibles et de données externes sont comparés pour la phase d'apprentissage. Les résultats expérimentaux sur deux corpora cibles distincts montrent que l'utilisation des données des corpora en question pour l'apprentissage itératif non supervisé et l'adaptation des paramètres de la PLDA peut améliorer un système existant, appris sur des données annotées externes. De tels résultats indiquent que la structuration automatique en locuteurs de petites collections non annotées ne devrait reposer que sur l'existence d'un corpus externe annoté, qui peut etre spécifiquement adaptéà chaque collection cible. Nous montronségalement qu'une collection suffisamment grande peut se passer de l'utilisation de ce corpus externe.ABSTRACTFirst investigations on self trained speaker diarizationThis paper investigates self trained cross-show speaker diarization applied to collections of French TV archives, based on an i-vector/PLDA framework. The parameters used for i-vectors extraction and PLDA scoring are trained in a unsupervised way, using the data of the collection itself. Performances are compared, using combinations of target data and external data for training. The experimental results on two distinct target corpora show that using data from the corpora themselves to perform unsupervised iterative training and domain adaptation of PLDA parameters can improve an existing system, trained on external annotated data. Such results indicate that performing speaker indexation on small collections of unlabeled audio archives should only rely on the availability of a sufficient external corpus, which can be specifically adapted to every target collection. We show that a minimum collection size is required to exclude the use of such an external bootstrap.Actes de la conférence conjointe JEP-TALN-RECITAL 2016, volume 1 : JEP",
        "id": 193799747
      },
      {
        "title": "Detecting co-derivative documents in large text collections",
        "text": "We have analyzed the SPEX algorithm byBernstein and Zobel (2004)for detecting co-derivative documents using duplicate n-grams. Although we totally agree with the claim that not using unique n-grams can greatly increase the efficiency and scalability of the process of detecting co-derivative documents, we have found serious bottlenecks in the way SPEX finds the duplicate n-grams. While the memory requirements for computing co-derivative documents can be reduced to up to 1 % by only using duplicate n-grams, SPEX needs about 40 times more memory for computing the list of duplicate n-grams itself. Therefore the memory requirements of the whole process are not reduced enough to make the algorithm practical for very large collections. We propose a solution for this problem using an external sort with the suffix array in-memory sorting and temporary file compression. The proposed algorithm for computing duplicate n-grams uses a fixed amount of memory for any input size.",
        "id": 2112310
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there any work that allows large numbers of model outputs to be encoded and compared by causal language models in a single forward pass?",
    "positive_ctxs": [
      {
        "title": "EEL: Efficiently Encoding Lattices for Reranking",
        "text": "Standard decoding approaches for conditional text generation tasks typically search for an output hypothesis with high model probability, but this may not yield the best hypothesis according to human judgments of quality. Reranking to optimize for downstream metrics can better optimize for quality, but many metrics of interest are computed with pre-trained language models, which are slow to apply to large numbers of hypotheses. We explore an approach for reranking hypotheses by using Transformers to efficiently encode lattices of generated outputs, a method we call EEL. With a single Transformer pass over the entire lattice, we can approximately compute a contextualized representation of each token as if it were only part of a single hypothesis in isolation. We combine this approach with a new class of token-factored rerankers (TFRs) that allow for efficient extraction of high reranker-scoring hypotheses from the lattice. Empirically, our approach incurs minimal degradation error compared to the exponentially slower approach of encoding each hypothesis individually. When applying EEL with TFRs across three text generation tasks, our results show both substantial speedup compared to naive reranking and often better performance on downstream metrics than comparable approaches.",
        "id": 258999673
      }
    ],
    "negative_ctxs": [
      {
        "title": "On the annotation of vague expressions: a case study on Romanian historical texts",
        "text": "Current approaches in Digital .Humanities tend to ignore a central aspect of any hermeneutic introspection: the intrinsic vagueness of analyzed texts. Especially when dealing with historical documents neglecting vagueness has important implications on the interpretation of the results. In this paper we present current limitation of annotation approaches and describe a current methodology for annotating vagueness for historical Romanian texts.",
        "id": 30604919
      },
      {
        "title": "Clustering of Multi-Word Named Entity variants: Multilingual Evaluation",
        "text": "Multi-word entities, such as organisation names, are frequently written in many different ways. We have previously automatically identified over one million acronym pairs in 22 languages, consisting of their short form (e.g. EC) and their corresponding long forms (e.g. European Commission, European Union Commission). In order to automatically group such long form variants as belonging to the same entity, we cluster them, using bottom-up hierarchical clustering and pair-wise string similarity metrics. In this paper, we address the issue of how to evaluate the named entity variant clusters automatically, with minimal human annotation effort. We present experiments that make use of Wikipedia redirection tables and we show that this method produces good results.",
        "id": 17957898
      },
      {
        "title": "Evaluation of automatic summaries: Metrics under varying data conditions",
        "text": "In evaluation of automatic summaries, it is necessary to employ multiple topics and human-produced models in order for the assessment to be stable and reliable. However, providing multiple topics and models is costly and time-consuming. This paper examines the relation between the number of available models and topics and the correlations with human judgment obtained by automatic metrics ROUGE and BE, as well as the manual Pyramid method. Testing all these methods on the same data set, taken from the TAC 2008 Summarization track, allows us to compare and contrast the methods under different conditions.",
        "id": 5312836
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "I'm exploring research that utilizes large datasets for the task of sentence simplification. Are there any prominent datasets sourced from Wikipedia that I could look into?",
    "positive_ctxs": [
      {
        "title": "Sentence Simplification with Deep Reinforcement Learning",
        "text": "Sentence simplification aims to make sentences easier to read and understand. Most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simple sentences. We address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework. Our model, which we call DRESS (as shorthand for Deep REinforcement Sentence Simplification), explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple, fluent, and preserve the meaning of the input. Experiments on three datasets demonstrate that our model outperforms competitive simplification systems. 1",
        "id": 7473831
      }
    ],
    "negative_ctxs": [
      {
        "title": "IMBALANCED SEMI-SUPERVISED LEARNING WITH BIAS ADAPTIVE CLASSIFIER",
        "text": "Pseudo-labeling has proven to be a promising semi-supervised learning (SSL) paradigm. Existing pseudo-labeling methods commonly assume that the class distributions of training data are balanced. However, such an assumption is far from realistic scenarios and thus severely limits the performance of current pseudolabeling methods under the context of class-imbalance. To alleviate this problem, we design a bias adaptive classifier that targets the imbalanced SSL setups. The core idea is to automatically assimilate the training bias caused by class imbalance via the bias adaptive classifier, which is composed of a novel bias attractor and the original linear classifier. The bias attractor is designed as a light-weight residual network and optimized through a bi-level learning framework. Such a learning strategy enables the bias adaptive classifier to fit imbalanced training data, while the linear classifier can provide unbiased label prediction for each class. We conduct extensive experiments under various imbalanced semi-supervised setups, and the results demonstrate that our method can be applied to different pseudo-labeling models and is superior to current state-of-the-art methods. Raffel. Mixmatch: A holistic approach to semi-supervised learning. NeurIPS, 32, 2019b. Paula Branco, Luís Torgo, and Rita P Ribeiro. A survey of predictive modeling on imbalanced domains. ACM Computing Surveys (CSUR), 49(2):1-50, 2016. Mateusz Buda, Atsuto Maki, and Maciej A Mazurowski. A systematic study of the class imbalance problem in convolutional neural networks. -supervised learning (chapelle, o. et al., eds.; 2006)[book reviews]. IEEE Transactions on Neural Networks, 20(3):542-542, 2009. . Smote: synthetic minority over-sampling technique. . Model-agnostic meta-learning for fast adaptation of deep networks.",
        "id": 257279756
      },
      {
        "title": "Cohesion, Entrainment and Task Success in Educational Dialog",
        "text": "Researchers often study dialog corpora to better understand what makes some dialogs more successful than others. In this talk I will examine the relationship between coherence/entrainment and task success, in several types of educational dialog corpora: 1) one-on-one tutoring, where students use dialog to interact with a human tutor in the physics domain, 2) one-on-one tutoring, where students instead interact with a spoken dialog system, and 3) engineering design, where student teams engage in multi-party dialog to complete a group project. I will first introduce several corpus-based measures of both lexical and acousticprosodic dialog cohesion and entrainment, and extend them to handle multi-party conversations. I will then show that the amount of cohesion and/or entrainment positively correlates with measures of educational task success in all of our corpora. Finally, I will discuss how we are using our findings to build better tutorial dialog systems.197",
        "id": 1538910
      },
      {
        "title": "Improving Generation and Evaluation of Visual Stories via Semantic Consistency",
        "text": "Story visualization is an underexplored task that falls at the intersection of many important research directions in both computer vision and natural language processing. In this task, given a series of natural language captions which compose a story, an agent must generate a sequence of images that correspond to the captions. Prior work has introduced recurrent generative models which outperform text-to-image synthesis models on this task. However, there is room for improvement of generated images in terms of visual quality, coherence and relevance. We present a number of improvements to prior modeling approaches, including (1) the addition of a dual learning framework that utilizes video captioning to reinforce the semantic alignment between the story and generated images, (2) a copy-transform mechanism for sequentiallyconsistent story visualization, and (3) MARTbased transformers to model complex interactions between frames. We present ablation studies to demonstrate the effect of each of these techniques on the generative power of the model for both individual images as well as the entire narrative. Furthermore, due to the complexity and generative nature of the task, standard evaluation metrics do not accurately reflect performance. Therefore, we also provide an exploration of evaluation metrics for the model, focused on aspects of the generated frames such as the presence/quality of generated characters, the relevance to captions, and the diversity of the generated images. We also present correlation experiments of our proposed automated metrics with human evaluations.",
        "id": 235097275
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Are there papers that propose contextualized calibration for the probability of answers in language models?",
    "positive_ctxs": [
      {
        "title": "Surface Form Competition: Why the Highest Probability Answer Isn't Always Right",
        "text": "Large language models have shown promising results in zero-shot settings (Brown et al.,  2020; Radford et al., 2019). For example, they can perform multiple choice tasks simply by conditioning on a question and selecting the answer with the highest probability. * Code is available at https://github.com/ peterwestuw/surface-form-competition",
        "id": 233296182
      }
    ],
    "negative_ctxs": [
      {
        "title": "DeepCx: A transition-based approach for shallow semantic parsing with complex constructional triggers",
        "text": "This paper introduces the SURFACE CON-STRUCTION LABELING (SCL) task, which expands the coverage of Shallow Semantic Parsing (SSP) to include frames triggered by complex constructions. We present DeepCx, a neural, transition-based system for SCL. As a test case for the approach, we apply DeepCx to the task of tagging causal language in English, which relies on a wider variety of constructions than are typically addressed in SSP. We report substantial improvements over previous tagging efforts on a causal language dataset. We also propose ways DeepCx could be extended to still more difficult constructions and to other semantic domains once appropriate datasets become available.",
        "id": 53080192
      },
      {
        "title": "A Corpus of Human-written Summaries of Line Graphs",
        "text": "We describe a corpus of human-written English language summaries of line graphs. This corpus is intended to help develop a system to automatically generate summaries capturing the most salient information conveyed by line graphs in popular media, as well as to evaluate the output of such a system.",
        "id": 15878323
      },
      {
        "title": "Normalising Audio Transcriptions for Unwritten Languages",
        "text": "The task of documenting the world's languages is a mainstream activity in linguistics which is yet to spill over into computational linguistics. We propose a new task of transcription normalisation as an algorithmic method for speeding up the process of transcribing audio sources, leading to text collections of usable quality. We report on the application of sentence and word alignment algorithms to this task, before describing a new algorithm. All of the algorithms are evaluated over synthetic datasets. Although the results are nuanced, the transcription normalisation task is suggested as an NLP contribution to the grand challenge of documenting the world's languages.",
        "id": 6605351
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper utilizes language models to generate singable lyrics that can go well with a predefined melody?",
    "positive_ctxs": [
      {
        "title": "Unsupervised Melody-to-Lyric Generation",
        "text": "Automatic melody-to-lyric generation is a task in which song lyrics are generated to go with a given melody. It is of significant practical interest and more challenging than unconstrained lyric generation as the music imposes additional constraints onto the lyrics. The training data is limited as most songs are copyrighted, resulting in models that underfit the complicated cross-modal relationship between melody and lyrics. In this work, we propose a method for generating high-quality lyrics without training on any aligned melody-lyric data. Specifically, we design a hierarchical lyric generation framework that first generates a song outline and second the complete lyrics. The framework enables disentanglement of training (based purely on text) from inference (melodyguided text generation) to circumvent the shortage of parallel data.We leverage the segmentation and rhythm alignment between melody and lyrics to compile the given melody into decoding constraints as guidance during inference. The two-step hierarchical design also enables content control via the lyric outline, a much-desired feature for democratizing collaborative song creation. Experimental results show that our model can generate high-quality lyrics that are more on-topic, singable, intelligible, and coherent than strong baselines, for example SongMASS (Sheng et al., 2021), a SOTA model trained on a parallel dataset, with a 24% relative overall quality improvement based on human ratings. 1",
        "id": 259370722
      }
    ],
    "negative_ctxs": [
      {
        "title": "Entailment graphs for text exploration",
        "text": "‡ Fondazione Bruno Kessler (FBK-irst), ItalyTaxonomy-based representations are widely used to model compactly large amounts of textual data. While current methods allow organizing knowledge at the lexical level (keywords/concepts/topics), there is an increasing demand to move towards more informative representations, which express properties of concepts and relations among them. This demand triggered our research on statement entailment graphs. In these graphs, nodes are natural language statements (propositions), comprising of predicates with their arguments and modifiers, while edges represent entailment relations between nodes. In this talk we report initial research that defines the properties of entailment graphs and their potential applications. Particularly, we show how entailment graphs can be profitably used for both knowledge acquisition and text exploration.Beyond providing a rich and informative representation, statement entailment graphs allow integrating multiple semantic inferences. So far, textual inference research focused on single, mutually independent, entailment judgments. However, in many scenarios there are dependencies among Text/Hypothesis pairs, which need to be captured consistently. This calls for global optimization algorithms for inter-dependent entailment judgments, taking advantage of the overall entailment graph structure (e.g. ensuring entailment graph transitivity).From the applied perspective, we are experimenting with entailment graphs in the context of the EX-CITEMENT project industrial scenarios. We focus on the text analytics domain, and particularly on the analysis of customer interactions across multiple channels, including speech, email, chat and social media, and multiple languages (English, German, Italian). For example, we would like to recognize that the complaint they charge too much for sandwiches entails food is too expensive, and allow an analyst to compactly navigate through an entailment graph that consolidates the information structure of a large number of customer statements. Our eventual applied goal is to develop a new generation of inference-based text exploration applications, which will enable businesses to better analyze their diverse and often unpredicted client content. This task will be exemplified with data collected from real customer interactions, while referring to the EXCITEMENT Open Platform that we developed as a generic open source framework for textual inferences.",
        "id": 42329141
      },
      {
        "title": "Analogical Dialogue Acts: Supporting Learning by Reading Analogies",
        "text": "Analogy is heavily used in written explanations, particularly in instructional texts. We introduce the concept of analogical dialogue acts (ADAs) which represent the roles utterances play in instructional analogies. We describe a catalog of such acts, based on ideas from structure-mapping theory. We focus on the operations that these acts lead to while understanding instructional texts, using the Structure-Mapping Engine (SME) and dynamic case construction in a computational model. We test this model on a small corpus of instructional analogies, expressed in simplified English, which were understood via a semiautomatic natural language system using analogical dialogue acts. The model enabled a system to answer questions after understanding the analogies that it was not able to answer without them.",
        "id": 3090899
      },
      {
        "title": "Enhancing The RATP-DECODA Corpus With Linguistic Annotations For Performing A Large Range Of NLP Tasks",
        "text": "In this article, we present the RATP-DECODA Corpus which is composed by a set of 67 hours of speech from telephone conversations of a Customer Care Service (CCS). This corpus is already available on line at http://sldr.org/sldr000847/fr in its first version. However, many enhancements have been made in order to allow the development of automatic techniques to transcript conversations and to capture their meaning. These enhancements fall into two categories: firstly, we have increased the size of the corpus with manual transcriptions from a new operational day; secondly we have added new linguistic annotations to the whole corpus (either manually or through an automatic processing) in order to perform various linguistic tasks from syntactic and semantic parsing to dialog act tagging and dialog summarization.",
        "id": 13300447
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a study that investigates enhancing token alignment in speech processing by employing inverse document frequency (idf)?",
    "positive_ctxs": [
      {
        "title": "SPLAT: Speech-Language Joint Pre-Training for Spoken Language Understanding",
        "text": "Spoken language understanding (SLU) requires a model to analyze input acoustic signal to understand its linguistic content and make predictions. To boost the models' performance, various pre-training methods have been proposed to learn rich representations from large-scale unannotated speech and text. However, the inherent disparities between the two modalities necessitate a mutual analysis. In this paper, we propose a novel semisupervised learning framework, SPLAT, to jointly pre-train the speech and language modules. Besides conducting a self-supervised masked language modeling task on the two individual modules using unpaired speech and text, SPLAT aligns representations from the two modules in a shared latent space using a small amount of paired speech and text. Thus, during fine-tuning, the speech module alone can produce representations carrying both acoustic information and contextual semantic knowledge of an input acoustic signal. Experimental results verify the effectiveness of our approach on various SLU tasks. For example, SPLAT improves the previous stateof-the-art performance on the Spoken SQuAD dataset by more than 10%.",
        "id": 235097520
      }
    ],
    "negative_ctxs": [
      {
        "title": "Are BLEU and Meaning Representation in Opposition?",
        "text": "One of possible ways of obtaining continuous-space sentence representations is by training neural machine translation (NMT) systems. The recent attention mechanism however removes the single point in the neural network from which the source sentence representation can be extracted. We propose several variations of the attentive NMT architecture bringing this meeting point back. Empirical evaluation suggests that the better the translation quality, the worse the learned sentence representations serve in a wide range of classification and similarity tasks. Rico Sennrich et al. 2017. Nematus: a toolkit for neural machine translation. In EACL. Xing Shi, Inkit Padhi, and Kevin Knight. 2016. Does string-based neural MT learn source syntax? In EMNLP. Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. In NIPS.",
        "id": 21717477
      },
      {
        "title": "Syntactic Annotations for the Google Books Ngram Corpus",
        "text": "We present a new edition of the Google Books Ngram Corpus, which describes how often words and phrases were used over a period of five centuries, in eight languages; it reflects 6% of all books ever published. This new edition introduces syntactic annotations: words are tagged with their part-of-speech, and headmodifier relationships are recorded. The annotations are produced automatically with statistical models that are specifically adapted to historical text. The corpus will facilitate the study of linguistic trends, especially those related to the evolution of syntax.",
        "id": 17707301
      },
      {
        "title": "Towards a richer wordnet representation of properties",
        "text": "This paper discusses how information on properties in a currently developed Danish thesaurus can be transferred to the Danish wordnet, DanNet, and in this way enrich the wordnet with the highly relevant links between properties and their external arguments (i.e. tasty -food). In spite of the fact that the thesaurus is still under development (two thirds still to be compiled) we perform an automatic transfer of relations from the thesaurus to the wordnet which shows promising results. In all, 2,362 property relations are automatically transferred to DanNet and 2% of the transferred material is manually validated. The pilot validation indicates that approx. 90 % of the transferred relations are correctly assigned whereas around 10% are either erroneous or just not very informative, a fact which, however, can partly be explained by the incompleteness of the material at its current stage. As a further consequence, the experiment has led to a richer specification of the editor guidelines to be used in the last compilation phase of the thesaurus.",
        "id": 14693730
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that examines a system for multimodal annotation intended to assist people in analyzing dialogues within video content?",
    "positive_ctxs": [
      {
        "title": "MONAH: Multi-Modal Narratives for Humans to analyze conversations",
        "text": "In conversational analyses, humans manually weave multimodal information into the transcripts, which is significantly time-consuming. We introduce a system that automatically expands the verbatim transcripts of videorecorded conversations using multimodal data streams. This system uses a set of preprocessing rules to weave multimodal annotations into the verbatim transcripts and promote interpretability. Our feature engineering contributions are two-fold: firstly, we identify the range of multimodal features relevant to detect rapport-building; secondly, we expand the range of multimodal annotations and show that the expansion leads to statistically significant improvements in detecting rapport-building.",
        "id": 231639161
      }
    ],
    "negative_ctxs": [
      {
        "title": "Preparing SNACS for Subjects and Objects",
        "text": "Research on adpositions and possessives in multiple languages has led to a small inventory of general-purpose meaning classes that disambiguate tokens. Importantly, that work has argued for a principled separation of the semantic role in a scene from the function coded by morphosyntax. Here, we ask whether this approach can be generalized beyond adpositions and possessives to cover all scene participants-including subjects and objectsdirectly, without reference to a frame lexicon. We present new guidelines for English and the results of an interannotator agreement study.",
        "id": 201708265
      },
      {
        "title": "Cross-lingual NIL Entity Clustering for Low-resource Languages",
        "text": "Clustering unlinkable entity mentions across documents in multiple languages (crosslingual NIL Clustering) is an important task as part of Entity Discovery and Linking (EDL). This task has been largely neglected by the EDL community because it is challenging to outperform simple edit distance or other heuristics based baselines. We propose a novel approach based on encoding the orthographic similarity of the mentions using a Recurrent Neural Network (RNN) architecture. Our model adapts a training procedure from the one-shot facial recognition literature in order to achieve this. We also perform several exploratory probing tasks on our name encodings in order to determine what specific types of information are likely to be encoded by our model. Experiments show our approach provides up to a 6.6% absolute CEAFm F-Score improvement over state-of-the-art methods and successfully captures phonological relations across languages.",
        "id": 199556521
      },
      {
        "title": "Learning Probabilistic Subcategorization Preference by Identifying Case Dependencies and Optimal Noun Class Generalization Level*",
        "text": "This paper proposes a novel method of learning probabilistic subcategorization preference. In the method, for the purpose of coping with the ambiguities of case dependencies and noun class generalization of argument/adjunct nouns, we introduce a data structure which represents a tuple of independent partial subcategorization frames. Each collocation of a verb and argument/adjunct nouns is assumed to be generated from one of the possible tuples of independent partial subcategorization frames. Parameters of subcategorization preference are then estimated so as to maximize the subcategorization preference function for each collocation of a verb and argument/adjunct nouns in the training corpus. We also describe the results of the experiments on learning probabilistic subcategorization preference from the EDR Japanese bracketed corpus, as well as those on evaluating the performance of subcategorization preference.",
        "id": 14308271
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research papers that explore applying knowledge distillation to information retrieval, specifically those that concentrate on methods using in-batch negatives",
    "positive_ctxs": [
      {
        "title": "In-Batch Negatives for Knowledge Distillation with Tightly-Coupled Teachers for Dense Retrieval",
        "text": "We present an efficient training approach to text retrieval with dense representations that applies knowledge distillation using the Col-BERT late-interaction ranking model. Specifically, we propose to transfer the knowledge from a bi-encoder teacher to a student by distilling knowledge from ColBERT's expressive MaxSim operator into a simple dot product. The advantage of the bi-encoder teacherstudent setup is that we can efficiently add inbatch negatives during knowledge distillation, enabling richer interactions between teacher and student models. In addition, using Col-BERT as the teacher reduces training cost compared to a full cross-encoder. Experiments on the MS MARCO passage and document ranking tasks and data from the TREC 2019 Deep Learning Track demonstrate that our approach helps models learn robust representations for dense retrieval effectively and efficiently.",
        "id": 235720578
      }
    ],
    "negative_ctxs": [
      {
        "title": "Fast Interleaved Bidirectional Sequence Generation",
        "text": "Independence assumptions during sequence generation can speed up inference, but parallel generation of highly inter-dependent tokens comes at a cost in quality. Instead of assuming independence between neighbouring tokens (semi-autoregressive decoding, SA), we take inspiration from bidirectional sequence generation and introduce a decoder that generates target words from the left-to-right and right-toleft directions simultaneously. We show that we can easily convert a standard architecture for unidirectional decoding into a bidirectional decoder by simply interleaving the two directions and adapting the word positions and selfattention masks. Our interleaved bidirectional decoder (IBDecoder) retains the model simplicity and training efficiency of the standard Transformer, and on five machine translation tasks and two document summarization tasks, achieves a decoding speedup of ∼2× compared to autoregressive decoding with comparable quality. Notably, it outperforms left-toright SA because the independence assumptions in IBDecoder are more felicitous. To achieve even higher speedups, we explore hybrid models where we either simultaneously predict multiple neighbouring tokens per direction, or perform multi-directional decoding by partitioning the target sequence. These methods achieve speedups to 4×-11× across different tasks at the cost of <1 BLEU or <0.5 ROUGE (on average). 1",
        "id": 225075957
      },
      {
        "title": "Machine Translation Decoding beyond Beam Search",
        "text": "Beam search is the go-to method for decoding auto-regressive machine translation models. While it yields consistent improvements in terms of BLEU, it is only concerned with finding outputs with high model likelihood, and is thus agnostic to whatever end metric or score practitioners care about. Our aim is to establish whether beam search can be replaced by a more powerful metricdriven search technique. To this end, we explore numerous decoding algorithms, including some which rely on a value function parameterised by a neural network, and report results on a variety of metrics. Notably, we introduce a Monte-Carlo Tree Search (MCTS) based method and showcase its competitiveness. We provide a blueprint for how to use MCTS fruitfully in language applications, which opens promising future directions. We find that which algorithm is best heavily depends on the characteristics of the goal metric; we believe that our extensive experiments and analysis will inform further research in this area.",
        "id": 233210339
      },
      {
        "title": "Published as a conference paper at ICLR 2022 ON THE CONNECTION BETWEEN LOCAL ATTENTION AND DYNAMIC DEPTH-WISE CONVOLUTION",
        "text": "Vision Transformer (ViT) attains state-of-the-art performance in visual recognition, and the variant, Local Vision Transformer, makes further improvements. The major component in Local Vision Transformer, local attention, performs the attention separately over small local windows. We rephrase local attention as a channel-wise locally-connected layer and analyze it from two network regularization manners, sparse connectivity and weight sharing, as well as dynamic weight computation. We point out that local attention resembles depth-wise convolution and its dynamic variants in sparse connectivity: there is no connection across channels, and each position is connected to the positions within a small local window. The main differences lie in (i) weight sharing -depth-wise convolution shares connection weights (kernel weights) across spatial positions and attention shares the connection weights across channels, and (ii) dynamic weight computation manners -local attention is based on dot-products between pairwise positions in the local window, and dynamic convolution is based on linear projections conducted on the center representation or the globally pooled representation. The connection between local attention and dynamic depth-wise convolution is empirically verified by the ablation study about weight sharing and dynamic weight computation in Local Vision Transformer and (dynamic) depth-wise convolution based network, namely (dynamic) DWNet. We empirically observe that the depth-wise convolution based DWNet and its dynamic variants with lower computation complexity perform on-par with or slightly better than Swin Transformer, an instance of Local Vision Transformer, for ImageNet classification, COCO object detection and ADE semantic segmentation.Published as a conference paper at ICLR 2022 sharing: the connection weights are shared across channels or within each group of channels. (iii) Dynamic weight: the connection weights are dynamically predicted according to each image instance.We connect local attention with depth-wise convolution(Chollet, 2017;Howard et al., 2017)and its dynamic variants that are also a channel-wise spatially-locally connected layer with optional dynamic connection weights. They are similar in sparse connectivity. The main differences lie in (i) weight sharing -depth-wise convolution shares connection weights (kernel weights) across spatial positions and attention shares the connection weights across channels, and (ii) dynamic weight computation manners -local attention is based on dot-products between pairwise positions in the local window, and dynamic convolution is based on linear projections conducted on the center representation or the globally pooled representation.We further present the empirical verification for the connection. We take the recently-developed Local Vision Transformer, Swin Transformer (Liu et al., 2021b), as an example, and study the empirical performance of local attention and (dynamic) depth-wise convolution in the same training settings as Swin Transformer. We replace the local attention layer with the (dynamic) depth-wise convolution layer, keeping the overall structure unchanged. The constructed model is named DWNet.The results show that the (dynamic) depth-wise convolution-based DWNet achieves comparable or slightly higher performance for ImageNet classification and two downstream tasks, COCO object detection and ADE semantic segmentation, and (dynamic) DWNet takes lower computation complexity. The ablation studies imply that weight sharing and dynamic weight improves the model capability. Specifically, (i) for Swin Transformer, weight sharing across channels is beneficial mainly for reducing the parameter (attention weight) complexity, and the attention-based dynamic weight scheme is advantageous in learning instance-specific weights and block-translation equivalent representations; (ii) for depth-wise convolution, weight sharing across positions is beneficial for reducing the parameter complexity as well as learning translation equivalent representations, and the linear projection-based dynamic weight scheme learns instance-specific weights.",
        "id": 247450510
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest a dataset for question-answering frameworks utilizing temporal knowledge graphs with broad coverage?",
    "positive_ctxs": [
      {
        "title": "Question Answering Over Temporal Knowledge Graphs",
        "text": "Temporal Knowledge Graphs (Temporal KGs) extend regular Knowledge Graphs by providing temporal scopes (e.g., start and end times) on each edge in the KG. While Question Answering over KG (KGQA) has received some attention from the research community, QA over Temporal KGs (Temporal KGQA) is a relatively unexplored area. Lack of broadcoverage datasets has been another factor limiting progress in this area. We address this challenge by presenting CRONQUESTIONS, the largest known Temporal KGQA dataset, clearly stratified into buckets of structural complexity. CRONQUESTIONS expands the only known previous dataset by a factor of 340×. We find that various state-of-the-art KGQA methods fall far short of the desired performance on this new dataset. In response, we also propose CRONKGQA, a transformerbased solution that exploits recent advances in Temporal KG embeddings, and achieves performance superior to all baselines, with an increase of 120% in accuracy over the next best performing method. Through extensive experiments, we give detailed insights into the workings of CRONKGQA, as well as situations where significant further improvements appear possible. In addition to the dataset, we have released our code as well.",
        "id": 235313508
      }
    ],
    "negative_ctxs": [
      {
        "title": "Using Syntactic and Semantic Context to Explore Psychodemographic Differences in Self-reference",
        "text": "Psychological analysis of language has repeatedly shown that an individual's rate of mentioning 1st person singular pronouns predicts a wealth of important demographic and psychological factors. However, these analyses are performed out of context -syntactic and semantic -which may change the magnitude or even direction of such relationships. In this paper, we put \"pronouns in their context\", exploring the relationship between self-reference and age, gender, and depression depending on syntactic position and verbal governor. We find that pronouns are overall more predictive when taking dependency relations and verb semantic categories into account, and, the direction of the relationship can change depending on the semantic class of the verbal governor.",
        "id": 15154458
      },
      {
        "title": "PEPDS: A Polite and Empathetic Persuasive Dialogue System for Charity Donation",
        "text": "Persuasive conversations for a social cause often require influencing other person's attitude or intention that may fail even with compelling arguments. The use of emotions and different types of polite tones as needed with facts may enhance the persuasiveness of a message. To incorporate these two aspects, we propose a polite, empathetic persuasive dialogue system (PEPDS). First, in a Reinforcement Learning (RL) setting, a Maximum Likelihood Estimation loss based model is finetuned by designing an efficient reward function consisting of five different sub rewards viz. Persuasion, Emotion, Politeness-Strategy Consistency, Dialogue-Coherence and Nonrepetitiveness. Then, to generate empathetic utterances for non-empathetic ones, an Empathetic transfer model is built upon the RL finetuned model. Due to the unavailability of an appropriate dataset, by utilizing the PERSUASION-FORGOOD dataset, we create two datasets, viz. EPP4G and ETP4G. EPP4G is used to train three transformer-based classification models as per persuasiveness, emotion and politenessstrategy to achieve respective reward feedbacks. The ETP4G dataset is used to train an empathetic transfer model. Our experimental results demonstrate that PEPDS increases the rate of persuasive responses with emotion and politeness acknowledgement compared to the current state-of-the-art dialogue models, while also enhancing the dialogue's engagement and maintaining the linguistic quality 1 .",
        "id": 252818930
      },
      {
        "title": "Fine-Grained Temporal Relation Extraction",
        "text": "We present a novel semantic framework for modeling temporal relations and event durations that maps pairs of events to realvalued scales for the purpose of constructing document-level event timelines. We use this framework to construct the largest temporal relations dataset to date, covering the entirety of the Universal Dependencies English Web Treebank. We use this dataset to train models for jointly predicting fine-grained temporal relations and event durations. We report strong results on our data and show the efficacy of a transfer-learning approach for predicting standard, categorical TimeML relations.",
        "id": 59599681
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a study that does data-augmentation for biomedical named-entity recognition by replacing entities in the original sentence with entities of the same type from semantically similar sentences?",
    "positive_ctxs": [
      {
        "title": "Simple Semantic-based Data Augmentation for Named Entity Recognition in Biomedical Texts",
        "text": "Data augmentation is important in addressing data sparsity and low resources in NLP. Unlike data augmentation for other tasks such as sentence-level and sentence-pair ones, data augmentation for named entity recognition (NER) requires preserving the semantic of entities. To that end, in this paper we propose a simple semantic-based data augmentation method for biomedical NER. Our method leverages semantic information from pre-trained language models for both entity-level and sentence-level. Experimental results on two datasets: i2b2-2010 (English) and VietBioNER (Vietnamese) showed that the proposed method could improve NER performance.",
        "id": 248780437
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 237099287
      },
      {
        "title": "Constructing a Culinary Interview Dialogue Corpus with Video Conferencing Tool",
        "text": "Interview is an efficient way to elicit knowledge from experts of different domains. In this paper, we introduce CIDC, an interview dialogue corpus in the culinary domain in which interviewers play an active role to elicit culinary knowledge from the cooking expert. The corpus consists of 308 interview dialogues (each about 13 minutes in length), which add up to a total of 64,000 utterances. We use a video conferencing tool for data collection, which allows us to obtain the facial expressions of the interlocutors as well as the screen-sharing contents. To understand the impact of the interlocutors' skill level, we divide the experts into \"professionals\" and \"enthusiasts\" and the interviewers into \"skilled interviewers\" and \"unskilled interviewers.\" For quantitative analysis, we report the statistics and the results of the post-interview questionnaire. We also conduct qualitative analysis on the collected interview dialogues and summarize the salient patterns of how interviewers elicit knowledge from the experts. The corpus serves the purpose to facilitate future research on the knowledge elicitation mechanism in interview dialogues.",
        "id": 251436397
      },
      {
        "title": "Too Many Questions? What Can We Do? : Multiple Question Span Detection",
        "text": "When a human interacts with an information retrieval chat bot, he/she can ask multiple questions at the same time. Current question answering systems can't handle this scenario effectively. In this paper we propose an approach to identify question spans in a given utterance, by posing this as a sequence labeling problem. The model is trained and evaluated over 4 different freely available datasets. To get a comprehensive coverage of the compound question scenarios, we also synthesize a dataset based on the natural question combination patterns. We exhibit improvement in the performance of the DrQA system when it encounters compound questions which suggests that this approach is vital for real-time human-chatbot interaction.",
        "id": 202594324
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Where can I find a paper that discusses annotating events in text with degrees of factuality, including categories such as certain, probable, and underspecified?",
    "positive_ctxs": [
      {
        "title": "Are You Sure That This Happened? Assessing the Factuality Degree of Events in Text",
        "text": "Identifying the veracity, or factuality, of event mentions in text is fundamental for reasoning about eventualities in discourse. Inferences derived from events judged as not having happened, or as being only possible, are different from those derived from events evaluated as factual. Event factuality involves two separate levels of information. On the one hand, it deals with polarity, which distinguishes between positive and negative instantiations of events. On the other, it has to do with degrees of certainty (e.g., possible, probable), an information level generally subsumed under the category of epistemic modality. This article aims at contributing to a better understanding of how event factuality is articulated in natural language. For that purpose, we put forward a linguistic-oriented computational model which has at its core an algorithm articulating the effect of factuality relations across levels of syntactic embedding. As a proof of concept, this model has been implemented in De Facto, a factuality profiler for eventualities mentioned in text, and tested against a corpus built specifically for the task, yielding an F 1 of 0.70 (macro-averaging) and 0.80 (micro-averaging). These two measures mutually compensate for an over-emphasis present in the other (either on the lesser or greater populated categories), and can therefore be interpreted as the lower and upper bounds of the De Facto's performance.",
        "id": 2239324
      }
    ],
    "negative_ctxs": [
      {
        "title": "Consistent CCG Parsing over Multiple Sentences for Improved Logical Reasoning",
        "text": "In formal logic-based approaches to Recognizing Textual Entailment (RTE), a Combinatory Categorial Grammar (CCG) parser is used to parse input premises and hypotheses to obtain their logical formulas. Here, it is important that the parser processes the sentences consistently; failing to recognize a similar syntactic structure results in inconsistent predicate argument structures among them, in which case the succeeding theorem proving is doomed to failure. In this work, we present a simple method to extend an existing CCG parser to parse a set of sentences consistently, which is achieved with an inter-sentence modeling with Markov Random Fields (MRF). When combined with existing logic-based systems, our method always shows improvement in the RTE experiments on English and Japanese languages.",
        "id": 4937809
      },
      {
        "title": "Workshop track -ICLR 2018 EFFICIENT RECURRENT NEURAL NETWORKS USING STRUCTURED MATRICES IN FPGAS",
        "text": "Recurrent Neural Networks (RNNs) are becoming increasingly important for time series-related applications which require efficient and real-time implementations. The recent pruning based work ESE (Han et al., 2017) suffers from degradation of performance/energy efficiency due to the irregular network structure after pruning. We propose block-circulant matrices for weight matrix representation in RNNs, thereby achieving simultaneous model compression and acceleration. We aim to implement RNNs in FPGA with highest performance and energy efficiency, with certain accuracy requirement (negligible accuracy degradation). Experimental results on actual FPGA deployments shows that the proposed framework achieves a maximum energy efficiency improvement of 35.7× compared with ESE.",
        "id": 4347685
      },
      {
        "title": "Results of the WMT19 Metrics Shared Task: Segment-Level and Strong MT Systems Pose Big Challenges",
        "text": "This paper presents the results of the WMT19 Metrics Shared Task.Participants were asked to score the outputs of the translations systems competing in the WMT19 News Translation Task with automatic metrics. 13 research groups submitted 24 metrics, 10 of which are reference-less \"metrics\" and constitute submissions to the joint task with WMT19 Quality Estimation Task, \"QE as a Metric\". In addition, we computed 11 baseline metrics, with 8 commonly applied baselines (BLEU, SentBLEU, NIST, WER, PER, TER, CDER, and chrF) and 3 reimplementations (chrF+, sacreBLEU-BLEU, and sacreBLEU-chrF). Metrics were evaluated on the system level, how well a given metric correlates with the WMT19 official manual ranking, and segment level, how well the metric correlates with human judgements of segment quality. This year, we use direct assessment (DA) as our only form of manual evaluation.",
        "id": 201742578
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper investigates the influence of the diversity of source tasks on the performance of target tasks in prompt tuning using CrossFit?",
    "positive_ctxs": [
      {
        "title": "Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?",
        "text": "Prompt tuning (PT) which only tunes the embeddings of an additional sequence of tokens per task, keeping the pre-trained language model (PLM) frozen, has shown remarkable performance in few-shot learning. Despite this, PT has been shown to rely heavily on good initialization of the prompt embeddings. In this work, we study meta prompt tuning (MPT) to systematically explore how meta-learning can help improve (if it can) cross-task generalization in PT through learning to initialize the prompt embeddings from other relevant tasks. We empirically analyze a representative set of meta learning algorithms in a wide range of adaptation settings with different source/target task configurations on a large set of few-shot tasks. With extensive experiments and analysis, we demonstrate the effectiveness of MPT. We find the improvement to be significant particularly on classification tasks. For other kinds of tasks such as question answering, we observe that while MPT can outperform PT in most cases, it does not always outperform multi-task learning. We further provide an in-depth analysis from the perspective of task similarity. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
        "id": 256900985
      }
    ],
    "negative_ctxs": [
      {
        "title": "Mind the data gap(s): Investigating power in speech and language datasets",
        "text": "Algorithmic oppression is an urgent and persistent problem in speech and language technologies. Considering power relations embedded in datasets before compiling or using them to train or test speech and language technologies is essential to designing less harmful, more just technologies. This paper presents a reflective exercise to recognise and challenge gaps and the power relations they reveal in speech and language datasets by applying principles of Data Feminism and Design Justice, and building on work on dataset documentation and sociolinguistics.",
        "id": 248780152
      },
      {
        "title": "A Speech and Gesture Spatial Corpus in Assisted Living",
        "text": "Ambient Assisted Living (AAL) is the name for a European technology and innovation funding programme. AAL research field is about intelligent assistant systems for a healthier and safer life in the preferred living environments through the use of Information and Communication Technologies (ICT). We focus specifically on speech and gesture interaction which can enhance the quality of lifestyle of people living in assistive environments, be they seniors or people with physical or cognitive disabilities. In this paper we describe our user study conducted in a lab at the University of Bremen in order to collect empirical speech and gesture data and later create and analyse a multimodal corpus. The user study is about a human user sitting in a wheelchair and performing certain inherently spatial tasks.",
        "id": 7778807
      },
      {
        "title": "Hubness and Pollution: Delving into Cross-Space Mapping for Zero-Shot Learning",
        "text": "Zero-shot methods in language, vision and other domains rely on a cross-space mapping function that projects vectors from the relevant feature space (e.g., visualfeature-based image representations) to a large semantic word space (induced in an unsupervised way from corpus data), where the entities of interest (e.g., objects images depict) are labeled with the words associated to the nearest neighbours of the mapped vectors. Zero-shot cross-space mapping methods hold great promise as a way to scale up annotation tasks well beyond the labels in the training data (e.g., recognizing objects that were never seen in training). However, the current performance of cross-space mapping functions is still quite low, so that the strategy is not yet usable in practical applications. In this paper, we explore some general properties, both theoretical and empirical, of the cross-space mapping function, and we build on them to propose better methods to estimate it. In this way, we attain large improvements over the state of the art, both in cross-linguistic (word translation) and cross-modal (image labeling) zero-shot experiments.",
        "id": 12187767
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that investigates efficient finetuning methods that only trains very very few parameters in language models?",
    "positive_ctxs": [
      {
        "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
        "text": "We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods. Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.",
        "id": 231672601
      }
    ],
    "negative_ctxs": [
      {
        "title": "應用混淆音矩陣之中英文音譯詞組自動抽取 郭金喜",
        "text": "",
        "id": 5757808
      },
      {
        "title": "MWEs as Non-propositional Content Indicators",
        "text": "We report that a proper employment of MWEs concerned enables us to put forth a tractable framework, which is based on a multiple nesting of semantic operations, for the processing of non-inferential, Nonpropositional Contents (NPCs) of natural Japanese sentences. Our framework is characterized by its broad syntactic and semantic coverage, enabling us to deal with multiply composite modalities and their semantic/pragmatic similarity. Also, the relationship between indirect(Searle, 1975)and direct speech, and equations peculiar to modal logic and its family(Mally, 1926;Prior, 1967)are treated in the similarity paradigm.",
        "id": 17759492
      },
      {
        "title": "Dynamic Online Conversation Recommendation",
        "text": "Trending topics in social media content evolve over time, and it is therefore crucial to understand social media users and their interpersonal communications in a dynamic manner. In this research we study dynamic online conversation recommendation, to help users engage in conversations that satisfy their evolving interests. Different from works in conversation recommendation which assume static user interests, our model captures the temporal aspects of user interests. Moreover, our model can cater for cold start problem where conversations are new and unseen in training. We propose a neural architecture to analyze changes of user interactions and interests over time, whose result is used to predict which discussions the users are likely to enter. We conduct experiments on large-scale collections of Reddit conversations. Results on three subreddits show that our model significantly outperforms state-of-the-art models based on static assumption of user interests. We further evaluate performance in cold start, and observe consistently better performance by our model when considering various degrees of sparsity of user's chatting history and conversation contexts. Lastly, our analysis also confirms the change of user interests. This further justify the advantage and efficacy of our model.",
        "id": 219720103
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Has there been any work that improves the work on integrated gradients by leveraging interpolation strategies to improve gradient accuracies?",
    "positive_ctxs": [
      {
        "title": "Discretized Integrated Gradients for Explaining Language Models",
        "text": "As a prominent attribution-based explanation algorithm, Integrated Gradients (IG) is widely adopted due to its desirable explanation axioms and the ease of gradient computation. It measures feature importance by averaging the model's output gradient interpolated along a straight-line path in the input data space. However, such straight-line interpolated points are not representative of text data due to the inherent discreteness of the word embedding space. This questions the faithfulness of the gradients computed at the interpolated points and consequently, the quality of the generated explanations. Here we propose Discretized Integrated Gradients (DIG), which allows effective attribution along non-linear interpolation paths. We develop two interpolation strategies for the discrete word embedding space that generates interpolation points that lie close to actual words in the embedding space, yielding more faithful gradient computation. We demonstrate the effectiveness of DIG over IG through experimental and human evaluations on multiple sentiment classification datasets. We provide the source code of DIG to encourage reproducible research 1 . . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
        "id": 237363853
      }
    ],
    "negative_ctxs": [
      {
        "title": "Rethinking representations: A log-bilinear model of phonotactics",
        "text": "Models of phonotactics include subsegmental representations in order to generalize to unattested sequences.These representations can be encoded in at least two ways: as discrete, phonetically-based features, or as continuous, distribution-based representations induced from the statistical patterning of sounds.Because phonological theory typically assumes that representations are discrete, past work has reduced continuous representations to discrete ones, which eliminates potentially relevant information.In this paper we present a model of phonotactics that can use continuous representations directly, and show that this approach yields competitive performance on modeling experimental judgments of English sonority sequencing.The proposed model broadens the space of possible phonotactic models by removing requirements for discrete features, and is a step towards an integrated picture of phonotactic learning based on distributional statistics and continuous representations.",
        "id": 259325958
      },
      {
        "title": "SEMI-PARAMETRIC INDUCING POINT NETWORKS AND NEURAL PROCESSES",
        "text": "We introduce semi-parametric inducing point networks (SPIN), a general-purpose architecture that can query the training set at inference time in a compute-efficient manner. Semi-parametric architectures are typically more compact than parametric models, but their computational complexity is often quadratic. In contrast, SPIN attains linear complexity via a cross-attention mechanism between datapoints inspired by inducing point methods. Querying large training sets can be particularly useful in meta-learning, as it unlocks additional training signal, but often exceeds the scaling limits of existing models. We use SPIN as the basis of the Inducing Point Neural Process, a probabilistic model which supports large contexts in metalearning and achieves high accuracy where existing models fail. In our experiments, SPIN reduces memory requirements, improves accuracy across a range of metalearning tasks, and improves state-of-the-art performance on an important practical problem, genotype imputation.",
        "id": 257834209
      },
      {
        "title": "Improving Text Normalization via Unsupervised Model and Discriminative Reranking",
        "text": "Various models have been developed for normalizing informal text. In this paper, we propose two methods to improve normalization performance. First is an unsupervised approach that automatically identifies pairs of a non-standard token and proper word from a large unlabeled corpus. We use semantic similarity based on continuous word vector representation, together with other surface similarity measurement. Second we propose a reranking strategy to combine the results from different systems. This allows us to incorporate information that is hard to model in individual systems as well as consider multiple systems to generate a final rank for a test case. Both word-and sentence-level optimization schemes are explored in this study. We evaluate our approach on data sets used in prior studies, and demonstrate that our proposed methods perform better than the state-of-the-art systems.",
        "id": 9342522
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which paper first showed that task-specific knowledge embedded in parameters can be extracted from one LLM using seed samples and transferred to another via parameter-efficient fine-tuning?",
    "positive_ctxs": [
      {
        "title": "SEEKING NEURAL NUGGETS: KNOWLEDGE TRANSFER IN LARGE LANGUAGE MODELS FROM A PARAMETRIC PERSPECTIVE",
        "text": "Large Language Models (LLMs) inherently encode a wealth of knowledge within their parameters through pre-training on extensive corpora. While prior research has delved into operations on these parameters to manipulate the underlying implicit knowledge-encompassing detection, editing, and merging-there remains an ambiguous understanding regarding their transferability across models with varying scales. In this paper, we seek to empirically investigate knowledge transfer from larger to smaller models through a parametric perspective. To achieve this, we employ sensitivity-based techniques to extract and align knowledgespecific parameters between different LLMs. Moreover, the LoRA module is used as the intermediary mechanism for injecting the extracted knowledge into smaller models. Evaluations across four benchmarks validate the efficacy of our proposed method. Our findings highlight the critical factors contributing to the process of parametric knowledge transfer, underscoring the transferability of model parameters across LLMs of different scales. We release code and data at",
        "id": 264172668
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 235097424
      },
      {
        "title": "A Clustering Approach for the Nearly Unsupervised Recognition of Nonliteral Language *",
        "text": "In this paper we present TroFi (Trope Finder), a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word-sense disambiguation and clustering techniques. TroFi uses sentential context instead of selectional constraint violations or paths in semantic hierarchies. It also uses literal and nonliteral seed sets acquired and cleaned without human supervision in order to bootstrap learning. We adapt a word-sense disambiguation algorithm to our task and augment it with multiple seed set learners, a voting schema, and additional features like SuperTags and extrasentential context. Detailed experiments on hand-annotated data show that our enhanced algorithm outperforms the baseline by 24.4%. Using the TroFi algorithm, we also build the TroFi Example Base, an extensible resource of annotated literal/nonliteral examples which is freely available to the NLP research community. * This research was partially supported by NSERC, Canada (RGPIN: 264905). We would like to thank Bill Dolan, Fred Popowich, Dan Fass, Katja Markert, Yudong Liu, and the anonymous reviewers for their comments.",
        "id": 11890804
      },
      {
        "title": "DBpedia Abstracts: A Large-Scale, Open, Multilingual NLP Training Corpus",
        "text": "The ever increasing importance of machine learning in Natural Language Processing is accompanied by an equally increasing need in large-scale training and evaluation corpora. Due to its size, its openness and relative quality, the Wikipedia has already been a source of such data, but on a limited scale. This paper introduces the DBpedia Abstract Corpus, a large-scale, open corpus of annotated Wikipedia texts in six languages, featuring over 11 million texts and over 97 million entity links. The properties of the Wikipedia texts are being described, as well as the corpus creation process, its format and interesting use-cases, like Named Entity Linking training and evaluation.",
        "id": 29658212
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "I'm researching on the efficacy of recurrent networks in language modeling and CCG supertagging. Could you point me to studies that explore LSTM architectures and model comparisons in these tasks?",
    "positive_ctxs": [
      {
        "title": "Colorless green recurrent networks dream hierarchically",
        "text": "Recurrent neural networks (RNNs) have achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate here to what extent RNNs learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues (\"The colorless green ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas ideas I ate with the chair sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep sleep furiously\"), and, for Italian, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about long-distance agreement, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallowpattern extractors, but they also acquire deeper grammatical competence. * The work was conducted during the internship at Facebook AI Research, Paris.",
        "id": 4460159
      },
      {
        "title": "Supertagging with LSTMs",
        "text": "In this paper we present new state-of-the-art performance on CCG supertagging and parsing. Our model outperforms existing approaches by an absolute gain of 1.5%. We analyze the performance of several neural models and demonstrate that while feed-forward architectures can compete with bidirectional LSTMs on POS tagging, models that encode the complete sentence are necessary for the long range syntactic information encoded in supertags.",
        "id": 11771220
      }
    ],
    "negative_ctxs": [
      {
        "title": "Evaluating Feature Extraction Methods for Knowledge-based Biomedical Word Sense Disambiguation",
        "text": "In this paper, we present an analysis of feature extraction methods via dimensionality reduction for the task of biomedical Word Sense Disambiguation (WSD). We modify the vector representations in the 2-MRD WSD algorithm, and evaluate four dimensionality reduction methods: Word Embeddings using Continuous Bag of Words and Skip Gram, Singular Value Decomposition (SVD), and Principal Component Analysis (PCA). We also evaluate the effects of vector size on the performance of each of these methods. Results are evaluated on five standard evaluation datasets (Abbrev.100, Abbrev.200, Abbrev.300, NLM-WSD, and MSH-WSD). We find that vector sizes of 100 are sufficient for all techniques except SVD, for which a vector size of 1500 is preferred. We also show that SVD performs on par with Word Embeddings for all but one dataset.",
        "id": 1841504
      },
      {
        "title": "",
        "text": "",
        "id": 227231871
      },
      {
        "title": "Grammar-based tools for the creation of tagging resources for an unresourced language: the case of Northern Sotho",
        "text": "We describe an architecture for the parallel construction of a tagger lexicon and an annotated reference corpus for the part-of-speech tagging of Nothern Sotho, a Bantu language of South Africa, for which no tagged resources have been available so far. Our tools make use of grammatical properties (morphological and syntactic) of the language. We use symbolic pretagging, followed by stochastic tagging, an architecture which proves useful not only for the bootstrapping of tagging resources, but also for the tagging of any new text. We discuss the tagset design, the tool architecture and the current state of our ongoing effort.",
        "id": 14356537
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there any paper that performs adversarial training on frame level for audio-visual representation learning?",
    "positive_ctxs": [
      {
        "title": "MIR-GAN: Refining Frame-Level Modality-Invariant Representations with Adversarial Network for Audio-Visual Speech Recognition",
        "text": "Audio-visual speech recognition (AVSR) attracts a surge of research interest recently by leveraging multimodal signals to understand human speech. Mainstream approaches addressing this task have developed sophisticated architectures and techniques for multi-modality fusion and representation learning. However, the natural heterogeneity of different modalities causes distribution gap between their representations, making it challenging to fuse them. In this paper, we aim to learn the shared representations across modalities to bridge their gap. Different from existing similar methods on other multimodal tasks like sentiment analysis, we focus on the temporal contextual dependencies considering the sequence-to-sequence task setting of AVSR. In particular, we propose an adversarial network to refine framelevel modality-invariant representations (MIR-GAN), which captures the commonality across modalities to ease the subsequent multimodal fusion process. Extensive experiments on public benchmarks LRS3 and LRS2 show that our approach outperforms the state-of-the-arts 1 . Pingchuan Ma, Stavros Petridis, and Maja Pantic. 2021. End-to-end audio-visual speech recognition with con-. 2019. Recurrent neural network transducer for audio-visual speech recognition. In 2019 IEEE automatic speech recognition and understanding workshop (ASRU), pages 905-912. IEEE. Harry McGurk and John MacDonald. 1976. Hearing lips and seeing voices. Nature, 264(5588):746-748.",
        "id": 259203396
      }
    ],
    "negative_ctxs": [
      {
        "title": "Classifying Dialogue Acts in One-on-one Live Chats",
        "text": "We explore the task of automatically classifying dialogue acts in 1-on-1 online chat forums, an increasingly popular means of providing customer service. In particular, we investigate the effectiveness of various features and machine learners for this task. While a simple bag-of-words approach provides a solid baseline, we find that adding information from dialogue structure and inter-utterance dependency provides some increase in performance; learners that account for sequential dependencies (CRFs) show the best performance. We report our results from testing using a corpus of chat dialogues derived from online shopping customer-feedback data.",
        "id": 477778
      },
      {
        "title": "Focusing on a Subset of Scripts Enhances the Learning Efficiency of Second Language Writing System",
        "text": "Memorizing the whole set of graphemes is generally accepted as the first step of learning a phonogramic language. However, it is demanding for L2 learners to familiarize the whole inventory of graphemes in advance if the language has a relatively large inventory size. We propose that learning a subset of graphemes would largely enhance the learning efficiency by reducing the memory burden. With homophony minimized, effort of acquiring vocabulary in elementary stage can be greatly reduced. In this paper, the writing system of Thai is used to illustrate the main idea. Besides, the method may also be extendable to Japanese and Korean, which grapheme inventory sizes are smaller.",
        "id": 17724446
      },
      {
        "title": "Language Modeling using Dynamic Bayesian Networks",
        "text": "In this paper we propose a new approach to language modeling based on dynamic Bayesian networks. The principle idea of our approach is to find the dependence relations between variables that represent different linguistic units (word, class, concept, ...) that constitutes a language model. In the context of this paper the linguistic units that we consider are syntactic classes and words. Our approach should not be considered as a model combination technique. Rather, it is an original and coherent methodology that processes words and classes in the same model. We attempt to identify and model the dependence of words and classes on their linguistic context. Our ultimate goal is to devise an automatic mechanism that extracts the best dependence relations between a word and its context, i.e., lexical and syntactic. Preliminary results are very encouraging, in particular the model in which a word depends not only on previous word but also on syntactic classes of two previous words. This model outperforms the bi-gram model.",
        "id": 9297900
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest a research article that explores generative methods for extracting information, specifically one that covers the generation of surface forms, labeling of entities, and classification of entity types?",
    "positive_ctxs": [
      {
        "title": "GenIE: Generative Information Extraction",
        "text": "Structured and grounded representation of text is typically formalized by closed information extraction, the problem of extracting an exhaustive set of (subject, relation, object) triplets that are consistent with a predefined set of entities and relations from a knowledge base schema. Most existing works are pipelines prone to error accumulation, and all approaches are only applicable to unrealistically small numbers of entities and relations. We introduce GenIE (generative information extraction), the first end-to-end autoregressive formulation of closed information extraction. GenIE naturally exploits the language knowledge from the pre-trained transformer by autoregressively generating relations and entities in textual form. Thanks to a new bi-level constrained generation strategy, only triplets consistent with the predefined knowledge base schema are produced. Our experiments show that GenIE is state-of-the-art on closed information extraction, generalizes from fewer training data points than baselines, and scales to a previously unmanageable number of entities and relations. With this work, closed information extraction becomes practical in realistic scenarios, providing new opportunities for downstream tasks. Finally, this work paves the way towards a unified end-to-end approach to the core tasks of information extraction.",
        "id": 245144839
      }
    ],
    "negative_ctxs": [
      {
        "title": "Published as a conference paper at ICLR 2023 FLIP: A PROVABLE DEFENSE FRAMEWORK FOR BACKDOOR MITIGATION IN FEDERATED LEARNING",
        "text": "Federated Learning (FL) is a distributed learning paradigm that enables different parties to train a model together for high quality and strong privacy protection. In this scenario, individual participants may get compromised and perform backdoor attacks by poisoning the data (or gradients). Existing work on robust aggregation and certified FL robustness does not study how hardening benign clients can affect the global model (and the malicious clients). In this work, we theoretically analyze the connection among cross-entropy loss, attack success rate, and clean accuracy in this setting. Moreover, we propose a trigger reverse engineering based defense and show that our method can achieve robustness improvement with guarantee (i.e., reducing the attack success rate) without affecting benign accuracy. We conduct comprehensive experiments across different datasets and attack settings. Our results on nine competing SOTA defense methods show the empirical superiority of our method on both single-shot and continuous FL backdoor attacks. Code is available at https://github.com/KaiyuanZh/FLIP.",
        "id": 253098063
      },
      {
        "title": "Why adversarial training can hurt robust accuracy",
        "text": "Machine learning classifiers with high test accuracy often perform poorly under adversarial attacks. It is commonly believed that adversarial training alleviates this issue. In this paper, we demonstrate that, surprisingly, the opposite may be true -Even though adversarial training helps when enough data is available, it may hurt robust generalization in the small sample size regime. We first prove this phenomenon for a high-dimensional linear classification setting with noiseless observations. Our proof provides explanatory insights that may also transfer to feature learning models. Further, we observe in experiments on standard image datasets that the same behavior occurs for perceptible attacks that effectively reduce class information such as mask attacks and object corruptions.",
        "id": 247244739
      },
      {
        "title": "Discovering Latent Concepts and Exploiting Ontological Features for Semantic Text Search",
        "text": "Named entities and WordNet words are important in defining the content of a text in which they occur. Named entities have ontological features, namely, their aliases, classes, and identifiers. WordNet words also have ontological features, namely, their synonyms, hypernyms, hyponyms, and senses. Those features of concepts may be hidden from their textual appearance. Besides, there are related concepts that do not appear in a query, but can bring out the meaning of the query if they are added. The traditional constrained spreading activation algorithms use all relations of a node in the network that will add unsuitable information into the query. Meanwhile, we only use relations represented in the query. We propose an ontology-based generalized Vector Space Model to semantic text search. It discovers relevant latent concepts in a query by relation constrained spreading activation. Besides, to represent a word having more than one possible direct sense, it combines the most specific common hypernym of the remaining undisambiguated multi-senses with the form of the word. Experiments on a benchmark dataset in terms of the MAP measure for the retrieval performance show that our model is 41.9% and 29.3% better than the purely keyword-based model and the traditional constrained spreading activation model, respectively.",
        "id": 3001852
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which work discusses an analysis of source and target contributions to output generation based on local interpretation when machine translation models experience hallucinations?",
    "positive_ctxs": [
      {
        "title": "Local Interpretation of Transformer Based on Linear Decomposition",
        "text": "In recent years, deep neural networks (DNNs) have achieved state-of-the-art performance on a wide range of tasks. However, limitations in interpretability have hindered their applications in the real world. This work proposes to interpret neural networks by linear decomposition and finds that the ReLU-activated Transformer can be considered as a linear model on a single input. We further leverage the linearity of the model and propose a linear decomposition of the model output to generate local explanations. Our evaluation of sentiment classification and machine translation shows that our method achieves competitive performance in efficiency and fidelity of explanation. In addition, we demonstrate the potential of our approach in applications with examples of error analysis on multiple tasks. 1 * Corresponding author. 1  We release our algorithm toolkit at https://github. com/DoubleVII/pydec.",
        "id": 259370787
      }
    ],
    "negative_ctxs": [
      {
        "title": "PERFECTLY SECURE STEGANOGRAPHY USING MINIMUM ENTROPY COUPLING",
        "text": "Steganography is the practice of encoding secret information into innocuous content in such a manner that an adversarial third party would not realize that there is hidden meaning.While this problem has classically been studied in security literature, recent advances in generative models have led to a shared interest among security and machine learning researchers in developing scalable steganography techniques.In this work, we show that a steganography procedure is perfectly secure under Cachin (1998)'s information-theoretic model of steganography if and only if it is induced by a coupling.Furthermore, we show that, among perfectly secure procedures, a procedure maximizes information throughput if and only if it is induced by a minimum entropy coupling.These insights yield what are, to the best of our knowledge, the first steganography algorithms to achieve perfect security guarantees for arbitrary covertext distributions.To provide empirical validation, we compare a minimum entropy coupling-based approach to three modern baselines-arithmetic coding, Meteor, and adaptive dynamic groupingusing GPT-2, WaveRNN, and Image Transformer as communication channels.We find that the minimum entropy coupling-based approach achieves superior encoding efficiency, despite its stronger security constraints.In aggregate, these results suggest that it may be natural to view information-theoretic steganography through the lens of minimum entropy coupling.",
        "id": 253117027
      },
      {
        "title": "Improvement of Statistical Machine Translation using Charater- Based Segmentation with Monolingual and Bilingual Information",
        "text": "We present a novel segmentation approach for Phrase-Based Statistical Machine Translation (PB-SMT) to languages where word boundaries are not obviously marked by using both monolingual and bilingual information and demonstrate that (1) unsegmented corpus is able to provide the nearly identical result compares to manually segmented corpus in PB-SMT task when a good heuristic character clustering algorithm is applied on it, (2) the performance of PB-SMT task has significantly increased when bilingual information are used on top of monolingual segmented result. Our technique, instead of focusing on word separation, mainly concentrate on a group of character. First, we group several characters that reside in an unsegmented corpus by employing predetermined constraints and certain heuristics algorithms. Secondly, we enhance the segmented result by incorporating the character group repacking based on alignment confidence. We evaluate the effectiveness of our method on PB-SMT task using English-Thai, English-Lao and English-Burmese language pairs and report the best improvement of 8.1% increase in BLEU score on English-Thai pair.",
        "id": 17919789
      },
      {
        "title": "",
        "text": "",
        "id": 227905523
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Which work pushes the limit of model quantization in BERT models by introducing a ternary network?",
    "positive_ctxs": [
      {
        "title": "BinaryBERT: Pushing the Limit of BERT Quantization",
        "text": "The rapid development of large pre-trained language models has greatly increased the demand for model compression techniques, among which quantization is a popular solution. In this paper, we propose Binary-BERT, which pushes BERT quantization to the limit by weight binarization. We find that a binary BERT is hard to be trained directly than a ternary counterpart due to its complex and irregular loss landscape. Therefore, we propose ternary weight splitting, which initializes BinaryBERT by equivalently splitting from a half-sized ternary network. The binary model thus inherits the good performance of the ternary one, and can be further enhanced by fine-tuning the new architecture after splitting. Empirical results show that our Binary-BERT has only a slight performance drop compared with the full-precision model while being 24× smaller, achieving the state-of-the-art compression results on the GLUE and SQuAD benchmarks. (a) Full-precision Model. (b) Ternary Model. (c) Binary Model. (d) All Together.",
        "id": 229923538
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 935515
      },
      {
        "title": "Lithium NLP: A System for Rich Information Extraction from Noisy User Generated Text on Social Media",
        "text": "In this paper, we describe the Lithium Natural Language Processing (NLP) system -a resource-constrained, highthroughput and language-agnostic system for information extraction from noisy user generated text on social media. Lithium NLP extracts a rich set of information including entities, topics, hashtags and sentiment from text. We discuss several real world applications of the system currently incorporated in Lithium products. We also compare our system with existing commercial and academic NLP systems in terms of performance, information extracted and languages supported. We show that Lithium NLP is at par with and in some cases, outperforms stateof-the-art commercial NLP systems. available at https://youtu.be/U-o6Efh6TZc",
        "id": 2900072
      },
      {
        "title": "Resolving Translation Ambiguity using Non-parallel Bilingual Corpora",
        "text": "This paper presents an unsupervised method for choosing the correct translation of a word in context. It learns disambiguation information from nonparallel bilinguM corpora (preferably in the same domain) free from tagging.Our method combines two existing unsupervised disambiguation algorithms: a word sense disambiguation algorithm based on distributional clustering and a translation disambiguation algorithm using target language corpora.For the given word in context, the former algorithm identifies its meaning as one of a number of predefined usage classes derived by clustering a large amount of usages in the source language corpus. The latter algorithm is responsible for associating each usage class (i.e., cluster) with a target word that is most relevant to the usage. This paper also shows preliminary results of translation experiments.",
        "id": 497469
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper first construct large-scale corpus to improve in-context learning of large language models in the pre-training stage?",
    "positive_ctxs": [
      {
        "title": "Pre-Training to Learn in Context",
        "text": "In-context learning, where pre-trained language models learn to perform tasks from task examples and instructions in their contexts, has attracted much attention in the NLP community. However, the ability of in-context learning is not fully exploited because language models are not explicitly trained to learn in context. To this end, we propose PICL (Pretraining for In-Context Learning), a framework to enhance the language models' in-context learning ability by pre-training the model on a large collection of \"intrinsic tasks\" in the general plain-text corpus using the simple language modeling objective. PICL encourages the model to infer and perform tasks by conditioning on the contexts while maintaining task generalization of pre-trained models. We evaluate the in-context learning performance of the model trained with PICL on seven widelyused text classification datasets and the SUPER-NATURALINSTRCTIONS benchmark, which contains 100+ NLP tasks formulated to text generation. Our experiments show that PICL is more effective and task-generalizable than a range of baselines, outperforming larger language models with nearly 4x parameters. The code is publicly available at https://github. com/thu-coai/PICL.",
        "id": 258715048
      }
    ],
    "negative_ctxs": [
      {
        "title": "Using Distributional Similarity of Multi-way Translations to Predict Multiword Expression Compositionality",
        "text": "We predict the compositionality of multiword expressions using distributional similarity between each component word and the overall expression, based on translations into multiple languages. We evaluate the method over English noun compounds, English verb particle constructions and German noun compounds. We show that the estimation of compositionality is improved when using translations into multiple languages, as compared to simply using distributional similarity in the source language. We further find that string similarity complements distributional similarity.",
        "id": 2527274
      },
      {
        "title": "Expert Stance Graphs for Computational Argumentation",
        "text": "We describe the construction of an Expert Stance Graph, a novel, large-scale knowledge resource that encodes the stance of more than 100,000 experts towards a variety of controversial topics. We suggest that this graph may be valuable for various fundamental tasks in computational argumentation. Experts and topics in our graph are Wikipedia entries. Both automatic and semi-automatic methods for building the graph are explored, and manual assessment validates the high accuracy of the resulting graph.",
        "id": 18648384
      },
      {
        "title": "A SYSTEM FOR TRANSLATING LOCATIVE PREPOSITIONS FROM ENGLISH INTO FRENCH*",
        "text": "Machine translation of locative prepositions is not straightforward, even between closely related languages. This paper discusses a system of translation of locative prepositions between English and French. The system is based on the premises that English and French do not always conceptualize objects in the same way, and that this accounts for the major differences in the ways that locative prepositions are used in these languages. This paper introduces knowledge representations of conceptualizations of objects, and a method for translating prepositions based on these conceptual representations.",
        "id": 321686
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "What work attempts to explore multi-hop reasoning by densifying commonsense knowledge graphs?",
    "positive_ctxs": [
      {
        "title": "Dense-ATOMIC: Towards Densely-connected ATOMIC with High Knowledge Coverage and Massive Multi-hop Paths",
        "text": "ATOMIC is a large-scale commonsense knowledge graph (CSKG) containing everyday ifthen knowledge triplets, i.e., {head event, relation, tail event}. The one-hop annotation manner made ATOMIC a set of independent bipartite graphs, which ignored the numerous links between events in different bipartite graphs and consequently caused shortages in knowledge coverage and multi-hop paths. In this work, we aim to construct Dense-ATOMIC with high knowledge coverage and massive multi-hop paths. The events in ATOMIC are normalized to a consistent pattern at first. We then propose a CSKG completion method called Rel-CSKGC to predict the relation given the head event and the tail event of a triplet, and train a CSKG completion model based on existing triplets in ATOMIC. We finally utilize the model to complete the missing links in ATOMIC and accordingly construct Dense-ATOMIC. Both automatic and human evaluation on an annotated subgraph of ATOMIC demonstrate the advantage of Rel-CSKGC over strong baselines. We further conduct extensive evaluations on Dense-ATOMIC in terms of statistics, human evaluation, and simple downstream tasks, all proving Dense-ATOMIC's advantages in Knowledge",
        "id": 258959081
      }
    ],
    "negative_ctxs": [
      {
        "title": "Word-Sense Disambiguation for Machine Translation",
        "text": "In word sense disambiguation, a system attempts to determine the sense of a word from contextual features. Major barriers to building a high-performing word sense disambiguation system include the difficulty of labeling data for this task and of predicting fine-grained sense distinctions. These issues stem partly from the fact that the task is being treated in isolation from possible uses of automatically disambiguated data. In this paper, we consider the related task of word translation, where we wish to determine the correct translation of a word from context. We can use parallel language corpora as a large supply of partially labeled data for this task. We present algorithms for solving the word translation problem and demonstrate a significant improvement over a baseline system. We then show that the word-translation system can be used to improve performance on a simplified machinetranslation task and can effectively and accurately prune the set of candidate translations for a word.",
        "id": 7241107
      },
      {
        "title": "Translating and the Computer 20",
        "text": "Cost, quality and time, not necessarily in that order, are the three most important factors in a company's translation strategy. Or, more pertinently, how to cut costs, how to improve quality and how to save time. Improvements in the quality and efficiency of translation can be effected at the very birth of a document, that is, at the time of authoring. There are several ways of ensuring consistency and quality of authoring to various degrees, all of which have a beneficial effect at the translation stage. This paper examines the concepts of controlled authoring and author memory and the supporting technologies, the processes involved in developing such techniques within an organisation and the business benefits that such techniques bring.",
        "id": 41780695
      },
      {
        "title": "The Alice Datasets: fMRI & EEG Observations of Natural Language Comprehension",
        "text": "The Alice Datasets combine observations from magnetic resonance imaging as well as electrophysiology while human participants listened to the same literary narrative in English. Along with these neural signals and the text of the story, we also provide a variety of word-by-word predictors motivated by research in computational linguistics and cognitive science. These predictors range from prosody to morphology to syntax. These annotated, naturalistic datasets can be used to replicate prior work and test new hypotheses about natural language comprehension in the brain.",
        "id": 218973758
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which paper is the first to model the helpfulness and harmlessness alignment of LLMs as a Constrained MDP problem?",
    "positive_ctxs": [
      {
        "title": "SAFE RLHF: SAFE REINFORCEMENT LEARNING FROM HUMAN FEEDBACK",
        "text": "With the development of large language models (LLMs), striking a balance between the performance and safety of AI systems has never been more critical.However, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training.To address this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe RLHF), a novel algorithm for human value alignment.Safe RLHF explicitly decouples human preferences regarding helpfulness and harmlessness, effectively avoiding the crowdworkers' confusion about the tension and allowing us to train separate reward and cost models.We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints.Leveraging the Lagrangian method to solve this constrained problem, Safe RLHF dynamically adjusts the balance between the two objectives during fine-tuning.Through a three-round fine-tuning using Safe RLHF, we demonstrate a superior ability to mitigate harmful responses while enhancing model performance compared to existing value-aligned algorithms.Experimentally, we finetuned the Alpaca-7B using Safe RLHF and aligned it with collected human preferences, significantly improving its helpfulness and harmlessness according to human evaluations.",
        "id": 264306078
      }
    ],
    "negative_ctxs": [
      {
        "title": "A Common Multimedia Annotation Framework for Cross Linking Cultural Heritage Digital Collections",
        "text": "In the context of the CATCH research program that is currently carried out at a number of large Dutch cultural heritage institutions our ambition is to combine and exchange heterogeneous multimedia annotations between projects and institutions. As first step we designed an Annotation Meta Model: a simple but powerful RDF/OWL model mainly addressing the anchoring of annotations to segments of the many different media types used in the collections of the archives, museums and libraries involved. The model includes support for the annotation of annotations themselves, and of segments of annotation values, to be able to layer annotations and in this way enable projects to process each other's annotation data as the primary data for further annotation. On basis of AMM we designed an application programming interface for accessing annotation repositories and implemented it both as a software library and as a web service. Finally, we report on our experiences with the application of model, API and repository when developing web applications for collection managers in cultural heritage institutions.",
        "id": 1771259
      },
      {
        "title": "95% Replicability for Manual Word Sense Tagging",
        "text": "",
        "id": 30754231
      },
      {
        "title": "INTERFACE DATABASES: DESIGN AND COLLECTION OF A MULTILINGUAL EMOTIONAL SPEECH DATABASE",
        "text": "As a part of the IST project Interface (\"Multimodal Analysis/Synthesis System for Human Interaction to Virtual and Augmented environments\"), an emotional speech database for Slovenian, English, Spanish, and French language has been recorded. The database is designed for general study of emotional speech as well as analysis of emotion characteristics for speech synthesis and for automatic emotion classification purposes. Six emotions have been defined: anger, sadness, joy, fear, disgust and surprise. The neutral styles were also recorded. One male speaker and one female speaker have been recorded, except for English language where two mail and one female speaker have been recorded. All the speakers are actors. The corpuses consist of 175-190 sentences for each language. For Spanish and Slovenian databases subjective evaluation tests have been made. The recorded Interface emotional speech database represents a good basis for emotional speech analysis and is also useful in synthesis of emotional speech.",
        "id": 44551203
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that examines how well structured pruning techniques perform in developing both small and precise models in natural language processing?",
    "positive_ctxs": [
      {
        "title": "Structured Pruning Learns Compact and Accurate Models",
        "text": "The growing size of neural language models has led to increased attention in model compression. The two predominant approaches are pruning, which gradually removes weights from a pre-trained model, and distillation, which trains a smaller compact model to match a larger one. Pruning methods can significantly reduce the model size but hardly achieve large speedups as distillation. However, distillation methods require large amounts of unlabeled data and are expensive to train. In this work, we propose a task-specific structured pruning method CoFi 1 (Coarse-and Fine-grained Pruning), which delivers highly parallelizable subnetworks and matches the distillation methods in both accuracy and latency, without resorting to any unlabeled data. Our key insight is to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads and hidden units) modules, which controls the pruning decision of each parameter with masks of different granularity. We also devise a layerwise distillation strategy to transfer knowledge from unpruned to pruned models during optimization. Our experiments on GLUE and SQuAD datasets show that CoFi yields models with over 10× speedups with a small accuracy drop, showing its effectiveness and efficiency compared to previous pruning and distillation approaches. 2",
        "id": 247922354
      }
    ],
    "negative_ctxs": [
      {
        "title": "A Framework to Generate High-Quality Datapoints for Multiple Novel Intent Detection",
        "text": "Systems like Voice-command based conversational agents are characterized by a predefined set of skills or intents to perform user specified tasks. In the course of time, newer intents may emerge requiring retraining. However, the newer intents may not be explicitly announced and need to be inferred dynamically. Thus, there are two important tasks at hand (a). identifying emerging new intents, (b). annotating data of the new intents so that the underlying classifier can be retrained efficiently. The tasks become specially challenging when a large number of new intents emerge simultaneously and there is a limited budget of manual annotation. In this paper, we propose MNID (Multiple Novel Intent Detection) which is a cluster based framework to detect multiple novel intents with budgeted human annotation cost. Empirical results on various benchmark datasets (of different sizes) demonstrate that MNID, by intelligently using the budget for annotation, outperforms the baseline methods in terms of accuracy and F1score.",
        "id": 248513007
      },
      {
        "title": "Analysis of Language Change in Collaborative Instruction Following",
        "text": "We analyze language change over time in a collaborative, goal-oriented instructional task, where utility-maximizing participants form conventions and increase their expertise. Prior work studied such scenarios mostly in the context of reference games, and consistently found that language complexity is reduced along multiple dimensions, such as utterance length, as conventions are formed. In contrast, we find that, given the ability to increase instruction utility, instructors increase language complexity along these previously studied dimensions to better collaborate with increasingly skilled instruction followers.",
        "id": 237453540
      },
      {
        "title": "DIFFERENTIALLY PRIVATE ADAPTIVE OPTIMIZATION WITH DELAYED PRECONDITIONERS",
        "text": "Privacy noise may negate the benefits of using adaptive optimizers in differentially private model training. Prior works typically address this issue by using auxiliary information (e.g., public data) to boost the effectiveness of adaptive optimization. In this work, we explore techniques to estimate and efficiently adapt to gradient geometry in private adaptive optimization without auxiliary data. Motivated by the observation that adaptive methods can tolerate stale preconditioners, we propose differentially private adaptive training with delayed preconditioners (DP 2 ), a simple method that constructs delayed but less noisy preconditioners to better realize the benefits of adaptivity. Theoretically, we provide convergence guarantees for our method for both convex and non-convex problems, and analyze trade-offs between delay and privacy noise reduction. Empirically, we explore DP 2 across several realworld datasets, demonstrating that it can improve convergence speed by as much as 4× relative to non-adaptive baselines and match the performance of state-of-the-art optimization methods that require auxiliary data.arXiv:2212.00309v2 [cs.LG] 7 Jun 2023John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization.",
        "id": 254125312
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which was the first paper to explore the online adaptation of neural MT metrics for use during the inference stage?",
    "positive_ctxs": [
      {
        "title": "Test-time Adaptation for Machine Translation Evaluation by Uncertainty Minimization",
        "text": "The neural metrics recently received considerable attention from the research community in the automatic evaluation of machine translation. Unlike text-based metrics that have interpretable and consistent evaluation mechanisms for various data sources, the reliability of neural metrics in assessing out-of-distribution data remains a concern due to the disparity between training data and real-world data. This paper aims to address the inference bias of neural metrics through uncertainty minimization during test time, without requiring additional data. Our proposed method comprises three steps: uncertainty estimation, test-time adaptation, and inference. Specifically, the model employs the prediction uncertainty of the current data as a signal to update a small fraction of parameters during test time and subsequently refine the prediction through optimization. To validate our approach, we apply the proposed method to three representative models and conduct experiments on the WMT21 benchmarks. The results obtained from both in-domain and out-of-distribution evaluations consistently demonstrate improvements in correlation performance across different models. Furthermore, we provide evidence that the proposed method effectively reduces model uncertainty. The code is publicly available at https://github.com/NLP2CT/TaU.",
        "id": 259370785
      }
    ],
    "negative_ctxs": [
      {
        "title": "Rephrasing Profanity in Chinese Text",
        "text": "This paper proposes a system that can detect and rephrase profanity in Chinese text. Rather than just masking detected profanity, we want to revise the input sentence by using inoffensive words while keeping their original meanings. 29 of such rephrasing rules were invented after observing sentences on real-word social websites. The overall accuracy of the proposed system is 85.56%",
        "id": 32076269
      },
      {
        "title": "",
        "text": "",
        "id": 393794
      },
      {
        "title": "Investigating Advanced Techniques for Document Content Similarity Applied to External Plagiarism Analysis",
        "text": "We present an approach to perform external plagiarism analysis by applying several similarity detection techniques, such as lexical measures and a textual entailment recognition system developed by our research group. Some of the least expensive features of this system are applied to all corpus documents to detect those that are likely to be plagiarized. After this is done, the whole system is applied over this subset of documents to extract the exact n-grams that have been plagiarized, given that we now have less data to process and therefore can use a more complex and costly function. Apart from the application of strictly lexical measures, we also experiment with a textual entailment recognition system to detect plagiarisms with a high level of obfuscation. In addition, we experiment with the application of a spell corrector and a machine translation system to handle misspellings and plagiarisms translated into different languages, respectively.",
        "id": 15268382
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Are there studies that investigate debiasing language models automatically using prompting?",
    "positive_ctxs": [
      {
        "title": "Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts",
        "text": "Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task. In this paper, we propose an automatic method to mitigate the biases in pretrained language models. Different from previous debiasing work that uses external corpora to finetune the pretrained models, we instead directly probe the biases encoded in pretrained models through prompts. Specifically, we propose a variant of the beam search method to automatically search for biased prompts such that the cloze-style completions are the most different with respect to different demographic groups. Given the identified biased prompts, we then propose a distribution alignment loss to mitigate the biases. Experiment results on standard datasets and metrics show that our proposed Auto-Debias approach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT. Moreover, the improvement in fairness does not decrease the language models' understanding abilities, as shown using the GLUE benchmark.",
        "id": 248780440
      }
    ],
    "negative_ctxs": [
      {
        "title": "MULTI-CLASS CLASSIFICATION WITHOUT MULTI- CLASS LABELS",
        "text": "This work presents a new strategy for multi-class classification that requires no class-specific labels, but instead leverages pairwise similarity between examples, which is a weaker form of annotation. The proposed method, meta classification learning, optimizes a binary classifier for pairwise similarity prediction and through this process learns a multi-class classifier as a submodule. We formulate this approach, present a probabilistic graphical model for it, and derive a surprisingly simple loss function that can be used to learn neural network-based models. We then demonstrate that this same framework generalizes to the supervised, unsupervised cross-task, and semi-supervised settings. Our method is evaluated against state of the art in all three learning paradigms and shows a superior or comparable accuracy, providing evidence that learning multi-class classification without multi-class labels is a viable learning option.",
        "id": 57375742
      },
      {
        "title": "",
        "text": "",
        "id": 235097504
      },
      {
        "title": "NEURAL GRAPH EVOLUTION: TOWARDS EFFICIENT AUTOMATIC ROBOT DESIGN",
        "text": "Despite the recent successes in robotic locomotion control, the design of robots, i.e., the design of their body structure, still heavily relies on human engineering. Automatic robot design has been a long studied subject, however, progress has been slow due to large combinatorial search space and the difficulty to efficiently evaluate the candidate structures. Note that one needs to both, search over many possible body structures, and choose among them based on how the robot with that structure performs in an environment. The latter means training an optimal controller given a candidate structure, which in itself is costly to obtain. In this paper, we propose Neural Graph Evolution (NGE), which performs evolutionary search in graph space, by iteratively evolving graph structures using simple mutation primitives. Key to our approach is to parameterize the control policies with graph neural networks, which allows us to transfer skills from previously evaluated designs during the graph search. This significantly reduces evaluation cost of new candidates and makes the search process orders of magnitude more efficient than that of past work. In addition, NGE applies Graph Mutation with Uncertainty (GM-UC) by incorporating model uncertainty, which reduces the search space by balancing exploration and exploitation. We show that NGE significantly outperforms previous methods in terms of convergence rate and final performance. As shown in experiments, NGE is the first algorithm that can automatically discover kinematically preferred robotic graph structures, such as a fish with two symmetric flat side-fins and a tail, or a cheetah with athletic front and back legs. NGE is extremely efficient, it finds plausible robotic structures within a day on a single 64 CPU-core Amazon EC2 machine. The code and project webpage are released 1 . * Two authors contribute equally.",
        "id": 88503668
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest an article that leverages the spatial information available in documents for multi-modal LMs by using a spatially-aware attention mechanism?",
    "positive_ctxs": [
      {
        "title": "LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding",
        "text": "Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. We propose LayoutLMv2 architecture with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. Specifically, with a two-stream multi-modal Transformer encoder, LayoutLMv2 uses not only the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks, which make it better capture the cross-modality interaction in the pre-training stage. Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture so that the model can fully understand the relative positional relationship among different text blocks. Experiment results show that LayoutLMv2 outperforms LayoutLM by a large margin and achieves new state-ofthe-art results on a wide variety of downstream visually-rich document understanding tasks, including FUNSD (0.7895 → 0.8420), CORD (0.9493 → 0.9601), SROIE (0.9524 → 0.9781), Kleister-NDA (0.8340 → 0.8520), RVL-CDIP (0.9443 → 0.9564), and DocVQA (0.7295 → 0.8672). We made our model and code publicly available at https://aka.ms /layoutlmv2.",
        "id": 229923949
      }
    ],
    "negative_ctxs": [
      {
        "title": "Integrating NLP Tools in a Distributed Environment: A Case Study Chaining a Tagger with a Dependency Parser",
        "text": "The present paper tackles the issue of PoS tag conversion within the framework of a distributed web service platform for the automatic creation of language resources. PoS tagging is now considered a \"solved problem\"; yet, because of the differences in the tagsets, interchange of the various PoS taggers available is still hampered. In this paper we describe the implementation of a PoS-tagged-corpus converter, which is needed for chaining together in a workflow the FreeLing PoS tagger for Italian and the DESR dependency parser, given that these two tools have been developed independently. The conversion problems experienced during the implementation, related to the properties of the different tagsets and of tagset conversion in general, are discussed together with the solutions adopted. Finally, the converter is evaluated by assessing the impact of conversion on the performance of the dependency parser by comparing with the outcome of the native pipeline. From this we learn that in most cases parsing errors are due to actual tagging errors, and not to conversion itself. Besides, information on accuracy loss is an important feature in a distributed environment of (NLP) services, where users need to decide which services best suit their needs.",
        "id": 15300153
      },
      {
        "title": "Leveraging Web Based Evidence Gathering for Drug Information Identification from Tweets",
        "text": "In this paper, we have explored web-based evidence gathering and different linguistic features to automatically extract drug names from tweets and further classify such tweets into Adverse Drug Events or not. We have evaluated our proposed models with the dataset as released by the SMM4H workshop shared Task-1 and Task-3 respectively. Our evaluation results shows that the proposed model achieved good results, with Precision, Recall and F-scores of 78.5%, 88% and 82.9% respectively for Task1 and 33.2%, 54.7% and 41.3% for Task3.",
        "id": 53622046
      },
      {
        "title": "Memory Consolidation for Contextual Spoken Language Understanding with Dialogue Logistic Inference",
        "text": "Dialogue contexts are proven helpful in the spoken language understanding (SLU) system and they are typically encoded with explicit memory representations. However, most of the previous models learn the context memory with only one objective to maximizing the SLU performance, leaving the context memory under-exploited. In this paper, we propose a new dialogue logistic inference (DLI) task to consolidate the context memory jointly with SLU in the multi-task framework. DLI is defined as sorting a shuffled dialogue session into its original logical order and shares the same memory encoder and retrieval mechanism as the SLU model. Our experimental results show that various popular contextual SLU models can benefit from our approach, and improvements are quite impressive, especially in slot filling.",
        "id": 174799129
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper used both automatically generated and manual templates with word tuples to adapt language models from one timestamp to another?",
    "positive_ctxs": [
      {
        "title": "Learning Dynamic Contextualised Word Embeddings via Template-based Temporal Adaptation",
        "text": "Dynamic contextualised word embeddings (DCWEs) represent the temporal semantic variations of words. We propose a method for learning DCWEs by time-adapting a pretrained Masked Language Model (MLM) using timesensitive templates. Given two snapshots C 1 and C 2 of a corpus taken respectively at two distinct timestamps T 1 and T 2 , we first propose an unsupervised method to select (a) pivot terms related to both C 1 and C 2 , and (b) anchor terms that are associated with a specific pivot term in each individual snapshot. We then generate prompts by filling manually compiled templates using the extracted pivot and anchor terms. Moreover, we propose an automatic method to learn time-sensitive templates from C 1 and C 2 , without requiring any human supervision. Next, we use the generated prompts to adapt a pretrained MLM to T 2 by fine-tuning using those prompts. Multiple experiments show that our proposed method reduces the perplexity of test sentences in C 2 , outperforming the current state-of-the-art.",
        "id": 251741028
      }
    ],
    "negative_ctxs": [
      {
        "title": "Perception audio-visuelle de séquences VCV produites par des personnes porteuses de Trisomie 21 : une étude préliminaire",
        "text": "RESUMELa parole des personnes avec trisomie 21 (T21) présente une altération systématique de l'intelligibilité qui n'a été quantifiée qu'auditivement. Or la modalité visuelle pourrait améliorer l'intelligibilité comme c'est le cas pour les personnes « ordinaires ». Cette étude compare la manière dont 24 participants ordinaires perçoivent des séquences VCV voyelle-consonne-voyelle) produites par quatre adultes (2 avec T21 et 2 ordinaires) et présentées dans le bruit en modalités auditive, visuelle et audiovisuelle. Les résultats confirment la perte d'intelligibilité en modalité auditive dans le cas de locuteurs porteurs de T21. Pour les deux locuteurs impliqués, l'intelligibilité visuelle est néanmoins équivalente à celle des deux locuteurs ordinaires et compensent le déficit d'intelligibilité auditive. Ces résultats suggèrent l'apport de la modalité visuelle vers une meilleure intelligibilité des personnes porteuses de T21.ABSTRACTAuditory-visual Perception of VCVs Produced by People with Down Syndrome: a Preliminary StudyThe speech of people with Down Syndrome (DS) is systematically altered resulting in an intelligibility loss. This was quantified only auditorily. The visual modality could actually improve intelligibility, as is the case for \"ordinary\" people. The present study compares the way 24 ordinary participants perceive VCV sequences (vowel-consonant-vowel) produced by four adults (2 with DS and 2 ordinary) and presented in noise in three modalities: auditory, auditoryvisual and visual. The results confirm an intelligibility loss in the auditory modality for speakers with DS. However, for the two speakers involved in this study, visual intelligibility is equivalent to that of the ordinary speakers and compensates for the auditory intelligibility loss. These results put forward the importance of integrating multimodality to improve the intelligibility of people with DS. MOTS-CLES : Parole, Multimodalité, Perception, Trisomie 21, Apport visuel.",
        "id": 148774062
      },
      {
        "title": "Example-Based Machine Translation Using a Dictionary of Word Pairs",
        "text": "Machine translation systems, whether rule-based, example-based, or statistical, all rely on dictionaries that are in essence mappings between individual words of the source and the target language. Criteria for the disambiguation of ambiguous words and for differences in word order between the two languages are not accounted for in the lexicon. Instead, these important issues are dealt with in the translation engines. Because the engines tend to be compact and (even with data-oriented approaches) do not fully reflect the complexity of the problem, this approach generally does not account for the more fine grained facets of word behavior. This leads to wrong generalizations and, as a consequence, translation quality tends to be poor. In this paper we suggest to approach this problem by using a new type of lexicon that is not based on individual words but on pairs of words. For each pair of consecutive words in the source language the lexicon lists the possible translations in the target language together with information on order and distance of the target words. The process of machine translation is then seen as a combinatorial problem: For all word pairs in a source sentence all possible translations are retrieved from the lexicon and then those translations are discarded that lead to contradictions when constructing the target sentence. This process implicitly leads to word sense disambiguation and to language specific reordering of words.",
        "id": 2710648
      },
      {
        "title": "SCALABLE BAYESIAN INVERSE REINFORCEMENT LEARNING",
        "text": "Bayesian inference over the reward presents an ideal solution to the ill-posed nature of the inverse reinforcement learning problem. Unfortunately current methods generally do not scale well beyond the small tabular setting due to the need for an inner-loop MDP solver, and even non-Bayesian methods that do themselves scale often require extensive interaction with the environment to perform well, being inappropriate for high stakes or costly applications such as healthcare. In this paper we introduce our method, Approximate Variational Reward Imitation Learning (AVRIL), that addresses both of these issues by jointly learning an approximate posterior distribution over the reward that scales to arbitrarily complicated state spaces alongside an appropriate policy in a completely offline manner through a variational approach to said latent reward. Applying our method to real medical data alongside classic control simulations, we demonstrate Bayesian reward inference in environments beyond the scope of current methods, as well as task performance competitive with focused offline imitation learning algorithms.",
        "id": 231918471
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which paper uses the latent diffusion model for the first time to solve offline reinforcement learning problems based on the sequential modeling framework?",
    "positive_ctxs": [
      {
        "title": "Efficient Planning with Latent Diffusion",
        "text": "Temporal abstraction and efficient planning pose significant challenges in offline reinforcement learning, mainly when dealing with domains that involve temporally extended tasks and delayed sparse rewards.Existing methods typically plan in the raw action space and can be inefficient and inflexible.Latent action spaces offer a more flexible paradigm, capturing only possible actions within the behavior policy support and decoupling the temporal structure between planning and modeling.However, current latent-action-based methods are limited to discrete spaces and require expensive planning.This paper presents a unified framework for continuous latent action space representation learning and planning by leveraging latent, score-based diffusion models.We establish the theoretical equivalence between planning in the latent action space and energy-guided sampling with a pretrained diffusion model and incorporate a novel sequence-level exact sampling method.Our proposed method, LatentDiffuser, demonstrates competitive performance on low-dimensional locomotion control tasks and surpasses existing methods in higher-dimensional tasks.",
        "id": 263334587
      }
    ],
    "negative_ctxs": [
      {
        "title": "APPLICATIONS OF A LEXICOGRAPHICAL DATA BASE FOR GERMAN",
        "text": "",
        "id": 11966677
      },
      {
        "title": "Prune-and-Score: Learning for Greedy Coreference Resolution",
        "text": "We propose a novel search-based approach for greedy coreference resolution, where the mentions are processed in order and added to previous coreference clusters. Our method is distinguished by the use of two functions to make each coreference decision: a pruning function that prunes bad coreference decisions from further consideration, and a scoring function that then selects the best among the remaining decisions. Our framework reduces learning of these functions to rank learning, which helps leverage powerful off-the-shelf rank-learners. We show that our Prune-and-Score approach is superior to using a single scoring function to make both decisions and outperforms several state-of-the-art approaches on multiple benchmark corpora including OntoNotes.",
        "id": 13532387
      },
      {
        "title": "Analyzing Curriculum Learning for Sentiment Analysis along Task Difficulty, Pacing and Visualization Axes",
        "text": "While Curriculum Learning (CL) has recently gained traction in Natural language Processing Tasks, it is still not adequately analyzed. Previous works only show their effectiveness but fail short to explain and interpret the internal workings fully. In this paper, we analyze curriculum learning in sentiment analysis along multiple axes. Some of these axes have been proposed by earlier works that need more in-depth study. Such analysis requires understanding where curriculum learning works and where it does not. Our axes of analysis include Task difficulty on CL, comparing CL pacing techniques, and qualitative analysis by visualizing the movement of attention scores in the model as curriculum phases progress. We find that curriculum learning works best for difficult tasks and may even lead to a decrement in performance for tasks with higher performance without curriculum learning. We see that One-Pass curriculum strategies suffer from catastrophic forgetting and attention movement visualization within curriculum pacing. This shows that curriculum learning breaks down the challenging main task into easier sub-tasks solved sequentially.",
        "id": 231979380
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which vision-language model paper in 2023 developed techniques that reduce input tokens to improve model inference speed?",
    "positive_ctxs": [
      {
        "title": "PuMer: Pruning and Merging Tokens for Efficient Vision Language Models",
        "text": "Large-scale vision language (VL) models use Transformers to perform cross-modal interactions between the input text and image. These cross-modal interactions are computationally expensive and memory-intensive due to the quadratic complexity of processing the input image and text. We present PuMer 1 : a token reduction framework that uses text-informed Pruning and modality-aware Merging strategies to progressively reduce the tokens of input image and text, improving model inference speed and reducing memory footprint. PuMer learns to keep salient image tokens related to the input text and merges similar textual and visual tokens by adding lightweight token reducer modules at several cross-modal layers in the VL model. Training PuMer is mostly the same as finetuning the original VL model but faster. Our evaluation for two vision language models on four downstream VL tasks shows PuMer increases inference throughput by up to 2x and reduces memory footprint by over 50% while incurring less than a 1% accuracy drop. 2",
        "id": 258959382
      }
    ],
    "negative_ctxs": [
      {
        "title": "Protein Word Detection using Text Segmentation Techniques",
        "text": "Literature in Molecular Biology is abundant with linguistic metaphors. There have been works in the past that attempt to draw parallels between linguistics and biology, driven by the fundamental premise that proteins have a language of their own. Since word detection is crucial to the decipherment of any unknown language, we attempt to establish a problem mapping from natural language text to protein sequences at the level of words. Towards this end, we explore the use of an unsupervised text segmentation algorithm to the task of extracting \"biological words\" from protein sequences. In particular, we demonstrate the effectiveness of using domain knowledge to complement data driven approaches in the text segmentation task, as well as in its biological counterpart. We also propose a novel extrinsic evaluation measure for protein words through protein family classification.",
        "id": 388111
      },
      {
        "title": "A New Framework for Sign Language Recognition based on 3D Handshape Identification and Linguistic Modeling",
        "text": "Current approaches to sign recognition by computer generally have at least some of the following limitations: they rely on laboratory conditions for sign production, are limited to a small vocabulary, rely on 2D modeling (and therefore cannot deal with occlusions and off-plane rotations), and/or achieve limited success. Here we propose a new framework that (1) provides a new tracking method less dependent than others on laboratory conditions and able to deal with variations in background and skin regions (such as the face, forearms, or other hands); (2) allows for identification of 3D hand configurations that are linguistically important in American Sign Language (ASL); and (3) incorporates statistical information reflecting linguistic constraints in sign production. For purposes of large-scale computer-based sign language recognition from video, the ability to distinguish hand configurations accurately is critical. Our current method estimates the 3D hand configuration to distinguish among 77 hand configurations linguistically relevant for ASL. Constraining the problem in this way makes recognition of 3D hand configuration more tractable and provides the information specifically needed for sign recognition. Further improvements are obtained by incorporation of statistical information about linguistic dependencies among handshapes within a sign derived from an annotated corpus of almost 10,000 sign tokens.",
        "id": 9217596
      },
      {
        "title": "Document Classification in Structured Military Messages",
        "text": "We present new results for the DSTO project on document classification of military messages. We report more specifically on the improvements to the Part-Of-Speech (POS) tagging, a probabilistic process that assigns a tag to a token, and discuss the training for Date Time Groups POS tags. A new implementation of the rule-based classifier is described. The results obtained on two databases of real military messages are encouraging and the document classification module has now been integrated with a query user interface.",
        "id": 17994176
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Have there been any advancements in language models that operate without tokenization and emphasize encoding at the character level, and what benefits might they have compared to conventional methods using subword tokenization?",
    "positive_ctxs": [
      {
        "title": "CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation",
        "text": "Pipelined NLP systems have largely been superseded by end-to-end neural modeling, yet nearly all commonly used models still require an explicit tokenization step. While recent tokenization approaches based on data-derived subword lexicons are less brittle than manually engineered tokenizers, these techniques are not equally suited to all languages, and the use of any fixed vocabulary may limit a model's ability to adapt. In this paper, we present CANINE, a neural encoder that operates directly on character sequences-without explicit tokenization or vocabulary-and a pre-training strategy that operates either directly on characters or optionally uses subwords as a soft inductive bias. To use its finer-grained input effectively and efficiently, CANINE combines downsampling, which reduces the input sequence length, with a deep transformer stack, which encodes context. CANINE outperforms a comparable mBERT model by 5.7 F1 on TYDI QA, a challenging multilingual benchmark, despite having fewer model parameters.73",
        "id": 232185112
      }
    ],
    "negative_ctxs": [
      {
        "title": "The Language of Power and its Cultural Influence",
        "text": "In this paper, we investigate whether the social goals of an individual can be recognized through analysis of the social actions indicated by their use of language. Specifically, we focus on recognizing when someone is pursuing power within a web forum. Individuals pursue power in order to increase their control over the actions and goals of the group. We cast the problem as social conversational entailment where we determine if a dialogue entails a hypothesis which states a dialogue participant is in pursuit of power. In the social conversational entailment framework the hypothesis is decomposed into a series of social commitments which define series of actions and responses that are indicative of the hypothesis. The social commitments are modeled as social acts which are pragmatic speech acts. We identify nine culturally neutral psychologically-motivated social acts that can be detected in language and are indicative of whether an individual is pursuing power. Our best results using social conversational entailment achieve an overall F-measure of 79.7% for predicting pursuit of power for English speakers and 78.3% for Chinese speakers.",
        "id": 15818826
      },
      {
        "title": "CMUQ@QALB-2014: An SMT-based System for Automatic Arabic Error Correction",
        "text": "In this paper, we describe the CMUQ system we submitted to The ANLP-QALB 2014 Shared Task on Automatic Text Correction for Arabic. Our system combines rule-based linguistic techniques with statistical language modeling techniques and machine translationbased methods. Our system outperforms the baseline and reaches an F-score of 65.42% on the test set of QALB corpus. This ranks us 3rd in the competition.",
        "id": 17910054
      },
      {
        "title": "Multilingual Neural Machine Translation involving Indian Languages",
        "text": "Neural Machine Translations (NMT) models are capable of translating a single bilingual pair and require a new model for each new language pair. Multilingual Neural Machine Translation models are capable of translating multiple language pairs, even pairs which it hasn't seen before in training. Availability of parallel sentences is a known problem in machine translation. Multilingual NMT model leverages information from all the languages to improve itself and performs better. We propose a data augmentation technique that further improves this model profoundly. The technique helps achieve a jump of more than 15 points in BLEU score from the Multilingual NMT Model. A BLEU score of 36.2 was achieved for Sindhi-English translation, which is higher than any score on the leaderboard of the LoResMT SharedTask at MT Summit 2019, which provided the data for the experiments.",
        "id": 219307271
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Are there any papers on training video-language models with contrastive approahes and evaluation on temporal localization tasks?",
    "positive_ctxs": [
      {
        "title": "Contrastive Video-Language Learning with Fine-grained Frame Sampling",
        "text": "Despite recent progress in video and language representation learning, the weak or sparse correspondence between the two modalities remains a bottleneck in the area. Most videolanguage models are trained via pair-level loss to predict whether a pair of video and text is aligned. However, even in paired video-text segments, only a subset of the frames are semantically relevant to the corresponding text, with the remainder representing noise; where the ratio of noisy frames is higher for longer videos. We propose FineCo (Fine-grained Contrastive Loss for Frame Sampling), an approach to better learn video and language representations with a fine-grained contrastive objective operating on video frames. It helps distil a video by selecting the frames that are semantically equivalent to the text, improving cross-modal correspondence. Building on the well established VideoCLIP model as a starting point, FineCo achieves state-of-the-art performance on YouCookII, a text-video retrieval benchmark with long videos. FineCo also achieves competitive results on text-video retrieval (MSR-VTT), and video question answering datasets (MSR-VTT QA and MSR-VTT MC) with shorter videos.",
        "id": 252815403
      }
    ],
    "negative_ctxs": [
      {
        "title": "Cross-Lingual Classification of Topics in Political Texts",
        "text": "In this paper, we propose an approach for cross-lingual topical coding of sentences from electoral manifestos of political parties in different languages. To this end, we exploit continuous semantic text representations and induce a joint multilingual semantic vector spaces to enable supervised learning using manually-coded sentences across different languages. Our experimental results show that classifiers trained on multilingual data yield performance boosts over monolingual topic classification.",
        "id": 27628985
      },
      {
        "title": "Two Tools for Creating and Visualizing Sub-sentential Alignments of Parallel Text",
        "text": "We present two web-based, interactive tools for creating and visualizing sub-sentential alignments of parallel text. Yawat is a tool to support distributed, manual word-and phrase-alignment of parallel text through an intuitive, web-based interface. Kwipc is an interface for displaying words or bilingual word pairs in parallel, word-aligned context.A key element of the tools presented here is the interactive visualization: alignment information is shown only for one pair of aligned words or phrases at a time. This allows users to explore the alignment space interactively without being overwhelmed by the amount of information available.",
        "id": 8106720
      },
      {
        "title": "Identifying Emotion Labels from Psychiatric Social Texts Using Independent Component Analysis",
        "text": "Accessing the web has been an efficient and effective means to acquire self-help knowledge when suffering from depressive problems. Many mental health websites have developed community-based services such as web forums and blogs for Internet users to share their depressive problems with other users and health professionals. Other users or health professionals can then make recommendations in response to these problems. Such communications produce a large number of documents called psychiatric social texts containing rich emotion labels representing different depressive problems. Automatically identify such emotion labels can make online psychiatric services more effective. This study proposes a framework combining latent semantic analysis (LSA) and independent component analysis (ICA) to extract concept-level features for emotion label identification. LSA is used to discover latent concepts that do not frequently occur in psychiatric social texts, and ICA is used to extract independent components by minimizing the term dependence among the concepts. By combining LSA and ICA, more useful latent concepts can be discovered for different emotion labels, and the dependence between them can also be minimized. The discriminant power of classifiers can thus be improved by training them on the independent components with minimized term overlap. Experimental results show that the use of conceptlevel features yielded better performance than the use of word-level features. Additionally, combining LSA and ICA improved the performance of using each LSA and ICA alone. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/",
        "id": 14685831
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest a study that investigates the incorporation of human intervention in generating adversarial examples to attack conversational agents, with the aim of improving classifier efficacy",
    "positive_ctxs": [
      {
        "title": "Recipes for building an open-domain chatbot",
        "text": "Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent persona. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.",
        "id": 216562425
      }
    ],
    "negative_ctxs": [
      {
        "title": "Designing an Evaluation Framework for Spoken Term Detection and Spoken Document Retrieval at the NTCIR-9 SpokenDoc Task",
        "text": "We describe the evaluation framework for spoken document retrieval for the IR for the Spoken Documents Task, conducted in the ninth NTCIR Workshop. The two parts of this task were a spoken term detection (STD) subtask and an ad hoc spoken document retrieval subtask (SDR). Both subtasks target search terms, passages and documents included in academic and simulated lectures of the Corpus of Spontaneous Japanese. Seven teams participated in the STD subtask and five in the SDR subtask. The results obtained through the evaluation in the workshop are discussed.",
        "id": 65373
      },
      {
        "title": "Concept Disambiguation for Improved Subject Access Using Multiple Knowledge Sources",
        "text": "We address the problem of mining text for relevant image metadata. Our work is situated in the art and architecture domain, where highly specialized technical vocabulary presents challenges for NLP techniques. To extract high quality metadata, the problem of word sense disambiguation must be addressed in order to avoid leading the searcher to the wrong image as a result of ambiguous -and thus faulty -metadata. In this paper, we present a disambiguation algorithm that attempts to select the correct sense of nouns in textual descriptions of art objects, with respect to a rich domain-specific thesaurus, the Art and Architecture Thesaurus (AAT). We performed a series of intrinsic evaluations using a data set of 600 subject terms extracted from an online National Gallery of Art (NGA) collection of images and text. Our results showed that the use of external knowledge sources shows an improvement over a baseline.",
        "id": 15313711
      },
      {
        "title": "Synthesising Personality with Neural Speech Synthesis",
        "text": "Matching the personality of conversational001 agents to the personality of the user can signifi-002 cantly improve the user experience, with many 003 successful examples in text-based chatbots.It 004 is also important for a voice-based system to 005 be able to alter the personality of the speech 006 as perceived by the users.In this pilot study, 007 fifteen voices were rated using Big Five per-008 sonality traits.Five content-neutral sentences 009 were chosen for the listening tests.The audio 010 data, together with two rated traits (Extrover-011 sion and Agreeableness), were used to train a 012 neural speech synthesiser based on one male 013 and one female voices.The effect of altering 014 the personality trait features was evaluated by a 015 second listening test.Both perceived extrover-016 sion and agreeableness in the synthetic voices 017 were affected significantly.The controllable 018 range was limited due to a lack of variance in 019 the source audio data.The perceived person-020 ality traits correlated with each other and with 021 the naturalness of the speech.022 1 Introduction 023 The law of attraction in human-robot interaction 024 means users prefer social robots with similar per-025 sonality traits to themselves (Park et al., 2012).026 Previous work has shown that it is possible to de-027 sign a text-based chatbot with a pre-defined per-028 sonality (Ahmad et al., 2020; Ruane et al., 2021), 029 and matching the personality of the agent to the 030 personality of the user can significantly improve 031 the user experience (Smestad and Volden, 2019; 032 * This author is currently affiliated with Cambridge University Press & Assessment.Research was conducted while studying at The University of Edinburgh.† This author is currently affiliated with Sanas.ai.Research was conducted while working at CereProc Ltd. text, which can be matching or mismatching, and 2) 323 adaptive personality based on the personality of the 324 user, as such adaptation is shown possible in text-325 based chatbots (Fernau et al., 2022).A multi-turn 326 conversational set-up can be used to experiment 327 the consistency of synthetised personality.The per-328 ception and impact of synthesised personality in 329 different cultural context can also be explored in 330 various user studies.331 5 Acknowledgements 332",
        "id": 263609421
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Is there any paper improves adversarial training by forming semantic aware label without extra pre-train time or data?",
    "positive_ctxs": [
      {
        "title": "Annealing Self-Distillation Rectification Improves Adversarial Training",
        "text": "In standard adversarial training, models are optimized to fit one-hot labels within allowable adversarial perturbation budgets. However, the ignorance of underlying distribution shifts brought by perturbations causes the problem of robust overfitting. To address this issue and enhance adversarial robustness, we analyze the characteristics of robust models and identify that robust models tend to produce smoother and well-calibrated outputs. Based on the observation, we propose a simple yet effective method, Annealing Self-Distillation Rectification (ADR), which generates soft labels as a better guidance mechanism that accurately reflects the distribution shift under attack during adversarial training. By utilizing ADR, we can obtain rectified distributions that significantly improve model robustness without the need for pre-trained models or extensive extra computation. Moreover, our method facilitates seamless plug-and-play integration with other adversarial training techniques by replacing the hard labels in their objectives. We demonstrate the efficacy of ADR through extensive experiments and strong performances across datasets.Preprint. Under review.",
        "id": 258833682
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 219310229
      },
      {
        "title": "Socratic Question Generation: A Novel Dataset, Models, and Evaluation",
        "text": "Socratic questioning is a form of reflective inquiry often employed in education to encourage critical thinking in students, or to elicit awareness of beliefs and perspectives in a subject during therapeutic counseling. Specific types of Socratic questions are employed for enabling reasoning and alternate views against the context of individual personal opinions on a topic. Socratic contexts are different from traditional question generation contexts where \"answer-seeking\" questions are generated against a given formal passage on a topic, narrative stories or conversations.We present SocratiQ, the first large dataset of 110K (question, context) pairs for enabling studies on Socratic Question Generation (SoQG ). We provide an in-depth study on the various types of Socratic questions and present models for generating Socratic questions against a given context through prompt tuning. Our automated and human evaluation results demonstrate that our SoQG models can produce realistic, type-sensitive, human-like Socratic questions enabling potential applications in counseling and coaching.",
        "id": 258378353
      },
      {
        "title": "Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions",
        "text": "The context in which language is used provides a strong signal for learning to recover its meaning. In this paper, we show it can be used within a grounded CCG semantic parsing approach that learns a joint model of meaning and context for interpreting and executing natural language instructions, using various types of weak supervision. The joint nature provides crucial benefits by allowing situated cues, such as the set of visible objects, to directly influence learning. It also enables algorithms that learn while executing instructions, for example by trying to replicate human actions. Experiments on a benchmark navigational dataset demonstrate strong performance under differing forms of supervision, including correctly executing 60% more instruction sets relative to the previous state of the art.",
        "id": 9963298
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there any paper that automatically creates a dataset for summarizing text from one language to another for a large collection of languages?",
    "positive_ctxs": [
      {
        "title": "CrossSum: Beyond English-Centric Cross-Lingual Summarization for 1,500+ Language Pairs",
        "text": "We present CrossSum, a large-scale crosslingual summarization dataset comprising 1.68 million article-summary samples in 1,500+ language pairs. We create CrossSum by aligning parallel articles written in different languages via cross-lingual retrieval from a multilingual abstractive summarization dataset and perform a controlled human evaluation to validate its quality. We propose a multistage data sampling algorithm to effectively train a cross-lingual summarization model capable of summarizing an article in any target language. We also introduce LaSE, an embedding-based metric for automatically evaluating model-generated summaries. LaSE is strongly correlated with ROUGE and, unlike ROUGE, can be reliably measured even in the absence of references in the target language. Performance on ROUGE and LaSE indicate that our proposed model consistently outperforms baseline models. To the best of our knowledge, CrossSum is the largest cross-lingual summarization dataset and the first ever that is not centered around English. We are releasing the dataset, training and evaluation scripts, and models to spur future research on cross-lingual summarization. The resources can be found at https: //github.com/csebuetnlp/CrossSum.",
        "id": 258947845
      }
    ],
    "negative_ctxs": [
      {
        "title": "Docalog: Multi-document Dialogue System using Transformer-based Span Retrieval",
        "text": "Information-seeking dialogue systems, including knowledge identification and response generation, aim to respond to users with fluent, coherent, and informative answers based on users' needs. This paper discusses our proposed approach, Docalog, for the DialDoc-22 (Multi-Doc2Dial) shared task. Docalog identifies the most relevant knowledge in the associated document, in a multi-document setting. Docalog, is a three-stage pipeline consisting of (1) a document retriever model (DR. TEIT), (2) an answer span prediction model, and (3) an ultimate span picker deciding on the most likely answer span, out of all predicted spans. In the test phase of MultiDoc2Dial 2022, Docalog achieved f1scores of 36.07% and 28.44% and SacreBLEU scores of 23.70% and 20.52%, respectively on the MDD-SEEN and MDD-UNSEEN folds.",
        "id": 248780343
      },
      {
        "title": "A SYNCHRONIZATION STRUCTURE OF SSTC AND ITS APPLICATIONS IN MACHINE TRANSLATION",
        "text": "In this paper, a flexible annotation schema called (SSTC) is introduced. In order to describe the correspondence between different languages, we propose a variant of SSTC called synchronous SSTC (S-SSTC). We will also describe how S-SSTC provides the flexibility to treat some of the non-standard cases, which are problematic to other synchronous formalisms. The proposed S-SSTC schema is well suited to describe the correspondence between different languages, in particular, relating a language with its translation in another language (i.e. in  Machine Translation). Also it can be used as annotation for translation systems that automatically extract transfer mappings(rules or examples) from bilingual corpora. The S-SSTC is very well suited for the construction of a Bilingual Knowledge Bank (BKB), where the examples are kept in form of S-SSTCs.",
        "id": 9137980
      },
      {
        "title": "A Text-based Query Interface to OWL Ontologies",
        "text": "Accessing structured data in the form of ontologies requires training and learning formal query languages (e.g., SeRQL or SPARQL) which poses significant difficulties for non-expert users. One of the ways to lower the learning overhead and make ontology queries more straightforward is through a Natural Language Interface (NLI). While there are existing NLIs to structured data with reasonable performance, they tend to require expensive customisation to each new domain or ontology. Additionally, they often require specific adherence to a pre-defined syntax which, in turn, means that users still have to undergo training. In this paper we present Question-based Interface to Ontologies (QuestIO) -a tool for querying ontologies using unconstrained language-based queries. QuestIO has a very simple interface, requires no user training and can be easily embedded in any system or used with any ontology or knowledge base without prior customisation.",
        "id": 7260067
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Have any research papers critically analyzed the performance speed of non-autoregressive translation models compared to autoregressive models",
    "positive_ctxs": [
      {
        "title": "Non-Autoregressive Machine Translation: It's Not as Fast as it Seems",
        "text": "Efficient machine translation models are commercially important as they can increase inference speeds, and reduce costs and carbon emissions. Recently, there has been much interest in non-autoregressive (NAR) models, which promise faster translation. In parallel to the research on NAR models, there have been successful attempts to create optimized autoregressive models as part of the WMT shared task on efficient translation. In this paper, we point out flaws in the evaluation methodology present in the literature on NAR models and we provide a fair comparison between a stateof-the-art NAR model and the autoregressive submissions to the shared task. We make the case for consistent evaluation of NAR models, and also for the importance of comparing NAR models with other widely used methods for improving efficiency. We run experiments with a connectionist-temporal-classification-based (CTC) NAR model implemented in C++ and compare it with AR models using wall clock times. Our results show that, although NAR models are faster on GPUs, with small batch sizes, they are almost always slower under more realistic usage conditions. We call for more realistic and extensive evaluation of NAR models in future work.",
        "id": 248512647
      }
    ],
    "negative_ctxs": [
      {
        "title": "Can SMT and RBMT Improve each other's Performance?-An Experiment with English-Hindi Translation",
        "text": "Rule-based machine translation (RBMT) and Statistical machine translation (SMT) are two well-known approaches for translation which have their own benefits.System architecture of SMT often complements RBMT, and the vice-versa. In this paper, we propose an effective method of serial coupling where we attempt to build a hybrid model that exploits the benefits of both the architectures. The first part of coupling is used to obtain good lexical selection and robustness, second part is used to improve syntax and the final one is designed to combine other modules along with the best phrase reordering. Our experiments on a English-Hindi product domain dataset show the effectiveness of the proposed approach with improvement in BLEU score.",
        "id": 2910089
      },
      {
        "title": "Conversational Robots: Building Blocks for Grounding Word Meaning",
        "text": "How can we build robots that engage in fluid spoken conversations with people, moving beyond canned responses to words and towards actually understanding? As a step towards addressing this question, we introduce a robotic architecture that provides a basis for grounding word meanings. The architecture provides perceptual, procedural, and affordance representations for grounding words. A perceptuallycoupled on-line simulator enables sensorymotor representations that can shift points of view. Held together, we show that this architecture provides a rich set of data structures and procedures that provide the foundations for grounding the meaning of certain classes of words. 1  We acknowledge that the words in this example, like most words, have numerous additional connotations that are not captured by the representations that we have suggested. For example, words such as touch, heavy and blue can be used metaphorically to refer to emotional actions and states. Things are not always physical perceivable objects, my usually indicates possession, and so forth. Barwise and Perry use the phrase \"efficiency of language\" to highlight the situation-dependent reusability of words and utterances(Barwise and Perry, 1983). However, for",
        "id": 358056
      },
      {
        "title": "Représentation vectorielle de paires de verbes pour la prédiction de relations lexicales",
        "text": "Dans cet article, nous proposons un modèle de représentations vectorielles de paire de mots, obtenues à partir d'une adaptation du modèle Skip-gram de Word2vec. Ce modèle est utilisé pour générer des vecteurs de paires de verbes, entraînées sur le corpus de textes anglais Ukwac. Les vecteurs sont évalués sur les données ConceptNet & EACL, sur une tâche de classification de relations lexicales. Nous comparons les résultats obtenus avec les vecteurs paires à des modèles utilisant des vecteurs mots, et testons l'évaluation avec des verbes dans leur forme originale et dans leur forme lemmatisée. Enfin, nous présentons des expériences où ces vecteurs paires sont utilisés sur une tâche d'identification de relation discursive entre deux segments de texte. Nos résultats sur le corpus anglais Penn Discourse Treebank, démontrent l'importance de l'information verbale pour la tâche, et la complémentarité de ces vecteurs paires avec les connecteurs discursifs des relations.ABSTRACTVerb-pairs embeddings for discourse relation predictionThis paper proposes a model to obtain vector representations of pairs of words, obtained from an adaptation of the Word2vec Skip-gram Model. This model is used to generate embeddings for pairs of verbs, trained on the english corpus Ukwac. The pair-embeddings are then evaluated on a classification task, where the goal is to predict the lexical relation between the input pair of words. The scores obtained on this task with the pair-embeddings are compared with the scores obtained with individual word-embeddings and with pairs of lemmatized verbs. Finally, the pair-embeddings are used on the discourse relation prediction task, on the Penn Discourse Treebank dataset, revealing the relevance of verbs for this task, and the complementarity between the verbs and the discourse connective.MOTS-CLÉS : Vecteur mot, vecteur relation de verbes, analyse de texte, prédiction de relation du discours.KEYWORDS: Word embedding, relation embedding between verbs, text analysis, discourse relation prediction.Actes de la 6e conférence conjointe Journées d'Études sur la Parole (JEP, 33e édition), Traitement Automatique des Langues Naturelles (TALN, 27e édition), Rencontre des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (RÉCITAL, 22e édition) Nancy, France, 08-19 juin 2020 Volume 3 : Rencontre des Étudiants Chercheurs en Informatique pour le TAL, pages 179-192. hal : hal-02786199. Cette oeuvre est mise à disposition sous licence Attribution 4.0 International. objectif de construire des représentations de relations entre une paire de mots, et d'évaluer la qualité de ces représentations dans différentes tâches. Pour ce faire, nous présentons un modèle permettant d'entraîner des représentations vectorielles de relations entre des paires de mots.Les représentations vectorielles de mots sont très utilisées en traitement automatique des langues depuis les années 1990, l'idée étant que les mots sémantiquement proches se retrouvent proches dans l'espace vectoriel des mots. On retrouve souvent dans la littérature le terme word embedding ou \"plongement lexical\" pour qualifier ces représentations vectorielles. Ces représentations sont fondées sur l'hypothèse distributionnelle(Harris, 1954): \"les lexèmes possédant un contexte linguistique similaire ont un sens similaire.\". On peut par exemple obtenir des représentations vectorielles de mots en construisant une matrice de co-occurrences, en comptant le nombre d'occurrences de mots dans le contexte d'autres mots(Turney & Pantel, 2010). L'inconveniant de cette méthode étant néanmmoins la taille importante des vecteurs obtenus, et leurs composantes qui sont majoritairement nulles.",
        "id": 221373817
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper was the first to propose combining human spoken language and sign language datasets with gloss annotations to enhance the performance of sign language translation?",
    "positive_ctxs": [
      {
        "title": "Neural Machine Translation Methods for Translating Text to Sign Language Glosses",
        "text": "State-of-the-art techniques common to low resource Machine Translation (MT) are applied to improve MT of spoken language text to Sign Language (SL) glosses. In our experiments, we improve the performance of the transformer-based models via (1) data augmentation, (2) semi-supervised Neural Machine Translation (NMT), (3) transfer learning and (4) multilingual NMT. The proposed methods are implemented progressively on two German SL corpora containing gloss annotations. Multilingual NMT combined with data augmentation appear to be the most successful setting, yielding statistically significant improvements as measured by three automatic metrics (up to over 6 points BLEU), and confirmed via human evaluation. Our best setting outperforms all previous work that report on the same test-set and is also confirmed on a corpus of the American Sign Language (ASL).",
        "id": 259370577
      }
    ],
    "negative_ctxs": [
      {
        "title": "Unsupervised Semantic Role Induction with Graph Partitioning",
        "text": "In this paper we present a method for unsupervised semantic role induction which we formalize as a graph partitioning problem. Argument instances of a verb are represented as vertices in a graph whose edge weights quantify their role-semantic similarity. Graph partitioning is realized with an algorithm that iteratively assigns vertices to clusters based on the cluster assignments of neighboring vertices. Our method is algorithmically and conceptually simple, especially with respect to how problem-specific knowledge is incorporated into the model. Experimental results on the CoNLL 2008 benchmark dataset demonstrate that our model is competitive with other unsupervised approaches in terms of F1 whilst attaining significantly higher cluster purity.",
        "id": 14436537
      },
      {
        "title": "Formalism for a language agnostic language learning game and productive grid generation",
        "text": "In this article, we describe the modifications of MagicWord, a language learning game focused on accuracy, in order to allow the integration of new languages. We first describe the motivations behind the design of the game. Then we explain the modifications performed before exploring the consequences both game-wise and language learning-wise.In order to improve their replay-value, language learning games need to rely on language resources of diverse complexity depending on their rules and objectives. In this paper, we tackle the issue of providing multi-language resources for a language learning letter game, MagicWord. Before exploring the technical difficulties as well as their intricacies both in terms of language representation, learning and gaming, we will explain the game, its objectives and the design process.",
        "id": 203657273
      },
      {
        "title": "Published as a conference paper at ICLR 2020 COHERENT GRADIENTS: AN APPROACH TO UNDERSTANDING GENERALIZATION IN GRADIENT DESCENT-BASED OPTIMIZATION",
        "text": "An open question in the Deep Learning community is why neural networks trained with Gradient Descent generalize well on real datasets even though they are capable of fitting random data. We propose an approach to answering this question based on a hypothesis about the dynamics of gradient descent that we call Coherent Gradients: Gradients from similar examples are similar and so the overall gradient is stronger in certain directions where these reinforce each other. Thus changes to the network parameters during training are biased towards those that (locally) simultaneously benefit many examples when such similarity exists. We support this hypothesis with heuristic arguments and perturbative experiments and outline how this can explain several common empirical observations about Deep Learning. Furthermore, our analysis is not just descriptive, but prescriptive. It suggests a natural modification to gradient descent that can greatly reduce overfitting.",
        "id": 211296676
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "What paper first proposed a robust perceptual similarity metric with certificates?",
    "positive_ctxs": [
      {
        "title": "LIPSIM: A PROVABLY ROBUST PERCEPTUAL SIMILARITY METRIC",
        "text": "Recent years have seen growing interest in developing and applying perceptual similarity metrics.Research has shown the superiority of perceptual metrics over pixel-wise metrics in aligning with human perception and serving as a proxy for the human visual system.On the other hand, as perceptual metrics rely on neural networks, there is a growing concern regarding their resilience, given the established vulnerability of neural networks to adversarial attacks.It is indeed logical to infer that perceptual metrics may inherit both the strengths and shortcomings of neural networks.In this work, we demonstrate the vulnerability of state-of-the-art perceptual similarity metrics based on an ensemble of ViT-based feature extractors to adversarial attacks.We then propose a framework to train a robust perceptual similarity metric called LipSim (Lipschitz Similarity Metric) with provable guarantees.By leveraging 1-Lipschitz neural networks as the backbone, LipSim provides guarded areas around each data point and certificates for all perturbations within an ℓ 2 ball.Finally, a comprehensive set of experiments shows the performance of LipSim in terms of natural and certified scores and on the image retrieval application.The code is available at https://github.com/SaraGhazanfari/LipSim.",
        "id": 264555382
      }
    ],
    "negative_ctxs": [
      {
        "title": "Automatic Assessment of Student Translations for Foreign Language Tutoring",
        "text": "This paper introduces the use of speech translation technology for a new type of voice-interactive Computer Aided Language Learning (CALL) application. We describe a computer game we have developed, in which the system presents sentences in a student's native language to elicit spoken translations in the target new language. A critical technology is an algorithm to automatically verify the appropriateness of the student's translation using linguistic analysis. Evaluation results are presented on the system's ability to match human judgment of the correctness of a student's translation, for a set of 1115 utterances collected from 9 learners of Mandarin Chinese translating flight domain sentences. We also demonstrate the effective use of context information to improve both recognition performance on non-native speech as well as the system's accuracy in judging the translation quality.",
        "id": 3889577
      },
      {
        "title": "Shared Task Papers",
        "text": "We explore a large number of features for cross-lingual pronoun prediction for translation between English and German/French. We find that features related to German/French are more informative than features related to English, regardless of the translation direction. Our most useful features are local context, dependency head features, and source pronouns. We also find that it is sometimes more successful to employ a 2-step procedure that first makes a binary choice between pronouns and other, then classifies pronouns. For the pronoun/other distinction POS ngrams were very useful.",
        "id": 5971846
      },
      {
        "title": "Typical Cases of Annotators' Disagreement in Discourse Annotations in Prague Dependency Treebank",
        "text": "In this paper, we present the first results of the parallel Czech discourse annotation in the Prague Dependency Treebank 2.0. Having established an annotation scenario for capturing semantic relations crossing the sentence boundary in a discourse, and having annotated the first sections of the treebank according to these guidelines, we report now on the results of the first evaluation of these manual annotations. We give an overview of the process of the annotation itself, which we believe is to a large degree language-independent and therefore accessible to any discourse researcher. Next, we describe the inter-annotator agreement measurement, and, most importantly, we classify and analyze the most common types of annotators' disagreement and propose solutions for the next phase of the annotation. The annotation is carried out on dependency trees (on the tectogrammatical layer), this approach is quite novel and it brings us some advantages when interpreting the syntactic structure of the discourse units.",
        "id": 17255390
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there any paper that applies symbolic distillation on black-box generalist language models to harvest high-quality counterfactual data for out-of-distribution generalization?",
    "positive_ctxs": [
      {
        "title": "DISCO: Distilling Counterfactuals with Large Language Models",
        "text": "Models trained with counterfactually augmented data learn representations of the causal structure of tasks, enabling robust generalization. However, high-quality counterfactual data is scarce for most tasks and not easily generated at scale. When crowdsourced, such data is typically limited in scale and diversity; when generated using supervised methods, it is computationally expensive to extend to new counterfactual dimensions. In this work, we introduce DISCO (DIStilled COunterfactual Data), a new method for automatically generating highquality counterfactual data at scale. DISCO engineers prompts to generate phrasal perturbations with a large general language model. Then, a task-specific teacher model filters these generations to distill high-quality counterfactual data. While task-agnostic, we apply our pipeline to the task of natural language inference (NLI) and find that on challenging evaluations such as the NLI stress test, comparatively smaller student models trained with DISCOgenerated counterfactuals are more robust (6% absolute) and generalize better across distributions (2%) compared to models trained without data augmentation. Furthermore, DISCOaugmented models are 10% more consistent between counterfactual pairs on three evaluation sets, demonstrating that DISCO augmentation enables models to more reliably learn causal representations. Our repository are available at: https://github.com/eric11eca/disco * Work done while at the Allen Institute for AI. Equal contribution Span Extraction Counterfactual Data Easy Ambigious Hard Counterfactual Over-generation Data Selection Seed Dataset Original: A young girl looks up as she rides on a merry-go-round. Spans: A young girl looks up as she rides on a merry-go-round. Counterfactual: A young girl looks up at a very tall roller coaster with an eager and excited look on her face. General LLM (GPT-3) Specialized Teacher Model Filtering Student Model Data Augmentation",
        "id": 254877039
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 226283810
      },
      {
        "title": "Extraction d'information de sous-catégorisation à partir des tables du LADL 1",
        "text": "Les tables du LADL (Laboratoire d'Automatique Documentaire et Linguistique) contiennent des données électroniques extensives sur les propriétés morphosyntaxiques et syntaxiques des foncteurs syntaxiques du français (verbes, noms, adjectifs). Ces données, dont on sait qu'elles sont nécessaires pour le bon fonctionnement des systèmes de traitement automatique des langues, ne sont cependant que peu utilisées par les systèmes actuels. Dans cet article, nous identifions les raisons de cette lacune et nous proposons une méthode de conversion des tables vers un format mieux approprié au traitement automatique des langues.AbstractMaurice Gross' grammar lexicon contains rich and exhaustive information about the morphosyntactic and syntactic properties of French syntactic functors (verbs, adjectives, nouns). Yet its use within natural language processing systems is hampered both by its non standard encoding and by a structure that is partly implicit and partly underspecified. In this paper, we present a method for translating this information into a format more amenable for use by NLP systems, we discuss the results obtained so far, we compare our approach with related work and we identify the possible further uses that can be made of the reformatted information.",
        "id": 61992109
      },
      {
        "title": "Improving reordering performance using higher order and structural features",
        "text": "Recent work has shown that word aligned data can be used to learn a model for reordering source sentences to match the target order. This model learns the cost of putting a word immediately before another word and finds the best reordering by solving an instance of the Traveling Salesman Problem (TSP). However, for efficiently solving the TSP, the model is restricted to pairwise features which examine only a pair of words and their neighborhood. In this work, we go beyond these pairwise features and learn a model to rerank the n-best reorderings produced by the TSP model using higher order and structural features which help in capturing longer range dependencies. In addition to using a more informative set of source side features, we also capture target side features indirectly by using the translation score assigned to a reordering. Our experiments, involving Urdu-English, show that the proposed approach outperforms a state-of-theart PBSMT system which uses the TSP model for reordering by 1.3 BLEU points, and a publicly available state-of-the-art MT system, Hiero, by 3 BLEU points.",
        "id": 313465
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Has any research tried to mitigate overfitting in weakly-supervised settings by introducing an adversarial framework where the influence of the labeling function is a hyperparameter for the feature representation?",
    "positive_ctxs": [
      {
        "title": "KnowMAN: Weakly Supervised Multinomial Adversarial Networks",
        "text": "The absence of labeled data for training neural models is often addressed by leveraging knowledge about the specific task, resulting in heuristic but noisy labels. The knowledge is captured in labeling functions, which detect certain regularities or patterns in the training samples and annotate corresponding labels for training. This process of weakly supervised training may result in an over-reliance on the signals captured by the labeling functions and hinder models to exploit other signals or to generalize well. We propose KnowMAN, an adversarial scheme that enables to control influence of signals associated with specific labeling functions. KnowMAN forces the network to learn representations that are invariant to those signals and to pick up other signals that are more generally associated with an output label. KnowMAN strongly improves results compared to direct weakly supervised learning with a pre-trained transformer language model and a feature-based baseline.",
        "id": 237532398
      }
    ],
    "negative_ctxs": [
      {
        "title": "Using Structured Content Plans for Fine-grained Syntactic Control in Pretrained Language Model Generation",
        "text": "Large pretrained language models offer powerful generation capabilities, but cannot be reliably controlled at a sub-sentential level. We propose to make such fine-grained control possible in pretrained LMs by generating text directly from a semantic representation, Abstract Meaning Representation (AMR), which is augmented at the node level with syntactic control tags. We experiment with English-language generation of three modes of syntax relevant to the framing of a sentence -verb voice, verb tense, and realization of human entities -and demonstrate that they can be reliably controlled, even in settings that diverge drastically from the training distribution. These syntactic aspects contribute to how information is framed in text, something that is important for applications such as summarization which aim to highlight salient information.",
        "id": 252586876
      },
      {
        "title": "Measure Country-Level Socio-Economic Indicators with Streaming News: An Empirical Study",
        "text": "Socio-economic conditions are difficult to measure. For example, the U.S. Bureau of Labor Statistics needs to conduct large-scale household surveys regularly to track the unemployment rate, an indicator widely used by economists and policy makers. We argue that events reported in streaming news can be used as \"micro-sensors\" for measuring socio-economic conditions. Similar to collecting surveys and then counting answers, it is possible to measure a socio-economic indicator by counting related events. In this paper, we propose Event-Centric Indicator Measure (ECIM), a novel approach to measure socioeconomic indicators with events. We empirically demonstrate strong correlations between ECIM values to several representative indicators in socio-economic research.",
        "id": 202786163
      },
      {
        "title": "The Ubiqus English-Inuktitut System for WMT20",
        "text": "This paper describes Ubiqus' submission to the WMT20 English-Inuktitut shared news translation task. Our main system, and only submission, is based on a multilingual approach, jointly training a Transformer model on several agglutinative languages.The English-Inuktitut translation task is challenging at every step, from data selection, preparation and tokenization to quality evaluation down the line. Difficulties emerge both because of the peculiarities of the Inuktitut language as well as the low-resource context.",
        "id": 227013461
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "I'm interested in understanding how perplexity is utilized in identifying misinformation or fact-checking. Are there studies discussing this application of perplexity?",
    "positive_ctxs": [
      {
        "title": "Towards Few-Shot Fact-Checking via Perplexity",
        "text": "Few-shot learning has drawn researchers' attention to overcome the problem of data scarcity. Recently, large pre-trained language models have shown great performance in fewshot learning for various downstream tasks, such as question answering and machine translation. Nevertheless, little exploration has been made to achieve few-shot learning for the fact-checking task. However, fact-checking is an important problem, especially when the amount of information online is growing exponentially every day. In this paper, we propose a new way of utilizing the powerful transfer learning ability of a language model via a perplexity score. The most notable strength of our methodology lies in its capability in fewshot learning. With only two training samples, our methodology can already outperform the Major Class baseline by more than an absolute 10% on the F1-Macro metric across multiple datasets. Through experiments, we empirically verify the plausibility of the rather surprising usage of the perplexity score in the context of fact-checking and highlight the strength of our few-shot methodology by comparing it to strong fine-tuning-based baseline models. Moreover, we construct and publicly release two new fact-checking datasets related to",
        "id": 232258000
      }
    ],
    "negative_ctxs": [
      {
        "title": "Dimensions of Abusive Language on Twitter",
        "text": "In this paper, we use a new categorical form of multidimensional register analysis to identify the main dimensions of functional linguistic variation in a corpus of abusive language, consisting of racist and sexist Tweets. By analysing the use of a wide variety of parts-ofspeech and grammatical constructions, as well as various features related to Twitter and computer-mediated communication, we discover three dimensions of linguistic variation in this corpus, which we interpret as being related to the degree of interactive, antagonistic and attitudinal language exhibited by individual Tweets. We then demonstrate that there is a significant functional difference between racist and sexist Tweets, with sexists Tweets tending to be more interactive and attitudinal than racist Tweets.",
        "id": 39055707
      },
      {
        "title": "HCS at SemEval-2017 Task 5: Sentiment Detection in Business News Using Convolutional Neural Networks",
        "text": "Task 5 of SemEval-2017 involves finegrained sentiment analysis on financial microblogs and news. Our solution for determining the sentiment score extends an earlier convolutional neural network for sentiment analysis in several ways. We explicitly encode a focus on a particular company, we apply a data augmentation scheme, and use a larger data collection to complement the small training data provided by the task organizers. The best results were achieved by training a model on an external dataset and then tuning it using the provided training dataset.",
        "id": 26958313
      },
      {
        "title": "Linear Context-Free Rewriting Systems and Deterministic Tree-Walking Transducers*",
        "text": "We show that the class of string languages generated by linear context-free rewriting systems is equal to the class of output languages of deterministic treewalking transducers. From equivalences that have previously been established we know that this class of languages is also equal to the string languages generated by context-free hypergraph grammars, multicomponent tree-adjoining grammars, and multiple contextfree grammars and to the class of yields of images of the regular tree languages under finite-copying topdown tree transducers.",
        "id": 18592508
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Are there any research papers investigating the improvement of radiology report summarization through the application of graph neural networks in conjunction with biomedical entity extraction?",
    "positive_ctxs": [
      {
        "title": "Word Graph Guided Summarization for Radiology Findings",
        "text": "Radiology reports play a critical role in communicating medical findings to physicians. In each report, the impression section summarizes essential radiology findings. In clinical practice, writing impression is highly demanded yet time-consuming and prone to errors for radiologists. Therefore, automatic impression generation has emerged as an attractive research direction to facilitate such clinical practice. Existing studies mainly focused on introducing salient word information to the general text summarization framework to guide the selection of the key content in radiology findings. However, for this task, a model needs not only capture the important words in findings but also accurately describe their relations so as to generate highquality impressions. In this paper, we propose a novel method for automatic impression generation, where a word graph is constructed from the findings to record the critical words and their relations, then a Word Graph guided Summarization model (WGSUM) is designed to generate impressions with the help of the word graph. Experimental results on two datasets, OPENI and MIMIC-CXR, confirm the validity and effectiveness of our proposed approach, where the state-of-the-art results are achieved on both datasets. Further experiments are also conducted to analyze the impact of different graph designs to the performance of our method. 1",
        "id": 236477655
      }
    ],
    "negative_ctxs": [
      {
        "title": "Knowledge Portals",
        "text": "Knowledge portals provide views onto domainspecific information on the World Wide Web, thus facilitating their users to find relevant, domainspecific information. The construction of intelligent access and the provisioning of information to knowledge portals, however, remained an ad hoc task requiring extensive manual editing and maintenance by the knowledge portal providers. In order to diminish these efforts we use ontologies as a conceptual backbone for providing, accessing and structuring information in a comprehensive approach for building and maintaining knowledge portals. We have built several experimental and one commercial knowledge portal for knowledge management tasks such as skill management and corporate history analysis that show how our approach is used in practice. This practice, however, has exhibited a number bottlenecks, many of which could be avoided or at least diminished by Human Language Technology. We have used HLT in order to reduce the costs of ontology engineering and in order to narrow the gap between finding knowledge in texts and providing it to the portal.",
        "id": 35749924
      },
      {
        "title": "Unsupervised Multi-Domain Adaptation with Feature Embeddings",
        "text": "Representation learning is the dominant technique for unsupervised domain adaptation, but existing approaches have two major weaknesses. First, they often require the specification of \"pivot features\" that generalize across domains, which are selected by taskspecific heuristics. We show that a novel but simple feature embedding approach provides better performance, by exploiting the feature template structure common in NLP problems.",
        "id": 399358
      },
      {
        "title": "Navigating through Dense Annotation Spaces",
        "text": "Pattern matching, or querying, over annotations is a general purpose paradigm for inspecting, navigating, mining, and transforming annotation repositories-the common representation basis for modern pipelined text processing frameworks. Configurability of such frameworks and expressiveness of feature structure-based annotation schemes account for the 'high density' of some such annotation repositories. This particular characteristic makes challenging the design of a pattern matching engine, capable of interpreting (or imposing) flat patterns over an arbitrarily dense annotation lattice. We present an approach where a finite state device carries out the application of (compiled) grammars over what is, in effect, a linearized 'projection' of a unique route through the lattice; a route derived by a mix of static pattern (grammar) analysis and interpretation of navigational directives within the extended grammar formalism. Our approach achieves a mix of finite state scanning and lattice traversal for expressive and efficient pattern matching in dense annotations stores.",
        "id": 15001324
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Can you recommend a conversational QA dataset where the human questioner does not have access to the evidence passage to simulate a more real-world information-seeking environment?",
    "positive_ctxs": [
      {
        "title": "QuAC : Question Answering in Context",
        "text": "We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-ofthe-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is significant room for future work on this data. Dataset, baseline, and leaderboard available at http://quac.ai.",
        "id": 52057510
      }
    ],
    "negative_ctxs": [
      {
        "title": "Data-driven Model Generalizability in Crosslinguistic Low-resource Morphological Segmentation",
        "text": "Common designs of model evaluation typically focus on monolingual settings, where different models are compared according to their performance on a single data set that is assumed to be representative of all possible data for the task at hand. While this may be reasonable for a large data set, this assumption is difficult to maintain in low-resource scenarios, where artifacts of the data collection can yield data sets that are outliers, potentially making conclusions about model performance coincidental. To address these concerns, we investigate model generalizability in crosslinguistic low-resource scenarios. Using morphological segmentation as the test case, we compare three broad classes of models with different parameterizations, taking data from 11 languages across 6 language families. In each experimental setting, we evaluate all models on a first data set, then examine their performance consistency when introducing new randomly sampled data sets with the same size and when applying the trained models to unseen test sets of varying sizes. The results demonstrate that the extent of model generalization depends on the characteristics of the data set, and does not necessarily rely heavily on the data set size. Among the characteristics that we studied, the ratio of morpheme overlap and that of the average number of morphemes per word between the training and test sets are the two most prominent factors. Our findings suggest that future work should adopt random sampling to construct data sets with different sizes in order to make more responsible claims about model evaluation.393",
        "id": 245769658
      },
      {
        "title": "IIT Gandhinagar at SemEval-2020 Task 9: Code-Mixed Sentiment Classification Using Candidate Sentence Generation and Selection",
        "text": "Code-mixing is the phenomenon of using multiple languages in the same utterance of a text or speech. It is a frequently used pattern of communication on various platforms such as social media sites, online gaming, product reviews, etc. Sentiment analysis of the monolingual text is a well-studied task. Code-mixing adds to the challenge of analyzing the sentiment of the text due to the non-standard writing style. We present a candidate sentence generation and selection based approach on top of the Bi-LSTM based neural classifier to classify the Hinglish code-mixed text into one of the three sentiment classes positive, negative, or neutral. The proposed approach shows an improvement in the system performance as compared to the Bi-LSTM based neural classifier. The results present an opportunity to understand various other nuances of code-mixing in the textual data, such as humor-detection, intent classification, etc.",
        "id": 220055815
      },
      {
        "title": "Zero Alignment of Verb Arguments in a Parallel Treebank",
        "text": "This paper analyses several points of interlingual dependency mismatch on the material of a parallel Czech-English dependency treebank. Particularly, the points of alignment mismatch between the valency frame arguments of the corresponding verbs are observed and described. The attention is drawn to the question whether such mismatches stem from the inherent semantic properties of the individual languages, or from the character of the used linguistic theory. Comments are made on the possible shifts in meaning. The authors use the findings to make predictions about possible machine translation implementation of the data.",
        "id": 13998481
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there a paper that uses Explainable AI techniques to investigate how language models represent the expression of morality?",
    "positive_ctxs": [
      {
        "title": "What does a Text Classifier Learn about Morality? An Explainable Method for Cross-Domain Comparison of Moral Rhetoric",
        "text": "Moral rhetoric influences our judgement. Although social scientists recognize moral expression as domain specific, there are no systematic methods for analyzing whether a text classifier learns the domain-specific expression of moral language or not. We propose Tomea, a method to compare a supervised classifier's representation of moral rhetoric across domains. Tomea enables quantitative and qualitative comparisons of moral rhetoric via an interpretable exploration of similarities and differences across moral concepts and domains. We apply Tomea on moral narratives in thirtyfive thousand tweets from seven domains. We extensively evaluate the method via a crowd study, a series of cross-domain moral classification comparisons, and a qualitative analysis of cross-domain moral expression.",
        "id": 259370854
      }
    ],
    "negative_ctxs": [
      {
        "title": "English Recipe Flow Graph Corpus",
        "text": "We present an annotated corpus of English cooking recipe procedures, and describe and evaluate computational methods for learning these annotations. The corpus consists of 300 recipes written by members of the public, which we have annotated with domain-specific linguistic and semantic structure. Each recipe is annotated with (1) 'recipe named entities' (r-NEs) specific to the recipe domain, and (2) a flow graph representing in detail the sequencing of steps, and interactions between cooking tools, food ingredients and the products of intermediate steps. For these two kinds of annotations, inter-annotator agreement ranges from 82.3 to 90.5 F1, indicating that our annotation scheme is appropriate and consistent. We experiment with producing these annotations automatically. For r-NE tagging we train a deep neural network NER tool; to compute flow graphs we train a dependency-style parsing procedure which we apply to the entire sequence of r-NEs in a recipe. In evaluations, our systems achieve 71.1 to 87.5 F1, demonstrating that our annotation scheme is learnable.",
        "id": 218977407
      },
      {
        "title": "A Logistic Regression Model of Determiner Omission in PPs Tibor Kiss Katja Keßelmeier Antje Müller",
        "text": "The realization of singular count nouns without an accompanying determiner inside a PP (determinerless PP, bare PP, Preposition-Noun Combination) has recently attracted some interest in computational linguistics. Yet, the relevant factors for determiner omission remain unclear, and conditions for determiner omission vary from language to language. We present a logistic regression model of determiner omission in German based on data obtained by applying annotation mining to a large, automatically and manually annotated corpus.",
        "id": 16669196
      },
      {
        "title": "SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving Out-of-Domain Robustness",
        "text": "Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples. Data augmentation is a common method used to prevent overfitting and improve OOD generalization. However, in natural language, it is difficult to generate new examples that stay on the underlying data manifold. We introduce SSMBA, a data augmentation method for generating synthetic training examples by using a pair of corruption and reconstruction functions to move randomly on a data manifold. We investigate the use of SSMBA in the natural language domain, leveraging the manifold assumption to reconstruct corrupted text with masked language models. In experiments on robustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently outperforms existing data augmentation methods and baseline models on both in-domain and OOD data, achieving gains of 0.8% accuracy on OOD Amazon reviews, 1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English. 1 . 2018. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805.",
        "id": 221836078
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that trains language models specifically on mental health-related social media data and the model is helpful for identifying mental health issues?",
    "positive_ctxs": [
      {
        "title": "MentalBERT: Publicly Available Pretrained Language Models for Mental Healthcare",
        "text": "Mental health is a critical issue in modern society, and mental disorders could sometimes turn to suicidal ideation without adequate treatment. Early detection of mental disorders and suicidal ideation from social content provides a potential way for effective social intervention. Recent advances in pretrained contextualized language representations have promoted the development of several domain-specific pretrained models and facilitated several downstream applications. However, there are no existing pretrained language models for mental healthcare. This paper trains and releases two pretrained masked language models, i.e., MentalBERT and MentalRoBERTa, to benefit machine learning for the mental healthcare research community. Besides, we evaluate our trained domain-specific models and several variants of pretrained language models on several mental disorder detection benchmarks and demonstrate that language representations pretrained in the target domain improve the performance of mental health detection tasks.",
        "id": 240288892
      }
    ],
    "negative_ctxs": [
      {
        "title": "Constructing a Model of Dialog",
        "text": "In the present paper communicative cycle",
        "id": 12405045
      },
      {
        "title": "Designing a Symbolic Intermediate Representation for Neural Surface Realization",
        "text": "Generated output from neural NLG systems often contain errors such as hallucination, repetition or contradiction. This work focuses on designing a symbolic intermediate representation to be used in multi-stage neural generation with the intention of reducing the frequency of failed outputs. We show that surface realization from this intermediate representation is of high quality and when the full system is applied to the E2E dataset it outperforms the winner of the E2E challenge. Furthermore, by breaking out the surface realization step from typically end-to-end neural systems, we also provide a framework for non-neural content selection and planning systems to potentially take advantage of semi-supervised pretraining of neural surface realization models.",
        "id": 166228140
      },
      {
        "title": "",
        "text": "",
        "id": 46941014
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that explores a pre-trained encoder-decoder architecture aimed at comprehending and generating code, potentially beneficial for enhancing automated code repair systems?",
    "positive_ctxs": [
      {
        "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation",
        "text": "Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https: //github.com/salesforce/CodeT5.",
        "id": 237386541
      }
    ],
    "negative_ctxs": [
      {
        "title": "Story Assembly in the R 2 aft Dyslexia Fluency Tutor",
        "text": "To overcome their substantial barriers to fluent reading, students with dyslexia need to be enticed to read more, and to read texts with carefully controlled lexical content. We describe and show examples from a prototype of the new R 2 aft story assembly engine, which generates an interactive text that has A) variable plot and B) lexical content which is individualized by decoding pattern.",
        "id": 7232815
      },
      {
        "title": "PREDICTING AND MANAGING SPOKEN DISFLUENCIES DURING HUMAN-COMPUTER INTERACTION*",
        "text": "This research characterizes the spontaneous spoken disfluencies typical of human-computer interaction, and presents a predictive model accounting for their occurrence. Data were collected during three empirical studies in which people spoke or wrote to a highly interactive simulated system. The studies involved within-subject factorial designs in which input modality and presentation format were varied. Spoken disfluency rates during human-computer interaction were documented to be substantially lower than rates typically observed during comparable human-human speech. Two separate factors, both associated with increased planning demands, were statistically related to increased speech disfluency rates: (1) length of utterance, and (2) lack of structure in the presentation format. Regression techniques revealed that a linear model based simply on utterance length accounts for over 77% of the variability in spoken disfluencies. Therefore, design techniques capable of channeling users' speech into briefer sentences potentially could eliminate most spoken disfluencies. In addition, the degree of structure in the presentation format was manipulated in a manner that successfully elimluated 60 to 70% of all disfluent speech. The long-term goal of this research is to provide empirical guidance for the design of robust spoken language technology.",
        "id": 19037207
      },
      {
        "title": "Learning Long-term Visual Dynamics with Region Proposal Interaction Networks",
        "text": "Learning long-term dynamics models is the key to understanding physical common sense. Most existing approaches on learning dynamics from visual input sidestep long-term predictions by resorting to rapid re-planning with short-term models. This not only requires such models to be super accurate but also limits them only to tasks where an agent can continuously obtain feedback and take action at each step until completion. In this paper, we aim to leverage the ideas from success stories in visual recognition tasks to build object representations that can capture inter-object and object-environment interactions over a long range. To this end, we propose Region Proposal Interaction Networks (RPIN), which reason about each object's trajectory in a latent region-proposal feature space. Thanks to the simple yet effective object representation, our approach outperforms prior methods by a significant margin both in terms of prediction quality and their ability to plan for downstream tasks, and also generalize well to novel environments. Our code is available at https://github.com/HaozhiQi/RPIN. Preprint. Under review.",
        "id": 220968856
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "In researching metrics for human-interaction with computer-assisted translation tools, which studies have analyzed parameters such as the keystroke ratio and mouse action ratio to quantify user effort or engagement?",
    "positive_ctxs": [
      {
        "title": "Statistical Approaches to Computer-Assisted Translation",
        "text": "Universitat Jaume I Current machine translation (MT) systems are still not perfect. In practice, the output from these systems needs to be edited to correct errors. A way of increasing the productivity of the whole translation process (MT plus human work) is to incorporate the human correction activities within the translation process itself, thereby shifting the MT paradigm to that of computer-assisted translation. This model entails an iterative process in which the human translator activity is included in the loop: In each iteration, a prefix of the translation is validated (accepted or amended) by the human and the system computes its best (or n-best) translation suffix hypothesis to complete this prefix. A successful framework for MT is the so-called statistical (or pattern recognition) framework. Interestingly, within this framework, the adaptation of MT systems to the interactive scenario affects mainly the search process, allowing a great reuse of successful techniques and models. In this article, alignment templates, phrase-based models, and stochastic finite-state transducers are used to develop computer-assisted translation systems. These systems were assessed in a European project (TransType2) in two real tasks: The translation of printer manuals; manuals and the translation of the Bulletin of the European Union. In each task, the following three pairs of languages were involved (in both translation directions):",
        "id": 92327
      },
      {
        "title": "Statistical phrase-based models for interactive computer-assisted translation",
        "text": "Obtaining high-quality machine translations is still a long way off. A postediting phase is required to improve the output of a machine translation system. An alternative is the so called computerassisted translation. In this framework, a human translator interacts with the system in order to obtain high-quality translations. A statistical phrase-based approach to computer-assisted translation is described in this article. A new decoder algorithm for interactive search is also presented, that combines monotone and nonmonotone search. The system has been assessed in the TransType-2 project for the translation of several printer manuals, from (to) English to (from) Spanish, German and French.",
        "id": 16308735
      }
    ],
    "negative_ctxs": [
      {
        "title": "Temporal information extraction from clinical text",
        "text": "In this paper, we present a method for temporal relation extraction from clinical narratives in French and in English. We experiment on two comparable corpora, the MERLOT corpus for French and the THYME corpus for English, and show that a common approach can be used for both languages.",
        "id": 16711873
      },
      {
        "title": "Joint Multi-modal Aspect-Sentiment Analysis with Auxiliary Cross-modal Relation Detection",
        "text": "Aspect terms extraction (ATE) and aspect sentiment classification (ASC) are two fundamental and fine-grained sub-tasks in aspect-level sentiment analysis (ALSA). In the textual analysis, jointly extracting both aspect terms and sentiment polarities has been drawn much attention due to the better applications than individual sub-task. However, in the multimodal scenario, the existing studies are limited to handle each sub-task independently, which fails to model the innate connection between the above two objectives and ignores the better applications. Therefore, in this paper, we are the first to jointly perform multi-modal ATE (MATE) and multi-modal ASC (MASC), and we propose a multi-modal joint learning approach with auxiliary cross-modal relation detection for multi-modal aspect-level sentiment analysis (MALSA). Specifically, we first build an auxiliary text-image relation detection module to control the proper exploitation of visual information. Second, we adopt the hierarchical framework to bridge the multi-modal connection between MATE and MASC, as well as separately visual guiding for each sub module. Finally, we can obtain all aspect-level sentiment polarities dependent on the jointly extracted specific aspects. Extensive experiments show the effectiveness of our approach against the joint textual approaches, pipeline and collapsed multi-modal approaches.",
        "id": 243865127
      },
      {
        "title": "Recognizing English Learners' Native Language from Their Writings",
        "text": "Native Language Identification (NLI), which tries to identify the native language (L1) of a second language learner based on their writings, is helpful for advancing second language learning and authorship profiling in forensic linguistics. With the availability of relevant data resources, much work has been done to explore the native language of a foreign language learner. In this report, we present our system for the first shared task in Native Language Identification (NLI). We use a linear SVM classifier and explore features of words, word and character n-grams, style, and metadata. Our official system achieves accuracy of 0.773, which ranks it 18 th among the 29 teams in the closed track.",
        "id": 18504651
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Are there studies that combine convolutional and recurrent neural network approaches to extract multiple types of features for relation extraction? If so, could you point me to one of them?",
    "positive_ctxs": [
      {
        "title": "Combining Recurrent and Convolutional Neural Networks for Relation Classification",
        "text": "This paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networks. For both models, we demonstrate the effect of different architectural choices. We present a new context representation for convolutional neural networks for relation classification (extended middle context). Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization. Finally, we show that combining convolutional and recurrent neural networks using a simple voting scheme is accurate enough to improve results. Our neural models achieve state-of-the-art results on the SemEval 2010 relation classification task.",
        "id": 17297069
      }
    ],
    "negative_ctxs": [
      {
        "title": "Czech MWE Database",
        "text": "In this paper we deal with a recently developed large Czech MWE database containing at the moment 160 000 MWEs (treated as lexical units). It was compiled from various resources such as encyclopedias and dictionaries, public databases of proper names and toponyms, collocations obtained from Czech WordNet, lists of botanical and zoological terms and others. We describe the structure of the database and give basic types of MWEs according to domains they belong to. We compare the built MWEs database with the corpus data from Czech National Corpus (approx. 100 mil. tokens) and present results of this comparison in the paper. These MWEs have not been obtained from the corpus since their frequencies in it are rather low. To obtain a more complete list of MWEs we propose and use a technique exploiting the Word Sketch Engine, which allows us to work with statistical parameters such as frequency of MWEs and their components as well as with the salience for the whole MWEs. We also discuss exploitation of the database for working out a more adequate tagging and lemmatization. The final goal is to be able to recognize MWEs in corpus text and lemmatize them as complete lexical units, i. e. to make tagging and lemmatization more adequate.",
        "id": 13443762
      },
      {
        "title": "What We Know About The Voynich Manuscript",
        "text": "The Voynich Manuscript is an undeciphered document from medieval Europe. We present current knowledge about the manuscript's text through a series of questions about its linguistic properties.",
        "id": 9925138
      },
      {
        "title": "LEARNING RECURRENT BINARY/TERNARY WEIGHTS",
        "text": "Recurrent neural networks (RNNs) have shown excellent performance in processing sequence data. However, they are both complex and memory intensive due to their recursive nature. These limitations make RNNs difficult to embed on mobile devices requiring real-time processes with limited hardware resources. To address the above issues, we introduce a method that can learn binary and ternary weights during the training phase to facilitate hardware implementations of RNNs. As a result, using this approach replaces all multiply-accumulate operations by simple accumulations, bringing significant benefits to custom hardware in terms of silicon area and power consumption. On the software side, we evaluate the performance (in terms of accuracy) of our method using long short-term memories (LSTMs) on various sequential models including sequence classification and language modeling. We demonstrate that our method achieves competitive results on the aforementioned tasks while using binary/ternary weights during the runtime. On the hardware side, we present custom hardware for accelerating the recurrent computations of LSTMs with binary/ternary weights. Ultimately, we show that LSTMs with binary/ternary weights can achieve up to 12× memory saving and 10× inference speedup compared to the full-precision implementation on an ASIC platform.",
        "id": 52894096
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Could you recommend studies that provide a baseline for experiments using supervised constituency parsers with a focus on few-shot learning settings and have also reported on the use of pre-training and data augmentation techniques for parser performance improvement?",
    "positive_ctxs": [
      {
        "title": "On the Role of Supervision in Unsupervised Constituency Parsing",
        "text": "We analyze several recent unsupervised constituency parsing models, which are tuned with respect to the parsing F 1 score on the Wall Street Journal (WSJ) development set (1,700 sentences). We introduce strong baselines for them, by training an existing supervised parsing model (Kitaev and Klein, 2018) on the same labeled examples they access. When training on the 1,700 examples, or even when using only 50 examples for training and 5 for development, such a few-shot parsing approach can outperform all the unsupervised parsing methods by a significant margin. Fewshot parsing can be further improved by a simple data augmentation method and selftraining. This suggests that, in order to arrive at fair conclusions, we should carefully consider the amount of labeled data used for model development. We propose two protocols for future work on unsupervised parsing: (i) use fully unsupervised criteria for hyperparameter tuning and model selection; (ii) use as few labeled examples as possible for model development, and compare to few-shot parsing trained on the same labeled examples. 1",
        "id": 222140735
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 227230629
      },
      {
        "title": "A Knowledge-based Approach to Text Classification",
        "text": "The paper presents a simple and effective knowledge-based approach for the task of text classification. The approach uses topic identification algorithm named FIFA to text classification. In this paper the basic process of text classification task and FIFA algorithm are described in detail. At last some results of experiment and evaluations are discussed.",
        "id": 8700588
      },
      {
        "title": "Le projet BabyTalk : génération de texte à partir de données hétérogènes pour la prise de décision en unité néonatale",
        "text": "Notre société génère une masse d'information toujours croissante, que ce soit en médecine, en météorologie, etc. La méthode la plus employée pour analyser ces données est de les résumer sous forme graphique. Cependant, il a été démontré qu'un résumé textuel est aussi un mode de présentation efficace. L'objectif du prototype BT-45, développé dans le cadre du projet Babytalk, est de générer des résumés de 45 minutes de signaux physiologiques continus et d'événements temporels discrets en unité néonatale de soins intensifs (NICU). L'article présente l'aspect génération de texte de ce prototype. Une expérimentation clinique a montré que les résumés humains améliorent la prise de décision par rapport à l'approche graphique, tandis que les textes de BT-45 donnent des résultats similaires à l'approche graphique. Une analyse a identifié certaines des limitations de BT-45 mais en dépit de cellesci, notre travail montre qu'il est possible de produire automatiquement des résumés textuels efficaces de données complexes.Abstract Nowadays large amount of data is produced every day in medicine, meteorology and other areas and the most common approach to analyse such data is to present it graphically. However, it has been shown that textual summarisation is also an effective approach. As part of the BabyTalk project, the prototype BT-45 was developed to generate summaries of 45 minutes of continuous physiological signals and discrete temporal events in a neonatal intensive care unit (NICU). The paper presents its architecture with an emphasis on its natural language generation part. A clinical experiment showed that human textual summaries led to better decision making than graphical presentation, whereas BT-45 texts led to similar results as visualisations. An analysis identified some of the reasons for the BT-45 texts inferiority, but, despite these deficiencies, our work shows that it is possible for computer systems to generate effective textual summaries of complex data.Mots-clés : Traitement automatique des langues naturelles ; Génération de texte ; Analyse de données ; Unité de soins intensifs ; Systèmes d'aide à la décision",
        "id": 69085348
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "I know about prompt tuning, but have any works tried learning embeddings that are inputted to every transformer layer in a language model?",
    "positive_ctxs": [
      {
        "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
        "text": "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were \"virtual tokens\". We apply prefix-tuning to GPT-2 for table-totext generation and to BART for summarization. We show that by modifying only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. 2020. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. Amodei. 2020. Language models are few-shot learners.",
        "id": 230433941
      }
    ],
    "negative_ctxs": [
      {
        "title": "UNSUPERVISED DISENTANGLEMENT WITH TENSOR PRODUCT REPRESENTATIONS ON THE TORUS",
        "text": "The current methods for learning representations with auto-encoders almost exclusively employ vectors as the latent representations. In this work, we propose to employ a tensor product structure for this purpose. This way, the obtained representations are naturally disentangled. In contrast to the conventional variations methods, which are targeted toward normally distributed features, the latent space in our representation is distributed uniformly over a set of unit circles. We argue that the torus structure of the latent space captures the generative factors effectively. We employ recent tools for measuring unsupervised disentanglement, and in an extensive set of experiments demonstrate the advantage of our method in terms of disentanglement, completeness, and informativeness. The code for our proposed method is available at https://github.com/rotmanmi/ Unsupervised-Disentanglement-Torus.",
        "id": 246822414
      },
      {
        "title": "",
        "text": "",
        "id": 237155074
      },
      {
        "title": "",
        "text": "",
        "id": 6086996
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper shows that in instruction tuning, the instructions can be compressed to small supporting sets of words that provide useful information?",
    "positive_ctxs": [
      {
        "title": "Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning",
        "text": "Large language models (LLMs) have shown impressive performance in following natural language instructions to solve unseen tasks. However, it remains unclear whether models truly understand task definitions and whether the human-written definitions are optimal. In this paper, we systematically study the role of task definitions in instruction learning. We first conduct an ablation analysis informed by human annotations to understand which parts of a task definition are most important, and find that model performance only drops substantially when removing contents describing the task output, in particular label information. Next, we propose an automatic algorithm to compress task definitions to a minimal supporting set of tokens, and find that 60% of tokens can be removed while maintaining or even improving model performance. Based on these results, we propose two strategies to help models better leverage task instructions: (1) providing only key information for tasks in a common structured format, and (2) adding a metatuning stage to help the model better understand the definitions. With these two strategies, we achieve a 4.2 Rouge-L improvement over 119 unseen test tasks. naneh Hajishirzi. 2022a. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560.",
        "id": 259063796
      }
    ],
    "negative_ctxs": [
      {
        "title": "Decomposing Consumer Health Questions",
        "text": "This paper presents a method for decomposing long, complex consumer health questions. Our approach largely decomposes questions using their syntactic structure, recognizing independent questions embedded in clauses, as well as coordinations and exemplifying phrases. Additionally, we identify elements specific to disease-related consumer health questions, such as the focus disease and background information. To achieve this, our approach combines rank-and-filter machine learning methods with rule-based methods. Our results demonstrate significant improvements over the heuristic methods typically employed for question decomposition that rely only on the syntactic parse tree.",
        "id": 1006882
      },
      {
        "title": "DEEPSPHERE: A GRAPH-BASED SPHERICAL CNN",
        "text": "Designing a convolution for a spherical neural network requires a delicate tradeoff between efficiency and rotation equivariance. DeepSphere, a method based on a graph representation of the sampled sphere, strikes a controllable balance between these two desiderata. This contribution is twofold. First, we study both theoretically and empirically how equivariance is affected by the underlying graph with respect to the number of vertices and neighbors. Second, we evaluate DeepSphere on relevant problems. Experiments show state-of-the-art performance and demonstrates the efficiency and flexibility of this formulation. Perhaps surprisingly, comparison with previous work suggests that anisotropic filters might be an unnecessary price to pay. Our code is available at https: //github.com/deepsphere.arXiv:2012.15000v1 [cs.LG] 30 Dec 2020Published as a conference paper at ICLR 2020As neural networks (NNs) have proved to be great tools for inference, variants have been developed to handle spherical data. Exploiting the locally Euclidean property of the sphere, early attempts used standard 2D convolutions on a grid sampling of the sphere (Boomsma & Frellsen, 2017;Su & Grauman, 2017;Coors et al., 2018). While simple and efficient, those convolutions are not equivariant to rotations. On the other side of this tradeoff,Cohen et al. (2018)andEsteves et al. (2018)proposed to perform proper spherical convolutions through the spherical harmonic transform. While equivariant to rotations, those convolutions are expensive (section 2).As a lack of equivariance can penalize performance (section 4.2) and expensive convolutions prohibit their application to some real-world problems, methods standing between these two extremes are desired.Cohen et al. (2019)proposed to reduce costs by limiting the size of the representation of the symmetry group by projecting the data from the sphere to the icosahedron. The distortions introduced by this projection might however hinder performance (section 4.3).Another approach is to represent the sampled sphere as a graph connecting pixels according to the distance between them (Bruna et al., 2013;Khasanova & Frossard, 2017;. While Laplacian-based graph convolutions are more efficient than spherical convolutions, they are not exactly equivariant . In this work, we argue that graph-based spherical CNNs strike an interesting balance, with a controllable tradeoff between cost and equivariance (which is linked to performance). Experiments on multiple problems of practical interest show the competitiveness and flexibility of this approach.",
        "id": 213692365
      },
      {
        "title": "TAPAS: Weakly Supervised Table Parsing via Pre-training",
        "text": "Answering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation. In this paper, we present TAPAS, an approach to question answering over tables without generating logical forms. TAPAS trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection. TAPAS extends BERT's architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end. We experiment with three different semantic parsing datasets, and find that TAPAS outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WIKISQL and WIKITQ, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our setting, from WIK-ISQL to WIKITQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.",
        "id": 214802901
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "I'm looking for innovative approaches to data annotation on platforms like Amazon Mechanical Turk that focus on maximizing document coverage. Is there any research discussing strategies that balance the trade-off between full annotation and broader document coverage?",
    "positive_ctxs": [
      {
        "title": "Partial Or Complete, That Is The Question",
        "text": "For many structured learning tasks, the data annotation process is complex and costly. Existing annotation schemes usually aim at acquiring completely annotated structures, under the common perception that partial structures are of low quality and could hurt the learning process. This paper questions this common perception, motivated by the fact that structures consist of interdependent sets of variables. Thus, given a fixed budget, partly annotating each structure may provide the same level of supervision, while allowing for more structures to be annotated. We provide an information theoretic formulation for this perspective and use it, in the context of three diverse structured learning tasks, to show that learning from partial structures can sometimes outperform learning from complete ones. Our findings may provide important insights into structured data annotation schemes and could support progress in learning protocols for structured tasks.",
        "id": 174800300
      }
    ],
    "negative_ctxs": [
      {
        "title": "Nearest Neighbor Knowledge Distillation for Neural Machine Translation",
        "text": "k-nearest-neighbor machine translation (kNN-MT), proposed byKhandelwal et al. (2021), has achieved many state-of-the-art results in machine translation tasks. Although effective, kNN-MT requires conducting kNN searches through the large datastore for each decoding step during inference, prohibitively increasing the decoding cost and thus leading to the difficulty for the deployment in real-world applications. In this paper, we propose to move the time-consuming kNN search forward to the preprocessing phase, and then introduce k Nearest Neighbor Knowledge Distillation (kNN-KD) that trains the base NMT model to directly learn the knowledge of kNN. Distilling knowledge retrieved by kNN can encourage the NMT model to take more reasonable target tokens into consideration, thus addressing the overcorrection problem. Extensive experimental results show that, the proposed method achieves consistent improvement over the stateof-the-art baselines including kNN-MT, while maintaining the same training and decoding speed as the standard NMT model. 1",
        "id": 248496823
      },
      {
        "title": "Dialogue Management based on Multi-domain Corpus",
        "text": "Dialogue Management (DM) is a key issue in Spoken Dialogue System. Most of the existing data-driven DM schemes train the dialogue policy for some specific domain (or vertical domain), only using the dialogue corpus in this domain, which might suffer from the scarcity of dialogue corpus in some domains. In this paper, we divide Dialogue Act (DA), as semantic representation of utterance, into DA type and slot parameter, where the former one is domain-independent and the latter one is domain-specific. Firstly, based on multiple-domain dialogue corpus, the DA type prediction model is trained via Recurrent Neutral Networks (RNN). Moreover, DA type decision problem is modeled as a multi-order POMDP, and transformed to be a one-order MDP with continuous states, which is solved by Natural Actor Critic (NAC) algorithm and applicable for every domain. Furthermore, a slot parameter selection scheme is designed to generate a complete machine DA according to the features of specific domain, which yields the Multi-domain Corpus based Dialogue Management (MCD-M) scheme. Finally, extensive experimental results illustrate the performance improvement of the MCDM scheme, compared with the existing schemes.",
        "id": 9281432
      },
      {
        "title": "NTUA-SLP at SemEval-2018 Task 2: Predicting Emojis using RNNs with Context-aware Attention",
        "text": "In this paper we present a deep-learning model that competed at SemEval-2018 Task 2 \"Multilingual Emoji Prediction\". We participated in subtask A, in which we are called to predict the most likely associated emoji in English tweets. The proposed architecture relies on a Long Short-Term Memory network, augmented with an attention mechanism, that conditions the weight of each word, on a \"context vector\" which is taken as the aggregation of a tweet's meaning. Moreover, we initialize the embedding layer of our model, with word2vec word embeddings, pretrained on a dataset of 550 million English tweets. Finally, our model does not rely on hand-crafted features or lexicons and is trained end-to-end with back-propagation. We ranked 2 nd out of 48 teams.",
        "id": 4943905
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Could you point me toward some large-scale multilingual Amazon customer review data?",
    "positive_ctxs": [
      {
        "title": "The Multilingual Amazon Reviews Corpus",
        "text": "We present the Multilingual Amazon Reviews Corpus (MARC), a large-scale collection of Amazon reviews for multilingual text classification. The corpus contains reviews in English, Japanese, German, French, Spanish, and Chinese, which were collected between 2015 and 2019. Each record in the dataset contains the review text, the review title, the star rating, an anonymized reviewer ID, an anonymized product ID, and the coarse-grained product category (e.g., 'books', 'appliances', etc.) The corpus is balanced across the 5 possible star ratings, so each rating constitutes 20% of the reviews in each language. For each language, there are 200,000, 5,000, and 5,000 reviews in the training, development, and test sets, respectively. We report baseline results for supervised text classification and zero-shot crosslingual transfer learning by fine-tuning a multilingual BERT model on reviews data. We propose the use of mean absolute error (MAE) instead of classification accuracy for this task, since MAE accounts for the ordinal nature of the ratings.",
        "id": 222141483
      }
    ],
    "negative_ctxs": [
      {
        "title": "FERMAT: An Alternative to Accuracy for Numerical Reasoning",
        "text": "While pre-trained language models achieve impressive performance on various NLP benchmarks, they still struggle with tasks that require numerical reasoning. Recent advances in improving numerical reasoning are mostly achieved using very large language models that contain billions of parameters and are not accessible to everyone. In addition, numerical reasoning is measured using a single score on existing datasets. As a result, we do not have a clear understanding of the strengths and shortcomings of existing models on different numerical reasoning aspects and therefore, potential ways to improve them apart from scaling them up. Inspired by CheckList (Ribeiro et al., 2020), we introduce a multi-view evaluation set for numerical reasoning in English, called FERMAT. Instead of reporting a single score on a whole dataset, FERMAT evaluates models on various key numerical reasoning aspects such as number understanding, mathematical operations, and training dependency. Apart from providing a comprehensive evaluation of models on different numerical reasoning aspects, FERMAT enables a systematic and automated generation of an arbitrarily large training or evaluation set for each aspect.The datasets and codes are publicly available to generate further multi-view data for ulterior tasks and languages. 1",
        "id": 258959201
      },
      {
        "title": "Unsupervised Event Coreference Resolution with Rich Linguistic Features",
        "text": "This paper examines how a new class of nonparametric Bayesian models can be effectively applied to an open-domain event coreference task. Designed with the purpose of clustering complex linguistic objects, these models consider a potentially infinite number of features and categorical outcomes. The evaluation performed for solving both within-and cross-document event coreference shows significant improvements of the models when compared against two baselines for this task.",
        "id": 739867
      },
      {
        "title": "Structural Ambiguity and Conceptual Relations",
        "text": "Lexical co-occurrence statistics are becoming widely used in the syntactic analysis of unconstrained text. However, analyses based solely on lexical relationships suffer from sparseness of data: it is sometimes necessary to use a less informed model in order to reliably estimate statistical parameters. For example, the \"lexical association\" strategy for resolving ambiguous prepositional phrase attachments[Hindle and Rooth. 1991]takes into account only the attachment site (a verb or its direct object) and the preposition, ignoring the object of the preposition.We investigated an extension of the lexical association strategy to make use of noun class information, thus permitting a disambiguation strategy to take more information into account. Although in preliminary experiments the extended strategy did not yield improved performance over lexical association alone. a qualitative analysis of the results suggests that the problem lies not in the noun class information, but rather in the multiplicity of classes available for each noun in the absence of sense disambiguation. This suggests several possible revisions of our proposal.",
        "id": 16000644
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Is there a research paper that has developed a customer service conversation dataset aimed at forecasting customer intentions, taking into account the limitations imposed by agent protocols?",
    "positive_ctxs": [
      {
        "title": "Action-Based Conversations Dataset: A Corpus for Building More In-Depth Task-Oriented Dialogue Systems",
        "text": "Existing goal-oriented dialogue datasets focus mainly on identifying slots and values. However, customer support interactions in reality often involve agents following multi-step procedures derived from explicitly-defined company policies as well. To study customer service dialogue systems in more realistic settings, we introduce the Action-Based Conversations Dataset (ABCD), a fully-labeled dataset with over 10K human-to-human dialogues containing 55 distinct user intents requiring unique sequences of actions constrained by policies to achieve task success.We propose two additional dialog tasks, Action State Tracking and Cascading Dialogue Success, and establish a series of baselines involving large-scale, pre-trained language models on this dataset. Empirical results demonstrate that while more sophisticated networks outperform simpler models, a considerable gap (50.8% absolute accuracy) still exists to reach human-level performance on ABCD. 1",
        "id": 233004708
      }
    ],
    "negative_ctxs": [
      {
        "title": "Toward Better Storylines with Sentence-Level Language Models",
        "text": "We propose a sentence-level language model which selects the next sentence in a story from a finite set of fluent alternatives. Since it does not need to model fluency, the sentence-level language model can focus on longer range dependencies, which are crucial for multisentence coherence. Rather than dealing with individual words, our method treats the story so far as a list of pre-trained sentence embeddings and predicts an embedding for the next sentence, which is more efficient than predicting word embeddings. Notably this allows us to consider a large number of candidates for the next sentence during training. We demonstrate the effectiveness of our approach with state-of-the-art accuracy on the unsupervised Story Cloze task and with promising results on larger-scale next sentence prediction tasks.",
        "id": 218551245
      },
      {
        "title": "ReAugKD: Retrieval-Augmented Knowledge Distillation For Pre-trained Language Models",
        "text": "Knowledge Distillation (KD)(Hinton et al., 2015)is one of the most effective approaches for deploying large-scale pre-trained language models in low-latency environments by transferring the knowledge contained in the largescale models to smaller student models. Previous KD approaches use the soft labels and intermediate activations generated by the teacher to transfer knowledge to the student model parameters alone. In this paper, we show that having access to non-parametric memory in the form of a knowledge base with the teacher's soft labels and predictions can further enhance student capacity and improve generalization. To enable the student to retrieve from the knowledge base effectively, we propose a new Retrieval-augmented KD framework with a loss function that aligns the relational knowledge in teacher and student embedding spaces. We show through extensive experiments that our retrieval mechanism can achieve state-of-the-art performance for taskspecific knowledge distillation on the GLUE benchmark(Wang et al., 2018a).",
        "id": 259370551
      },
      {
        "title": "Exploiting Multi-Word Units in History-Based Probabilistic Generation",
        "text": "We present a simple history-based model for sentence generation from LFG f-structures, which improves on the accuracy of previous models by breaking down PCFG independence assumptions so that more f-structure conditioning context is used in the prediction of grammar rule expansions. In addition, we present work on experiments with named entities and other multi-word units, showing a statistically significant improvement of generation accuracy. Tested on section 23 of the Penn Wall Street Journal Treebank, the techniques described in this paper improve BLEU scores from 66.52 to 68.82, and coverage from 98.18% to 99.96%.",
        "id": 10339151
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Has there been any research that uses multiple models to learn the preferences of individual annotators, and then ensembles these models to obtain majority-vote preference scores while also having an uncertainty measure?",
    "positive_ctxs": [
      {
        "title": "Dealing with Disagreements: Looking Beyond the Majority Vote in Subjective Annotations",
        "text": "Majority voting and averaging are common approaches used to resolve annotator disagreements and derive single ground truth labels from multiple annotations. However, annotators may systematically disagree with one another, often reflecting their individual biases and values, especially in the case of subjective tasks such as detecting affect, aggression, and hate speech. Annotator disagreements may capture important nuances in such tasks that are often ignored while aggregating annotations to a single ground truth. In order to address this, we investigate the efficacy of multi-annotator models. In particular, our multi-task based approach treats predicting each annotators' judgements as separate subtasks, while sharing a common learned representation of the task. We show that this approach yields same or better performance than aggregating labels in the data prior to training across seven different binary classification tasks. Our approach also provides a way to estimate uncertainty in predictions, which we demonstrate better correlate with annotation disagreements than traditional methods. Being able to model uncertainty is especially useful in deployment scenarios where knowing when not to make a prediction is important.92",
        "id": 238634750
      }
    ],
    "negative_ctxs": [
      {
        "title": "Automatic Generation of Information State Update Dialogue Systems that Dynamically Create Voice XML, as Demonstrated on the iPhone",
        "text": "We demonstrate DUDE 1 (Dialogue and Understanding Development Environment), a prototype development environment that automatically generates dialogue systems from business-user resources and databases. These generated spoken dialogue systems (SDS) are then deployed on an industry standard Voice XML platform. Specifically, the deployed system works by dynamically generating context-sensitive Voice XML pages. The dialogue move of each page is determined in real time by the dialogue manager, which is an Information State Update engine. Firstly, we will demonstrate the development environment which includes automatic generation of speech recognition grammars for robust interpretation of spontaneous speech, and uses the application database to generate lexical entries and grammar rules. A simple graphical interface allows users (i.e. developers) to easily and quickly create and the modify SDS without the need for expensive service providers. Secondly, we will demonstrate the deployed system which enables participants to call up and speak to the SDS recently created. We will also show a pre-built application running on the iPhone and Google Android phone for searching for places such as restaurants, hotels and museums.",
        "id": 672449
      },
      {
        "title": "Developing a Fine-Grained Corpus for a Less-resourced Language: the case of Kurdish *",
        "text": "Kurdish is a less-resourced language consisting of different dialects written in various scripts. Approximately 30 million people in different countries speak the language. The lack of corpora is one of the main obstacles in Kurdish language processing. In this paper, we present KTC-the Kurdish Textbooks Corpus, which is composed of 31 K-12 textbooks in Sorani dialect. The corpus is normalized and categorized into 12 educational subjects containing 693,800 tokens (110,297 types). Our resource is publicly available for non-commercial use under the CC BY-NC-SA 4.0 license 1 .",
        "id": 200031907
      },
      {
        "title": "Context-Sensitive Electronic Dictionaries",
        "text": "This paper introduces a context-sensitive electronic dictionary that provides translations for any piece of text displayed on a computer screen, without requiring user interaction. This is achieved through a process of three phases: text acquisition from the screen, morpho-syntactic analysis of the context of the selected word, and the dictionary lookup. As with other similar tools available, this program usually works with dictionaries adapted from one or more printed dictionaries. To implement context sensitive features, however, traditional dictionary entries need to be restructured. By splitting up entries into smaller pieces and indexing them in a special way, the program is able to display a restricted set of information that is relevant to the context. Based on the information in the dictionaries, the program is able to recognize-even discontinuous-multiword expressions on the screen. The program has three major features which we believe make it unique for the time being, and which the development focused on: linguistic flexibility (stemming, morphological analysis and shallow parsing), open architecture (three major architectural blocks, all replaceable along public documented APIs), and flexible user interface (replaceable dictionaries, direct user feedback). In this paper, we assess the functional requirements of a context-sensitive dictionary as a start; then we explain the program's three phases of operation, focusing on the implementation of the lexicons and the context-sensitive features. We conclude the paper by comparing our tool to other similar publicly available products, and summarize plans for future development.",
        "id": 11852941
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper shows assessment of training instabilities at different levels for language models?",
    "positive_ctxs": [
      {
        "title": "Measuring the Instability of Fine-Tuning",
        "text": "Fine-tuning pre-trained language models on downstream tasks with varying random seeds has been shown to be unstable, especially on small datasets. Many previous studies have investigated this instability and proposed methods to mitigate it. However, most studies only used the standard deviation of performance scores (SD) as their measure, which is a narrow characterization of instability. In this paper, we analyze SD and six other measures quantifying instability at different levels of granularity. Moreover, we propose a systematic framework to evaluate the validity of these measures. Finally, we analyze the consistency and difference between different measures by reassessing existing instability mitigation methods. We hope our results will inform the development of better measurements of fine-tuning instability. 1Layer-wise Learning Rate Decay (Howard and Ruder, 2018, LLRD) assigns decreasing learning rates from the topmost layer to the bottom layer by a constant hyper-parameter discounting factor η. Howard and Ruder (2018) empirically show that models trained using LLRD are more stable, by retaining more generalizable pre-trained knowledge in bottom layers, while forgetting specialized pre-train knowledge in top layers.Re-init (Zhang et al., 2021) stabilizes fine-tuning by re-initializing the top k layers of PLMs. The underlying intuition is similar to LLRD: top layers of PLMs contain more pre-train task specific knowledge, and transferring it may hurt stability.",
        "id": 256868814
      }
    ],
    "negative_ctxs": [
      {
        "title": "The RWTH System for Statistical Translation of Spoken Dialogues",
        "text": "This paper gives an overview of our work on statistical machine translation of spoken dialogues, in particular in the framework of the Verbmobil project. The goal of the Verbmobil project is the translation of spoken dialogues in the domains of appointment scheduling and travel planning. Starting with the Bayes decision rule as in speech recognition, we show how the required probability distributions can be structured into three parts: the language model, the alignment model and the lexicon model. We describe the components of the system and report results on the Verbmobil task. The experience obtained in the Verbmobil project, in particular a large-scale end-to-end evaluation, showed that the statistical approach resulted in significantly lower error rates than three competing translation approaches: the sentence error rate was 29% in comparison with 52% to 62% for the other translation approaches.",
        "id": 8473810
      },
      {
        "title": "Experiments with Interactive Question-Answering",
        "text": "This paper describes a novel framework for interactive question-answering (Q/A) based on predictive questioning. Generated off-line from topic representations of complex scenarios, predictive questions represent requests for information that capture the most salient (and diverse) aspects of a topic. We present experimental results from large user studies (featuring a fully-implemented interactive Q/A system named FERRET) that demonstrates that surprising performance is achieved by integrating predictive questions into the context of a Q/A dialogue.",
        "id": 17979800
      },
      {
        "title": "Know What You Don't Know: Modeling a Pragmatic Speaker that Refers to Objects of Unknown Categories",
        "text": "Zero-shot learning in Language & Vision is the task of correctly labelling (or naming) objects of novel categories. Another strand of work in L&V aims at pragmatically informative rather than \"correct\" object descriptions, e.g. in reference games. We combine these lines of research and model zero-shot reference games, where a speaker needs to successfully refer to a novel object in an image. Inspired by models of \"rational speech acts\", we extend a neural generator to become a pragmatic speaker reasoning about uncertain object categories. As a result of this reasoning, the generator produces fewer nouns and names of distractor categories as compared to a literal speaker. We show that this conversational strategy for dealing with novel objects often improves communicative success, in terms of resolution accuracy of an automatic listener.",
        "id": 189762140
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Is it possible to adatp named entity recognition systems to learn new entities by combining knowledge distillation and synthetic data augmentation by using a two-stage training approach?",
    "positive_ctxs": [
      {
        "title": "Learn and Review: Enhancing Continual Named Entity Recognition via Reviewing Synthetic Samples",
        "text": "Traditional methods for named entity recognition (NER) classify mentions into a fixed set of pre-defined entity types. However, in many real-world scenarios, new entity types are incrementally involved. To investigate this problem, continual learning is introduced for NER. However, the existing method depends on the relevance between tasks and is prone to inter-type confusion. In this paper, we propose a novel two-stage framework Learn-and-Review (L&R) for continual NER under the type-incremental setting to alleviate the above issues. Specifically, for the learning stage, we distill the old knowledge from teacher to a student on the current dataset. For the reviewing stage, we first generate synthetic samples of old types to augment the dataset. Then, we further distill new knowledge from the above student and old knowledge from the teacher to get an enhanced student on the augmented dataset. This stage has the following advantages: (1) The synthetic samples mitigate the gap between the old and new task and thus enhance the further distillation; (2) Different types of entities are jointly seen during training which alleviates the inter-type confusion. Experimental results show that L&R outperforms the state-of-the-art method on CoNLL-03 and OntoNotes-5.0.",
        "id": 248779939
      }
    ],
    "negative_ctxs": [
      {
        "title": "ISTIC's Triangular Machine Translation System for WMT' 2021",
        "text": "This paper describes the ISTIC's submission to the Triangular Machine Translation Task of Russian-to-Chinese machine translation for WMT' 2021. In order to fully utilize the provided corpora and promote the translation performance from Russian to Chinese, the pivot method is used in our system which pipelines the Russian-to-English translator and the English-to-Chinese translator to form a Russian-to-Chinese translator. Our system is based on the Transformer architecture and several effective strategies are adopted to improve the quality of translation, including corpus filtering, data pre-processing, system combination, model averaging, model ensemble and reranking.",
        "id": 245855694
      },
      {
        "title": "ModelTalker Voice Recorder -An Interface System for Recording a Corpus of Speech for Synthesis",
        "text": "We will demonstrate the ModelTalker Voice Recorder (MT Voice Recorder) -an interface system that lets individuals record and bank a speech database for the creation of a synthetic voice. The system guides users through an automatic calibration process that sets pitch, amplitude, and silence. The system then prompts users with both visual (text-based) and auditory prompts. Each recording is",
        "id": 543388
      },
      {
        "title": "Domain Adaptation for Parsing",
        "text": "We compare two different methods in domain adaptation applied to constituent parsing: parser combination and cotraining, each used to transfer information from the source domain of news to the target domain of natural dialogs, in a setting without annotated data. Both methods outperform the baselines and reach similar results. Parser combination profits most from the large amounts of training data combined with a robust probability model. Co-training, in contrast, relies on a small set of higher quality data.",
        "id": 9510501
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Which work shows that only emplying instance-level uncertainty metrics could results in sample redundancy in active learning?",
    "positive_ctxs": [
      {
        "title": "ACTUNE: Uncertainty-Based Active Self-Training for Active Fine-Tuning of Pretrained Language Models",
        "text": "While pre-trained language model (PLM) finetuning has achieved strong performance in many NLP tasks, the fine-tuning stage can be still demanding in labeled data. Recent works have resorted to active fine-tuning to improve the label efficiency of PLM fine-tuning, but none of them investigate the potential of unlabeled data. We propose ACTUNE, a new framework that leverages unlabeled data to improve the label efficiency of active PLM fine-tuning. ACTUNE switches between data annotation and model self-training based on uncertainty: it selects high-uncertainty unlabeled samples for active annotation and lowuncertainty ones for model self-training. Under this framework, we design (1) a regionaware sampling strategy that reduces redundancy when actively querying for annotations and (2) a momentum-based memory bank that dynamically aggregates the model's pseudo labels to suppress label noise in self-training. Experiments on 6 text classification datasets show that ACTUNE outperforms the strongest active learning and self-training baselines and improves the label efficiency of PLM finetuning by 56.2% on average. Our implementation is available at https://github. com/yueyu1030/actune.",
        "id": 250390500
      }
    ],
    "negative_ctxs": [
      {
        "title": "Identifying and Utilizing the Class of Monosemous Japanese Functional Expressions in Machine Translation",
        "text": "In the \"Sandglass\" machine translation architecture, we identify the class of monosemous Japanese functional expressions and utilize it in the task of translating Japanese functional expressions into English. We employ the semantic equivalence classes of a recently compiled large scale hierarchical lexicon of Japanese functional expressions. We then study whether functional expressions within a class can be translated into a single canonical English expression. Next, we introduce two types of ambiguities of functional expressions and identify monosemous functional expressions. In the evaluation of our translation rules for Japanese functional expressions, we directly apply those rules to monosemous functional expressions, and show that the proposed framework outperforms commercial machine translation software products. We further study how to extract rules for translating functional expressions in Japanese patent documents into English. In the result of this study, we show that translation rules manually developed based on the corpus for Japanese language grammar learners is reliable also in the patent domain.",
        "id": 16997140
      },
      {
        "title": "The Semantics of a Definiendum Constrains both the Lexical Semantics and the Lexicosyntactic Patterns in the Definiens",
        "text": "Most current definitional question answering systems apply one-size-fits-",
        "id": 18979309
      },
      {
        "title": "InterlinguaPlus Machine Translation Approach for Under-Resourced Languages: Ekegusii & Swahili",
        "text": "This paper elucidates the InterlinguaPlus design and its application in bi-directional text translations between Ekegusii and Kiswahili languages unlike the traditional translation pairs, one-by-one. Therefore, any of the languages can be the source or target language. The first section is an overview of the project, which is followed by a brief review of Machine Translation. The next section discusses the implementation of the system using Carabao's open machine translation framework and the results obtained. So far, the translation results have been plausible particularly for the resource-scarce local languages and clearly affirm morphological similarities inherent in Bantu languages.",
        "id": 18766258
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that concentrates on pinpointing sentence components that carry hateful expressions, potentially aiding in sentence-level standardization for content moderation?",
    "positive_ctxs": [
      {
        "title": "SemEval-2021 Task 5: Toxic Spans Detection",
        "text": "The Toxic Spans Detection task of SemEval-2021 required participants to predict the spans of toxic posts that were responsible for the toxic label of the posts. The task could be addressed as supervised sequence labeling, using training data with gold toxic spans provided by the organisers. It could also be treated as rationale extraction, using classifiers trained on potentially larger external datasets of posts manually annotated as toxic or not, without toxic span annotations. For the supervised sequence labeling approach and evaluation purposes, posts previously labeled as toxic were crowd-annotated for toxic spans. Participants submitted their predicted spans for a held-out test set, and were scored using character-based F1. This overview summarises the work of the 36 teams that provided system descriptions.",
        "id": 236460230
      }
    ],
    "negative_ctxs": [
      {
        "title": "The aNALoGuE Challenge: Non Aligned Language GEneration",
        "text": "We propose a shared task based on recent advances in learning to generate natural language from meaning representations using semantically unaligned data. The aNALoGuE challenge aims to evaluate and compare recent corpus-based methods with respect to their scalability to data size and target complexity, as well as to assess predictive quality of automatic evaluation metrics.",
        "id": 40317724
      },
      {
        "title": "Boosting Radiology Report Generation by Infusing Comparison Prior",
        "text": "Recent transformer-based models have made significant strides in generating radiology reports from chest X-ray images. However, a prominent challenge remains: these models often lack prior knowledge, resulting in the generation of synthetic reports that mistakenly reference non-existent prior exams. This discrepancy can be attributed to a knowledge gap between radiologists and the generation models. While radiologists possess patientspecific prior information, the models solely receive X-ray images at a specific time point. To tackle this issue, we propose a novel approach that leverages a rule-based labeler to extract comparison prior information from radiology reports. This extracted comparison prior is then seamlessly integrated into stateof-the-art transformer-based models, enabling them to produce more realistic and comprehensive reports. Our method is evaluated on English report datasets, such as IU X-ray and MIMIC-CXR. The results demonstrate that our approach surpasses baseline models in terms of natural language generation metrics. Notably, our model generates reports that are free from false references to non-existent prior exams, setting it apart from previous models. By addressing this limitation, our approach represents a significant step towards bridging the gap between radiologists and generation models in the domain of medical report generation.",
        "id": 258557717
      },
      {
        "title": "A SPEECH-FIRST MODEL FOR REPAIR DETECTION AND CORRECTION",
        "text": "Interpreting fully natural speech is an important goal for spoken language understanding systems. However, while corpus studies have shown that about 10% of spontaneous utterances contain self-corrections, or RE-PAIRS, little is known about the extent to which cues in the speech signal may facilitate repair processing. We identify several cues based on acoustic and prosodic analysis of repairs in a corpus of spontaneous speech, and propose methods for exploiting these cues to detect and correct repairs. We test our acoustic-prosodic cues with other lexical cues to repair identification and find that precision rates of 89-93% and recall of 78-83% can be achieved, depending upon the cues employed, from a prosodically labeled corpus.Recently,  and  have proposed a two-stage method for processing repairs. In the first stage, lexical pattern",
        "id": 62267053
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper introduce a DRO (distribution robust optimization) like training objective for doing adversarial training without constructing adversarial samples.",
    "positive_ctxs": [
      {
        "title": "DSRM: Boost Textual Adversarial Training with Distribution Shift Risk Minimization",
        "text": "Adversarial training is one of the bestperforming methods in improving the robustness of deep language models. However, robust models come at the cost of high time consumption, as they require multi-step gradient ascents or word substitutions to obtain adversarial samples. In addition, these generated samples are deficient in grammatical quality and semantic consistency, which impairs the effectiveness of adversarial training. To address these problems, we introduce a novel, effective procedure for instead adversarial training with only clean data. Our procedure, distribution shift risk minimization (DSRM), estimates the adversarial loss by perturbing the input data's probability distribution rather than their embeddings. This formulation results in a robust model that minimizes the expected global loss under adversarial attacks. Our approach requires zero adversarial samples for training and reduces time consumption by up to 70% compared to current best-performing adversarial training methods. Experiments demonstrate that DSRM considerably improves BERT's resistance to textual adversarial attacks and achieves state-of-the-art robust accuracy on various benchmarks.",
        "id": 259262427
      }
    ],
    "negative_ctxs": [
      {
        "title": "Detecting English Grammatical Errors based on Machine Translation",
        "text": "Many people are learning English as a second or foreign language, and there are estimated 375 million English as a Second Language (ESL) and 750 million English as a Foreign Language (EFL) learners around the world according toGraddol (2006). Evidently,",
        "id": 33616749
      },
      {
        "title": "Heterogeneous Computing for Example-Based Translation of Spoken Language",
        "text": "Spoken language translation requires both (1) high accuracy and (2) a real-time response which are difficult to achieve using conventional technologies. To fulfill the first requirement, we have adopted an Example-Based Approach. It generates a target sentence by combining partial translations obtained by mimicking best-match partial translation examples. To fulfill the second requirement, this paper proposes using a Heterogeneous Computing Platform consisting of Multiple Instruction Multiple Data (MIMD) and Single Instruction Multiple Data (SIMD) parallel machines. Example-Based Approach is dominated by two processes, each of which is optimally accelerated by utilizing MIMD and SIMD, respectively, a) to build the source structure, and b) to retrieve the bestmatch examples. Experimental results show that Example-Based Approach is drastically speeded up with the Heterogeneous Computing Platform and has a performance sufficient for real-time response, even with a large vocabulary and a highly ambiguous sentence.",
        "id": 18569633
      },
      {
        "title": "Sentence-Permuted Paragraph Generation",
        "text": "Generating paragraphs of diverse contents is important in many applications. Existing generation models produce similar contents from homogenized contexts due to the fixed left-toright sentence order. Our idea is permuting the sentence orders to improve the content diversity of multi-sentence paragraph. We propose a novel framework PermGen whose objective is to maximize the expected log-likelihood of output paragraph distributions with respect to all possible sentence orders. PermGen uses hierarchical positional embedding and designs new procedures for both training phase and inference phase. Experiments on three paragraph generation benchmarks demonstrate Per-mGen generates more diverse outputs with a higher quality than existing models.",
        "id": 233240749
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "What recent research has been conducted on improving few-shot learning in pre-trained language models through the use of prompt-based fine tuning techniques?",
    "positive_ctxs": [
      {
        "title": "Making Pre-trained Language Models Better Few-shot Learners",
        "text": "The recent GPT-3 model(Brown et al., 2020)achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF-better few-shot fine-tuning of language models 1 -a suite of simple and complementary techniques for finetuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning. 2 * The first two authors contributed equally. 1 Alternatively, language models' best friends forever. 2 Our implementation is publicly available at https:// github.com/princeton-nlp/LM-BFF. Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Association for Computational Linguistics (ACL). Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Association for Computational Linguistics (ACL).",
        "id": 229923710
      }
    ],
    "negative_ctxs": [
      {
        "title": "Using Aspect Extraction Approaches to Generate Review Summaries and User Profiles",
        "text": "Reviews of products or services on Internet marketplace websites contain a rich amount of information. Users often wish to survey reviews or review snippets from the perspective of a certain aspect, which has resulted in a large body of work on aspect identification and extraction from such corpora. In this work, we evaluate a newly-proposed neural model for aspect extraction on two practical tasks. The first is to extract canonical sentences of various aspects from reviews, and is judged by human evaluators against alternatives. A kmeans baseline does remarkably well in this setting. The second experiment focuses on the suitability of the recovered aspect distributions to represent users by the reviews they have written. Through a set of review reranking experiments, we find that aspect-based profiles can largely capture notions of user preferences, by showing that divergent users generate markedly different review rankings. 1 words that co-occur with each other get mapped to points close to each other in the embedding space(Harris, 1968;Schütze, 1998).2 www.airbnb.com",
        "id": 13746285
      },
      {
        "title": "Faking it: Synthetic text-to-speech synthesis for under-resourced languages - Experimental design",
        "text": "Speech synthesis or text-to-speech (TTS) systems are currently available for a number of the world's major languages, but for thousands of the world's 'minor' languages no such technology is available. While awaiting the development of such technology, we would like to try the stop-gap solution of using an existing TTS system for a major language (the base language) to 'fake' TTS for a minor language (the target language). This paper describes the design for an experiment which involves finding a suitable base language for the Australian Aboriginal language Pitjantjajara as a target language, and evaluating its usability in the real-life situation of providing language technology support for speakers of the target language whose understanding of the local majority language is limited, for example in the scenario of going to the doctor.",
        "id": 15668195
      },
      {
        "title": "Knowledge Mining and Discovery for Searching in Literary Texts",
        "text": "The article describes a query system on texts and literary material with advanced information retrieval tools suitable to retrieve the content of a text, either as material specifically organized with respect to linguistic, stylistic and rethoric features, and in its historical, social and cultural context. As a test bed we chose the Dante's characters of al di là. This method of investigation should help a scholar of a literary text to realize part of his interpretative intentions. For this purpose, we will adopt advanced methodologies in knowledge management and knowledge discovery to be applied to a rich representation of the tagged content of a text.",
        "id": 9819513
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which paper first used language models to emulate tool executions for studying the risks of language model agents?",
    "positive_ctxs": [
      {
        "title": "Identifying the Risks of LM Agents with an LM-Emulated Sandbox",
        "text": "Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks-such as leaking private data or causing financial losses. Identifying these risks is labor-intensive, necessitating implementing the tools, setting up the environment for each test scenario manually, and finding risky cases. As tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tail risks. To address these challenges, we introduce ToolEmu: a framework that uses a LM to emulate tool execution and enables scalable testing of LM agents against a diverse range of tools and scenarios. Alongside the emulator, we develop an LM-based automatic safety evaluator that examines agent failures and quantifies associated risks. We test both the tool emulator and evaluator through human evaluation and find that 68.8% of failures identified with ToolEmu would be valid real-world agent failures. Using our curated initial benchmark consisting of 36 high-stakes tools and 144 test cases, we provide a quantitative risk analysis of current LM agents and identify numerous failures with potentially severe outcomes. Notably, even the safest LM agent exhibits such failures 23.9% of the time according to our evaluator, underscoring the need to develop safer LM agents for real-world deployment. 1 * Equal contribution. Contact {yjruan, honghuad}@cs.toronto.edu. 1 Project website, demo, and open-source code can be found at",
        "id": 262944419
      }
    ],
    "negative_ctxs": [
      {
        "title": "Learning Phrase Boundaries for Hierarchical Phrase-based Translation",
        "text": "Hierarchical phrase-based models provide a powerful mechanism to capture non-local phrase reorderings for statistical machine translation (SMT). However, many phrase reorderings are arbitrary because the models are weak on determining phrase boundaries for patternmatching. This paper presents a novel approach to learn phrase boundaries directly from word-aligned corpus without using any syntactical information. We use phrase boundaries, which indicate the beginning/ending of phrase reordering, as soft constraints for decoding. Experimental results and analysis show that the approach yields significant improvements over the baseline on large-scale Chineseto-English translation.",
        "id": 10105041
      },
      {
        "title": "Using Parallel Texts and Lexicons for Verbal Word Sense Disambiguation",
        "text": "We present a system for verbal Word Sense Disambiguation (WSD) that is able to exploit additional information from parallel texts and lexicons. It is an extension of our previous WSD method(Dušek et al., 2014), which gave promising results but used only monolingual features. In the follow-up work described here, we have explored two additional ideas: using English-Czech bilingual resources (as features only -the task itself remains a monolingual WSD task), and using a \"hybrid\" approach, adding features extracted both from a parallel corpus and from manually aligned bilingual valency lexicon entries, which contain subcategorization information. Albeit not all types of features proved useful, both ideas and additions have led to significant improvements for both languages explored.",
        "id": 8742160
      },
      {
        "title": "Towards Practical Semantic Interoperability in NLP Platforms",
        "text": "Interoperability is a necessity for the development of complex tasks that require the interconnection of several NLP services. This article presents the approaches that were adopted in three scenarios to address their respective interoperability issues. The first scenario describes the creation of a common REST API for a specific platform, the second scenario presents the interconnection of several platforms via mapping of different representation formats and the third scenario shows the complexities of interoperability through semantic schema mapping or automatic translation.",
        "id": 250164399
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research that examines how optimized continuous prompts perform against discrete prompts in relational tasks?",
    "positive_ctxs": [
      {
        "title": "Factual Probing Is [MASK]: Learning vs. Learning to Recall",
        "text": "Petroni et al. (2019)demonstrated that it is possible to retrieve world facts from a pretrained language model by expressing them as cloze-style prompts and interpret the model's prediction accuracy as a lower bound on the amount of factual information it encodes. Subsequent work has attempted to tighten the estimate by searching for better prompts, using a disjoint set of facts as training data. In this work, we make two complementary contributions to better understand these factual probing techniques. First, we propose OPTIPROMPT, a novel and efficient method which directly optimizes in continuous embedding space. We find this simple method is able to predict an additional 6.4% of facts in the LAMA benchmark. Second, we raise a more important question: Can we really interpret these probing results as a lower bound? Is it possible that these prompt-search methods learn from the training data too? We find, somewhat surprisingly, that the training data used by these methods contains certain regularities of the underlying fact distribution, and all the existing prompt methods, including ours, are able to exploit them for better fact prediction. We conduct a set of control experiments to disentangle \"learning\" from \"learning to recall\", providing a more detailed picture of what different prompts can reveal about pre-trained language models. 1",
        "id": 233210199
      }
    ],
    "negative_ctxs": [
      {
        "title": "PARADOCS : l'entremetteur de documents parallèles indépendant de la langue",
        "text": "Les corpus parallèles sont la pierre angulaire de plusieurs technologies de traduction automatique et des efforts conséquents sont régulièrement portés afin d'en réunir de nouveaux. L'expérience montre que la stratégie visant à réduire l'intervention manuelle dans cet exercice n'est jamais la même d'un corpus à l'autre. Ce constat nous a amené à développer PARADOCS, un entremetteur de documents parallèles qui utilise les entités numériques des documents afin de les apparier. Un classificateur est entraîné à décider des documents parallèles et un moteur de recherche d'information est utilisé afin de réduire l'espace de recherche des paires de documents parallèles. Nous montrons l'efficacité de PARADOCS sur de nombreuses tâches avec de nombreuses paires de langues.ABSTRACT. Parallel corpora are the bread and butter of a number of machine translation technologies. Therefore, important efforts are regularly spent in acquiring new ones. This task often involves a rather cumbersome manual inspection and it is rather difficult to set up a strategy that fits all the needs. We thus developed PARADOCS, a system aiming at doing this automatically. Our solution exploits numerical entities in documents in order to pair them. A classifier trained to recognize parallel text coupled to an information retrieval engine controlling the search space of candidate pairs are the main components of our approach. We tested PARADOCS on a number of tasks involving numerous pairs of languages and report good results. MOTS-CLÉS : corpus parallèles, recherche d'information, traduction automatique.",
        "id": 42870841
      },
      {
        "title": "Évaluation automatique de la satisfaction client à partir de conversations de type \"chat\" par réseaux de neurones récurrents avec mécanisme d'attention",
        "text": "Cet article présente des méthodes permettant l'évaluation de la satisfaction client à partir de très vastes corpus de conversation de type \"chat\" entre des clients et des opérateurs. Extraire des connaissances dans ce contexte demeure un défi pour les méthodes de traitement automatique des langues de par la dimension interactive et les propriétés de ce nouveau type de langage à l'intersection du langage écrit et parlé. Nous présentons une étude utilisant des réponses à des sondages utilisateurs comme supervision faible permettant de prédire la satisfaction des usagers d'un service en ligne d'assistance technique et commerciale.ABSTRACTCustomer satisfaction prediction with attention-based RNNs from a chat contact center corpus This paper presents methods to perform knowledge extraction from very large databases of WEB chat conversations between operators and clients in customer contact centers. Extracting knowledge from chat corpus is a challenging research issue. Simply applying traditional text mining tools is clearly sub-optimal as it takes into account neither the interaction dimension nor the particular nature of this language which shares properties of both spoken and written language. We present a method predicting users satisfaction in a chat-based service trained on answers from users to satisfaction surveys. Question Alias J'ai été accompagné(e) et j'ai eu les explications pour faire par moi-même Accompagnement J'ai été écouté(e) et ma demande a été prise en charge Ecoute J'ai été bien conseillé(e) Conseil La solution proposée par Orange me convient Solution Suite à votre contact avec le Service Clients, recommanderiez-vous Orange à vos proches ? RecommanderSi certaines questions portent directement sur l'interaction en elle même (\"Accompagnement\",",
        "id": 150982715
      },
      {
        "title": "Classifying Taxonomic Relations between Pairs of Wikipedia Articles",
        "text": "Natural language generation systems rely on taxonomic thesauri for tasks such as lexical choice and aggregation. WordNet is one such taxonomy, but it is limited in size. Motivated by the needs of a generation system in the scientific literature domain, we present a method for building a taxonomic thesaurus from Wikipedia articles, where each article represents a potential concept in the taxonomy. We propose framing the problem of creating a taxonomy as a classification task of the potential relations between individual Wikipedia article pairs, and show that a supervised algorithm can achieve high precision in this task with very little training data.",
        "id": 12591323
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there any paper that leverages graph neural network by integrating label information for multi-label low-resource intent classification?",
    "positive_ctxs": [
      {
        "title": "Dual Class Knowledge Propagation Network for Multi-label Few-shot Intent Detection",
        "text": "Multi-label intent detection aims to assign multiple labels to utterances and attracts increasing attention as a practical task in task-oriented dialogue systems. As dialogue domains change rapidly and new intents emerge fast, the lack of annotated data motivates multi-label few-shot intent detection. However, previous studies are confused by the identical representation of the utterance with multiple labels and overlook the intrinsic intra-class and inter-class interactions. To address these two limitations, we propose a novel dual class knowledge propagation network in this paper. In order to learn well-separated representations for utterances with multiple intents, we first introduce a labelsemantic augmentation module incorporating class name information. For better consideration of the inherent intra-class and inter-class relations, an instance-level and a class-level graph neural network are constructed, which not only propagate label information but also propagate feature structure. And we use a simple yet effective method to predict the intent count of each utterance. Extensive experimental results on two multi-label intent datasets have demonstrated that our proposed method outperforms strong baselines by a large margin.",
        "id": 259370656
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 227231501
      },
      {
        "title": "",
        "text": "",
        "id": 237204590
      },
      {
        "title": "Camtology: Intelligent Information Access for Science",
        "text": "We describe a novel semantic search engine for scientific literature. The Camtology system allows for sentence-level searches of PDF files and combines text and image searches, thus facilitating the retrieval of information present in tables and figures. It allows the user to generate complex queries for search terms that are related through particular grammatical/semantic relations in an intuitive manner. The system uses Grid processing to parallelise the analysis of large numbers of papers.",
        "id": 15415565
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that investigates employing graph attention techniques for integrating multiple modalities for identifying emotions?",
    "positive_ctxs": [
      {
        "title": "COGMEN: COntextualized GNN based Multimodal Emotion recognitioN",
        "text": "Emotions are an inherent part of human interactions, and consequently, it is imperative to develop AI systems that understand and recognize human emotions. During a conversation involving various people, a person's emotions are influenced by the other speaker's utterances and their own emotional state over the utterances. In this paper, we propose COntextualized Graph Neural Network based Multimodal Emotion recognitioN (COGMEN) system that leverages local information (i.e., inter/intra dependency between speakers) and global information (context). The proposed model uses Graph Neural Network (GNN) based architecture to model the complex dependencies (local and global information) in a conversation. Our model gives state-of-theart (SOTA) results on IEMOCAP and MOSEI datasets, and detailed ablation experiments show the importance of modeling information at both levels.",
        "id": 248524998
      }
    ],
    "negative_ctxs": [
      {
        "title": "Analyse des inférences pour la fouille d'opinion en chinois",
        "text": "La fouille d'opinion est une activité essentielle pour la veille économique, facilitée par les réseaux sociaux et forums dédiés. L'analyse repose généralement sur des lexiques de sentiments. Pourtant, certaines opinions sont exprimées au moyen d'inférences. Dans cet article, nous proposons une classification des inférences utilisées en chinois dans des commentaires touristiques, à des fins de fouille d'opinion, selon trois niveaux d'analyse (réalisation sémantique, modalité de réalisation, et mode de production). Nous démontrons l'intérêt d'analyser les différents types d'inférence pour déterminer la polarité des opinions exprimées en corpus. Nous présentons également de premiers résultats fondés sur des plongements lexicaux.ABSTRACTAnalysis of Inferences in Chinese for Opinion MiningOpinion mining is an essential activity for economic watch, made easier by social networks and ad hoc forums. The analysis generally relies on lexicon of sentiments. Nevertheless, some opinions are expressed through inferences. In this paper, we propose a classification of inferences used in Chinese in tourist comments, for an opinion mining task, based on three levels of analysis (semantic realization, modality of realization and production mode). We proved the interest to analyze the distinct types of inferences to identify the polarity of opinions expressed in corpora. We also present some results based on word embeddings. MOTS-CLÉS : Inférences, fouille d'opinion, polarité.",
        "id": 220835075
      },
      {
        "title": "Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations",
        "text": "Modern conversational AI systems support natural language understanding for a wide variety of capabilities. While a majority of these tasks can be accomplished using a simple and flat representation of intents and slots, more sophisticated capabilities require complex hierarchical representations supported by semantic parsing. State-of-the-art semantic parsers are trained using supervised learning with data labeled according to a hierarchical schema which might be costly to obtain or not readily available for a new domain. In this work, we explore the possibility of generating synthetic data for neural semantic parsing using a pretrained denoising sequence-to-sequence model (i.e., BART). Specifically, we first extract masked templates from the existing labeled utterances, and then fine-tune BART to generate synthetic utterances conditioning on the extracted templates. Finally, we use an auxiliary parser (AP) to filter the generated utterances. The AP guarantees the quality of the generated data. We show the potential of our approach when evaluating on the Facebook TOP dataset 1 for navigation domain.",
        "id": 226246180
      },
      {
        "title": "Natural Questions: A Benchmark for Question Answering Research",
        "text": "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.453",
        "id": 86611921
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Have any studies explored the creation of memory management systems in AI to improve sustained conversational capabilities and tackle the challenge of contextual retention over extended periods?",
    "positive_ctxs": [
      {
        "title": "Long Time No See! Open-Domain Conversation with Long-Term Persona Memory",
        "text": "Most of the open-domain dialogue models tend to perform poorly in the setting of long-term human-bot conversations. The possible reason is that they lack the capability of understanding and memorizing long-term dialogue history information. To address this issue, we present a novel task of Long-term Memory Conversation (LeMon) and then build a new dialogue dataset DuLeMon and a dialogue generation framework PLATO-LTM with a Long-Term Memory (LTM) mechanism. This LTM mechanism enables our system to accurately extract and continuously update long-term persona memory without requiring multiple-session dialogue datasets for model training. To our knowledge, this is the first attempt to conduct real-time dynamic management of persona information of both parties, including the user and the bot. Results on DuLeMon indicate that PLATO-LTM can significantly outperform baselines in terms of long-term dialogue consistency, leading to better dialogue engagingness 1 .",
        "id": 247411350
      }
    ],
    "negative_ctxs": [
      {
        "title": "Contextualized Sparse Representations for Real-Time Open-Domain Question Answering",
        "text": "Open-domain question answering can be formulated as a phrase retrieval problem, in which we can expect huge scalability and speed benefit but often suffer from low accuracy due to the limitation of existing phrase representation models. In this paper, we aim to improve the quality of each phrase embedding by augmenting it with a contextualized sparse representation (SPARC). Unlike previous sparse vectors that are term-frequencybased (e.g., tf-idf) or directly learned (only few thousand dimensions), we leverage rectified self-attention to indirectly learn sparse vectors in n-gram vocabulary space. By augmenting the previous phrase retrieval model(Seo et al., 2019)with SPARC, we show 4%+ improvement in CuratedTREC and SQuAD-Open. Our CuratedTREC score is even better than the best known retrieve & read model with at least 45x faster inference speed. 1",
        "id": 219058995
      },
      {
        "title": "End-to-End Neural Pipeline for Goal-Oriented Dialogue Systems using GPT-2",
        "text": "The goal-oriented dialogue system needs to be optimized for tracking the dialogue flow and carrying out an effective conversation under various situations to meet the user goal. The traditional approach to building such a dialogue system is to take a pipelined modular architecture, where its modules are optimized individually. However, such an optimization scheme does not necessarily yield an overall performance improvement of the whole system. On the other hand, end-to-end dialogue systems with monolithic neural architecture are often trained only with input-output utterances, without taking into account the entire annotations available in the corpus. This scheme makes it difficult for goal-oriented dialogues where the system needs to be integrated with external systems or to provide interpretable information about why the system generated a particular response. In this paper, we present an end-to-end neural architecture for dialogue systems that addresses both challenges above. Our dialogue system achieved the success rate of 68.32%, the language understanding score of 4.149, and the response appropriateness score of 4.287 in human evaluations, which ranked the system at the top position in the end-to-end multi-domain dialogue system task in the 8th dialogue systems technology challenge (DSTC8).",
        "id": 219719687
      },
      {
        "title": "BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent Summarization",
        "text": "Most existing text summarization datasets are compiled from the news domain, where summaries have a flattened discourse structure. In such datasets, summary-worthy content often appears in the beginning of input articles. Moreover, large segments from input articles are present verbatim in their respective summaries. These issues impede the learning and evaluation of systems that can understand an article's global content structure as well as produce abstractive summaries with high compression ratio. In this work, we present a novel dataset, BIGPATENT, consisting of 1.3 million records of U.S. patent documents along with human written abstractive summaries. Compared to existing summarization datasets, BIGPATENT has the following properties: i) summaries contain a richer discourse structure with more recurring entities, ii) salient content is evenly distributed in the input, and iii) lesser and shorter extractive fragments are present in the summaries. Finally, we train and evaluate baselines and popular learning models on BIGPATENT to shed light on new challenges and motivate future directions for summarization research.1 BIGPATENT dataset is available to download online at evasharma.github.io/bigpatent.",
        "id": 182953211
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Is there research that investigates embedding multi-bit data into watermarks to improve resilience to text corruption, particularly aimed at safeguarding keywords and syntactic elements from modification?",
    "positive_ctxs": [
      {
        "title": "Robust Multi-bit Natural Language Watermarking through Invariant Features",
        "text": "Recent years have witnessed a proliferation of valuable original natural language contents found in subscription-based media outlets, web novel platforms, and outputs of large language models. However, these contents are susceptible to illegal piracy and potential misuse without proper security measures. This calls for a secure watermarking system to guarantee copyright protection through leakage tracing or ownership identification. To effectively combat piracy and protect copyrights, a multi-bit watermarking framework should be able to embed adequate bits of information and extract the watermarks in a robust manner despite possible corruption. In this work, we explore ways to advance both payload and robustness by following a well-known proposition from image watermarking and identify features in natural language that are invariant to minor corruption. Through a systematic analysis of the possible sources of errors, we further propose a corruption-resistant infill model. Our full method improves upon the previous work on robustness by +16.8% point on average on four datasets, three corruption types, and two corruption ratios. 1",
        "id": 259129912
      }
    ],
    "negative_ctxs": [
      {
        "title": "Semi-automatic Annotation of Chinese Word Structure",
        "text": "Chinese word structure annotation is potentially useful for many NLP tasks, especially for Chinese word segmentation. Li and Zhou (2012) have presented an annotation for word structures in the Penn Chinese Treebank. But they only consider words that have productive affixes, which covers 35% of word types in that corpus. In this paper, we propose a linguistically inspired annotation that covers various morphological derivations of Chinese in a more general way, such that almost all multiple-character words can be structurally analyzed. As manual annotation is expensive, we propose a semi-supervised approach to automatic annotation, which combines the maximum entropy learning and the EM iteration for the Gaussian mixture model. The proposed method has achieved an accuracy of 90% on the testing set.",
        "id": 10002136
      },
      {
        "title": "Fostering the Next Generation of European Language Technology: Recent Developments -Emerging Initiatives -Challenges and Opportunities",
        "text": "META-NET is a European network of excellence, founded in 2010, that consists of 60 research centres in 34 European countries. One of the key visions and goals of META-NET is a truly multilingual Europe, which is substantially supported and realised through language technologies. In this article we provide an overview of recent developments around the multilingual Europe topic, we also describe recent and upcoming events as well as recent and upcoming strategy papers. Furthermore, we provide overviews of two new emerging initiatives, the CEF.AT and ELRC activity on the one hand and the Cracking the Language Barrier federation on the other. The paper closes with several suggested next steps in order to address the current challenges and to open up new opportunities.",
        "id": 27942273
      },
      {
        "title": "A Hybrid Hierarchical Model for Multi-Document Summarization",
        "text": "Scoring sentences in documents given abstract summaries created by humans is important in extractive multi-document summarization. In this paper, we formulate extractive summarization as a two step learning problem building a generative model for pattern discovery and a regression model for inference. We calculate scores for sentences in document clusters based on their latent characteristics using a hierarchical topic model. Then, using these scores, we train a regression model based on the lexical and structural characteristics of the sentences, and use the model to score sentences of new documents to form a summary. Our system advances current state-of-the-art improving ROUGE scores by ∼7%. Generated summaries are less redundant and more coherent based upon manual quality evaluations.",
        "id": 5833592
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research that investigates using autoencoder architectures to generate e-commerce product descriptions by integrating product titles, features, and supplementary descriptions crafted by marketers?",
    "positive_ctxs": [
      {
        "title": "Interactive Latent Knowledge Selection for E-commerce Product Copywriting Generation",
        "text": "As the multi-modal e-commerce is thriving, high-quality advertising product copywriting has gain more attentions, which plays a crucial role in the e-commerce recommender, advertising and even search platforms. The advertising product copywriting is able to enhance the user experience by highlighting the product's characteristics with textual descriptions and thus to improve the likelihood of user click and purchase. Automatically generating product copywriting has attracted noticeable interests from both academic and industrial communities, where existing solutions merely make use of a product's title and attribute information to generate its corresponding description. However, in addition to the product title and attributes, we observe that there are various auxiliary descriptions created by the shoppers or marketers in the ecommerce platforms (namely human knowledge), which contains valuable information for product copywriting generation, yet always accompanying lots of noises. In this work, we propose a novel solution to automatically generating product copywriting that involves all the title, attributes and denoised auxiliary knowledge. To be specific, we design an end-to-end generation framework equipped with two variational autoencoders that works interactively to select informative human knowledge and generate diverse copywriting. Experiments on real-world e-commerce product copywriting datasets demonstrate that our proposed method outperforms various baselines with regard to both automatic and human evaluation metrics.",
        "id": 248780467
      }
    ],
    "negative_ctxs": [
      {
        "title": "Parallel corpora for the Galician language: building and processing of the CLUVI (Linguistic Corpus of the University of Vigo)",
        "text": "In this paper, we present the methodology developed by the SLI (Computational Linguistics Group of the University of Vigo) for the building and processing of the CLUVI Corpus, showing the TMX-based XML specification designed to encode both morphosyntactic features and translation alignments in parallel corpora, and the solutions adopted for making the CLUVI parallel corpora freely available over the WWW",
        "id": 18658160
      },
      {
        "title": "TWEETQA: A Social Media Focused Question Answering Dataset",
        "text": "With social media becoming increasingly popular on which lots of news and real-time events are reported, developing automated question answering systems is critical to the effectiveness of many applications that rely on realtime knowledge. While previous datasets have concentrated on question answering (QA) for formal text like news and Wikipedia, we present the first large-scale dataset for QA over social media data. To ensure that the tweets we collected are useful, we only gather tweets used by journalists to write news articles. We then ask human annotators to write questions and answers upon these tweets. Unlike other QA datasets like SQuAD in which the answers are extractive, we allow the answers to be abstractive. We show that two recently proposed neural models that perform well on formal texts are limited in their performance when applied to our dataset. In addition, even the finetuned BERT model is still lagging behind human performance with a large margin. Our results thus point to the need of improved QA systems targeting social media text. 1",
        "id": 196174735
      },
      {
        "title": "Reranking Models in Fine-grained Opinion Analysis",
        "text": "We describe the implementation of reranking models for fine-grained opinion analysis -marking up opinion expressions and extracting opinion holders. The reranking approach makes it possible to model complex relations between multiple opinions in a sentence, allowing us to represent how opinions interact through the syntactic and semantic structure. We carried out evaluations on the MPQA corpus, and the experiments showed significant improvements over a conventional system that only uses local information: for both tasks, our system saw recall boosts of over 10 points.",
        "id": 15851196
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Which corpora are frequently used in research to benchmark English readability assessment tools?",
    "positive_ctxs": [
      {
        "title": "The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 163-173, On Improving the Accuracy of Readability Classification using Insights from Second Language Acquisition",
        "text": "We investigate the problem of readability assessment using a range of lexical and syntactic features and study their impact on predicting the grade level of texts. As empirical basis, we combined two web-based text sources, Weekly Reader and BBC Bitesize, targeting different age groups, to cover a broad range of school grades. On the conceptual side, we explore the use of lexical and syntactic measures originally designed to measure language development in the production of second language learners. We show that the developmental measures from Second Language Acquisition (SLA) research when combined with traditional readability features such as word length and sentence length provide a good indication of text readability across different grades. The resulting classifiers significantly outperform the previous approaches on readability classification, reaching a classification accuracy of 93.3%.",
        "id": 10919200
      },
      {
        "title": "OneStopEnglish corpus: A new corpus for automatic readability assessment and text simplification",
        "text": "This paper describes the collection and compilation of the OneStopEnglish corpus of texts written at three reading levels, and demonstrates its usefulness for through two applications -automatic readability assessment and automatic text simplification. The corpus consists of 189 texts, each in three versions (567 in total). The corpus is now freely available under a CC by-SA 4.0 license 1 and we hope that it would foster further research on the topics of readability assessment and text simplification.",
        "id": 46937952
      }
    ],
    "negative_ctxs": [
      {
        "title": "Evaluating Agent Interactions Through Episodic Knowledge Graphs",
        "text": "We present a new method based on episodic Knowledge Graphs (eKGs) for evaluating (multimodal)  conversational agents in open domains. This graph is generated by interpreting raw signals during conversation and is able to capture the accumulation of knowledge over time. We apply structural and semantic analysis of the resulting graphs and translate the properties into qualitative measures. We compare these measures with existing automatic and manual evaluation metrics commonly used for conversational agents. Our results show that our Knowledge-Graph-based evaluation provides more qualitative insights into interaction and the agent's behavior.",
        "id": 252519245
      },
      {
        "title": "Multi-dimensional abstractness in cross-domain mappings",
        "text": "Metaphor is a cognitive process that shapes abstract target concepts by mapping them to concrete source concepts. Thus, many computational approaches to metaphor make reference, directly or indirectly, to the abstractness of words and concepts. The property of abstractness, however, remains theoretically and empirically unexplored. This paper implements a multi-dimensional definition of abstractness and tests the usefulness of each dimension for detecting cross-domain mappings.",
        "id": 14826968
      },
      {
        "title": "Colors in Context: A Pragmatic Neural Model for Grounded Language Understanding",
        "text": "We present a model of pragmatic referring expression interpretation in a grounded communication task (identifying colors from descriptions) that draws upon predictions from two recurrent neural network classifiers, a speaker and a listener, unified by a recursive pragmatic reasoning framework. Experiments show that this combined pragmatic model interprets color descriptions more accurately than the classifiers from which it is built, and that much of this improvement results from combining the speaker and listener perspectives. We observe that pragmatic reasoning helps primarily in the hardest cases: when the model must distinguish very similar colors, or when few utterances adequately express the target color. Our findings make use of a newly-collected corpus of human utterances in color reference games, which exhibit a variety of pragmatic behaviors. We also show that the embedded speaker model reproduces many of these pragmatic behaviors.",
        "id": 1854889
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Is there a dataset available for open-domain targeted sentiment analysis containing user reviews from platforms like Yelp and Amazon?",
    "positive_ctxs": [
      {
        "title": "YASO: A Targeted Sentiment Analysis Evaluation Dataset for Open-Domain Reviews",
        "text": "Current TSA evaluation in a cross-domain setup is restricted to the small set of review domains available in existing datasets. Such an evaluation is limited, and may not reflect true performance on sites like Amazon or Yelp that host diverse reviews from many domains. To address this gap, we present YASO -a new TSA evaluation dataset of open-domain user reviews. YASO contains 2215 English sentences from dozens of review domains, annotated with target terms and their sentiment. Our analysis verifies the reliability of these annotations, and explores the characteristics of the collected data. Benchmark results using five contemporary TSA systems show there is ample room for improvement on this challenging new dataset. YASO is available at github.com/IBM/yaso-tsa. 1 E.g. Yelp has more than 1,200 business categories here. 2  The name is an acronym of the data sources.",
        "id": 237497416
      }
    ],
    "negative_ctxs": [
      {
        "title": "'Am I the Bad One'? Predicting the Moral Judgement of the Crowd Using Pre-trained Language Models",
        "text": "Natural language processing (NLP) has been shown to perform well in various tasks, such as answering questions, ascertaining natural language inference and anomaly detection. However, there are few NLP-related studies that touch upon the moral context conveyed in text. This paper studies whether state-of-the-art, pre-trained language models are capable of passing moral judgments on posts retrieved from a popular Reddit user board. Reddit is a social discussion website and forum where posts are promoted by users through a voting system. In this work, we construct a dataset that can be used for moral judgement tasks by collecting data from the AITA? (Am I the A*******?) subreddit. To model our task, we harnessed the power of pre-trained language models, including BERT, RoBERTa, RoBERTa-large, ALBERT and Longformer. We then fine-tuned these models and evaluated their ability to predict the correct verdict as judged by users for each post in the datasets. RoBERTa showed relative improvements across the three datasets, exhibiting a rate of 87% accuracy and a Matthews correlation coefficient (MCC) of 0.76, while the use of the Longformer model slightly improved the performance when used with longer sequences, achieving 87% accuracy and 0.77 MCC.",
        "id": 252411574
      },
      {
        "title": "Published as a conference paper at ICLR 2020 SHIFTED AND SQUEEZED 8-BIT FLOATING POINT FOR- MAT FOR LOW-PRECISION TRAINING OF DEEP NEU- RAL NETWORKS",
        "text": "Training with larger number of parameters while keeping fast iterations is an increasingly adopted strategy and trend for developing better performing Deep Neural Network (DNN) models. This necessitates increased memory footprint and computational requirements for training. Here we introduce a novel methodology for training deep neural networks using 8-bit floating point (FP8) numbers. Reduced bit precision allows for a larger effective memory and increased computational speed. We name this method Shifted and Squeezed FP8 (S2FP8). We show that, unlike previous 8-bit precision training methods, the proposed method works out-of-the-box for representative models: ResNet-50, Transformer and NCF. The method can maintain model accuracy without requiring fine-tuning loss scaling parameters or keeping certain layers in single precision. We introduce two learnable statistics of the DNN tensors -shifted and squeezed factors that are used to optimally adjust the range of the tensors in 8-bits, thus minimizing the loss in information due to quantization. * Work performed during an internship at Intel † Equal contribution arXiv:2001.05674v1 [cs.LG]",
        "id": 210699231
      },
      {
        "title": "Referring Expression Generation in time-constrained communication",
        "text": "In game-like applications and many others, an underlying Natural Language Generation system may have to express urgency or other dynamic aspects of a fast-evolving situation as text, which may be considerably different from text produced under so-called 'normal' circumstances (e.g., without time constrains). As a means to shed light on possible differences of this kind, this paper addresses the computational generation of natural language text in time-constrained communication by presenting two experiments that use the attribute selection task of definite descriptions (or Referring Expression Generation -REG) as a working example. In the first experiment, we describe a psycholinguistic study in which human participants are engaged in a time-constrained reference production task. This results in a corpus of time-constrained descriptions to be compared with 'normal' descriptions available from an existing (i.e., with no time constraint) REG corpus. In the second experiment, we discuss how a REG algorithm may be customised so as to produce time-constrained descriptions that resemble those produced by human speakers in similar situations. The proposed algorithm is then evaluated against the time-constrained descriptions produced by the human subjects in the first experiment, and it is shown to outperform standard approaches to REG in these conditions.",
        "id": 21697022
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Is there a study that investigates if large language models can assist with generating ideas pertinent to scientific concepts during the scientific writing ideation stage?",
    "positive_ctxs": [
      {
        "title": "Sparks: Inspiration for Science Writing using Language Models",
        "text": "Large-scale language models are rapidly improving, performing well on a variety of tasks with little to no customization. In this work we investigate how language models can support science writing, a challenging writing task that is both open-ended and highly constrained. We present a system for generating \"sparks\", sentences related to a scientific concept intended to inspire writers. We run a user study with 13 STEM graduate students and find three main use cases of sparks-inspiration, translation, and perspective-each of which correlates with a unique interaction pattern. We also find that while participants were more likely to select higher quality sparks, the overall quality of sparks seen by a given participant did not correlate with their satisfaction with the tool. 1",
        "id": 239009871
      }
    ],
    "negative_ctxs": [
      {
        "title": "Fast Interleaved Bidirectional Sequence Generation",
        "text": "Independence assumptions during sequence generation can speed up inference, but parallel generation of highly inter-dependent tokens comes at a cost in quality. Instead of assuming independence between neighbouring tokens (semi-autoregressive decoding, SA), we take inspiration from bidirectional sequence generation and introduce a decoder that generates target words from the left-to-right and right-toleft directions simultaneously. We show that we can easily convert a standard architecture for unidirectional decoding into a bidirectional decoder by simply interleaving the two directions and adapting the word positions and selfattention masks. Our interleaved bidirectional decoder (IBDecoder) retains the model simplicity and training efficiency of the standard Transformer, and on five machine translation tasks and two document summarization tasks, achieves a decoding speedup of ∼2× compared to autoregressive decoding with comparable quality. Notably, it outperforms left-toright SA because the independence assumptions in IBDecoder are more felicitous. To achieve even higher speedups, we explore hybrid models where we either simultaneously predict multiple neighbouring tokens per direction, or perform multi-directional decoding by partitioning the target sequence. These methods achieve speedups to 4×-11× across different tasks at the cost of <1 BLEU or <0.5 ROUGE (on average). 1",
        "id": 225075957
      },
      {
        "title": "Semantic Web based Machine Translation",
        "text": "This paper describes the experimental combination of traditional Natural Language Processing (NLP) technology with the Semantic Web building stack in order to extend the expert knowledge required for a Machine Translation (MT) task. Therefore, we first give a short introduction in the state of the art of MT and the Semantic Web and discuss the problem of disambiguation being one of the common challenges in MT which can only be solved using world knowledge during the disambiguation process. In the following, we construct a sample sentence which demonstrates the need for world knowledge and design a prototypical program as a successful solution for the outlined translation problem. We conclude with a critical view on the developed approach.",
        "id": 10349161
      },
      {
        "title": "Automatic annotation of speculation in biomedical texts: new perspec- tives and large-scale evaluation",
        "text": "One emergent field in text mining tools applied to biological texts is the automatic detection of speculative sentences. In this paper, we test on a large scale BioExcom, a rule-based system which annotates and categorizes automatically speculative sentences (\"prior\" and \"new\"). This work enables us to highlight a more restrictive way to consider speculations, viewed as a source of knowledge, and to discuss the criteria used to determine if a sentence is speculative or not. By doing so, we demonstrate the efficiency of BioExcom to extract these types of speculations and we argue the importance of this tool for biologists, who are also interested in finding hypotheses.",
        "id": 16956988
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Could you provide me with a reference that discusses the development of classifiers for suicide risk detection in a low-resource language, with a specific focus on using explicit suicide-related terminology?",
    "positive_ctxs": [
      {
        "title": "Detecting Suicide Risk in Online Counseling Services: A Study in a Low-Resource Language",
        "text": "With the increased awareness of situations of mental crisis and their societal impact, online services providing emergency support are becoming commonplace in many countries. Computational models, trained on discussions between help-seekers and providers, can support suicide prevention by identifying at-risk individuals. However, the lack of domain-specific models, especially in low-resource languages, poses a significant challenge for the automatic detection of suicide risk. We propose a model that combines pre-trained language models (PLM) with a fixed set of manually crafted (and clinically approved) set of suicidal cues, followed by a two-stage fine-tuning process. Our model achieves 0.91 ROC-AUC and an F2-score of 0.55, significantly outperforming an array of strong baselines even early on in the conversation, which is critical for real-time detection in the field. Moreover, the model performs well across genders and age groups.",
        "id": 252199533
      }
    ],
    "negative_ctxs": [
      {
        "title": "Relation Schema Induction using Tensor Factorization with Side Information",
        "text": "Given a set of documents from a specific domain (e.g., medical research journals), how do we automatically build a Knowledge Graph (KG) for that domain? Automatic identification of relations and their schemas, i.e., type signature of arguments of relations (e.g., undergo(Patient, Surgery)), is an important first step towards this goal. We refer to this problem as Relation Schema Induction (RSI). In this paper, we propose Schema Induction using Coupled Tensor Factorization (SICTF), a novel tensor factorization method for relation schema induction. SICTF factorizes Open Information Extraction (OpenIE) triples extracted from a domain corpus along with additional side information in a principled way to induce relation schemas. To the best of our knowledge, this is the first application of tensor factorization for the RSI problem. Through extensive experiments on multiple real-world datasets, we find that SICTF is not only more accurate than state-of-the-art baselines, but also significantly faster (about 14x faster).",
        "id": 12051021
      },
      {
        "title": "",
        "text": "",
        "id": 204793182
      },
      {
        "title": "Simple Semi-supervised Dependency Parsing",
        "text": "We present a simple and effective semisupervised method for training dependency parsers. We focus on the problem of lexical representation, introducing features that incorporate word clusters derived from a large unannotated corpus. We demonstrate the effectiveness of the approach in a series of dependency parsing experiments on the Penn Treebank and Prague Dependency Treebank, and we show that the cluster-based features yield substantial gains in performance across a wide range of conditions. For example, in the case of English unlabeled second-order parsing, we improve from a baseline accuracy of 92.02% to 93.16%, and in the case of Czech unlabeled second-order parsing, we improve from a baseline accuracy of 86.13% to 87.13%. In addition, we demonstrate that our method also improves performance when small amounts of training data are available, and can roughly halve the amount of supervised data required to reach a desired level of performance.",
        "id": 1916754
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "What resources or toolkits are available to facilitate prompt-based learning model development in PyTorch?",
    "positive_ctxs": [
      {
        "title": "OpenPrompt: An Open-source Framework for Prompt-learning",
        "text": "Prompt-learning has become a new paradigm in modern natural language processing, which directly adapts pre-trained language models (PLMs) to cloze-style prediction, autoregressive modeling, or sequence to sequence generation, resulting in promising performances on various tasks. However, no standard implementation framework of prompt-learning is proposed yet, and most existing promptlearning codebases, often unregulated, only provide limited implementations for specific scenarios. Since there are many details such as templating strategy, initializing strategy, and verbalizing strategy, etc., need to be considered in prompt-learning, practitioners face impediments to quickly adapting the desired prompt learning methods to their applications. In this paper, we present Open-Prompt, a unified easy-to-use toolkit to conduct prompt-learning over PLMs.Open-Prompt is a research-friendly framework that is equipped with efficiency, modularity, and extendibility, and its combinability allows the freedom to combine different PLMs, task formats, and prompting modules in a unified paradigm. Users could expediently deploy prompt-learning frameworks and evaluate the generalization of them on different NLP tasks without constraints. 1",
        "id": 241033259
      }
    ],
    "negative_ctxs": [
      {
        "title": "Robust Domain Adaptation for Relation Extraction via Clustering Consistency",
        "text": "We propose a two-phase framework to adapt existing relation extraction classifiers to extract relations for new target domains. We address two challenges: negative transfer when knowledge in source domains is used without considering the differences in relation distributions; and lack of adequate labeled samples for rarer relations in the new domain, due to a small labeled data set and imbalance relation distributions. Our framework leverages on both labeled and unlabeled data in the target domain. First, we determine the relevance of each source domain to the target domain for each relation type, using the consistency between the clustering given by the target domain labels and the clustering given by the predictors trained for the source domain. To overcome the lack of labeled samples for rarer relations, these clusterings operate on both the labeled and unlabeled data in the target domain. Second, we trade-off between using relevance-weighted sourcedomain predictors and the labeled target data. Again, to overcome the imbalance distribution, the source-domain predictors operate on the unlabeled target data. Our method outperforms numerous baselines and a weakly-supervised relation extraction method on ACE 2004 and YAGO. * The work is done while Nguyen was a research staff in Nanyang Technological University, Singapore.",
        "id": 17856659
      },
      {
        "title": "Detecting dementia in Mandarin Chinese using transfer learning from a parallel corpus",
        "text": "Machine learning has shown promise for automatic detection of Alzheimer's disease (AD) through speech; however, efforts are hampered by a scarcity of data, especially in languages other than English. We propose a method to learn a correspondence between independently engineered lexicosyntactic features in two languages, using a large parallel corpus of outof-domain movie dialogue data. We apply it to dementia detection in Mandarin Chinese, and demonstrate that our method outperforms both unilingual and machine translation-based baselines. This appears to be the first study that transfers feature domains in detecting cognitive decline.",
        "id": 67856188
      },
      {
        "title": "Improving Translation Selection with a New Translation Model Trained by Independent Monolingual Corpora",
        "text": "We propose a novel statistical translation model to improve translation selection of collocation. In the statistical approach that has been popularly applied for translation selection, bilingual corpora are used to train the translation model. However, there exists a formidable bottleneck in acquiring large-scale bilingual corpora, in particular for language pairs involving Chinese. In this paper, we propose a new approach to training the translation model by using unrelated monolingual corpora. First, a Chinese corpus and an English corpus are parsed with dependency parsers, respectively, and two dependency triple databases are generated. Then, the similarity between a Chinese word and an English word can be estimated using the two monolingual dependency triple databases with the help of a simple Chinese-English dictionary. This cross-language word similarity is used to simulate the word translation probability. Finally, the generated translation model is used together with the language model trained with the English dependency database to realize translation of Chinese collocations into English. To demonstrate the effectiveness of this method, we performed various experiments with verb-object collocation translation. The experiments produced very promising results.Section 5, some related works are introduced. Finally in Section 6, we draw conclusions and discuss future work.A New Statistical Machine Translation ModelIn this section, we will describe the proposed translation model. First, we will report our observations from a sample word-aligned bilingual corpus in order to verify our assumption. After that, we will introduce the method for estimating the cross-language word similarity by means of two monolingual corpora. Finally, we will give a formal description of the new translation model.",
        "id": 2456265
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research that assesses how well large language models, such as GPT-3, perform at coreference resolution when tested in a few-shot learning context?",
    "positive_ctxs": [
      {
        "title": "Large Language Models are Few-Shot Clinical Information Extractors",
        "text": "A long-running goal of the clinical NLP community is the extraction of important variables trapped in clinical notes. However, roadblocks have included dataset shift from the general domain and a lack of public clinical corpora and annotations. In this work, we show that large language models, such as InstructGPT (Ouyang et al., 2022), perform well at zero-and few-shot information extraction from clinical text despite not being trained specifically for the clinical domain. Whereas text classification and generation performance have already been studied extensively in such models, here we additionally demonstrate how to leverage them to tackle a diverse set of NLP tasks which require more structured outputs, including span identification, token-level sequence classification, and relation extraction. Further, due to the dearth of available data to evaluate these systems, we introduce new datasets for benchmarking fewshot clinical information extraction based on a manual re-annotation of the CASI dataset (Moon et al., 2014) for new tasks 1 . On the clinical extraction tasks we studied, the GPT-3 systems significantly outperform existing zeroand few-shot baselines. . 2021. Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare (HEALTH), 3(1):1-23.A study of machine-learning-based approaches to extract clinical entities and their assertions from discharge summaries. improves prompt-based learning for large language models. arXiv preprint arXiv:2202.00828.Hunter Lang, Aravindan Vijayaraghavan, and David Sontag. 2022b. Training subset selection for weak supervision. arXiv preprint arXiv:2206.02914. . 2021. Assessing the impact of automated suggestions on decision making: Domain experts mediate model errors but take less initiative. . 2019b. Entity-relation extraction as multi-turn question answering. arXiv preprint arXiv:1905.05529. . 2022a. Qaner: Prompting question answering models for fewshot named entity recognition. arXiv preprint arXiv:2203.01543. Raffel. 2022b. Few-shot parameter-efficient finetuning is better and cheaper than in-context learning. arXiv preprint arXiv:2205.05638.",
        "id": 249062918
      }
    ],
    "negative_ctxs": [
      {
        "title": "A PARADIGM-BASED MORPHOLOGICAL ANALYZER",
        "text": "",
        "id": 34504645
      },
      {
        "title": "Automatic Detection of Machine Generated Text: A Critical Survey",
        "text": "Text generative models (TGMs) excel in producing text that matches the style of human language reasonably well. Such TGMs can be misused by adversaries, e.g., by automatically generating fake news and fake product reviews that can look authentic and fool humans. Detectors that can distinguish text generated by TGM from human written text play a vital role in mitigating such misuse of TGMs. Recently, there has been a flurry of works from both natural language processing (NLP) and machine learning (ML) communities to build accurate detectors for English. Despite the importance of this problem, there is currently no work that surveys this fast-growing literature and introduces newcomers to important research challenges. In this work, we fill this void by providing a critical survey and review of this literature to facilitate a comprehensive understanding of this problem. We conduct an in-depth error analysis of the state-of-the-art detector and discuss research directions to guide future work in this exciting area.",
        "id": 226237099
      },
      {
        "title": "Graph Enhanced Dual Attention Network for Document-Level Relation Extraction",
        "text": "Document-level relation extraction requires inter-sentence reasoning capabilities to capture local and global contextual information for multiple relational facts. To improve inter-sentence reasoning, we propose to characterize the complex interaction between sentences and potential relation instances via a Graph Enhanced Dual Attention network (GEDA). In GEDA, sentence representation generated by the sentence-to-relation (S2R) attention is refined and synthesized by a Heterogeneous Graph Convolutional Network before being fed into the relation-to-sentence (R2S) attention . We further design a simple yet effective regularizer based on the natural duality of the S2R and R2S attention, whose weights are also supervised by the supporting evidence of relation instances during training. An extensive set of experiments on an existing large-scale dataset show that our model achieves competitive performance, especially for the inter-sentence relation extraction, while the neural predictions can also be interpretable and easily observed. * Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/.",
        "id": 227230619
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there any paper that uses token-level loss to enhance sentence-level embedding learning?",
    "positive_ctxs": [
      {
        "title": "Dual-Alignment Pre-training for Cross-lingual Sentence Embedding",
        "text": "Recent studies have shown that dual encoder models trained with the sentence-level translation ranking task are effective methods for cross-lingual sentence embedding. However, our research indicates that token-level alignment is also crucial in multilingual scenarios, which has not been fully explored previously. Based on our findings, we propose a dual-alignment pre-training (DAP) framework for cross-lingual sentence embedding that incorporates both sentence-level and token-level alignment. To achieve this, we introduce a novel representation translation learning (RTL) task, where the model learns to use one-side contextualized token representation to reconstruct its translation counterpart. This reconstruction objective encourages the model to embed translation information into the token representation. Compared to other token-level alignment methods such as translation language modeling, RTL is more suitable for dual encoder architectures and is computationally efficient. Extensive experiments on three sentencelevel cross-lingual benchmarks demonstrate that our approach can significantly improve sentence embedding. Our code is available at https://github.com/ChillingDream/DAP.",
        "id": 258715255
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 219302229
      },
      {
        "title": "Analyse formelle d'exigences en langue naturelle pour la conception de systèmes cyber-physiques",
        "text": "Cet article explore la construction de représentations formelles d'énoncés en langue naturelle. Le passage d'un langage naturel à une représentation logique est réalisé avec un formalisme grammatical, reliant l'analyse syntaxique de l'énoncé à une représentation sémantique. Nous ciblons l'aspect comportemental des cahiers des charges pour les systèmes cyber-physiques, c'est-à-dire tout type de systèmes dans lesquels des composants logiciels interagissent étroitement avec un environnement physique. Dans ce cadre, l'enjeu serait d'apporter une aide au concepteur. Il s'agit de permettre de simuler et vérifier, par des méthodes automatiques ou assistées, des cahiers des charges \"systèmes\" exprimés en langue naturelle. Cet article présente des solutions existantes qui pourraient être combinées en vue de la résolution de la problématique exposée.ABSTRACTFormal analysis of natural language requirements for the design of cyber-physical systemsThis paper focuses on the construction of formal representations of natural language texts. The mapping from a natural language to a logical representation is realized with a grammatical formalism, linking the syntactic analysis of the text to a semantic representation. We target the behavioral aspect of the specifications for cyber-physical systems, ie any type of system in which software components interact closely with a physical environment. In this way, the challenge would be to provide assistance to the designer. So, we could simulate and verify, by automatic or assisted methods, \"systems\" specifications expressed in natural language. This paper presents some existing contributions that could enable progress on this issue.",
        "id": 53090141
      },
      {
        "title": "Verbframator: Semi-Automatic Verb Frame Annotator Tool with Special Reference to Marathi",
        "text": "The sentence is incomplete without a verb in a language. A verb is majorly responsible for giving the meaning to a sentence. Any sentence can be represented in the form of a verb frame. Verb frames are mainly developed as a knowledge resource which can be used in various semantic level Natural Language Processing (NLP) activities. This paper presents the Verbframator -a verb frame annotator tool which automatically extracts and generates verb frames of example sentences from Marathi wordnet. It also helps in generating Shakti Standard Format (SSF) files of the given example sentences. The generated verb frames and SSF files can be used in the dependency tree banking and other NLP applications like machine translation, paraphrasing, natural language generation, etc.",
        "id": 17645754
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Provide an example of a paper which proposes a method to learn a dynamic (conditioned on the input) sequence tokenizer (segmenter) via standard gradient backpropagation.",
    "positive_ctxs": [
      {
        "title": "Efficient Transformers with Dynamic Token Pooling",
        "text": "Transformers achieve unrivalled performance in modelling language, but remain inefficient in terms of memory and time complexity. A possible remedy is to reduce the sequence length in the intermediate layers by pooling fixed-length segments of tokens. Nevertheless, natural units of meaning, such as words or phrases, display varying sizes. To address this mismatch, we equip language models with a dynamic-pooling mechanism, which predicts segment boundaries in an autoregressive fashion. We compare several methods to infer boundaries, including end-to-end learning through stochastic re-parameterisation, supervised learning (based on segmentations from subword tokenizers or spikes in conditional entropy), as well as linguistically motivated boundaries. We perform character-level evaluation on texts from multiple datasets and morphologically diverse languages. The results demonstrate that dynamic pooling, which jointly segments and models language, is both faster and more accurate than vanilla Transformers and fixed-length pooling within the same computational budget.",
        "id": 253581399
      }
    ],
    "negative_ctxs": [
      {
        "title": "Semi-Supervised Policy Initialization for Playing Games with Language Hints",
        "text": "Using natural language as a hint can supply an additional reward for playing sparse-reward games. Achieving a goal should involve several different hints, while the given hints are usually incomplete. Those unmentioned latent hints still rely on the sparse reward signal, and make the learning process difficult. In this paper, we propose semi-supervised initialization (SSI) that allows the agent to learn from various possible hints before training under different tasks. Experiments show that SSI not only helps to learn faster (1.2x) but also has a higher success rate (11% relative improvement) of the final policy.",
        "id": 235097328
      },
      {
        "title": "Guided Alignment Training for Topic-Aware Neural Machine Translation",
        "text": "In this paper, we propose an effective way for biasing the attention mechanism of a sequence-to-sequence neural machine translation (NMT) model towards the well-studied statistical word alignment models. We show that our novel guided alignment training approach improves translation quality on real-life ecommerce texts consisting of product titles and descriptions, overcoming the problems posed by many unknown words and a large type/token ratio. We also show that meta-data associated with input texts such as topic or category information can significantly improve translation quality when used as an additional signal to the decoder part of the network. With both novel features, the BLEU score of the NMT system on a product title set improves from 18.6 to 21.3%. Even larger MT quality gains are obtained through domain adaptation of a general domain NMT system to e-commerce data. The developed NMT system also performs well on the IWSLT speech translation task, where an ensemble of four variant systems outperforms the phrase-based baseline by 2.1% BLEU absolute.",
        "id": 17078659
      },
      {
        "title": "Persona or Context? Towards Building Context adaptive Personalized Persuasive Virtual Sales Assistant",
        "text": "Task-oriented conversational agents are gaining immense popularity and success in a wide range of tasks, from flight ticket booking to online shopping. However, the existing systems presume that end-users will always have a pre-determined and servable task goal, which results in dialogue failure in hostile scenarios, such as goal unavailability. On the other hand, human agents accomplish users' tasks even in a large number of goal unavailability scenarios by persuading them towards a very similar and servable goal. Motivated by the limitation, we propose and build a novel end-to-end multi-modal persuasive dialogue system incorporated with a personalized persuasive module aided goal controller and goal persuader. The goal controller recognizes goal conflicting/unavailability scenarios and formulates a new goal, while the goal persuader persuades users using a personalized persuasive strategy identified through dialogue context. We also present a novel automatic evaluation metric called Persuasiveness Measurement Rate (PMeR) for quantifying the persuasive capability of a conversational agent. The obtained improvements (both quantitative and qualitative) firmly establish the superiority and need of the proposed context-guided, personalized persuasive virtual agent over existing traditional task-oriented virtual agents. Furthermore, we also curated a multi-modal persuasive conversational dialogue corpus annotated with intent, slot, sentiment, and dialogue act for e-commerce domain 1 .",
        "id": 253481040
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there any paper that proposes a new multimodal video dataset that image-level multimodal models do not work well?",
    "positive_ctxs": [
      {
        "title": "Revealing Single Frame Bias for Video-and-Language Learning",
        "text": "Training an effective video-and-language model intuitively requires multiple frames as model inputs. However, it is unclear whether using multiple frames is beneficial to downstream tasks, and if yes, whether the performance gain is worth the drastically-increased computation and memory costs resulting from using more frames. In this work, we explore single-frame models for video-and-language learning. On a diverse set of video-andlanguage tasks (including text-to-video retrieval and video question answering), we show the surprising result that, with large-scale pretraining and a proper frame ensemble strategy at inference time, a single-frame trained model that does not consider temporal information can achieve better performance than existing methods that use multiple frames for training. This result reveals the existence of a strong \"static appearance bias\" in popular video-andlanguage datasets. Therefore, to allow for a more comprehensive evaluation of videoand-language models, we propose two new retrieval tasks based on existing fine-grained action recognition datasets that encourage temporal modeling. Our code is available at https: //github.com/jayleicn/singularity.",
        "id": 249431866
      }
    ],
    "negative_ctxs": [
      {
        "title": "Do \"Transitive Adjectives\" Really Exist?",
        "text": "I argue that the so-called psychological predicates like komapta 'thankful,' mwusepta 'fearful,' silhtà loathsome,' or kulipta 'missing' require a nominative subject and a locative or dative complement, challenging the claim, a conventional wisdom originated fromKuno(1973), that they are two-place \"transitive adjectives\" requiring a nominative direct object. I also show that those adjectives are subject to having the locative-dative complement extracted, which is ultimately realized as a focused subject or a topic. Thus, in this type of double nominative constructions, the first nominative is a focused subject, and the second nominative forms an embedded clause with the psychological predicate, which functions as the predicate of the whole sentence.",
        "id": 17911959
      },
      {
        "title": "A Lexical Resource of Hebrew Verb-Noun Multi-Word Expressions",
        "text": "A verb-noun Multi-Word Expression (MWE) is a combination of a verb and a noun with or without other words, in which the combination has a meaning different from the meaning of the words considered separately. In this paper, we present a new lexical resource of Hebrew Verb-Noun MWEs (VN-MWEs). The VN-MWEs of this resource were manually collected and annotated from five different web resources. In addition, we analyze the lexical properties of Hebrew VN-MWEs by classifying them to three types: morphological, syntactic, and semantic. These two contributions are essential for designing algorithms for automatic VN-MWEs extraction. The analysis suggests some interesting features of VN-MWEs for exploration. The lexical resource enables to sample a set of positive examples for Hebrew VN-MWEs. This set of examples can either be used for training supervised algorithms or as seeds in unsupervised bootstrapping algorithms. Thus, this resource is a first step towards automatic identification of Hebrew VN-MWEs, which is important for natural language understanding, generation and translation systems.",
        "id": 39149029
      },
      {
        "title": "",
        "text": "",
        "id": 219300132
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest a study that explores a cohesive pre-training method for code representation learning across different modalities?",
    "positive_ctxs": [
      {
        "title": "UniXcoder: Unified Cross-Modal Pre-training for Code Representation",
        "text": "Pre-trained models for programming languages have recently demonstrated great success on code intelligence. To support both code-related understanding and generation tasks, recent works attempt to pre-train unified encoder-decoder models. However, such encoder-decoder framework is sub-optimal for auto-regressive tasks, especially code completion that requires a decoder-only manner for efficient inference. In this paper, we present UniXcoder, a unified cross-modal pre-trained model for programming language. The model utilizes mask attention matrices with prefix adapters to control the behavior of the model and leverages cross-modal contents like AST and code comment to enhance code representation. To encode AST that is represented as a tree in parallel, we propose a one-to-one mapping method to transform AST in a sequence structure that retains all structural information from the tree. Furthermore, we propose to utilize multi-modal contents to learn representation of code fragment with contrastive learning, and then align representations among programming languages using a cross-modal generation task. We evaluate UniXcoder on five code-related tasks over nine datasets. To further evaluate the performance of code fragment representation, we also construct a dataset for a new task, called zero-shot code-to-code search. Results show that our model achieves state-of-the-art performance on most tasks and analysis reveals that comment and AST can both enhance UniXcoder.",
        "id": 247315559
      }
    ],
    "negative_ctxs": [
      {
        "title": "Cross-discourse Development of Supervised Sentiment Analysis in the Clinical Domain",
        "text": "Current approaches to sentiment analysis assume that the sole discourse function of sentiment-bearing texts is expressivity. However, the persuasive discourse function also utilises expressive language. In this work, we present the results of training supervised classifiers on a new corpus of clinical texts that contain documents with an expressive discourse function, and we test the learned models on a subset of the same corpus containing persuasive texts. The results of this indicate that despite the difference in discourse function, the learned models perform favourably.",
        "id": 2540417
      },
      {
        "title": "",
        "text": "",
        "id": 14597336
      },
      {
        "title": "NEAR-OPTIMAL REPRESENTATION LEARNING FOR HIERARCHICAL REINFORCEMENT LEARNING",
        "text": "We study the problem of representation learning in goal-conditioned hierarchical reinforcement learning. In such hierarchical structures, a higher-level controller solves tasks by iteratively communicating goals which a lower-level policy is trained to reach. Accordingly, the choice of representation -the mapping of observation space to goal space -is crucial. To study this problem, we develop a notion of sub-optimality of a representation, defined in terms of expected reward of the optimal hierarchical policy using this representation. We derive expressions which bound the sub-optimality and show how these expressions can be translated to representation learning objectives which may be optimized in practice. Results on a number of difficult continuous-control tasks show that our approach to representation learning yields qualitatively better representations as well as quantitatively better hierarchical policies, compared to existing methods. 1",
        "id": 52909341
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that explores the improvement of Recurrent Neural Network Transducers (RNN-T) through the integration of cross-attention mechanisms for use in simultaneous translation?",
    "positive_ctxs": [
      {
        "title": "Cross Attention Augmented Transducer Networks for Simultaneous Translation",
        "text": "This paper proposes a novel architecture, Cross Attention Augmented Transducer (CAAT), for simultaneous translation. The framework aims to jointly optimize the policy and translation models. To effectively consider all possible READ-WRITE simultaneous translation action paths, we adapt the online automatic speech recognition (ASR) model, RNN-T, but remove the strong monotonic constraint, which is critical for the translation task to consider reordering. To make CAAT work, we introduce a novel latency loss whose expectation can be optimized by a forward-backward algorithm. We implement CAAT with Transformer while the general CAAT architecture can also be implemented with other attention-based encoder-decoder frameworks. Experiments on both speech-to-text (S2T) and text-to-text (T2T) simultaneous translation tasks show that CAAT achieves significantly better latency-quality trade-offs compared to the state-of-the-art simultaneous translation approaches. 1",
        "id": 243865395
      }
    ],
    "negative_ctxs": [
      {
        "title": "Position Offset Label Prediction for Grammatical Error Correction",
        "text": "We introduce a novel position offset label prediction subtask to the encoder-decoder architecture for grammatical error correction (GEC) task. To keep the meaning of the input sentence unchanged, only a few words should be inserted or deleted during correction, and most of tokens in the erroneous sentence appear in the paired correct sentence with limited position movement. Inspired by this observation, we design an auxiliary task to predict position offset label (POL) of tokens, which is naturally capable of integrating different correction editing operations into a unified framework. Based on the predicted POL, we further propose a new copy mechanism (P-copy) to replace the vanilla copy module. Experimental results on Chinese, English and Japanese datasets demonstrate that our proposed POL-Pc framework obviously improves the performance of baseline models. Moreover, our model yields consistent performance gain over various data augmentation methods. Especially, after incorporating synthetic data, our model achieves a 38.95 F 0.5 score on Chinese GEC dataset, which outperforms the previous state-of-the-art by a wide margin of 1.98 points.",
        "id": 252818994
      },
      {
        "title": "Challenges in Urdu Text Tokenization and Sentence Boundary Disambiguation",
        "text": "Urdu is morphologically rich language with different nature of its characters. Urdu text tokenization and sentence boundary disambiguation is difficult as compared to the language like English. Major hurdle for tokenization is improper use of space between words, where as absence of case discrimination makes the sentence boundary detection a difficult task. In this paper some issues regarding both of these language processing tasks have been identified.",
        "id": 14464094
      },
      {
        "title": "Resolving Implicit References in Instructional Texts",
        "text": "The usage of (co-)referring expressions in discourse contributes to the coherence of a text.However, text comprehension can be difficult when referring expressions are non-verbalized and have to be resolved in the discourse context. In this paper, we propose a novel dataset of such implicit references, which we automatically derive from insertions of references in collaboratively edited how-to guides. Our dataset consists of 6,014 instances, making it one of the largest datasets of implicit references and a useful starting point to investigate misunderstandings caused by underspecified language. We test different methods for resolving implicit references in our dataset based on the Generative Pre-trained Transformer model (GPT) and compare them to heuristic baselines. Our experiments indicate that GPT can accurately resolve the majority of implicit references in our data. Finally, we investigate remaining errors and examine human preferences regarding different resolutions of an implicit reference given the discourse context.",
        "id": 241583375
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "I am looking for research that has explored topic and frame conditioning in transformer language models to enhance the quality of generated argument claims. Is there a paper discussing this approach?",
    "positive_ctxs": [
      {
        "title": "Aspect-Controlled Neural Argument Generation",
        "text": "We rely on arguments in our daily lives to deliver our opinions and base them on evidence, making them more convincing in turn. However, finding and formulating arguments can be challenging. In this work, we present the Arg-CTRL-a language model for argument generation that can be controlled to generate sentence-level arguments for a given topic, stance, and aspect. We define argument aspect detection as a necessary method to allow this fine-granular control and crowdsource a dataset with 5,032 arguments annotated with aspects. Our evaluation shows that the Arg-CTRL is able to generate high-quality, aspectspecific arguments, applicable to automatic counter-argument generation. We publish the model weights and all datasets and code to train the Arg-CTRL. 1",
        "id": 218470025
      }
    ],
    "negative_ctxs": [
      {
        "title": "Clinical Outcome Prediction from Admission Notes using Self-Supervised Knowledge Integration",
        "text": "Outcome prediction from clinical text can prevent doctors from overlooking possible risks and help hospitals to plan capacities. We simulate patients at admission time, when decision support can be especially valuable, and contribute a novel admission to discharge task with four common outcome prediction targets: Diagnoses at discharge, procedures performed, in-hospital mortality and length-of-stay prediction. The ideal system should infer outcomes based on symptoms, pre-conditions and risk factors of a patient. We evaluate the effectiveness of language models to handle this scenario and propose clinical outcome pretraining to integrate knowledge about patient outcomes from multiple public sources. We further present a simple method to incorporate ICD code hierarchy into the models. We show that our approach improves performance on the outcome tasks against several baselines. A detailed analysis reveals further strengths of the model, including transferability, but also weaknesses such as handling of vital values and inconsistencies in the underlying data.",
        "id": 231846970
      },
      {
        "title": "Multi-layered Annotation of Conversation-like Narratives in German",
        "text": "This work presents two corpora based on excerpts from two German novels with an informal narration style. We performed fine-grained multi-layer annotations of animate referents, assigning local and global prominence-lending features to the annotated referring expressions. In addition, our corpora include annotations of intra-sentential segments, which can serve as a more reliable unit of length measurement. Furthermore, we present two exemplary studies demonstrating how to use these corpora.",
        "id": 259376874
      },
      {
        "title": "Fusion of linguistic, neural and sentence-transformer features for improved term alignment",
        "text": "Crosslingual terminology alignment task has many practical applications. In this work, we propose an aligning method for the shared task of the 15th Workshop on Building and Using Comparable Corpora. Our method combines several different approaches into one cohesive machine learning model, based on SVM. From shared-task specific and external sources, we crafted four types of features: cognate-based, dictionary-based, embedding-based, and combined features, which combine aspects of the other three types. We added a post-processing re-scoring method, which reduces the effect of hubness, where some terms are nearest neighbours of many other terms. We achieved the average precision score of 0.833 on the English-French training set of the shared task.",
        "id": 252624481
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that explores employing independently sampled dropout masks to generate positive pairs in contrastive learning for sentence embeddings?",
    "positive_ctxs": [
      {
        "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings",
        "text": "This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using \"entailment\" pairs as positives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2% and 2.2% improvement compared to previous best results. We also show-both theoretically and empirically-that contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available. 1",
        "id": 233296292
      }
    ],
    "negative_ctxs": [
      {
        "title": "The DCU Discourse Parser for Connective, Argument Identification and Explicit Sense Classification",
        "text": "This paper describes our submission to the CoNLL-2015 shared task on discourse parsing. We factor the pipeline into subcomponents which are then used to form the final sequential architecture. Focusing on achieving good performance when inferring explicit discourse relations, we apply maximum entropy and recurrent neural networks to different sub-tasks such as connective identification, argument extraction, and sense classification. The our final system achieves 16.51%, 12.73% and 11.15% overall F1 scores on the dev, WSJ and blind test sets, respectively.",
        "id": 16489279
      },
      {
        "title": "RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms",
        "text": "Pre-trained language models (PTLMs) have achieved impressive performance on commonsense inference benchmarks, but their ability to employ commonsense to make robust inferences, which is crucial for effective communications with humans, is debated. In the pursuit of advancing fluid human-AI communication, we propose a new challenge, RICA: Robust Inference using Commonsense Axioms, that evaluates robust commonsense inference despite textual perturbations. To generate data for this challenge, we develop a systematic and scalable procedure using commonsense knowledge bases and probe PTLMs across two different evaluation settings. Extensive experiments on our generated probe sets with more than 10k statements show that PTLMs perform no better than random guessing on the zero-shot setting, are heavily impacted by statistical biases, and are not robust to perturbation attacks. We also find that fine-tuning on similar statements offer limited gains, as PTLMs still fail to generalize to unseen inferences. Our new large-scale benchmark exposes a significant gap between PTLMs and human-level language understanding and offers a new challenge for PTLMs to demonstrate commonsense. 1 . 2020. Evaluating commonsense in pretrained language models. AAAI.",
        "id": 221865854
      },
      {
        "title": "SCIR-QA at SemEval-2017 Task 3: CNN Model Based on Similar and Dissimilar Information between Keywords for Question Similarity",
        "text": "We describe a method of calculating the similarity between questions in community QA. Questions in cQA are usually very long and there are a lot of useless information about calculating the similarity between questions. Therefore, we implement a CNN model based on similar and dissimilar information on questions keywords. We extract the keywords of questions, and then model the similar and dissimilar information between the keywords, and use the CNN model to calculate the similarity.",
        "id": 26478127
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Can you find a research paper that discusses using structured pruning techniques to scale down language models, where the original model being pruned has billions of parameters?",
    "positive_ctxs": [
      {
        "title": "SHEARED LLAMA: ACCELERATING LANGUAGE MODEL PRE-TRAINING VIA STRUCTURED PRUNING",
        "text": "The popularity of LLaMA(Touvron et al., 2023a;b)  and other recently emerged moderate-sized large language models (LLMs) highlights the potential of building smaller yet powerful LLMs.Regardless, the cost of training such models from scratch on trillions of tokens remains high.In this work, we study structured pruning as an effective means to develop smaller LLMs from pre-trained, larger models.Our approach employs two key techniques: (1) targeted structured pruning, which prunes a larger model to a specified target shape by removing layers, heads, and intermediate and hidden dimensions in an end-to-end manner, and (2) dynamic batch loading, which dynamically updates the composition of sampled data in each training batch based on varying losses across different domains.We demonstrate the efficacy of our approach by presenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B and 2.7B parameters.Sheared-LLaMA models outperform state-of-the-art open-source models of equivalent sizes, such as Pythia, INCITE, and OpenLLaMA models, on a wide range of downstream and instruction tuning evaluations, while requiring only 3% of compute compared to training such models from scratch.This work provides compelling evidence that leveraging existing LLMs with structured pruning is a far more cost-effective approach for building smaller LLMs. 1",
        "id": 263830786
      }
    ],
    "negative_ctxs": [
      {
        "title": "Direction is what you need: Improving Word Embedding Compression in Large Language Models",
        "text": "The adoption of Transformer-based models in natural language processing (NLP) has led to great success using a massive number of parameters. However, due to deployment constraints in edge devices, there has been a rising interest in the compression of these models to improve their inference time and memory footprint. This paper presents a novel loss objective to compress token embeddings in the Transformer-based models by leveraging an AutoEncoder architecture. More specifically, we emphasize the importance of the direction of compressed embeddings with respect to original uncompressed embeddings. The proposed method is task-agnostic and does not require further language modeling pre-training. Our method significantly outperforms the commonly used SVD-based matrix-factorization approach in terms of initial language model Perplexity. Moreover, we evaluate our proposed approach over SQuAD v1.1 dataset and several downstream tasks from the GLUE benchmark, where we also outperform the baseline in most scenarios. Our code is public. 1 .",
        "id": 235436167
      },
      {
        "title": "A Call for Clarity in Reporting BLEU Scores",
        "text": "The field of machine translation faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to \"the\" BLEU score, BLEU is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for usersupplied reference processing, and provide a new tool, SACREBLEU, 1 to facilitate this. . 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. ArXiv eprints, abs/1609.08144.",
        "id": 13751870
      },
      {
        "title": "",
        "text": "",
        "id": 236477523
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that investigates graph-based methods for predicting connections in knowledge graphs, focusing on n-ary relational facts rather than just simple triple structures?",
    "positive_ctxs": [
      {
        "title": "Link Prediction on N-ary Relational Facts: A Graph-based Approach",
        "text": "Link prediction on knowledge graphs (KGs) is a key research topic. Previous work mainly focused on binary relations, paying less attention to higher-arity relations although they are ubiquitous in real-world KGs. This paper considers link prediction upon n-ary relational facts and proposes a graph-based approach to this task.The key to our approach is to represent the nary structure of a fact as a small heterogeneous graph, and model this graph with edge-biased fully-connected attention. The fully-connected attention captures universal inter-vertex interactions, while with edge-aware attentive biases to particularly encode the graph structure and its heterogeneity. In this fashion, our approach fully models global and local dependencies in each n-ary fact, and hence can more effectively capture associations therein. Extensive evaluation verifies the effectiveness and superiority of our approach. It performs substantially and consistently better than current state-of-the-art across a variety of n-ary relational benchmarks. Our code is publicly available. 1",
        "id": 234763248
      }
    ],
    "negative_ctxs": [
      {
        "title": "Correcting ESL Errors Using Phrasal SMT Techniques",
        "text": "This paper presents a pilot study of the use of phrasal Statistical Machine Translation (SMT) techniques to identify and correct writing errors made by learners of English as a Second Language (ESL). Using examples of mass noun errors found in the Chinese Learner Error Corpus (CLEC) to guide creation of an engineered training set, we show that application of the SMT paradigm can capture errors not well addressed by widely-used proofing tools designed for native speakers. Our system was able to correct 61.81% of mistakes in a set of naturallyoccurring examples of mass noun errors found on the World Wide Web, suggesting that efforts to collect alignable corpora of pre-and post-editing ESL writing samples offer can enable the development of SMT-based writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of ESL learners.",
        "id": 757808
      },
      {
        "title": "Hantology-A Linguistic Resource for Chinese Language Processing and Studying",
        "text": "Hantology, a character-based Chinese language resource is created to provide an infrastructure for language processing and research on the writing system. Unlike alphabetic or syllabic writing systems, the ideographic writing system of Chinese poses both a challenge and an opportunity. The challenge is that a totally different resources structure must be created to represent and process speaker's conventionalization of the language. The rare opportunity is that the structure itself is enriched with conceptual classification and can be utilized for ontology building. We describe the contents and possible applications of Hantology in this paper. The applications of Hantology include: (1) an account for the diachronic development of Chinese lexica (2) character-based language processing, (3) a study of conceptual structure differences in Chinese and English, and (4) comparisons of different ideographic writing systems.",
        "id": 12446797
      },
      {
        "title": "Evaluation of Computational Linguistic Techniques for Identifying Significant Topics for Browsing Applications",
        "text": "AbstractEvaluation of natural language processing tools and systems must focus on two complementary aspects: first, evaluation of the accuracy of the output, and second, evaluation of the functionality of the output as embedded in an application. This paper presents evaluations of two aspects of LinkIT, a tool for noun phrase identification linking, sorting and filtering. LinkIT[Evans 1998] uses a head sorting method[Wacholder 1998] to organize and rank simplex noun phrases (SNPs). LinkIT is to identify significant topics in domainindependent documents. The first evaluation, reported in D.K.Evans et al. 2000 compares the output of the Noun Phrase finder in LinkIT to two other systems. Issues of establishing a gold standard and criteria for matching are discussed. The second evaluation directly concerns the construction of the browsing application. We present results from Wacholder et al. 2000 on a qualitative evaluation which compares three shallow processing methods for extracting index terms, i.e., terms that can be used to model the content of documents. We analyze both quality and coverage. We discuss how experimental results such as these guide the building of an effective browsing applications.",
        "id": 17461076
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Have any research papers explored methods to improve BERT's efficiency on long-text tasks, such as early exiting or self-distillation strategies?",
    "positive_ctxs": [
      {
        "title": "BERxiT: Early Exiting for BERT with Better Fine-Tuning and Extension to Regression",
        "text": "The slow speed of BERT has motivated much research on accelerating its inference, and the early exiting idea has been proposed to make trade-offs between model quality and efficiency. This paper aims to address two weaknesses of previous work: (1) existing fine-tuning strategies for early exiting models fail to take full advantage of BERT; (2) methods to make exiting decisions are limited to classification tasks. We propose a more advanced fine-tuning strategy and a learning-toexit module that extends early exiting to tasks other than classification. Experiments demonstrate improved early exiting for BERT, with better trade-offs obtained by the proposed finetuning strategy, successful application to regression tasks, and the possibility to combine it with other acceleration methods. Source code can be found at https://github.com/ castorini/berxit.",
        "id": 233189542
      }
    ],
    "negative_ctxs": [
      {
        "title": "ForceReader: a BERT-based Interactive Machine Reading Comprehension Model with Attention Separation",
        "text": "The release of BERT revolutionized the development of NLP. Various BERT-based reading comprehension models have been proposed, thus updating the performance ranking of reading comprehension tasks. However, the above BERT-based models inherently employ BERT's combined input method, representing the input question and paragraph as a single packed sequence, without further modification for reading comprehension. This paper makes an in-depth analysis of this input method, proposes a problem of this approach. We call it attention deconcentration. Accordingly, this paper proposes ForceReader, a BERT-based interactive machine reading comprehension model. First, ForceReader proposes a novel solution called the Attention Separation Representation to respond to attention deconcentration. Moreover, starting from the logical nature of reading comprehension tasks, ForceReader adopts Multi-mode Reading, and Interactive Reasoning strategy. For the calculation of attention, ForceReader employs Conditional Background Attention to solve the lack of the overall context semantic after the separation of attention. As an integral model, ForceReader shows a significant improvement in reading comprehension tasks compared to BERT. Moreover, this paper makes detailed visual analyses of the attention and propose strategies accordingly. This may be another argument to the explanations of the attention.This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/.",
        "id": 227231086
      },
      {
        "title": "",
        "text": "",
        "id": 227230617
      },
      {
        "title": "Natural-language Interactive Narratives in Imaginal Exposure Therapy for Obsessive-Compulsive Disorder",
        "text": "Obsessive-compulsive disorder (OCD) is an anxiety-based disorder that affects around 2.5% of the population. A common treatment for OCD is exposure therapy, where the patient repeatedly confronts a feared experience, which has the long-term effect of decreasing their anxiety. Some exposures consist of reading and writing stories about an imagined anxiety-provoking scenario. In this paper, we present a technology that enables patients to interactively contribute to exposure stories by supplying natural language input (typed or spoken) that advances a scenario. This interactivity could potentially increase the patient's sense of immersion in an exposure and contribute to its success. We introduce the NLP task behind processing inputs to predict new events in the scenario, and describe our initial approach. We then illustrate the future possibility of this work with an example of an exposure scenario authored with our application.",
        "id": 30226677
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a study that explores mitigating bias in natural language understanding via example reweighting?",
    "positive_ctxs": [
      {
        "title": "End-to-End Self-Debiasing Framework for Robust NLU Training",
        "text": "Existing Natural Language Understanding (NLU) models have been shown to incorporate dataset biases leading to strong performance on in-distribution (ID) test sets but poor performance on out-of-distribution (OOD) ones. We introduce a simple yet effective debiasing framework whereby the shallow representations of the main model are used to derive a bias model and both models are trained simultaneously. We demonstrate on three well studied NLU tasks that despite its simplicity, our method leads to competitive OOD results. It significantly outperforms other debiasing approaches on two tasks, while still delivering high in-distribution performance.",
        "id": 236477370
      }
    ],
    "negative_ctxs": [
      {
        "title": "CUE Vectors: Modular Training of Language Models Conditioned on Diverse Contextual Signals",
        "text": "We propose a framework to modularize the training of neural language models that use diverse forms of sentence-external context (including metadata) by eliminating the need to jointly train sentence-external and withinsentence encoders. Our approach, contextual universal embeddings (CUE), trains LMs on one set of context, such as date and author, and adapts to novel metadata types, such as article title, or previous sentence. The model consists of a pretrained neural sentence LM, a BERT-based context encoder, and a masked transformer decoder that estimates LM probabilities using sentence-internal and sentenceexternal information. When context or metadata are unavailable, our model learns to combine contextual and sentence-internal information using noisy oracle unigram embeddings as a proxy. Real contextual information can be introduced later and used to adapt a small number of parameters that map contextual data into the decoder's embedding space. We validate the CUE framework on a NYTimes text corpus with multiple metadata types, for which the LM perplexity can be lowered from 36.6 to 27.4 by conditioning on context. Bootstrapping a contextual LM with only a subset of the context/metadata during training retains 85% of the achievable gain. Training the model initially with proxy context retains 67% of the perplexity gain after adapting to real context. Furthermore, we can swap one type of pretrained sentence LM for another without retraining the context encoders, by only adapting the decoder model. Overall, we obtain a modular framework that allows incremental, scalable training of context-enhanced LMs.",
        "id": 247476288
      },
      {
        "title": "Word Semantic Similarity for Morphologically Rich Languages",
        "text": "In this work, we investigate the role of morphology on the performance of semantic similarity for morphologically rich languages, such as German and Greek. The challenge in processing languages with richer morphology than English, lies in reducing estimation error while addressing the semantic distortion introduced by a stemmer or a lemmatiser. For this purpose, we propose a methodology for selective stemming, based on a semantic distortion metric. The proposed algorithm is tested on the task of similarity estimation between words using two types of corpus-based similarity metrics: co-occurrence-based and context-based. The performance on morphologically rich languages is boosted by stemming with the context-based metric, unlike English, where the best results are obtained by the co-occurrence-based metric. A key finding is that the estimation error reduction is different when a word is used as a feature, rather than when it is used as a target word.",
        "id": 10678830
      },
      {
        "title": "Linguistically Light Lexical Extensions for Ontologies",
        "text": "An increasing number of enterprises are beginning to include semantic web ontologies into their Information Extraction (IE) and Text Analytics (TA) applications. This can be challenging for a TA group wishing to avail of semantic web ontologies due to the manual effort of retargeting and tailoring language resources within the TA system to a new domain to meet customer needs. A lightweight lexical layer within an ontology offers a solution to this problem. Furthermore, the identification of class instances within unstructured text for either the purposes of ontology population or semantic annotation are usually limited to term mentions of proper noun, personal noun or fixed key phrases within Text Analytics or Ontology Based Information Extraction (OBIE) applications. These systems do not generalise to cope with compound nominal classes of multi word expressions. LEON, a set of Lexical Extensions for Ontologies offers a solution to this problem. We describe LEON, which encodes light linguistic features of lexical entries for concepts within an ontology, as well as a lightweight lexical analyzer which complies the LEON metadata into efficient an dictionary format to drive large scale identification and semantic annotation of concepts mentioned in text.",
        "id": 11003403
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "I'm exploring efficient transformer architectures for language embeddings and came across some work that utilizes advanced pre-trained models. Which paper should I reference to learn more about the use of XLM-R for multilingual representation learning in a transformer-based setting?",
    "positive_ctxs": [
      {
        "title": "Unsupervised Cross-lingual Representation Learning at Scale",
        "text": "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of crosslingual transfer tasks. We train a Transformerbased masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing perlanguage performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available. 1",
        "id": 207880568
      }
    ],
    "negative_ctxs": [
      {
        "title": "LAGRANGIAN NEURAL NETWORKS",
        "text": "Accurate models of the world are built upon notions of its underlying symmetries. In physics, these symmetries correspond to conservation laws, such as for energy and momentum. Yet even though neural network models see increasing use in the physical sciences, they struggle to learn these symmetries. In this paper, we propose Lagrangian Neural Networks (LNNs), which can parameterize arbitrary Lagrangians using neural networks. In contrast to models that learn Hamiltonians, LNNs do not require canonical coordinates, and thus perform well in situations where canonical momenta are unknown or difficult to compute. Unlike previous approaches, our method does not restrict the functional form of learned energies and will produce energy-conserving models for a variety of tasks. We test our approach on a double pendulum and a relativistic particle, demonstrating energy conservation where a baseline approach incurs dissipation and modeling relativity without canonical coordinates where a Hamiltonian approach fails. Finally, we show how this model can be applied to graphs and continuous systems using a Lagrangian Graph Network, and demonstrate it on the 1D wave equation. * Also affiliated with Princeton University",
        "id": 212644628
      },
      {
        "title": "《全唐詩》的分析、探勘與應用-風格、對仗、社會網路與對聯 Textual Analysis of Complete Tang Poems for Discoveries and Applications -Style, Antitheses, Social Networks, and Couplets",
        "text": "1The Complete Tang Poems (CTP) is the most important collection for studying Tang poetry, which in turn is arguably a very influential part of the Chinese literature. Our analyzing the CTP from the perspectives of antithesis 2 , collocation and distributional semantics offers some interesting overviews of the styles and imageries embedded in the works of some representative Tang poets. Our analyses include (1) a quantitative comparison of the uses of \"wind\" and \"moon\" in Li Bai's and Du Fu's works and (2) the functions of colors in Tang poems. In particular, we explored the appearances of \"white\" color, which is the most frequent color in Tang poems. Colors in static poems are like audios in motion pictures, so we thought the analyses could lead us to an important facet of the poems. In addition, we extracted social networks of poets from the poems, and built a simple couplet suggestion kit based on the textual analysis of the poems.關鍵詞：數位人文、中國文學、全唐詩、詞彙語意、共現分析、文本分析、語料庫 分析、中國歷代人物傳記資料庫",
        "id": 12980287
      },
      {
        "title": "Generalised Unsupervised Domain Adaptation of Neural Machine Translation with Cross-Lingual Data Selection",
        "text": "This paper considers the unsupervised domain adaptation problem for neural machine translation (NMT), where we assume the access to only monolingual text in either the source or target language in the new domain. We propose a cross-lingual data selection method to extract in-domain sentences in the missing language side from a large generic monolingual corpus. Our proposed method trains an adaptive layer on top of multilingual BERT by contrastive learning to align the representation between the source and target language. This then enables the transferability of the domain classifier between the languages in a zero-shot manner. Once the in-domain data is detected by the classifier, the NMT model is then adapted to the new domain by jointly learning translation and domain discrimination tasks. We evaluate our cross-lingual data selection method on NMT across five diverse domains in three language pairs, as well as a real-world scenario of translation for COVID-19. The results show that our proposed method outperforms other selection baselines up to +1.5 BLEU score.",
        "id": 237453654
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Could you suggest a paper that introduces an approach to relation extraction that involves learning syntax dependency structures using a tree LSTM model?",
    "positive_ctxs": [
      {
        "title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks",
        "text": "Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).",
        "id": 3033526
      }
    ],
    "negative_ctxs": [
      {
        "title": "Logic Against Bias: Textual Entailment Mitigates Stereotypical Sentence Reasoning",
        "text": "Due to their similarity-based learning objectives, pretrained sentence encoders often internalize stereotypical assumptions that reflect the social biases that exist within their training corpora. In this paper, we describe several kinds of stereotypes concerning different communities that are present in popular sentence representation models, including pretrained next sentence prediction and contrastive sentence representation models. We compare such models to textual entailment models that learn language logic for a variety of downstream language understanding tasks. By comparing strong pretrained models based on text similarity with textual entailment learning, we conclude that the explicit logic learning with textual entailment can significantly reduce bias and improve the recognition of social communities, without an explicit de-biasing process. The code, model, and data associated with this work are publicly available at https: //github.com/luohongyin/ESP.git.",
        "id": 257482682
      },
      {
        "title": "Improving Variational Autoencoder for Text Modelling with Timestep-Wise Regularisation",
        "text": "The Variational Autoencoder (VAE) is a popular and powerful model applied to text modelling to generate diverse sentences. However, an issue known as posterior collapse (or KL loss vanishing) happens when the VAE is used in text modelling, where the approximate posterior collapses to the prior, and the model will totally ignore the latent variables and be degraded to a plain language model during text generation. Such an issue is particularly prevalent when RNN-based VAE models are employed for text modelling. In this paper, we propose a simple, generic architecture called Timestep-Wise Regularisation VAE (TWR-VAE), which can effectively avoid posterior collapse and can be applied to any RNN-based VAE models. The effectiveness and versatility of our model are demonstrated in different tasks, including language modelling and dialogue response generation. * Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons. org/licenses/by/4.0/.",
        "id": 226236766
      },
      {
        "title": "Optimizing Features in Active Machine Learning for Complex Qualitative Content Analysis",
        "text": "We propose a semi-automatic approach for content analysis that leverages machine learning (ML) being initially trained on a small set of hand-coded data to perform a first pass in coding, and then have human annotators correct machine annotations in order to produce more examples to retrain the existing model incrementally for better performance. In this \"active learning\" approach, it is equally important to optimize the creation of the initial ML model given less training data so that the model is able to capture most if not all positive examples, and filter out as many negative examples as possible for human annotators to correct. This paper reports our attempt to optimize the initial ML model through feature exploration in a complex content analysis project that uses a multidimensional coding scheme, and contains codes with sparse positive examples. While different codes respond optimally to different combinations of features, we show that it is possible to create an optimal initial ML model using only a single combination of features for codes with at least 100 positive examples in the gold standard corpus.",
        "id": 11104974
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "What recent developments in transformer architecture aim to improve the multi-head self-attention mechanism for better transmission of unprocessed attention scores and more stable training?",
    "positive_ctxs": [
      {
        "title": "RealFormer: Transformer Likes Residual Attention",
        "text": "Transformer is the backbone of modern NLP models. In this paper, we propose Real-Former, a simple and generic technique to create Residual Attention Layer Transformer networks that significantly outperform the canonical Transformer and its variants (BERT, ETC, etc.) on a wide spectrum of tasks including Masked Language Modeling, GLUE, SQuAD, Neural Machine Translation, WikiHop, HotpotQA, Natural Questions, and OpenKP. We also observe empirically that RealFormer stabilizes training and leads to models with sparser attention. Source code and pre-trained checkpoints for RealFormer can be found at https",
        "id": 229376913
      }
    ],
    "negative_ctxs": [
      {
        "title": "Published as a conference paper at ICLR 2016 UNSUPERVISED AND SEMI-SUPERVISED LEARNING WITH CATEGORICAL GENERATIVE ADVERSARIAL NETWORKS",
        "text": "In this paper we present a method for learning a discriminative classifier from unlabeled or partially labeled data. Our approach is based on an objective function that trades-off mutual information between observed examples and their predicted categorical class distribution, against robustness of the classifier to an adversarial generative model. The resulting algorithm can either be interpreted as a natural generalization of the generative adversarial networks (GAN) framework or as an extension of the regularized information maximization (RIM) framework to robust classification against an optimal adversary. We empirically evaluate our method -which we dub categorical generative adversarial networks (or CatGAN) -on synthetic data as well as on challenging image classification tasks, demonstrating the robustness of the learned classifiers. We further qualitatively assess the fidelity of samples generated by the adversarial generator that is learned alongside the discriminative classifier, and identify links between the CatGAN objective and discriminative clustering algorithms (such as RIM).",
        "id": 6230637
      },
      {
        "title": "How Much Do Modifications to Transformer Language Models Affect Their Ability to Learn Linguistic Knowledge?",
        "text": "Recent progress in large pretrained language models (LMs) has led to a growth of analyses examining what kinds of linguistic knowledge are encoded by these models. Due to computational constraints, existing analyses are mostly conducted on publicly-released LM checkpoints, which makes it difficult to study how various factors during training affect the models' acquisition of linguistic knowledge. In this paper, we train a suite of small-scale Transformer LMs that differ from each other with respect to architectural decisions (e.g., self-attention configuration) or training objectives (e.g., multi-tasking, focal loss). We evaluate these LMs on BLiMP, a targeted evaluation benchmark of multiple English linguistic phenomena. Our experiments show that while none of these modifications yields significant improvements on aggregate, changes to the loss function result in promising improvements on several subcategories (e.g., detecting adjunct islands, correctly scoping negative polarity items). We hope our work offers useful insights for future research into designing Transformer LMs that more effectively learn linguistic knowledge.",
        "id": 248368718
      },
      {
        "title": "Statistical syntactic parsing for Latvian",
        "text": "Syntactic parsing is an important technique in the natural language processing, yet Latvian is still lacking an efficient general coverage syntax parser. This paper reports on the first experiments on statistical syntactic parsing for Latvian -a highly inflective Indo-European language with a relatively free word order. We have induced a statistical parser from a small, non-balanced Latvian Treebank using the MaltParser toolkit and measured the unlabeled attachment score (UAS). As MaltParser is based on the dependency grammar approach, we have also developed a convertor from the hybrid dependency-based annotation model used in the Latvian Treebank to the pure dependency annotation model. We have obtained a promising 74.63% UAS in 10-fold cross-validation using only ~2500 sentences. The results revealed that best results can be achieved using non-projective stack parsing algorithm with lazy arc adding strategy, but comparably good results can be achieved using projective parsing algorithms combined with appropriate projectiviziation preprocessing.",
        "id": 1076208
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "In the context of simultaneous machine translation, which tool or technique could I use to generate ground-truth alignments for training models to interpret and generate translations incrementally?",
    "positive_ctxs": [
      {
        "title": "A Simple, Fast, and Effective Reparameterization of IBM Model 2",
        "text": "We present a simple log-linear reparameterization of IBM Model 2 that overcomes problems arising from Model 1's strong assumptions and Model 2's overparameterization.Efficient inference, likelihood evaluation, and parameter estimation algorithms are provided. Training the model is consistently ten times faster than Model 4. On three large-scale translation tasks, systems built using our alignment model outperform IBM Model 4.An open-source implementation of the alignment model described in this paper is available from http://github.com/clab/fast align .",
        "id": 8476273
      }
    ],
    "negative_ctxs": [
      {
        "title": "Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Generation in Machine Translation from Deep Syntactic Trees",
        "text": "In this paper we explore a generative model for recovering surface syntax and strings from deep-syntactic tree structures. Deep analysis has been proposed for a number of language and speech processing tasks, such as machine translation and paraphrasing of speech transcripts. In an effort to validate one such formalism of deep syntax, the Praguian Tectogrammatical Representation (TR), we present a model of synthesis for English which generates surface-syntactic trees as well as strings. We propose a generative model for function word insertion (prepositions, definite/indefinite articles, etc.) and subphrase reordering. We show by way of empirical results that this model is effective in constructing acceptable English sentences given impoverished trees.",
        "id": 12088021
      },
      {
        "title": "MULTI-AGENT INTERACTIONS MODELING WITH COR- RELATED POLICIES",
        "text": "In multi-agent systems, complex interacting behaviors arise due to the high correlations among agents. However, previous work on modeling multi-agent interactions from demonstrations is primarily constrained by assuming the independence among policies and their reward structures. In this paper, we cast the multi-agent interactions modeling problem into a multi-agent imitation learning framework with explicit modeling of correlated policies by approximating opponents' policies, which can recover agents' policies that can regenerate similar interactions. Consequently, we develop a Decentralized Adversarial Imitation Learning algorithm with Correlated policies (CoDAIL), which allows for decentralized training and execution. Various experiments demonstrate that CoDAIL can better regenerate complex interactions close to the demonstrators and outperforms state-of-theart multi-agent imitation learning methods.",
        "id": 210157251
      },
      {
        "title": "Neural Discourse Structure for Text Categorization",
        "text": "We show that discourse structure, as defined by Rhetorical Structure Theory and provided by an existing discourse parser, benefits text categorization. Our approach uses a recursive neural network and a newly proposed attention mechanism to compute a representation of the text that focuses on salient content, from the perspective of both RST and the task. Experiments consider variants of the approach and illustrate its strengths and weaknesses.Although the food was amazing] A [and I was in love with the spicy pork burrito,] B [the service was really awful.] C [We watched our waiter serve himself many drinks.] D [He kept running into the bathroom] E [instead of grabbing our bill.] F",
        "id": 5914002
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper first attempts to take potential dependencies among same-level labels into account in Hierarchical Text Classification?",
    "positive_ctxs": [
      {
        "title": "Peer-Label Assisted Hierarchical Text Classification",
        "text": "Hierarchical text classification (HTC) is a challenging task, in which the labels of texts can be organized into a category hierarchy. To deal with the HTC problem, many existing works focus on utilizing the parent-child relationships that are explicitly shown in the hierarchy. However, texts with a category hierarchy also have some latent relevancy among labels in the same level of the hierarchy. We refer to these labels as peer labels, from which the peer effects are originally utilized in our work to improve the classification performance. To fully explore the peer-label relationship, we develop a PeerHTC method. This method innovatively measures the latent relevancy of peer labels through several metrics and then encodes the relevancy with a Graph Convolutional Neural Network. We also propose a sample importance learning method to ameliorate the side effects raised by modelling the peer label relevancy. Our experiments on several standard datasets demonstrate the evidence of peer labels and the superiority of PeerHTC over other state-of-the-art HTC methods in terms of classification accuracy.",
        "id": 259370602
      }
    ],
    "negative_ctxs": [
      {
        "title": "Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on Twitter",
        "text": "Hate speech in the form of racist and sexist remarks are a common occurrence on social media. For that reason, many social media services address the problem of identifying hate speech, but the definition of hate speech varies markedly and is largely a manual effort(BBC, 2015;Lomas, 2015).We provide a list of criteria founded in critical race theory, and use them to annotate a publicly available corpus of more than 16k tweets. We analyze the impact of various extra-linguistic features in conjunction with character n-grams for hatespeech detection. We also present a dictionary based the most indicative words in our data.",
        "id": 1721388
      },
      {
        "title": "Event extraction from Twitter using Non-Parametric Bayesian Mixture Model with Word Embeddings",
        "text": "To extract structured representations of newsworthy events from Twitter, unsupervised models typically assume that tweets involving the same named entities and expressed using similar words are likely to belong to the same event. Hence, they group tweets into clusters based on the cooccurrence patterns of named entities and topical keywords. However, there are two main limitations. First, they require the number of events to be known beforehand, which is not realistic in practical applications. Second, they don't recognise that the same named entity might be referred to by multiple mentions and tweets using different mentions would be wrongly assigned to different events. To overcome these limitations, we propose a nonparametric Bayesian mixture model with word embeddings for event extraction, in which the number of events can be inferred automatically and the issue of lexical variations for the same named entity can be dealt with properly. Our model has been evaluated on three datasets with sizes ranging between 2,499 and over 60 million tweets. Experimental results show that our model outperforms the baseline approach on all datasets by 5-8% in F-measure.",
        "id": 12331876
      },
      {
        "title": "DOP: Off-Policy Multi-Agent Decomposed Policy Gradients",
        "text": "Recently, multi-agent policy gradient (MAPG) methods witness vigorous progress. However, there is a discrepancy between the performance of MAPG methods and state-of-the-art multi-agent value-based approaches. In this paper, we investigate the causes that hinder the performance of MAPG algorithms and present a multiagent decomposed policy gradient method (DOP). This method introduces the idea of value function decomposition into the multi-agent actor-critic framework. Based on this idea, DOP supports efficient off-policy learning and addresses the issue of centralized-decentralized mismatch and credit assignment in both discrete and continuous action spaces. We formally show that DOP critics have sufficient representational capability to guarantee convergence. In addition, empirical evaluations on the StarCraft II micromanagement benchmark and multi-agent particle environments demonstrate that our method significantly outperforms state-of-the-art value-based and policy-based multi-agent reinforcement learning algorithms. Demonstrative videos are available at https",
        "id": 220769181
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which paper trains on linear regression to hypothesize how fine-tuning affects language models?",
    "positive_ctxs": [
      {
        "title": "UNDERSTANDING CATASTROPHIC FORGETTING IN LANGUAGE MODELS VIA IMPLICIT INFERENCE",
        "text": "Fine-tuning (via methods such as instruction-tuning or reinforcement learning from human feedback) is a crucial step in training language models to robustly carry out tasks of interest.However, we lack a systematic understanding of the effects of fine-tuning, particularly on tasks outside the narrow fine-tuning distribution.In a simplified scenario, we demonstrate that improving performance on tasks within the fine-tuning data distribution comes at the expense of suppressing model capabilities on other tasks.This degradation is especially pronounced for tasks \"closest\" to the fine-tuning distribution.We hypothesize that language models implicitly infer the task of the prompt corresponds, and the fine-tuning process predominantly skews this task inference towards tasks in the fine-tuning distribution.To test this hypothesis, we propose Conjugate Prompting to see if we can recover pretrained capabilities.Conjugate prompting artificially makes the task look farther from the fine-tuning distribution while requiring the same capability.We find that conjugate prompting systematically recovers some of the pretraining capabilities on our synthetic setup.We then apply conjugate prompting to real-world LLMs using the observation that fine-tuning distributions are typically heavily skewed towards English.We find that simply translating the prompts to different languages can cause the fine-tuned models to respond like their pretrained counterparts instead.This allows us to recover the in-context learning abilities lost via instruction tuning, and more concerningly, to recover harmful content generation suppressed by safety fine-tuning in chatbots like ChatGPT.",
        "id": 262054014
      }
    ],
    "negative_ctxs": [
      {
        "title": "Multilingual Semantic Parsing : Parsing Multiple Languages into Semantic Representations",
        "text": "We consider multilingual semantic parsing -the task of simultaneously parsing semantically equivalent sentences from multiple different languages into their corresponding formal semantic representations. Our model is built on top of the hybrid tree semantic parsing framework, where natural language sentences and their corresponding semantics are assumed to be generated jointly from an underlying generative process. We first introduce a variant of the joint generative process, which essentially gives us a new semantic parsing model within the framework. Based on the different models that can be developed within the framework, we then investigate several approaches for performing the multilingual semantic parsing task. We present our evaluations on a standard dataset annotated with sentences in multiple languages coming from different language families.",
        "id": 13287001
      },
      {
        "title": "A Corpus-based Multidimensional Analysis of Linguistic Features of Truth and Deception",
        "text": "This study sets out to examine the linguistic difference between truthful and deceptive texts. In order to take more linguistic features into consideration, this research applied multidimensional analysis, which can reduce many linguistic features into several factors. This study used a self-built corpus containing 100 truthful texts and 100 deceptive texts. TextMind was employed to annotate these Chinese texts automatically. SPSS version 20 was utilized for t-tests and multidimensional analysis. The discussion of the data was divided into two parts: word count and word per sentence, and multidimensional analysis. This research reveals that word count and word per sentence of deceptive discourse are significantly smaller than those of truthful discourse. The results of multidimensional analysis suggest that deceptive discourse displays a weaker performance on dimensions of narration, interpersonal relationship, and perception.",
        "id": 202232093
      },
      {
        "title": "",
        "text": "",
        "id": 188723581
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there any paper that constructs augmented training data based on the entity-to-entity correlations?",
    "positive_ctxs": [
      {
        "title": "PeerDA: Data Augmentation via Modeling Peer Relation for Span Identification Tasks *",
        "text": "Span identification aims at identifying specific text spans from text input and classifying them into pre-defined categories. Different from previous works that merely leverage the Subordinate (SUB) relation (i.e. if a span is an instance of a certain category) to train models, this paper for the first time explores the Peer (PR) relation, which indicates that two spans are instances of the same category and share similar features. Specifically, a novel Peer Data Augmentation (PeerDA) approach is proposed which employs span pairs with the PR relation as the augmentation data for training. PeerDA has two unique advantages: (1) There are a large number of PR span pairs for augmenting the training data. (2) The augmented data can prevent the trained model from over-fitting the superficial span-category mapping by pushing the model to leverage the span semantics. Experimental results on ten datasets over four diverse tasks across seven domains demonstrate the effectiveness of PeerDA. Notably, PeerDA achieves state-of-the-art results on six of them.",
        "id": 252918792
      }
    ],
    "negative_ctxs": [
      {
        "title": "Provable Fast Greedy Compressive Summarization with Any Monotone Submodular Function",
        "text": "Submodular maximization with the greedy algorithm has been studied as an effective approach to extractive summarization. This approach is known to have three advantages: its applicability to many useful submodular objective functions, the efficiency of the greedy algorithm, and the provable performance guarantee. However, when it comes to compressive summarization, we are currently missing a counterpart of the extractive method based on submodularity. In this paper, we propose a fast greedy method for compressive summarization. Our method is applicable to any monotone submodular objective function, including many functions well-suited for document summarization. We provide an approximation guarantee of our greedy algorithm. Experiments show that our method is about 100 to 400 times faster than an existing method based on integer-linearprogramming (ILP) formulations and that our method empirically achieves more than 95%-approximation.",
        "id": 44062370
      },
      {
        "title": "Finding and Generating a Missing Part for Story Completion",
        "text": "Creating a story is difficult. Professional writers often experience a writer's block. Thus, providing automatic support to writers is crucial but also challenging. Recently, in the field of generating and understanding stories, story completion (SC) has been proposed as a method for generating missing parts of an incomplete story. Despite this method's usefulness in providing creative support, its applicability is currently limited because it requires the user to have prior knowledge of the missing part of a story. Writers do not always know which part of their writing is flawed. To overcome this problem, we propose a novel approach called \"missing position prediction (MPP).\" Given an incomplete story, we aim to predict the position of the missing part. We also propose a novel method for MPP and SC. We first conduct an experiment focusing on MPP, and our analysis shows that highly accurate predictions can be obtained when the missing part of a story is the beginning or the end. This suggests that if a story has a specific beginning or end, they play significant roles. We conduct an experiment on SC using MPP, and our proposed method demonstrates promising results.",
        "id": 227231104
      },
      {
        "title": "SafeWebUH at SemEval-2023 Task 11: Learning Annotator Disagreement in Derogatory Text: Comparison of Direct Training vs Aggregation",
        "text": "Subjectivity and difference of opinion are key social phenomena, and it is crucial to take these into account in the annotation and detection process of derogatory textual content. In this paper, we use four datasets provided by SemEval-2023 Task 11 and fine-tune a BERT model to capture the disagreement in the annotation. We find individual annotator modeling and aggregation lowers the Cross-Entropy score by an average of 0.21, compared to the direct training on the soft labels. Our findings further demonstrate that annotator metadata contributes to the average 0.029 reduction in the Cross-Entropy score.",
        "id": 258436777
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which pre-trained model is specifically designed for low-resource dialogue summarization tasks?",
    "positive_ctxs": [
      {
        "title": "DIONYSUS: A Pre-trained Model for Low-Resource Dialogue Summarization",
        "text": "Dialogue summarization has recently garnered significant attention due to its wide range of applications. However, existing methods for summarizing dialogues have limitations because they do not take into account the inherent structure of dialogue and rely heavily on labeled data, which can lead to poor performance in new domains. In this work, we propose DIONYSUS (dynamic input optimization in pre-training for dialogue summarization), a pre-trained encoder-decoder model for summarizing dialogues in any new domain. To pretrain DIONYSUS, we create two pseudo summaries for each dialogue example: one from a fine-tuned summarization model and the other from important dialogue turns. We then choose one of these pseudo summaries based on information distribution differences in different types of dialogues. This selected pseudo summary serves as the objective for pre-training DIONYSUS using a self-supervised approach on a large dialogue corpus. Our experiments show that DIONYSUS outperforms existing methods on six datasets, as demonstrated by its ROUGE scores in zero-shot and few-shot settings.",
        "id": 254877347
      }
    ],
    "negative_ctxs": [
      {
        "title": "Integrating a Rule-based with a Hierarchical Translation System",
        "text": "Recent developments on hybrid systems that combine rule-based machine translation (RBMT) systems with statistical machine translation (SMT) generally neglect the fact that RBMT systems tend to produce more syntactically well-formed translations than data-driven systems. This paper proposes a method that alleviates this issue by preserving more useful structures produced by RBMT systems and utilizing them in a SMT system that operates on hierarchical structures instead of flat phrases alone. For our experiments, we use Joshua as the decoder(Li et al., 2009). It is the first attempt towards a tighter integration of MT systems from different paradigms that both support hierarchical analyses. Preliminary results show consistent improvements over the previous approach.",
        "id": 15849173
      },
      {
        "title": "",
        "text": "",
        "id": 195348815
      },
      {
        "title": "COMPUTATIONAL UNDERSTANDING",
        "text": "",
        "id": 3926948
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there a Chinese hate speech paper that constructs an insulting lexicon while building the dataset?",
    "positive_ctxs": [
      {
        "title": "Facilitating Fine-grained Detection of Chinese Toxic Language: Hierarchical Taxonomy, Resources, and Benchmarks",
        "text": "Disclaimer:The samples presented by this paper may be considered offensive or vulgar.",
        "id": 258557119
      }
    ],
    "negative_ctxs": [
      {
        "title": "Revisiting the Practical Effectiveness of Constituency Parse Extraction from Pre-trained Language Models",
        "text": "Constituency Parse Extraction from Pre-trained Language Models (CPE-PLM) is a recent paradigm that attempts to induce constituency parse trees relying only on the internal knowledge of pre-trained language models. While attractive in the perspective that similar to in-context learning, it does not require taskspecific fine-tuning, the practical effectiveness of such an approach still remains unclear, except that it can function as a probe for investigating language models' inner workings. In this work, we mathematically reformulate CPE-PLM and propose two advanced ensemble methods tailored for it, demonstrating that the new parsing paradigm can be competitive with common unsupervised parsers by introducing a set of heterogeneous PLMs combined using our techniques. Furthermore, we explore some scenarios where the trees generated by CPE-PLM are practically useful. Specifically, we show that CPE-PLM is more effective than typical supervised parsers in few-shot settings.",
        "id": 252819507
      },
      {
        "title": "Modeling Speech Acts in Asynchronous Conversations: A Neural-CRF Approach",
        "text": "Participants in an asynchronous conversation (e.g., forum, e-mail) interact with each other at different times, performing certain communicative acts, called speech acts (e.g., question, request). In this article, we propose a hybrid approach to speech act recognition in asynchronous conversations. Our approach works in two main steps: a long short-term memory recurrent neural network (LSTM-RNN) first encodes each sentence separately into a task-specific distributed representation, and this is then used in a conditional random field (CRF) model to capture the conversational dependencies between sentences. The LSTM-RNN model uses pretrained word embeddings learned from a large conversational corpus and is trained to classify sentences into speech act types. The CRF model can consider arbitrary graph structures to model conversational dependencies in an asynchronous conversation. In addition, to mitigate the problem of limited annotated data in the asynchronous domains, we adapt the LSTM-RNN model to learn from synchronous conversations (e.g., meetings), using domain adversarial training of neural networks. Empirical evaluation shows the effectiveness of our approach over existing ones: (i) LSTM-RNNs provide better task-specific representations, (ii) conversational word embeddings benefit the LSTM-RNNs more than the off-the-shelf ones, (iii) adversarial training gives better domain-invariant representations, and (iv) the global CRF model improves over local models.",
        "id": 52289309
      },
      {
        "title": "A tool for detecting French-English cognates and false friends",
        "text": "Les congénères sont des mots qui ont au moins un sens en commun entre deux langues en plus d'avoir une orthographie semblable. La reconnaissance de ce type de mots permet aux apprenants de langue seconde ou étrangère d'enrichir plus rapidement leur vocabulaire et d'améliorer leur compréhension écrite. Toutefois, les faux amis sont des paires de mots qui à l'écrit ont des similarités, mais ils ont des significations différentes. Pour leur part, les congénères partiels sont des mots qui ont la même signification dans certains contextes dans chacune des deux langues. Cet article présente une méthode pour la classification automatique des paires des mots classées en congénères ou faux amis, en utilisant des mesures de similarité orthographiques et des méthodes d'apprentissage automatique. Ainsi, nous construisons des listes complètes des congénères et des faux amis entre les deux langues. Nous désambiguisons les congénères partiels dans des contextes spécifiques. Nos méthodes sont évaluées pour le français et l'anglais, mais elles seraient applicables à d'autres paires des langues. Nous avons construit un outil qui prend ces listes et marque dans un texte français les mots qui ont des congénères ou des faux amis en anglais, dans le but d'aider les apprenants en français langue seconde ou étrangère à améliorer leur compréhension écrite et à développer une meilleure rétention.Abstract. Cognates are pairs of words in different languages similar in spelling and meaning. They can help a second-language learner on the tasks of vocabulary expansion and reading comprehension. False friends are pairs of words that have similar spelling but different meanings. Partial cognates are pairs of words in two languages that have the same meaning in some, but not all contexts. In this article we present a method to automatically classify a pair of words as cognates or false friends, by using several measures of orthographic similarity as features for classification. We use this method to create complete lists of cognates and false friends between two languages. We also disambiguate partial cognates in context. We applied all our methods to French and English, but they can be applied to other pairs of languages as well. We built a tool that takes the produced lists and annotates a French text with equivalent English cognates or false friends, in order to help second-language learners improve their reading comprehension skills and retention rate.Mots-clés : congénères, faux amis, congénères partiels, mesures de similarité orthographiques, apprentissage automatique, apprentissage des langues assisté par ordinateur.",
        "id": 18611450
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Find the NLP paper that focuses on dialogue generation and introduces advancements in the augmentation of one-to-many or one-to-one dialogue data by conducting augmentation within the semantic space.",
    "positive_ctxs": [
      {
        "title": "DialoGPS: Dialogue Path Sampling in Continuous Semantic Space for Data Augmentation in Multi-Turn Conversations",
        "text": "In open-domain dialogue generation tasks, contexts and responses in most datasets are one-to-one mapped, violating an important many-to-many characteristic: a context leads to various responses, and a response answers multiple contexts. Without such patterns, models poorly generalize and prefer responding safely. Many attempts have been made in either multi-turn settings from a one-to-many perspective or in a many-to-many perspective but limited to single-turn settings. The major challenge to many-to-many augment multiturn dialogues is that discretely replacing each turn with semantic similarity breaks fragile context coherence. In this paper, we propose DialoGue Path Sampling (DialoGPS) method in continuous semantic space, the first manyto-many augmentation method for multi-turn dialogues. Specifically, we map a dialogue to our extended Brownian Bridge, a special Gaussian process. We sample latent variables to form coherent dialogue paths in the continuous space. A dialogue path corresponds to a new multi-turn dialogue and is used as augmented training data. We show the effect of DialoGPS with both automatic and human evaluation.",
        "id": 259286910
      }
    ],
    "negative_ctxs": [
      {
        "title": "Generating Market Comments Referring to External Resources",
        "text": "Comments on a stock market often include the reason or cause of changes in stock prices, such as \"Nikkei turns lower as yen's rise hits exporters.\" Generating such informative sentences requires capturing the relationship between different resources, including a target stock price. In this paper, we propose a model for automatically generating such informative market comments that refer to external resources. We evaluated our model through an automatic metric in terms of BLEU and human evaluation done by an expert in finance. The results show that our model outperforms the existing model both in BLEU scores and human judgment.",
        "id": 53249692
      },
      {
        "title": "Neural Machine Translation with Reordering Embeddings",
        "text": "The reordering model plays an important role in phrase-based statistical machine translation. However, there are few works that exploit the reordering information in neural machine translation. In this paper, we propose a reordering mechanism to learn the reordering embedding of a word based on its contextual information. These reordering embeddings are stacked together with self-attention networks to learn sentence representation for machine translation. The reordering mechanism can be easily integrated into both the encoder and the decoder in the Transformer translation system. Experimental results on WMT'14 English-to-German, NIST Chinese-to-English, and WAT ASPEC Japanese-to-English translation tasks demonstrate that the proposed methods can significantly improve the performance of the Transformer translation system. . 2017a. Improved neural machine translation with a syntax-aware encoder and decoder.",
        "id": 196208300
      },
      {
        "title": "Reasoning about Quantities in Natural Language",
        "text": "Little work from the Natural Language Processing community has targeted the role of quantities in Natural Language Understanding. This paper takes some key steps towards facilitating reasoning about quantities expressed in natural language. We investigate two different tasks of numerical reasoning. First, we consider Quantity Entailment, a new task formulated to understand the role of quantities in general textual inference tasks. Second, we consider the problem of automatically understanding and solving elementary school math word problems. In order to address these quantitative reasoning problems we first develop a computational approach which we show to successfully recognize and normalize textual expressions of quantities. We then use these capabilities to further develop algorithms to assist reasoning in the context of the aforementioned tasks.",
        "id": 17364624
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper first studied the efficiency robustness of multi-exit language models?",
    "positive_ctxs": [
      {
        "title": "Dynamic Transformers Provide a False Sense of Efficiency",
        "text": "Despite much success in natural language processing (NLP), pre-trained language models typically lead to a high computational cost during inference. Multi-exit is a mainstream approach to address this issue by making a tradeoff between efficiency and accuracy, where the saving of computation comes from an early exit. However, whether such saving from earlyexiting is robust remains unknown. Motivated by this, we first show that directly adapting existing adversarial attack approaches targeting model accuracy cannot significantly reduce inference efficiency. To this end, we propose a simple yet effective attacking framework, SAME, a novel slowdown attack framework on multi-exit models, which is specially tailored to reduce the efficiency of the multi-exit models. By leveraging the multi-exit models' design characteristics, we utilize all internal predictions to guide the adversarial sample generation instead of merely considering the final prediction. Experiments on the GLUE benchmark show that SAME can effectively diminish the efficiency gain of various multi-exit models by 80% on average, convincingly validating its effectiveness and generalization ability.",
        "id": 258832833
      }
    ],
    "negative_ctxs": [
      {
        "title": "RECONSIDER: Improved Re-Ranking using Span-Focused Cross-Attention for Open Domain Question Answering",
        "text": "State-of-the-art Machine Reading Comprehension (MRC) models for Open-domain Question Answering (QA) are typically trained for span selection using distantly supervised positive examples and heuristically retrieved negative examples. This training scheme possibly explains empirical observations that these models achieve a high recall amongst their top few predictions, but a low overall accuracy, motivating the need for answer re-ranking. We develop a successful re-ranking approach (RECONSIDER) for span-extraction tasks that improves upon the performance of MRC models, even beyond large-scale pre-training. RE-CONSIDER is trained on positive and negative examples extracted from high confidence MRC model predictions, and uses in-passage span annotations to perform span-focused reranking over a smaller candidate set. As a result, RECONSIDER learns to eliminate close false positives, achieving a new extractive state of the art on four QA tasks, with 45.5% Exact Match accuracy on Natural Questions with real user questions, and 61.7% on TriviaQA. We will release all related data, models, and code 1 . * Work done while at Facebook AI. 1 github.com/facebookresearch/reconsider",
        "id": 235097577
      },
      {
        "title": "Dialect Identification under Domain Shift: Experiments with Discriminating Romanian and Moldavian",
        "text": "This paper describes a set of experiments for discriminating between two closely related language varieties, Moldavian and Romanian, under a substantial domain shift. The experiments were conducted as part of the Romanian dialect identification task in the VarDial 2020 evaluation campaign. Our best system based on linear SVM classifier obtained the first position in the shared task with an F1 score of 0.79, supporting the earlier results showing (unexpected) success of machine learning systems in this task. The additional experiments reported in this paper also show that adapting to the test set is useful when the training set is from another domain. However, the benefit of adaptation becomes doubtful even when using a small amount of data from the target domain.This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/.",
        "id": 227231071
      },
      {
        "title": "Temporal Expressions in Japanese-to-English Machine Translation",
        "text": "This paper describes in outline a method for translating Japanese temporal expressions into English. We argue that temporal expressions form a special subset of language that is best handled as a special module in machine translation. The paper deals with problems of lexical idiosyncrasy as well as the choice of articles and prepositions within temporal expressions. In addition temporal expressions are considered as parts of larger structures, and the question of whether to translate them as noun phrases or adverbials is addressed.",
        "id": 3264879
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest a study examining how transformer models utilize feed-forward neural networks (FFNs) to encode factual information?",
    "positive_ctxs": [
      {
        "title": "Transformer Feed-Forward Layers Are Key-Value Memories",
        "text": "Feed-forward layers constitute two-thirds of a transformer model's parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformerbased language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys' input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model's layers via residual connections to produce the final output distribution.",
        "id": 229923720
      }
    ],
    "negative_ctxs": [
      {
        "title": "A Multi-party Multi-modal Dataset for Focus of Visual Attention in Human-human and Human-robot Interaction",
        "text": "This papers describes a data collection setup and a newly recorded dataset. The main purpose of this dataset is to explore patterns in the focus of visual attention of humans under three different conditions -two humans involved in task-based interaction with a robot; same two humans involved in task-based interaction where the robot is replaced by a third human, and a free three-party human interaction. The dataset contains two parts -6 sessions with duration of approximately 3 hours and 9 sessions with duration of approximately 4.5 hours. Both parts of the dataset are rich in modalities and recorded data streams -they include the streams of three Kinect v2 devices (color, depth, infrared, body and face data), three high quality audio streams, three high resolution GoPro video streams, touch data for the task-based interactions and the system state of the robot. In addition, the second part of the dataset introduces the data streams from three Tobii Pro Glasses 2 eye trackers. The language of all interactions is English and all data streams are spatially and temporally aligned.",
        "id": 39290532
      },
      {
        "title": "Two-Neighbor Orientation Model with Cross-Boundary Global Contexts",
        "text": "Long distance reordering remains one of the greatest challenges in statistical machine translation research as the key contextual information may well be beyond the confine of translation units. In this paper, we propose Two-Neighbor Orientation (TNO) model that jointly models the orientation decisions between anchors and two neighboring multi-unit chunks which may cross phrase or rule boundaries. We explicitly model the longest span of such chunks, referred to as Maximal Orientation Span, to serve as a global parameter that constrains underlying local decisions. We integrate our proposed model into a state-of-the-art string-to-dependency translation system and demonstrate the efficacy of our proposal in a large-scale Chinese-to-English translation task. On NIST MT08 set, our most advanced model brings around +2.0 BLEU and -1.0 TER improvement.",
        "id": 5665391
      },
      {
        "title": "Anchor Text Extraction for Academic Search",
        "text": "*",
        "id": 3176893
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there a paper that uses similarity scores to check knowledge in diffusion models",
    "positive_ctxs": [
      {
        "title": "Multilingual Conceptual Coverage in Text-to-Image Models",
        "text": "AltDiffusion DallE 2 StableDiffusion 2 DallE mega 1Figure 1: A selection of images generated by DALLE-mega, Stable Diffusion 2, DALLE-2, and AltDiffusion, illustrating their conceptual coverage of \"dog,\" \"airplane,\" and \"face\" across English, Spanish, German, Chinese (simplified), Japanese, Hebrew, and Indonesian. Coverage of the concepts varies considerably across model and language, and can be observed in the consistency and correctness of images generated under simple prompts.AbstractWe propose \"Conceptual Coverage Across Languages\" (CoCo-CroLa), a technique for benchmarking the degree to which any generative text-to-image system provides multilingual parity to its training language in terms of tangible nouns. For each model we can assess \"conceptual coverage\" of a given target language relative to a source language by comparing the population of images generated for a series of tangible nouns in the source language to the population of images generated for each noun under translation in the target language. This technique allows us to estimate how wellsuited a model is to a target language as well as identify model-specific weaknesses, spurious correlations, and biases without a-priori assumptions. We demonstrate how it can be used to benchmark T2I models in terms of multilinguality, and how despite its simplicity it is a good proxy for impressive generalization. . 2021. Peco: Examining single sentence label leakage in natural language inference datasets through progressive evaluation of cluster outliers. ArXiv preprint, abs/2112.09237.",
        "id": 259063887
      }
    ],
    "negative_ctxs": [
      {
        "title": "Implicit Proposal Filtering in Multi-Party Consensus-Building Conversations",
        "text": "An attempt was made to statistically estimate proposals which survived the discussion to be incorporated in the final agreement in an instance of a Japanese design conversation. Low level speech and vision features of hearer behaviors corresponding to aiduti, noddings and gaze were found to be a positive predictor of survival. The result suggests that non-linguistic hearer responses work as implicit proposal filters in consensus building, and could provide promising candidate features for the purpose of recognition and summarization of meeting events.",
        "id": 1286260
      },
      {
        "title": "Extracting Important Sentences with Support Vector Machines",
        "text": "Extracting sentences that contain important information from a document is a form of text summarization. The technique is the key to the automatic generation of summaries similar to those written by humans. To achieve such extraction, it is important to be able to integrate heterogeneous pieces of information. One approach, parameter tuning by machine learning, has been attracting a lot of attention. This paper proposes a method of sentence extraction based on Support Vector Machines (SVMs). To confirm the method's performance, we conduct experiments that compare our method to three existing methods. Results on the Text Summarization Challenge (TSC) corpus show that our method offers the highest accuracy. Moreover, we clarify the different features effective for extracting different document genres.",
        "id": 709985
      },
      {
        "title": "Unanimous Prediction for 100% Precision with Application to Learning Semantic Mappings",
        "text": "Can we train a system that, on any new input, either says \"don't know\" or makes a prediction that is guaranteed to be correct? We answer the question in the affirmative provided our model family is wellspecified. Specifically, we introduce the unanimity principle: only predict when all models consistent with the training data predict the same output. We operationalize this principle for semantic parsing, the task of mapping utterances to logical forms. We develop a simple, efficient method that reasons over the infinite set of all consistent models by only checking two of the models. We prove that our method obtains 100% precision even with a modest amount of training data from a possibly adversarial distribution. Empirically, we demonstrate the effectiveness of our approach on the standard GeoQuery dataset.",
        "id": 7303682
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which paper formally defines the problem of model selection in llm agent for multi-modal reasoning?",
    "positive_ctxs": [
      {
        "title": "TOWARDS ROBUST MULTI-MODAL REASONING VIA MODEL SELECTION",
        "text": "The reasoning capabilities of LLM (Large Language Model) are widely acknowledged in recent research, inspiring studies on tool learning and autonomous agents. LLM serves as the \"brain\" of agent, orchestrating multiple tools for collaborative multi-step task solving. Unlike methods invoking tools like calculators or weather APIs for straightforward tasks, multi-modal agents excel by integrating diverse AI models for complex challenges. However, current multi-modal agents neglect the significance of model selection: they primarily focus on the planning and execution phases, and will only invoke predefined task-specific models for each subtask, making the execution fragile. Meanwhile, other traditional model selection methods are either incompatible with or suboptimal for the multi-modal agent scenarios, due to ignorance of dependencies among subtasks arising by multi-step reasoning. To this end, we identify the key challenges therein and propose the M 3 framework as a plug-in with negligible runtime overhead at test-time. This framework improves model selection and bolsters the robustness of multi-modal agents in multi-step reasoning. In the absence of suitable benchmarks, we create MS-GQA, a new dataset specifically designed to investigate the model selection challenge in multi-modal agents. Our experiments reveal that our framework enables dynamic model selection, considering both user inputs and subtask dependencies, thereby robustifying the overall reasoning process. Our code and benchmark: https://github.com/LINs-lab/M3.",
        "id": 263909212
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "In this paper, we argue in favor of reconsidering models for word meaning, using as a basis results from cognitive science on human concept representation. More specifically, we argue for a more flexible representation of word meaning than the assignment of a single best-fitting dictionary sense to each occurrence: Either use dictionary senses, but view them as having fuzzy boundaries, and assume that an occurrence can activate multiple senses to different degrees. Or move away from dictionary senses completely, and only model similarities between individual word usages. We argue that distributional models provide a flexible framework for experimenting with alternative models of word meanings, and discuss example models.",
        "id": 17640849
      },
      {
        "title": "A Fast and Accurate Vietnamese Word Segmenter",
        "text": "We propose a novel approach to Vietnamese word segmentation. Our approach is based on the Single Classification Ripple Down Rules methodology(Compton and Jansen, 1990), where rules are stored in an exception structure and new rules are only added to correct segmentation errors given by existing rules. Experimental results on the benchmark Vietnamese treebank show that our approach outperforms previous state-of-the-art approaches JVnSegmenter, vnTokenizer, DongDu and UETsegmenter in terms of both accuracy and performance speed. Our code is open-source and available at: https://github.com/datquocnguyen/RDRsegmenter.",
        "id": 871724
      },
      {
        "title": "Context-Sensitive Syntactic Source-Reordering by Statistical Transduction",
        "text": "How well can a phrase translation model perform if we permute the source words to fit target word order as perfectly as word alignment might allow? And how well would it perform if we limit the allowed permutations to ITGlike tree-transduction operations on the source parse tree? First we contribute oracle results showing great potential for performance improvement by source-reordering, ranging from 1.5 to 4 BLEU points depending on language pair. Although less outspoken, the potential of tree-based source-reordering is also significant. Our second contribution is a source reordering model that works with two kinds of tree transductions: the one permutes the order of sibling subtrees under a node, and the other first deletes layers in the parse tree in order to exploit sibling permutation at the remaining levels.The statistical parameters of the model we introduce concern individual tree transductions conditioned on contextual features of the tree resulting from all preceding transductions. Experiments in translating from English to Spanish/Dutch/Chinese show significant improvements of respectively 0.6/1.2/2.0 BLEU points.",
        "id": 17261224
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that offers an in-depth examination of the shortcomings associated with pretraining evaluation measures such as BERTScore, particularly regarding their alignment with human evaluative assessments?",
    "positive_ctxs": [
      {
        "title": "A Fine-Grained Analysis of BERTScore",
        "text": "BERTScore(Zhang et al., 2020), a recently proposed automatic metric for machine translation quality, uses BERT (Devlin et al., 2019), a large pre-trained language model to evaluate candidate translations with respect to a gold translation. Taking advantage of BERT's semantic and syntactic abilities, BERTScore seeks to avoid the flaws of earlier approaches like BLEU, instead scoring candidate translations based on their semantic similarity to the gold sentence. However, BERT is not infallible; while its performance on NLP tasks set a new state of the art in general, studies of specific syntactic and semantic phenomena have shown where BERT's performance deviates from that of humans more generally.This naturally raises the questions we address in this paper: what are the strengths and weaknesses of BERTScore? Do they relate to known weaknesses on the part of BERT? We find that while BERTScore can detect when a candidate differs from a reference in important content words, it is less sensitive to smaller errors, especially if the candidate is lexically or stylistically similar to the reference.",
        "id": 245855878
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 222141011
      },
      {
        "title": "Thai Grapheme-Based Speech Recognition",
        "text": "In this paper we present the results for building a grapheme-based speech recognition system for Thai. We experiment with different settings for the initial context independent system, different number of acoustic models and different contexts for the speech unit. In addition, we investigate the potential of an enhanced tree clustering method as a way of sharing parameters across models. We compare our system with two phoneme-based systems; one that uses a hand-crafted dictionary and another that uses an automatically generated dictionary. Experiment results show that the grapheme-based system with enhanced tree clustering outperforms the phoneme-based system using an automatically generated dictionary, and has comparable results to the phoneme-based system with the handcrafted dictionary.",
        "id": 5153552
      },
      {
        "title": "Learning to Identify Metaphors from a Corpus of Proverbs",
        "text": "In this paper, we experiment with a resource consisting of metaphorically annotated proverbs on the task of word-level metaphor recognition. We observe that existing feature sets do not perform well on this data. We design a novel set of features to better capture the peculiar nature of proverbs and we demonstrate that these new features are significantly more effective on the metaphorically dense proverb data.",
        "id": 14555476
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Can we find the solution of the Bilevel optimization when the lower-level problem is nonconvex?",
    "positive_ctxs": [
      {
        "title": "On Penalty Methods for Nonconvex Bilevel Optimization and First-Order Stochastic Approximation",
        "text": "In this work, we study first-order algorithms for solving Bilevel Optimization (BO) where the objective functions are smooth but possibly nonconvex in both levels and the variables are restricted to closed convex sets. As a first step, we study the landscape of BO through the lens of penalty methods, in which the upper-and lower-level objectives are combined in a weighted sum with penalty parameter σ > 0. In particular, we establish a strong connection between the penalty function and the hyper-objective by explicitly characterizing the conditions under which the values and derivatives of the two must be O(σ)-close. A by-product of our analysis is the explicit formula for the gradient of hyper-objective when the lower-level problem has multiple solutions under minimal conditions, which could be of independent interest. Next, viewing the penalty formulation as O(σ)-approximation of the original BO, we propose firstorder algorithms that find an ϵ-stationary solution by optimizing the penalty formulation with σ = O(ϵ). When the perturbed lower-level problem uniformly satisfies the small-error proximal error-bound (EB) condition, we propose a first-order algorithm that converges to an ϵ-stationary point of the penalty function, using in total O(ϵ −3 ) and O(ϵ −7 ) accesses to first-order (stochastic) gradient oracles when the oracle is deterministic and oracles are noisy, respectively. Under an additional assumption on stochastic oracles, we show that the algorithm can be implemented in a fully single-loop manner, i.e., with O(1) samples per iteration, and achieves the improved oracle-complexity of O(ϵ −3 ) and O(ϵ −5 ), respectively.",
        "id": 261530996
      }
    ],
    "negative_ctxs": [
      {
        "title": "Domain Specific Word Extraction from Hierarchical Web Documents: A First Step Toward Building Lexicon Trees from Web Corpora",
        "text": "Domain specific words and ontological information among words are important resources for general natural language applications. This paper proposes a statistical model for finding domain specific words (DSW s) in particular domains, and thus building the association among them. When applying this model to the hierarchical structure of the web directories node-by-node, the document tree can potentially be converted into a large semantically annotated lexicon tree. Some preliminary results show that the current approach is better than a conventional TF-IDF approach for measuring domain specificity. An average precision of 65.4% and an average recall of 36.3% are observed if the top-10% candidates are extracted as domain-specific words.",
        "id": 15313951
      },
      {
        "title": "A Syntax-Aware Edit-based System for Text Simplification",
        "text": "Edit-based text simplification systems have attained much attention in recent years due to their ability to produce simplification solutions that are interpretable, as well as requiring less training examples compared to traditional seq2seq systems. Edit-based systems learn edit operations at a word level, but it is well known that many of the operations performed when simplifying text are of a syntactic nature. In this paper we propose to add syntactic information into a well known editbased system. We extend the system with a graph convolutional network module that mimics the dependency structure of the sentence, thus giving the model an explicit representation of syntax. We perform a series of experiments in English, Spanish and Italian, and report improvements of the state of the art in four out of five datasets. Further analysis shows that syntactic information is always beneficial, and suggest that syntax is more helpful in complex sentences.",
        "id": 243891435
      },
      {
        "title": "",
        "text": "Mental health is getting more and more attention recently, depression being a very common illness nowadays, but also other disorders like anxiety, obsessive-compulsive disorders, feeding disorders, autism, or attention-deficit/hyperactivity disorders. The huge amount of data from social media and the recent advances of deep learning models provide valuable means to automatically detecting mental disorders from plain text. In this article, we experiment with state-of-the-art methods on the SMHD mental health conditions dataset from Reddit (Cohan et al.,  2018). Our contribution is threefold: using a dataset consisting of more illnesses than most studies, focusing on general text rather than mental health support groups and classification by posts rather than individuals or groups. For the automatic classification of the diseases, we employ three deep learning models: BERT, RoBERTa and XLNET. We double the baseline established byCohan et al. (2018), on just a sample of their dataset. We improve the results obtained by Jiang et al. (2020) on post-level classification. The accuracy obtained by the eating disorder classifier is the highest due to the pregnant presence of discussions related to calories, diets, recipes etc., whereas depression had the lowest F1 score, probably because depression is more difficult to identify in linguistic acts.UEFISCDI, project number 108, COTOHILI, within PNCDI III.",
        "id": 243907938
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research that explores identifying excess and insufficient translations in evaluating machine translation, especially regarding resolving label discrepancies in cases of omitted content?",
    "positive_ctxs": [
      {
        "title": "Detecting Over-and Undertranslations with Contrastive Conditioning",
        "text": "Omission and addition of content is a typical issue in neural machine translation. We propose a method for detecting such phenomena with off-the-shelf translation models. Using contrastive conditioning, we compare the likelihood of a full sequence under a translation model to the likelihood of its parts, given the corresponding source or target sequence. This allows to pinpoint superfluous words in the translation and untranslated words in the source even in the absence of a reference translation. The accuracy of our method is comparable to a supervised method that requires a custom quality estimation model.",
        "id": 247223093
      }
    ],
    "negative_ctxs": [
      {
        "title": "CREAD: Combined Resolution of Ellipses and Anaphora in Dialogues",
        "text": "Anaphora and ellipses are two common phenomena in dialogues. Without resolving referring expressions and information omission, dialogue systems may fail to generate consistent and coherent responses. Traditionally, anaphora is resolved by coreference resolution and ellipses by query rewrite. In this work, we propose a novel joint learning framework of modeling coreference resolution and query rewriting for complex, multi-turn dialogue understanding. Given an ongoing dialogue between a user and a dialogue assistant, for the user query, our joint learning model first predicts coreference links between the query and the dialogue context, and then generates a selfcontained rewritten user query. To evaluate our model, we annotate a dialogue based coreference resolution dataset, MuDoCo, with rewritten queries. Results show that the performance of query rewrite can be substantially boosted (+2.3% F1) with the aid of coreference modeling. Furthermore, our joint model outperforms the state-of-the-art coreference resolution model (+2% F1) on this dataset. Daniel M Bikel, Vittorio Castelli, Radu Florian, and  Ding-jung Han. 2009. Entity linking and slot filling through statistical processing and inference rules. In TAC.Anders Björkelund and Jonas Kuhn. 2014. Learning structured perceptrons for coreference resolution with latent antecedents and non-local features. In",
        "id": 234790024
      },
      {
        "title": "Published as a conference paper at ICLR 2023 GRAPH NEURAL NETWORK-INSPIRED KERNELS FOR GAUSSIAN PROCESSES IN SEMI-SUPERVISED LEARN- ING",
        "text": "Gaussian processes (GPs) are an attractive class of machine learning models because of their simplicity and flexibility as building blocks of more complex Bayesian models. Meanwhile, graph neural networks (GNNs) emerged recently as a promising class of models for graph-structured data in semi-supervised learning and beyond. Their competitive performance is often attributed to a proper capturing of the graph inductive bias. In this work, we introduce this inductive bias into GPs to improve their predictive performance for graph-structured data. We show that a prominent example of GNNs, the graph convolutional network, is equivalent to some GP when its layers are infinitely wide; and we analyze the kernel universality and the limiting behavior in depth. We further present a programmable procedure to compose covariance kernels inspired by this equivalence and derive example kernels corresponding to several interesting members of the GNN family. We also propose a computationally efficient approximation of the covariance matrix for scalable posterior inference with large-scale data. We demonstrate that these graph-based kernels lead to competitive classification and regression performance, as well as advantages in computation time, compared with the respective GNNs. Published as a conference paper at ICLR 2023 can be recursively computed if the weights (and biases) in each layer are iid Gaussian. Similar results for other architectures, such as convolution layers and residual connections, were subsequently established in the literature (Novak et al., 2019; Garriga-Alonso et al., 2019).One focus of this work is to establish a similar relationship between GNNs and the limiting GPs. We will derive the covariance kernel that incorporates the graph inductive bias as GNNs do. We start with one of the most widely studied GNNs, the graph convolutional network (GCN)(Kipf & Welling, 2017), and analyze the kernel universality as well as the limiting behavior when the depth also tends to infinity. We then derive covariance kernels from other GNNs by using a programmable procedure that corresponds every building block of a neural network to a kernel operation.Meanwhile, we design efficient computational procedures for posterior inference (i.e., regression and classification). GPs are notoriously difficult to scale because of the cubic complexity with respect to the number of training data. Benchmark graph datasets used by the GNN literature may contain thousands or even millions of labeled nodes(Hu et al., 2020b). The semi-supervised setting worsens the scenario, as the covariance matrix needs to be (recursively) evaluated in full because of the graph convolution operation. We propose a Nyström-like scheme to perform low-rank approximations and apply the approximation recursively on each layer, to yield a low-rank kernel matrix.",
        "id": 256827686
      },
      {
        "title": "LEARNING AUDIO-VISUAL SPEECH REPRESENTATION BY MASKED MULTIMODAL CLUSTER PREDICTION",
        "text": "Video recordings of speech contain correlated audio and visual information, providing a strong signal for speech representation learning from the speaker's lip movements and the produced sound. We introduce Audio-Visual Hidden Unit BERT (AV-HuBERT), a self-supervised representation learning framework for audio-visual speech, which masks multi-stream video input and predicts automatically discovered and iteratively refined multimodal hidden units. AV-HuBERT learns powerful audio-visual speech representation benefiting both lip-reading and automatic speech recognition. On the largest public lip-reading benchmark LRS3 (433 hours), AV-HuBERT achieves 32.5% WER with only 30 hours of labeled data, outperforming the former state-of-the-art approach (33.6%) trained with a thousand times more transcribed video data (31K hours) (Makino et al., 2019). The lip-reading WER is further reduced to 26.9% when using all 433 hours of labeled data from LRS3 and combined with self-training. Using our audio-visual representation on the same benchmark for audio-only speech recognition leads to a 40% relative WER reduction over the state-of-the-art performance (1.3% vs 2.3%). Our code and models are available at https://github.com/ facebookresearch/av_hubert * Work done at Meta AI Published as a conference paper at ICLR 2022 including but not limited to keyword spotting in sign language (Albanie et al., 2020), speech enhancement (Xu et al., 2020) and talking face generation(Chen et al., 2018).In this paper, we present Audio-Visual Hidden Unit BERT (AV-HuBERT), a multimodal selfsupervised speech representation learning framework. It encodes masked audio and image sequences into audio-visual features via a hybrid ResNet-transformer architecture to predict the predetermined sequence of discrete cluster assignments. The target cluster assignments are initially generated from signal processing-based acoustic features (e.g., MFCC) and iteratively refined using the features learned by the audio-visual encoder via k-means clustering. AV-HuBERT simultaneously captures linguistic and phonetic information for unmasked regions from both the lipmovement and audio streams into its latent representations, then encodes their long-range temporal relationships to solve the masked-prediction task.The contextualized representations learned by AV-HuBERT show excellent transferability to the lipreading task, where only the visual modality is available. Pre-training on audio and visual input streams led to substantially better results than only visual input. In the low-resource setup using only 30 hours of labeled data from LRS3 (Afouras et al., 2018b), our model achieves a lip-reading WER of 32.5%, outperforming the previous state-of-the-art model (33.6%) trained on 31,000 hours of transcribed videos (Makino et al., 2019). Using the complete 433 hours from LRS3 further reduces WER to 28.6%. We further show AV-HuBERT and self-training are complementary to each other: combining both sets a new lip-reading WER record of 26.9%. In addition, we show that the multimodal clusters derived from AV-HuBERT can be used to pre-train a HuBERT model for audio-based speech recognition, outperforming the previous state-of-the-art model (2.3%) and the unimodal HuBERT pre-trained on audio clusters (1.5%) by a large margin (1.3%).",
        "id": 245769552
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which paper first derived online occupany estimation technique to get sqrt(T) bound for reinforcement learning in adversarial linear MDP?",
    "positive_ctxs": [
      {
        "title": "Towards Optimal Regret in Adversarial Linear MDPs with Bandit Feedback",
        "text": "We study online reinforcement learning in linear Markov decision processes with adversarial losses and bandit feedback, without prior knowledge on transitions or access to simulators.We introduce two algorithms that achieve improved regret performance compared to existing approaches.The first algorithm, although computationally inefficient, ensures a regret of O( √ K), where K is the number of episodes.This is the first result with the optimal K dependence in the considered setting.The second algorithm, which is based on the policy optimization framework, guarantees a regret of O(K 3 /4 ) and is computationally efficient.Both our results significantly improve over the state-of-the-art: a computationally inefficient algorithm by Kong et al. [2023]  with O(K 4 /5 + poly( 1 /λmin)) regret, for some problem-dependent constant λ min that can be arbitrarily close to zero, and a computationally efficient algorithm by Sherman et al. [2023b]  with O(K 6 /7 ) regret.* The authors are listed in alphabetical order.is the horizon length.The challenge is that this conversion depends on the transition of the MDP, which is not available to the learner.Therefore, the learner has to estimate the feature of every policy during the learning process.Previous work in this direction [Kong et al., 2023]  faced obstacles in controlling the estimation error and was only able to show a K 4 /5 +poly( 1 /λ min ) regret bound assuming there exists an exploratory policy inducing a covariance matrix λ min I.We addressed the obstacles through 1) state space discretization (Section 3.2), and 2) model-free estimation for the occupancy measure of policies over the discretized state space (Section 3.3).These allow us to emulate the success in the tabular case [Jin et al., 2020a] and obtain the tight √ K regret.Efficient K 3 /4 algorithm.The efficient algorithm is based on the policy optimization framework[Luo et al., 2021].Different from previous works that all use exponential weights, we use Follow-the-Regularized-Leader (FTRL) with log-determinant (logdet) barrier regularizer to perform policy updates, which has the benefit of keeping the algorithm more stable [Zimmert and Lattimore, 2022, Liu et al., 2023a].We carefully combine logdet-FTRL with existing algorithmic/analysis techniques to further improve the regret bound.These include 1) an initial exploration phase to control the transition estimation error [Sherman et al., 2023a], 2) optimistic least-square policy evaluation in bonus construction [Sherman et al., 2023b], 3) dilated bonus construction[Luo et al., 2021], and 4) a tighter concentration bound for covariance matrix estimation [Liu et al., 2023a].Related WorkIn this subsection, we review prior works on adversarial MDPs and policy optimization.Learning in Adversarial MDPs.Adversarial MDPs refer to a class of MDP problems where the transition is fixed while the loss function changes over time.Learning adversarial tabular MDPs under bandit feedback and unknown transition has been extensively studied [",
        "id": 264288929
      }
    ],
    "negative_ctxs": [
      {
        "title": "A Latent Topic Extracting Method based on Events in a Document and its Application",
        "text": "Recently, several latent topic analysis methods such as LSI, pLSI, and LDA have been widely used for text analysis. However, those methods basically assign topics to words, but do not account for the events in a document. With this background, in this paper, we propose a latent topic extracting method which assigns topics to events. We also show that our proposed method is useful to generate a document summary based on a latent topic.",
        "id": 15648889
      },
      {
        "title": "Chapter Captor: Text Segmentation in Novels",
        "text": "Books are typically segmented into chapters and sections, representing coherent subnarratives and topics. We investigate the task of predicting chapter boundaries, as a proxy for the general task of segmenting long texts. We build a Project Gutenberg chapter segmentation data set of 9,126 English novels, using a hybrid approach combining neural inference and rule matching to recognize chapter title headers in books, achieving an F1-score of 0.77 on this task. Using this annotated data as ground truth after removing structural cues, we present cut-based and neural methods for chapter segmentation, achieving an F1-score of 0.453 on the challenging task of exact break prediction over book-length documents. Finally, we reveal interesting historical trends in the chapter structure of novels.",
        "id": 226262395
      },
      {
        "title": "Cross-linguistic annotation of narrativity for English/French verb tense disambiguation",
        "text": "This paper presents manual and automatic annotation experiments for a pragmatic verb tense feature (narrativity) in English/French parallel corpora. The feature is considered to play an important role for translating English Simple Past tense into French, where three different tenses are available. Whether the French Passé Composé, Passé Simple or Imparfait should be used is highly dependent on a longer-range context, in which either narrative events ordered in time or mere non-narrative state of affairs in the past are described. This longer-range context is usually not available to current machine translation (MT) systems, that are trained on parallel corpora. Annotating narrativity prior to translation is therefore likely to help current MT systems. Our experiments show that narrativity can be reliably identified with kappa-values of up to 0.91 in manual annotation and with F1 scores of up to 0.72 in automatic annotation.",
        "id": 8200466
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research that explores how the loss of spatial information impacts the effectiveness of global features in visual tasks?",
    "positive_ctxs": [
      {
        "title": "ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension",
        "text": "Training a referring expression comprehension (ReC) model for a new visual domain requires collecting referring expressions, and potentially corresponding bounding boxes, for images in the domain. While large-scale pre-trained models are useful for image classification across domains, it remains unclear if they can be applied in a zero-shot manner to more complex tasks like ReC. We present ReCLIP, a simple but strong zero-shot baseline that repurposes CLIP, a state-of-the-art large-scale model, for ReC. Motivated by the close connection between ReC and CLIP's contrastive pre-training objective, the first component of ReCLIP is a region-scoring method that isolates object proposals via cropping and blurring, and passes them to CLIP. However, through controlled experiments on a synthetic dataset, we find that CLIP is largely incapable of performing spatial reasoning off-the-shelf. Thus, the second component of ReCLIP is a spatial relation resolver that handles several types of spatial relations. We reduce the gap between zero-shot baselines from prior work and supervised models by as much as 29% on RefCOCOg, and on RefGTA (video game imagery), ReCLIP's relative improvement over supervised ReC models trained on real images is 8%. * This work was done while Sanjay, Will, and Matt were affiliated with AI2. (a) RefCOCO+ (Yu et al., 2016) (b) RefGTA (Tanaka et al., 2019) . 2017. Bottom-up and top-down attention for image captioning and vqa. ArXiv, abs/1707.07998.",
        "id": 248118561
      }
    ],
    "negative_ctxs": [
      {
        "title": "Impact of Politically Biased Data on Hate Speech Classification",
        "text": "One challenge that social media platforms are facing nowadays is hate speech. Hence, automatic hate speech detection has been increasingly researched in recent years -in particular with the rise of deep learning. A problem of these models is their vulnerability to undesirable bias in training data. We investigate the impact of political bias on hate speech classification by constructing three politicallybiased data sets (left-wing, right-wing, politically neutral) and compare the performance of classifiers trained on them. We show that (1) political bias negatively impairs the performance of hate speech classifiers and (2) an explainable machine learning model can help to visualize such bias within the training data. The results show that political bias in training data has an impact on hate speech classification and can become a serious issue.",
        "id": 226284001
      },
      {
        "title": "A Language-Independent Unsupervised Model for Morphological Segmentation",
        "text": "Morphological segmentation has been shown to be beneficial to a range of NLP tasks such as machine translation, speech recognition, speech synthesis and information retrieval. Recently, a number of approaches to unsupervised morphological segmentation have been proposed. This paper describes an algorithm that draws from previous approaches and combines them into a simple model for morphological segmentation that outperforms other approaches on English and German, and also yields good results on agglutinative languages such as Finnish and Turkish. We also propose a method for detecting variation within stems in an unsupervised fashion. The segmentation quality reached with the new algorithm is good enough to improve grapheme-to-phoneme conversion.",
        "id": 6645042
      },
      {
        "title": "Inter-rater Agreement Measures and the Refinement of Metrics in the PLATO MT Evaluation Paradigm",
        "text": "The PLATO machine translation (MT) evaluation (MTE) research program has as a goal the systematic development of a predictive relationship between discrete, welldefined MTE metrics and the specific information processing tasks that can be reliably performed with output. Traditional measures of quality, informed by the International Standards for Language Engineering (ISLE), namely, clarity, coherence, morphology, syntax, general and domain-specific lexical robustness, and named-entity translation, as well as a DARPAinspired measure of adequacy are its core. For robust validation, indispensable for refinement of tests and guidelines, we measure inter-rater reliability on the assessments. Here we report on our results, focusing on the PLATO Clarity and Coherence assessments, and we discuss our method for iteratively refining both the linguistic metrics and the guidelines for applying them within the PLATO evaluation paradigm. Finally, we discuss reasons why kappa might not be the best measure of interrater agreement for our purposes, and suggest directions for future investigation.",
        "id": 2528404
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "What research is available on the concept of using prefix tokens as a parameter-efficient method for fine-tuning language models?",
    "positive_ctxs": [
      {
        "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
        "text": "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were \"virtual tokens\". We apply prefix-tuning to GPT-2 for table-totext generation and to BART for summarization. We show that by modifying only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. 2020. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. Amodei. 2020. Language models are few-shot learners.",
        "id": 230433941
      }
    ],
    "negative_ctxs": [
      {
        "title": "I Cast Detect Thoughts: Learning to Converse and Guide with Intents and Theory-of-Mind in Dungeons and Dragons",
        "text": "We propose a novel task, G4C, to study teacher-student natural language interactions in a goal-driven and grounded environment. Dungeons and Dragons (D&D), a role-playing game, provides an ideal setting to investigate such interactions. Here, the Dungeon Master (DM), i.e., the teacher, guides the actions of several players-students, each with their own personas and abilities-to achieve shared goals grounded in a fantasy world. Our approach is to decompose and model these interactions into (1) the DM's intent to guide players towards a given goal; (2) the DM's guidance utterance to the players expressing this intent; and (3) a theory-of-mind (ToM) model that anticipates the players' reaction to the guidance one turn into the future. We develop a novel reinforcement learning (RL) method for training a DM that generates guidance for players by rewarding utterances where the intent matches the ToM-anticipated player actions. Human and automated evaluations show that a DM trained to explicitly model intents and incorporate ToM of the players using RL generates better-quality guidance that is 3x more likely to fulfill the DM's intent than a vanilla natural language generation (NLG) approach.",
        "id": 258987457
      },
      {
        "title": "Substring-based unsupervised transliteration with phonetic and contextual knowledge",
        "text": "We propose an unsupervised approach for substring-based transliteration which incorporates two new sources of knowledge in the learning process: (i) context by learning substring mappings, as opposed to single character mappings, and (ii) phonetic features which capture cross-lingual character similarity via prior distributions.Our approach is a two-stage iterative, boot-strapping solution, which vastly outperforms Ravi and Knight (2009)'s state-of-the-art unsupervised transliteration method and outperforms a rule-based baseline by up to 50% for top-1 accuracy on multiple language pairs. We show that substring-based models are superior to character-based models, and observe that their top-10 accuracy is comparable to the top-1 accuracy of supervised systems.Our method only requires a phonemic representation of the words. This is possible for many language-script combinations which have a high grapheme-to-phoneme correspondence e.g. scripts of Indian languages derived from the Brahmi script. Hence, Indian languages were the focus of our experiments. For other languages, a grapheme-to-phoneme converter would be required.",
        "id": 573977
      },
      {
        "title": "Learning Comment Controversy Prediction in Web Discussions Using Incidentally Supervised Multi-Task CNNs",
        "text": "Comments on web news contain controversies that manifest as inter-group agreementconflicts. Tracking such rapidly evolving controversy could ease conflict resolution or journalist-user interaction. However, this presupposes controversy online-prediction that scales to diverse domains using incidental supervision signals instead of manual labeling. To more deeply interpret commentcontroversy model decisions we frame prediction as binary classification and evaluate baselines and multi-task CNNs that use an auxiliary news-genre-encoder. Finally, we use ablation and interpretability methods to determine the impacts of topic, discourse and sentiment indicators, contextual vs. global word influence, as well as genre-keywords vs. per-genrecontroversy keywords -to find that the models learn plausible controversy features using only incidentally supervised signals.",
        "id": 53235483
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "In the area of argument mining, could you point to literature that uses of dependency parsers to determine the argumentativeness of texts in dialogue systems?",
    "positive_ctxs": [
      {
        "title": "Dialo-AP: A Dependency Parsing Based Argument Parser for Dialogues",
        "text": "While neural approaches to argument mining (AM) have advanced considerably, most of the recent work has been limited to parsing monologues. With an urgent interest in the use of conversational agents for broader societal applications, there is a need to advance the stateof-the-art in argument parsers for dialogues. This enables progress towards more purposeful conversations involving persuasion, debate and deliberation. This paper discusses Dialo-AP, an end-to-end argument parser that constructs argument graphs from dialogues. We formulate AM as dependency parsing of elementary and argumentative discourse units; the system is trained using extensive pre-training and curriculum learning comprising nine diverse corpora. Dialo-AP is capable of generating argument graphs from dialogues by performing all subtasks of AM. Compared to existing state-ofthe-art baselines, Dialo-AP achieves significant improvements across all tasks, which is further validated through rigorous human evaluation.",
        "id": 252818918
      }
    ],
    "negative_ctxs": [
      {
        "title": "Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them",
        "text": "Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between \"gender-neutralized\" words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.",
        "id": 73729169
      },
      {
        "title": "Statistical methods for retrieving most significant paragraphs in newspaper articles • jea@dt fct uni pt",
        "text": "Retrieving a most stgulficant paragraph m a newspaper arUcle can act as a kind of surnmanzatmn It can gwe the human reader some hints on the contents of the arucle and help him to decide whether It deseei'ves a full readmg or not It may also act as a filter for a robust natural language understanding system, to extract relevant mformatton from that paragraph m order to enable conceptual mformauon retrieval Talang a newspaper arUcle and a base corpus, word co-occurrences w3th higher resolving power are ~dent~fied These co-occurrences are used to estabhsh hnks between the paragraphs of the arUcle The paragraph which presents the larger number of hnks tO other paragraphs ~s considered a most slgmficant one Though designed and tested for the Portuguese language, the staUshcal nature of our proposal should ensure ns portabtlny to other languages",
        "id": 14181397
      },
      {
        "title": "DISTRIBUTIONAL CLUSTERING OF ENGLISH WORDS",
        "text": "We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical \"soft\" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data.",
        "id": 6713452
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Have any papers tried to address the background-shift problem in named entity recognition by identifying non-entity type tokens belonging to old entity types through knowledge distillation from an old model?",
    "positive_ctxs": [
      {
        "title": "Distilling Causal Effect from Miscellaneous Other-Class for Continual Named Entity Recognition",
        "text": "Continual Learning for Named Entity Recognition (CL-NER) aims to learn a growing number of entity types over time from a stream of data. However, simply learning Other-Class in the same way as new entity types amplifies the catastrophic forgetting and leads to a substantial performance drop. The main cause behind this is that Other-Class samples usually contain old entity types, and the old knowledge in these Other-Class samples is not preserved properly. Thanks to the causal inference, we identify that the forgetting is caused by the missing causal effect from the old data. To this end, we propose a unified causal framework to retrieve the causality from both new entity types and Other-Class. Furthermore, we apply curriculum learning to mitigate the impact of label noise and introduce a self-adaptive weight for balancing the causal effects between new entity types and Other-Class. Experimental results on three benchmark datasets show that our method outperforms the state-of-theart method by a large margin. Moreover, our method can be combined with the existing stateof-the-art methods to improve the performance in CL-NER. 1",
        "id": 252780822
      }
    ],
    "negative_ctxs": [
      {
        "title": "Say the Right Thing Right: Ethics Issues in Natural Language Generation Systems",
        "text": "We discuss the ethical implications of Natural Language Generation systems. We use one particular system as a case study to identify and classify issues, and we provide an ethics checklist, in the hope that future system designers may benefit from conducting their own ethics reviews based on our checklist.",
        "id": 15291488
      },
      {
        "title": "Temporal/Locative WHs and Null-P Incorporation*",
        "text": "This paper is an investigation on the categorial status of locative and temporal WHs. We argue, based on empirical data, for the null P hypothesis proposed byHuang (1982), and against the proposal of Murasugi and Saito (1992) that when and where are sentential arguments. We suggest that the problem raised inMurasugi and Saito (1992)for the null P analysis can be solved by the null P incorporation hypothesis. We further address the issues related to the applications of the P incorporation.",
        "id": 16650242
      },
      {
        "title": "Novel Feature Discovery for Task-Oriented Dialog Systems",
        "text": "A novel feature represents a cluster of semantically equivalent novel user requests e.g., requests to play a song on a service or reading user's messages. Detecting and supporting novel features is crucial towards wider adoption of dialog systems by end users. Intuitively, features are represented by a combination of intents, slot types and/or their values. For example, while playing a song is a feature represented by a single intent (PlayMusic) only, playing a song on a service is another feature represented by the combination of PlayMusic intent and ServiceName slot type. Prior work on novelty detection limits the scope of features to those represented by novel single intents, leading to (1) giant clusters spanning several user-perceived fine-grained features belonging to the same intent, (2) incoherent interpretation of clusters from users' perspective (no direct connection to some user-perceived feature), and (3) missing those features spanning several intents. In this work, we introduce feature discovery as opposed to single intent discovery, which aims at discovering novel features spanning a combination of intents and slots, and present a technique for discovering novel features from user utterances. Experiments on two datasets demonstrate the effectiveness of our approach and consistently show its ability to detect novel features.",
        "id": 258378330
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "*Could you suggest a dataset with legally or ethically contentious content, and labels for acceptable and non-acceptable questions.",
    "positive_ctxs": [
      {
        "title": "SQUARE: A Large-Scale Dataset of Sensitive Questions and Acceptable Responses Created Through Human-Machine Collaboration",
        "text": "The potential social harms that large language models pose, such as generating offensive content and reinforcing biases, are steeply rising. Existing works focus on coping with this concern while interacting with ill-intentioned users, such as those who explicitly make hate speech or elicit harmful responses. However, discussions on sensitive issues can become toxic even if the users are well-intentioned. For safer models in such scenarios, we present the Sensitive Questions and Acceptable Response (SQUARE) dataset, a large-scale Korean dataset of 49k sensitive questions with 42k acceptable and 46k non-acceptable responses. The dataset was constructed leveraging HyperCLOVA in a human-in-the-loop manner based on real news headlines. Experiments show that acceptable response generation significantly improves for HyperCLOVA and GPT-3, demonstrating the efficacy of this dataset.ReferencesSanghwan Bae, Donghyun Kwak, Sungdong Kim, Donghoon Ham, Soyoung Kang, Sang-Woo Lee, and Woomyoung Park. 2022. Building a role specified open-domain dialogue system leveraging largescale language models. In . 2022. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In",
        "id": 258960423
      }
    ],
    "negative_ctxs": [
      {
        "title": "Exploitation of Co-reference in Distributional Semantics",
        "text": "The aim of distributional semantics is to model the similarity of the meaning of words via the words they occur with. Thereby, it relies on the distributional hypothesis implying that similar words have similar contexts. Deducing meaning from the distribution of words is interesting as it can be done automatically on large amounts of freely available raw text. It is because of this convenience that most current state-of-the-art-models of distributional semantics operate on raw text, although there have been successful attempts to integrate other kinds of-e.g., syntactic-information to improve distributional semantic models. In contrast, less attention has been paid to semantic information in the research community. One reason for this is that the extraction of semantic information from raw text is a complex, elaborate matter and in great parts not yet satisfyingly solved. Recently, however, there have been successful attempts to integrate a certain kind of semantic information, i.e., co-reference. Two basically different kinds of information contributed by co-reference with respect to the distribution of words will be identified. We will then focus on one of these and examine its general potential to improve distributional semantic models as well as certain more specific hypotheses.",
        "id": 23699712
      },
      {
        "title": "Sentiment Analysis and Topic Modeling for Public Perceptions of Air Travel: COVID Issues and Policy Amendments",
        "text": "Among many industries, air travel is impacted by the COVID pandemic. Airlines and airports rely on public sector information to enforce guidelines for ensuring health and safety of travelers. Such guidelines can be policy amendments or laws during the pandemic. In response to the inception of COVID preventive policies, travelers have exercised freedom of expression via the avenue of online reviews. This avenue facilitates voicing public concern while anonymizing / concealing user identity as needed. It is important to assess opinions on policy amendments to ensure transparency and openness, while also preserving confidentiality and ethics. Hence, this study leverages data science to analyze, with identity protection, the online reviews of airlines and airports since 2017, considering impacts of COVID issues and relevant policy amendments since 2020. Supervised learning with VADER sentiment analysis is deployed to predict changes in opinion from 2017 to date. Unsupervised learning with LDA topic modeling is employed to discover air travelers' major areas of concern before and after the pandemic. This study reveals that COVID policies have worsened public perceptions of air travel and aroused notable new concerns, affecting economics, environment and health.",
        "id": 252624562
      },
      {
        "title": "",
        "text": "",
        "id": 237010917
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "What is a paper studying data being collected in bundles in reinforcement learning ?",
    "positive_ctxs": [
      {
        "title": "Sample-Efficiency in Multi-Batch Reinforcement Learning: The Need for Dimension-Dependent Adaptivity",
        "text": "We theoretically explore the relationship between sample-efficiency and adaptivity in reinforcement learning.An algorithm is sample-efficient if it uses a number of queries n to the environment that is polynomial in the dimension d of the problem.Adaptivity refers to the frequency at which queries are sent and feedback is processed to update the querying strategy.To investigate this interplay, we employ a learning framework that allows sending queries in K batches, with feedback being processed and queries updated after each batch.This model encompasses the whole adaptivity spectrum, ranging from non-adaptive 'offline' (K \" 1) to fully adaptive (K \" n) scenarios, and regimes in between.For the problems of policy evaluation and best-policy identification under d-dimensional linear function approximation, we establish Ωplog log dq lower bounds on the number of batches K required for sample-efficient algorithms with n \" Oppolypdqq queries.Our results show that just having adaptivity (K ą 1) does not necessarily guarantee sampleefficiency.Notably, the adaptivity-boundary for sample-efficiency is not between offline reinforcement learning (K \" 1), where sample-efficiency was known to not be possible, and adaptive settings.Instead, the boundary lies between different regimes of adaptivity and depends on the problem dimension.",
        "id": 263609164
      }
    ],
    "negative_ctxs": [
      {
        "title": "Evaluating Response Strategies in a Web-Based Spoken Dialogue Agent",
        "text": "While the notion of a cooperative response has been the focus of considerable research in natural language dialogue systems, there has been little empirical work demonstrating how such responses lead to more efficient, natural, or successful dialogues. This paper presents an experimental evaluation of two alternative response strategies in TOOT, a spoken dialogue agent that allows users to access train schedules stored on the web via a telephone conversation. We compare the performance of two versions of TOOT (literal and cooperative), by having users carry out a set of tasks with each version. By using hypothesis testing methods, we show that a combination of response strategy, application task, and task/strategy interactions account for various types of performance differences. By using the PARADISE evaluation framework to estimate an overall performance function, we identify interdependencies that exist between speech recognition and response strategy. Our results elaborate the conditions under which TOOT' s cooperative rather than literal strategy contributes to greater performance.",
        "id": 1136893
      },
      {
        "title": "Point2SSM: Learning Morphological Variations of Anatomies from Point Clouds",
        "text": "We introduce Point2SSM, a novel unsupervised learning approach that can accurately construct correspondence-based statistical shape models (SSMs) of anatomy directly from point clouds. SSMs are crucial in clinical research for analyzing the population-level morphological variation in bones and organs. However, traditional methods for creating SSMs have limitations that hinder their widespread adoption, such as the need for noise-free surface meshes or binary volumes, reliance on assumptions or predefined templates, and simultaneous optimization of the entire cohort leading to lengthy inference times given new data. Point2SSM overcomes these barriers by providing a data-driven solution that infers SSMs directly from raw point clouds, reducing inference burdens and increasing applicability as point clouds are more easily acquired. Deep learning on 3D point clouds has seen recent success in unsupervised representation learning, point-to-point matching, and shape correspondence; however, their application to constructing SSMs of anatomies is largely unexplored. In this work, we benchmark state-of-the-art point cloud deep networks on the task of SSM and demonstrate that they are not robust to the challenges of anatomical SSM, such as noisy, sparse, or incomplete input and significantly limited training data. Point2SSM addresses these challenges via an attention-based module that provides correspondence mappings from learned point features. We demonstrate that the proposed method significantly outperforms existing networks in terms of both accurate surface sampling and correspondence, better capturing population-level statistics.Preprint. Under review.",
        "id": 258865994
      },
      {
        "title": "ALEXSIS-PT: A New Resource for Portuguese Lexical Simplification",
        "text": "Lexical simplification (LS) is the task of automatically replacing complex words for easier ones making texts more accessible to various target populations (e.g. individuals with low literacy, individuals with learning disabilities, second language learners). To train and test models, LS systems usually require corpora that feature complex words in context along with their candidate substitutions. To continue improving the performance of LS systems we introduce ALEXSIS-PT, a novel multi-candidate dataset for Brazilian Portuguese LS containing 9,605 candidate substitutions for 387 complex words. ALEXSIS-PT has been compiled following the ALEXSIS protocol for Spanish opening exciting new avenues for crosslingual models. ALEXSIS-PT is the first LS multi-candidate dataset that contains Brazilian newspaper articles. We evaluated four models for substitute generation on this dataset, namely mDistilBERT, mBERT, XLM-R, and BERTimbau. BERTimbau achieved the highest performance across all evaluation metrics.",
        "id": 252367466
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Which works shows that training large language models with purely mathematical and structural data can exhibit emergence of causal reasoning faster?",
    "positive_ctxs": [
      {
        "title": "Learning Multi-Step Reasoning by Solving Arithmetic Tasks",
        "text": "Mathematical reasoning is regarded as a necessary ability for Language Models (LMs). Recent works demonstrate large LMs' impressive performance in solving math problems. The success is attributed to their Chain-of-Thought (CoT) reasoning abilities, i.e., the ability to decompose complex questions into step-by-step reasoning chains, but such ability seems only to emerge from models with abundant parameters. This work investigates how to incorporate relatively small LMs with the capabilities of multi-step reasoning. We propose to inject such abilities by continually pre-training LMs on a synthetic dataset MSAT which is composed of Multi-step Arithmetic Tasks. Our experiments on four math word problem datasets show the effectiveness of the proposed method in enhancing LMs' math reasoning abilities. 1  , et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. . 2021. Mwptoolkit: An open-source framework for deep learning-based math word problem solvers. arXiv preprint arXiv:2109.00799.",
        "id": 259089125
      }
    ],
    "negative_ctxs": [
      {
        "title": "ACTUNE: Uncertainty-Based Active Self-Training for Active Fine-Tuning of Pretrained Language Models",
        "text": "While pre-trained language model (PLM) finetuning has achieved strong performance in many NLP tasks, the fine-tuning stage can be still demanding in labeled data. Recent works have resorted to active fine-tuning to improve the label efficiency of PLM fine-tuning, but none of them investigate the potential of unlabeled data. We propose ACTUNE, a new framework that leverages unlabeled data to improve the label efficiency of active PLM fine-tuning. ACTUNE switches between data annotation and model self-training based on uncertainty: it selects high-uncertainty unlabeled samples for active annotation and lowuncertainty ones for model self-training. Under this framework, we design (1) a regionaware sampling strategy that reduces redundancy when actively querying for annotations and (2) a momentum-based memory bank that dynamically aggregates the model's pseudo labels to suppress label noise in self-training. Experiments on 6 text classification datasets show that ACTUNE outperforms the strongest active learning and self-training baselines and improves the label efficiency of PLM finetuning by 56.2% on average. Our implementation is available at https://github. com/yueyu1030/actune.",
        "id": 250390500
      },
      {
        "title": "Subject and Object Dependency Extraction Using Finite-State Transducers",
        "text": "We describe and evaluate an approach for fast automatic recognition and extraction of subject and object dependency relations from large French corpora, using a sequence of finite-state transducers. The extraction is performed in two major steps: incremental finite-state parsing and extraction of subject/verb and object/verb relations. Our incremental and cautious approach during the first phase allows the system to deal successfully with complex phenomena such as embeddings, coordination of VPs and NPs or non-standard word order. The extraction requires no subcategorisation information. It relies on POS information only. After describing the two steps, we give the results of an evaluation on various types of unrestricted corpora. Precision is around 90-97% for subjects (84-88% for objects) and recall around 86-92% for subjects (80-90% for objects). We also provide some error analysis; in particular, we evaluate the impact of POS tagging errors on subject/object dependency extraction.",
        "id": 2711126
      },
      {
        "title": "",
        "text": "",
        "id": 44252659
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Have any research papers been published on models for representing sentences in under-resourced languages like Slovenian or Romanian?",
    "positive_ctxs": [
      {
        "title": "Language-agnostic BERT Sentence Embedding",
        "text": "While BERT is an effective method for learning monolingual sentence embeddings for semantic similarity and embedding based transfer learning (Reimers and Gurevych, 2019), BERT based cross-lingual sentence embeddings have yet to be explored. We systematically investigate methods for learning multilingual sentence embeddings by combining the best methods for learning monolingual and cross-lingual representations including: masked language modeling (MLM), translation language modeling (TLM) (Conneau and Lample, 2019), dual encoder translation ranking (Guo et al., 2018), and additive margin softmax(Yang et al., 2019a). We show that introducing a pre-trained multilingual language model dramatically reduces the amount of parallel training data required to achieve good performance by 80%. Composing the best of these methods produces a model that achieves 83.7% bi-text retrieval accuracy over 112 languages on Tatoeba, well above the 65.5% achieved by Artetxe and Schwenk (2019b), while still performing competitively on monolingual transfer learning benchmarks (Conneau and Kiela, 2018). Parallel data mined from CommonCrawl using our best model is shown to train competitive NMT models for en-zh and en-de. We publicly release our best multilingual sentence embedding model for 109+ languages at https://tfhub.dev/ google/LaBSE. * Equal contributions. † Work done while at Google.",
        "id": 220347683
      }
    ],
    "negative_ctxs": [
      {
        "title": "Learning Disentangled Textual Representations via Statistical Measures of Similarity",
        "text": "When working with textual data, a natural application of disentangled representations is fair classification where the goal is to make predictions without being biased (or influenced) by sensitive attributes that may be present in the data (e.g., age, gender or race). Dominant approaches to disentangle a sensitive attribute from textual representations rely on learning simultaneously a penalization term that involves either an adversarial loss (e.g., a discriminator) or an information measure (e.g., mutual information). However, these methods require the training of a deep neural network with several parameter updates for each update of the representation model. As a matter of fact, the resulting nested optimization loop is both time consuming, adding complexity to the optimization dynamic, and requires a fine hyperparameter selection (e.g., learning rates, architecture). In this work, we introduce a family of regularizers for learning disentangled representations that do not require training. These regularizers are based on statistical measures of similarity between the conditional probability distributions with respect to the sensitive attributes. Our novel regularizers do not require additional training, are faster and do not involve additional tuning while achieving better results both when combined with pretrained and randomly initialized text encoders.",
        "id": 248572355
      },
      {
        "title": "",
        "text": "",
        "id": 233365334
      },
      {
        "title": "A Preliminary Study of Statistically Predictive Syntactic Complexity Features and Manual Simplifications in Basque",
        "text": "In this paper, we present a comparative analysis of statistically predictive syntactic features of complexity and the treatment of these features by humans when simplifying texts. To that end, we have used a list of the most five statistically predictive features obtained automatically and the Corpus of Basque Simplified Texts (CBST) to analyse how the syntactic phenomena in these features have been manually simplified. Our aim is to go beyond the descriptions of operations found in the corpus and relate the multidisciplinary findings to understand text complexity from different points of view. We also present some issues that can be important when analysing linguistic complexity.This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/",
        "id": 16856890
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there any paper that utilizes Gaussian processes to analyze the vulnerability of text-conditioned generative models?",
    "positive_ctxs": [
      {
        "title": "Query-Efficient Black-Box Red Teaming via Bayesian Optimization",
        "text": "The deployment of large-scale generative models is often restricted by their potential risk of causing harm to users in unpredictable ways. We focus on the problem of black-box red teaming, where a red team generates test cases and interacts with the victim model to discover a diverse set of failures with limited query access. Existing red teaming methods construct test cases based on human supervision or language model (LM) and query all test cases in a brute-force manner without incorporating any information from past evaluations, resulting in a prohibitively large number of queries. To this end, we propose Bayesian red teaming (BRT), novel query-efficient blackbox red teaming methods based on Bayesian optimization, which iteratively identify diverse positive test cases leading to model failures by utilizing the pre-defined user input pool and the past evaluations. Experimental results on various user input pools demonstrate that our method consistently finds a significantly larger number of diverse positive test cases under the limited query budget than the baseline methods. The source code is available at https://github.com/snu-mllab/Bayesian-Red-Teaming.",
        "id": 258960443
      }
    ],
    "negative_ctxs": [
      {
        "title": "Domain Adaptation in Multilingual and Multi-Domain Monolingual Settings for Complex Word Identification",
        "text": "Complex word identification (CWI) is a cornerstone process towards proper text simplification. CWI is highly dependent on context, whereas its difficulty is augmented by the scarcity of available datasets which vary greatly in terms of domains and languages. As such, it becomes increasingly more difficult to develop a robust model that generalizes across a wide array of input examples. In this paper, we propose a novel training technique for the CWI task based on domain adaptation to improve the target character and context representations. This technique addresses the problem of working with multiple domains, inasmuch as it creates a way of smoothing the differences between the explored datasets. Moreover, we also propose a similar auxiliary task, namely text simplification, that can be used to complement lexical complexity prediction. Our model obtains a boost of up to 2.42% in terms of Pearson Correlation Coefficients in contrast to vanilla training techniques, when considering the CompLex from the Lexical Complexity Prediction 2021 dataset. At the same time, we obtain an increase of 3% in Pearson scores, while considering a cross-lingual setup relying on the Complex Word Identification 2018 dataset. In addition, our model yields state-ofthe-art results in terms of Mean Absolute Error.",
        "id": 248779868
      },
      {
        "title": "Temporal Discourse Models for Narrative Structure",
        "text": "Getting a machine to understand human narratives has been a classic challenge for NLP and AI. This paper proposes a new representation for the temporal structure of narratives. The representation is parsimonious, using temporal relations as surrogates for discourse relations. The narrative models, called Temporal Discourse Models, are treestructured, where nodes include abstract events interpreted as pairs of time points and where the dominance relation is expressed by temporal inclusion. Annotation examples and challenges are discussed, along with a report on progress to date in creating annotated corpora.",
        "id": 17755594
      },
      {
        "title": "TransPerfect's Private Neural Machine Translation Portal",
        "text": "We will present our solution to replace the usage of publicly available machine translation (MT) services in companies where privacy and confidentiality are key. Our MT portal can translate across a variety of languages using neural machine translation, and supports an extensive number of file types. Corporations are using it to enable multilingual communication everywhere.",
        "id": 258890929
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "How to achieve zero-shot lip reading?",
    "positive_ctxs": [
      {
        "title": "OpenSR: Open-Modality Speech Recognition via Maintaining Multi-Modality Alignment",
        "text": "Speech Recognition builds a bridge between the multimedia streaming (audio-only, visualonly or audio-visual) and the corresponding text transcription. However, when training the specific model of new domain, it often gets stuck in the lack of new-domain utterances, especially the labeled visual utterances. To break through this restriction, we attempt to achieve zero-shot modality transfer by maintaining the multi-modality alignment in phoneme space learned with unlabeled multimedia utterances in the high resource domain during the pretraining(Shi et al., 2022), and propose a training system Open-modality Speech Recognition (OpenSR) that enables the models trained on a single modality (e.g., audio-only) applicable to more modalities (e.g., visual-only and audio-visual). Furthermore, we employ a cluster-based prompt tuning strategy to handle the domain shift for the scenarios with only common words in the new domain utterances. We demonstrate that OpenSR enables modality transfer from one to any in three different settings (zero-, few-and fullshot), and achieves highly competitive zeroshot performance compared to the existing fewshot and full-shot lip-reading methods. To the best of our knowledge, OpenSR achieves the state-of-the-art performance of word error rate in LRS2 on audio-visual speech recognition and lip-reading with 2.7% and 25.0%, respectively. The code and demo are available at https://github.com/Exgc/OpenSR.",
        "id": 259137817
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 220061061
      },
      {
        "title": "Towards Ontology Engineering Based on Linguistic Analysis Introduction and Related Work",
        "text": "In this paper we describe OntoLT, a plug-in for the widely used Protégé ontology development tool that supports the interactive extraction and/or extension of ontologies from text. The OntoLT approach aims at providing an environment for the integration of linguistic analysis in ontology development. OntoLT enables the definition of mapping rules with which concepts and attributes can be extracted automatically from linguistically annotated text collections. Mapping rules are defined by use of a constraint language. Constraints are implemented as XPATH expressions over the XML-based linguistic annotation. If all constraints are satisfied, the mapping rule activates one or more operators that describe in which way the ontology should be extended if a candidate is found.",
        "id": 17129783
      },
      {
        "title": "A Data-centric Framework for Improving Domain-specific Machine Reading Comprehension Datasets",
        "text": "Low-quality data can cause downstream problems in high-stakes applications. Data-centric approach emphasizes on improving dataset quality to enhance model performance. Highquality datasets are needed for general-purpose Large Language Models (LLMs) training, as well as for domain-specific models, which are usually small in size as it is costly to engage a large number of domain experts for their creation. Thus, it is vital to ensure high-quality domain-specific training data. In this paper, we propose a framework for enhancing the data quality of original datasets 1 . We applied the proposed framework to four biomedical datasets and showed relative improvement of up to 33%/40% for fine-tuning of retrieval/reader models on the BioASQ dataset when using back translation to enhance the original dataset quality.",
        "id": 257912882
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research that investigates generative modeling approaches for event extraction, specifically focusing on leveraging large pre-trained language models to reduce the complexity of template engineering?",
    "positive_ctxs": [
      {
        "title": "TEXT2EVENT: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction",
        "text": "Event extraction is challenging due to the complex structure of event records and the semantic gap between text and event. Traditional methods usually extract event records by decomposing the complex structure prediction task into multiple subtasks. In this paper, we propose TEXT2EVENT, a sequence-to-structure generation paradigm that can directly extract events from the text in an end-to-end manner. Specifically, we design a sequence-to-structure network for unified event extraction, a constrained decoding algorithm for event knowledge injection during inference, and a curriculum learning algorithm for efficient model learning. Experimental results show that, by uniformly modeling all tasks in a single model and universally predicting different labels, our method can achieve competitive performance using only record-level annotations in both supervised learning and transfer learning settings.",
        "id": 235458429
      }
    ],
    "negative_ctxs": [
      {
        "title": "Question Answering through Transfer Learning from Large Fine-grained Supervision Data",
        "text": "We show that the task of question answering (QA) can significantly benefit from the transfer learning of models trained on a different large, fine-grained QA dataset. We achieve the state of the art in two well-studied QA datasets, WikiQA and SemEval-2016 (Task 3A), through a basic transfer learning technique from SQuAD. For WikiQA, our model outperforms the previous best model by more than 8%. We demonstrate that finer supervision provides better guidance for learning lexical and syntactic information than coarser supervision, through quantitative results and visual analysis. We also show that a similar transfer learning procedure achieves the state of the art on an entailment task. . 2016. Kelp at semeval-2016 task 3: Learning semantic relations between questions and answers. SemEval 16:1116-1123.",
        "id": 7928230
      },
      {
        "title": "IN SEARCH OF THE REAL INDUCTIVE BIAS: ON THE ROLE OF IMPLICIT REGULARIZATION IN DEEP LEARNING",
        "text": "We present experiments demonstrating that some other form of capacity control, different from network size, plays a central role in learning multi-layer feedforward networks. We argue, partially through analogy to matrix factorization, that this is an inductive bias that can help shed light on deep learning.",
        "id": 6021932
      },
      {
        "title": "Classical Chinese Sentence Segmentation",
        "text": "Sentence segmentation is a fundamental issue in Classical Chinese language processing. To facilitate reading and processing of the raw Classical Chinese data, we propose a statistical method to split unstructured Classical Chinese text into smaller pieces such as sentences and clauses. The segmenter based on the conditional random field (CRF) model is tested under different tagging schemes and various features including n-gram, jump, word class, and phonetic information. We evaluated our method on four datasets from several eras (i.e., from the 5th century BCE to the 19th century). Our CRF segmenter achieves an F-score of 83.34% and can be applied on a variety of data from different eras.",
        "id": 3874199
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper studies how current retrieval systems handle queries which contain multiple constraints?",
    "positive_ctxs": [
      {
        "title": "QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations",
        "text": "Formulating selective information needs results in queries that implicitly specify set operations, such as intersection, union, and difference. For instance, one might search for \"shorebirds that are not sandpipers\" or \"science-fiction films shot in England\". To study the ability of retrieval systems to meet such information needs, we construct QUEST, a dataset of 3357 natural language queries with implicit set operations, that map to a set of entities corresponding to Wikipedia documents. The dataset challenges models to match multiple constraints mentioned in queries with corresponding evidence in documents and correctly perform various set operations. The dataset is constructed semi-automatically using Wikipedia category names. Queries are automatically composed from individual categories, then paraphrased and further validated for naturalness and fluency by crowdworkers. Crowdworkers also assess the relevance of entities based on their documents and highlight attribution of query constraints to spans of document text. We analyze several modern retrieval systems, finding that they often struggle on such queries. Queries involving negation and conjunction are particularly challenging and systems are further challenged with combinations of these operations. 1",
        "id": 258822815
      }
    ],
    "negative_ctxs": [
      {
        "title": "USP-EACH Frequency-based Greedy Attribute Selection for Referring Expressions Generation",
        "text": "Both greedy and domain-oriented REG algorithms have significant strengths but tend to perform poorly according to humanlikeness criteria as measured by, e.g., Dice scores. In this work we describe an attempt to combine both perspectives into a single attribute selection strategy to be used as part of the Dale & Reiter Incremental algorithm in the REG Challenge 2008, and the results in both Furniture and People domains.",
        "id": 8526022
      },
      {
        "title": "MULTI-CROSSRE A Multi-Lingual Multi-Domain Dataset for Relation Extraction",
        "text": "Most research in Relation Extraction (RE) involves the English language, mainly due to the lack of multi-lingual resources. We propose MULTI-CROSSRE, the broadest multi-lingual dataset for RE, including 26 languages in addition to English, and covering six text domains. MULTI-CROSSRE is a machine translated version of CrossRE (Bassignana and Plank, 2022a), with a sub-portion including more than 200 sentences in seven diverse languages checked by native speakers. We run a baseline model over the 26 new datasets and-as sanity check-over the 26 back-translations to English. Results on the back-translated data are consistent with the ones on the original English CrossRE, indicating high quality of the translation and the resulting dataset.",
        "id": 215509168
      },
      {
        "title": "應用詞向量模型於日文單詞可讀性評估之研究 Japanese Word Readability Assessment using Word Embeddings",
        "text": "In text analysis, text readability has been an important research topic for many years. However, most studies focus on the document readability rather than the word readability. To decide the readability levels of words, linguists need to spend a large amount of human effort and assessment time.The most challenging problem faced in the task of automatic word readability assessment is the lack of research on readability for each word. In this study, we propose a novel assessment model for word readability called WR-kNN based on the word embedding technology by calculating the word vectors",
        "id": 233029479
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "What research articles should I consult to understand a method for quantitatively assessing how successful neuron interventions are at altering a model's predictions?",
    "positive_ctxs": [
      {
        "title": "Sparse Interventions in Language Models with Differentiable Masking",
        "text": "There has been a lot of interest in understanding what information is captured by hidden representations of language models (LMs). Typically, interpretation methods i) do not guarantee that the model actually uses the encoded information, and ii) do not discover small subsets of neurons responsible for a considered phenomenon. Inspired by causal mediation analysis, we propose a method that discovers within a neural LM a small subset of neurons responsible for a particular linguistic phenomenon, i.e., subsets causing a change in the corresponding token emission probabilities. We use a differentiable relaxation to approximately search through the combinatorial space. An L 0 regularization term ensures that the search converges to discrete and sparse solutions. We apply our method to analyze subject-verb number agreement and gender bias detection in LSTMs. We observe that it is fast and finds better solutions than the alternative (REINFORCE). Our experiments confirm that each of these phenomenons is mediated through a small subset of neurons that do not play any other discernible role.",
        "id": 245123965
      }
    ],
    "negative_ctxs": [
      {
        "title": "COMPUTATIONAL ANALYSIS OF PREDICATIONAL STRUCTURES IN ENGLISH",
        "text": "The results of a computational analysis of all predications, finite and non-finite, in a one-million-word corpus of present-day American English (the \"Brown Corpus\") are presented. The analysis shows the nature of the syntactic differences among the various genres of writing represented in the data base, especially between informative prose and imaginative prose.The results also demonstrate that syntactic complexity, if defined as the number of predications per sentence, is not directly predictable from sentence length.The purpose of this paper is to present an outline of the procedures and the summary of the results of a computational analysis of the structure of predications in a large and representative sample of English texts.",
        "id": 27120
      },
      {
        "title": "Large-scale Semantic Networks: Annotation and Evaluation",
        "text": "We introduce a large-scale semantic-network annotation effort based on the MutliNet formalism. Annotation is achieved via a process which incorporates several independent tools including a MultiNet graph editing tool, a semantic concept lexicon, a user-editable knowledge-base for semantic concepts, and a MultiNet parser. We present an evaluation metric for these semantic networks, allowing us to determine the quality of annotations in terms of inter-annotator agreement. We use this metric to report the agreement rates for a pilot annotation effort involving three annotators.",
        "id": 9840979
      },
      {
        "title": "Acquiring Hyponymy Relations from Web Documents",
        "text": "This paper describes an automatic method for acquiring hyponymy relations from HTML documents on the WWW. Hyponymy relations can play a crucial role in various natural language processing systems. Most existing acquisition methods for hyponymy relations rely on particular linguistic patterns, such as \"NP such as NP\". Our method, however, does not use such linguistic patterns, and we expect that our procedure can be applied to a wide range of expressions for which existing methods cannot be used. Our acquisition algorithm uses clues such as itemization or listing in HTML documents and statistical measures such as document frequencies and verb-noun co-occurrences.",
        "id": 5510520
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "ould you direct me to research that shows that the transfer of specialized knowledge between various ABSA tasks if trained under the same paradigm?",
    "positive_ctxs": [
      {
        "title": "Aspect Sentiment Quad Prediction as Paraphrase Generation *",
        "text": "Aspect-based sentiment analysis (ABSA) has been extensively studied in recent years, which typically involves four fundamental sentiment elements, including the aspect category, aspect term, opinion term, and sentiment polarity. Existing studies usually consider the detection of partial sentiment elements, instead of predicting the four elements in one shot. In this work, we introduce the Aspect Sentiment Quad Prediction (ASQP) task, aiming to jointly detect all sentiment elements in quads for a given opinionated sentence, which can reveal a more comprehensive and complete aspect-level sentiment structure. We further propose a novel PARAPHRASE modeling paradigm to cast the ASQP task to a paraphrase generation process. On one hand, the generation formulation allows solving ASQP in an end-to-end manner, alleviating the potential error propagation in the pipeline solution. On the other hand, the semantics of the sentiment elements can be fully exploited by learning to generate them in the natural language form. Extensive experiments on benchmark datasets show the superiority of our proposed method and the capacity of crosstask transfer with the proposed unified PARA-PHRASE modeling framework.",
        "id": 238259938
      }
    ],
    "negative_ctxs": [
      {
        "title": "Lexicon Enhanced Chinese Sequence Labelling Using BERT Adapter",
        "text": "Lexicon information and pre-trained models, such as BERT, have been combined to explore Chinese sequence labelling tasks due to their respective strength. However, existing methods solely fuse lexicon features via a shallow and random initialized sequence layer and do not integrate them into the bottom layers of BERT. In this paper, we propose Lexicon Enhanced BERT (LEBERT) for Chinese sequence labelling, which integrates external lexicon knowledge into BERT layers directly by a Lexicon Adapter layer. Compared with the existing methods, our model facilitates deep lexicon knowledge fusion at the lower layers of BERT. Experiments on ten Chinese datasets of three tasks including Named Entity Recognition, Word Segmentation, and Part-of-Speech Tagging, show that LEBERT achieves the stateof-the-art results.",
        "id": 234741719
      },
      {
        "title": "",
        "text": "",
        "id": 261341822
      },
      {
        "title": "22 ème Traitement Automatique des Langues Naturelles",
        "text": "Nous présentons des travaux préliminaires sur une approche permettant d'ajouter des termes bilingues à un système de Traduction Automatique Statistique (TAS) à base de segments. Les termes sont non seulement inclus individuellement, mais aussi avec des contextes les englobant. Tout d'abord nous générons ces contextes en généralisant des motifs (ou patrons) observés pour des mots de même nature syntaxique dans un corpus bilingue. Enfin, nous filtrons les contextes qui n'atteignent pas un certain seuil de confiance, à l'aide d'une méthode de sélection de bi-segments inspirée d'une approche de sélection de données, précédemment appliquée à des textes bilingues alignés.Abstract.Statistical machine translation adaptation through terminological enrichment based on virtual phrase generation and filteringWe propose a technique for adding bilingual terms to a phrase-based SMT system which includes not only individual words, but also induces phrasal contexts around these words. We first generate these contexts by generalizing patterns observed for similar words in a bilingual corpus, but then filter out those contexts that fall below a certain confidence threshold, based on an original phrase-pair selection process inspired by existing sentence selection techniques.Mots-clés : Traduction Automatique Statistique, Génération Automatique de Texte, contexte phrastique, terminologie bilingue.",
        "id": 171887294
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper is among the earliest to train on extensive collection of signing video and subtitle pairs available from online platforms?",
    "positive_ctxs": [
      {
        "title": "Gloss-Free End-to-End Sign Language Translation",
        "text": "In this paper, we tackle the problem of sign language translation (SLT) without gloss annotations. Although intermediate representation like gloss has been proven effective, gloss annotations are hard to acquire, especially in large quantities. This limits the domain coverage of translation datasets, thus handicapping real-world applications. To mitigate this problem, we design the Gloss-Free End-to-end sign language translation framework (GloFE). Our method improves the performance of SLT in the gloss-free setting by exploiting the shared underlying semantics of signs and the corresponding spoken translation. Common concepts are extracted from the text and used as a weak form of intermediate representation. The global embedding of these concepts is used as a query for cross-attention to find the corresponding information within the learned visual features. In a contrastive manner, we encourage the similarity of query results between samples containing such concepts and decrease those that do not. We obtained state-of-the-art results on large-scale datasets, including OpenASL and How2Sign. 1",
        "id": 258832883
      }
    ],
    "negative_ctxs": [
      {
        "title": "Simultaneous Identification of Biomedical Named-Entity and Functional Relations Using Statistical Parsing Techniques *",
        "text": "In this paper we propose a statistical parsing technique that simultaneously identifies biomedical named-entities (NEs) and extracts subcellular localization relations for bacterial proteins from the text in MEDLINE articles. We build a parser that derives both syntactic and domain-dependent semantic information and achieves an F-score of 48.4% for the relation extraction task. We then propose a semi-supervised approach that incorporates noisy automatically labeled data to improve the F-score of our parser to 83.2%. Our key contributions are: learning from noisy data, and building an annotated corpus that can benefit relation extraction research.",
        "id": 9128091
      },
      {
        "title": "A Semi-supervised Multi-task Learning Approach to Classify Customer Contact Intents",
        "text": "In the area of customer support, understanding customers' intents is a crucial step. Machine learning plays a vital role in this type of intent classification. In reality, it is typical to collect confirmation from customer support representatives (CSRs) regarding the intent prediction, though it can unnecessarily incur prohibitive cost to ask CSRs to assign existing or new intents to the mis-classified cases. Apart from the confirmed cases with and without intent labels, there can be a number of cases with no human curation. This data composition (Positives + Unlabeled + multiclass Negatives) creates unique challenges for model development. In response to that, we propose a semi-supervised multi-task learning paradigm. In this manuscript, we share our experience in building text-based intent classification models for a customer support service on an E-commerce website. We improve the performance significantly by evolving the model from multiclass classification to semi-supervised multi-task learning by leveraging the negative cases, domain-and taskadaptively pretrained ALBERT on customer contact texts, and a number of un-curated data with no labels. In the evaluation, the final model boosts the average AUC ROC by almost 20 points compared to the baseline finetuned multiclass classification ALBERT model.",
        "id": 235421748
      },
      {
        "title": "LARGE SCALE DISTRIBUTED NEURAL NETWORK TRAINING THROUGH ONLINE DISTILLATION",
        "text": "Techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model. However, due to increased testtime cost (for ensembles) and increased complexity of the training pipeline (for distillation), these techniques are challenging to use in industrial settings. In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup or many new hyperparameters. Our first claim is that online distillation enables us to use extra parallelism to fit very large datasets about twice as fast. Crucially, we can still speed up training even after we have already reached the point at which additional parallelism provides no benefit for synchronous or asynchronous stochastic gradient descent. Two neural networks trained on disjoint subsets of the data can share knowledge by encouraging each model to agree with the predictions the other model would have made. These predictions can come from a stale version of the other model so they can be safely computed using weights that only rarely get transmitted. Our second claim is that online distillation is a cost-effective way to make the exact predictions of a model dramatically more reproducible. We support our claims using experiments on the Criteo Display Ad Challenge dataset, ImageNet, and the largest to-date dataset used for neural language modeling, containing 6 × 10 11 tokens and based on the Common Crawl repository of web data.",
        "id": 2331610
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there a paper that supports the use of automated coherence metrics in topic model evaluations?",
    "positive_ctxs": [
      {
        "title": "Large-Scale Correlation Analysis of Automated Metrics for Topic Models",
        "text": "Automated coherence metrics constitute an important and popular way to evaluate topic models. Previous works present a mixed picture of their presumed correlation with human judgement. In this paper, we conduct a large-scale correlation analysis of coherence metrics. We propose a novel sampling approach to mine topics for the purpose of metric evaluation, and conduct the analysis via three large corpora showing that certain automated coherence metrics are correlated. Moreover, we extend the analysis to measure topical differences between corpora. Lastly, we examine the reliability of human judgement by conducting an extensive user study, which is designed as an amalgamation of different proxy tasks to derive a finer insight into the human decision-making processes. Our findings reveal some correlation between automated coherence metrics and human judgement, especially for generic corpora.",
        "id": 259370596
      }
    ],
    "negative_ctxs": [
      {
        "title": "Benchmarking Aggression Identification in Social Media",
        "text": "In this paper, we present the report and findings of the Shared Task on Aggression Identification organised as part of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC -1) at COLING 2018. The task was to develop a classifier that could discriminate between Overtly Aggressive, Covertly Aggressive, and Non-aggressive texts. For this task, the participants were provided with a dataset of 15,000 aggression-annotated Facebook Posts and Comments each in Hindi (in both Roman and Devanagari script) and English for training and validation. For testing, two different sets -one from Facebook and another from a different social media -were provided. A total of 130 teams registered to participate in the task, 30 teams submitted their test runs, and finally 20 teams also sent their system description paper which are included in the TRAC workshop proceedings. The best system obtained a weighted F-score of 0.64 for both Hindi and English on the Facebook test sets, while the best scores on the surprise set were 0.60 and 0.50 for English and Hindi respectively. The results presented in this report depict how challenging the task is. The positive response from the community and the great levels of participation in the first edition of this shared task also highlights the interest in this topic. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/",
        "id": 59336626
      },
      {
        "title": "On the Relevance and Learner Dependence of Co-text Complexity for Exercise Difficulty",
        "text": "Adaptive exercise sequencing in Intelligent Language Tutoring Systems (ILTS) aims to select exercises for individual learners that match their abilities. For exercises practicing forms in isolation, it may be sufficient for sequencing to consider the form being practiced. But when exercises embed the forms in a sentence or bigger language context, little is known about how the nature of this co-text influences learners in completing the exercises.To fill the gap, based on data from two large field studies conducted with an English ILTS in German secondary schools, we analyze the impact of co-text complexity on learner performance for different exercise types and learners at different proficiency levels. The results show that co-text complexity is an important predictor for a learner's performance on practice exercises, especially for gap filling and Jumbled Sentences exercises, and particularly for learners at higher proficiency levels.",
        "id": 258761891
      },
      {
        "title": "When classifying grammatical role, BERT doesn't care about word order. . . except when it matters",
        "text": "Because meaning can often be inferred from lexical semantics alone, word order is often a redundant cue in natural language. For example, the words chopped, chef, and onion are more likely used to convey \"The chef chopped the onion,\" not \"The onion chopped the chef.\" Recent work has shown large language models to be surprisingly word order invariant, but crucially has largely considered natural prototypical inputs, where compositional meaning mostly matches lexical expectations. To overcome this confound, we probe grammatical role representation in English BERT and GPT-2, on instances where lexical expectations are not sufficient, and word order knowledge is necessary for correct classification. Such nonprototypical instances are naturally occurring English sentences with inanimate subjects or animate objects, or sentences where we systematically swap the arguments to make sentences like \"The onion chopped the chef\". We find that, while early layer embeddings are largely lexical, word order is in fact crucial in defining the later-layer representations of words in semantically non-prototypical positions. Our experiments isolate the effect of word order on the contextualization process, and highlight how models use context in the uncommon, but critical, instances where it matters.",
        "id": 247447149
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there any paper that explores and annotates the effectiveness of using testimonials or anecdotes in discussions?",
    "positive_ctxs": [
      {
        "title": "StoryARG: a corpus of narratives and personal experiences in argumentative texts",
        "text": "Humans are storytellers, even in communication scenarios which are assumed to be more rationality-oriented, such as argumentation. Indeed, supporting arguments with narratives or personal experiences (henceforth, stories) is a very natural thing to do -and yet, this phenomenon is largely unexplored in computational argumentation. Which role do stories play in an argument? Do they make the argument more effective? What are their narrative properties? To address these questions, we collected and annotated StoryARG, a dataset sampled from well-established corpora in computational argumentation (ChangeMyView and RegulationRoom), and the Social Sciences (Europolis), as well as comments to New York Times articles. StoryARG contains 2451 textual spans annotated at two levels. At the argumentative level, we annotate the function of the story (e.g., clarification, disclosure of harm, search for a solution, establishing speaker's authority), as well as its impact on the effectiveness of the argument and its emotional load. At the level of narrative properties, we annotate whether the story has a plot-like development, is factual or hypothetical, and who the protagonist is.What makes a story effective in an argument? Our analysis of the annotations in StoryARG uncover a positive impact on effectiveness for stories which illustrate a solution to a problem, and in general, annotator-specific preferences that we investigate with regression analysis.",
        "id": 259370693
      }
    ],
    "negative_ctxs": [
      {
        "title": "C-PLANNING: AN AUTOMATIC CURRICULUM FOR LEARNING GOAL-REACHING TASKS",
        "text": "Goal-conditioned reinforcement learning (RL) can solve tasks in a wide range of domains, including navigation and manipulation, but learning to reach distant goals remains a central challenge to the field. Learning to reach such goals is particularly hard without any offline data, expert demonstrations, and reward shaping. In this paper, we propose an algorithm to solve the distant goal-reaching task by using search at training time to automatically generate a curriculum of intermediate states. Our algorithm, Classifier-Planning (C-Planning), frames the learning of the goal-conditioned policies as expectation maximization: the E-step corresponds to planning an optimal sequence of waypoints using graph search, while the M-step aims to learn a goal-conditioned policy to reach those waypoints. Unlike prior methods that combine goal-conditioned RL with graph search, ours performs search only during training and not testing, significantly decreasing the compute costs of deploying the learned policy. Empirically, we demonstrate that our method is more sample efficient that prior methods. Moreover, it is able to solve very long horizons manipulation and navigation tasks, tasks that prior goalconditioned methods and methods based on graph search fail to solve.",
        "id": 239769065
      },
      {
        "title": "Zoho at SemEval-2019 Task 9: Semi-supervised Domain Adaptation using Tri-training for Suggestion Mining",
        "text": "This paper describes our submission for the SemEval-2019 Suggestion Mining task. A simple Convolutional Neural Network (CNN) classifier with contextual word representations from a pre-trained language model is used for sentence classification. The model is trained using tri-training, a semi-supervised bootstrapping mechanism for labelling unseen data. Tri-training proved to be an effective technique to accommodate domain shift for cross-domain suggestion mining (Subtask B) where there is no hand labelled training data. For in-domain evaluation (Subtask A), we use the same technique to augment the training set. Our system ranks thirteenth in Subtask A with an F 1 -score of 68.07 and third in Subtask B with an F 1 -score of 81.94.",
        "id": 67856536
      },
      {
        "title": "A tool for enhanced search of multilingual digital libraries of e-journals",
        "text": "This paper outlines the main features of Bibliša, a tool that offers various possibilities of enhancing queries submitted to large collections of TMX documents generated from aligned parallel articles residing in multilingual digital libraries of e-journals. The queries initiated by a simple or multiword keyword, in Serbian or English, can be expanded by Bibliša, both semantically and morphologically, using different supporting monolingual and multilingual resources, such as wordnets and electronic dictionaries. The tool operates within a complex system composed of several modules including a web application, which makes it readily accessible on the web. Its functionality has been tested on a collection of 44 TMX documents generated from articles published bilingually by the journal INFOtecha, yielding encouraging results. Further enhancements of the tool are underway, with the aim of transforming it from a powerful full-text and metadata search tool, to a useful translator's aid, which could be of assistance both in reviewing terminology used in context and in refining the multilingual resources used within the system.",
        "id": 14485511
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper first constructed a structured knowledge base to interconnect different human social roles and attributes?",
    "positive_ctxs": [
      {
        "title": "PEACOK: Persona Commonsense Knowledge for Consistent and Engaging Narratives",
        "text": "Sustaining coherent and engaging narratives requires dialogue or storytelling agents to understand how the personas of speakers or listeners ground the narrative.",
        "id": 258480238
      }
    ],
    "negative_ctxs": [
      {
        "title": "Building a Bio-Event Annotated Corpus for the Acquisition of Semantic Frames from Biomedical Corpora",
        "text": "This paper reports on the design and construction of a bio-event annotated corpus which was developed with a specific view to the acquisition of semantic frames from biomedical corpora. We describe the adopted annotation scheme and the annotation process, which is supported by a dedicated annotation tool. The annotated corpus contains 677 abstracts of biomedical research articles.",
        "id": 6881086
      },
      {
        "title": "RWTH-PHOENIX-Weather: A Large Vocabulary Sign Language Recognition and Translation Corpus",
        "text": "This paper introduces the RWTH-PHOENIX-Weather corpus, a video-based, large vocabulary corpus of German Sign Language suitable for statistical sign language recognition and translation. In contrast to most available sign language data collections, the RWTH-PHOENIX-Weather corpus has not been recorded for linguistic research but for the use in statistical pattern recognition. The corpus contains weather forecasts recorded from German public TV which are manually annotated using glosses distinguishing sign variants, and time boundaries have been marked on the sentence and the gloss level. Further, the spoken German weather forecast has been transcribed in a semi-automatic fashion using a state-of-the-art automatic speech recognition system. Moreover, an additional translation of the glosses into spoken German has been created to capture allowable translation variability. In addition to the corpus, experimental baseline results for hand and head tracking, statistical sign language recognition and translation are presented.",
        "id": 2516961
      },
      {
        "title": "NO TRAINING REQUIRED: EXPLORING RANDOM EN- CODERS FOR SENTENCE CLASSIFICATION",
        "text": "We explore various methods for computing sentence representations from pretrained word embeddings without any training, i.e., using nothing but random parameterizations. Our aim is to put sentence embeddings on more solid footing by 1) looking at how much modern sentence embeddings gain over random methods-as it turns out, surprisingly little; and by 2) providing the field with more appropriate baselines going forward-which are, as it turns out, quite strong. We also make important observations about proper experimental protocol for sentence classification evaluation, together with recommendations for future research. * Work done as an intern at Facebook AI Research. 1 Code available at https://github.com/facebookresearch/randsent. We go down a well-paved avenue of exploration in the machine learning research community, and exploit an insight originally due to Cover (1965): \"A complex pattern-classification problem, cast in a high-dimensional space nonlinearly, is more likely to be linearly separable than in a lowdimensional space, provided that the space is not densely populated.\" That is, we examine three types of models for obtaining randomly computed sentence representations from pre-trained word embeddings: bag of random embedding projections, randomly initialized recurrent networks and echo state networks.Our goal is not to obtain a new state of the art, but to put current state of the art methods on more solid footing by 1) looking at how much they gain compared to random methods; and 2) providing the field with more solid baselines going forward. We make several important observations about proper experimental protocol for sentence classification evaluation; and finish with a list of takeaway recommendations.",
        "id": 59336240
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Is there a specialized question answering dataset that concentrates on intricate tables within certain sectors, like the airline industry?",
    "positive_ctxs": [
      {
        "title": "AIT-QA: Question Answering Dataset over Complex Tables in the Airline Industry",
        "text": "",
        "id": 235623770
      }
    ],
    "negative_ctxs": [
      {
        "title": "Vocabulary Decomposition for Estonian Open Vocabulary Speech Recognition",
        "text": "Speech recognition in many morphologically rich languages suffers from a very high out-of-vocabulary (OOV) ratio. Earlier work has shown that vocabulary decomposition methods can practically solve this problem for a subset of these languages. This paper compares various vocabulary decomposition approaches to open vocabulary speech recognition, using Estonian speech recognition as a benchmark. Comparisons are performed utilizing large models of 60000 lexical items and smaller vocabularies of 5000 items. A large vocabulary model based on a manually constructed morphological tagger is shown to give the lowest word error rate, while the unsupervised morphology discovery method Morfessor Baseline gives marginally weaker results. Only the Morfessor-based approach is shown to adequately scale to smaller vocabulary sizes.",
        "id": 524673
      },
      {
        "title": "BioinformaticsUA: Concept Recognition in Clinical Narratives Using a Modular and Highly Efficient Text Processing Framework",
        "text": "Clinical texts, such as discharge summaries or test reports, contain a valuable amount of information that, if efficiently and effectively mined, could be used to infer new knowledge, possibly leading to better diagnosis and therapeutics. With this in mind, the SemEval-2014 Analysis of Clinical Text task aimed at assessing and improving current methods for identification and normalization of concepts occurring in clinical narrative. This paper describes our approach in this task, which was based on a fully modular architecture for text mining. We followed a pure dictionary-based approach, after performing error analysis to refine our dictionaries.We obtained an F-measure of 69.4% in the entity recognition task, achieving the second best precision over all submitted runs (81.3%), with above average recall (60.5%). In the normalization task, we achieved a strict accuracy of 53.1% and a relaxed accuracy of 87.0%.",
        "id": 8675882
      },
      {
        "title": "QCS: A Tool for Querying, Clustering, and Summarizing Documents",
        "text": "The QCS information retrieval (IR) system is presented as a tool for querying, clustering, and summarizing document sets. QCS has been developed as a modular development framework, and thus facilitates the inclusion of new technologies targeting these three IR tasks. Details of the system architecture, the QCS interface, and preliminary results are presented.",
        "id": 5517454
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Are there any recent papers investigating the use of expert and anti-expert models together to guide text generation and mitigate toxic output?",
    "positive_ctxs": [
      {
        "title": "DEXPERTS: Decoding-Time Controlled Text Generation with Experts and Anti-Experts",
        "text": "Despite recent advances in natural language generation, it remains challenging to control attributes of generated text. We propose DEX-PERTS: Decoding-time Experts, a decodingtime method for controlled text generation that combines a pretrained language model with \"expert\" LMs and/or \"anti-expert\" LMs in a product of experts. Intuitively, under the ensemble, tokens only get high probability if they are considered likely by the experts and unlikely by the anti-experts. We apply DEXPERTS to language detoxification and sentiment-controlled generation, where we outperform existing controllable generation methods on both automatic and human evaluations. Moreover, because DEXPERTS operates only on the output of the pretrained LM, it is effective with (anti-)experts of smaller size, including when operating on GPT-3. Our work highlights the promise of tuning small LMs on text with (un)desirable attributes for efficient decoding-time steering.",
        "id": 235313967
      }
    ],
    "negative_ctxs": [
      {
        "title": "Assessing Monotonicity Reasoning in Dutch through Natural Language Inference",
        "text": "In this paper we investigate monotonicity reasoning in Dutch, through a novel Natural Language Inference dataset. Monotonicity reasoning shows to be highly challenging for Transformer-based language models in English and here, we corroborate those findings using a parallel Dutch dataset, obtained by translating the Monotonicity Entailment Dataset of Yanaka et al.(2019). After fine-tuning two Dutch language models BERTje and RobBERT on the Dutch NLI dataset SICK-NL, we find that performance severely drops on the monotonicity reasoning dataset, indicating poor generalization capacity of the models. We provide a detailed analysis of the test results by means of the linguistic annotations in the dataset. We find that models struggle with downward entailing contexts, and argue that this is due to a poor understanding of negation. Additionally, we find that the choice of monotonicity context affects model performance on conjunction and disjunction. We hope that this new resource paves the way for further research in generalization of neural reasoning models in Dutch, and contributes to the development of better language technology for Natural Language Inference, specifically for Dutch.",
        "id": 258378229
      },
      {
        "title": "Sentence and Clause Level Emotion Annotation, Detection, and Classification in a Multi-Genre Corpus",
        "text": "Predicting emotion categories (e.g. anger, joy, sadness) expressed by a sentence is challenging due to inherent multi-label smaller pieces such as phrases and clauses. To date, emotion has been studied in single genre, while models of human behaviors or situational awareness in the event of disasters require emotion modeling in multi-genres. In this paper, we expand and unify existing annotated data in different genres (emotional blog post, news title, and movie reviews) using an inventory of 8 emotions from Plutchik's Wheel of Emotions tags. We develop systems for automatically detecting and classifying emotions in text, in different textual genres and granularity levels, namely, sentence and clause levels in a supervised setting. We explore the effectiveness of clause annotation in sentence-level emotion detection and classification (EDC). To our knowledge, our EDC system is the first to target the clause level; further we provide emotion annotation for movie reviews dataset for the first time.",
        "id": 21707078
      },
      {
        "title": "",
        "text": "",
        "id": 218973847
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper first aggregates statements to represent political actors and learns the mapping from languages to representation via pre-training?",
    "positive_ctxs": [
      {
        "title": "UPPAM: A Unified Pre-training Architecture for Political Actor Modeling based on Language",
        "text": "Modeling political actors is at the core of quantitative political science. Existing works have incorporated contextual information to better learn the representation of political actors for specific tasks through graph models. However, they are limited to the structure and objective of training settings and can not be generalized to all politicians and other tasks. In this paper, we propose a Unified Pre-training Architecture for Political Actor Modeling based on language (UPPAM). In UPPAM, we aggregate statements to represent political actors and learn the mapping from languages to representation, instead of learning the representation of particular persons. We further design structureaware contrastive learning and behavior-driven contrastive learning tasks, to inject multidimensional information in the political context into the mapping. In this framework, we can profile political actors from different aspects and solve various downstream tasks. Experimental results demonstrate the effectiveness and capability of generalization of our method. * Corresponding author.",
        "id": 259370636
      }
    ],
    "negative_ctxs": [
      {
        "title": "SRA PROJECT FOR ARPA / USACOM",
        "text": "",
        "id": 30836513
      },
      {
        "title": "Cairo: An Alignment Visualization Tool",
        "text": "While developing a suite of tools for statistical machine translation research, we recognized the need for a visualization tool that would allow researchers to examine and evaluate specific word correspondences generated by a translation system. We developed Cairo to fill this need. Cairo is a free, open-source, portable, user-friendly, GUI-driven program written in Java that provides a visual representation of word correspondences between bilingual pairs of sentences, as well as relevant translation model parameters. This program can be easily adapted for visualization of correspondences in bi-texts based on probability distributions.",
        "id": 2091007
      },
      {
        "title": "A chance-corrected measure of inter-annotator agreement for syntax",
        "text": "Following the works ofCarletta (1996)andArtstein and Poesio (2008), there is an increasing consensus within the field that in order to properly gauge the reliability of an annotation effort, chance-corrected measures of inter-annotator agreement should be used. With this in mind, it is striking that virtually all evaluations of syntactic annotation efforts use uncorrected parser evaluation metrics such as bracket F 1 (for phrase structure) and accuracy scores (for dependencies).In this work we present a chance-corrected metric based on Krippendorff's α, adapted to the structure of syntactic annotations and applicable both to phrase structure and dependency annotation without any modifications. To evaluate our metric we first present a number of synthetic experiments to better control the sources of noise and gauge the metric's responses, before finally contrasting the behaviour of our chance-corrected metric with that of uncorrected parser evaluation metrics on real corpora. 1",
        "id": 17542707
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research that employs a relaxed l0 regularization for structured pruning to downsize language models with transformer architectures?",
    "positive_ctxs": [
      {
        "title": "Structured Pruning Learns Compact and Accurate Models",
        "text": "The growing size of neural language models has led to increased attention in model compression. The two predominant approaches are pruning, which gradually removes weights from a pre-trained model, and distillation, which trains a smaller compact model to match a larger one. Pruning methods can significantly reduce the model size but hardly achieve large speedups as distillation. However, distillation methods require large amounts of unlabeled data and are expensive to train. In this work, we propose a task-specific structured pruning method CoFi 1 (Coarse-and Fine-grained Pruning), which delivers highly parallelizable subnetworks and matches the distillation methods in both accuracy and latency, without resorting to any unlabeled data. Our key insight is to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads and hidden units) modules, which controls the pruning decision of each parameter with masks of different granularity. We also devise a layerwise distillation strategy to transfer knowledge from unpruned to pruned models during optimization. Our experiments on GLUE and SQuAD datasets show that CoFi yields models with over 10× speedups with a small accuracy drop, showing its effectiveness and efficiency compared to previous pruning and distillation approaches. 2",
        "id": 247922354
      }
    ],
    "negative_ctxs": [
      {
        "title": "Two/Too Simple Adaptations of Word2Vec for Syntax Problems",
        "text": "We present two simple modifications to the models in the popular Word2Vec tool, in order to generate embeddings more suited to tasks involving syntax. The main issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing using our proposed models.",
        "id": 14800090
      },
      {
        "title": "Identification of Fertile Translations in Medical Comparable Corpora : a Morpho-Compositional Approach",
        "text": "This paper defines a method for lexicon in the biomedical domain from comparable corpora. The method is based on compositional translation and exploits morpheme-level translation equivalences. It can generate translations for a large variety of morphologically constructed words and can also generate 'fertile' translations. We show that fertile translations increase the overall quality of the extracted lexicon for English to French translation.",
        "id": 13983070
      },
      {
        "title": "A Binarized Neural Network Joint Model for Machine Translation",
        "text": "The neural network joint model (NNJM), which augments the neural network language model (NNLM) with an m-word source context window, has achieved large gains in machine translation accuracy, but also has problems with high normalization cost when using large vocabularies. Training the NNJM with noise-contrastive estimation (NCE), instead of standard maximum likelihood estimation (MLE), can reduce computation cost. In this paper, we propose an alternative to NCE, the binarized NNJM (BNNJM), which learns a binary classifier that takes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks.",
        "id": 16588388
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there such a reading comprehension dataset in understanding a snippet from a long story book, while it requires to integrate the necessary long history texts before the snippet to full understand it?",
    "positive_ctxs": [
      {
        "title": "Personality Understanding of Fictional Characters during Book Reading",
        "text": "Comprehending characters' personalities is a crucial aspect of story reading. As readers engage with a story, their understanding of a character evolves based on new events and information; and multiple fine-grained aspects of personalities can be perceived. This leads to a natural problem of situated and fine-grained personality understanding. The problem has not been studied in the NLP field, primarily due to the lack of appropriate datasets mimicking the process of book reading. We present the first labeled dataset PERSONET for this problem. Our novel annotation strategy involves annotating user notes from online reading apps as a proxy for the original books. Experiments and human studies indicate that our dataset construction is both efficient and accurate; and our task heavily relies on long-term context to achieve accurate predictions for both machines and humans. 1",
        "id": 258740758
      }
    ],
    "negative_ctxs": [
      {
        "title": "SOME LINGUISTIC ASPECTS FOR AUTOMATIC TEXT UNDERSTANDING",
        "text": "This paper proposes a system of mapping classes of syntactic structures as instruments for automatic text under-",
        "id": 32969140
      },
      {
        "title": "L3Cube-MahaNER: A Marathi Named Entity Recognition Dataset and BERT models",
        "text": "Named Entity Recognition (NER) is a basic NLP task and finds major applications in conversational and search systems. It helps us identify key entities in a sentence used for the downstream application. NER or similar slot filling systems for popular languages have been heavily used in commercial applications. In this work, we focus on Marathi, an Indian language, spoken prominently by the people of Maharashtra state. Marathi is a low resource language and still lacks useful NER resources. We present L3Cube-MahaNER, the first major gold standard named entity recognition dataset in Marathi. We also describe the manual annotation guidelines followed during the process. In the end, we benchmark the dataset on different CNN, LSTM, and Transformer based models like mBERT, XLM-RoBERTa, IndicBERT, MahaBERT, etc. The MahaBERT provides the best performance among all the models. The data and models are available at https://github.com/l3cubepune/MarathiNLP",
        "id": 248157575
      },
      {
        "title": "CHOOSE YOUR WORDS CAREFULLY",
        "text": "",
        "id": 62222649
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "What open-source dataset combined knowledge retrieval with constraint satisfaction queries?",
    "positive_ctxs": [
      {
        "title": "KITAB: EVALUATING LLMS ON CONSTRAINT SATISFACTION FOR INFORMATION RETRIEVAL",
        "text": "We study the ability of state-of-the art models to answer constraint satisfaction queries for information retrieval (e.g., \"a list of ice cream shops in San Diego\").In the past, such queries were considered to be tasks that could only be solved via web-search or knowledge bases.More recently, large language models (LLMs) have demonstrated initial emergent abilities in this task.However, many current retrieval benchmarks are either saturated or do not measure constraint satisfaction.Motivated by rising concerns around factual incorrectness and hallucinations of LLMs, we present KITAB, a new dataset for measuring constraint satisfaction abilities of language models.KITAB consists of book-related data across more than 600 authors and 13,000 queries, and also offers an associated dynamic data collection and constraint verification approach for acquiring similar test data for other authors.Our extended experiments on GPT4 and GPT3.5 characterize and decouple common failure modes across dimensions such as information popularity, constraint types, and context availability.Results show that in the absence of context, models exhibit severe limitations as measured by irrelevant information, factual errors, and incompleteness, many of which exacerbate as information popularity decreases.While context availability mitigates irrelevant information, it is not helpful for satisfying constraints, identifying fundamental barriers to constraint satisfaction.We open source our contributions to foster further research on improving constraint satisfaction abilities of future models. 1",
        "id": 264439509
      }
    ],
    "negative_ctxs": [
      {
        "title": "Learning Sequence-to-Sequence Correspondences from Parallel Corpora via Sequential Pattern Mining",
        "text": "We present an unsupervised extraction of sequence-to-sequence correspondences from parallel corpora by sequential pattern mining. The main characteristics of our method are two-fold. First, we propose a systematic way to enumerate all possible translation pair candidates of rigid and gapped sequences without falling into combinatorial explosion. Second, our method uses an efficient data structure and algorithm for calculating frequencies in a contingency table for each translation pair candidate. Our method is empirically evaluated using English-Japanese parallel corpora of 6 million words. Results indicate that it works well for multi-word translations, giving 56-84% accuracy at 19% token coverage and 11% type coverage.Our Basic IdeaOur approach is illustrated inFigure 1. We concatenate corresponding parallel sentences into bilingual sequences to which sequential pattern mining is applied. By doing so, we obtain the following effects:• It exhaustively generates all possible translation can-1 As of this writing, we learn that Moore will present his results on named entity at EACL 2003.",
        "id": 9769888
      },
      {
        "title": "Improving Pronoun Resolution by Incorporating Coreferential Information of Candidates",
        "text": "Coreferential information of a candidate, such as the properties of its antecedents, is important for pronoun resolution because it reflects the salience of the candidate in the local discourse. Such information, however, is usually ignored in previous learning-based systems. In this paper we present a trainable model which incorporates coreferential information of candidates into pronoun resolution. Preliminary experiments show that our model will boost the resolution performance given the right antecedents of the candidates. We further discuss how to apply our model in real resolution where the antecedents of the candidate are found by a separate noun phrase resolution module. The experimental results show that our model still achieves better performance than the baseline.",
        "id": 18896239
      },
      {
        "title": "A low-complexity, broad-coverage probabilistic Dependency Parser for English",
        "text": "Large-scale parsing is still a complex and timeconsuming process, often so much that it is infeasible in real-world applications. The parsing system described here addresses this problem by combining finite-state approaches, statistical parsing techniques and engineering knowledge, thus keeping parsing complexity as low as possible at the cost of a slight decrease in performance. The parser is robust and fast and at the same time based on strong linguistic foundations.",
        "id": 1031765
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper first use the attention weights to guide the simultaneous inference of speech translation models?",
    "positive_ctxs": [
      {
        "title": "Attention as a Guide for Simultaneous Speech Translation",
        "text": "In simultaneous speech translation (SimulST), effective policies that determine when to write partial translations are crucial to reach high output quality with low latency. Towards this objective, we propose EDATT (Encoder-Decoder Attention), an adaptive policy that exploits the attention patterns between audio source and target textual translation to guide an offlinetrained ST model during simultaneous inference. EDATT exploits the attention scores modeling the audio-translation relation to decide whether to emit a partial hypothesis or wait for more audio input. This is done under the assumption that, if attention is focused towards the most recently received speech segments, the information they provide can be insufficient to generate the hypothesis (indicating that the system has to wait for additional audio input). Results on en→{de, es} show that EDATT yields better results compared to the SimulST state of the art, with gains respectively up to 7 and 4 BLEU points for the two languages, and with a reduction in computational-aware latency up to 1.4s and 0.7s compared to existing SimulST policies applied to offline-trained models.",
        "id": 254685574
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 236459899
      },
      {
        "title": "Enhancing a large scale dictionary with a two-level system",
        "text": "We present in this paper a morphological analyzer and generator for French that contains a dictionary of 700,000 inflected words called DELAF 1, and a full twolevel system aimed at the analysis of new derivatives. Hence, this tool recognizes and generates both correct inflected forms of French simple words (DELAF lookup procedure) and new derivatives and their inflected forms (two-level analysis). Moreover, a clear distinction is made between dictionary look-up processes and new words analyses in order to clearly identify the analyses that involve heuristic rules.We tested this tool upon a French corpus of 1,300,000words with significant results (Clemenceau D. 1992). With regards to efficiency, since this tool is compiled into a unique transducer, it provides a very fast look-up procedure (1,100 words per second) at a low memory cost (around 1.3 Mb in RAM).",
        "id": 20376100
      },
      {
        "title": "",
        "text": "",
        "id": 261341545
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which paper first proved that wide-enough transformer architectures trained with gradient methods on enough data would learn to solve relational reasoning tasks?",
    "positive_ctxs": [
      {
        "title": "When can transformers reason with abstract symbols?",
        "text": "We investigate the capabilities of transformer large language models (LLMs) on relational reasoning tasks involving abstract symbols.Such tasks have long been studied in the neuroscience literature as fundamental building blocks for more complex abilities in programming, mathematics, and verbal reasoning.For (i) regression tasks, we prove that transformers generalize when trained, but require astonishingly large quantities of training data.For (ii) next-token-prediction tasks with symbolic labels, we show an \"inverse scaling law\": transformers fail to generalize as their embedding dimension increases.For both settings (i) and (ii), we propose subtle transformer modifications which can reduce the amount of data needed by adding two trainable parameters per head.",
        "id": 264147017
      }
    ],
    "negative_ctxs": [
      {
        "title": "Automated Generation of Test Suites for Error Analysis of Concept Recognition Systems",
        "text": "We present an architecture and implementation of a system that builds structured test suites for concept recognition systems. The system applies provided test case definitions to a target concept vocabulary, to generate test cases organised according to those definitions. Test case definitions capture particular characteristics, or produce regular transformations, of concept terms. The test suites produced by the system enable detailed, systematic, error analysis of the performance of concept recognition systems.",
        "id": 11447561
      },
      {
        "title": "Summarizing Student Responses to Reflection Prompts",
        "text": "We propose to automatically summarize student responses to reflection prompts and introduce a novel summarization algorithm that differs from traditional methods in several ways. First, since the linguistic units of student inputs range from single words to multiple sentences, our summaries are created from extracted phrases rather than from sentences. Second, the phrase summarization algorithm ranks the phrases by the number of students who semantically mention a phrase in a summary. Experimental results show that the proposed phrase summarization approach achieves significantly better summarization performance on an engineering course corpus in terms of ROUGE scores when compared to other summarization methods, including MEAD, LexRank and MMR.",
        "id": 5910159
      },
      {
        "title": "",
        "text": "",
        "id": 193276396
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Are there any papers that use a world model for planning to ensure that decisions meet constraints?",
    "positive_ctxs": [
      {
        "title": "SAFEDREAMER: SAFE REINFORCEMENT LEARNING WITH WORLD MODELS",
        "text": "The deployment of Reinforcement Learning (RL) in real-world applications is constrained by its failure to satisfy safety criteria.Existing Safe Reinforcement Learning (SafeRL) methods, which rely on cost functions to enforce safety, often fail to achieve zero-cost performance in complex scenarios, especially vision-only tasks.These limitations are primarily due to model inaccuracies and inadequate sample efficiency.The integration of world models has proven effective in mitigating these shortcomings.In this work, we introduce SafeDreamer, a novel algorithm incorporating Lagrangian-based methods into world model planning processes within the superior Dreamer framework.Our method achieves nearly zero-cost performance on various tasks, spanning low-dimensional and vision-only input, within the Safety-Gymnasium benchmark, showcasing its efficacy in balancing performance and safety in RL tasks.Further details and resources are available on the project website: https://sites.google.com/view/safedreamer.",
        "id": 259924554
      }
    ],
    "negative_ctxs": [
      {
        "title": "Transition-based Semantic Dependency Parsing with Pointer Networks",
        "text": "Transition-based parsers implemented with Pointer Networks have become the new state of the art in dependency parsing, excelling in producing labelled syntactic trees and outperforming graph-based models in this task. In order to further test the capabilities of these powerful neural networks on a harder NLP problem, we propose a transition system that, thanks to Pointer Networks, can straightforwardly produce labelled directed acyclic graphs and perform semantic dependency parsing. In addition, we enhance our approach with deep contextualized word embeddings extracted from BERT. The resulting system not only outperforms all existing transitionbased models, but also matches the best fullysupervised accuracy to date on the SemEval 2015 Task 18 English datasets among previous state-of-the-art graph-based parsers.",
        "id": 218900899
      },
      {
        "title": "INTEGRATING SEMANTICS kNO FLEXIBLE SYNTAX BY EXPLOITING ISONORPHISM BETWEEN GRAIelATICAL AND SEMANTICAL RELATIONS",
        "text": "This work concerns integration between syntax and semantics. Syntactic and semantic activities rely on separate bodies of knowledges. Integration is obtained by exploiting the isomorphism between grammatical relations (among immediate constitu~ errs) and conceptual relations, thanks to a limited set of formal mapping rules. Syntactic analysis does not construct all the explicit parse trees but just a graph that represents all the plausible grammatical relations among immediate constituents. Such graph gives the semantic interpreter, based on Conceptual Graphs formalism, the discriminative power required to establish conceptual relations.",
        "id": 2693928
      },
      {
        "title": "Abstract Meaning Representation Guided Graph Encoding and Decoding for Joint Information Extraction",
        "text": "The tasks of Rich Semantic Parsing, such as Abstract Meaning Representation (AMR), share similar goals with Information Extraction (IE) to convert natural language texts into structured semantic representations. To take advantage of such similarity, we propose a novel AMR-guided framework for joint information extraction to discover entities, relations, and events with the help of a pre-trained AMR parser. Our framework consists of two novel components: 1) an AMR based semantic graph aggregator to let the candidate entity and event trigger nodes collect neighborhood information from AMR graph for passing message among related knowledge elements; 2) an AMR guided graph decoder to extract knowledge elements based on the order decided by the hierarchical structures in AMR. Experiments on multiple datasets have shown that the AMR graph encoder and decoder have provided significant gains and our approach has achieved new state-of-the-art performance on all IE subtasks 1 . have-anrel-role-91 wife son :ARG0-of :ARG1-of :ARG1 :ARG2 :ARG2 \"Scott Peterson\" house :location have-anrel-role-91",
        "id": 235097301
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper surveyed the datasets and tasks of asking clarification questions in conversational systems??",
    "positive_ctxs": [
      {
        "title": "A Survey on Asking Clarification Questions Datasets in Conversational Systems",
        "text": "The ability to understand a user's underlying needs is critical for conversational systems, especially with limited input from users in a conversation. Thus, in such a domain, Asking Clarification Questions (ACQs) to reveal users' true intent from their queries or utterances arise as an essential task. However, it is noticeable that a key limitation of the existing ACQs studies is their incomparability, from inconsistent use of data, distinct experimental setups and evaluation strategies. Therefore, in this paper, to assist the development of ACQs techniques, we comprehensively analyse the current ACQs research status, which offers a detailed comparison of publicly available datasets, and discusses the applied evaluation metrics, joined with benchmarks for multiple ACQs-related tasks. In particular, given a thorough analysis of the ACQs task, we discuss a number of corresponding research directions for the investigation of ACQs as well as the development of conversational systems.",
        "id": 258887719
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 232021812
      },
      {
        "title": "ZikaHack 2016: A digital disease detection competition",
        "text": "Effective response to infectious diseases outbreaks relies on the rapid and early detection of those outbreaks. Invalidated, yet timely and openly available digital information can be used for the early detection of outbreaks. Public health surveillance authorities can exploit these early warnings to plan and co-ordinate rapid surveillance and emergency response programs. In 2016, a digital disease detection competition named ZikaHack was launched. The objective of the competition was for multidisciplinary teams to design, develop and demonstrate innovative digital disease detection solutions to retrospectively detect the 2015-16 Brazilian Zika virus outbreak earlier than traditional surveillance methods. In this paper, an overview of the Zik-aHack competition is provided. The challenges and lessons learned in organizing this competition are also discussed for use by other researchers interested in organizing similar competitions.",
        "id": 32752811
      },
      {
        "title": "Exploiting Semantic Role Labeling, WordNet and Wikipedia for Coreference Resolution",
        "text": "In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources. These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels. We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns.",
        "id": 1212389
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which is the first multimodal model combining text and speech transformers trained without labelled text-speech pairs?",
    "positive_ctxs": [
      {
        "title": "Introducing Semantics into Speech Encoders",
        "text": "Recent studies find existing self-supervised speech encoders contain primarily acoustic rather than semantic information. As a result, pipelined supervised automatic speech recognition (ASR) to large language model (LLM) systems achieve state-of-the-art results on semantic spoken language tasks by utilizing rich semantic representations from the LLM. These systems come at the cost of labeled audio transcriptions, which is expensive and time-consuming to obtain. We propose a taskagnostic unsupervised way of incorporating semantic information from LLMs into selfsupervised speech encoders without labeled audio transcriptions. By introducing semantics, we improve existing speech encoder spoken language understanding (SLU) performance by over 5% on intent classification (IC), with modest gains in named entity resolution (NER) and slot filling (SF), and spoken question answering (SQA) FF1 score by over 2%. Our approach, which uses no ASR data, achieves similar performance as methods trained on over 100 hours of labeled audio transcripts, demonstrating the feasibility of unsupervised semantic augmentations to existing speech encoders.Conference on Acoustics, Speech and SignalProcessing (ICASSP), pages 7478-7482. IEEE. Duc Le, Akshat Shrivastava, Paden Tomasello, Suyoun Kim, Aleksandr Livshits, Ozlem Kalinli, and Michael L Seltzer. 2022. Deliberation model for on-device spoken language understanding. arXiv preprint arXiv:2204.01893. . 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461. . 2020. Multilingual speech translation with efficient finetuning of pretrained models. arXiv preprint arXiv:2010.12829. Piero Molino, Yaroslav Dudin, and Sai Sumanth Miryala. 2019. Ludwig: a type-based declarative deep learning toolbox. arXiv preprint arXiv:1909.07930.",
        "id": 253523259
      }
    ],
    "negative_ctxs": [
      {
        "title": "Investigating the cross-linguistic potential of VerbNet -style classification",
        "text": "Verb classes which integrate a wide range of linguistic properties(Levin, 1993)have proved useful for natural language processing (NLP) applications. However, the real-world use of these classes has been limited because for most languages, no resources similar to VerbNet (Kipper-Schuler, 2005) are available. We apply a verb clustering approach developed for English to French -a language for which no such experiment has been conducted yet. Our investigation shows that not only the general methodology but also the best performing features are transferable between the languages, making it possible to learn useful VerbNet style classes for French automatically without languagespecific tuning.",
        "id": 3507562
      },
      {
        "title": "",
        "text": "",
        "id": 219300781
      },
      {
        "title": "Analyzing BERT's Knowledge of Hypernymy via Prompting",
        "text": "The high performance of large pretrained language models (LLMs) such as BERT (Devlin  et al., 2019)  on NLP tasks has prompted questions about BERT's linguistic capabilities, and how they differ from humans'. In this paper, we approach this question by examining BERT's knowledge of lexical semantic relations. We focus on hypernymy, the \"is-a\" relation that relates a word to a superordinate category.We use a prompting methodology to simply ask BERT what the hypernym of a given word is. We find that, in a setting where all hypernyms are guessable via prompting, BERT knows hypernyms with up to 57% accuracy. Moreover, BERT with prompting outperforms other unsupervised models for hypernym discovery even in an unconstrained scenario. However, BERT's predictions and performance on a dataset containing uncommon hyponyms and hypernyms indicate that its knowledge of hypernymy is still limited.",
        "id": 241583486
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research that investigates techniques for creating counterfactual examples to enhance question-answering systems, such as training a T5 model augmented with retrieval?",
    "positive_ctxs": [
      {
        "title": "Retrieval-guided Counterfactual Generation for QA",
        "text": "Deep NLP models have been shown to be brittle to input perturbations. Recent work has shown that data augmentation using counterfactuals -i.e. minimally perturbed inputscan help ameliorate this weakness. We focus on the task of creating counterfactuals for question answering, which presents unique challenges related to world knowledge, semantic diversity, and answerability. To address these challenges, we develop a Retrieve-Generate-Filter (RGF) technique to create counterfactual evaluation and training data with minimal human supervision. Using an open-domain QA framework and question generation model trained on original task data, we create counterfactuals that are fluent, semantically diverse, and automatically labeled. Data augmentation with RGF counterfactuals improves performance on out-of-domain and challenging evaluation sets over and above existing methods, in both the reading comprehension and open-domain QA settings. Moreover, we find that RGF data leads to significant improvements to robustness to local perturbations. 1",
        "id": 238856938
      }
    ],
    "negative_ctxs": [
      {
        "title": "Modelling the Reduplicating Lushootseed Morphology with an FST and LSTM",
        "text": "In this paper, we present an FST based approach for conducting morphological analysis, lemmatization and generation of Lushootseed words. Furthermore, we use the FST to generate training data for an LSTM based neural model and train this model to do morphological analysis. The neural model reaches a 71.9% accuracy on the test data. Furthermore, we discuss reduplication types in the Lushootseed language forms. The approach involves the use of both attested instances of reduplication and bare stems for applying a variety of reduplications to, as it is unclear just how much variation can be attributed to the individual speakers and authors of the source materials. That is, there may be areal factors that can be aligned with certain types of reduplication and their frequencies.",
        "id": 259833791
      },
      {
        "title": "SURF: SEMI-SUPERVISED REWARD LEARNING WITH DATA AUGMENTATION FOR FEEDBACK-EFFICIENT PREFERENCE-BASED REINFORCEMENT LEARNING",
        "text": "Preference-based reinforcement learning (RL) has shown potential for teaching agents to perform the target tasks without a costly, pre-defined reward function by learning the reward with a supervisor's preference between the two agent behaviors. However, preference-based learning often requires a large amount of human feedback, making it difficult to apply this approach to various applications. This data-efficiency problem, on the other hand, has been typically addressed by using unlabeled samples or data augmentation techniques in the context of supervised learning. Motivated by the recent success of these approaches, we present SURF, a semi-supervised reward learning framework that utilizes a large amount of unlabeled samples with data augmentation. In order to leverage unlabeled samples for reward learning, we infer pseudo-labels of the unlabeled samples based on the confidence of the preference predictor. To further improve the label-efficiency of reward learning, we introduce a new data augmentation that temporally crops consecutive subsequences from the original behaviors. Our experiments demonstrate that our approach significantly improves the feedback-efficiency of the state-ofthe-art preference-based method on a variety of locomotion and robotic manipulation tasks.",
        "id": 247613305
      },
      {
        "title": "",
        "text": "",
        "id": 219302491
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper first shows that large language models can be prompted to act like professional annotators to evaluate text generation quality?",
    "positive_ctxs": [
      {
        "title": "Can Large Language Models Be an Alternative to Human Evaluation?",
        "text": "Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms. Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided. In this paper, we explore if such an ability of the LLMs can be used as an alternative to human evaluation. We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation. We use human evaluation and LLM evaluation to evaluate the texts in two NLP tasks: openended story generation and adversarial attacks. We show that the result of LLM evaluation is consistent with the results obtained by expert human evaluation: the texts rated higher by human experts are also rated higher by the LLMs. We also find that the results of LLM evaluation are stable over different formatting of the task instructions and the sampling algorithm used to generate the answer. We are the first to show the potential of using LLMs to assess the quality of texts and discuss the limitations and ethical considerations of LLM evaluation. , et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. , et al. 2022. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.",
        "id": 258461287
      }
    ],
    "negative_ctxs": [
      {
        "title": "A method for the approximation of incremental understanding of explicit utterance meaning using predictive models in finite domains",
        "text": "This paper explores the relationship between explicit and predictive models of incremental speech understanding in a dialogue system that supports a finite set of user utterance meanings. We present a method that enables the approximation of explicit understanding using information implicit in a predictive understanding model for the same domain. We show promising performance for this method in a corpus evaluation, and discuss its practical application and annotation costs in relation to some alternative approaches.",
        "id": 7776388
      },
      {
        "title": "NatLogAttack: A Framework for Attacking Natural Language Inference Models with Natural Logic",
        "text": "Reasoning has been a central topic in artificial intelligence from the beginning. The recent progress made on distributed representation and neural networks continues to improve the state-of-the-art performance of natural language inference. However, it remains an open question whether the models perform real reasoning to reach their conclusions or rely on spurious correlations. Adversarial attacks have proven to be an important tool to help evaluate the Achilles' heel of the victim models. In this study, we explore the fundamental problem of developing attack models based on logic formalism. We propose NatLogAttack to perform systematic attacks centring around natural logic, a classical logic formalism that is traceable back to Aristotle's syllogism and has been closely developed for natural language inference. The proposed framework renders both label-preserving and label-flipping attacks. We show that compared to the existing attack models, NatLogAttack generates better adversarial examples with fewer visits to the victim models. The victim models are found to be more vulnerable under the label-flipping setting. NatLogAttack provides a tool to probe the existing and future NLI models' capacity from a key viewpoint and we hope more logicbased attacks will be further explored for understanding the desired property of reasoning. 1",
        "id": 259360453
      },
      {
        "title": "A Personalized Markov Clustering and Deep Learning Approach for Arabic Text Categorization",
        "text": "Text categorization has become a key research field in the NLP community. However, most works in this area are focused on Western languages ignoring other Semitic languages like Arabic. These languages are of immense political and social importance necessitating robust categorization techniques. In this paper, we present a novel three-stage technique to efficiently classify Arabic documents into different categories based on the words they contain. We leverage the significance of root-words in Arabic and incorporate a combination of Markov clustering and Deep Belief Networks to classify Arabic words into separate groups (clusters). Our approach is tested on two public datasets giving a F-Measure of 91.02%.",
        "id": 1912220
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Are there any papers that study whether you can identify if a LLM has been instructed to hide some information?",
    "positive_ctxs": [
      {
        "title": "HOW TO CATCH AN AI LIAR: LIE DETECTION IN BLACK-BOX LLMS BY ASKING UNRELATED QUESTIONS",
        "text": "Large language models (LLMs) can \"lie\", which we define as outputting false statements despite \"knowing\" the truth in a demonstrable sense.LLMs might \"lie\", for example, when instructed to output misinformation.Here, we develop a simple lie detector that requires neither access to the LLM's activations (black-box) nor ground-truth knowledge of the fact in question.The detector works by asking a predefined set of unrelated follow-up questions after a suspected lie, and feeding the LLM's yes/no answers into a logistic regression classifier.Despite its simplicity, this lie detector is highly accurate and surprisingly general.When trained on examples from a single setting-prompting GPT-3.5 to lie about factual questionsthe detector generalises out-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie, (3) sycophantic lies, and (4) lies emerging in real-life scenarios such as sales.These results indicate that LLMs have distinctive lierelated behavioural patterns, consistent across architectures and contexts, which could enable general-purpose lie detection.",
        "id": 263152829
      }
    ],
    "negative_ctxs": [
      {
        "title": "Unsupervised AMR-Dependency Parse Alignment",
        "text": "In this paper, we introduce an Abstract Meaning Representation (AMR) to Dependency Parse aligner. Alignment is a preliminary step for AMR parsing, and our aligner improves current AMR parser performance. Our aligner involves several different features, including named entity tags and semantic role labels, and uses Expectation-Maximization training. Results show that our aligner reaches an 87.1% F-Score score with the experimental data, and enhances AMR parsing.",
        "id": 7514610
      },
      {
        "title": "Lexical Knowledge Representation with Contexonyms",
        "text": "Inter-word associations like stagger -drunken, or intra-word sense divisions (e.g. write a diary vs. write an article) are difficult to compile using a traditional lexicographic approach. As an alternative, we present a model that reflects this kind of subtle lexical knowledge. Based on the minimal sense of a word (clique), the model (1) selects contextually related words (contexonyms) and(2)classifies them in a multi-dimensional semantic space. Trained on very large corpora, the model provides relevant, organized contexonyms that reflect the fine-grained connotations and contextual usage of the target word, as well as the distinct senses of homonyms and polysemous words. Further study on the neighbor effect showed that the model can handle the data sparseness problemstagger -drunken, which could be informative for non-English speakers or machines, are too numerous to be processed. Intra-word relations share this problem: while the English word write is considered to have the same semantic value in \"write a diary\" and \"write an article\", the French wordsécrire and rédiger, respectively, are widely used in these two phrases. This sort of sense division is also too minute and too frequent to be captured using conventional manual lexicography techniques.An alternative would therefore be to automatically generate the related words for a given word, which could serve as a reference. Clearly, contextually related words are meaningful indicators of the target word's semantic value in a given context. For instance, two sets of words { lit, candle, cigarette } and { tennis, final, win } are trustworthy cue-word sets for disambiguating the word match; stupid is more closely related to blunder than to error(Edmonds and Hirst, 2002), and peace distinguishes treaty from contract(Dagan and Itai, 1994).Such word lists may be obtained for target words by selecting seed words and performing an iterative, decision-list-making task(Yarowsky, 1995), or by latent semantic indexing (LSI)(Landauer et al., 1998). A common limitation of these approaches, however, is that they do not provide a fully automatic method for organizing the related words obtained: identifying seed words needs human intervention and LSI does not provide an automatic classification other than a restricted matching-based one that requires an encyclopedia as a source text(Laham, 1997).",
        "id": 13512315
      },
      {
        "title": "Classification-based Quality Estimation: Small and Efficient Models for Real-world Applications",
        "text": "Sentence-level Quality Estimation (QE) of machine translation is traditionally formulated as a regression task, and the performance of QE models is typically measured by Pearson correlation with human labels. Recent QE models have achieved previously-unseen levels of correlation with human judgments, but they rely on large multilingual contextualized language models that are computationally expensive and thus infeasible for many real-world applications. In this work, we evaluate several model compression techniques for QE and find that, despite their popularity in other NLP tasks, they lead to poor performance in this regression setting. We observe that a full model parameterization is required to achieve SoTA results in a regression task. However, we argue that the level of expressiveness of a model in a continuous range is unnecessary given the downstream applications of QE, and show that reframing QE as a classification problem and evaluating QE models using classification metrics would better reflect their actual performance in real-world applications.",
        "id": 237563027
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "I'm looking into morphological embedding algorithms that build upon the word2vec model by utilizing character n-grams. Which papers should I read to learn more about this approach?",
    "positive_ctxs": [
      {
        "title": "Enriching Word Vectors with Subword Information",
        "text": "Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Many popular models to learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for morphologically rich languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skip-gram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram, words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpus quickly. We evaluate the obtained word representations on five different languages, on word similarity and analogy tasks.",
        "id": 207556454
      }
    ],
    "negative_ctxs": [
      {
        "title": "WIT: A Toolkit for Building Robust and Real-Time Spoken Dialogue Systems",
        "text": "This paper describes WI'I; a toolkit for building spoken dialogue systems. WIT features an incremental understanding mechanism that enables robust utterance understanding and realtime responses. WIT's ability to compile domain-dependent system specifications into internal knowledge sources makes building spoken dialogue systems much easier than :it is from scratch.157 the anonymous reviewers were of' great help.",
        "id": 228130
      },
      {
        "title": "AUTOMATED QUALITY MONITORING FOR CALL CENTERS USING SPEECH AND NLP TECHNOLOGIES",
        "text": "This paper describes an automated system for assigning quality scores to recorded call center conversations. The system combines speech recognition, pattern matching, and maximum entropy classification to rank calls according to their measured quality. Calls at both ends of the spectrum are flagged as \"interesting\" and made available for further human monitoring. In this process, the ASR transcript is used to answer a set of standard quality control questions such as \"did the agent use courteous words and phrases,\" and to generate a question-based score. This is interpolated with the probability of a call being \"bad,\" as determined by maximum entropy operating on a set of ASR-derived features such as \"maximum silence length\" and the occurrence of selected n-gram word sequences. The system is trained on a set of calls with associated manual evaluation forms. We present precision and recall results from IBM's North American Help Desk indicating that for a given amount of listening effort, this system triples the number of bad calls that are identified, over the current policy of randomly sampling calls. The application that will be demonstrated is a research prototype that was built in conjunction with IBM's North American call centers.",
        "id": 18047189
      },
      {
        "title": "Annotating Expressions of Opinion and Emotion in the Italian Content Annotation Bank",
        "text": "In this paper we describe the result of manually annotating I-CAB, the Italian Content Annotation Bank, by expressions of private state (EPSs), i.e., expressions that denote the presence of opinions, emotions, and other cognitive states. The aim of this effort was the generation of a standard resource for supporting the development of opinion extraction algorithms for Italian, and of a benchmark for testing such algorithms. To this end we have employed a previously existing annotation language (here dubbed WWC, from the initials of its proponents). We here describe the results of this annotation effort, including the results of a thorough inter-annotator agreement test. We conclude by discussing how WWC can be adapted to the specificities of a Romance language such as Italian.",
        "id": 9520585
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Which paper specifies the typical configurations used in fine-tuning deep bidirectional transformers like BERT and RoBERTa for language understanding tasks?",
    "positive_ctxs": [
      {
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "text": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
        "id": 52967399
      }
    ],
    "negative_ctxs": [
      {
        "title": "The Effects of Word Prediction on Communication Rate for AAC",
        "text": "Individuals using an Augmentative and Alternative Communication (AAC) device communicate at less than 10% of the speed of \"traditional\" speech, creating a large communication gap. In this user study, we compare the communication rate of pseudo-impaired individuals using two different word prediction algorithms and a system without word prediction. Our results show that word prediction can increase AAC communication rate and that more accurate predictions significantly improve communication rate.",
        "id": 16511639
      },
      {
        "title": "Multimodal Joint Attribute Prediction and Value Extraction for E-commerce Product",
        "text": "Product attribute values are essential in many e-commerce scenarios, such as customer service robots, product recommendations, and product retrieval. While in the real world, the attribute values of a product are usually incomplete and vary over time, which greatly hinders the practical applications. In this paper, we propose a multimodal method to jointly predict product attributes and extract values from textual product descriptions with the help of the product images. We argue that product attributes and values are highly correlated, e.g., it will be easier to extract the values on condition that the product attributes are given. Thus, we jointly model the attribute prediction and value extraction tasks from multiple aspects towards the interactions between attributes and values. Moreover, product images have distinct effects on our tasks for different product attributes and values. Thus, we selectively draw useful visual information from product images to enhance our model. We annotate a multimodal product attribute value dataset that contains 87,194 instances, and the experimental results on this dataset demonstrate that explicitly modeling the relationship between attributes and values facilitates our method to establish the correspondence between them, and selectively utilizing visual product information is necessary for the task. Our code and dataset are available 1 .",
        "id": 221703022
      },
      {
        "title": "Mapping product descriptions to a large ontology",
        "text": "In this paper we describe an information retrieval approach for mapping online business information texts to concepts in a large ontology. We adopt the traditional vector space model by representing the texts as queries and the concept labels in the ontology as documents.Because of the size of the ontology and the fact that concept labels are very sparse and generic, we conducted additional experiments for reducing the set of concepts, as well as the enrichment and enlargement of concept labels.The documents in our collection were of too poor quality for this task, and although we show that our enrichment technique did provide us with an ontology with good overall similarity to our query collection, individual concepts did not include enough terms for our method to achieve good results.",
        "id": 8323291
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there any paper that uses prompt tuning in multi-answer QA?",
    "positive_ctxs": [
      {
        "title": "Answering Ambiguous Questions via Iterative Prompting",
        "text": "In open-domain question answering, due to the ambiguity of questions, multiple plausible answers may exist. To provide feasible answers to an ambiguous question, one approach is to directly predict all valid answers, but this can struggle with balancing relevance and diversity. An alternative is to gather candidate answers and aggregate them, but this method can be computationally costly and may neglect dependencies among answers. In this paper, we present AmbigPrompt to address the imperfections of existing approaches to answering ambiguous questions. Specifically, we integrate an answering model with a prompting model in an iterative manner. The prompting model adaptively tracks the reading process and progressively triggers the answering model to compose distinct and relevant answers. Additionally, we develop a task-specific postpretraining approach for both the answering model and the prompting model, which greatly improves the performance of our framework. Empirical studies on two commonly-used open benchmarks show that AmbigPrompt achieves state-of-the-art or competitive results while using less memory and having a lower inference latency than competing approaches. Additionally, AmbigPrompt also performs well in low-resource settings. The code are available at: https://github. com/sunnweiwei/AmbigPrompt.",
        "id": 259370644
      }
    ],
    "negative_ctxs": [
      {
        "title": "A STRUCTURED REPRESENTATION OF WORD-SENSES IrOR SEMANTIC ANALYSIS",
        "text": "A framework for a structured representation of semantic knowledge (e.g. word-senses) has been defined at the IBM Scientific Center of Roma, as part of a project on Italian Text Understanding. This representation, based on the conceptual graphs formalism [SOW84], expresses deep knowledge (pragmatic) on word-senses. The knowledge base data structure is such as to provide easy access by the semantic verification algorithm. This paper discusses some important problem related to the definition of a semantic knowledge base, as depth versus generality, hierarchical ordering of concept types, etc., and describes the solutions adopted within the text understanding project.",
        "id": 12363636
      },
      {
        "title": "Probing Script Knowledge from Pre-Trained Models",
        "text": "Script knowledge is critical for humans to understand the broad daily tasks and routine activities in the world. Recently researchers have explored the large-scale pre-trained language models (PLMs) to perform various script related tasks, such as story generation, temporal ordering of event, future event prediction and so on. However, it's still not well studied in terms of how well the PLMs capture the script knowledge. To answer this question, we design three probing tasks: inclusive sub-event selection, starting sub-event selection and temporal ordering to investigate the capabilities of PLMs with and without fine-tuning. The three probing tasks can be further used to automatically induce a script for each main event given all the possible sub-events. Taking BERT as a case study, by analyzing its performance on script induction as well as each individual probing task, we conclude that the stereotypical temporal knowledge among the sub-events is well captured in BERT, however the inclusive or starting sub-event knowledge is barely encoded.",
        "id": 248299724
      },
      {
        "title": "Interactive Group Suggesting for Twitter",
        "text": "The number of users on Twitter has drastically increased in the past years. However, Twitter does not have an effective user grouping mechanism. Therefore tweets from other users can quickly overrun and become inconvenient to read. In this paper, we propose methods to help users group the people they follow using their provided seeding users. Two sources of information are used to build sub-systems: textural information captured by the tweets sent by users, and social connections among users. We also propose a measure of fitness to determine which subsystem best represents the seed users and use it for target user ranking. Our experiments show that our proposed framework works well and that adaptively choosing the appropriate sub-system for group suggestion results in increased accuracy.",
        "id": 18172634
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Have any research papers investigated human capacity to distinguish AI-generated text from human-authored text?",
    "positive_ctxs": [
      {
        "title": "All That's 'Human' Is Not Gold: Evaluating Human Evaluation of Generated Text",
        "text": "Human evaluations are typically considered the gold standard in natural language generation, but as models' fluency improves, how well can evaluators detect and judge machinegenerated text? We run a study assessing nonexperts' ability to distinguish between humanand machine-authored text (GPT2 and GPT3) in three domains (stories, news articles, and recipes). We find that, without training, evaluators distinguished between GPT3-and humanauthored text at random chance level. We explore three approaches for quickly training evaluators to better identify GPT3-authored text (detailed instructions, annotated examples, and paired examples) and find that while evaluators' accuracy improved up to 55%, it did not significantly improve across the three domains. Given the inconsistent results across text domains and the often contradictory reasons evaluators gave for their judgments, we examine the role untrained human evaluations play in NLG evaluation and provide recommendations to NLG researchers for improving human evaluations of text generated from state-of-the-art models.",
        "id": 235694265
      }
    ],
    "negative_ctxs": [
      {
        "title": "HEGEL: Hypergraph Transformer for Long Document Summarization",
        "text": "Extractive summarization for long documents is challenging due to the extended structured input context. The long-distance sentence dependency hinders cross-sentence relations modeling, the critical step of extractive summarization. This paper proposes HEGEL, a hypergraph neural network for long document summarization by capturing high-order crosssentence relations. HEGEL updates and learns effective sentence representations with hypergraph transformer layers and fuses different types of sentence dependencies, including latent topics, keywords coreference, and section structure. We validate HEGEL by conducting extensive experiments on two benchmark datasets, and experimental results demonstrate the effectiveness and efficiency of HEGEL.",
        "id": 252780923
      },
      {
        "title": "Semantics WORD SENSE ACQUISITION FOR MULTILINGUAL TEXT INTERPRETATION *",
        "text": "",
        "id": 7711342
      },
      {
        "title": "ACQUIRING LEXICAL KNOWLEDGE FOR ANAPHORA RESOLUTION",
        "text": "The lack of adequate bases of commonsense or even lexical knowledge is perhaps the main obstacle to the development of highperformance, robust tools for semantic interpretation. It is also generally accepted that, notwithstanding the increasing availability in recent years of substantial hand-coded lexical resources such as WordNet and EuroWordNet, addressing the commonsense knowledge bottleneck will eventually require the development of effective techniques for acquiring such information automatically, e.g., from corpora. We discuss research aimed at improving the performance of anaphora resolution systems by acquiring the commonsense knowledge require to resolve the more complex cases of anaphora, such as bridging references. We focus in particular on the problem of acquiring information about part-of relations.",
        "id": 6397234
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which paper first study POMDP with enhanced feedback on observations?",
    "positive_ctxs": [
      {
        "title": "Sample-Efficient Learning of POMDPs with Multiple Observations In Hindsight",
        "text": "This paper studies the sample-efficiency of learning in Partially Observable Markov Decision Processes (POMDPs), a challenging problem in reinforcement learning that is known to be exponentially hard in the worst-case. Motivated by real-world settings such as loading in game playing, we propose an enhanced feedback model called \"multiple observations in hindsight\", where after each episode of interaction with the POMDP, the learner may collect multiple additional observations emitted from the encountered latent states, but may not observe the latent states themselves. We show that sample-efficient learning under this feedback model is possible for two new subclasses of POMDPs: multi-observation revealing POMDPs and distinguishable POMDPs. Both subclasses generalize and substantially relax revealing POMDPs-a widely studied subclass for which sample-efficient learning is possible under standard trajectory feedback. Notably, distinguishable POMDPs only require the emission distributions from different latent states to be different instead of linearly independent as required in revealing POMDPs. * Fudan University.",
        "id": 259360601
      }
    ],
    "negative_ctxs": [
      {
        "title": "PanLex and LEXTRACT: Translating all Words of all Languages of the World",
        "text": "PanLex is a lemmatic translation resource which combines a large number of translation dictionaries and other translingual lexical resources. It currently covers 1353 language varieties and 12M expressions, but aims to cover all languages and up to 350M expressions. This paper describes the resource and current applications of it, as well as lextract, a new effort to expand the coverage of PanLex via semi-automatic dictionary scraping.",
        "id": 1761416
      },
      {
        "title": "RDoC Task at BioNLP-OST 2019: A Mental Health Informatics Task with Research Domain Criteria",
        "text": "BioNLP Open Shared Tasks (BioNLP-OST) is an international competition organized to facilitate development and sharing of computational tasks of biomedical text mining and solutions to them. For BioNLP-OST 2019, we introduced a new mental health informatics task called \"RDoC Task\", which is composed of two subtasks: information retrieval and sentence extraction through National Institutes of Mental Health's Research Domain Criteria framework. Five and four teams around the world participated in the two tasks, respectively. According to the performance on the two tasks, we observe that there is room for improvement for text mining on brain research and mental illness.",
        "id": 207764605
      },
      {
        "title": "Rapid Prototyping of Robust Language Understanding Modules for Spoken Dialogue Systems",
        "text": "Language understanding (LU) modules for spoken dialogue systems in the early phases of their development need to be (i) easy to construct and (ii) robust against various expressions. Conventional methods of LU are not suitable for new domains, because they take a great deal of effort to make rules or transcribe and annotate a sufficient corpus for training. In our method, the weightings of the Weighted Finite State Transducer (WFST) are designed on two levels and simpler than those for conventional WFST-based methods. Therefore, our method needs much fewer training data, which enables rapid prototyping of LU modules. We evaluated our method in two different domains. The results revealed that our method outperformed baseline methods with less than one hundred utterances as training data, which can be reasonably prepared for new domains. This shows that our method is appropriate for rapid prototyping of LU modules.",
        "id": 11379865
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "What approaches have been used to address the limitations of the beam search method in neural machine translation systems in terms of considering the full target context?",
    "positive_ctxs": [
      {
        "title": "Discriminative Reranking for Neural Machine Translation",
        "text": "Reranking models enable the integration of rich features to select a better output hypothesis within an n-best list or lattice. These models have a long history in NLP, and we revisit discriminative reranking for modern neural machine translation models by training a large transformer architecture. This takes as input both the source sentence as well as a list of hypotheses to output a ranked list. The reranker is trained to predict the observed distribution of a desired metric, e.g. BLEU, over the n-best list. Since such a discriminator contains hundreds of millions of parameters, we improve its generalization using pre-training and data augmentation techniques. Experiments on four WMT directions show that our discriminative reranking approach is effective and complementary to existing generative reranking approaches, yielding improvements of up to 4 BLEU over the beam search output.",
        "id": 236460293
      },
      {
        "title": "Simple and Effective Noisy Channel Modeling for Neural Machine Translation",
        "text": "Previous work on neural noisy channel modeling relied on latent variable models that incrementally process the source and target sentence. This makes decoding decisions based on partial source prefixes even though the full source is available. We pursue an alternative approach based on standard sequence to sequence models which utilize the entire source. These models perform remarkably well as channel models, even though they have neither been trained on, nor designed to factor over incomplete target sentences. Experiments with neural language models trained on billions of words show that noisy channel models can outperform a direct model by up to 3.2 BLEU on WMT'17 German-English translation. We evaluate on four language-pairs and our channel models consistently outperform strong alternatives such right-to-left reranking models and ensembles of direct models. 1",
        "id": 201058550
      }
    ],
    "negative_ctxs": [
      {
        "title": "D-STAG : un formalisme d'analyse automatique de discours fondé sur les TAG synchrones",
        "text": "Nous proposons D-STAG, un nouveau formalisme pour l'analyse automatique de la structure discursive des textes. Les analyses produites par D-STAG sont des structures de discours hiérarchiques annotées de relations de discours, qui sont compatibles avec les structures de discours produites en SDRT. L'analyse discursive prolonge l'analyse phrastique, sans modifier celle-ci, ce qui rend envisageable la mise en oeuvre d'un analyseur de discours.ABSTRACT. We propose D-STAG, a new formalism for the automatic analysis of the discourse structure of texts. The analyses computed by D-STAG are hierarchical discourse structures annotated with discourse relations, that are compatible with discourse structures computed in SDRT. The discourse analysis extends the sententiall analysis, without modifying it, which makes conceivable the realization of a discourse analyzer.",
        "id": 12480349
      },
      {
        "title": "This Phrase-Based SMT System is Out of Order: Generalised Word Reordering in Machine Translation",
        "text": "Many natural language processes have some degree of preprocessing of data: tokenisation, stemming and so on. In the domain of Statistical Machine Translation it has been shown that word reordering as a preprocessing step can help the translation process.Recently, hand-written rules for reordering in German-English translation have shown good results, but this is clearly a labour-intensive and language pair-specific approach. Two possible sources of the observed improvement are that (1) the reordering explicitly matches the syntax of the source language more closely to that of the target language, or that (2) it fits the data better to the mechanisms of phrasal SMT; but it is not clear which. In this paper, we apply a general principle based on dependency distance minimisation to produce reorderings. Our languageindependent approach achieves half of the improvement of a reimplementation of the handcrafted approach, and suggests that reason (2) is a possible explanation for why that reordering approach works.Help you I can, yes.Jedi Master Yoda",
        "id": 15272106
      },
      {
        "title": "Semantics, Discourse and Statistical Machine Translation",
        "text": "In the past decade, statistical machine translation (SMT) has been advanced from word-based SMT to phrase-and syntax-based SMT. Although this advancement produces significant improvements in BLEU scores, crucial meaning errors and lack of cross-sentence connections at discourse level still hurt the quality of SMT-generated translations. More recently, we have witnessed two active movements in SMT research: one towards combining semantics and SMT in attempt to generate not only grammatical but also meaningpreserved translations, and the other towards exploring discourse knowledge for document-level machine translation in order to capture intersentence dependencies.The emergence of semantic SMT are due to the combination of two factors: the necessity of semantic modeling in SMT and the renewed interest of designing models tailored to relevant NLP/SMT applications in the semantics community. The former is represented by recent numerous studies on exploring word sense disambiguation, semantic role labeling, bilingual semantic representations as well as semantic evaluation for SMT. The latter is reflected in CoNLL shared tasks, SemEval and SenEval exercises in recent years.The need of capturing cross-sentence dependencies for document-level SMT triggers the resurgent interest of modeling translation from the perspective of discourse. Discourse phenomena, such as coherent relations, discourse topics, lexical cohesion that are beyond the scope of conventional sentence-level n-grams, have been recently considered and explored in the context of SMT.This tutorial aims at providing a timely and combined introduction of such recent work along these two trends as discourse is inherently connected with semantics. The tutorial has three parts. The first part critically reviews the phrase-and syntax-based SMT. The second part is devoted to the lines of research oriented to semantic SMT, including a brief introduction of semantics, lexical and shallow semantics tailored to SMT, semantic representations in SMT, semantically motivated evaluation as well as advanced topics on deep semantic learning for SMT. The third part is dedicated to recent work on SMT with discourse, including a brief review on discourse studies from linguistics and computational viewpoints, discourse research from monolingual to multilingual, discourse-based SMT and a few advanced topics.The tutorial is targeted for researchers in the SMT, semantics and discourse communities. In particular, the expected audience comes from two groups: 1) Researchers and students in the SMT community who want to design cutting-edge models and algorithms for semantic SMT with various semantic knowledge and representations, and who would like to advance SMT from sentence-bysentence translation to document-level translation with discourse information; 2) Researchers and students from the semantics and discourse community who are interested in developing models and methods and adapting them to SMT.OutlineSMT Overall Review (30 minutes)• SMT architecture • phrase-and syntax-based SMT 2. Semantics and SMT (1 hour and 15 minutes)",
        "id": 6864375
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "What molecular representation learning paper introduced a benchmark that focuses on learning over thermodynamically-accessible conformer ensembles across diverse molecular properties and chemical reactions?",
    "positive_ctxs": [
      {
        "title": "Learning Over Molecular Conformer Ensembles: Datasets and Benchmarks",
        "text": "Molecular Representation Learning (MRL) has proven impactful in numerous biochemical applications such as drug discovery and enzyme design.While Graph Neural Networks (GNNs) are effective at learning molecular representations from a 2D molecular graph or a single 3D structure, existing works often overlook the flexible nature of molecules, which continuously interconvert across conformations via chemical bond rotations and minor vibrational perturbations.To better account for molecular flexibility, some recent works formulate MRL as an ensemble learning problem, focusing on explicitly learning from a set of conformer structures.However, most of these studies have limited datasets, tasks, and models.In this work, we introduce the first MoleculAR Conformer Ensemble Learning (MARCEL) benchmark to thoroughly evaluate the potential of learning on conformer ensembles and suggest promising research directions.MARCEL includes four datasets covering diverse molecule-and reaction-level properties of chemically diverse molecules including organocatalysts and transition-metal catalysts, extending beyond the scope of common GNN benchmarks that are confined to drug-like molecules.In addition, we conduct a comprehensive empirical study, which benchmarks representative 1D, 2D, and 3D molecular representation learning models, along with two strategies that explicitly incorporate conformer ensembles into 3D MRL models.Our findings reveal that direct learning from an accessible conformer space can improve performance on a variety of tasks and models.",
        "id": 263334596
      }
    ],
    "negative_ctxs": [
      {
        "title": "First approach toward Semantic Role Labeling for Basque",
        "text": "In this paper, we present the first Semantic Role Labeling system developed for Basque. The system is implemented using machine learning techniques and trained with the Reference Corpus for the Processing of Basque (EPEC). In our experiments the classifier that offers the best results is based on Support Vector Machines. Our system achieves 84.30 F1 score in identifying the PropBank semantic role for a given constituent and 82.90 F1 score in identifying the VerbNet role. Our study establishes a baseline for Basque SRL. Although there are no directly comparable systems for English we can state that the results we have achieved are quite good. In addition, we have performed a Leave-One-Out feature selection procedure in order to establish which features are the worthiest regarding argument classification. This will help smooth the way for future stages of Basque SRL and will help draw some of the guidelines of our research.",
        "id": 16556429
      },
      {
        "title": "Applying Multi-Sense Embeddings for German Verbs to Determine Semantic Relatedness and to Detect Non-Literal Language",
        "text": "Up to date, the majority of computational models still determines the semantic relatedness between words (or larger linguistic units) on the type level. In this paper, we compare and extend multi-sense embeddings, in order to model and utilise word senses on the token level. We focus on the challenging class of complex verbs, and evaluate the model variants on various semantic tasks: semantic classification; predicting compositionality; and detecting non-literal language usage. While there is no overall best model, all models significantly outperform a word2vec single-sense skip baseline, thus demonstrating the need to distinguish between word senses in a distributional semantic model.",
        "id": 14741568
      },
      {
        "title": "Automatically Detecting Syntactic Errors in Sentences Written by Learners of Chinese as a Foreign Language",
        "text": "This paper proposed a method that can automatically detect syntax errors in Chinese sentences. The algorithm for identifying syntax errors proposed in this study is known as KNGED, which uses a large database of rules to identify whether syntax errors exist in a sentence. The rules were generated either manually or automatically. This paper further proposed an algorithm for identifying the type of error that a sentence contained. Experimental results shown that the false positive rate and F1-measure of the proposed method for detecting syntax errors in Chinese sentences are 0.90 and 0.65.",
        "id": 17679593
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "What prior works suggested that exposure bias could lead to hallucinations in neural machine translation models?",
    "positive_ctxs": [
      {
        "title": "On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation",
        "text": "The standard training algorithm in neural machine translation (NMT) suffers from exposure bias, and alternative algorithms have been proposed to mitigate this. However, the practical impact of exposure bias is under debate. In this paper, we link exposure bias to another well-known problem in NMT, namely the tendency to generate hallucinations under domain shift. In experiments on three datasets with multiple test domains, we show that exposure bias is partially to blame for hallucinations, and that training with Minimum Risk Training, which avoids exposure bias, can mitigate this. Our analysis explains why exposure bias is more problematic under domain shift, and also links exposure bias to the beam search problem, i.e. performance deterioration with increasing beam size. Our results provide a new justification for methods that reduce exposure bias: even if they do not increase performance on in-domain test sets, they can increase model robustness to domain shift.",
        "id": 218538004
      },
      {
        "title": "Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation",
        "text": "In Neural Machine Translation (and, more generally, conditional language modeling), the generation of a target token is influenced by two types of context: the source and the prefix of the target sequence. While many attempts to understand the internal workings of NMT models have been made, none of them explicitly evaluates relative source and target contributions to a generation decision. We argue that this relative contribution can be evaluated by adopting a variant of Layerwise Relevance Propagation (LRP). Its underlying 'conservation principle' makes relevance propagation unique: differently from other methods, it evaluates not an abstract quantity reflecting token importance, but the proportion of each token's influence. We extend LRP to the Transformer and conduct an analysis of NMT models which explicitly evaluates the source and target relative contributions to the generation process. We analyze changes in these contributions when conditioning on different types of prefixes, when varying the training objective or the amount of training data, and during the training process. We find that models trained with more data tend to rely on source information more and to have more sharp token contributions; the training process is non-monotonic with several stages of different nature. 1",
        "id": 224818197
      }
    ],
    "negative_ctxs": [
      {
        "title": "Domain-Specific Image Captioning",
        "text": "We present a data-driven framework for image caption generation which incorporates visual and textual features with varying degrees of spatial structure. We propose the task of domain-specific image captioning, where many relevant visual details cannot be captured by off-the-shelf general-domain entity detectors. We extract previously-written descriptions from a database and adapt them to new query images, using a joint visual and textual bag-of-words model to determine the correctness of individual words. We implement our model using a large, unlabeled dataset of women's shoes images and natural language descriptions (Berg et al., 2010). Using both automatic and human evaluations, we show that our captioning method effectively deletes inaccurate words from extracted captions while maintaining a high level of detail in the generated output.Query Image GIST Nearest-NeighborExtraction: Classic ballet flats with decorative canvas strap and patent leather covered buckle.Compression: Classic ballet flats covered.Query Image GIST Nearest-NeighborExtraction: This shoe is the perfect shoe for you , featuring an open toe and a lace up upper with a high heel , and a two tone color .Compression: This shoe is the shoe , featuring an open toe and upper with a high heel .",
        "id": 16430728
      },
      {
        "title": "Some Properties of Preposition and Subordinate Conjunction Attachments*",
        "text": "Determining the attachments of prepositions and subordinate conjunctions is a key problem in parsing natural language. This paper presents a trainable approach to making these attachments through transformation sequences and error-driven learning. Our approach is broad coverage, and accounts for roughly three times the attachment cases that have previously been handled by corpus-based techniques. In addition, our approach is based on a simplified model of syntax that is more consistent with the practice in current state-of-the-art language processing systems. This paper sketches syntactic and algorithmic details, and presents experimental results on data sets derived from the Penn Treebank. We obtain an attachment accuracy of 75.4% for the general case, the first such corpus-based result to be reported. For the restricted cases previously studied with corpusbased methods, our approach yields an accuracy comparable to current work (83.1%).",
        "id": 3264475
      },
      {
        "title": "Prepositions in Applications: A Survey and Introduction to the Special Issue",
        "text": "",
        "id": 6602375
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "What paper mitigates the vocabulary size limitation when pretraining multilingual masked language models using a contrastive loss?",
    "positive_ctxs": [
      {
        "title": "Headless Language Models: Learning without Predicting with Contrastive Weight Tying",
        "text": "Self-supervised pre-training of language models usually consists in predicting probability distributions over extensive token vocabularies.In this study, we propose an innovative method that shifts away from probability prediction and instead focuses on reconstructing input embeddings in a contrastive fashion via Constrastive Weight Tying (CWT).We apply this approach to pretrain Headless Language Models in both monolingual and multilingual contexts.Our method offers practical advantages, substantially reducing training computational requirements by up to 20 times, while simultaneously enhancing downstream performance and data efficiency.We observe a significant +1.6 GLUE score increase and a notable +2.7 LAMBADA accuracy improvement compared to classical LMs within similar compute budgets.",
        "id": 262013288
      }
    ],
    "negative_ctxs": [
      {
        "title": "Understanding Language Preference for Expression of Opinion and Sentiment: What do Hindi-English Speakers do on Twitter?",
        "text": "Linguistic research on multilingual societies has indicated that there is usually a preferred language for expression of emotion and sentiment (Dewaele, 2010). Paucity of data has limited such studies to participant interviews and speech transcriptions from small groups of speakers. In this paper, we report a study on 430,000 unique tweets from Indian users, specifically Hindi-English bilinguals, to understand the language of preference, if any, for expressing opinion and sentiment. To this end, we develop classifiers for opinion detection in these languages, and further classifying opinionated tweets into positive, negative and neutral sentiments. Our study indicates that Hindi (i.e., the native language) is preferred over English for expression of negative opinion and swearing. As an aside, we explore some common pragmatic functions of codeswitching through sentiment detection.",
        "id": 15456486
      },
      {
        "title": "LANGUAGES OF ANALOGICAL STRINGS",
        "text": "",
        "id": 11813565
      },
      {
        "title": "Graph-combined Coreference Resolution Methods on Conversational Machine Reading Comprehension with Pre-trained Language Model",
        "text": "Coreference resolution such as for anaphora has been an essential challenge that is commonly found in conversational machine reading comprehension (CMRC). This task aims to determine the referential entity to which a pronoun refers on the basis of contextual information. Existing approaches based on pre-trained language models (PLMs) mainly rely on an endto-end method, which still has limitations in clarifying referential dependency. In this study, a novel graph-based approach is proposed to integrate the coreference of given text into graph structures (called coreference graphs), which can pinpoint a pronoun's referential entity. We propose two graph-combined methods, evidence-enhanced and the fusion model, for CMRC to integrate coreference graphs from different levels of the PLM architecture. Evidenceenhanced refers to textual level methods that include an evidence generator (for generating new text to elaborate a pronoun) and enhanced question (for rewriting a pronoun in a question) as PLM input. The fusion model is a structural level method that combines the PLM with a graph neural network. We evaluated these approaches on a CoQA pronoun-containing dataset and the whole CoQA dataset. The result showed that our methods can outperform baseline PLM methods with BERT and RoBERTa.",
        "id": 248780004
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper first proposed a cross-domain language model to automatically generate much labeled data for a unlabeled target domain?",
    "positive_ctxs": [
      {
        "title": "Cross-Domain Data Augmentation with Domain-Adaptive Language Modeling for Aspect-Based Sentiment Analysis",
        "text": "Cross-domain Aspect-Based Sentiment Analysis (ABSA) aims to leverage the useful knowledge from a source domain to identify aspectsentiment pairs in sentences from a target domain. To tackle the task, several recent works explore a new unsupervised domain adaptation framework, i.e., Cross-Domain Data Augmentation (CDDA), aiming to directly generate much labeled target-domain data based on the labeled source-domain data. However, these CDDA methods still suffer from several issues: 1) preserving many source-specific attributes such as syntactic structures; 2) lack of fluency and coherence; 3) limiting the diversity of generated data. To address these issues, we propose a new cross-domain Data Augmentation approach based on Domain-Adaptive Language Modeling named DA 2 LM, which contains three stages: 1) assigning pseudo labels to unlabeled target-domain data; 2) unifying the process of token generation and labeling with a Domain-Adaptive Language Model (DALM) to learn the shared context and annotation across domains; 3) using the trained DALM to generate labeled target-domain data. Experiments show that DA 2 LM consistently outperforms previous feature adaptation and CDDA methods on both ABSA and Aspect Extraction tasks. The source code is publicly released at https://github.com/NUSTM/DALM.",
        "id": 259370800
      }
    ],
    "negative_ctxs": [
      {
        "title": "Higher-Order Coloured Unification and Natural Language Semantics",
        "text": "In this paper, we show that Higher-Order Coloured Unification -a form of unification developed for automated theorem proving -provides a general theory for modeling the interface between the interpretation process and other sources of linguistic, non semantic information. In particular, it provides the general theory for the Primary Occurrence Restriction which (Dalrymple et al., 1991)'s analysis called for. Proc. CADE94, LNAI, pages 635-649, Nancy, France. Steve G. Pulman. 1995. Higher-order unification and the interpretation of focus. Paper submitted for publication. Kai von Fintel. 1995. A minimal theory of adverbial quantification. Unpublished draft Ms. MIT, Cambridge, March.",
        "id": 12147910
      },
      {
        "title": "Minimally Supervised Domain-Adaptive Parse Reranking for Relation Extraction",
        "text": "The paper demonstrates how the generic parser of a minimally supervised information extraction framework can be adapted to a given task and domain for relation extraction (RE). For the experiments a generic deep-linguistic parser was employed that works with a largely hand-crafted headdriven phrase structure grammar (HPSG) for English. The output of this parser is a list of n best parses selected and ranked by a MaxEnt parse-ranking component, which had been trained on a more or less generic HPSG treebank. It will be shown how the estimated confidence of RE rules learned from the n best parses can be exploited for parse reranking. The acquired reranking model improves the performance of RE in both training and test phases with the new first parses. The obtained significant boost of recall does not come from an overall gain in parsing performance but from an application-driven selection of parses that are best suited for the RE task. Since the readings best suited for successful rule extraction and instance extraction are often not the readings favored by a regular parser evaluation, generic parsing accuracy actually decreases. The novel method for taskspecific parse reranking does not require any annotated data beyond the semantic seed, which is needed anyway for the RE task.",
        "id": 15295970
      },
      {
        "title": "In Plain Sight: Media Bias Through the Lens of Factual Reporting",
        "text": "The increasing prevalence of political bias in news media calls for greater public awareness of it, as well as robust methods for its detection. While prior work in NLP has primarily focused on the lexical bias captured by linguistic attributes such as word choice and syntax, other types of bias stem from the actual content selected for inclusion in the text. In this work, we investigate the effects of informational bias: factual content that can nevertheless be deployed to sway reader opinion. We first produce a new dataset, BASIL, of 300 news articles annotated with 1,727 bias spans 1 and find evidence that informational bias appears in news articles more frequently than lexical bias. We further study our annotations to observe how informational bias surfaces in news articles by different media outlets. Lastly, a baseline model for informational bias prediction is presented by finetuning BERT on our labeled data, indicating the challenges of the task and future directions.",
        "id": 202537179
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there a paper that shows that language models' error distribution is different for unfamiliar entities that is not apparent when models are evaluated on familiar entities alone?",
    "positive_ctxs": [
      {
        "title": "Factual or Contextual? Disentangling Error Types in Entity Description Generation",
        "text": "In the task of entity description generation, given a context and a specified entity, a model must describe that entity correctly and in a contextually-relevant way. In this task, as well as broader language generation tasks, the generation of a nonfactual description (factual error) versus an incongruous description (contextual error) is fundamentally different, yet often conflated. We develop an evaluation paradigm that enables us to disentangle these two types of errors in naturally occurring textual contexts. We find that factuality and congruity are often at odds, and that models specifically struggle with accurate descriptions of entities that are less familiar to people. This shortcoming of language models raises concerns around the trustworthiness of such models, since factual errors on less well-known entities are exactly those that a human reader will not recognize.",
        "id": 259093058
      }
    ],
    "negative_ctxs": [
      {
        "title": "Enriching Entity Translation Discovery using Selective Temporality",
        "text": "This paper studies named entity translation and proposes \"selective temporality\" as a new feature, as using temporal features may be harmful for translating \"atemporal\" entities. Our key contribution is building an automatic classifier to distinguish temporal and atemporal entities then align them in separate procedures to boost translation accuracy by 6.1%.",
        "id": 13878778
      },
      {
        "title": "Translating and the Computer 14",
        "text": "The paper describes the BCI, a prototype interactive machine-translation system, constructed by connecting English and Swedish versions of the SRI Core Language Engine through a transfer component. Transfer takes place at the level of Quasi Logical Form (QLF), a contextually sensitive logical form representation which is deep enough for dealing with cross-linguistic differences. Theoretical arguments are presented to support the claim that QLF transfer represents a good compromise between the opposing paradigms of syntactic transfer and semantic interlinguabased MT. An annotated example dialogue is shown. A follow-on project, in which the BCI is used as the core of a spoken-language translation system, is briefly described.",
        "id": 29911601
      },
      {
        "title": "AttesTable at SemEval-2021 Task 9: Extending Statement Verification with Tables for Unknown Class, and Semantic Evidence Finding",
        "text": "This paper describes our approach for Task 9 of SemEval 2021: Statement Verification and Evidence Finding with Tables. We participated in both subtasks, namely statement verification and evidence finding. For the subtask of statement verification, we extend the TAPAS model to adapt to the 'unknown' class of statements by finetuning it on an augmented version of the task data. For the subtask of evidence finding, we finetune the DistilBERT model in a Siamese setting.",
        "id": 236459991
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper first introduced document content as an intermediate generation target and utilized textual document identifiers in generative retrieval?",
    "positive_ctxs": [
      {
        "title": "TOME: A Two-stage Approach for Model-based Retrieval",
        "text": "Recently, model-based retrieval has emerged as a new paradigm in text retrieval that discards the index in the traditional retrieval model and instead memorizes the candidate corpora using model parameters. This design employs a sequence-to-sequence paradigm to generate document identifiers, which enables the complete capture of the relevance between queries and documents and simplifies the classic indexretrieval-rerank pipeline. Despite its attractive qualities, there remain several major challenges in model-based retrieval, including the discrepancy between pre-training and fine-tuning, and the discrepancy between training and inference. To deal with the above challenges, we propose a novel two-stage model-based retrieval approach called TOME, which makes two major technical contributions, including the utilization of tokenized URLs as identifiers and the design of a two-stage generation architecture. We also propose a number of training strategies to deal with the training difficulty as the corpus size increases. Extensive experiments and analysis on MS MARCO and Natural Questions demonstrate the effectiveness of our proposed approach, and we investigate the scaling laws of TOME by examining various influencing factors.",
        "id": 258762633
      }
    ],
    "negative_ctxs": [
      {
        "title": "The representation of syntactically unexpressed com to nouns",
        "text": "We address the representation of nouns having complex argument structures like deverbal nominalisations. In particular we address the semantic representation of syntactically unexpressed arguments.We put forward a treatment of this kind of optional complements in a framework that combines HPSG syntax and the semantic approach in GL(Pustejovsky, 1995).",
        "id": 8564163
      },
      {
        "title": "Using Machine Learning to Maintain Rule-based Named-Entity Recognition and Classification Systems",
        "text": "This paper presents a method that assists in maintaining a rule-based named-entity recognition and classification system. The underlying idea is to use a separate system, constructed with the use of machine learning, to monitor the performance of the rule-based system. The training data for the second system is generated with the use of the rule-based system, thus avoiding the need for manual tagging. The disagreement of the two systems acts as a signal for updating the rule-based system. The generality of the approach is illustrated by applying it to large corpora in two different languages: Greek and French. The results are very encouraging, showing that this alternative use of machine learning can assist significantly in the maintenance of rulebased systems.",
        "id": 15620379
      },
      {
        "title": "Rethinking Reusable Resources",
        "text": "We address the common and recurring problem of data reuse, focusing on the following topics: (i) the current state of affairs (in particular, problems with data); (ii) requirements for change; (iii) the proposed solution (its problems and advantages, as well as related work in this area), including the canonical-, I/O-, and data transformation models; (iv) maintenance issues; (v) implementation and deployment aspects; (vi) conclusions and future directions, including results from work done so far and aspects that merit future work.",
        "id": 21351087
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that investigates a clustering-based efficient attention mechanism within Transformer models?",
    "positive_ctxs": [
      {
        "title": "Efficient Content-Based Sparse Attention with Routing Transformers",
        "text": "Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n 1.5 d) from O(n 2 d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow. 1 53",
        "id": 212718077
      }
    ],
    "negative_ctxs": [
      {
        "title": "Identification of Drug-Related Medical Conditions in Social Media",
        "text": "Monitoring social media has been shown to be an interesting approach for the early detection of drug adverse effects. In this paper, we describe a system which extracts medical entities in French drug reviews written by users. We focus on the identification of medical conditions, which is based on the concept of post-coordination: we first extract minimal medical-related entities (pain, stomach) then we combine them to identify complex ones (It was the worst [pain I ever felt in my stomach]). These two steps are respectively performed by two classifiers, the first being based on Conditional Random Fields and the second one on Support Vector Machines. The overall results of the minimal entity classifier are the following: P=0.926; R=0.849; F1=0.886. A thourough analysis of the feature set shows that, when combined with word lemmas, clusters generated by word2vec are the most valuable features. When trained on the output of the first classifier, the second classifier's performances are the following: p=0.683;r=0.956;f1=0.797. The addition of post-processing rules did not add any significant global improvement but was found to modify the precision/recall ratio.",
        "id": 26521766
      },
      {
        "title": "Gradient Origin Networks",
        "text": "This paper proposes a new type of implicit generative model that is able to quickly learn a latent representation without an explicit encoder. This is achieved with an implicit neural network that takes as inputs points in the coordinate space alongside a latent vector initialised with zeros. The gradients of the data fitting loss with respect to this zero vector are jointly optimised to act as latent points that capture the data manifold. The results show similar characteristics to autoencoders, but with fewer parameters and the advantages of implicit representation networks. * Authors contributed equally.",
        "id": 220364453
      },
      {
        "title": "Sympathy Begins with a Smile, Intelligence Begins with a Word: Use of Multimodal Features in Spoken Human-Robot Interaction",
        "text": "Recognition of social signals, from human facial expressions or prosody of speech, is a popular research topic in human-robot interaction studies. There is also a long line of research in the spoken dialogue community that investigates user satisfaction in relation to dialogue characteristics. However, very little research relates a combination of multimodal social signals and language features detected during spoken face-to-face human-robot interaction to the resulting user perception of a robot. In this paper we show how different emotional facial expressions of human users, in combination with prosodic characteristics of human speech and features of human-robot dialogue, correlate with users' impressions of the robot after a conversation. We find that happiness in the user's recognised facial expression strongly correlates with likeability of a robot, while dialogue-related features (such as number of human turns or number of sentences per robot utterance) correlate with perceiving a robot as intelligent. In addition, we show that facial expression, emotional features, and prosody are better predictors of human ratings related to perceived robot likeability and anthropomorphism, while linguistic and non-linguistic features more often predict perceived robot intelligence and interpretability. As such, these characteristics may in future be used as an online reward signal for in-situ Reinforcement Learningbased adaptive human-robot dialogue systems. Figure 1: Left: a live view of experimental setup showing a participant interacting with Pepper. Right: a diagram of experimental setup showing the participant (green) and the robot (white) positioned face to face. The scene was recorded by cameras (triangles C) from the robot's perspective focusing on the face of the participant and from the side, showing the whole scene. The experimenter (red) was seated behind a divider.",
        "id": 10586665
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Have any research papers tried to create conversational agents with inner states represented by a knowledge graph that can be continually updated based on the agent’s environment?",
    "positive_ctxs": [
      {
        "title": "Towards Socially Intelligent Agents with Mental State Transition and Human Value",
        "text": "Building a socially intelligent agent involves many challenges. One of which is to track the agent's mental state transition and teach the agent to make decisions guided by its value like a human. Towards this end, we propose to incorporate mental state simulation and value modeling into dialogue agents. First, we build a hybrid mental state parser that extracts information from both the dialogue and event observations and maintains a graphical representation of the agent's mind; Meanwhile, the transformer-based value model learns human preferences from the human value dataset, VALUENET. Empirical results show that the proposed model attains state-of-the-art performance on the dialogue/action/emotion prediction task in the fantasy text-adventure game dataset, LIGHT. We also show example cases to demonstrate: (i) how the proposed mental state parser can assist the agent's decision by grounding on the context like locations and objects, and (ii) how the value model can help the agent make decisions based on its personal priorities.Alex Abella. 2009. Soldiers of reason: The RAND corporation and the rise of the American empire. , et al. 2021. Alexa conversations: An extensible data-driven approach for building task-oriented dialogue systems. arXiv preprint arXiv:2104.09088.",
        "id": 252847559
      }
    ],
    "negative_ctxs": [
      {
        "title": "Paradigmatic Modifiability Statistics for the Extraction of Complex Multi-Word Terms",
        "text": "We here propose a new method which sets apart domain-specific terminology from common non-specific noun phrases. It is based on the observation that terminological multi-word groups reveal a considerably lesser degree of distributional variation than non-specific noun phrases. We define a measure for the observable amount of paradigmatic modifiability of terms and, subsequently, test it on bigram, trigram and quadgram noun phrases extracted from a 104-million-word biomedical text corpus. Using a community-wide curated biomedical terminology system as an evaluation gold standard, we show that our algorithm significantly outperforms a variety of standard term identification measures. We also provide empirical evidence that our methodolgy is essentially domain-and corpus-size-independent.",
        "id": 15273358
      },
      {
        "title": "",
        "text": "",
        "id": 208981197
      },
      {
        "title": "LEARNING NON-DETERMINISTIC REPRESENTATIONS WITH ENERGY-BASED ENSEMBLES",
        "text": "The goal of a generative model is to capture the distribution underlying the data, typically through latent variables. After training, these variables are often used as a new representation, more effective than the original features in a variety of learning tasks. However, the representations constructed by contemporary generative models are usually point-wise deterministic mappings from the original feature space. Thus, even with representations robust to class-specific transformations, statistically driven models trained on them would not be able to generalize when the labeled data is scarce. Inspired by the stochasticity of the synaptic connections in the brain, we introduce Energy-based Stochastic Ensembles. These ensembles can learn non-deterministic representations, i.e., mappings from the feature space to a family of distributions in the latent space. These mappings are encoded in a distribution over a (possibly infinite) collection of models. By conditionally sampling models from the ensemble, we obtain multiple representations for every input example and effectively augment the data. We propose an algorithm similar to contrastive divergence for training restricted Boltzmann stochastic ensembles. Finally, we demonstrate the concept of the stochastic representations on a synthetic dataset as well as test them in the one-shot learning scenario on MNIST.",
        "id": 16521404
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which paper considers both weights and activations when pruning large language models?",
    "positive_ctxs": [
      {
        "title": "A SIMPLE AND EFFECTIVE PRUNING APPROACH FOR LARGE LANGUAGE MODELS",
        "text": "As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance.Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive.In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs.Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis.Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is.We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks.Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update.Code is available at https://github.com/locuslab/wanda.",
        "id": 259203115
      }
    ],
    "negative_ctxs": [
      {
        "title": "COM2SENSE: A Commonsense Reasoning Benchmark with Complementary Sentences",
        "text": "Commonsense reasoning is intuitive for humans but has been a long-term challenge for artificial intelligence (AI). Recent advancements in pretrained language models have shown promising results on several commonsense benchmark datasets. However, the reliability and comprehensiveness of these benchmarks towards assessing model's commonsense reasoning ability remains unclear. To this end, we introduce a new commonsense reasoning benchmark dataset comprising natural language true/false statements, with each sample paired with its complementary counterpart, resulting in 4k sentence pairs. We propose a pairwise accuracy metric to reliably measure an agent's ability to perform commonsense reasoning over a given situation. The dataset is crowdsourced and enhanced with an adversarial model-in-the-loop setup to incentivize challenging samples. To facilitate a systematic analysis of commonsense capabilities, we design our dataset along the dimensions of knowledge domains, reasoning scenarios and numeracy. Experimental results demonstrate that our strongest baseline (UnifiedQA-3B), after fine-tuning, achieves 71% standard accuracy and 51% pairwise accuracy, well below human performance ( 95% for both metrics).",
        "id": 235293697
      },
      {
        "title": "",
        "text": "Cet article présente les premières expériences sur le français d'identification automatique des relations discursives implicites (i.e., non marquées par un connecteur). Nos systèmes exploitent des exemples implicites annotés, ainsi que des exemples implicites artificiels obtenus à partir d'exemples explicites par suppression du connecteur, une méthode introduite parMarcu et Echihabi (2002). Les précédentes études sur l'anglais montrent que l'utilisation à l'entraînement des données artificielles dégrade largement les performances sur les données naturelles, ce qui reflète des différences importantes en termes de distribution. Ce constat, qui tient aussi pour le français, nous a amenés à envisager différentes méthodes, inspirées de l'adaptation de domaine, visant à combiner plus efficacement les données. Nous évaluons ces méthodes sur le corpus ANNODIS : notre meilleur système obtient 41,7 % d'exactitude, soit un gain significatif de 4,4 % par rapport à un modèle n'utilisant que les données naturelles.ABSTRACT. This paper presents the first experiments on French in automatic identification of implicit discourse relations (i.e. relations that lack an overt connective). Our systems exploit hand-labeled implicit examples, along with artificial implicit examples obtained from explicit examples by suppressing their connective, followingMarcu et Echihabi (2002). Previous work on English shows that using artificial data for training largely degrades performance on natural data, reflecting important differences in the distribution. This conclusion, that also holds for French, has led us to consider various methods inspired by domain adaptation to better combine the data. We evaluate these methods on the ANNODIS corpus: our best system achieves a 41.7 % accuracy, that is a significant gain of 4.4 % compared to a model using only the natural data. MOTS-CLÉS : structure discursive, relations discursives implicites, apprentissage automatique. KEYWORDS: discourse structure, implicit discourse relations, machine learning.TAL. Volume 55 -n • 1/2014, pages 135 à 165 4. Des détails sur cette méthode sont donnés en section 3.",
        "id": 21403075
      },
      {
        "title": "Multi-level Gated Recurrent Neural Network for Dialog Act Classification",
        "text": "In this paper we focus on the problem of dialog act (DA) labelling. This problem has recently attracted a lot of attention as it is an important sub-part of an automatic dialog model, which is currently in great demand. Traditional methods tend to see this problem as a sequence labelling task and deal with it by applying classifiers with rich features. Most of the current neural network models still omit the sequential information in the conversation. Henceforth, we apply a novel multi-level gated recurrent neural network (GRNN) with non-textual information to predict the DA tag. Our model not only utilizes textual information, but also makes use of non-textual and contextual information. In comparison, our model has shown significant improvement over previous works on the Switchboard Dialog Act (SWDA) data by over 6%.This work is licenced under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/",
        "id": 15118981
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which is one of the first papers to highlight and resolve the distribution shift in RLHF?",
    "positive_ctxs": [
      {
        "title": "PARL: A Unified Framework for Policy Alignment in Reinforcement Learning",
        "text": "We present a novel unified bilevel optimization-based framework, PARL, formulated to address the recently highlighted critical issue of policy alignment in reinforcement learning using utility or preferencebased feedback.We identify a major gap within current algorithmic designs for solving policy alignment due to a lack of precise characterization of the dependence of the alignment objective on the data generated by policy trajectories.This shortfall contributes to the sub-optimal performance observed in contemporary algorithms.Our framework addressed these concerns by explicitly parameterizing the distribution of the upper alignment objective (reward design) by the lower optimal variable (optimal policy for the designed reward).Interestingly, from an optimization perspective, our formulation leads to a new class of stochastic bilevel problems where the stochasticity at the upper objective depends upon the lower-level variable.To demonstrate the efficacy of our formulation in resolving alignment issues in RL, we devised an algorithm named A-PARL to solve PARL problem, establishing sample complexity bounds of order O(1/T ).Our empirical results substantiate that the proposed PARL can address the alignment concerns in RL by showing significant improvements (up to 63% in terms of required samples) for policy alignment in large-scale environments of the Deepmind control suite and Meta world tasks.",
        "id": 260683028
      }
    ],
    "negative_ctxs": [
      {
        "title": "Statistical Generation: Three Methods Compared and Evaluated",
        "text": "Statistical NLG has largely meant n-gram modelling which has the considerable advantages of lending robustness to NLG systems, and of making automatic adaptation to new domains from raw corpora possible. On the downside, n-gram models are expensive to use as selection mechanisms and have a built-in bias towards shorter realisations. This paper looks at treebank-training of generators, an alternative method for building statistical models for NLG from raw corpora, and two different ways of using treebank-trained models during generation. Results show that the treebank-trained generators achieve improvements similar to a 2-gram generator over a baseline of random selection. However, the treebank-trained generators achieve this at a much lower cost than the 2-gram generator, and without its strong preference for shorter realisations.",
        "id": 8493310
      },
      {
        "title": "TEAM UFAL @ CreativeSumm 2022: BART and SamSum based few-shot approach for creative Summarization",
        "text": "This system description paper details TEAM UFAL's approach for the SummScreen, TVMegasite subtask of the CreativeSumm shared task. The subtask deals with creating summaries for dialogues from TV Soap operas. We utilized BART based pre-trained model fine-tuned on SamSum dialouge summarization dataset. Few examples from Au-toMin dataset and the dataset provided by the organizers were also inserted into the data as a few-shot learning objective. The additional data was manually broken into chunks based on different boundaries in summary and the dialogue file. For inference we choose a similar strategy as the top-performing team at AutoMin 2021, where the data is split into chunks, either on [SCENE_CHANGE]  or exceeding a predefined token length, to accommodate the maximum token possible in the pre-trained model for one example. We implemented two different strategies as splits on [SCENE_CHANGE]   did not necessarily mean having less than 1024 tokens in a segment.",
        "id": 252819498
      },
      {
        "title": "Findings of the 2017 DiscoMT Shared Task on Cross-lingual Pronoun Prediction",
        "text": "We describe the design, the setup, and the evaluation results of the DiscoMT 2017 shared task on cross-lingual pronoun prediction. The task asked participants to predict a target-language pronoun given a source-language pronoun in the context of a sentence. We further provided a lemmatized target-language human-authored translation of the source sentence, and automatic word alignments between the source sentence words and the targetlanguage lemmata. The aim of the task was to predict, for each target-language pronoun placeholder, the word that should replace it from a small, closed set of classes, using any type of information that can be extracted from the entire document.We offered four subtasks, each for a different language pair and translation direction: English-to-French, Englishto-German, German-to-English, and Spanish-to-English.Five teams participated in the shared task, making submissions for all language pairs. The evaluation results show that all participating teams outperformed two strong n-gram-based language model-based baseline systems by a sizable margin.",
        "id": 7382485
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper proposed a learning-based data augmentation method for improving compositional generalization of language models?",
    "positive_ctxs": [
      {
        "title": "Learning to Substitute Spans towards Improving Compositional Generalization",
        "text": "Despite the rising prevalence of neural sequence models, recent empirical evidences suggest their deficiency in compositional generalization. One of the current de-facto solutions to this problem is compositional data augmentation, aiming to incur additional compositional inductive bias. Nonetheless, the improvement offered by existing handcrafted augmentation strategies is limited when successful systematic generalization of neural sequence models requires multi-grained compositional bias (i.e., not limited to either lexical or structural biases only) or differentiation of training sequences in an imbalanced difficulty distribution. To address the two challenges, we first propose a novel compositional augmentation strategy dubbed Span Substitution (SpanSub) that enables multi-grained composition of substantial substructures in the whole training set. Over and above that, we introduce the Learning to Substitute Span (L2S2) framework which empowers the learning of span substitution probabilities in SpanSub in an end-to-end manner by maximizing the loss of neural sequence models, so as to outweigh those challenging compositions with elusive concepts and novel surroundings. Our empirical results on three standard compositional generalization benchmarks, including SCAN, COGS and GeoQuery (with an improvement of at most 66.5%, 10.3%, 1.2%, respectively), demonstrate the superiority of SpanSub, L2S2 and their combination. * Corresponding authors N: Emma V: saw N: a cat N: A hedgehog V: met N: Paula V: saw N: a cat N: Paula A hedgehog met Paula .Emma saw a cat . Paula saw a cat .",
        "id": 259076194
      }
    ],
    "negative_ctxs": [
      {
        "title": "Towards a Proper Treatment of Adjuncts in Japanese",
        "text": "In this paper we will discuss interpretation of adverbs in Japanese. We will explore the division of labor between the syntactic requirements, semantic requirements, and discourse-contextual constraints involving adverbial interpretation. It will then be argued that this inter-modular approach utilizing LFG explains various elusive paradigms of the adverbs.",
        "id": 36175639
      },
      {
        "title": "Complexity of finding the BLEU-optimal hypothesis in a confusion network",
        "text": "Confusion networks are a simple representation of multiple speech recognition or translation hypotheses in a machine translation system. A typical operation on a confusion network is to find the path which minimizes or maximizes a certain evaluation metric. In this article, we show that this problem is generally NP-hard for the popular BLEU metric, as well as for smaller variants of BLEU. This also holds for more complex representations like generic word graphs. In addition, we give an efficient polynomial-time algorithm to calculate unigram BLEU on confusion networks, but show that even small generalizations of this data structure render the problem to be NP-hard again.Since finding the optimal solution is thus not always feasible, we introduce an approximating algorithm based on a multi-stack decoder, which finds a (not necessarily optimal) solution for n-gram BLEU in polynomial time.",
        "id": 648535
      },
      {
        "title": "",
        "text": "",
        "id": 29452235
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a study that investigates graph-based modeling of interactivity among various modalities over time within non-aligned multimodal sequential datasets?",
    "positive_ctxs": [
      {
        "title": "MTAG: Modal-Temporal Attention Graph for Unaligned Human Multimodal Language Sequences",
        "text": "Human communication is multimodal in nature; it is through multiple modalities such as language, voice, and facial expressions, that opinions and emotions are expressed. Data in this domain exhibits complex multi-relational and temporal interactions. Learning from this data is a fundamentally challenging research problem. In this paper, we propose Modal-Temporal Attention Graph (MTAG). MTAG is an interpretable graph-based neural model that provides a suitable framework for analyzing multimodal sequential data. We first introduce a procedure to convert unaligned multimodal sequence data into a graph with heterogeneous nodes and edges that captures the rich interactions across modalities and through time. Then, a novel graph fusion operation, called MTAG fusion, along with a dynamic pruning and read-out technique, is designed to efficiently process this modal-temporal graph and capture various interactions. By learning to focus only on the important interactions within the graph, MTAG achieves state-ofthe-art performance on multimodal sentiment analysis and emotion recognition benchmarks, while utilizing significantly fewer model parameters. 1",
        "id": 235097361
      }
    ],
    "negative_ctxs": [
      {
        "title": "The European Thesaurus on International Relations and Area Studies - A Multilingual Resource for Indexing, Retrieval, and Translation",
        "text": "The multilingual European Thesaurus on International Relations and Area Studies (European Thesaurus) is a special subject thesaurus for the field of international affairs. It is intended for use in libraries and documentation centres of academic institutions and international organizations. The European Thesaurus was established in a collaborative project involving a number of leading European research institutes on international politics. It integrates the controlled terminologies of several existing thesauri. The European Thesaurus comprises about 8,200 terms and proper names from the 24 subject areas covered by the thesaurus. Because of its multilinguality, the European Thesaurus can not only be used for indexing, retrieval and terminological reference, but serves also as a translation tool for the languages represented. The establishment of cross-concordances to related thesauri extends the range of application of the European Thesaurus even further. They enable the treatment of semantic heterogeneity within subject gateways. The European Thesaurus is available both in a seven-lingual printversion as well as in an eight-lingual online-version. To reflect the changes in terminology the European Thesaurus is regularly being amended and modified. Further languages are going to be included.",
        "id": 23412271
      },
      {
        "title": "",
        "text": "",
        "id": 232021801
      },
      {
        "title": "Source-side Dependency Tree Reordering Models with Subtree Movements and Constraints",
        "text": "We propose a novel source-side dependency tree reordering model for statistical machine translation, in which subtree movements and constraints are represented as reordering events associated with the widely used lexicalized reordering models. This model allows us to not only efficiently capture the statistical distribution of the subtree-to-subtree transitions in training data, but also utilize it directly at the decoding time to guide the search process. Using subtree movements and constraints as features in a log-linear model, we are able to help the reordering models make better selections. It also allows the subtle importance of monolingual syntactic movements to be learned alongside other reordering features. We show improvements in translation quality in English→Spanish and English→Iraqi translation tasks.",
        "id": 13716132
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research that examines how multihead attention networks discern word interrelations in news content for detecting political perspectives?",
    "positive_ctxs": [
      {
        "title": "Using Social and Linguistic Information to Adapt Pretrained Representations for Political Perspective Identification",
        "text": "Understanding the political perspective shaping the way events are discussed in the media is increasingly important due to the dramatic change in news distribution. With the advance in text classification models, the performance of political perspective detection is also improving rapidly. However, current deep learning based text models often require a large amount of supervised data for training, which can be very expensive to obtain for this task. Meanwhile, models pre-trained on the general source and task (e.g. BERT) lack the ability to focus on bias-related text span. In this paper, we propose a novel framework that pretrains the text model using signals from the rich social and linguistic context that is readily available, including entity mentions, news sharing, and frame indicators. The pre-trained models benefit from tasks related to bias detection and therefore are easier to train with the bias labels. We demonstrate the effectiveness of our proposed framework by experiments on two news bias datasets. The models with pre-training achieve significant improvement in performance and are capable of identifying the text span for bias better.The two articles discuss the presentation of John 1 https://en.wikipedia.org/wiki/2021_ storming_of_the_United_States_Capitol",
        "id": 236478070
      }
    ],
    "negative_ctxs": [
      {
        "title": "Improved Statistical Machine Translation for Resource-Poor Languages Using Related Resource-Rich Languages",
        "text": "We propose a novel language-independent approach for improving statistical machine translation for resource-poor languages by exploiting their similarity to resource-rich ones. More precisely, we improve the translation from a resourcepoor source language X 1 into a resourcerich language Y given a bi-text containing a limited number of parallel sentences for X 1 -Y and a larger bi-text for X 2 -Y for some resource-rich language X 2 that is closely related to X 1 . The evaluation for Indonesian→English (using Malay) and Spanish→English (using Portuguese and pretending Spanish is resource-poor) shows an absolute gain of up to 1.35 and 3.37 Bleu points, respectively, which is an improvement over the rivaling approaches, while using much less additional data.",
        "id": 62762739
      },
      {
        "title": "Natural Language Models for Predicting Programming Comments",
        "text": "Statistical language models have successfully been used to describe and analyze natural language documents. Recent work applying language models to programming languages is focused on the task of predicting code, while mainly ignoring the prediction of programmer comments. In this work, we predict comments from JAVA source files of open source projects, using topic models and n-grams, and we analyze the performance of the models given varying amounts of background data on the project being predicted. We evaluate models on their comment-completion capability in a setting similar to codecompletion tools built into standard code editors, and show that using a comment completion tool can save up to 47% of the comment typing.",
        "id": 6706547
      },
      {
        "title": "Fully Connected Neural Network with Advance Preprocessor to Identify Aggression over Facebook and Twitter",
        "text": "Aggression Identification and Hate Speech detection had become an essential part of cyberharassment and cyberbullying and an automatic aggression identification can lead to the interception of such trolling. Following the same idealization, vista.ue team participated in the workshop which included a shared task on 'Aggression Identification'.A dataset of 15,000 aggression-annotated Facebook Posts and Comments written in Hindi (in both Roman and Devanagari script) and English languages were made available and different classification models were designed. This paper presents a model that outperforms Facebook FastText (Joulin et al., 2016a) and deep learning models over this dataset. Especially, the English developed system, when used to classify Twitter text, outperforms all the shared task submitted systems.Related WorkMachine Learning and Deep Learning approaches are been used in a multitude of problems and the text classification is one of them. Many researchers and the companies are working on text classification to get meaningful and relevant information out of text corpora. Next, research published from 2011 to 2018 over aggression, hate speech, offensive, and abusive language identification is presented.Schmidt and Wiegand (2017) present \"A Survey on Hate Speech Detection using Natural Language Processing\". Mainly, the authors empathize on features for hate speech detection, namely bag of word including unigram, bigram and trigram word representations and also character level n-gram features. This work is licensed under a Creative Commons Attribution 4.0 International License.License details:",
        "id": 53540549
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Is there any paper that explores ways to parameterize neural networks as proximal operators?",
    "positive_ctxs": [
      {
        "title": "What's in a Prior? Learned Proximal Networks for Inverse Problems",
        "text": "Proximal operators are ubiquitous in inverse problems, commonly appearing as part of algorithmic strategies to regularize problems that are otherwise ill-posed.Modern deep learning models have been brought to bear for these tasks too, as in the framework of plug-and-play or deep unrolling, where they loosely resemble proximal operators.Yet, something essential is lost in employing these purely data-driven approaches: there is no guarantee that a general deep network represents the proximal operator of any function, nor is there any characterization of the function for which the network might provide some approximate proximal.This not only makes guaranteeing convergence of iterative schemes challenging but, more fundamentally, complicates the analysis of what has been learned by these networks about their training data.Herein we provide a framework to develop learned proximal networks (LPN), prove that they provide exact proximal operators for a data-driven nonconvex regularizer, and show how a new training strategy, dubbed proximal matching, provably promotes the recovery of the log-prior of the true data distribution.Such LPN provide general, unsupervised, expressive proximal operators that can be used for general inverse problems with convergence guarantees.We illustrate our results in a series of cases of increasing complexity, demonstrating that these models not only result in state-of-the-art performance, but provide a window into the resulting priors learned from data.",
        "id": 264426077
      }
    ],
    "negative_ctxs": [
      {
        "title": "Linking Pictographs to Synsets: Sclera2Cornetto",
        "text": "Social inclusion of people with Intellectual and Developmental Disabilities can be promoted by offering them ways to independently use the internet. People with reading or writing disabilities can use pictographs instead of text. We present a resource in which we have linked a set of 5710 pictographs to lexical-semantic concepts in Cornetto, a Wordnet-like database for Dutch. We show that, by using this resource in a text-to-pictograph translation system, we can greatly improve the coverage comparing with a baseline where words are converted into pictographs only if the word equals the filename.",
        "id": 1209041
      },
      {
        "title": "Jointly Identifying Entities and Extracting Relations in Encyclopedia Text via A Graphical Model Approach *",
        "text": "In this paper, we investigate the problem of entity identification and relation extraction from encyclopedia articles, and we propose a joint discriminative probabilistic model with arbitrary graphical structure to optimize all relevant subtasks simultaneously. This modeling offers a natural formalism for exploiting rich dependencies and interactions between relevant subtasks to capture mutual benefits, as well as a great flexibility to incorporate a large collection of arbitrary, overlapping and nonindependent features. We show the parameter estimation algorithm of this model. Moreover, we propose a new inference method, namely collective iterative classification (CIC), to find the most likely assignments for both entities and relations. We evaluate our model on real-world data from Wikipedia for this task, and compare with current state-of-the-art pipeline and joint models, demonstrating the effectiveness and feasibility of our approach.",
        "id": 6814450
      },
      {
        "title": "FEUP at SemEval-2017 Task 5: Predicting Sentiment Polarity and Intensity with Financial Word Embeddings",
        "text": "This paper presents the approach developed at the Faculty of Engineering of University of Porto, to participate in SemEval 2017, Task 5: Fine-grained Sentiment Analysis on Financial Microblogs and News. The task consisted in predicting a real continuous variable from -1.0 to +1.0 representing the polarity and intensity of sentiment concerning companies/stocks mentioned in short texts. We modeled the task as a regression analysis problem and combined traditional techniques such as pre-processing short texts, bag-of-words representations and lexical-based features with enhanced financial specific bag-ofembeddings. We used an external collection of tweets and news headlines mentioning companies/stocks from S&P 500 to create financial word embeddings which are able to capture domain-specific syntactic and semantic similarities. The resulting approach obtained a cosine similarity score of 0.69 in sub-task 5.1 -Microblogs and 0.68 in sub-task 5.2 -News Headlines.",
        "id": 16311779
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest a thorough comparative analysis or review of the performance of different pretrained transformer architectures, such as BERT, in text ranking applications?",
    "positive_ctxs": [
      {
        "title": "Pretrained Transformers for Text Ranking: BERT and Beyond",
        "text": "The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query for a particular task. Although the most common formulation of text ranking is search, instances of the task can also be found in many text processing applications. This tutorial provides an overview of text ranking with neural network architectures known as transformers, of which BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019) is the best-known example. These models produce high quality results across many domains, tasks, and settings.This tutorial, which is based on the preprint (Lin et al., 2020a) of a forthcoming book to be published by Morgan and & Claypool under the Synthesis Lectures on Human Language Technologies series, provides an overview of existing work as a single point of entry for practitioners who wish to deploy transformers for text ranking in real-world applications and researchers who wish to pursue work in this area. We cover a wide range of techniques, grouped into two categories: transformer models that perform reranking in multi-stage ranking architectures and learned dense representations that perform ranking directly. . 2019. Cross-domain modeling of sentence-level evidence for document retrieval. In Amodei. 2020. Language models are few-shot learners. arXiv:2005.14165.",
        "id": 222310837
      }
    ],
    "negative_ctxs": [
      {
        "title": "PLAN-BASED RELAXED REWARD SHAPING FOR GOAL-DIRECTED TASKS",
        "text": "In high-dimensional state spaces, the usefulness of Reinforcement Learning (RL) is limited by the problem of exploration. This issue has been addressed using potential-based reward shaping (PB-RS) previously. In the present work, we introduce Final-Volume-Preserving Reward Shaping (FV-RS). FV-RS relaxes the strict optimality guarantees of PB-RS to a guarantee of preserved long-term behavior. Being less restrictive, FV-RS allows for reward shaping functions that are even better suited for improving the sample efficiency of RL algorithms. In particular, we consider settings in which the agent has access to an approximate plan. Here, we use examples of simulated robotic manipulation tasks to demonstrate that plan-based FV-RS can indeed significantly improve the sample efficiency of RL over plan-based PB-RS.INTRODUCTIONReinforcement Learning (RL) provides a general framework for autonomous agents to learn complex behavior, adapt to changing environments, and generalize to unseen tasks and environments with little human interference or engineering effort. However, RL in high-dimensional state spaces generally suffers from a difficult exploration problem, making learning prohibitively slow and sample-inefficient for many real-world tasks with sparse rewards.A possible strategy to increase the sample efficiency of RL algorithms is reward shaping (Mataric,  1994; Randløv & Alstrøm, 1998), in particular potential-based reward shaping (PB-RS) (Ng et al.,  1999). Reward shaping provides a dense reward signal to the RL agent, enabling it to converge faster to the optimal policy. In robotics tasks, approximate domain knowledge is often available and can be used by a planning algorithm to generate approximate plans. Here, the resulting plan can be provided to the RL agent using plan-based reward shaping (Grzes & Kudenko, 2008; Brys et al.,  2015). Thus, plan-based reward shaping offers a natural way to combine the efficiency of planning with the flexibility of RL. We analyze the use of plan-based reward shaping for RL. The key novelty is that we theoretically introduce Final-Volume-Preserving Reward Shaping (FV-RS), a superset of PB-RS. Intuitively speaking, FV-RS allows for shaping rewards that convey the information encoded in the shaping reward in a more direct way than PB-RS, since the value of following a policy is not only determined by the shaping reward at the end of the trajectory, but can also depend on all intermediate states.While FV-RS inevitably relaxes the optimality guarantees provided by PB-RS, we show in the experiments that FV-RS can significantly improve sample efficiency beyond PB-RS, e.g. allowing RL agents to learn simulated 10-dimensional continuous robotic manipulation tasks after ca. 300 rollout episodes. We argue that the strict notion of optimality in PB-RS is not necessary in many robotics applications, while on the other hand relaxing PB-RS to FV-RS facilitates speeding up the learning process. Using FV-RS could be a better trade-off between optimality and sample efficiency in many domains. The contributions of this work are:• We introduce FV-RS as a new class of reward shaping for RL methods in general.arXiv:2107.06661v1 [cs.AI] 14 Jul 2021Published as a conference paper at ICLR 2021• We propose to specifically use FV-RS for plan-based reward shaping.• We show that compared to no RS and plan-based PB-RS, plan-based FV-RS significantly increases the sample efficiency in several robotic manipulation tasks.",
        "id": 235614336
      },
      {
        "title": "Understanding the Performance of Statistical MT Systems: A Linear Regression Framework",
        "text": "We present a framework for the analysis of Machine Translation performance. We use multivariate linear models to determine the impact of a wide range of features on translation performance. Our assumption is that variables that most contribute to predict translation performance are the key to understand the differences between good and bad translations. During training, we learn the regression parameters that better predict translation quality using a wide range of input features based on the translation model and the first-best translation hypotheses. We use a linear regression with regularization. Our results indicate that with regularized linear regression, we can achieve higher levels of correlation between our predicted values and the actual values of the quality metrics. Our analysis shows that the performance for in-domain data is largely dependent on the characteristics of the translation model. On the other hand, out-of domain data can benefit from better reordering strategies.",
        "id": 14409460
      },
      {
        "title": "On the Role of Explicit Morphological Feature Representation in Syntactic Dependency Parsing for German",
        "text": "We investigate the question whether an explicit feature representation for morphological features is necessary when parsing German with a fully lexicalized, statistical dependency parser. We use two morphosyntactic phenomena of German to show that while lexicalization does indeed suffice to a large extent when recovering the internal structure of noun phrases, an accurate explicit representation can support the correct selection of its grammatical function.",
        "id": 876880
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Which paper introduced the task of creating extended, coherent dialogues from brief summaries?",
    "positive_ctxs": [
      {
        "title": "Summary Grounded Conversation Generation",
        "text": "Many conversation datasets have been constructed in the recent years using crowdsourcing. However, the data collection process can be time consuming and presents many challenges to ensure data quality. Since language generation has improved immensely in recent years with the advancement of pretrained language models, we investigate how such models can be utilized to generate entire conversations, given only a summary of a conversation as the input. We explore three approaches to generate summary grounded conversations, and evaluate the generated conversations using automatic measures and human judgements. We also show that the accuracy of conversation summarization can be improved by augmenting a conversation summarization dataset with generated conversations.",
        "id": 235359080
      }
    ],
    "negative_ctxs": [
      {
        "title": "User Requirements Analysis for Meeting Information Retrieval Based on Query Elicitation",
        "text": "We present a user requirements study for Question Answering on meeting records that assesses the difficulty of users questions in terms of what type of knowledge is required in order to provide the correct answer. We grounded our work on the empirical analysis of elicited user queries. We found that the majority of elicited queries (around 60%) pertain to argumentative processes and outcomes. Our analysis also suggests that standard keyword-based Information Retrieval can only deal successfully with less than 20% of the queries, and that it must be complemented with other types of metadata and inference.",
        "id": 5332858
      },
      {
        "title": "Multivalent Entailment Graphs for Question Answering",
        "text": "Drawing inferences between open-domain natural language predicates is a necessity for true language understanding. There has been much progress in unsupervised learning of entailment graphs for this purpose. We make three contributions: (1) we reinterpret the Distributional Inclusion Hypothesis to model entailment between predicates of different valencies, like DEFEAT(Biden, Trump) WIN(Biden);(2) we actualize this theory by learning unsupervised Multivalent Entailment Graphs of open-domain predicates; and (3) we demonstrate the capabilities of these graphs on a novel question answering task. We show that directional entailment is more helpful for inference than non-directional similarity on questions of fine-grained semantics. We also show that drawing on evidence across valencies answers more questions than by using only the same valency evidence.",
        "id": 233289793
      },
      {
        "title": "Legal Friction of State Civil Apparatus Neutrality in Indonesia",
        "text": "",
        "id": 159321124
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there any paper that uses data collected from the Dark Web, specifically onion domains, to pretrain a language model?",
    "positive_ctxs": [
      {
        "title": "DarkBERT: A Language Model for the Dark Side of the Internet",
        "text": "Recent research has suggested",
        "id": 258685798
      }
    ],
    "negative_ctxs": [
      {
        "title": "ExCAR: Event Graph Knowledge Enhanced Explainable Causal Reasoning",
        "text": "Prior work infers the causation between events mainly based on the knowledge induced from the annotated causal event pairs. However, additional evidence information intermediate to the cause and effect remains unexploited. By incorporating such information, the logical law behind the causality can be unveiled, and the interpretability and stability of the causal reasoning system can be improved. To facilitate this, we present an Event graph knowledge enhanced explainable CAusal Reasoning framework (ExCAR). ExCAR first acquires additional evidence information from a large-scale causal event graph as logical rules for causal reasoning. To learn the conditional probabilistic of logical rules, we propose the Conditional Markov Neural Logic Network (CMNLN) that combines the representation learning and structure learning of logical rules in an end-to-end differentiable manner. Experimental results demonstrate that ExCAR outperforms previous state-of-the-art methods. Adversarial evaluation shows the improved stability of Ex-CAR over baseline systems. Human evaluation shows that ExCAR can achieve a promising explainable performance.",
        "id": 236460013
      },
      {
        "title": "Specifying Conceptual Models Using Restricted Natural Language",
        "text": "The key activity to design an information system is conceptual modelling which brings out and describes the general knowledge that is required to build a system. In this paper we propose a novel approach to conceptual modelling where the domain experts will be able to specify and construct a model using a restricted form of natural language. A restricted natural language is a subset of a natural language that has well-defined computational properties and therefore can be translated unambiguously into a formal notation. We will argue that a restricted natural language is suitable for writing precise and consistent specifications that lead to executable conceptual models. Using a restricted natural language will allow the domain experts to describe a scenario in the terminology of the application domain without the need to formally encode this scenario. The resulting textual specification can then be automatically translated into the language of the desired conceptual modelling framework.",
        "id": 67865751
      },
      {
        "title": "HIT&QMUL at SemEval-2022 Task 9: Label-Enclosed Generative Question Answering (LEG-QA)",
        "text": "This paper presents the second place system for the R2VQ: competence-based multimodal question answering shared task. The task consisted in building question answering systems that could process procedural recipes involving both text and image, and enriched with semantic and cooking roles. We tackled the task by using a text-to-text generative model based on the transformer architecture, with the aim of generalising across different question types. Our proposed architecture incorporates a novel approach for enriching input texts by incorporating semantic and cooking role labels through what we call Label-Enclosed Generative Question Answering (LEG-QA). Our model achieves a score of 91.3, with a significant improvement over the baseline (65.34) and close to the top-ranked system ((92.5). After describing the submitted system, we analyse the impact of the different components of LEG-QA as well as perform an error analysis.",
        "id": 250390917
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "What paper mitigates language model sampling errors due to the softmax bottleneck?",
    "positive_ctxs": [
      {
        "title": "CLOSING THE CURIOUS CASE OF NEURAL TEXT DEGENERATION",
        "text": "Despite their ubiquity in language generation, it remains unknown why truncation sampling heuristics like nucleus sampling are so effective.We provide a theoretical explanation for the effectiveness of the truncation sampling by proving that truncation methods that discard tokens below some probability threshold (the most common type of truncation) can guarantee that all sampled tokens have nonzero true probability.However, thresholds are a coarse heuristic, and necessarily discard some tokens with nonzero true probability as well.In pursuit of a more precise sampling strategy, we show that we can leverage a known source of model errors, the softmax bottleneck, to prove that certain tokens have nonzero true probability, without relying on a threshold.Based on our findings, we develop an experimental truncation strategy and the present pilot studies demonstrating the promise of this type of algorithm.Our evaluations show that our method outperforms its threshold-based counterparts under automatic and human evaluation metrics for low-entropy (i.e., close to greedy) open-ended text generation.Our theoretical findings and pilot experiments provide both insight into why truncation sampling works, and make progress toward more expressive sampling algorithms that better surface the generative capabilities of large language models.",
        "id": 263608672
      }
    ],
    "negative_ctxs": [
      {
        "title": "REVISITING ADAPTERS WITH ADVERSARIAL TRAINING",
        "text": "While adversarial training is generally used as a defense mechanism, recent works show that it can also act as a regularizer. By co-training a neural network on clean and adversarial inputs, it is possible to improve classification accuracy on the clean, non-adversarial inputs. We demonstrate that, contrary to previous findings, it is not necessary to separate batch statistics when co-training on clean and adversarial inputs, and that it is sufficient to use adapters with few domain-specific parameters for each type of input. We establish that using the classification token of a Vision Transformer (VIT) as an adapter is enough to match the classification performance of dual normalization layers, while using significantly less additional parameters. First, we improve upon the top-1 accuracy of a non-adversarially trained VIT-B16 model by +1.12% on IMAGENET (reaching 83.76% top-1 accuracy). Second, and more importantly, we show that training with adapters enables model soups through linear combinations of the clean and adversarial tokens. These model soups, which we call adversarial model soups, allow us to trade-off between clean and robust accuracy without sacrificing efficiency. Finally, we show that we can easily adapt the resulting models in the face of distribution shifts. Our VIT-B16 obtains top-1 accuracies on IMAGENET variants that are on average +4.00% better than those obtained with Masked Autoencoders. * Work done during an internship at DeepMind",
        "id": 252780742
      },
      {
        "title": "Using Random Indexing to improve Singular Value Decomposition for Latent Semantic Analysis",
        "text": "We present results from using Random Indexing for Latent Semantic Analysis to handle Singular Value Decomposition tractability issues. We compare Latent Semantic Analysis, Random Indexing and Latent Semantic Analysis on Random Indexing reduced matrices. In this study we use a corpus comprising 1003 documents from the MEDLINE-corpus. Our results show that Latent Semantic Analysis on Random Indexing reduced matrices provide better results on Precision and Recall than Random Indexing only. Furthermore, computation time for Singular Value Decomposition on a Random Indexing reduced matrix is almost halved compared to Latent Semantic Analysis.",
        "id": 7483866
      },
      {
        "title": "Published as a conference paper at ICLR 2023 LDMIC: LEARNING-BASED DISTRIBUTED MULTI- VIEW IMAGE CODING",
        "text": "Multi-view image compression plays a critical role in 3D-related applications. Existing methods adopt a predictive coding architecture, which requires joint encoding to compress the corresponding disparity as well as residual information. This demands collaboration among cameras and enforces the epipolar geometric constraint between different views, which makes it challenging to deploy these methods in distributed camera systems with randomly overlapping fields of view. Meanwhile, distributed source coding theory indicates that efficient data compression of correlated sources can be achieved by independent encoding and joint decoding, which motivates us to design a learning-based distributed multi-view image coding (LDMIC) framework. With independent encoders, LDMIC introduces a simple yet effective joint context transfer module based on the crossattention mechanism at the decoder to effectively capture the global inter-view correlations, which is insensitive to the geometric relationships between images. Experimental results show that LDMIC significantly outperforms both traditional and learning-based MIC methods while enjoying fast encoding speed. Code is released at https://github.com/Xinjie-Q/LDMIC.",
        "id": 256194482
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research that examines the effect of example sequencing on machine learning model efficacy in few-shot learning scenarios?",
    "positive_ctxs": [
      {
        "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
        "text": "When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, finetuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are \"fantastic\" and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true fewshot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13% relative improvement for GPTfamily models across eleven different established text classification tasks.",
        "id": 233296494
      }
    ],
    "negative_ctxs": [
      {
        "title": "LEARNING TO RESOLVE BRIDGING REFERENCES",
        "text": "We use machine learning techniques to find the best combination of local focus and lexical distance features for identifying the anchor of mereological bridging references. We find that using first mention, utterance distance, and lexical distance computed using either Google or WordNet results in an accuracy significantly higher than obtained in previous experiments.",
        "id": 14194
      },
      {
        "title": "Modeling Pronunciation Variation for Bi-Lingual Mandarin/Taiwanese Speech Recognition",
        "text": "In this paper, a bi-lingual large vocaburary speech recognition experiment based on the idea of modeling pronunciation variations is described. The two languages under study are Mandarin Chinese and Taiwanese (Min-nan). These two languages are basically mutually unintelligible, and they have many words with the same Chinese characters and the same meanings, although they are pronounced differently. Observing the bi-lingual corpus, we found five types of pronunciation variations for Chinese characters. A one-pass, three-layer recognizer was developed that includes a combination of bi-lingual acoustic models, an integrated pronunciation model, and a tree-structure based searching net. The recognizer's performance was evaluated under three different pronunciation models. The results showed that the character error rate with integrated pronunciation models was better than that with pronunciation models, using either the knowledge-based or the data-driven approach. The relative frequency ratio was also used as a measure to choose the best number of pronunciation variations for each Chinese character. Finally, the best character error rates in Mandarin and Taiwanese testing sets were found to be 16.2% and 15.0%, respectively, when the average number of pronunciations for one Chinese character was 3.9.",
        "id": 5434385
      },
      {
        "title": "Not Just Iconic: Emoji Interpretation is Shaped by Use",
        "text": "Where do the meaning of emoji come from? Though it is often assumed that emoji are fully iconic, with meanings derived from their visual forms, we argue that this is only one component of their meaning. We surveyed users and non-users of the Chinese social media platform WeChat for their interpretations of emoji specific to WeChat. We find that some emoji show significant differences in their interpretations between users and non-users, as well as how familiar a person is with the specific emoji's use. We argue that this reflects a more complex process for building the meaning of emoji on a platform than pure iconicity.",
        "id": 260063134
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which paper contains quantitative results demonstrating taking VQ tokens as inputs is inferior to pixel images for dense recognition tasks?",
    "positive_ctxs": [
      {
        "title": "ADDP: Learning General Representations for Image Recognition and Generation with Alternating Denoising Diffusion Process",
        "text": "Image recognition and generation have long been developed independently of each other. With the recent trend towards general-purpose representation learning, the development of general representations for both recognition and generation tasks is also promoted. However, preliminary attempts mainly focus on generation performance, but are still inferior on recognition tasks. These methods are modeled in the vector-quantized (VQ) space, whereas leading recognition methods use pixels as inputs. Our key insights are twofold: (1) pixels as inputs are crucial for recognition tasks; (2) VQ tokens as reconstruction targets are beneficial for generation tasks. These observations motivate us to propose an Alternating Denoising Diffusion Process (ADDP) that integrates these two spaces within a single representation learning framework. In each denoising step, our method first decodes pixels from previous VQ tokens, then generates new VQ tokens from the decoded pixels. The diffusion process gradually masks out a portion of VQ tokens to construct the training samples. The learned representations can be used to generate diverse high-fidelity images and also demonstrate excellent transfer performance on recognition tasks. Extensive experiments show that our method achieves competitive performance on unconditional generation, ImageNet classification, COCO detection, and ADE20k segmentation. Importantly, our method represents the first successful development of general representations applicable to both generation and dense recognition tasks. Code shall be released.",
        "id": 259108646
      }
    ],
    "negative_ctxs": [
      {
        "title": "MLMLM: Link Prediction with Mean Likelihood Masked Language Model",
        "text": "Knowledge Bases (KBs) are easy to query, verifiable, and interpretable. They however scale with man-hours and high-quality data. Masked Language Models (MLMs), such as BERT, scale with computing power as well as unstructured raw text data. The knowledge contained within those models is however not directly interpretable. We propose to perform link prediction with MLMs to address both the KBs scalability issues and the MLMs interpretability issues. To do that we introduce MLMLM, Mean Likelihood Masked Language Model, an approach comparing the mean likelihood of generating the different entities to perform link prediction in a tractable manner. We obtain State of the Art (SotA) results on the WN18RR dataset and the best nonentity-embedding based results on the FB15k-237 dataset. We also obtain convincing results on link prediction on previously unseen entities, making MLMLM a suitable approach to introducing new entities to a KB.",
        "id": 221703752
      },
      {
        "title": "Are Pre-trained Convolutions Better than Pre-trained Transformers?",
        "text": "In the era of pre-trained language models, Transformers are the de facto choice of model architectures.While recent research has shown promise in entirely convolutional, or CNN, architectures, they have not been explored using the pre-train-fine-tune paradigm. In the context of language models, are convolutional models competitive to Transformers when pre-trained? This paper investigates this research question and presents several interesting findings. Across an extensive set of experiments on 8 datasets/tasks, we find that CNN-based pre-trained models are competitive and outperform their Transformer counterpart in certain scenarios, albeit with caveats. Overall, the findings outlined in this paper suggest that conflating pre-training and architectural advances is misguided and that both advances should be considered independently. We believe our research paves the way for a healthy amount of optimism in alternative architectures.",
        "id": 236460291
      },
      {
        "title": "Riemannian Optimization for Skip-Gram Negative Sampling",
        "text": "Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in \"word2vec\" software, is usually optimized by stochastic gradient descent. However, the optimization of SGNS objective can be viewed as a problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.",
        "id": 27132307
      }
    ]
  }
]