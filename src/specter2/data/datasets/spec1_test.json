[
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there such a factuality evaluation dataset that can be used to evaluate the performance of fact-checking models on summaries generated by latest summarization models?",
    "positive_ctxs": [
      {
        "title": "",
        "text": "The propensity of abstractive summarization models to make factual errors has been studied extensively, including design of metrics to detect factual errors and annotation of errors in current systems' outputs. However, the everevolving nature of summarization systems, metrics, and annotated benchmarks makes factuality evaluation a moving target, and drawing clear comparisons among metrics has become increasingly difficult. In this work, we aggregate factuality error annotations from nine existing datasets and stratify them according to the underlying summarization model. We compare performance of state-of-the-art factuality metrics, including recent ChatGPT-based metrics, on this stratified benchmark and show that their performance varies significantly across different types of summarization models. Critically, our analysis shows that much of the recent improvement in the factuality detection space has been on summaries from older (pre-Transformer) models instead of more relevant recent summarization models. We further perform a finer-grained analysis per error-type and find similar performance variance across error types for different factuality metrics. Our results show that no one metric is superior in all settings or for all error types, and we provide recommendations for best practices given these insights. 1",
        "id": 249062579
      }
    ],
    "negative_ctxs": [
      {
        "title": "An Evaluation Resource for Geographic Information Retrieval",
        "text": "In this paper we present an evaluation resource for geographic information retrieval developed within the Cross Language Evaluation Forum (CLEF). The GeoCLEF track is dedicated to the evaluation of geographic information retrieval systems. The resource encompasses more than 600,000 documents, 75 topics so far, and more than 100,000 relevance judgments for these topics. Geographic information retrieval requires an evaluation resource which represents realistic information needs and which is geographically challenging. Some experimental results and analysis are reported.",
        "id": 1963673
      },
      {
        "title": "UNIFICATION WITH LAZY NON-REDUNDANT COPYING",
        "text": "This paper presents a unification procedure which eliminates the redundant copying of structures by using a lazy incremental copying appr0a~:h to achieve structure sharing. Copying of structures accounts for a considerable amount of the total processing time. Several methods have been proposed to minimize the amount of necessary copying. Lazy Incremental Copying (LIC) is presented as a new solution to the copying problem. It synthesizes ideas of lazy copying with the notion of chronological dereferencing for achieving a high amount of structure sharing.",
        "id": 14046171
      },
      {
        "title": "",
        "text": "",
        "id": 61178558
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "What research exists on the impact of scaling on prompt tuning efficiency in pre-trained language models?",
    "positive_ctxs": [
      {
        "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
        "text": "In this work, we explore \"prompt tuning,\" a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient \"prompt ensembling.\" We release code and model checkpoints to reproduce our experiments. 1ReferencesRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,  Danilo Giampiccolo, Bernardo Magnini, and Idan  Szpektor. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognising textual entailment, volume 6, pages 6-4. Venice. . 2019a. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
        "id": 233296808
      }
    ],
    "negative_ctxs": [
      {
        "title": "Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers",
        "text": "ChatGPT is a large language model developed by OpenAI 1 . Despite its impressive performance across various tasks, no prior work has investigated its capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of ChatGPT on various benchmark biomedical tasks, such as relation extraction, document classification, question answering, and summarization. To the best of our knowledge, this is the first work that conducts an extensive evaluation of ChatGPT in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zeroshot ChatGPT even outperforms the state-ofthe-art fine-tuned generative transformer models, such as BioGPT and BioBART. This suggests that ChatGPT's pre-training on large text corpora makes it quite specialized even in the biomedical domain. Our findings demonstrate that ChatGPT has the potential to be a valuable tool for various tasks in the biomedical domain that lack large annotated data. . 2019. BART:Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461.",
        "id": 259096053
      },
      {
        "title": "Vowel sequences in Old Japanese: from a corpus-based approach",
        "text": "This paper investigates vowel sequences in OldJapanese from a corpus-based approach by using data from the Oxford-NINJAL Corpus of Old Japanese (ONCOJ). Three conditions are taken into consideration to analyze the corpus data, and they are (a) within a phonological word, (b) through morphological process, and (c) by grammatical element. The results have shown that vowel sequences in Old Japanese are not random combinations of vowels, but they are phonologically constrained. In the first and second conditions, the vowel sequences tend to show vowel[-front] + vowel [-low]. In the third condition, the first vowel of the vowel sequence is -i-. In addition to frequency, this paper also discusses the types of words and compares types with frequencies.",
        "id": 258463943
      },
      {
        "title": "Capturing Pragmatic Knowledge in Article Usage Prediction using LSTMs",
        "text": "We examine the potential of recurrent neural networks for handling pragmatic inferences involving complex contextual cues for the task of article usage prediction. We train and compare several variants of Long Short-Term Memory (LSTM) networks with an attention mechanism. Our model outperforms a previous state-of-the-art system, achieving up to 96.63% accuracy on the WSJ/PTB corpus. In addition, we perform a series of analyses to understand the impact of various model choices. We find that the gain in performance can be attributed to the ability of LSTMs to pick up on contextual cues, both local and further away in distance, and that the model is able to solve cases involving reasoning about coreference and synonymy. We also show how the attention mechanism contributes to the interpretability of the model's effectiveness.",
        "id": 8341728
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Are there any studies on incorporating external commonsense knowledge into conversational models to enhance emotional support?",
    "positive_ctxs": [
      {
        "title": "MISC: A MIxed Strategy-Aware Model Integrating COMET for Emotional Support Conversation",
        "text": "Applying existing methods to emotional support conversation-which provides valuable assistance to people who are in need-has two major limitations: (a) they generally employ a conversation-level emotion label, which is too coarse-grained to capture user's instant mental state; (b) most of them focus on expressing empathy in the response(s) rather than gradually reducing user's distress. To address the problems, we propose a novel model MISC, which firstly infers the user's fine-grained emotional status, and then responds skillfully using a mixture of strategy. Experimental results on the benchmark dataset demonstrate the effectiveness of our method and reveal the benefits of fine-grained emotion understanding as well as mixed-up strategy modeling. Our code and data could be found in https: //github.com/morecry/MISC.",
        "id": 247748640
      }
    ],
    "negative_ctxs": [
      {
        "title": "GENERALIZED TENSOR MODELS FOR RECURRENT NEURAL NETWORKS",
        "text": "Recurrent Neural Networks (RNNs) are very successful at solving challenging problems with sequential data. However, this observed efficiency is not yet entirely explained by theory. It is known that a certain class of multiplicative RNNs enjoys the property of depth efficiency -a shallow network of exponentially large width is necessary to realize the same score function as computed by such an RNN. Such networks, however, are not very often applied to real life tasks. In this work, we attempt to reduce the gap between theory and practice by extending the theoretical analysis to RNNs which employ various nonlinearities, such as Rectified Linear Unit (ReLU), and show that they also benefit from properties of universality and depth efficiency. Our theoretical results are verified by a series of extensive computational experiments.",
        "id": 59413817
      },
      {
        "title": "LIFELONG LEARNING WITH DYNAMICALLY EXPAND- ABLE NETWORKS",
        "text": "We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets in lifelong learning scenarios on multiple public datasets, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch model with substantially fewer number of parameters.",
        "id": 3693512
      },
      {
        "title": "",
        "text": "",
        "id": 233364425
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper first proposed shared adapter module across layers?",
    "positive_ctxs": [
      {
        "title": "One Network, Many Masks: Towards More Parameter-Efficient Transfer Learning",
        "text": "Fine-tuning pre-trained language models for multiple tasks tends to be expensive in terms of storage. To mitigate this, parameter-efficient transfer learning (PETL) methods have been proposed to address this issue, but they still require a significant number of parameters and storage when being applied to broader ranges of tasks. To achieve even greater storage reduction, we propose PROPETL, a novel method that enables efficient sharing of a single PETL module which we call prototype network (e.g., adapter, LoRA, and prefix-tuning) across layers and tasks. We then learn binary masks to select different sub-networks from the shared prototype network and apply them as PETL modules into different layers. We find that the binary masks can determine crucial information from the network, which is often ignored in previous studies. Our work can also be seen as a type of pruning method, where we find that overparameterization also exists in the seemingly small PETL modules. We evaluate PROPETL on various downstream tasks and show that it can outperform other PETL methods with approximately 10% of the parameter storage required by the latter.",
        "id": 258960642
      }
    ],
    "negative_ctxs": [
      {
        "title": "Building a SentiWordNet For Odia",
        "text": "As a discipline of Natural",
        "id": 2894283
      },
      {
        "title": "Chinese Syntactic Parsing Evaluation",
        "text": "The paper introduced the task designing ideas, data preparation methods, evaluation metrics and results of the second Chinese syntactic parsing evaluation (CIPS-Bakeoff-ParsEval-2010) jointed with SIGHAN Bakeoff tasks.",
        "id": 2706681
      },
      {
        "title": "Resolving Spatial References using Crowdsourced Geographical Data",
        "text": "We present a study in which we seek to interpret spatial references that are part of in-situ route descriptions. Our aim is to resolve these references to actual entities and places in the city using a crowdsourced geographic database (Open-StreetMap). We discuss the problems related to this task, and present a possible automatic reference resolution method that can find the correct referent in 68% of the cases using features that are easily computable from the map.",
        "id": 4646592
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Are there studies examining how well question answering systems perform on queries that cannot be directly recalled from their training data?",
    "positive_ctxs": [
      {
        "title": "Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets",
        "text": "Ideally Open-Domain Question Answering models should exhibit a number of competencies, ranging from simply memorizing questions seen at training time, to answering novel question formulations with answers seen during training, to generalizing to completely novel questions with novel answers. However, single aggregated test set scores do not show the full picture of what capabilities models truly have. In this work, we perform a detailed study of the test sets of three popular open-domain benchmark datasets with respect to these competencies. We find that 60-70% of test-time answers are also present somewhere in the training sets. We also find that 30% of test-set questions have a near-duplicate paraphrase in their corresponding training sets. Using these findings, we evaluate a variety of popular open-domain models to obtain greater insight into what extent they can actually generalize, and what drives their overall performance. We find that all models perform dramatically worse on questions that cannot be memorized from training sets, with a mean absolute performance difference of 63% between repeated and non-repeated data. Finally we show that simple nearest-neighbor models outperform a BART closed-book QA model, further highlighting the role that training set memorization plays in these benchmarks.",
        "id": 221005781
      }
    ],
    "negative_ctxs": [
      {
        "title": "A Learning-based Sampling Approach to Extractive Summarization",
        "text": "In this paper we present a novel resampling model for extractive meeting summarization. With resampling based on the output of a baseline classifier, our method outperforms previous research in the field. Further, we compare an existing resampling technique with our model. We report on an extensive series of experiments on a large meeting corpus which leads to classification improvement in weighted precision and f-score.",
        "id": 14263284
      },
      {
        "title": "Cutting Recursive Autoencoder Trees",
        "text": "Deep Learning models enjoy considerable success in Natural Language Processing. While deep architectures produce useful representations that lead to improvements in various tasks, they are often difficult to interpret. This makes the analysis of learned structures particularly difficult. In this paper, we rely on empirical tests to see whether a particular structure makes sense. We present an analysis of the Semi-Supervised Recursive Autoencoder, a well-known model that produces structural representations of text. We show that for certain tasks, the structure of the autoencoder can be significantly reduced without loss of classification accuracy and we evaluate the produced structures using human judgment.",
        "id": 7788178
      },
      {
        "title": "Ensembles of Classifiers for Cleaning Web Parallel Corpora and Translation Memories",
        "text": "The last years witnessed an increasing interest in the automatic methods for spotting false translation units in translation memories. This problem presents a great interest to industry as there are many translation memories that contain errors. A closely related line of research deals with identifying sentences that do not align in the parallel corpora mined from the web. The task of spotting false translations is modeled as a binary classification problem. It is known that in certain conditions the ensembles of classifiers improve over the performance of the individual members. In this paper we benchmark the most popular ensemble of classifiers: Majority Voting, Bagging, Stacking and Ada Boost at the task of spotting false translation units for translation memories and parallel web corpora. We want to know if for this specific problem any ensemble technique improves the performance of the individual classifiers and if there is a difference between the data in translation memories and parallel web corpora with respect to this task.",
        "id": 150847
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which paper examined the scalability of instruction-tuning with respect to Mixture of Expert models?",
    "positive_ctxs": [
      {
        "title": "Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models",
        "text": "Sparse Mixture-of-Experts (MoE) is a neural architecture design that can be utilized to add learnable parameters to Large Language Models (LLMs) without increasing inference cost. Instruction tuning is a technique for training LLMs to follow instructions. We advocate combining these two approaches, as we find that MoE models benefit more from instruction tuning than dense models. In particular, we conduct empirical studies across three experimental setups: (i) Direct finetuning on individual downstream tasks devoid of instruction tuning; (ii) Instruction tuning followed by in-context few-shot or zero-shot generalization on downstream tasks; and (iii) Instruction tuning supplemented by further finetuning on individual downstream tasks. In the first scenario, MoE models overall underperform dense models of identical computational capacity. This narrative, however, dramatically changes with the introduction of instruction tuning (second and third scenario), used independently or in conjunction with task-specific finetuning. Our most powerful model, FLAN-MOE 32B , surpasses the performance of FLAN-PALM 62B on four benchmark tasks, while using only a third of the FLOPs. The advancements embodied by FLAN-MOE inspire a reevaluation of the design principles of large-scale, high-performance language models in the framework of task-agnostic learning. * Work done at Google Preprint. Under review. arXiv:2305.14705v2 [cs.CL] 5 Jul 2023 2.2 Instruction Fine-tuning RecipeWe fine-tune FLAN-MOE using the prefix language model objective on the FLAN collective dataset[4,28]. Each FLAN-MOE will inherit the auxiliary loss setting during pre-training. All the model parameters will be updated. We adapt the sequence length of each FLAN-MOE to 2, 048 for input and 512 for output based on the relative position embedding. The dropout rate is 0.05 and the expert dropout rate is 0.2. The learning rate is 1e −4 . The optimizer setting follows [4].ExperimentWe study FLAN-MOE in the context of instruction-tuning. We first perform a controlled comparison of FLAN-MOE to an equivalent \"standard\" dense encoder-decoder Transformer (T5), across a range of model sizes in Section 3.2. We subsequently demonstrate in Section 3.3 that scaling up our model, referred to as FLAN-MOE, can attain remarkable performance levels. Our most extensive model, FLAN-ST 32B , surpasses the performance of FLAN-PALM 62B while utilizing less than 30% of FLOPs per token. We further ablate the various design decisions in the next Section. 3.1 Settings Traning Data. By default, all models are trained on the 1,836 finetuning tasks by combining four mixtures from prior work: Muffin, T0-SF, NIV2, and CoT, as in [4]. Specifically, Muffin comprises 80 tasks from [52] and 26 dialog/program synthesis tasks; T0-SF comprises 193 tasks from [44]; NIV2 comprises 1554 tasks from [51]; CoT comprises 9 reasoning tasks.Evaluations. We conduct both zero-shot and few-shot evaluations on held-out tasks as in [4] which were not included as part of the finetuning data. We use MMLU [16] that includes exam questions from 57 tasks such as mathematics, history, law, and medicine; BBH includes 23 challenging",
        "id": 259342096
      }
    ],
    "negative_ctxs": [
      {
        "title": "Explorations in the Speakers' Interaction Experience and Self-assessments",
        "text": "The paper focuses on the interlocutors' self-evaluation in Finnish and Estonian first encounter dialogues. It studies affective and emotive impressions of the participants after they have met the partner for the first time, and presents comparison of the evaluation along the gender, age and education parameters. The results bring forward some statistically significant differences between the two groups, and point to different, culturally determined evaluation scales. The paper discusses the impact of the findings on the complex issues related to the evaluation of automatic interactive systems, and carries over to such applications as intelligent training and tutoring systems, and interactions with robots, encouraging further studies on the interlocutors' engagement in interaction and their evaluation of the success of the interaction.Kõnelejate suhtluskogemuse ja enesehinnangute uuringudKOKKUVÕTEArtikkel keskendub vestluskaaslaste enesehinnangutele esmakohtumisel peetud dialoogides soome ja eesti keeles. Uuritakse osalejate afektiivseid ja emotiivseid muljeid pärast seda, kui nad on kohtunud partneriga esmakordselt, ja esitatakse hinnangute võrdlus soo, vanuse ja hariduse parameetrite alusel. Tulemused toovad esile statistiliselt olulised erinevused kahe rühma vahel ja viitavad erinevatele, kultuuriliselt determineeritud hinnanguskaaladele. Artikkel analüüsib nende tulemuste mõju keerulistele probleemidele, mis on seotud automaatsete interaktiivsete süsteemide evalveerimisega, ja arendab edasi selliseid rakendusi nagu intelligentsed treenimis-ja õpetamissüsteemid ning suhtlus robotitega, pannes aluse edasistele uuringutele suhtlejate vestlusesse lülitumise ja vestluse edukuse hindamise kohta.VÕTMESÕNAD : dialoog, vestlusesse lülitumine, enesehinnang, kultuuridevaheline evalveerimine",
        "id": 16628419
      },
      {
        "title": "Discovering the Unknown Knowns: Turning Implicit Knowledge in the Dataset into Explicit Training Examples for Visual Question Answering",
        "text": "Visual question answering (VQA) is challenging not only because the model has to handle multi-modal information, but also because it is just so hard to collect sufficient training examples -there are too many questions one can ask about an image. As a result, a VQA model trained solely on human-annotated examples could easily over-fit specific question styles or image contents that are being asked, leaving the model largely ignorant about the sheer diversity of questions. Existing methods address this issue primarily by introducing an auxiliary task such as visual grounding, cycle consistency, or debiasing. In this paper, we take a drastically different approach. We found that many of the \"unknowns\" to the learned VQA model are indeed \"known\" in the dataset implicitly. For instance, questions asking about the same object in different images are likely paraphrases; the number of detected or annotated objects in an image already provides the answer to the \"how many\" question, even if the question has not been annotated for that image. Building upon these insights, we present a simple data augmentation pipeline SIMPLEAUG to turn this \"known\" knowledge into training examples for VQA. We show that these augmented examples can notably improve the learned VQA models' performance, not only on the VQA-CP dataset with language prior shifts but also on the VQA v2 dataset without such shifts. Our method further opens up the door to leverage weakly-labeled or unlabeled images in a principled way to enhance VQA models. Our code and data are publicly available at https://github.com/ heendung/simpleAUG.",
        "id": 237490881
      },
      {
        "title": "the The Transfer Phase of Mu Machine",
        "text": "Sys t (.~mSurface ~rker Coluan=B apanese ] Deep English rfaee Harker| Case Relation Surface Harker J Column-(: Japanese ] Translation Part~of-Hodifler [ Equivalent",
        "id": 2008605
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Are there sequential learning guarantees for configuring a linear system solver under a distributional assumption on the systems' target vectors?",
    "positive_ctxs": [
      {
        "title": "LEARNING TO RELAX: SETTING SOLVER PARAMETERS ACROSS A SEQUENCE OF LINEAR SYSTEM INSTANCES",
        "text": "Solving a linear system Ax = b is a fundamental scientific computing primitive for which numerous solvers and preconditioners have been developed.These come with parameters whose optimal values depend on the system being solved and are often impossible or too expensive to identify; thus in practice sub-optimal heuristics are used.We consider the common setting in which many related linear systems need to be solved, e.g. during a single numerical simulation.In this scenario, can we sequentially choose parameters that attain a near-optimal overall number of iterations, without extra matrix computations?We answer in the affirmative for Successive Over-Relaxation (SOR), a standard solver whose parameter ω has a strong impact on its runtime.For this method, we prove that a bandit online learning algorithm-using only the number of iterations as feedback-can select parameters for a sequence of instances such that the overall cost approaches that of the best fixed ω as the sequence length increases.Furthermore, when given additional structural information, we show that a contextual bandit method asymptotically achieves the performance of the instance-optimal policy, which selects the best ω for each instance.Our work provides the first learning-theoretic treatment of high-precision linear system solvers and the first end-to-end guarantees for data-driven scientific computing, demonstrating theoretically the potential to speed up numerical methods using well-understood learning algorithms.",
        "id": 263609239
      }
    ],
    "negative_ctxs": [
      {
        "title": "Employing Topic Models for Pattern-based Semantic Class Discovery",
        "text": "A semantic class is a collection of items (words or phrases) which have semantically peer or sibling relationship. This paper studies the employment of topic models to automatically construct semantic classes, taking as the source data a collection of raw semantic classes (RASCs), which were extracted by applying predefined patterns to web pages. The primary requirement (and challenge) here is dealing with multi-membership: An item may belong to multiple semantic classes; and we need to discover as many as possible the different semantic classes the item belongs to. To adopt topic models, we treat RASCs as \"documents\", items as \"words\", and the final semantic classes as \"topics\". Appropriate preprocessing and postprocessing are performed to improve results quality, to reduce computation cost, and to tackle the fixed-k constraint of a typical topic model. Experiments conducted on 40 million web pages show that our approach could yield better results than alternative approaches. Countries, and U.S. states. In the second step, each item is assigned a label of \"Good\", \"Fair\", or \"Bad\" with respect to each SSC. For example, \"silver\" is labeled \"Good\" with respect to \"colors\" and \"chemical elements\". We adopt metric MnDCG (Section 4.2) as our evaluation metric.Approaches for comparison: We compare our approach with the alternative approaches discussed in Section 3.4.2.LDA: Our approach with LDA as the topic model. The implementation of LDA is based on Blei's code of variational EM for LDA 5 . pLSI: Our approach with pLSI as the topic model. The implementation of pLSI is based onSchein, et al. (2002).KMedoids-RASC:The RASC clustering approach illustrated in Section 3.4.2, with the K-Medoids clustering algorithm utilized.DBSCAN-RASC:The RASC clustering approach with DBSCAN utilized.KMedoids-Item:The item clustering approach with the K-Medoids utilized.DBSCAN-Item:The item clustering approach with the DBSCAN clustering algorithm utilized.",
        "id": 9470648
      },
      {
        "title": "Genre Separation Network with Adversarial Training for Cross-genre Relation Extraction",
        "text": "Relation Extraction suffers from dramatical performance decrease when training a model on one genre and directly applying it to a new genre, due to the distinct feature distributions. Previous studies address this problem by discovering a shared space across genres using manually crafted features, which requires great human effort. To effectively automate this process, we design a genre-separation network, which applies two encoders, one genreindependent and one genre-shared, to explicitly extract genre-specific and genre-agnostic features. Then we train a relation classifier using the genre-agnostic features on the source genre and directly apply to the target genre. Experiment results on three distinct genres of the ACE dataset show that our approach achieves up to 6.1% absolute F1-score gain compared to previous methods. By incorporating a set of external linguistic features, our approach outperforms the state-of-the-art by 1.7% absolute F1 gain. We make all programs of our model publicly available for research purpose 1 .",
        "id": 53080778
      },
      {
        "title": "FINITE-STATE APPROXIMATION OF PHRASE STRUCTURE GRAMMARS",
        "text": "Phrase-structure grammars are an effective representation for important syntactic and semantic aspects of natural languages, but are computationally too demanding for use as language models in real-time speech recognition. An algorithm is described that computes finite-state approximations for context-free grammars and equivalent augmented phrase-structure grammar formalisms. The approximation is exact for certain contextfree grammars generating regular languages, including all left-linear and right-linear context-free grammars. The algorithm has been used to construct finite-state language models for limiteddomain speech recognition tasks.",
        "id": 5094703
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which vision-language model can demonstrate that visual grounding could facilitate efficient language acquisition? (OctoBERT)",
    "positive_ctxs": [
      {
        "title": "World-to-Words: Grounded Open Vocabulary Acquisition through Fast Mapping in Vision-Language Models",
        "text": "The ability to connect language units to their referents in the physical world, referred to as grounding, is crucial to learning and understanding grounded meanings of words. While humans demonstrate fast mapping in new word learning, it remains unclear whether modern vision-language models can truly represent language with their grounded meanings, and how grounding may further bootstrap new word learning. To this end, we introduce Grounded Open Vocabulary Acquisition (GOVA) to examine grounding and bootstrapping in openworld language learning. As an initial attempt, we propose World-to-Words (W2W), a novel visually-grounded language model by pre-training on image-text pairs highlighting grounding as an objective. Through extensive experiments and analysis, we demonstrate that W2W is a more coherent and fast grounded word learner, and that the grounding ability acquired during pre-training helps the model to learn unseen words more rapidly and robustly.",
        "id": 259165546
      }
    ],
    "negative_ctxs": [
      {
        "title": "SemEval-2007 Task 09: Multilevel Semantic Annotation of Catalan and Spanish",
        "text": "In this paper we describe SemEval-2007 task number 9 (Multilevel Semantic Annotation of Catalan and Spanish). In this task, we aim at evaluating and comparing automatic systems for the annotation of several semantic linguistic levels for Catalan and Spanish. Three semantic levels are considered: noun sense disambiguation, named entity recognition, and semantic role labeling.",
        "id": 219307725
      },
      {
        "title": "Towards a standard evaluation method for grammatical error detection and correction",
        "text": "We present a novel evaluation method for grammatical error correction that addresses problems with previous approaches and scores systems in terms of improvement on the original text. Our method evaluates corrections at the token level using a globally optimal alignment between the source, a system hypothesis, and a reference. Unlike the M 2 Scorer, our method provides scores for both detection and correction and is sensitive to different types of edit operations.",
        "id": 2723528
      },
      {
        "title": "New Developments in Ontological Semantics",
        "text": "In this paper we discuss ongoing activity within the approach to natural language processing known as ontological semantics, as defined in Nirenburg and Raskin (forthcoming). After a brief discussion of the principal tenets on which this approach is built, and a revision of extant implementations that have led toward its present form, we concentrate on some specific aspects that are key to the development of this approach, such as the acquisition of the semantics of lexical items and, intimately connected with this, the ontology, the central resource in this approach. Although we review the fundamentals of the approach, the focus is on practical aspects of implementation, such as the automation of static knowledge acquisition and the acquisition of scripts to enrich the ontology further.",
        "id": 16658942
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there any paper that utilizes graph structure to model conversation history?",
    "positive_ctxs": [
      {
        "title": "History Semantic Graph Enhanced Conversational KBQA with Temporal Information Modeling",
        "text": "Context information modeling is an important task in conversational KBQA. However, existing methods usually assume the independence of utterances and model them in isolation. In this paper, we propose a History Semantic Graph Enhanced KBQA model (HSGE) that is able to effectively model long-range semantic dependencies in conversation history while maintaining low computational cost. The framework incorporates a context-aware encoder, which employs a dynamic memory decay mechanism and models context at different levels of granularity. We evaluate HSGE on a widely used benchmark dataset for complex sequential question answering. Experimental results demonstrate that it outperforms existing baselines averaged on all question types.",
        "id": 259137386
      }
    ],
    "negative_ctxs": [
      {
        "title": "Exploiting Document Structures and Cluster Consistencies for Event Coreference Resolution",
        "text": "We study the problem of event coreference resolution (ECR) that seeks to group coreferent event mentions into the same clusters. Deep learning methods have recently been applied for this task to deliver state-of-the-art performance. However, existing deep learning models for ECR are limited in that they cannot exploit important interactions between relevant objects for ECR, e.g., context words and entity mentions, to support the encoding of document-level context. In addition, consistency constraints between golden and predicted clusters of event mentions have not been considered to improve representation learning in prior deep learning models for ECR. This work addresses such limitations by introducing a novel deep learning model for ECR. At the core of our model are document structures to explicitly capture relevant objects for ECR. Our document structures introduce diverse knowledge sources (discourse, syntax, semantics) to compute edges/interactions between structure nodes for document-level representation learning. We also present novel regularization techniques based on consistencies of golden and predicted clusters for event mentions in documents. Extensive experiments show that our model achieve state-of-the-art performance on two benchmark datasets.",
        "id": 236460010
      },
      {
        "title": "A UIMA Database Interface for Managing NLP-related Text Annotations",
        "text": "NLP and automatic text analysis necessarily involve the annotation of natural language texts. The Apache Unstructured Information Management applications (UIMA) framework is used in several projects, tools and resources, and has become a de facto standard in this area. Despite the multiple use of UIMA as a document-based schema, it does not provide native database support. In order to facilitate distributed storage and enable UIMA-based projects to perform targeted queries, we have developed the UIMA Database Interface (UIMA DI). UIMA DI sets up an environment for a generic use of UIMA documents in database systems. In addition, the integration of UIMA DI into rights and resource management tools enables user and group-specific access to UIMA documents and provides data protection. Finally, UIMA documents can be made accessible for third party programs. UIMA DI, which we evaluate in relation to file system-based storage, is available under the GPLv3 license via GitHub.",
        "id": 21700667
      },
      {
        "title": "TSDPMM: Incorporating Prior Topic Knowledge into Dirichlet Process Mixture Models for Text Clustering",
        "text": "Dirichlet process mixture model (DPM-M) has great potential for detecting the underlying structure of data. Extensive studies have applied it for text clustering in terms of topics. However, due to the unsupervised nature, the topic clusters are always less satisfactory. Considering that people often have some prior knowledge about which potential topics should exist in given data, we aim to incorporate such knowledge into the DPMM to improve text clustering. We propose a novel model TSDPMM based on a new seeded Pólya urn scheme. Experimental results on document clustering across three datasets demonstrate our proposed TSDPMM significantly outperforms stateof-the-art DPMM model and can be applied in a lifelong learning framework.",
        "id": 14659218
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that investigates aspect-based sentiment analysis across different languages, incorporating methods such as code-switching of aspect terms?",
    "positive_ctxs": [
      {
        "title": "Cross-lingual Aspect-based Sentiment Analysis with Aspect Term Code-Switching *",
        "text": "Many efforts have been made in solving the Aspect-based sentiment analysis (ABSA) task. While most existing studies focus on English texts, handling ABSA in resource-poor languages remains a challenging problem. In this paper, we consider the unsupervised crosslingual transfer for the ABSA task, where only labeled data in the source language is available and we aim at transferring its knowledge to the target language having no labeled data. To this end, we propose an alignment-free label projection method to obtain high-quality pseudolabeled data of the target language with the help of the translation system, which could preserve more accurate task-specific knowledge in the target language. For better utilizing the source and translated data, as well as enhancing the cross-lingual alignment, we design an aspect code-switching mechanism to augment the training data with code-switched bilingual sentences. To further investigate the importance of language-specific knowledge in solving the ABSA problem, we distill the above model on the unlabeled target language data which improves the performance to the same level of the supervised method.",
        "id": 243865296
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 208264552
      },
      {
        "title": "A GRADIENT FLOW FRAMEWORK FOR ANALYZING NETWORK PRUNING",
        "text": "Recent network pruning methods focus on pruning models early-on in training. To estimate the impact of removing a parameter, these methods use importance measures that were originally designed to prune trained models. Despite lacking justification for their use early-on in training, such measures result in surprisingly low accuracy loss. To better explain this behavior, we develop a general framework that uses gradient flow to unify state-of-the-art importance measures through the norm of model parameters. We use this framework to determine the relationship between pruning measures and evolution of model parameters, establishing several results related to pruning models early-on in training: (i) magnitude-based pruning removes parameters that contribute least to reduction in loss, resulting in models that converge faster than magnitude-agnostic methods; (ii) loss-preservation based pruning preserves first-order model evolution dynamics and is therefore appropriate for pruning minimally trained models; and (iii) gradient-norm based pruning affects second-order model evolution dynamics, such that increasing gradient norm via pruning can produce poorly performing models. We validate our claims on several VGG-13, MobileNet-V1, and ResNet-56 models trained on CIFAR-10/CIFAR-100. 1 gradient flow refers to gradient descent with infinitesimal learning rate; see Equation 6 for a short primer.Published as a conference paper at ICLR 2021 what properties make a parameter dispensable according to a particular importance measure. Our findings follow. (i) Magnitude-based pruning measures remove parameters that contribute least to reduction in loss. This enables magnitude-based pruned models to achieve faster convergence than magnitude-agnostic measures. (ii) Loss-preservation based measures remove parameters with the least tendency to change, thus preserving first-order model evolution dynamics. This shows that loss-preservation is appropriate for pruning models early-on in training as well. (iii) Gradient-norm based pruning is linearly related to second-order model evolution dynamics. Increasing gradient norm via pruning for even slightly trained models can permanently damage earlier layers, producing poorly performing architectures. This behavior is a result of aggressively pruning filters that maximally increase model loss. We validate our claims on several VGG-13, MobileNet-V1, and ResNet-56 models trained on CIFAR-10 and CIFAR-100.",
        "id": 221879071
      },
      {
        "title": "Some Notes on p(e)re-Reduplication in Bulgarian and Ukrainian: A Corpus-based Study",
        "text": "We present a comparative study of p(e)re-",
        "id": 252995552
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which paper first tried to fine-tune LLMs with chain-of-thoughts and program-of-thoughts for math reasoning?",
    "positive_ctxs": [
      {
        "title": "MAMMOTH: BUILDING MATH GENERALIST MODELS THROUGH HYBRID INSTRUCTION TUNING",
        "text": "We introduce MAmmoTH, a series of open-source large language models (LLMs) specifically tailored for general math problem-solving.The MAmmoTH models are trained on MathInstruct, our meticulously curated instruction tuning dataset.MathInstruct is compiled from 13 math datasets with intermediate rationales, six of which have rationales newly curated by us.It presents a unique hybrid of chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also ensures extensive coverage of diverse fields in math.The hybrid of CoT and PoT not only unleashes the potential of tool use but also allows different thought processes for different math problems.As a result, the MAmmoTH series substantially outperform existing open-source models on nine mathematical reasoning datasets across all scales with an average accuracy gain between 16% and 32%.Remarkably, our MAmmoTH-7B model reaches 33% on MATH (a competition-level dataset), which exceeds the best open-source 7B model (WizardMath) by 23%, and the MAmmoTH-34B model achieves 44% accuracy on MATH, even surpassing GPT-4's CoT result.Our work underscores the importance of diverse problem coverage and the use of hybrid rationales in developing superior math generalist models.",
        "id": 261696697
      }
    ],
    "negative_ctxs": [
      {
        "title": "Polyglot Contextual Representations Improve Crosslingual Transfer",
        "text": "We introduce Rosita, a method to produce multilingual contextual word representations by training a single language model on text from multiple languages.Our method combines the advantages of contextual word representations with those of multilingual representation learning. We produce language models from dissimilar language pairs (English/Arabic and English/Chinese) and use them in dependency parsing, semantic role labeling, and named entity recognition, with comparisons to monolingual and noncontextual variants. Our results provide further evidence for the benefits of polyglot learning, in which representations are shared across multiple languages.",
        "id": 67855733
      },
      {
        "title": "Knowledge Graph and Corpus Driven Segmentation and Answer Inference for Telegraphic Entity-seeking Queries",
        "text": "Much recent work focuses on formal interpretation of natural question utterances, with the goal of executing the resulting structured queries on knowledge graphs (KGs) such as Freebase. Here we address two limitations of this approach when applied to open-domain, entity-oriented Web queries. First, Web queries are rarely wellformed questions. They are \"telegraphic\", with missing verbs, prepositions, clauses, case and phrase clues. Second, the KG is always incomplete, unable to directly answer many queries. We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment: a base entity e 1 , a relation type r, a target entity type t 2 , and contextual words s. The query seeks entity e 2 ∈ t 2 where r(e 1 , e 2 ) holds, further evidenced by schema-agnostic words s. Query segmentation is integrated with the KG and an unstructured corpus where mentions of entities have been linked to the KG. We do not trust the best or any specific query segmentation. Instead, evidence in favor of candidate e 2 s are aggregated across several segmentations. Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG, using over a thousand telegraphic queries adapted from TREC, INEX, and Web-Questions, show the efficacy of our approach. For one benchmark, MAP improves from 0.2-0.29 (competitive baselines) to 0.42 (our system). NDCG@10 improves from 0.29-0.36 to 0.54.",
        "id": 9526475
      },
      {
        "title": "Neural Machine Translation with Monolingual Translation Memory *",
        "text": "Prior work has proved that Translation memory (TM) can boost the performance of Neural Machine Translation (NMT). In contrast to existing work that uses bilingual corpus as TM and employs source-side similarity search for memory retrieval, we propose a new framework that uses monolingual memory and performs learnable memory retrieval in a crosslingual manner. Our framework has unique advantages. First, the cross-lingual memory retriever allows abundant monolingual data to be TM. Second, the memory retriever and NMT model can be jointly optimized for the ultimate translation goal. Experiments show that the proposed method obtains substantial improvements. Remarkably, it even outperforms strong TM-augmented NMT baselines using bilingual TM. Owning to the ability to leverage monolingual data, our model also demonstrates effectiveness in low-resource and domain adaptation scenarios.",
        "id": 235166182
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you point me to research that tackles the issue of disambiguating word senses in infrequent and zero-shot scenarios?",
    "positive_ctxs": [
      {
        "title": "Rare and Zero-shot Word Sense Disambiguation using Z-Reweighting",
        "text": "Word sense disambiguation (WSD) is a crucial problem in the natural language processing (NLP) community. Current methods achieve decent performance by utilizing supervised learning and large pre-trained language models. However, the imbalanced training dataset leads to poor performance on rare senses and zero-shot senses. There are more training instances and senses for words with top frequency ranks than those with low frequency ranks in the training dataset. We investigate the statistical relation between word frequency rank and word sense number distribution. Based on the relation, we propose a Z-reweighting method on the word level to adjust the training on the imbalanced dataset. The experiments show that the Z-reweighting strategy achieves performance gain on the standard English all words WSD benchmark. Moreover, the strategy can help models generalize better on rare and zero-shot senses.",
        "id": 248779910
      }
    ],
    "negative_ctxs": [
      {
        "title": "Building A Chinese WordNet Via Class-Based Translation Model",
        "text": "Semantic lexicons are indispensable to research in lexical semantics and word sense disambiguation (WSD). For the study of WSD for English text, researchers have been using different kinds of lexicographic resources, including machine readable dictionaries (MRDs), machine readable thesauri, and bilingual corpora. In recent years, WordNet has become the most widely used resource for the study of WSD and lexical semantics in general. This paper describes the Class-Based Translation Model and its application in assigning translations to nominal senses in WordNet in order to build a prototype Chinese WordNet. Experiments and evaluations show that the proposed approach can potentially be adopted to speed up the construction of WordNet for Chinese and other languages.",
        "id": 18521734
      },
      {
        "title": "Covid or not Covid? Topic Shift in Information Cascades on Twitter",
        "text": "Social media have become a valuable source of information. However, its power to shape public opinion can be dangerous, especially in the case of misinformation. The existing studies on misinformation detection hypothesise that the initial message is fake. In contrast, we focus on information distortion occurring in cascades as the initial message is quoted or receives a reply. We show a significant topic shift in information cascades on Twitter during the Covid-19 pandemic providing valuable insights for the automatic analysis of information distortion.This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/.",
        "id": 227231347
      },
      {
        "title": "OpenUE: An Open Toolkit of Universal Extraction from Text",
        "text": "Natural language processing covers a wide variety of tasks with token-level or sentencelevel understandings. In this paper, we provide a simple insight that most tasks can be represented in a single universal extraction format. We introduce a prototype model and provide an open-source and extensible toolkit called OpenUE for various extraction tasks. OpenUE allows developers to train custom models to extract information from the text and supports quick model validation for researchers. Besides, OpenUE provides various functional modules to maintain sufficient modularity and extensibility. Except for the toolkit, we also deploy an online demo 1 with restful APIs to support real-time extraction without training and deploying. Additionally, the online system can extract information in various tasks, including relational triple extraction, slot & intent detection, event extraction, and so on. We release the source code, datasets, and pretrained models to promote future researches in",
        "id": 226283775
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest a study that proposes high-parameter efficeint fine-tuning techinque that only trains the bias terms?",
    "positive_ctxs": [
      {
        "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
        "text": "We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods. Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.",
        "id": 231672601
      }
    ],
    "negative_ctxs": [
      {
        "title": "FUNCTIONAL STRUCTURES FOR PARSING DEPENDENCY CONSTRAINT5",
        "text": "",
        "id": 9669733
      },
      {
        "title": "SemLink 2: Chasing Lexical Resources",
        "text": "The SemLink resource provides mappings between a variety of lexical semantic ontologies, each with their strengths and weaknesses. To take advantage of these differences, the ability to move between resources is essential. This work describes advances made to improve the usability of the SemLink resource: the automatic addition of new instances and mappings, manual corrections, sense-based vectors and collocation information, and architecture built to automatically update the resource when versions of the underlying resources change. These updates improve coverage, provide new tools to leverage the capabilities of these resources, and facilitate seamless updates, ensuring the consistency and applicability of these mappings in the future. 1",
        "id": 243864612
      },
      {
        "title": "Phrase Structure Trees Bear More Fruit than You Would Have Thought 1",
        "text": "In this paper we will present several results concerning phrase structure trees. These results show that phrase structure trees, when viewed in certain ways, have much more descriptive power than one would have thought. We have given a brief account of local constraints on structural descriptions and an intuitive proof of a theorem about local constraints. We have compared the local constraints approach to some aspects of Gazdar's framework and that of Peters and Ritchie and of Karttunen. We have also presented some results on skeletons (phrase structure trees without labels) which show that phrase structure trees, even when deprived of the labels, retain in a certain sense all the structural information. This result has implications for grammatical inference procedures.",
        "id": 1552091
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research that investigates the impact of randomly removing words from sentences as a data augmentation strategy to mitigate overfitting in NLP models?",
    "positive_ctxs": [
      {
        "title": "DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings",
        "text": "We propose DiffCSE, an unsupervised contrastive learning framework for learning sentence embeddings. DiffCSE learns sentence embeddings that are sensitive to the difference between the original sentence and an edited sentence, where the edited sentence is obtained by stochastically masking out the original sentence and then sampling from a masked language model. We show that DiffSCE is an instance of equivariant contrastive learning (Dangovski et al., 2021), which generalizes contrastive learning and learns representations that are insensitive to certain types of augmentations and sensitive to other \"harmful\" types of augmentations. Our experiments show that DiffCSE achieves state-of-the-art results among unsupervised sentence representation learning methods, outperforming unsupervised SimCSE 1 by 2.3 absolute points on semantic textual similarity tasks. 2",
        "id": 248299679
      }
    ],
    "negative_ctxs": [
      {
        "title": "Evaluation of Online Dialogue Policy Learning Techniques",
        "text": "The number of applied Dialogue Systems is ever increasing in several service providing and other applications as a way to efficiently and inexpensively serve large numbers of customers. A DS that employs some form of adaptation to the environment and its users is called an Adaptive Dialogue System (ADS). A significant part of the research community has lately focused on ADS and many existing or novel techniques are being applied to this problem. One of the most promising techniques is Reinforcement Learning (RL) and especially online RL. This paper focuses on online RL techniques used to achieve adaptation in Dialogue Management and provides an evaluation of various such methods in an effort to aid the designers of ADS in deciding which method to use. To the best of our knowledge there is no other work to compare online RL techniques on the dialogue management problem.",
        "id": 18538074
      },
      {
        "title": "Optimal k-arization of Synchronous Tree-Adjoining Grammar",
        "text": "SynchronousTree-Adjoining Grammar (STAG) is a promising formalism for syntaxaware machine translation and simultaneous computation of natural-language syntax and semantics. Current research in both of these areas is actively pursuing its incorporation. However, STAG parsing is known to be NP-hard due to the potential for intertwined correspondences between the linked nonterminal symbols in the elementary structures. Given a particular grammar, the polynomial degree of efficient STAG parsing algorithms depends directly on the rank of the grammar: the maximum number of correspondences that appear within a single elementary structure. In this paper we present a compile-time algorithm for transforming a STAG into a strongly-equivalent STAG that optimally minimizes the rank, k, across the grammar. The algorithm performs in O(|G| + |Y | · L 3 G ) time where L G is the maximum number of links in any single synchronous tree pair in the grammar and Y is the set of synchronous tree pairs of G.",
        "id": 218606769
      },
      {
        "title": "A Data-Driven, Factorization Parser for CCG Dependency Structures",
        "text": "This paper is concerned with building CCG-grounded, semantics-oriented deep dependency structures with a data-driven, factorization model. Three types of factorization together with different higherorder features are designed to capture different syntacto-semantic properties of functor-argument dependencies. Integrating heterogeneous factorizations results in intractability in decoding. We propose a principled method to obtain optimal graphs based on dual decomposition. Our parser obtains an unlabeled f-score of 93.23 on the CCGBank data, resulting in an error reduction of 6.5% over the best published result. which yields a significant improvement over the best published result in the literature. Our implementation is available at http://www.icst. pku.edu.cn/lcwm/grass.",
        "id": 9531504
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper introduced the human-evaluated timeliness metric for misinformation detection?",
    "positive_ctxs": [
      {
        "title": "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments",
        "text": "We present a human-in-the-loop evaluation framework for fact-checking novel misinformation claims and identifying social media messages that support them. Our approach extracts check-worthy claims, which are aggregated and ranked for review. Stance classifiers are then used to identify tweets supporting novel misinformation claims, which are further reviewed to determine whether they violate relevant policies. To demonstrate the feasibility of our approach, we develop a baseline system based on modern NLP methods for human-in-the-loop fact-checking in the domain of COVID-19 treatments. We make our data 1 and detailed annotation guidelines available to support the evaluation of human-in-the-loop systems that identify novel misinformation directly from raw usergenerated content.",
        "id": 254853990
      }
    ],
    "negative_ctxs": [
      {
        "title": "Knowledge-Based Labeling of Semantic Relationships in English",
        "text": "An increasing number of NLP tasks require semantic labels to be assigned, not only to entities that appear in textual elements, but to the relationships between those entities. Interest is growing in shallow semantic role labeling as well as in deep semantic distance metrics grounded in ontologies, as each of these contributes to better understanding and organization of text. In this work I apply knowledgebased techniques to identify and explore deep semantic relationships in several styles of English text: nominal compounds, full sentences in the domain of knowledge acquisition, and phrase-level labels for images in a collection. I also present work on a graphical tool for exploring the relationship between domain text and deep domain knowledge.",
        "id": 5177350
      },
      {
        "title": "NEUPL: NEURAL POPULATION LEARNING",
        "text": "Learning in strategy games (e.g. StarCraft, poker) requires the discovery of diverse policies. This is often achieved by iteratively training new policies against existing ones, growing a policy population that is robust to exploit. This iterative approach suffers from two issues in real-world games: a) under finite budget, approximate best-response operators at each iteration needs truncating, resulting in under-trained good-responses populating the population; b) repeated learning of basic skills at each iteration is wasteful and becomes intractable in the presence of increasingly strong opponents. In this work, we propose Neural Population Learning (NeuPL) as a solution to both issues. NeuPL offers convergence guarantees to a population of best-responses under mild assumptions. By representing a population of policies within a single conditional model, NeuPL enables transfer learning across policies. Empirically, we show the generality, improved performance and efficiency of NeuPL across several test domains 1 . Most interestingly, we show that novel strategies become more accessible, not less, as the neural population expands.The need for learning not one, but a population of strategies is rooted in classical game theory. Consider the purely cyclical game of rock-paper-scissors, the performance of individual strategies is meaningless as improving against one entails losing to another. By contrast, performance can be meaningfully examined between populations. A population consisting of pure strategies {rock, paper} does well against a singleton population of {scissors} because in the meta-game where both populations are revealed, a player picking strategies from the former can always beat a player choosing from the latter 2 . This observation underpins the unifying population learning framework of Policy Space Response Oracle (PSRO) where a new policy is trained to best-respond to a mixture over previous policies at each iteration, following a meta-strategy solver(Lanctot et al., 2017). Most impressively, Vinyals et al. (2019) explored the strategy game of StarCraft with a league of policies, using a practical variation of PSRO. The league counted close to a thousand sophisticated deep RL agents as the population collectively became robust to exploits.Unfortunately, such empirical successes often come at considerable costs. Population learning algorithms with theoretical guarantees are traditionally studied in normal-form games(Brown, 1951;McMahan et al., 2003)where best-responses can be solved exactly. This is in stark contrast to real-world Game-of-Skills (Czarnecki et al., 2020) -such games are often temporal in nature, where best-responses can only be approximated with computationally intensive methods (e.g. deep RL). This has two implications. First, for a given opponent, one cannot efficiently tell apart good-responses that temporarily plateaued at local optima from globally optimal best-responses. As a result, approximate best-response operators are often truncated prematurely, according to hand-crafted schedules(Lanctot et al., 2017;Mcaleer et al., 2020). Second, real-world games often afford strategy-agnostic transitive",
        "id": 246863450
      },
      {
        "title": "Competitive Grouping in Integrated Phrase Segmentation and Alignment Model",
        "text": "This article describes the competitive grouping algorithm at the core of our Integrated Segmentation and Alignment (ISA) model. ISA extracts phrase pairs from a bilingual corpus without requiring the precalculated word alignment as many other phrase alignment models do. Experiments conducted within the WPT-05 shared task on statistical machine translation demonstrate the simplicity and effectiveness of this approach.",
        "id": 2455968
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which paper first studied differential privacy for in-context learning to prevent prompt leakage attacks?",
    "positive_ctxs": [
      {
        "title": "PRIVACY-PRESERVING IN-CONTEXT LEARNING FOR LARGE LANGUAGE MODELS",
        "text": "In-context learning (ICL) is an important capability of Large Language Models (LLMs), enabling these models to dynamically adapt based on specific, in-context exemplars, thereby improving accuracy and relevance. However, LLM's responses may leak the sensitive private information contained in in-context exemplars. To address this challenge, we propose Differentially Private In-context Learning (DP-ICL), a general paradigm for privatizing ICL tasks. The key idea for DP-ICL paradigm is generating differentially private responses through a noisy consensus among an ensemble of LLM's responses based on disjoint exemplar sets. Based on the general paradigm of DP-ICL, we instantiate several techniques showing how to privatize ICL for text classification and language generation. We evaluate DP-ICL on four text classification benchmarks and two language generation tasks, and our empirical results show that DP-ICL achieves a strong utility-privacy tradeoff.",
        "id": 258436870
      }
    ],
    "negative_ctxs": [
      {
        "title": "Résumé automatique de textes d'opinion Résumé automatique de textes d'opinion",
        "text": "Nous présentons dans cet article un système de résumé automatique tourné vers l'analyse de blogs, où sont exprimées à la fois des informations factuelles et des prises de position sur les faits considérés. Notre système de résumé est fondé sur une approche nouvelle qui mêle analyse de la redondance et repérage des informations nouvelles dans les textes ; ce système générique est en outre enrichi d'un module de calcul de la polarité de l'opinion véhiculée afin de traiter de façon appropriée la subjectivité qui est le propre des billets de blogs. Le système est évalué sur l'anglais, à travers la participation à la campagne d'évaluation internationale TAC (Text Analysis Conference) où notre système a obtenu des performances satisfaisantes.ABSTRACT. In this paper, we present a summarization system that is specifically designed to process blog posts, where factual information is mixed with opinions on the discussed facts. Our approach combines redundancy analysis with new information tracking and is enriched by a module that computes the polarity of textual fragments in order to summarize blog posts more efficiently. The system is evaluated against English data, especially through the participation in TAC (Text Analysis Conference), an international evaluation framework for automatic summarization, in which our system obtained interesting results. MOTS-CLÉS : résumé automatique, analyse de blogs, analyse de l'opinion, redondance, subjectivité, évaluation de résumés automatiques, évaluation de l'analyse de l'opinion.",
        "id": 194963461
      },
      {
        "title": "",
        "text": "",
        "id": 238638462
      },
      {
        "title": "Omorfi-Free and open source morphological lexical database for Finnish",
        "text": "This demonstration presents a freely available open source lexical database omorfi.Omorfi is a mature lexicographical database project, started out as a single-person single-purpose free open source morphological analyser project, omorfi has since grown to be used in variety of applications including spell-checking, statistical and rule-based machine translation, treebanking, joint syntactic and morphological parsing, poetry generation, information extraction. In this demonstration we hope to show both the variety of end-user facing applications as well as the tools and interfaces for computational linguists to make the best use of a developing product. We show a shallow database arrangement that has allowed a great variety of contributors from different projects to extend the lexical database while not breaking the continued use of existing end-applications. We hope to show both the best current practices for lexical data management and software engineering with regards to continuous external project integration of a constantly developing product. As case examples we show some of the integrations with following applications: Voikko spell-checking for Windows, Mac OS X, Linux and Android, statistical machine translation pipelines with moses, rule-based machine translation with apertium and traditional xerox style morphological analysis and generation. morphological segmentation, as well as application programming interfaces for python and Java.",
        "id": 31131573
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Have any research papers examined whether using language models for providing evidence in fact-checking systems risks propagating biases?",
    "positive_ctxs": [
      {
        "title": "Towards Few-Shot Fact-Checking via Perplexity",
        "text": "Few-shot learning has drawn researchers' attention to overcome the problem of data scarcity. Recently, large pre-trained language models have shown great performance in fewshot learning for various downstream tasks, such as question answering and machine translation. Nevertheless, little exploration has been made to achieve few-shot learning for the fact-checking task. However, fact-checking is an important problem, especially when the amount of information online is growing exponentially every day. In this paper, we propose a new way of utilizing the powerful transfer learning ability of a language model via a perplexity score. The most notable strength of our methodology lies in its capability in fewshot learning. With only two training samples, our methodology can already outperform the Major Class baseline by more than an absolute 10% on the F1-Macro metric across multiple datasets. Through experiments, we empirically verify the plausibility of the rather surprising usage of the perplexity score in the context of fact-checking and highlight the strength of our few-shot methodology by comparing it to strong fine-tuning-based baseline models. Moreover, we construct and publicly release two new fact-checking datasets related to",
        "id": 232258000
      }
    ],
    "negative_ctxs": [
      {
        "title": "Sentence Completion Tests for Training and Assessment in a Computational Linguistics Curriculum",
        "text": "This paper presents a novel type of test, halfway between multiple-choice and free-form text, used for training and assessment in several courses in a Computational Linguistics curriculum. We will describe the principles of the test, the different ways in which it can be used by learners, and the tools developed for authoring. Use of this type of test is not limited to the field of Computational Linguistics. Wherever text heavy or even picture based topics are taught use of this type of test is possible.",
        "id": 45598818
      },
      {
        "title": "Parsing and Subcategorization Data",
        "text": "In this paper, we compare the performance of a state-of-the-art statistical parser(Bikel, 2004)in parsing written and spoken language and in generating subcategorization cues from written and spoken language. Although Bikel's parser achieves a higher accuracy for parsing written language, it achieves a higher accuracy when extracting subcategorization cues from spoken language. Additionally, we explore the utility of punctuation in helping parsing and extraction of subcategorization cues. Our experiments show that punctuation is of little help in parsing spoken language and extracting subcategorization cues from spoken language. This indicates that there is no need to add punctuation in transcribing spoken corpora simply in order to help parsers.",
        "id": 5436060
      },
      {
        "title": "MANDARIN LOANWORD PHONOLOGY AND OPTIMALITY THEORY: EVIDENCE FROM TRANSLITERATED AMERICAN STATE NAMES AND TYPHOON NAMES",
        "text": "This paper aims to examine how the consonant clusters and illicit codas are modified in Mandarin loanwords transliterated from English, and to argue that no rules need to be involved and that a purely constraint-based approach-within the framework of Optimality Theory-can explain the data.The data collected from transliterated American state names and state names display the onset-coda inconsistency in Mandarin loanwords. All the onset consonant clusters in the data are faithfully parsed into Mandarin syllables, with inserting vowels to shun consonant cluster. The coda clusters and illicit codas are generally parsed. However, the coda liquids may be parsed in some cases but unparsed in others. The preference of insertion in Mandarin loanwords can be explained by the interaction between the constraints--*COMPLEX, CODACON, MAX-I0, and DEMO. The distinctive behavior of coda liquids can be accounted for by the effect of MINwD.",
        "id": 2523524
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there any paper that employs code LLMs to iteratively generate and refine code with execution results to improve the performance?",
    "positive_ctxs": [
      {
        "title": "Self-Edit: Fault-Aware Code Editor for Code Generation",
        "text": "Large language models (LLMs) have demonstrated an impressive ability to generate codes on competitive programming tasks. However, with limited sample numbers, LLMs still suffer from poor accuracy. Inspired by the process of human programming, we propose a generateand-edit approach named Self-Edit that utilizes execution results of the generated code from LLMs to improve the code quality on the competitive programming task. We execute the generated code on the example test case provided in the question and wrap execution results into a supplementary comment. Utilizing this comment as guidance, our fault-aware code editor is employed to correct errors in the generated code. We perform extensive evaluations across two competitive programming datasets with nine different LLMs. Compared to directly generating from LLMs, our approach can improve the average of pass@1 by 89% on APPS-dev, 31% on APPS-test, and 48% on HumanEval over nine popular code generation LLMs with parameter sizes ranging from 110M to 175B. Compared to other post-processing methods, our method demonstrates superior accuracy and efficiency. . 2022. Incoder:A generative model for code infilling and synthesis. CoRR, abs/2204.05999. . 2022. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. In NeurIPS. . 2023a. Enabling programming thinking in large language models toward code generation. arXiv preprint arXiv:2305.06599. . 2022a. Codeeditor: Learning to edit source code with pre-trained models. arXiv preprint arXiv:2210.17040.",
        "id": 258557186
      }
    ],
    "negative_ctxs": [
      {
        "title": "GRACE: Gradient Harmonized and Cascaded Labeling for Aspect-based Sentiment Analysis",
        "text": "In this paper, we focus on the imbalance issue, which is rarely studied in aspect term extraction and aspect sentiment classification when regarding them as sequence labeling tasks. Besides, previous works usually ignore the interaction between aspect terms when labeling polarities. We propose a GRadient hArmonized and CascadEd labeling model (GRACE) to solve these problems. Specifically, a cascaded labeling module is developed to enhance the interchange between aspect terms and improve the attention of sentiment tokens when labeling sentiment polarities. The polarities sequence is designed to depend on the generated aspect terms labels. To alleviate the imbalance issue, we extend the gradient harmonized mechanism used in object detection to the aspect-based sentiment analysis by adjusting the weight of each label dynamically. The proposed GRACE adopts a post-pretraining BERT as its backbone. Experimental results demonstrate that the proposed model achieves consistency improvement on multiple benchmark datasets and generates state-of-the-art results.",
        "id": 221836262
      },
      {
        "title": "",
        "text": "",
        "id": 241583171
      },
      {
        "title": "Assessing the Limits of Straightforward Models for Nested Named Entity Recognition in Spanish Clinical Narratives",
        "text": "Nested Named Entity Recognition (NER) is an information extraction task that aims to identify entities that may be nested within other entity mentions. Despite the availability of several corpora with nested entities in the Spanish clinical domain, most previous work has overlooked them due to the lack of models and a clear annotation scheme for dealing with the task. To fill this gap, this paper provides an empirical study of straightforward methods for tackling the nested NER task on two Spanish clinical datasets, Clinical Trials, and the Chilean Waiting List. We assess the advantages and limitations of two sequence labeling approaches; one based on Multiple LSTM-CRF architectures and another on Joint labeling models. To better understand the differences between these models, we compute taskspecific metrics that adequately measure the ability of models to detect nested entities and perform a fine-grained comparison across models. Our experimental results show that employing domain-specific language models trained from scratch significantly improves the performance obtained with strong domain-specific and general-domain baselines, achieving stateof-the-art results in both datasets. Specifically, we obtained F 1 scores of 89.21 and 83.16 in Clinical Trials and the Chilean Waiting List, respectively. Interestingly enough, we observe that the task-specific metrics and analysis properly reflect the limitations of the models when recognizing nested entities. Finally, we perform a case study on an aggregated NER dataset created from several clinical corpora in Spanish. We highlight how entity length and the simultaneous recognition of inner and outer entities are the most critical variables for the nested NER task.",
        "id": 256461274
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Can you suggest research that deals with the multiple input sources in clinical text processing by combining several recurrent modules which are each responsible for a single source of information?",
    "positive_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 235097235
      }
    ],
    "negative_ctxs": [
      {
        "title": "Suggestion Mining from Opinionated Text",
        "text": "In addition to the positive and negative sentiments expressed by speakers, opinions on the web also convey suggestions. Such text comprise of advice, recommendations and tips on a variety of points of interest. We propose that suggestions can be extracted from the available opinionated text and put to several use cases. The problem has been identified only recently as a viable task, and there is a lot of scope for research in the direction of problem definition, datasets, and methods. From an abstract view, standard algorithms for tasks like sentence classification and keyphrase extraction appear to be usable for suggestion mining. However, initial experiments reveal that there is a need for new methods, or variations in the existing ones for addressing the problem specific challenges. We present a research proposal which divides the problem into three main research questions; we walk through them, presenting our analysis, results, and future directions.",
        "id": 14195818
      },
      {
        "title": "YNU-HPCC at SemEval-2021 Task 6: Combining ALBERT and Text-CNN for Persuasion Detection in Texts and Images",
        "text": "In recent years, memes combining image and text have been widely used in social media, and memes are one of the most popular types of content used in online disinformation campaigns. In this paper, our study on the detection of persuasion techniques in texts and images in SemEval-2021 Task 6 is summarized. For propaganda technology detection in text, we propose a combination model of both AL-BERT and Text-CNN for text classification, as well as a BERT-based multi-task sequence labeling model for propaganda technology coverage span detection. For the meme classification task involved in text understanding and visual feature extraction, we designed a parallel channel model divided into text and image channels. Our method 1 achieved a good performance on subtasks 1 and 3. The micro F 1scores of 0.492, 0.091, and 0.446 achieved on the test sets of the three subtasks ranked 12th, 7th, and 11th, respectively, and all are higher than the baseline model.",
        "id": 236460072
      },
      {
        "title": "Back up your Stance: Recognizing Arguments in Online Discussions",
        "text": "In online discussions, users often back up their stance with arguments. Their arguments are often vague, implicit, and poorly worded, yet they provide valuable insights into reasons underpinning users' opinions. In this paper, we make a first step towards argument-based opinion mining from online discussions and introduce a new task of argument recognition. We match usercreated comments to a set of predefined topic-based arguments, which can be either attacked or supported in the comment. We present a manually-annotated corpus for argument recognition in online discussions. We describe a supervised model based on comment-argument similarity and entailment features. Depending on problem formulation, model performance ranges from 70.5% to 81.8% F1-score, and decreases only marginally when applied to an unseen topic.",
        "id": 15983978
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend studies on hierarchical modeling of user interests for tailoring news recommendation systems?",
    "positive_ctxs": [
      {
        "title": "HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation",
        "text": "User interest modeling is critical for personalized news recommendation. Existing news recommendation methods usually learn a single user embedding for each user from their previous behaviors to represent their overall interest. However, user interest is usually diverse and multi-grained, which is difficult to be accurately modeled by a single user embedding. In this paper, we propose a news recommendation method with hierarchical user interest modeling, named HieRec. Instead of a single user embedding, in our method each user is represented in a hierarchical interest tree to better capture their diverse and multi-grained interest in news. We use a three-level hierarchy to represent 1) overall user interest; 2) user interest in coarse-grained topics like sports; and 3) user interest in fine-grained topics like football. Moreover, we propose a hierarchical user interest matching framework to match candidate news with different levels of user interest for more accurate user interest targeting. Extensive experiments on two real-world datasets validate our method can effectively improve the performance of user modeling for personalized news recommendation.",
        "id": 235368202
      }
    ],
    "negative_ctxs": [
      {
        "title": "SLEDGE-Z: A Zero-Shot Baseline for COVID-19 Literature Search",
        "text": "With worldwide concerns surrounding the Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2), there is a rapidly growing body of scientific literature on the virus. Clinicians, researchers, and policymakers need to be able to search these articles effectively. In this work, we present a zeroshot ranking algorithm that adapts to COVIDrelated scientific literature. Our approach filters training data from another collection down to medical-related queries, uses a neural reranking model pre-trained on scientific text (SciBERT), and filters the target document collection. This approach ranks top among zeroshot methods on the TREC COVID Round 1 leaderboard, and exhibits a P@5 of 0.80 and an nDCG@10 of 0.68 when evaluated on both Round 1 and 2 judgments. Despite not relying on TREC-COVID data, our method outperforms models that do. As one of the first search methods to thoroughly evaluate",
        "id": 222310503
      },
      {
        "title": "Weak Connectivity in (Un)bounded Dependency Constructions *",
        "text": "This paper argues that various kinds of displaced structures in English should be licensed by a more explicitly formulated type of rule schema in order to deal with what is called weak connectivity in English. This paper claims that the filler and the gap site cannot maintain the total identity of features but a partial overlap since the two positions need to obey the structural forces that come from occupying respective positions. One such case is the missing object construction where the subject fillers and the object gaps are to observe requirements that are imposed on the respective positions. Others include passive constructions and topicalized structures. In this paper, it is argued that the feature discrepancy comes from the different syntactic positions in which the fillers are assumed to be located before and after displacement. In order to capture this type of mismatch, syntactically relevant features are handled separately from the semantically motivated features in order to deal with the syntactically imposed requirements.",
        "id": 2760209
      },
      {
        "title": "Predicting Fine-Tuning Performance with Probing",
        "text": "Large NLP models have recently shown impressive performance in language understanding tasks, typically evaluated by their finetuned performance. Alternatively, probing has received increasing attention as being a lightweight method for interpreting the intrinsic mechanisms of large NLP models. In probing, post-hoc classifiers are trained on \"out-ofdomain\" datasets that diagnose specific abilities. While probing the language models has led to insightful findings, they appear disjointed from the development of models. This paper explores the utility of probing deep NLP models to extract a proxy signal widely used in model development -the fine-tuning performance. We find that it is possible to use the accuracies of only three probing tests to predict the fine-tuning performance with errors 40% -80% smaller than baselines. We further discuss possible avenues where probing can empower the development of deep NLP models.",
        "id": 249121168
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Can you recommend a paper that uses an NLI model for sentence-level relation extraction using hypothesis generation and verification with entity-type constraints?",
    "positive_ctxs": [
      {
        "title": "Label Verbalization and Entailment for Effective Zero-and Few-Shot Relation Extraction",
        "text": "Relation extraction systems require large amounts of labeled examples which are costly to annotate. In this work we reformulate relation extraction as an entailment task, with simple, hand-made, verbalizations of relations produced in less than 15 minutes per relation. The system relies on a pretrained textual entailment engine which is run as-is (no training examples, zero-shot) or further fine-tuned on labeled examples (few-shot or fully trained). In our experiments on TACRED we attain 63% F1 zero-shot, 69% with 16 examples per relation (17% points better than the best supervised system on the same conditions), and only 4 points short of the state-of-the-art (which uses 20 times more training data). We also show that the performance can be improved significantly with larger entailment models, up to 12 points in zero-shot, giving the best results to date on TACRED when fully trained. The analysis shows that our few-shot systems are especially effective when discriminating between relations, and that the performance difference in low data regimes comes mainly from identifying no-relation cases.",
        "id": 237442211
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 219300751
      },
      {
        "title": "",
        "text": "",
        "id": 170476009
      },
      {
        "title": "Associating Collocations with WordNet Senses Using Hybrid Models",
        "text": "In this paper, we introduce a hybrid method to associate English collocations with sense class members chosen from WordNet. Our combinational approach includes a learning-based method, a paraphrase-based method and a sense frequency ranking method. At training time, a set of collocations with their tagged senses is prepared. We use the sentence information extracted from a large corpus and cross-lingual information to train a learning-based model. At run time, the corresponding senses of an input collocation will be decided via majority voting. The three outcomes participated in voting are as follows: 1. the result from a learning-based model; 2. the result from a paraphrase-based model; 3. the result from sense frequency ranking method. The sense with most votes will be associated with the input collocation. Evaluation shows that the hybrid model achieves significant improvement when comparing with the other method described in evaluation time. Our method provides more reliable result on associating collocations with senses that can help lexicographers in compilation of collocations dictionaries and assist learners to understand collocation usages.",
        "id": 2908106
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "How can SQL-to-text be utilized to improve text-to-SQL parsing through data augmentation techniques?",
    "positive_ctxs": [
      {
        "title": "Data Augmentation with Hierarchical SQL-to-Question Generation for Cross-domain Text-to-SQL Parsing",
        "text": "Data augmentation has attracted a lot of research attention in the deep learning era for its ability in alleviating data sparseness. The lack of labeled data for unseen evaluation databases is exactly the major challenge for cross-domain text-to-SQL parsing. Previous works either require human intervention to guarantee the quality of generated data, or fail to handle complex SQL queries. This paper presents a simple yet effective data augmentation framework. First, given a database, we automatically produce a large number of SQL queries based on an abstract syntax tree grammar. For better distribution matching, we require that at least 80% of SQL patterns in the training data are covered by generated queries. Second, we propose a hierarchical SQL-to-question generation model to obtain high-quality natural language questions, which is the major contribution of this work. Finally, we design a simple sampling strategy that can greatly improve training efficiency given large amounts of generated data. Experiments on three cross-domain datasets, i.e., WikiSQL and Spider in English, and DuSQL in Chinese, show that our proposed data augmentation framework can consistently improve performance over strong baselines, and the hierarchical generation component is the key for the improvement.",
        "id": 232104941
      }
    ],
    "negative_ctxs": [
      {
        "title": "Probabilistic Frame Induction",
        "text": "In natural-language discourse, related events tend to appear near each other to describe a larger scenario. Such structures can be formalized by the notion of a frame (a.k.a. template), which comprises a set of related events and prototypical participants and event transitions. Identifying frames is a prerequisite for information extraction and natural language generation, and is usually done manually. Methods for inducing frames have been proposed recently, but they typically use ad hoc procedures and are difficult to diagnose or extend. In this paper, we propose the first probabilistic approach to frame induction, which incorporates frames, events, and participants as latent topics and learns those frame and event transitions that best explain the text. The number of frame components is inferred by a novel application of a split-merge method from syntactic parsing. In end-to-end evaluations from text to induced frames and extracted facts, our method produces state-of-the-art results while substantially reducing engineering effort.",
        "id": 278288
      },
      {
        "title": "",
        "text": "",
        "id": 218974069
      },
      {
        "title": "A Noisy-Channel Approach to Question Answering",
        "text": "We introduce a probabilistic noisychannel model for question answering and we show how it can be exploited in the context of an end-to-end QA system. Our noisy-channel system outperforms a stateof-the-art rule-based QA system that uses similar resources. We also show that the model we propose is flexible enough to accommodate within one mathematical framework many QA-specific resources and techniques, which range from the exploitation of WordNet, structured, and semi-structured databases to reasoning, and paraphrasing.",
        "id": 12305296
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "When using pretrained transformer models for generating sentence embeddings, I've heard different strategies such as mean pooling and using the CLS token's embedding. What study shows that mean pooling outperforms CLS in semantic similarity tasks?",
    "positive_ctxs": [
      {
        "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
        "text": "BERT(Devlin et al., 2018)and RoBERTa (Liu  et al., 2019)  has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods. 1",
        "id": 201646309
      }
    ],
    "negative_ctxs": [
      {
        "title": "FontLex: A Typographical Lexicon based on Affective Associations",
        "text": "The task of selecting suitable fonts for a given text is non-trivial, as tens of thousands of fonts are available, and the choice of font has been shown to affect the perception of the text as well as of the author or of the brand being advertized. Aiming to support the development of font recommendation tools, we create a typographical lexicon providing associations between words and fonts. We achieve this by means of affective evocations, making use of font-emotion and word-emotion relationships. For this purpose, we first determine font vectors for a set of ten emotion attributes, based on word similarities and antonymy information. We evaluate these associations through a user study via Mechanical Turk, which, for eight of the ten emotions, shows a strong user preference towards the fonts that are found to be congruent by our predicted data. Subsequently, this data is used to calculate font vectors for specific words, by relying on the emotion associations of a given word. This leads to a set of font associations for 6.4K words. We again evaluate the resulting dataset using Mechanical Turk, on 25 randomly sampled words. For the majority of these words, the responses indicate that fonts with strong associations are preferred, and for all except 2 words, fonts with weak associations are dispreferred. Finally, we further extend the dataset using synonyms of font attributes and emotion names. The resulting FontLex resource provides mappings between 6.7K words and 200 fonts.",
        "id": 21691078
      },
      {
        "title": "A Multiword Expression Data Set: Annotating Non-Compositionality and Conventionalization for English Noun Compounds",
        "text": "Scarcity of multiword expression data sets raises a fundamental challenge to evaluating the systems that deal with these linguistic structures. In this work we attempt to address this problem for a subclass of multiword expressions by producing a large data set annotated by experts and validated by common statistical measures. We present a set of 1048 noun-noun compounds annotated as non-compositional, compositional, conventionalized and not conventionalized. We build this data set following common trends in previous work while trying to address some of the well known issues such as small number of annotated instances, quality of the annotations, and lack of availability of true negative instances.",
        "id": 345344
      },
      {
        "title": "Character-Level Linguistic Features Extraction for Text-to-Speech System",
        "text": "摘要 優良的語言文脈訊息是語音合成的關鍵部分，傳統的文脈訊息都是依賴於自然語言處理 (Natural Language Processing，NLP)，使用 parser 分析文字。但是 parser 設計困難無法 專門為語音合成設計；所以我們想直接以字元為處理單元建立一個 end-to-end 的語音合 成系統，在這想法下我們改用字元層級(character-level)的 word2vec 與遞迴類神經網路， 直接將輸入字元序列轉換成隱藏特徵向量當做語言合成的文脈訊息。最後我們利用一中 英夾雜語音合成系統測試此想法，語音合成的實驗的結果表明，我們提出的方式的確比 傳統使用 parse 的方式有更好的性能。 關鍵詞：語音合成、語言特徵、文脈訊息",
        "id": 23967599
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "What paper first showed that you can score the code explanations using the pass@k metric?",
    "positive_ctxs": [
      {
        "title": "OCTOPACK: INSTRUCTION TUNING CODE LARGE LANGUAGE MODELS",
        "text": "Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile COMMITPACK: 4 terabytes of Git commits across 350 programming languages. We benchmark COMMITPACK against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HUMANEVALPACK, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OCTOCODER and OCTOGEEX, achieve the best performance across HUMANEVALPACK among all permissive models, demonstrating COMMITPACK's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack. import numpy as np import matplotlib.pyplot as plt # generate sample data x_data = np.linspace (-5, 5, 20)   y_data = np.random.normal(0.0, 1.0, x_data.size) plt.plot(x_data, y_data, 'o') plt.show() Code Before Commit MessageCode AfterChange to sin() function with noise import math import numpy as np import matplotlib.pyplot as plt",
        "id": 260886874
      }
    ],
    "negative_ctxs": [
      {
        "title": "Linguistic representation of Finnish in the medical domain spoken language translation system Mots-clefs -Keywords",
        "text": "Grammaire d'unification, traduction automatique multilingue de la parole, interlingue, souslangage, finnois.Domain specific unification grammar, multilingual spoken language translation, interlingua, sub-language, Finnish.Résumé -AbstractDans cet article nous décrivons le développement des ressources linguistiques du finnois pour un système de traduction automatique de la parole dans le domaine médical: MedSLT. Le travail inclut la construction des corpus médicaux en finnois, le développement de la grammaire finlandaise pour la génération, le développement du lexique finlandais et la définition des règles de mapping interlingue-finnois pour la traduction multilingue. Nous avons découvert que le finnois peut être introduit dans l'architecture existante de MedSLT sans trop de difficultés. En effet, malgré les différences entre l'anglais et le finnois, la grammaire finlandaise a pu être créée en adaptant manuellement la grammaire anglaise originale. Les premiers résultats de l'évaluation de la traduction anglais-finnois sont encourageants.This paper describes the development of Finnish linguistic resources for use in MedSLT, an Open Source medical domain speech-to-speech translation system. The paper describes the collection of medical Finnish corpora, the creation of a Finnish grammar by adapting the original English grammar, the composition of a domain specific Finnish lexicon and the definition of interlingua to Finnish mapping rules for multilingual translation. It is shown that Finnish can be effectively introduced into the existing MedSLT framework and that despite the differences between English and Finnish, the Finnish grammar can be created by manual adaptation from the original English grammar. Regarding further development, the initial evaluation results of English-Finnish speech-to-speech translation are encouraging.Marianne Santaholma",
        "id": 10919306
      },
      {
        "title": "Emotion Detection with Neural Personal Discrimination",
        "text": "There have been a recent line of works to automatically predict the emotions of posts in social media. Existing approaches consider the posts individually and predict their emotions independently. Different from previous researches, we explore the dependence among relevant posts via the authors' backgrounds, since the authors with similar backgrounds, e.g., gender, location, tend to express similar emotions. However, such personal attributes are not easy to obtain in most social media websites, and it is hard to capture attributesaware words to connect similar people. Accordingly, we propose a Neural Personal Discrimination (NPD) approach to address above challenges by determining personal attributes from posts, and connecting relevant posts with similar attributes to jointly learn their emotions. In particular, we employ adversarial discriminators to determine the personal attributes, with attention mechanisms to aggregate attributes-aware words. In this way, social correlationship among different posts can be better addressed. Experimental results show the usefulness of personal attributes, and the effectiveness of our proposed NPD approach in capturing such personal attributes with significant gains over the state-of-the-art models.",
        "id": 201657071
      },
      {
        "title": "Modelling Temporal Document Sequences for Clinical ICD Coding",
        "text": "Past studies on the ICD coding problem focus on predicting clinical codes primarily based on the discharge summary. This covers only a small fraction of the notes generated during each hospital stay and leaves potential for improving performance by analysing all the available clinical notes. We propose a hierarchical transformer architecture that uses text across the entire sequence of clinical notes in each hospital stay for ICD coding, and incorporates embeddings for text metadata such as their position, time, and type of note. While using all clinical notes increases the quantity of data substantially, superconvergence can be used to reduce training costs. We evaluate the model on the MIMIC-III dataset. Our model exceeds the prior state-of-the-art when using only discharge summaries as input, and achieves further performance improvements when all clinical notes are used as input.",
        "id": 257205854
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "I'm looking for a paper that discusses improvements in constituency parsing performance by applying a partition strategy for content embedding and positional embedding within self-attention and label attention layers.",
    "positive_ctxs": [
      {
        "title": "Constituency Parsing with a Self-Attentive Encoder",
        "text": "We demonstrate that replacing an LSTM encoder with a self-attentive architecture can lead to improvements to a state-ofthe-art discriminative constituency parser. The use of attention makes explicit the manner in which information is propagated between different locations in the sentence, which we use to both analyze our model and propose potential improvements. For example, we find that separating positional and content information in the encoder can lead to improved parsing accuracy. Additionally, we evaluate different approaches for lexical representation. Our parser achieves new state-ofthe-art results for single models trained on the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset.",
        "id": 19206893
      }
    ],
    "negative_ctxs": [
      {
        "title": "Evaluating the morphological competence of Machine Translation Systems",
        "text": "While recent changes in Machine Translation state-of-the-art brought translation quality a step further, it is regularly acknowledged that the standard automatic metrics do not provide enough insights to fully measure the impact of neural models. This paper proposes a new type of evaluation focused specifically on the morphological competence of a system with respect to various grammatical phenomena. Our approach uses automatically generated pairs of source sentences, where each pair tests one morphological contrast. This methodology is used to compare several systems submitted at WMT'17 for English into Czech and Latvian.",
        "id": 11877258
      },
      {
        "title": "An Extensible Framework for Efficient Document Management Using RDF and OWL",
        "text": "In this paper, we describe an integrated approach towards dealing with various semantic and structural issues associated with document management. We provide motivations for using XML, RDF and OWL in building a seamless architecture to serve not only as a document exchange service but also to enable higher level services such as annotations, metadata access and querying. The key idea is to manifest differential treatments for the actual document structure, semantic content of the document and ontological document organization. The deployment of this architecture in the PROTEUS project 1 provides an industrial setting for evaluation and further specification.",
        "id": 18471591
      },
      {
        "title": "Annotation of Events and Temporal Expressions in French Texts",
        "text": "We present two modules for the recognition and annotation of temporal expressions and events in French texts according to the TimeML specification language. The Temporal Expression Tagger we have developed is based on a large coverage cascade of finite state transducers and our Event Tagger on a set of simple heuristics applied over local context in a chunked text. We present results of a preliminary evaluation and compare them with those obtained by a similar system.",
        "id": 17196199
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that examines the application of specialized architecture in pre-trained language models to enhance text-to-SQL tasks?",
    "positive_ctxs": [
      {
        "title": "Structure-Grounded Pretraining for Text-to-SQL",
        "text": "Learning to capture text-table alignment is essential for tasks like text-to-SQL. A model needs to correctly recognize natural language references to columns and values and to ground them in the given database schema. In this paper, we present a novel weakly supervised Structure-Grounded pretraining framework (STRUG) for text-to-SQL that can effectively learn to capture text-table alignment based on a parallel text-table corpus. We identify a set of novel pretraining tasks: column grounding, value grounding and columnvalue mapping, and leverage them to pretrain a text-table encoder. Additionally, to evaluate different methods under more realistic text-table alignment settings, we create a new evaluation set Spider-Realistic based on Spider dev set with explicit mentions of column names removed, and adopt eight existing textto-SQL datasets for cross-database evaluation. STRUG brings significant improvement over BERT LARGE in all settings. Compared with existing pretraining methods such as GRAPPA, STRUG achieves similar performance on Spider, and outperforms all baselines on more realistic sets. All the code and data used in this work is public available at https://aka.ms/ strug.",
        "id": 225066679
      }
    ],
    "negative_ctxs": [
      {
        "title": "Chinese Named Entity Recognition with a Multi-Phase Model",
        "text": "Chinese named entity recognition is one of the difficult and challenging tasks of NLP. In this paper, we present a Chinese named entity recognition system using a multi-phase model. First, we segment the text with a character-level CRF model. Then we apply three word-level CRF models to the labeling person names, location names and organization names in the segmentation results, respectively. Our systems participated in the NER tests on open and closed tracks of Microsoft Research (MSRA). The actual evaluation results show that our system performs well on both the open tracks and closed tracks.",
        "id": 11514373
      },
      {
        "title": "Stochastically Evaluating the Validity of Partial Parse Trees in Incremental Parsing",
        "text": "This paper proposes a method for evaluating the validity of partial parse trees constructed in incremental parsing. Our method is based on stochastic incremental parsing, and it incrementally evaluates the validity for each partial parse tree on a wordby-word basis. In our method, incremental parser returns partial parse trees at the point where the validity for the partial parse tree becomes greater than a threshold. Our technique is effective for improving the accuracy of incremental parsing.1.[t] X , where t is a terminal symbol and X is a nonterminal symbol.",
        "id": 15337818
      },
      {
        "title": "Incremental Parsing in Bounded Memory",
        "text": "This tutorial will describe the use of a factored probabilistic sequence model for parsing speech and text using a bounded store of three to four incomplete constituents over time, in line with recent estimates of human shortterm working memory capacity. This formulation uses a grammar transform to minimize memory usage during parsing. Incremental operations on incomplete constituents in this transformed representation then define an extended domain of locality similar to those defined in mildly context-sensitive grammar formalisms, which can similarly be used to process long-distance and crossed-and-nested dependencies.Incremental Parsing in Bounded Memory",
        "id": 15386832
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper proposes the two-stage training method, i.e., task-specific fine-tuning and cross-domain pre-training, to train an open-domain dialogue evaluator using the self-collected dataset.",
    "positive_ctxs": [
      {
        "title": "RADE: Reference-Assisted Dialogue Evaluation for Open-Domain Dialogue",
        "text": "Evaluating open-domain dialogue systems is challenging for reasons such as the one-tomany problem, i.e., many appropriate responses other than just the golden response. As of now, automatic evaluation methods need better consistency with humans, while reliable human evaluation can be time-and cost-intensive. To this end, we propose the Reference-Assisted Dialogue Evaluation (RADE) approach under the multi-task learning framework, which leverages the pre-created utterance as reference other than the gold response to relief the one-tomany problem. Specifically, RADE explicitly compares reference and the candidate response to predict their overall scores. Moreover, an auxiliary response generation task enhances prediction via a shared encoder. To support RADE, we extend three datasets with additional rated responses other than just a golden response by human annotation. Experiments on our three datasets and two existing benchmarks demonstrate the effectiveness of our method, where Pearson, Spearman, and Kendall correlations with human evaluation outperform stateof-the-art baselines.Satanjeev Banerjee and Alon Lavie. 2005. METEOR:An automatic metric for MT evaluation with improved correlation with human judgments. In ACL. . 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.",
        "id": 259370539
      }
    ],
    "negative_ctxs": [
      {
        "title": "The development of a morphosyntactic tagset for Afrikaans and its use with statistical tagging",
        "text": "In this paper, we present a morphosyntactic tagset for Afrikaans based on the guidelines developed by the Expert Advisory Group on Language Engineering Standards (EAGLES). We compare our slim yet expressive tagset, MAATS (Morphosyntactic AfrikAans TagSet), with an existing one which primarily focuses on a detailed morphosyntactic and semantic description of word forms. MAATS will primarily be used for the extraction of lexical data from large pos-tagged corpora. We not only focus on morphosyntactic properties but also on the processability with statistical tagging. We discuss the tagset design and motivate our classification of Afrikaans word forms, in particular we focus on the categorization of verbs and conjunctions. The complete tagset in presented and we briefly discuss each word class. In a case study with an Afrikaans newspaper corpus, we evaluate our tagset with four different statistical taggers. Despite a relatively small amount of training data, however with a large tagger lexicon, TnT-Tagger scores 97.05 % accuracy. Additionally, we present some error sources and discuss future work.",
        "id": 18559676
      },
      {
        "title": "Double Graph Based Reasoning for Document-level Relation Extraction",
        "text": "Document-level relation extraction aims to extract relations among entities within a document. Different from sentence-level relation extraction, it requires reasoning over multiple sentences across a document. In this paper, we propose Graph Aggregation-and-Inference Network (GAIN) featuring double graphs. GAIN first constructs a heterogeneous mention-level graph (hMG) to model complex interaction among different mentions across the document. It also constructs an entitylevel graph (EG), based on which we propose a novel path reasoning mechanism to infer relations between entities. Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous state-of-the-art. Our code is available at https://github.com/ DreamInvoker/GAIN. * Equal contribution. † Corresponding author. Elias Brown [1] Elias Brown (May 9, 1793-July 7, 1857) was a U.S. Representative from Maryland. [2] Born near Baltimore, Maryland, Brown attended the common schools. … [7] He died near Baltimore, Maryland, and is interred in a private cemetery near Eldersburg, Maryland.",
        "id": 221996144
      },
      {
        "title": "",
        "text": "",
        "id": 227230343
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "What research exists on employing generative models with latent variable to capture semantic dependencies in conversational systems?",
    "positive_ctxs": [
      {
        "title": "GTM: A Generative Triple-Wise Model for Conversational Question Generation",
        "text": "Generating some appealing questions in opendomain conversations is an effective way to improve human-machine interactions and lead the topic to a broader or deeper direction. To avoid dull or deviated questions, some researchers tried to utilize answer, the \"future\" information, to guide question generation. However, they separate a post-questionanswer (PQA) triple into two parts: postquestion (PQ) and question-answer (QA) pairs, which may hurt the overall coherence. Besides, the QA relationship is modeled as a one-to-one mapping that is not reasonable in open-domain conversations. To tackle these problems, we propose a generative triple-wise model with hierarchical variations for open-domain conversational question generation (CQG). Latent variables in three hierarchies are used to represent the shared background of a triple and one-to-many semantic mappings in both PQ and QA pairs. Experimental results on a largescale CQG dataset show that our method significantly improves the quality of questions in terms of fluency, coherence and diversity over competitive baselines.",
        "id": 235358586
      }
    ],
    "negative_ctxs": [
      {
        "title": "Negative Concord and Restructuring in Palestinian Arabic: A Comparison of TAG and CCG Analyses",
        "text": "This paper discusses interactions between negative concord and restructuring/clause union in Palestinian Arabic. Analyses formulated in Tree Adjoining Grammar and Combinatorial Categorial Grammar are compared, with the conclusion that a perspicuous analysis of the the intricacies of the data requires aspects of both formalisms; in particular, the TAG notion of the extended domain of locality and the CCG notion of flexible constituency.",
        "id": 11275509
      },
      {
        "title": "WarwickDCS: From Phrase-Based to Target-Specific Sentiment Recognition",
        "text": "We present and evaluate several hybrid systems for sentiment identification for Twitter, both at the phrase and document (tweet) level. Our approach has been to use a novel combination of lexica, traditional NLP and deep learning features. We also analyse techniques based on syntactic parsing and tokenbased association to handle topic specific sentiment in subtask C. Our strategy has been to identify subphrases relevant to the designated topic/target and assign sentiment according to our subtask A classifier. Our submitted subtask A classifier ranked fourth in the Se-mEval official results while our BASELINE and µPARSE classifiers for subtask C would have ranked second.",
        "id": 4159041
      },
      {
        "title": "CMUQ@QALB-2014: An SMT-based System for Automatic Arabic Error Correction",
        "text": "In this paper, we describe the CMUQ system we submitted to The ANLP-QALB 2014 Shared Task on Automatic Text Correction for Arabic. Our system combines rule-based linguistic techniques with statistical language modeling techniques and machine translationbased methods. Our system outperforms the baseline and reaches an F-score of 65.42% on the test set of QALB corpus. This ranks us 3rd in the competition.",
        "id": 17910054
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which paper proposes to integrate black-box LLMs with a pool of smaller but specialized language models?",
    "positive_ctxs": [
      {
        "title": "KNOWLEDGE CARD: FILLING LLMS' KNOWLEDGE GAPS WITH PLUG-IN SPECIALIZED LANGUAGE MODELS",
        "text": "By design, large language models (LLMs) are static general-purpose models, expensive to retrain or update frequently.As they are increasingly adopted for knowledge-intensive tasks, it becomes evident that these design choices lead to failures to generate factual, relevant, and up-to-date knowledge.To this end, we propose KNOWLEDGE CARD, a modular framework to plug in new factual and relevant knowledge into general-purpose LLMs.We first introduce knowledge cards-specialized language models trained on corpora from specific domains and sources.Knowledge cards serve as parametric repositories that are selected at inference time to generate background knowledge for the base LLM.We then propose three content selectors to dynamically select and retain information in documents generated by knowledge cards, specifically controlling for relevance, brevity, and factuality of outputs.Finally, we propose two complementary integration approaches to augment the base LLM with the (relevant, factual) knowledge curated from the specialized LMs.Through extensive experiments, we demonstrate that KNOWLEDGE CARD achieves state-of-the-art performance on six benchmark datasets.Ultimately, KNOWLEDGE CARD framework enables dynamic synthesis and updates of knowledge from diverse domains.Its modularity will ensure that relevant knowledge can be continuously updated through the collective efforts of the research community.",
        "id": 258741298
      }
    ],
    "negative_ctxs": [
      {
        "title": "SACRY: Syntax-based Automatic Crossword puzzle Resolution sYstem",
        "text": "In this paper, we present our Crossword Puzzle Resolution System (SACRY), which exploits syntactic structures for clue reranking and answer extraction. SACRY uses a database (DB) containing previously solved CPs in order to generate the list of candidate answers. Additionally, it uses innovative features, such as the answer position in the rank and aggregated information such as the min, max and average clue reranking scores. Our system is based on WebCrow, one of the most advanced systems for automatic crossword puzzle resolution. Our extensive experiments over our two million clue dataset show that our approach highly improves the quality of the answer list, enabling the achievement of unprecedented results on the complete CP resolution tasks, i.e., accuracy of 99.17%.",
        "id": 7975508
      },
      {
        "title": "FTD at SemEval-2023 Task 3: News Genre and Propaganda Detection by Comparing Mono-and Multilingual Models with Fine-tuning on Additional Data",
        "text": "We report our participation in the SemEval-2023 shared task on propaganda detection and describe our solutions with pre-trained models and their ensembles. For Subtask 1 (News Genre Categorisation), we report the impact of several settings, such as the choice of the classification models (monolingual or multilingual or their ensembles), the choice of the training sets (base or additional sources), the impact of detection certainty in making a classification decision as well as the impact of other hyperparameters. In particular, we fine-tune models on additional data for other genre classification tasks, such as FTD. We also try adding texts from genre-homogenous corpora, such as Panorama, Babylon Bee for satire and Giganews for for reporting texts. We also make prepared models for Subtasks 2 and 3 with finetuning the corresponding models first for Subtask 1. The code needed to reproduce the experiments is available. 1Subtask 1: News genresWe experiment with all the languages and report our results on the dev sets. In all the models we use, the input size is restricted to k tokens (in most cases, k = 512). To tackle it, we take first k tokens 1 549",
        "id": 259376610
      },
      {
        "title": "Statistical Filtering and Subcategorization Frame Acquisition",
        "text": "Research \"into the automatic acquisition of subcategorization frames (SCFS) from corpora is starting to produce large-scale computational lexicons which include valuable frequency information. However, the accuracy of the resulting lexicons shows room for improvement. One significant source of error lies in the statistical filtering used by some researchers to remove noise from automatically acquired subcategorization frames. In this paper, we compare three different approaches to filtering out spurious hypotheses. Two hypothesis tests perform poorly, compared to filtering frames on the basis of relative frequency. We discuss reasons for this and consider directions for future research.",
        "id": 13579675
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that assesses if language models use extended contextual information, with experiments on the book dataset from Project Gutenberg?",
    "positive_ctxs": [
      {
        "title": "Do Long-Range Language Models Actually Use Long-Range Context?",
        "text": "Language models are generally trained on short, truncated input sequences, which limits their ability to use discourse-level information present in long-range context to improve their predictions. Recent efforts to improve the efficiency of self-attention have led to a proliferation of long-range Transformer language models, which can process much longer sequences than models of the past. However, the ways in which such models take advantage of the longrange context remain unclear. In this paper, we perform a fine-grained analysis of two longrange Transformer language models (including the Routing Transformer, which achieves state-of-the-art perplexity on the PG-19 longsequence LM benchmark dataset) that accept input sequences of up to 8K tokens. Our results reveal that providing long-range context (i.e., beyond the previous 2K tokens) to these models only improves their predictions on a small set of tokens (e.g., those that can be copied from the distant context) and does not help at all for sentence-level prediction tasks. Finally, we discover that PG-19 contains a variety of different document types and domains, and that long-range context helps most for literary novels (as opposed to textbooks or magazines).",
        "id": 237572264
      }
    ],
    "negative_ctxs": [
      {
        "title": "Indirectly Determined Comparison and Difference: The Case of Japanese",
        "text": "When making comparisons,",
        "id": 234487216
      },
      {
        "title": "DECOMPOSITION OF JAPANESE SENTENCES INTO NORMAL FORMS BASED ON HUMAN LINGUISTIC PROCESS",
        "text": "A diversity and a flexibility of language expression forms are awkward problems for the machine processing of language, such as translation, indexing and question-answering. This paper presents a method of decomposing Japanese sentences appearing in the Patent Documents on \"Pulse network\", into normal forms.First, the linguistic information is analysed and classified based on the human linguistic process. Then, predicate functions, phrase functions and operators are introduced as the normal forms. Finally, the decomposing procedure and some experimental results are shown.492-",
        "id": 13036650
      },
      {
        "title": "Conceptual text representation for multi-lingual generation and translation",
        "text": "This paper presents some ideas and preliminary results o f a project aim ed a t automatic generation and translation o f text horn conceptual (interlingual) representations. In the first part we give some arguments for treating translation and generation as closely coupled processes and motivate the need for conceptual text representations to aid these tasks. In the second part we describe an implemented method fw multi-lingual gene ration o f sentences.",
        "id": 11198970
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "What research first proposed a new kind of cascaded diffusion of a Markov process?",
    "positive_ctxs": [
      {
        "title": "RELAY DIFFUSION: UNIFYING DIFFUSION PROCESS ACROSS RESOLUTIONS FOR IMAGE SYNTHESIS",
        "text": "Diffusion models achieved great success in image synthesis, but still face challenges in high-resolution generation. Through the lens of discrete cosine transformation, we find the main reason is that the same noise level on a higher resolution results in a higher Signal-to-Noise Ratio in the frequency domain. In this work, we present Relay Diffusion Model (RDM), which transfers a low-resolution image or noise into an equivalent high-resolution one for diffusion model via blurring diffusion and block noise. Therefore, the diffusion process can continue seamlessly in any new resolution or model without restarting from pure noise or lowresolution conditioning. RDM achieves state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256×256, surpassing previous works such as ADM, LDM and DiT by a large margin. All the codes and checkpoints are open-sourced at https://github.com/THUDM/RelayDiffusion. Figure 1: (left): Generated Samples by RDM on ImageNet 256×256 and CelebA-HQ 256×256. (right): Benchmarking recent diffusion models on class-conditional ImageNet 256×256 generation without any guidance. RDM can achieve a FID of 1.87 if with classifier-free guidance. arXiv:2309.03350v1 [cs.CV] 4 Sep 2023Preprint generative models in recent years. However, challenges still exist in the training of diffusion models for high-resolution images. More specifically, there are two main obstacles:Training Efficiency. Although equipped with UNet to balance the memory and computation cost across different resolutions, diffusion models still require a large amount of resources to train on high-resolution images. One popular solution is to train the diffusion model on a latent (usually 4× compression rate in resolution) space and map the result back as pixels(Rombach et al., 2022), which is fast but inevitably suffers from some low-level artifacts. The cascaded method  trains a series of varying-size super-resolution diffusion models, which is effective but needs a complete sampling for each stage separately.Noise Schedule. Diffusion models need a noise schedule to control the amount of the isotropic Gaussian noise at each step. The setting of the noise schedule shows great influence over the performance, and most current models follow the linear (Ho et al., 2020) or cosine  schedule. However, an ideal noise schedule should be resolution-dependent (SeeFigure 2or Chen (2023)), resulting in suboptimal performance to train high-resolution models directly with common schedules designed for resolutions of 32×32 or 64×64 pixels.",
        "id": 261582259
      }
    ],
    "negative_ctxs": [
      {
        "title": "An alternate approach towards meaningful lyric generation in Tamil",
        "text": "This paper presents our on-going work to improve the lyric generation component of the Automatic Lyric Generation system for the Tamil Language. An earlier version of the system used an n-gram based model to generate lyrics that match the given melody. This paper identifies some of the deficiencies in the melody analysis and text generation components of the earlier system and explains the new approach used to tackle those drawbacks. The two central approaches discussed in this paper are: (1) An improved mapping scheme for matching melody with words and (2) Knowledge-based Text Generation algorithm based on an existing Ontology and Tamil Morphology Generator.",
        "id": 9454296
      },
      {
        "title": "Towards Topic Labeling with Phrase Entailment and Aggregation",
        "text": "We propose a novel framework for topic labeling that assigns the most representative phrases for a given set of sentences covering the same topic. We build an entailment graph over phrases that are extracted from the sentences, and use the entailment relations to identify and select the most relevant phrases. We then aggregate those selected phrases by means of phrase generalization and merging. We motivate our approach by applying over conversational data, and show that our framework improves performance significantly over baseline algorithms.",
        "id": 11197260
      },
      {
        "title": "Who is GPT-3? An Exploration of Personality, Values and Demographics",
        "text": "Language models such as GPT-3 have caused a furore in the research community. Some studies found that GPT-3 has some creative abilities and makes mistakes that are on par with human behaviour. This paper answers a related question: Who is GPT-3? We administered two validated measurement tools to GPT-3 to assess its personality, the values it holds and its selfreported demographics. Our results show that GPT-3 scores similarly to human samples in terms of personality and -when provided with a model response memory -in terms of the values it holds. We provide the first evidence of psychological assessment of the GPT-3 model and thereby add to our understanding of this language model. We close with suggestions for future research that moves social science closer to language models and vice versa. * Equal first-authorship contribution: authorship order for MM and NR was determined by a random number generator.",
        "id": 252595794
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a paper that builds a writing assistant with autocomplete capabilities conditioned on user intent?",
    "positive_ctxs": [
      {
        "title": "IGA: An Intent-Guided Authoring Assistant",
        "text": "While large-scale pretrained language models have significantly improved writing assistance functionalities such as autocomplete, more complex and controllable writing assistants have yet to be explored. We leverage advances in language modeling to build an interactive writing assistant that generates and rephrases text according to fine-grained author specifications. Users provide input to our Intent-Guided Assistant (IGA) in the form of text interspersed with tags that correspond to specific rhetorical directives (e.g., adding description or contrast, or rephrasing a particular sentence). We fine-tune a language model on a dataset heuristically-labeled with author intent, which allows IGA to fill in these tags with generated text that users can subsequently edit to their liking. A series of automatic and crowdsourced evaluations confirm the quality of IGA's generated outputs, while a smallscale user study demonstrates author preference for IGA over baseline methods in a creative writing task. We release our dataset, code, and demo to spur further research into AI-assisted writing.",
        "id": 233231544
      }
    ],
    "negative_ctxs": [
      {
        "title": "Automatic Chinese Abbreviation Generation Using Conditional Random Field",
        "text": "This paper presents a new method for automatically generating abbreviations for Chinese organization names. Abbreviations are commonly used in spoken Chinese, especially for organization names. The generation of Chinese abbreviation is much more complex than English abbreviations, most of which are acronyms and truncations. The abbreviation generation process is formulated as a character tagging problem and the conditional random field (CRF) is used as the tagging model. A carefully selected group of features is used in the CRF model. After generating a list of abbreviation candidates using the CRF, a length model is incorporated to re-rank the candidates. Finally the full-name and abbreviation co-occurrence information from a web search engine is utilized to further improve the performance. We achieved top-10 coverage of 88.3% by the proposed method.",
        "id": 860754
      },
      {
        "title": "Joint Prediction of Morphosyntactic Categories for Fine-Grained Arabic Part-of-Speech Tagging Exploiting Tag Dictionary Information",
        "text": "Part-of-speech (POS) tagging for morphologically rich languages such as Arabic is a challenging problem because of their enormous tag sets. One reason for this is that in the tagging scheme for such languages, a complete POS tag is formed by combining tags from multiple tag sets defined for each morphosyntactic category. Previous approaches in Arabic POS tagging applied one model for each morphosyntactic tagging task, without utilizing shared information between the tasks. In this paper, we propose an approach that utilizes this information by jointly modeling multiple morphosyntactic tagging tasks with a multi-task learning framework. We also propose a method of incorporating tag dictionary information into our neural models by combining word representations with representations of the sets of possible tags. Our experiments showed that the joint model with tag dictionary information results in an accuracy of 91.38% on the Penn Arabic Treebank data set, with an absolute improvement of 2.11% over the current state-of-the-art tagger. 1",
        "id": 20260724
      },
      {
        "title": "Speaker Naming in Movies",
        "text": "We propose a new model for speaker naming in movies that leverages visual, textual, and acoustic modalities in an unified optimization framework. To evaluate the performance of our model, we introduce a new dataset consisting of six episodes of the Big Bang Theory TV show and eighteen full movies covering different genres. Our experiments show that our multimodal model significantly outperforms several competitive baselines on the average weighted F-score metric. To demonstrate the effectiveness of our framework, we design an end-to-end memory network model that leverages our speaker naming model and achieves state-of-the-art results on the subtitles task of the MovieQA 2017 Challenge.",
        "id": 44084674
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "In the context of Named Entity Recognition tasks across multiple languages, which work highlights the necessity of retrieving related knowledge to aid in the annotation of ambiguous named entities?",
    "positive_ctxs": [
      {
        "title": "CrossWeigh: Training Named Entity Tagger from Imperfect Annotations",
        "text": "Everyone makes mistakes. So do human annotators when curating labels for named entity recognition (NER). Such label mistakes might hurt model training and interfere model comparison. In this study, we dive deep into one of the widely-adopted NER benchmark datasets, CoNLL03 NER. We are able to identify label mistakes in about 5.38% test sentences, which is a significant ratio considering that the state-of-the-art test F 1 score is already around 93%. Therefore, we manually correct these label mistakes and form a cleaner test set. Our re-evaluation of popular models on this corrected test set leads to more accurate assessments, compared to those on the original test set. More importantly, we propose a simple yet effective framework, CrossWeigh, to handle label mistakes during NER model training. Specifically, it partitions the training data into several folds and train independent NER models to identify potential mistakes in each fold. Then it adjusts the weights of training data accordingly to train the final NER model. Extensive experiments demonstrate significant improvements of plugging various NER models into our proposed framework on three datasets. All implementations and corrected test set are available at our Github repo 1 . * Equal Contributions. 1 https://github.com/ZihanWangKi/ CrossWeigh",
        "id": 202540591
      }
    ],
    "negative_ctxs": [
      {
        "title": "WASSERSTEIN AUTO-ENCODED MDPS FORMAL VERIFICATION OF EFFICIENTLY DISTILLED RL POLICIES WITH MANY- SIDED GUARANTEES",
        "text": "Although deep reinforcement learning (DRL) has many success stories, the largescale deployment of policies learned through these advanced techniques in safetycritical scenarios is hindered by their lack of formal guarantees. Variational Markov Decision Processes (VAE-MDPs) are discrete latent space models that provide a reliable framework for distilling formally verifiable controllers from any RL policy. While the related guarantees address relevant practical aspects such as the satisfaction of performance and safety properties, the VAE approach suffers from several learning flaws (posterior collapse, slow learning speed, poor dynamics estimates), primarily due to the absence of abstraction and representation guarantees to support latent optimization. We introduce the Wasserstein auto-encoded MDP (WAE-MDP), a latent space model that fixes those issues by minimizing a penalized form of the optimal transport between the behaviors of the agent executing the original policy and the distilled policy, for which the formal guarantees apply. Our approach yields bisimulation guarantees while learning the distilled policy, allowing concrete optimization of the abstraction and representation model quality. Our experiments show that, besides distilling policies up to 10 times faster, the latent model quality is indeed better in general. Moreover, we present experiments from a simple time-to-failure verification algorithm on the latent space. The fact that our approach enables such simple verification techniques highlights its applicability.",
        "id": 257663689
      },
      {
        "title": "An End-to-End Approach for Full Bridging Resolution",
        "text": "In this article, we describe our submission to the CODI-CRAC 2021 Shared Task on Anaphora Resolution in Dialogues -Track BR (Gold) 1 . We demonstrate the performance of an end-to-end transformer-based higher-order coreference model finetuned for the task of full bridging. We find that while our approach is not effective at modeling the complexities of the task, it performs well on bridging resolution, suggesting a need for investigations into a robust anaphor identification model for future improvements.",
        "id": 241583697
      },
      {
        "title": "Purity Homophily in Social Networks -Invited Talk",
        "text": "Does sharing moral values encourage people to connect and form communities? The importance of moral homophily (love of same) has been recognized by social scientists, but the types of moral similarities that drive this phenomenon are still unknown. In this talk, I will present a series of experiments (both large-scale, observational social-media analyses and behavioral lab experiments) that investigate which types of moral similarities influence tie formations. Our results indicate that social network processes reflect moral selection, and both online and offline differences in moral purity concerns are particularly predictive of social distance.Dr. Morteza Dehghani is an Assistant Professor of psychology, computer science and the Brain and Creativity Institute at University of Southern California. His research spans the boundary between psychology and artificial intelligence, as does his education. His work investigates properties of cognition by using documents of the social discourse, such as narratives, social media, transcriptions of speeches and news articles, in conjunction to behavioral studies.16",
        "id": 34363067
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper measured how well the source-translation contribution by the translation model can be used to detect its own hallucinations?",
    "positive_ctxs": [
      {
        "title": "Detecting and Mitigating Hallucinations in Machine Translation: Model Internal Workings Alone Do Well, Sentence Similarity Even Better",
        "text": "While the problem of hallucinations in neural machine translation has long been recognized, so far the progress on its alleviation is very little. Indeed, recently it turned out that without artificially encouraging models to hallucinate, previously existing methods fall short and even the standard sequence log-probability is more informative. It means that internal characteristics of the model can give much more information than we expect, and before using external models and measures, we first need to ask: how far can we go if we use nothing but the translation model itself ? We propose to use a method that evaluates the percentage of the source contribution to a generated translation. Intuitively, hallucinations are translations \"detached\" from the source, hence they can be identified by low source contribution. This method improves detection accuracy for the most severe hallucinations by a factor of 2 and is able to alleviate hallucinations at test time on par with the previous best approach that relies on external models. Next, if we move away from internal model characteristics and allow external tools, we show that using sentence similarity from cross-lingual embeddings further improves these results. We release the code of our experiments. 1",
        "id": 254823170
      }
    ],
    "negative_ctxs": [
      {
        "title": "A STATISTICAL APPROACH TO LANGUAGE TRANSLATION",
        "text": "An approach to automatic translation is outlined that utilizes technklues of statistical inl'ormatiml extraction from large data bases. The method is based on the availability of pairs of large corresponding texts that are translations of each other. In our case, the iexts are in English and French. Fundamental to the technique is a complex glossary of correspondence of fixed locutions. The steps of the proposed translation process are: (1) Partition the source text into a set of fixed locutioris. (2) Use the glossary plus coutextual information to select tim corresponding set of fixed Ioctttions into a sequen{e forming the target sentence. (3) Arrange the words of the talget fixed locutions into a sequence forming the target sentence. We have developed stalistical techniques facilitating both tile autonlatic creation of the glossary, and the performance of tile three translation steps, all on the basis of an aliglnncllt of corresponding sentences in tile two texts. While wc are not yet able to provide examples of French / English tcanslation, we present some encouraging intermediate results concerning glossary creation and the arrangement of target WOl'd seq lie)lees.",
        "id": 5216540
      },
      {
        "title": "What Is Going through Your Mind? Metacognitive Events Classification in Human-Agent Interactions",
        "text": "For an agent, either human or artificial, to show intelligent interactive behaviour implies assessments of the reliability of own and others' thoughts, feelings and beliefs. Agents capable of these robust evaluations are able to adequately interpret their own and others' cognitive and emotional processes, anticipate future actions, and improve their decision-making and interactive performances across domains and contexts. Reliable instruments to assess interlocutors' mindful capacities for monitoring and regulation -metacognition -in human-agent interaction in real-time and continuously are of crucial importance however challenging to design. The presented study reports Concurrent Think Aloud (CTA) experiments in order to access and evaluate metacognitive dispositions and attitudes of participants in human-agent interactions. A typology of metacognitive events related to the 'verbalized' monitoring, interpretation, reflection and regulation activities observed in a multimodal dialogue has been designed, and serves as a valid tool to identify relation between participants' behaviour analysed in terms of ISO 24617-2 compliant dialogue acts and the corresponding metacognitive indicators.",
        "id": 252624520
      },
      {
        "title": "A novel Textual Encoding paradigm based on Semantic Web tools and semantics",
        "text": "In this paper we perform a preliminary evaluation on how Semantic Web technologies such as RDF and OWL can be used to perform textual encoding. Among the potential advantages, we notice how RDF, given its conceptual graph structure, appears naturally suited to deal with overlapping hierarchies of annotations, something notoriously problematic using classic XML based markup. To conclude, we show how complex querying can be performed using slight modifications of already existing Semantic Web query tools.",
        "id": 167401
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper first combines rewriting and expansion methods to reformulate a query for conversational search?",
    "positive_ctxs": [
      {
        "title": "ConvGQR: Generative Query Reformulation for Conversational Search",
        "text": "In conversational search, the user's real search intent for the current conversation turn is dependent on the previous conversation history. It is challenging to determine a good search query from the whole conversation context. To avoid the expensive re-training of the query encoder, most existing methods try to learn a rewriting model to de-contextualize the current query by mimicking the manual query rewriting. However, manually rewritten queries are not always the best search queries. Thus, training a rewriting model on them would lead to sub-optimal queries. Another useful information to enhance the search query is the potential answer to the question. In this paper, we propose ConvGQR, a new framework to reformulate conversational queries based on generative pre-trained language models (PLMs), one for query rewriting and another for generating potential answers. By combining both, ConvGQR can produce better search queries. In addition, to relate query reformulation to the retrieval task, we propose a knowledge infusion mechanism to optimize both query reformulation and retrieval. Extensive experiments on four conversational search datasets demonstrate the effectiveness of ConvGQR.",
        "id": 258887946
      }
    ],
    "negative_ctxs": [
      {
        "title": "NLGbAse: a free linguistic resource for Natural Language Processing systems",
        "text": "Availability of labeled language resources, such as annotated corpora and domain dependent labeled language resources is crucial for experiments in the field of Natural Language Processing. Most often, due to lack of resources, manual verification and annotation of electronic text material is a prerequisite for the development of NLP tools. In the context of under-resourced language, the lack of copora becomes a crucial problem because most of the research efforts are supported by organizations with limited funds. Using free, multilingual and highly structured corpora like Wikipedia to produce automatically labeled language resources can be an answer to those needs. This paper introduces NLGbAse, a multilingual linguistic resource built from the Wikipedia encyclopedic content. This system produces structured metadata which make possible the automatic annotation of corpora with syntactical and semantical labels. A metadata contains semantical and statistical informations related to an encyclopedic document. To validate our approach, we built and evaluated a Named Entity Recognition tool, trained with Wikipedia corpora annotated by our system.",
        "id": 14517993
      },
      {
        "title": "NAIST Simultaneous Speech-to-Text Translation System for IWSLT 2022",
        "text": "This paper describes NAIST's simultaneous speech translation systems developed for IWSLT 2022 Evaluation Campaign. We participated the speech-to-speech track for Englishto-German and English-to-Japanese. Our primary submissions were end-to-end systems using adaptive segmentation policies based on Prefix Alignment.",
        "id": 248780175
      },
      {
        "title": "A TAG-derived Database for Treebank Search and Parser Analysis",
        "text": "Recent work has proposed the use of an extracted tree grammar as the basis for treebank analysis, in which queries are stated over the elementary trees, which are small chunks of syntactic structure. In this work we integrate search over the derivation tree with this approach in order to analyze differences between two sets of annotation on the same text, an important problem for parser analysis and evaluation of inter-annotator agreement.",
        "id": 18434088
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which paper first found that when transformers are trained to in-context learn function classes, they might exhibit generalization followed by memorization, in certain settings?",
    "positive_ctxs": [
      {
        "title": "In-Context Learning through the Bayesian Prism",
        "text": "In-context learning is one of the surprising and useful features of large language models. How it works is an active area of research. Recently, stylized meta-learning-like setups have been devised that train these models on a sequence of input-output pairs (x, f (x)) from a function class using the language modeling loss and observe generalization to unseen functions from the same class. One of the main discoveries in this line of research has been that for several problems such as linear regression, trained transformers learn algorithms for learning functions in context. However, the inductive biases of these models resulting in this behavior are not clearly understood. A model with unlimited training data and compute is a Bayesian predictor: it learns the pretraining distribution. It has been shown that high-capacity transformers mimic the Bayesian predictor for linear regression. In this paper, we show empirical evidence of transformers exhibiting the behavior of this ideal learner across different linear and non-linear function classes. We also extend the previous setups to work in the multitask setting and verify that transformers can do in-context learning in this setup as well and the Bayesian perspective sheds light on this setting also. Finally, via the example of learning Fourier series, we study the inductive bias for in-context learning. We find that in-context learning may or may not have simplicity bias depending on the pretraining data distribution. * Equal Contribution arXiv:2306.04891v1 [cs.LG] 8 Jun 2023 w ∼ N (0 d , I). We are particularly interested in the underdetermined region i.e. k < d. Gaussian prior enables explicit PME computation: both PME and maximum a posteriori (MAP) solution agree and are equal to the minimum L 2 -norm solution of the equations forming the training examples i.e. min w ∥w∥ 2 s.t. w T x i = f (x i ), ∀i ≤ k Standard Ordinary Least Squares (OLS) solvers return the minimum L 2 -norm solution, and thus PME and MAP too, in the underdetermined region i.e. k < d. Skewed-Covariance Regression (F Skew-DR ). This setup is similar to dense-regression, except we assume the following prior on weight vector: w ∼ N (0, Σ), where Σ ∈ R d×d is the covariance matrix with eigenvalues proportional to 1/i 2 , where i ∈ [1, d]. For this prior on w, we can use the same (but more general) argument for dense regression above to obtain the PME and MAP which will be equal and can be obtained by minimizing w T Σ −1 w w.r.t to the constraints w T x i = f (x i ). This setup was motivated by Garg et al. [2022], where it was used to sample x i values for out-of-distribution (OOD) evaluation, but not as a prior on w. Sparse Regression (F SR ). In sparse regression, we assume w to be an s-sparse vector in R d i.e. out of its d components only s are non-zero. Following Garg et al. [2022], to sample w for constructing prompts P, we first sample w ∼ N (0 d , I) and then randomly set its d − s components as 0. We consider s = 3 throughout our experiments. While computing the PME appears to be intractable here, the MAP solution can be estimated using Lasso by assuming a Laplacian prior on w Tibshirani [1996].Sign-Vector Regression (F SVR ).Here, we assume w to be a sign vector in {−1, +1} d . For constructing prompts P, we sample d independent Bernoulli random variables b j with a mean of 0.5 and obtain w = [2b 1 − 1, · · · , 2b d − 1] T . While computing the exact PME in this case as well remains intractable, the optimal solution for k > d/2 can be obtained by minimizing the L ∞ norm ∥w∥ ∞ w.r.t. the constraints specified by the input-output examples (w T x i = f (x i )) Mangasarian and Recht [2011]. A specific variation. In general, for the exact recovery of a vector w, the set of all these vectors must satisfy specific convexity conditions Chandrasekaran et al. [2012]. We question if Transformers also require such conditions. To test the same, we define a task F ZR where the convexity conditions are not met and train transformers for regression on this task. Here, w ∈ {z; z | z ∈ {−2, −1, 1, 2} d/2 }, where ; denotes concatenation. Note that the size of this set is 2 d , the same as the size of {−1, 1} d .Low-Rank Regression (F LowRank-DR ).In this case, w is assumed to be a flattened version of a matrix W ∈ R q×q (d = q 2 ) with a rank r, where r ≪ q. A strong baseline, in this case, is to minimize the nuclear norm L * of W i.e. ∥W∥ * subject to constraints w T x i = f (x i ). To sample the rank-r matrix W, we sample A ∼ N (0, 1), s.t. A ∈ R q×r and independently a matrix B of the same shape and distribution, and set W = AB T . Recovery bounds. For each function class above, there is a bound on the minimum number of in-context examples needed for the exact recovery of the solution vector w. The bounds for sparse, sign-vector and low-rank regression are 2s log(d/s) + 5s/4, d/2, and 3r(2q − r) respectively Chandrasekaran et al. [2012].",
        "id": 259108565
      }
    ],
    "negative_ctxs": [
      {
        "title": "Identification of Coreference Between Names and Faces",
        "text": "To retrieve multimedia contents by their meaning, it is necessary to use not only the contents of distinct media, such as image or language, but also a certain semantic relation holding between them. For this purpose, in this paper, we propose a method to find coreferences between human names in the article of newspaper and human faces in the accompanying photograph. The method we proposed is based on the machine learning and the hypothesis driven combining method for identifying names and corresponding faces. Our experimental results show that the recall and precision rate of our method are better than those of the system which uses information exclusively from either text media or image media.",
        "id": 14600913
      },
      {
        "title": "Attentive Mimicking: Better Word Embeddings by Attending to Informative Contexts",
        "text": "Learning high-quality embeddings for rare words is a hard problem because of sparse context information. Mimicking (Pinter et al.,  2017)  has been proposed as a solution: given embeddings learned by a standard algorithm, a model is first trained to reproduce embeddings of frequent words from their surface form and then used to compute embeddings for rare words. In this paper, we introduce attentive mimicking: the mimicking model is given access not only to a word's surface form, but also to all available contexts and learns to attend to the most informative and reliable contexts for computing an embedding. In an evaluation on four tasks, we show that attentive mimicking outperforms previous work for both rare and medium-frequency words. Thus, compared to previous work, attentive mimicking improves embeddings for a much larger part of the vocabulary, including the mediumfrequency range.",
        "id": 102350748
      },
      {
        "title": "Coling 2008: Companion volume -Posters and Demonstrations",
        "text": "Treating classification as seeking minimum cuts in the appropriate graph has proven effective in a number of applications. The power of this approach lies in its ability to incorporate label-agreement preferences among pairs of instances in a provably tractable way. Label disagreement preferences are another potentially rich source of information, but prior NLP work within the minimum-cut paradigm has not explicitly incorporated it. Here, we report on work in progress that examines several novel heuristics for incorporating such information. Our results, produced within the context of a politically-oriented sentiment-classification task, demonstrate that these heuristics allow for the addition of label-disagreement information in a way that improves classification accuracy while preserving the efficiency guarantees of the minimum-cut framework.",
        "id": 1932999
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Can you recommend research that uses an LLM to generate better prompts/tempates given task input/output?",
    "positive_ctxs": [
      {
        "title": "Making Pre-trained Language Models Better Few-shot Learners",
        "text": "The recent GPT-3 model(Brown et al., 2020)achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF-better few-shot fine-tuning of language models 1 -a suite of simple and complementary techniques for finetuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning. 2 * The first two authors contributed equally. 1 Alternatively, language models' best friends forever. 2 Our implementation is publicly available at https:// github.com/princeton-nlp/LM-BFF. Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Association for Computational Linguistics (ACL). Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Association for Computational Linguistics (ACL).",
        "id": 229923710
      }
    ],
    "negative_ctxs": [
      {
        "title": "Fast and Accurate Unlexicalized Parsing via Structural Annotations",
        "text": "We suggest a new annotation scheme for unlexicalized PCFGs that is inspired by formal language theory and only depends on the structure of the parse trees. We evaluate this scheme on the TüBa-D/Z treebank w.r.t. several metrics and show that it improves both parsing accuracy and parsing speed considerably. We also show that our strategy can be fruitfully combined with known ones like parent annotation to achieve accuracies of over 90% labeled F 1 and leaf-ancestor score. Despite increasing the size of the grammar, our annotation allows for parsing more than twice as fast as the PCFG baseline.",
        "id": 15708792
      },
      {
        "title": "",
        "text": "",
        "id": 187484830
      },
      {
        "title": "Data-Driven Sentence Simplification: Survey and Benchmark",
        "text": "Sentence Simplification (SS) aims to modify a sentence in order to make it easier to read and understand. In order to do so, several rewriting transformations can be performed such as replacement, reordering, and splitting. Executing these transformations while keeping sentences grammatical, preserving their main idea, and generating simpler output, is a challenging and still far from solved problem. In this article, we survey research on SS, focusing on approaches that attempt to learn how to simplify using corpora of aligned original-simplified sentence pairs in English, which is the dominant paradigm nowadays. We also include a benchmark of different approaches on common data sets so as to compare them and highlight their strengths and limitations. We expect that this survey will serve as a starting point for researchers interested in the task and help spark new ideas for future developments.",
        "id": 209536754
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that investigates improving retrieval-based conversational systems using by combining masked language modeling and relevance classification objectives?",
    "positive_ctxs": [
      {
        "title": "Fine-grained Post-training for Improving Retrieval-based Dialogue Systems",
        "text": "Retrieval-based dialogue systems display an outstanding performance when pre-trained language models are used, which includes bidirectional encoder representations from transformers (BERT). During the multi-turn response selection, BERT focuses on training the relationship between the context with multiple utterances and the response. However, this method of training is insufficient when considering the relations between each utterance in the context. This leads to a problem of not completely understanding the context flow that is required to select a response. To address this issue, we propose a new fine-grained post-training method that reflects the characteristics of the multi-turn dialogue. Specifically, the model learns the utterance level interactions by training every short context-response pair in a dialogue session. Furthermore, by using a new training objective, the utterance relevance classification, the model understands the semantic relevance and coherence between the dialogue utterances. Experimental results show that our model achieves new state-of-the-art with significant margins on three benchmark datasets. This suggests that the fine-grained post-training method is highly effective for the response selection task. 1",
        "id": 235097662
      }
    ],
    "negative_ctxs": [
      {
        "title": "A HYBRID APPROACH TO ADAPTIVE STATISTICAL LANGUAGE MODELING",
        "text": "We desert'be our latest attempt at adaptive language modeling. At the heart of our approach is a Maximum Entropy (ME) model which inc.orlxnates many knowledge sources in a consistent manner. The other components are a selective unigram cache, a conditional bigram cache, and a conventionalstatic trigram. We describe the knowledge sources used to build such a model with ARPA's official WSJ corpus, and report on perplexity and word error rate results obtained with it. Then, three different adaptation paradigms are discussed, and an additional experiment, based on AP wire data, is used to compare them.",
        "id": 70704
      },
      {
        "title": "Entropy Reduction correlates with temporal lobe activity",
        "text": "Using the Entropy Reduction incremental complexity metric, we relate high gamma power signals from the brains of epileptic patients to incremental stages of syntactic analysis in English and French. We find that signals recorded intracranially from the anterior Inferior Temporal Sulcus (aITS) and the posterior Inferior Temporal Gyrus (pITG) correlate with wordby-word Entropy Reduction values derived from phrase structure grammars for those languages. In the anterior region, this correlation persists even in combination with surprisal co-predictors from PCFG and ngram models. The result confirms the idea that the brain's temporal lobe houses a parsing function, one whose incremental processing difficulty profile reflects changes in grammatical uncertainty.",
        "id": 3915049
      },
      {
        "title": "Published as a conference paper at ICLR 2023 NO REASON FOR NO SUPERVISION: IMPROVED GENERALIZATION IN SUPERVISED MODELS",
        "text": "We consider the problem of training a deep neural network on a given classification task, e.g., ImageNet-1K (IN1K), so that it excels at both the training task as well as at other (future) transfer tasks. These two seemingly contradictory properties impose a trade-off between improving the model's generalization and maintaining its performance on the original task. Models trained with self-supervised learning tend to generalize better than their supervised counterparts for transfer learning; yet, they still lag behind supervised models on IN1K. In this paper, we propose a supervised learning setup that leverages the best of both worlds. We extensively analyze supervised training using multi-scale crops for data augmentation and an expendable projector head, and reveal that the design of the projector allows us to control the trade-off between performance on the training task and transferability. We further replace the last layer of class weights with class prototypes computed on the fly using a memory bank and derive two models: t-ReX that achieves a new state of the art for transfer learning and outperforms top methods such as DINO and PAWS on IN1K, and t-ReX* that matches the highly optimized RSB-A1 model on IN1K while performing better on transfer tasks. Code and pretrained models: https://europe.naverlabs.com/t-rex",
        "id": 257482840
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper first applied the chain-of-thought technique in the text summarization field?",
    "positive_ctxs": [
      {
        "title": "Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method",
        "text": "Automatic summarization generates concise summaries that contain key ideas of source documents. As the most mainstream datasets for the news sub-domain, CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be noisy, mainly in terms of factual hallucination and information redundancy. To address this challenge, we first annotate new expertwriting Element-aware test sets following the \"Lasswell Communication Model\" proposed by Lasswell (1948), allowing reference summaries to focus on more fine-grained news elements objectively and comprehensively. Utilizing the new test sets, we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMs' zero-shot summaries in prior work. Further, we propose a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more finegrained details of source documents into the final summaries that correlate with the human writing mindset. Experimental results show our method outperforms state-of-the-art fine-tuned PLMs and zero-shot LLMs by +4.33/+4.77 in ROUGE-L on the two datasets, respectively. Dataset and code are publicly available at https://github.com/Alsace08/SumCoT.",
        "id": 258841145
      }
    ],
    "negative_ctxs": [
      {
        "title": "Machine Translation: A Cognitive Linguistics Approach",
        "text": "This paper describes a novel approach to the construction of an interlinguabased MT system. This approach emerges from the developing field of Cognitive Linguistics. The objective of this paper is to address one of the main problems in MT systems (besides the source text ambiguity problem): the apparent complexity and irregularity of translation data. The author suggests that the key to a coherent account for some of the over-diversified translation data lies in the integration of an additional knowledge-base into the translation system: the knowledge of language usage. This knowledge is not part of the structure of language itself but derives from cognitive mechanisms which language acts upon. Cognitive Linguistics introduces a framework which gives a central role, in any understanding of semantics, to various kinds of schemas which are triggered by the language but which are not an explicit part of language itself. The paper examines some intriguing examples of English grammatical structures and their diversified translation into Hebrew. The analysis shows that by extracting the right schema from the source text, we can generate a better representation of the text's meaning which leads to easier and more accurate generation of the target text.",
        "id": 18090758
      },
      {
        "title": "Dependency Parsing of Japanese Spoken Monologue Based on Clause Boundaries",
        "text": "Spoken monologues feature greater sentence length and structural complexity than do spoken dialogues. To achieve high parsing performance for spoken monologues, it could prove effective to simplify the structure by dividing a sentence into suitable language units. This paper proposes a method for dependency parsing of Japanese monologues based on sentence segmentation. In this method, the dependency parsing is executed in two stages: at the clause level and the sentence level. First, the dependencies within a clause are identified by dividing a sentence into clauses and executing stochastic dependency parsing for each clause. Next, the dependencies over clause boundaries are identified stochastically, and the dependency structure of the entire sentence is thus completed. An experiment using a spoken monologue corpus shows this method to be effective for efficient dependency parsing of Japanese monologue sentences.",
        "id": 14156544
      },
      {
        "title": "A Graph Kernel for Protein-Protein Interaction Extraction",
        "text": "In this paper, we propose a graph kernel based approach for the automated extraction of protein-protein interactions (PPI) from scientific literature. In contrast to earlier approaches to PPI extraction, the introduced alldependency-paths kernel has the capability to consider full, general dependency graphs. We evaluate the proposed method across five publicly available PPI corpora providing the most comprehensive evaluation done for a machine learning based PPI-extraction system. Our method is shown to achieve state-of-theart performance with respect to comparable evaluations, achieving 56.4 F-score and 84.8 AUC on the AImed corpus. Further, we identify several pitfalls that can make evaluations of PPI-extraction systems incomparable, or even invalid. These include incorrect crossvalidation strategies and problems related to comparing F-score results achieved on different evaluation resources.",
        "id": 7725084
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "I'm using Local SGD with a decaying learning rate for distributed training. Which paper offers guidance on setting the synchronization period in Local SGD to optimize test accuracy?",
    "positive_ctxs": [
      {
        "title": "A QUADRATIC SYNCHRONIZATION RULE FOR DISTRIBUTED DEEP LEARNING",
        "text": "In distributed deep learning with data parallelism, synchronizing gradients at each training step can cause a huge communication overhead, especially when many nodes work together to train large models.Local gradient methods, such as Local SGD, address this issue by allowing workers to compute locally for H steps without synchronizing with others, hence reducing communication frequency.While H has been viewed as a hyperparameter to trade optimization efficiency for communication cost, recent research indicates that setting a proper H value can lead to generalization improvement.Yet, selecting a proper H is elusive.This work proposes a theory-grounded method for determining H, named the Quadratic Synchronization Rule (QSR), which recommends dynamically setting H in proportion to 1 η 2 as the learning rate η decays over time.Extensive ImageNet experiments on ResNet and ViT show that local gradient methods with QSR consistently improve the test accuracy over other synchronization strategies. 1 Compared with the standard data parallel training, QSR enables Local AdamW on ViT-B to cut the training time on 16 or 64 GPUs down from 26.7 to 20.2 hours or from 8.6 to 5.5 hours and, at the same time, achieves 1.16% or 0.84% higher top-1 validation accuracy.",
        "id": 264426013
      }
    ],
    "negative_ctxs": [
      {
        "title": "Combining Resources for Open Source Machine Translation",
        "text": "In this paper, we present a Japanese→English machine translation system that combines rule-based and statistical translation. Our system is unique in that all of its components are freely available as open source software. We describe the development of the rule-based translation engine including transfer rule acquisition from an open bilingual dictionary. We also show how translations from both translation engines are combined through a simple ranking mechanism and compare their outputs.",
        "id": 35316263
      },
      {
        "title": "Joint Arabic Segmentation and Part-Of-Speech Tagging",
        "text": "Arabic has a very co mp lex morphological system, though a very structured one. Character patterns are often indicative of word class and word segmentation. In this paper, we e xplore a novel approach to Arabic word segmentation and part-of-speech tagging relying on character info rmation. The approach is lexicon-free and does not require any morphological analysis, eliminat ing the factor of dictionary coverage. Using character-based analysis, the developed system yielded stateof-the-art accuracy comparing favourably with other taggers that involve external resources.",
        "id": 16150164
      },
      {
        "title": "ANOTHER LOOK AT NOMINAL COMPOUNDS",
        "text": "We present a progress report on our research on nominal compounds (NC's). Recent approaches to this probiem in linguistics and natural ianguage processing (NLP) are reviewed and criticized. We argue that the notion of \"roie nominal\", which is at the interface of linguistic and extraiinguistic knowledge, is crucial for characterizing NC'e as weII as other Iinguistic phenomena. We examine a number of constraints on the semantic interpretation ruies for NC's. Proposals are made that shouid improve the capability of NLP systems to deaI with NC's.",
        "id": 15803406
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Are there any studies investigating example-based approaches to predict user intent in few-shot learning contexts?",
    "positive_ctxs": [
      {
        "title": "CONVFIT: Conversational Fine-Tuning of Pretrained Language Models",
        "text": "Transformer-based language models (LMs) pretrained on large text collections are proven to store a wealth of semantic knowledge. However, 1) they are not effective as sentence encoders when used off-the-shelf, and 2) thus typically lag behind conversationally pretrained (e.g., via response selection) encoders on conversational tasks such as intent detection (ID). In this work, we propose CON-VFIT, a simple and efficient two-stage procedure which turns any pretrained LM into a universal conversational encoder (after Stage 1 CONVFIT-ing) and task-specialised sentence encoder (after Stage 2). We demonstrate that 1) full-blown conversational pretraining is not required, and that LMs can be quickly transformed into effective conversational encoders with much smaller amounts of unannotated data; 2) pretrained LMs can be fine-tuned into task-specialised sentence encoders, optimised for the fine-grained semantics of a particular task. Consequently, such specialised sentence encoders allow for treating ID as a simple semantic similarity task based on interpretable nearest neighbours retrieval. We validate the robustness and versatility of the CON-VFIT framework with such similarity-based inference on the standard ID evaluation sets: CONVFIT-ed LMs achieve state-of-the-art ID performance across the board, with particular gains in the most challenging, few-shot setups. ConvFiT: Stage 1 (Behavioral) fine-tuning on Reddit data Stage 2 loss (xi, xj) = (senti, sentj)",
        "id": 237581476
      },
      {
        "title": "Example-Driven Intent Prediction with Observers",
        "text": "A key challenge of dialog systems research is to effectively and efficiently adapt to new domains. A scalable paradigm for adaptation necessitates the development of generalizable models that perform well in few-shot settings. In this paper, we focus on the intent classification problem which aims to identify user intents given utterances addressed to the dialog system. We propose two approaches for improving the generalizability of utterance classification models: (1) observers and (2) example-driven training. Prior work has shown that BERT-like models tend to attribute a significant amount of attention to the [CLS]   token, which we hypothesize results in diluted representations. Observers are tokens that are not attended to, and are an alternative to the [CLS] token as a semantic representation of utterances. Example-driven training learns to classify utterances by comparing to examples, thereby using the underlying encoder as a sentence similarity model. These methods are complementary; improving the representation through observers allows the example-driven model to better measure sentence similarities. When combined, the proposed methods attain state-of-the-art results on three intent prediction datasets (BANKING77, CLINC150,  HWU64)  in both the full data and few-shot (10 examples per intent) settings. Furthermore, we demonstrate that the proposed approach can transfer to new intents and across datasets without any additional training.",
        "id": 224725731
      }
    ],
    "negative_ctxs": [
      {
        "title": "RECASTING GRADIENT-BASED META-LEARNING AS HIERARCHICAL BAYES",
        "text": "Meta-learning allows an intelligent agent to leverage prior learning episodes as a basis for quickly improving performance on a novel task. Bayesian hierarchical modeling provides a theoretical framework for formalizing meta-learning as inference for a set of parameters that are shared across tasks. Here, we reformulate the model-agnostic meta-learning algorithm (MAML) of Finn et al.(2017)as a method for probabilistic inference in a hierarchical Bayesian model. In contrast to prior methods for meta-learning via hierarchical Bayes, MAML is naturally applicable to complex function approximators through its use of a scalable gradient descent procedure for posterior inference. Furthermore, the identification of MAML as hierarchical Bayes provides a way to understand the algorithm's operation as a meta-learning procedure, as well as an opportunity to make use of computational strategies for efficient inference. We use this opportunity to propose an improvement to the MAML algorithm that makes use of techniques from approximate inference and curvature estimation.",
        "id": 3484654
      },
      {
        "title": "Dependency parser demo",
        "text": "",
        "id": 30964362
      },
      {
        "title": "SYMMETRIC PROJECTION IN JAPANESE AND ENGLISH: A MODIFICATION OF STABLER'S PARSING/GENERATION MODEL FOR MINIMALIST GRAMMAR",
        "text": "ABSTRCATThe essence of standard X-bar theory is that structure building is asymmetric in the sense that a complex structure inherits properties from only one of its constituents. There are some structures, however, that are best analyzed as reflecting the properties of all their constituents. This kind of symmetric projection should in principle be allowed within the minimalist program if the union of the features of all the constituents contains no incompatible features. This claim is supported by the fact that Japanese wh-phrases marked with ka can function as indefinites as well as interrogatives. Under the assumptions that a wh-phrase with ka has the same internal structure regardless of its interpretations and that ka has no category feature, merging a wh-phrase with ka is a case of symmetric projection. The properties of both ka and its sister wh-phrase interact with those of the predicate taking the ka-phrase as its argument or adjunct, which ensures that an appropriate interpretation will be picked up from the two possible interpretations of the ka-phrase.",
        "id": 14076419
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Which work shows that reducing the number of training epochs effectively limits the impact of backdoor attack, but the method decreases the prediction accuracy?",
    "positive_ctxs": [
      {
        "title": "Concealed Data Poisoning Attacks on NLP Models",
        "text": "Adversarial attacks alter NLP model predictions by perturbing test-time inputs. However, it is much less understood whether, and how, predictions can be manipulated with small, concealed changes to the training data. In this work, we develop a new data poisoning attack that allows an adversary to control model predictions whenever a desired trigger phrase is present in the input. For instance, we insert 50 poison examples into a sentiment model's training set that causes the model to frequently predict Positive whenever the input contains \"James Bond\". Crucially, we craft these poison examples using a gradient-based procedure so that they do not mention the trigger phrase. We also apply our poison attack to language modeling (\"Apple iPhone\" triggers negative generations) and machine translation (\"iced coffee\" mistranslated as \"hot coffee\"). We conclude by proposing three defenses that can mitigate our attack at some cost in prediction accuracy or extra human annotation.",
        "id": 233230124
      }
    ],
    "negative_ctxs": [
      {
        "title": "Accelerating NMT Batched Beam Decoding with LMBR Posteriors for Deployment",
        "text": "We describe a batched beam decoding algorithm for NMT with LMBR n-gram posteriors, showing that LMBR techniques still yield gains on top of the best recently reported results with Transformers. We also discuss acceleration strategies for deployment, and the effect of the beam size and batching on memory and speed.",
        "id": 13742419
      },
      {
        "title": "A Constraint Programming Approach to Probabilistic Syntactic Processing",
        "text": "",
        "id": 5066372
      },
      {
        "title": "Correcting Comma Errors in Learner Essays, and Restoring Commas in Newswire Text",
        "text": "While the field of grammatical error detection has progressed over the past few years, one area of particular difficulty for both native and non-native learners of English, comma placement, has been largely ignored. We present a system for comma error correction in English that achieves an average of 89% precision and 25% recall on two corpora of unedited student essays. This system also achieves state-of-theart performance in the sister task of restoring commas in well-formed text. For both tasks, we show that the use of novel features which encode long-distance information improves upon the more lexically-driven features used in prior work.",
        "id": 7894256
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there any work that attacks language models in dialogue generation?",
    "positive_ctxs": [
      {
        "title": "White-Box Multi-Objective Adversarial Attack on Dialogue Generation",
        "text": "Pre-trained transformers are popular in stateof-the-art dialogue generation (DG) systems. Such language models are, however, vulnerable to various adversarial samples as studied in traditional tasks such as text classification, which inspires our curiosity about their robustness in DG systems. One main challenge of attacking DG models is that perturbations on the current sentence can hardly degrade the response accuracy because the unchanged chat histories are also considered for decision-making. Instead of merely pursuing pitfalls of performance metrics such as BLEU, ROUGE, we observe that crafting adversarial samples to force longer generation outputs benefits attack effectiveness-the generated responses are typically irrelevant, lengthy, and repetitive. To this end, we propose a white-box multi-objective attack method called DGSlow. Specifically, DGSlow balances two objectives-generation accuracy and length, via a gradient-based multiobjective optimizer and applies an adaptive searching mechanism to iteratively craft adversarial samples with only a few modifications. Comprehensive experiments 1 on four benchmark datasets demonstrate that DGSlow could significantly degrade state-of-the-art DG models with a higher success rate than traditional accuracy-based methods. Besides, our crafted sentences also exhibit strong transferability in attacking other models.",
        "id": 258546855
      }
    ],
    "negative_ctxs": [
      {
        "title": "AUTO-ENCODING SEQUENTIAL MONTE CARLO",
        "text": "We build on auto-encoding sequential Monte Carlo (AESMC): 1 a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models. Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions. We develop additional theoretical insights and experiment with a new training procedure which can improve both model and proposal learning. We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.",
        "id": 20140417
      },
      {
        "title": "The Centre and Periphery of Discourse Connectives",
        "text": "The paper tries to contribute to the general definition of discourse connectives. It examines connectives in broader sense, i.e. all language expressions that have an ability to express discourse relations within a text (e.g. both conjunctions like but, and, or and expressions like the condition for this is, due to this situation etc.). The paper tries to classify connectives from different perspectives and to divide them into several groups to specify their similarities and differences. We try to discuss various attributes an expression must have to be a connective. We understand discourse connectives as a set of expressions with a center and periphery and we focus here mainly on the periphery -i.e. on description of the secondary connectives (like the reason is simple, this means that... etc.) because it is not much investigated but a very current theme of discourse analysis.Discourse Connectives in the Prague Dependency TreebankOur research on discourse connectives in Czech was carried out on the data of the Prague Dependency Treebank (PDT) -a manually annotated corpus of about 50 thousand sentences from newspaper texts containing, among others, annotation of discourse relations.",
        "id": 9473711
      },
      {
        "title": "Using J-K-fold Cross Validation to Reduce Variance When Tuning NLP Models",
        "text": "K-fold cross validation (CV) is a popular method for estimating the true performance of machine learning models, allowing model selection and parameter tuning. However, the very process of CV requires random partitioning of the data and so our performance estimates are in fact stochastic, with variability that can be substantial for natural language processing tasks. We demonstrate that these unstable estimates cannot be relied upon for effective parameter tuning. The resulting tuned parameters are highly sensitive to how our data is partitioned, meaning that we often select sub-optimal parameter choices and have serious reproducibility issues.Instead, we propose to use the less variable J-K-fold CV, in which J independent K-fold cross validations are used to assess performance. Our main contributions are extending J-K-fold CV from performance estimation to parameter tuning and investigating how to choose J and K. We argue that variability is more important than bias for effective tuning and so advocate lower choices of K than are typically seen in the NLP literature, instead use the saved computation to increase J. To demonstrate the generality of our recommendations we investigate a wide range of case-studies: sentiment classification (both general and target-specific), part-of-speech tagging and document classification.",
        "id": 49311178
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "What paper investigated the effect of the relative position (closer or further away) of the most pertinent retrieved code snippets on repository-level code completion performance?",
    "positive_ctxs": [
      {
        "title": "RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems",
        "text": "Large Language Models (LLMs) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers.However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios.To fill this gap, we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems.RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline).Each task respectively measures the system's ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction.RepoBench aims to facilitate a more complete comparison of performance and encouraging continuous improvement in auto-completion systems.",
        "id": 259075246
      }
    ],
    "negative_ctxs": [
      {
        "title": "Sentiment Propagation via Implicature Constraints",
        "text": "Opinions may be expressed implicitly via inference over explicit sentiments and events that positively/negatively affect entities (goodFor/badFor events). We investigate how such inferences may be exploited to improve sentiment analysis, given goodFor/badFor event information. We apply Loopy Belief Propagation to propagate sentiments among entities. The graph-based model improves over explicit sentiment classification by 10 points in precision and, in an evaluation of the model itself, we find it has an 89% chance of propagating sentiments correctly.",
        "id": 15146734
      },
      {
        "title": "Are Larger Pretrained Language Models Uniformly Better? Comparing Performance at the Instance Level",
        "text": "Larger language models have higher accuracy on average, but are they better on every single instance (datapoint)? Some work suggests larger models have higher out-ofdistribution robustness, while other work suggests they have lower accuracy on rare subgroups. To understand these differences, we investigate these models at the level of individual instances. However, one major challenge is that individual predictions are highly sensitive to noise in the randomness in training. We develop statistically rigorous methods to address this, and after accounting for pretraining and finetuning noise, we find that our BERT-LARGE is worse than BERT-MINI on at least 1−4% of instances across MNLI, SST-2, and QQP, compared to the overall accuracy improvement of 2−10%. We also find that finetuning noise increases with model size, and that instance-level accuracy has momentum: improvement from BERT-MINI to BERT-MEDIUM correlates with improvement from BERT-MEDIUM to BERT-LARGE . Our findings suggest that instance-level predictions provide a rich source of information; we therefore recommend that researchers supplement model weights with model predictions.",
        "id": 234482939
      },
      {
        "title": "Generating Pattern-Based Entailment Graphs for Relation Extraction",
        "text": "Relation extraction is the task of recognizing and extracting relations between entities or concepts in texts. A common approach is to exploit existing knowledge to learn linguistic patterns expressing the target relation and use these patterns for extracting new relation mentions. Deriving relation patterns automatically usually results in large numbers of candidates, which need to be filtered to derive a subset of patterns that reliably extract correct relation mentions. We address the pattern selection task by exploiting the knowledge represented by entailment graphs, which capture semantic relationships holding among the learned pattern candidates. This is motivated by the fact that a pattern may not express the target relation explicitly, but still be useful for extracting instances for which the relation holds, because its meaning entails the meaning of the target relation. We evaluate the usage of both automatically generated and gold-standard entailment graphs in a relation extraction scenario and present favorable experimental results, exhibiting the benefits of structuring and selecting patterns based on entailment graphs.",
        "id": 6159455
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "What are some scholarly articles that explore the enhancement of dense retrieval in student models through the application of prediction distributions from teacher models?",
    "positive_ctxs": [
      {
        "title": "ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction",
        "text": "Neural information retrieval (IR) has greatly advanced search and other knowledgeintensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6-10×.",
        "id": 244799249
      }
    ],
    "negative_ctxs": [
      {
        "title": "Trans-disciplinary spoken language processing studies for scientific understanding of second language learner's characteristics",
        "text": "Language and speech proficiency is considered to be an important factor to identify human beings. Though traditional studies on language and speech can reveal many aspects of their characteristics, we have not yet had a complete view of human's language and speech ability. I believe that trans-disciplinary studies will enable us to have its scientific modeling. In this talk, I would like to introduce our research efforts on segmental duration control as an example of research towards computational human modeling. The computational modeling of segmental duration that we have been studying around three decades not only contributes to prosody control in speech synthesis technology but also gives an integrated view of individual timing characteristics studied in phonetic science. Together with duration control modeling, a series of perceptual studies on duration modifications needed for model evaluation have suggested us a unified view of scientific understanding on rhythm and timing. Through the introduction of our current efforts on the objective evaluation of 2nd language (L2) proficiency in speech timing control, we will see that these models and findings are useful for L2 learning and acquisition. To conclude my talk, I finally introduce research consortium called AESOP (Asian English Speech cOrpus Project) where researchers in different fields (speech science, informatics, phonetics, psychology and language education) have started to work together by collecting commonly sharable L2 language and speech data.5",
        "id": 30820840
      },
      {
        "title": "",
        "text": "",
        "id": 236779173
      },
      {
        "title": "A Practical of Memory-based Approach for Improving Accuracy of MT",
        "text": "Rule-Based Machine Translation (RBMT)[1] approach is a major approach in MT research. It needs linguistic knowledge to create appropriate rules of translation. However, we cannot completely add all linguistic rules to the system because adding new rules may cause a conflict with the old ones. So, we propose a memory based approach to improve the translation quality without modifying the existing linguistic rules. This paper analyses the translation problems and shows how this approach works.",
        "id": 27010733
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a study that examines how incorporating external commonsense knowledge into conversational agents can better interpret user emotions and refine their response formulation techniques?",
    "positive_ctxs": [
      {
        "title": "MISC: A MIxed Strategy-Aware Model Integrating COMET for Emotional Support Conversation",
        "text": "Applying existing methods to emotional support conversation-which provides valuable assistance to people who are in need-has two major limitations: (a) they generally employ a conversation-level emotion label, which is too coarse-grained to capture user's instant mental state; (b) most of them focus on expressing empathy in the response(s) rather than gradually reducing user's distress. To address the problems, we propose a novel model MISC, which firstly infers the user's fine-grained emotional status, and then responds skillfully using a mixture of strategy. Experimental results on the benchmark dataset demonstrate the effectiveness of our method and reveal the benefits of fine-grained emotion understanding as well as mixed-up strategy modeling. Our code and data could be found in https: //github.com/morecry/MISC.",
        "id": 247748640
      }
    ],
    "negative_ctxs": [
      {
        "title": "Making Computers Laugh: Investigations in Automatic Humor Recognition",
        "text": "Humor is one of the most interesting and puzzling aspects of human behavior. Despite the attention it has received in fields such as philosophy, linguistics, and psychology, there have been only few attempts to create computational models for humor recognition or generation. In this paper, we bring empirical evidence that computational approaches can be successfully applied to the task of humor recognition. Through experiments performed on very large data sets, we show that automatic classification techniques can be effectively used to distinguish between humorous and non-humorous texts, with significant improvements observed over apriori known baselines.",
        "id": 5766640
      },
      {
        "title": "Extended HMM and Ranking models for Chinese Spelling Correction",
        "text": "Spelling correction has been studied for many decades, which can be classified into two categories: (1) regular text spelling correction, (2) query spelling correction. Although the two tasks share many common techniques, they have different concerns. This paper presents our work on the CLP-2014 bake-off. The task focuses on spelling checking on foreigner Chinese essays. Compared to online search query spelling checking task, more complicated techniques can be applied for better performance. Therefore, we proposed a unified framework for Chinese essays spelling correction based on extended HMM and ranker-based models, together with a rule-based model for further polishing. Our system showed better performance on the test dataset.",
        "id": 13253132
      },
      {
        "title": "Published as a conference paper at ICLR 2023 CERTIFIED TRAINING: SMALL BOXES ARE ALL YOU NEED",
        "text": "To obtain, deterministic guarantees of adversarial robustness, specialized training methods are used. We propose, SABR, a novel such certified training method, based on the key insight that propagating interval bounds for a small but carefully selected subset of the adversarial input region is sufficient to approximate the worst-case loss over the whole region while significantly reducing approximation errors. We show in an extensive empirical evaluation that SABR outperforms existing certified defenses in terms of both standard and certifiable accuracies across perturbation magnitudes and datasets, pointing to a new class of certified training methods promising to alleviate the robustness-accuracy trade-off.",
        "id": 252780995
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you direct me to research that evaluates few-shot slot tagging model performance by averaging micro-F1 scores across different test episodes?",
    "positive_ctxs": [
      {
        "title": "Inverse is Better! Fast and Accurate Prompt for Few-shot Slot Tagging",
        "text": "Prompting methods recently achieve impressive success in few-shot learning. These methods modify input samples with prompt sentence pieces, and decode label tokens to map samples to corresponding labels. However, such a paradigm is very inefficient for the task of slot tagging. Since slot tagging samples are multiple consecutive words in a sentence, the prompting methods have to enumerate all n-grams token spans to find all the possible slots, which greatly slows down the prediction. To tackle this, we introduce an inverse paradigm for prompting. Different from the classic prompts mapping tokens to labels, we reversely predict slot values given slot types. Such inverse prompting only requires a oneturn prediction for each slot type and greatly speeds up the prediction. Besides, we propose a novel Iterative Prediction Strategy, from which the model learns to refine predictions by considering the relations between different slot types. We find, somewhat surprisingly, the proposed method not only predicts faster but also significantly improves the effect (improve over 6.1 F1-scores on 10-shot setting) and achieves new state-of-the-art performance.",
        "id": 247939641
      }
    ],
    "negative_ctxs": [
      {
        "title": "An Analysis of Active Learning Strategies for Sequence Labeling Tasks",
        "text": "Active learning is well-suited to many problems in natural language processing, where unlabeled data may be abundant but annotation is slow and expensive. This paper aims to shed light on the best active learning approaches for sequence labeling tasks such as information extraction and document segmentation. We survey previously used query selection strategies for sequence models, and propose several novel algorithms to address their shortcomings. We also conduct a large-scale empirical comparison using multiple corpora, which demonstrates that our proposed methods advance the state of the art.",
        "id": 8197231
      },
      {
        "title": "GPLSI-IXA: Using Semantic Classes to Acquire Monosemous Training Examples from Domain Texts",
        "text": "This paper summarizes our participation in task #17 of SemEval-2 (All-words WSD on a specific domain) using a supervised class-based Word Sense Disambiguation system. Basically, we use Support Vector Machines (SVM) as learning algorithm and a set of simple features to build three different models. Each model considers a different training corpus: Sem-Cor (SC), examples from monosemous words extracted automatically from background data (BG), and both SC and BG (SCBG). Our system explodes the monosemous words appearing as members of a particular WordNet semantic class to automatically acquire class-based annotated examples from the domain text. We use the class-based examples gathered from the domain corpus to adapt our traditional system trained on SemCor. The evaluation reveal that the best results are achieved training with SemCor and the background examples from monosemous words, obtaining results above the first sense baseline and the fifth best position in the competition rank.",
        "id": 17053517
      },
      {
        "title": "Hate-Speech and Offensive Language Detection in Roman Urdu",
        "text": "The task of automatic hate-speech and offensive language detection in social media content is of utmost importance due to its implications in unprejudiced society concerning race, gender, or religion. Existing research in this area, however, is mainly focused on the English language, limiting the applicability to particular demographics. Despite its prevalence, Roman Urdu (RU) lacks language resources, annotated datasets, and language models for this task. In this study, we: (1) Present a lexicon of hateful words in RU, (2) Develop an annotated dataset called RUHSOLD consisting of 10, 012 tweets in RU with both coarse-grained and fine-grained labels of hate-speech and offensive language, (3) Explore the feasibility of transfer learning of five existing embedding models to RU, (4) Propose a novel deep learning architecture called CNN-gram for hatespeech and offensive language detection and compare its performance with seven current baseline approaches on RUHSOLD dataset, and (5) Train domain-specific embeddings on more than 4.7 million tweets and make them publicly available. We conclude that transfer learning is more beneficial as compared to training embedding from scratch and that the proposed model exhibits greater robustness as compared to the baselines.",
        "id": 226262272
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "I'm looking into the distillation process of language models and would like to examine studies that specifically discuss the attention mechanism alignment in the teacher-student model architecture. Are there any papers you can suggest?",
    "positive_ctxs": [
      {
        "title": "MINILMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers",
        "text": "We generalize deep self-attention distillation in MINILM (Wang et al., 2020) by only using self-attention relation distillation for taskagnostic compression of pretrained Transformers. In particular, we define multi-head selfattention relations as scaled dot-product between the pairs of query, key, and value vectors within each self-attention module. Then we employ the above relational knowledge to train the student model. Besides its simplicity and unified principle, more favorably, there is no restriction in terms of the number of student's attention heads, while most previous work has to guarantee the same head number between teacher and student. Moreover, the fine-grained self-attention relations tend to fully exploit the interaction knowledge learned by Transformer. In addition, we thoroughly examine the layer selection strategy for teacher models, rather than just relying on the last layer as in MINILM. We conduct extensive experiments on compressing both monolingual and multilingual pretrained models. Experimental results demonstrate that our models 1 distilled from base-size and large-size teachers (BERT, RoBERTa and XLM-R) outperform the state-of-the-art.",
        "id": 229923069
      }
    ],
    "negative_ctxs": [
      {
        "title": "Innovators@SMM4H'22: An Ensembles Approach for self-reporting of COVID-19 Vaccination Status Tweets",
        "text": "With the Surge in COVID-19, the number of social media postings related to the vaccine has grown, specifically tracing the confirmed reports by the users regarding the COVID-19 vaccine dose termed as Vaccine Surveillance.To mitigate this research problem, we present our novel ensembled approach for self-reporting COVID-19 vaccination status tweets into two labels, namely Vaccine Chatter and Self Report. We utilize state-of-the-art models, namely BERT, RoBERTa, and XLNet. Our model provides promising results with 0.77, 0.93, and 0.66 as precision, recall, and F1-score (respectively), comparable to the corresponding median scores of 0.77, 0.9, and 0.68 (respectively). The model gave an overall accuracy of 93.43. We also present an empirical analysis of the results to present how well the tweet was able to classify and report. We release our code base here https://github.com/Zohair0209/ SMM4H-2022-Task6.git",
        "id": 252818969
      },
      {
        "title": "Improving word alignment for low resource languages using English monolingual SRL",
        "text": "We introduce a new statistical machine translation approach specifically geared to learning translation from low resource languages, that exploits monolingual English semantic parsing to bias inversion transduction grammar (ITG) induction. We show that in contrast to conventional statistical machine translation (SMT) training methods, which rely heavily on phrase memorization, our approach focuses on learning bilingual correlations that help translating low resource languages, by using the output language semantic structure to further narrow down ITG constraints. This approach is motivated by previous research which has shown that injecting a semantic frame based objective function while training SMT models improves the translation quality. We show that including a monolingual semantic objective function during the learning of the translation model leads towards a semantically driven alignment which is more efficient than simply tuning loglinear mixture weights against a semantic frame based evaluation metric in the final stage of statistical machine translation training. We test our approach with three different language pairs and demonstrate that our model biases the learning towards more semantically correct alignments. Both GIZA++ and ITG based techniques fail to capture meaningful bilingual constituents, which is required when trying to learn translation models for low resource languages. In contrast, our proposed model not only improve translation by injecting a monolingual objective function to learn bilingual correlations during early training of the translation model, but also helps to learn more meaningful correlations with a relatively small data set, leading to a better alignment compared to either conventional ITG or traditional GIZA++ based approaches.",
        "id": 10060918
      },
      {
        "title": "Ordering Phrases with Function Words",
        "text": "This paper presents a Function Word centered, Syntax-based (FWS) solution to address phrase ordering in the context of statistical machine translation (SMT). Motivated by the observation that function words often encode grammatical relationship among phrases within a sentence, we propose a probabilistic synchronous grammar to model the ordering of function words and their left and right arguments. We improve phrase ordering performance by lexicalizing the resulting rules in a small number of cases corresponding to function words. The experiments show that the FWS approach consistently outperforms the baseline system in ordering function words' arguments and improving translation quality in both perfect and noisy word alignment scenarios.",
        "id": 390966
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which work proposes an approach to improve candidate responses in the smart reply task by directly optimizing the metric to ensure that a response is selected by the user?",
    "positive_ctxs": [
      {
        "title": "Model-Based Simulation for Optimising Smart Reply",
        "text": "Smart Reply (SR) systems present a user with a set of replies, of which one can be selected in place of having to type out a response. To perform well at this task, a system should be able to effectively present the user with a diverse set of options, to maximise the chance that at least one of them conveys the user's desired response. This is a significant challenge, due to the lack of datasets containing sets of responses to learn from. Resultantly, previous work has focused largely on post-hoc diversification, rather than explicitly learning to predict sets of responses. Motivated by this problem, we present a novel method SIMSR, that employs model-based simulation to discover high-value response sets, through simulating possible user responses with a learned world model. Unlike previous approaches, this allows our method to directly optimise the end-goal of SR-maximising the relevance of at least one of the predicted replies. Empirically on two public datasets, when compared to SoTA baselines, our method achieves up to 21% and 18% improvement in ROUGE score and Self-ROUGE score respectively.",
        "id": 258947156
      }
    ],
    "negative_ctxs": [
      {
        "title": "Japanese Named Entity Recognition from Automatic Speech Recognition Using Pre-trained Models",
        "text": "Japanese named entities extracted from automatic speech recognition frequently contain speech recognition errors and unknown named entities due to abbreviations and aliases. One possible solution to this problem of the named entity extraction task is to use a pre-trained model trained on a large quantity of text to acquire various contextual information. In this study, we performed named entity recognition on the logs of a task-oriented dialogue system for road traffic information in Fukui, Japan, using pre-trained BERT-based models and T5. In our experiments using our prepared data, the F1 scores of BERT and T5 are higher than that of string match by 20.2 point and 21.1 points, respectively. The results confirmed that these pre-trained models exhibited significantly higher accuracies on unseen entities than methods based on dictionary matching.",
        "id": 258463954
      },
      {
        "title": "EXPRES Corpus for A Field-specific Automated Exploratory Study of L2 English Expert Scientific Writing",
        "text": "Field Specific Expert Scientific Writing in English as a Lingua Franca is essential for the effective research networking and dissemination worldwide. Extracting the linguistic profile of the research articles written in L2 English can help young researchers and expert scholars in various disciplines adapt to the scientific writing norms of their communities of practice. In this exploratory study, we present and test an automated linguistic assessment model that includes features relevant for the cross-disciplinary second language framework: Text Complexity Analysis features, such as Syntactic and Lexical Complexity, and Field Specific Academic Word Lists. We analyse how these features vary across four disciplinary fields (Economics, IT, Linguistics and Political Science) in a corpus of L2-English Expert Scientific Writing, part of the EXPRES corpus (Corpus of Expert Writing in Romanian and English). The variation in field specific writing is also analysed in groups of linguistic features extracted from the higher visibility (Hv) versus lower visibility (Lv) journals. After applying lexical sophistication, lexical variation and syntactic complexity formulae, significant differences between disciplines were identified, mainly that research articles from Lv journals have higher lexical complexity, but lower syntactic complexity than articles from Hv journals; while academic vocabulary proved to have discipline specific variation.",
        "id": 252460901
      },
      {
        "title": "Multi-Class Confidence Weighted Algorithms",
        "text": "The recently introduced online confidence-weighted (CW) learning algorithm for binary classification performs well on many binary NLP tasks. However, for multi-class problems CW learning updates and inference cannot be computed analytically or solved as convex optimization problems as they are in the binary case. We derive learning algorithms for the multi-class CW setting and provide extensive evaluation using nine NLP datasets, including three derived from the recently released New York Times corpus. Our best algorithm outperforms state-of-the-art online and batch methods on eight of the nine tasks. We also show that the confidence information maintained during learning yields useful probabilistic information at test time.",
        "id": 11451209
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "What paper first associate the modeling frequency with input human skeletons under the NeRF framework?",
    "positive_ctxs": [
      {
        "title": "POSE MODULATED AVATARS FROM VIDEO",
        "text": "It is now possible to reconstruct dynamic human motion and shape from a sparse set of cameras using Neural Radiance Fields (NeRF) driven by an underlying skeleton. However, a challenge remains to model the deformation of cloth and skin in relation to skeleton pose. Unlike existing avatar models that are learned implicitly or rely on a proxy surface, our approach is motivated by the observation that different poses necessitate unique frequency assignments. Neglecting this distinction yields noisy artifacts in smooth areas or blurs fine-grained texture and shape details in sharp regions. We develop a two-branch neural network that is adaptive and explicit in the frequency domain. The first branch is a graph neural network that models correlations among body parts locally, taking skeleton pose as input. The second branch combines these correlation features to a set of global frequencies and then modulates the feature encoding. Our experiments demonstrate that our network outperforms state-of-the-art methods in terms of preserving details and generalization capabilities.",
        "id": 261076339
      }
    ],
    "negative_ctxs": [
      {
        "title": "Universal Dependencies for Learner English",
        "text": "We introduce the Treebank of Learner English (TLE), the first publicly available syntactic treebank for English as a Second Language (ESL). The TLE provides manually annotated POS tags and Universal Dependency (UD) trees for 5,124 sentences from the Cambridge First Certificate in English (FCE) corpus. The UD annotations are tied to a pre-existing error annotation of the FCE, whereby full syntactic analyses are provided for both the original and error corrected versions of each sentence. Further on, we delineate ESL annotation guidelines that allow for consistent syntactic treatment of ungrammatical English. Finally, we benchmark POS tagging and dependency parsing performance on the TLE dataset and measure the effect of grammatical errors on parsing accuracy. We envision the treebank to support a wide range of linguistic and computational research on second language acquisition as well as automatic processing of ungrammatical language 1 .",
        "id": 8462706
      },
      {
        "title": "Using Linguist's Assistant for Language Description and Translation",
        "text": "The Linguist's Assistant (LA) is a practical computational paradigm for describing languages. LA seeks to specify in semantic representations a large subset of possible written communication. These semantic representations then become the starting point and organizing principle from which a linguist describes the linguistic surface forms of a language using LA's visual lexicon and grammatical rule development interface. The resulting computational description can then be used in our document authoring and translation applications.",
        "id": 1224439
      },
      {
        "title": "Interactive Relation Extraction in Main Memory Database Systems",
        "text": "We present INDREX-MM, a main memory database system for interactively executing two interwoven tasks, declarative relation extraction from text and their exploitation with SQL. INDREX-MM simplifies these tasks for the user with powerful SQL extensions for gathering statistical semantics, for executing open information extraction and for integrating relation candidates with domain specific data. We demonstrate these functions on 800k documents from Reuters RCV1 with more than a billion linguistic annotations and report execution times in the order of seconds.",
        "id": 1384156
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which backdoor paper first used the CLIP to suppress benign features and enhance poisoning features to design triggers?",
    "positive_ctxs": [
      {
        "title": "Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios",
        "text": "Recent deep neural networks (DNNs) have come to rely on vast amounts of training data, providing an opportunity for malicious attackers to exploit and contaminate the data to carry out backdoor attacks. These attacks significantly undermine the reliability of DNNs. However, existing backdoor attack methods make unrealistic assumptions, assuming that all training data comes from a single source and that attackers have full access to the training data. In this paper, we address this limitation by introducing a more realistic attack scenario where victims collect data from multiple sources, and attackers cannot access the complete training data. We refer to this scenario as data-constrained backdoor attacks. In such cases, previous attack methods suffer from severe efficiency degradation due to the entanglement between benign and poisoning features during the backdoor injection process.IntroductionDeep neural networks (DNNs) are widely utilized and powerful machine learning algorithms inspired by the structure and functioning of the human brain. They excel at learning intricate patterns in data, making them invaluable for various applications such as image recognition[17,21], natural language processing[33,68], image generation[20,30], and anomaly detection[45,64]. However, the effectiveness of DNNs heavily relies on the quantity and quality of the training data. For instance, Stable Diffusion [49], a generative model with 983 million parameters, owes its success in image generation tasks to pre-training on 5 billion image-text pairs. Similarly, GPT-3 [3], a language model with 175 billion * Equal Contribution.",
        "id": 259165262
      }
    ],
    "negative_ctxs": [
      {
        "title": "Lessons Learned in Part-of-Speech Tagging of Conversational Speech",
        "text": "This paper examines tagging models for spontaneous English speech transcripts. We analyze the performance of state-of-the-art tagging models, either generative or discriminative, left-to-right or bidirectional, with or without latent annotations, together with the use of ToBI break indexes and several methods for segmenting the speech transcripts (i.e., conversation side, speaker turn, or humanannotated sentence). Based on these studies, we observe that: (1) bidirectional models tend to achieve better accuracy levels than left-toright models, (2) generative models seem to perform somewhat better than discriminative models on this task, and (3) prosody improves tagging performance of models on conversation sides, but has much less impact on smaller segments. We conclude that, although the use of break indexes can indeed significantly improve performance over baseline models without them on conversation sides, tagging accuracy improves more by using smaller segments, for which the impact of the break indexes is marginal.",
        "id": 2216155
      },
      {
        "title": "Published as a conference paper at ICLR 2020 LEARNING TO REPRESENT PROGRAMS WITH PROPERTY SIGNATURES",
        "text": "We introduce the notion of property signatures, a representation for programs and program specifications meant for consumption by machine learning algorithms. Given a function with input type τ in and output type τ out , a property is a function of type: (τ in , τ out ) → Bool that (informally) describes some simple property of the function under consideration. For instance, if τ in and τ out are both lists of the same type, one property might ask 'is the input list the same length as the output list?'. If we have a list of such properties, we can evaluate them all for our function to get a list of outputs that we will call the property signature. Crucially, we can 'guess' the property signature for a function given only a set of input/output pairs meant to specify that function. We discuss several potential applications of property signatures and show experimentally that they can be used to improve over a baseline synthesizer so that it emits twice as many programs in less than one-tenth of the time.",
        "id": 211252650
      },
      {
        "title": "",
        "text": "",
        "id": 171124654
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research on detecting common errors like additions and omissions in machine translation?",
    "positive_ctxs": [
      {
        "title": "Detecting Over-and Undertranslations with Contrastive Conditioning",
        "text": "Omission and addition of content is a typical issue in neural machine translation. We propose a method for detecting such phenomena with off-the-shelf translation models. Using contrastive conditioning, we compare the likelihood of a full sequence under a translation model to the likelihood of its parts, given the corresponding source or target sequence. This allows to pinpoint superfluous words in the translation and untranslated words in the source even in the absence of a reference translation. The accuracy of our method is comparable to a supervised method that requires a custom quality estimation model.",
        "id": 247223093
      }
    ],
    "negative_ctxs": [
      {
        "title": "INTEGRATING WORD BOUNDARY IDENTIFICATION WITH SENTENCE UNDERSTANDING",
        "text": "Chinese sentences are written with no special delimiters such as space to indicate word boundaries. Existing Chinese NLP systems therefore employ preprocessors to segment sentences into words. Contrary to the conventional wisdom of separating this issue from the task of sentence understanding, we propose an integrated model that performs word boundary identification in lockstep with sentence understanding. In this approach, there is no distinction between rules for word boundary identification and rules for sentence understanding. These two functions are combined. Word boundary ambiguities are detected, especially the fallacious ones, when they block the primary task of discovering the inter-relationships among the various constituents of a sentence, which essentially is the essence of the understanding process. In this approach, statistical information is also incorporated, providing the system a quick and fairly reliable starting ground to carry out the primary task of relationship-building.",
        "id": 7993951
      },
      {
        "title": "Shapes of Emotions: Multimodal Emotion Recognition in Conversations via Emotion Shifts",
        "text": "Emotion Recognition in Conversations (ERC)is an important and active research area. Recent work has shown the benefits of using multiple modalities (e.g., text, audio, and video) for the ERC task. In a conversation, participants tend to maintain a particular emotional state unless some stimuli evokes a change. There is a continuous ebb and flow of emotions in a conversation. Inspired by this observation, we propose a multimodal ERC model and augment it with an emotion-shift component that improves performance. The proposed emotion-shift component is modular and can be added to any existing multimodal ERC model (with a few modifications). We experiment with different variants of the model, and results show that the inclusion of emotion shift signal helps the model to outperform existing models for ERC on MOSEI and IEMOCAP datasets.",
        "id": 244896303
      },
      {
        "title": "NTT Statistical Machine Translation System for IWSLT 2008",
        "text": "The NTT Statistical Machine Translation System consists of two primary components: a statistical machine translation decoder and a reranker. The decoder generates kbest translation canditates using a hierarchical phrase-based translation based on synchronous context-free grammar. The decoder employs a linear feature combination among several real-valued scores on translation and language models. The reranker reorders the k-best translation candidates using Ranking SVMs with a large number of sparse features. This paper describes the two components and presents the results for the evaluation campaign of IWSLT 2008.",
        "id": 5794284
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Can you suggest a corpus that contains French encyclopedia documents with semantic annotations and includes a test set of manually written question/answer triplets that align with the constraints of FrameNet semantic analysis?",
    "positive_ctxs": [
      {
        "title": "Semantic Frame Parsing for Information Extraction : the CALOR corpus",
        "text": "This paper presents a publicly available corpus of French encyclopedic history texts annotated according to the Berkeley FrameNet formalism. The main difference in our approach compared to previous works on semantic parsing with FrameNet is that we are not interested here in full text parsing but rather on partial parsing. The goal is to select from the FrameNet resources the minimal set of frames that are going to be useful for the applicative framework targeted, in our case Information Extraction from encyclopedic documents. Such an approach leverages the manual annotation of larger corpora than those obtained through full text parsing and therefore opens the door to alternative methods for Frame parsing than those used so far on the FrameNet 1.5 benchmark corpus. The approaches compared in this study rely on an integrated sequence labeling model which jointly optimizes frame identification and semantic role segmentation and identification. The models compared are CRFs and multitasks bi-LSTMs.",
        "id": 21725691
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 218977408
      },
      {
        "title": "DEEPDSL: A COMPILATION-BASED DOMAIN- SPECIFIC LANGUAGE FOR DEEP LEARNING",
        "text": "In recent years, Deep Learning (DL) has found great success in domains such as multimedia understanding. However, the complex nature of multimedia data makes it difficult to develop DL-based software. The state-of-the-art tools, such as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their applicable domains, are programming libraries with fixed user interface, internal representation, and execution environment. This makes it difficult to implement portable and customized DL applications. In this paper, we present DeepDSL, a domain specific language (DSL) embedded in Scala, that compiles deep networks written in DeepDSL to Java source code. Deep DSL provides (1) intuitive constructs to support compact encoding of deep networks; (2) symbolic gradient derivation of the networks; (3) static analysis for memory consumption and error detection; and (4) DSL-level optimization to improve memory and runtime efficiency. DeepDSL programs are compiled into compact, efficient, customizable, and portable Java source code, which operates the CUDA and CUDNN interfaces running on Nvidia GPU via a Java Native Interface (JNI) library. We evaluated DeepDSL with a number of popular DL networks. Our experiments show that the compiled programs have very competitive runtime performance and memory efficiency compared to the existing libraries.",
        "id": 11345245
      },
      {
        "title": "",
        "text": "",
        "id": 237155090
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Name a paper which proposes a probabilsitic formulation of retrosynthesis.",
    "positive_ctxs": [
      {
        "title": "RETRO-FALLBACK: RETROSYNTHETIC PLANNING IN AN UNCERTAIN WORLD",
        "text": "Retrosynthesis is the task of proposing a series of chemical reactions to create a desired molecule from simpler, buyable molecules.While previous works have proposed algorithms to find optimal solutions for a range of metrics (e.g.shortest, lowest-cost), these works generally overlook the fact that we have imperfect knowledge of the space of possible reactions, meaning plans created by the algorithm may not work in a laboratory.In this paper we propose a novel formulation of retrosynthesis in terms of stochastic processes to account for this uncertainty.We then propose a novel greedy algorithm called retro-fallback which maximizes the probability that at least one synthesis plan can be executed in the lab.Using in-silico benchmarks we demonstrate that retro-fallback generally produces better sets of synthesis plans than the popular MCTS and retro* algorithms.",
        "id": 264128166
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 226283589
      },
      {
        "title": "\"PolNet -Polish WordNet\" project: PolNet 2.0 -a short description of the release",
        "text": "In December 2011/January 2012 we have released the main deliverable of the project \"PolNet -Polish WordNet\". It was first presented and distributed (as PolNet 1.0) at the 5th Language and Technology Conference in Poznań(2011)and (informally, with kind permission of the organizers) distributed during the Global Wordnet Conference in Matsue, Japan, in January 2012. We intend to present to the participants of the GWC 2014 the characteristics of the new, extended release of PolNet.",
        "id": 15068580
      },
      {
        "title": "Nefnir: A high accuracy lemmatizer for Icelandic",
        "text": "Lemmatization, finding the basic morphological form of a word in a corpus, is an important step in many natural language processing tasks when working with morphologically rich languages. We describe and evaluate Nefnir, a new open source lemmatizer for Icelandic. Nefnir uses suffix substitution rules, derived from a large morphological database, to lemmatize tagged text. Evaluation shows that for correctly tagged text, Nefnir obtains an accuracy of 99.55%, and for text tagged with a PoS tagger, the accuracy obtained is 96.88%.",
        "id": 198967844
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Could you direct me towards a study that explores the potential to predict a reader's native language based on their eye movement patterns while reading English texts?",
    "positive_ctxs": [
      {
        "title": "Predicting Native Language from Gaze",
        "text": "A fundamental question in language learning concerns the role of a speaker's first language in second language acquisition. We present a novel methodology for studying this question: analysis of eye-movement patterns in second language reading of free-form text. Using this methodology, we demonstrate for the first time that the native language of English learners can be predicted from their gaze fixations when reading English. We provide analysis of classifier uncertainty and learned features, which indicates that differences in English reading are likely to be rooted in linguistic divergences across native languages. The presented framework complements production studies and offers new ground for advancing research on multilingualism. 1",
        "id": 14515265
      }
    ],
    "negative_ctxs": [
      {
        "title": "Published as a conference paper at ICLR 2020 LEARNING TO GROUP: A BOTTOM-UP FRAMEWORK FOR 3D PART DISCOVERY IN UNSEEN CATEGORIES",
        "text": "We address the problem of discovering 3D parts for objects in unseen categories.",
        "id": 211132680
      },
      {
        "title": "Published as a conference paper at ICLR 2023 DOES DEEP LEARNING LEARN TO ABSTRACT? A SYSTEMATIC PROBING FRAMEWORK",
        "text": "Abstraction is a desirable capability for deep learning models, which means to induce abstract concepts from concrete instances and flexibly apply them beyond the learning context. At the same time, there is a lack of clear understanding about both the presence and further characteristics of this capability in deep learning models. In this paper, we introduce a systematic probing framework to explore the abstraction capability of deep learning models from a transferability perspective. A set of controlled experiments are conducted based on this framework, providing strong evidence that two probed pre-trained language models (PLMs), T5 and GPT2, have the abstraction capability. We also conduct in-depth analysis, thus shedding further light: (1) the whole training phase exhibits a \"memorize-thenabstract\" two-stage process; (2) the learned abstract concepts are gathered in a few middle-layer attention heads, rather than evenly distributed throughout the model;(3) the probed abstraction capabilities exhibit robustness against concept mutations, and are more robust to low-level/source-side mutations than high-level/target-side ones; (4) generic pre-training is critical to the emergence of abstraction capability, and PLMs exhibit better abstraction with larger model sizes and data scales. * Work done during an internship at Microsoft Research.Abstract ConceptsInput Output… …Input Output… …Surface PatternsInput Output… …Abstract ConceptsFigure 1: Motivating example: the abstract concepts learned in task A can be effectively reused in task B, but surface patterns are useless. Unused patterns or concepts are whitened after the update.generally reused. We consider designing multiple tasks with shared abstract concepts and totally different surface patterns, then tracing whether the learning on one task can boost the performance on another.Figure 1demonstrates a motivating example.Published as a conference paper at ICLR 2023 Aiming Task Train Set ഥ Probing Task Test Set ഥ Transfer Set Abstract Concepts Same Task-Specific Characteristics Different Contrast Task Contrast Set ഥ Abstract Concepts Broken Main Exp ⟹ Pretrain Finetune Test ഥ Control Exp ⇑ ഥ",
        "id": 257102348
      },
      {
        "title": "Introduction to Frontiers in Corpus Annotation",
        "text": "A new annotated corpus can have a pivotal role in the future of computational linguistics. Corpus annotation can define new NLP tasks and set new standards. This may put many of the papers presented at this workshop on the cutting edge of our field.A standard, however, is a double edged sword. A standard corpus urges users to accept the theory of how to represent things that underlie that corpus. For example, a Penn Treebank theory of grammar is implicit in Penn-Treebank-based parsers. This can be a problem if one rejects some aspects of that theory. Also one may object to a particular system of annotation because some theories generalize to cover new ground (e.g., new languages) better than others. Nevertheless, advantages of accepting a corpus as standard include the following:It is straight-forward to compare the performance of the set of systems that produce the same form of output, e.g., Penn Treebank-based parsers can be compared in terms of how well they reproduce the Penn Treebank.Alternative systems based on a standard are largely interchangeable. Thus a system that uses one Penn-Treebank-based parser as a component can easily be adapted to use another better performing Penn-Treebank-based parser.",
        "id": 32824591
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research that examines how syntactic configurations affect aspect-level sentiment analysis when employing a pretrained model such as RoBERTa?",
    "positive_ctxs": [
      {
        "title": "Does syntax matter? A strong baseline for Aspect-based Sentiment Analysis with RoBERTa",
        "text": "Aspect-Based Sentiment Analysis (ABSA), aiming at predicting the polarities for aspects, is a fine-grained task in the field of sentiment analysis. Previous work showed syntactic information, e.g. dependency trees, can effectively improve the ABSA performance. Recently, pre-trained models (PTMs) also have shown their effectiveness on ABSA. Therefore, the question naturally arises whether PTMs contain sufficient syntactic information for ABSA so that we can obtain a good ABSA model only based on PTMs. In this paper, we firstly compare the induced trees from PTMs and the dependency parsing trees on several popular models for the ABSA task, showing that the induced tree from finetuned RoBERTa (FT-RoBERTa) outperforms the parser-provided tree. The further analysis experiments reveal that the FT-RoBERTa Induced Tree is more sentiment-word-oriented and could benefit the ABSA task. The experiments also show that the pure RoBERTa-based model can outperform or approximate to the previous SOTA performances on six datasets across four languages since it implicitly incorporates the task-oriented syntactic information.",
        "id": 233209848
      }
    ],
    "negative_ctxs": [
      {
        "title": "Image-Mediated Learning for Zero-Shot Cross-Lingual Document Retrieval",
        "text": "We propose an image-mediated learning approach for cross-lingual document retrieval where no or only a few parallel corpora are available. Using the images in image-text documents of each language as the hub, we derive a common semantic subspace bridging two languages by means of generalized canonical correlation analysis. For the purpose of evaluation, we create and release a new document dataset consisting of three types of data (English text, Japanese text, and images). Our approach substantially enhances retrieval accuracy in zero-shot and few-shot scenarios where text-to-text examples are scarce.",
        "id": 14768436
      },
      {
        "title": "The RWTH Aachen University Filtering System for the WMT 2018 Parallel Corpus Filtering Task",
        "text": "This paper describes the submission of RWTH Aachen University for the De→En parallel corpus filtering task of the EMNLP 2018 Third Conference on Machine Translation (WMT 2018). We use several rule-based, heuristic methods to preselect sentence pairs. These sentence pairs are scored with count-based and neural systems as language and translation models. In addition to single sentence-pair scoring, we further implement a simple redundancy removing heuristic. Our best performing corpus filtering system relies on recurrent neural language models and translation models based on the transformer architecture. A model trained on 10M randomly sampled tokens reaches a performance of 9.2% BLEU on newstest2018. Using our filtering and ranking techniques we achieve 34.8% BLEU.",
        "id": 53246405
      },
      {
        "title": "Morphological Tagging to Resolve Morphological Ambiguities",
        "text": "The issue of this paper is to present the advantages of a morphological tagging of English in order to resolve morphological ambiguities. Such a way of tagging seems to be more efficient because it allows an intention description of morphological forms compared with the extensive collection of usual dictionaries. This method has already been experimented on French and has given promising results. It is very relevant since it allows both to bring hidden morphological rules to light which are very useful especially for foreign learners and take lexical creativity into account. Moreover, this morphological tagging was conceived in relation to the subsequent disambiguation which is mainly based on local grammars. The purpose is to create a morphological analyser being easily adaptable and modifiable and avoiding the usual errors of the ordinary morphological taggers linked to dictionaries.",
        "id": 9323576
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Is there work on text classification that explores using BERT for the text and using GNN for the label hierarchy?",
    "positive_ctxs": [
      {
        "title": "Hierarchy-aware Label Semantics Matching Network for Hierarchical Text Classification",
        "text": "Hierarchical text classification is an important yet challenging task due to the complex structure of the label hierarchy. Existing methods ignore the semantic relationship between text and labels, so they cannot make full use of the hierarchical information. To this end, we formulate the text-label semantics relationship as a semantic matching problem and thus propose a hierarchy-aware label semantics matching network (HiMatch). First, we project text semantics and label semantics into a joint embedding space. We then introduce a joint embedding loss and a matching learning loss to model the matching relationship between the text semantics and the label semantics. Our model captures the text-label semantics matching relationship among coarse-grained labels and fine-grained labels in a hierarchy-aware manner. The experimental results on various benchmark datasets verify that our model achieves state-of-the-art results.",
        "id": 236460056
      }
    ],
    "negative_ctxs": [
      {
        "title": "UD-Japanese BCCWJ: Universal Dependencies Annotation for the Balanced Corpus of Contemporary Written Japanese",
        "text": "In this paper, we describe a corpus UD Japanese-BCCWJ that was created by converting the Balanced Corpus of Contemporary Written Japanese (BCCWJ), a Japanese language corpus, to adhere to the UD annotation schema. The BCCWJ already assigns dependency information at the level of the bunsetsu (a Japanese syntactic unit comparable to the phrase). We developed a program to convert the BCCWJto UD based on this dependency structure, and this corpus is the result of completely automatic conversion using the program. UD Japanese-BCCWJ is the largestscale UD Japanese corpus and the secondlargest of all UD corpora, including 1,980 documents, 57,109 sentences, and 1,273k words across six distinct domains.",
        "id": 53643217
      },
      {
        "title": "",
        "text": "",
        "id": 218974227
      },
      {
        "title": "Unediting: Detecting Disfluencies Without Careful Transcripts",
        "text": "Speech transcripts often only capture semantic content, omitting disfluencies that can be useful for analyzing social dynamics of a discussion. This work describes steps in building a model that can recover a large fraction of locations where disfluencies were present, by transforming carefully annotated text to match the standard transcription style, introducing a two-stage model for handling different types of disfluencies, and applying semi-supervised learning. Experiments show improvement in disfluency detection on Supreme Court oral arguments, nearly 23% improvement in F1.",
        "id": 849336
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Are there any resources available for translating Tunisian Arabic dialect that contain both manually translated comments by native speakers and additional data augmented through methods like segmentation at stop words level?",
    "positive_ctxs": [
      {
        "title": "Parallel resources for Tunisian Arabic dialect translation",
        "text": "The difficulty of processing dialects is clearly observed in the high cost of building representative corpus, in particular for machine translation. Indeed, all machine translation systems require a huge amount and good management of training data, which represents a challenge in a lowresource setting such as the Tunisian Arabic dialect. In this paper, we present a data augmentation technique to create a parallel corpus for Tunisian Arabic dialect written in social media and standard Arabic in order to build a Machine Translation (MT) model. The created corpus was used to build a sentence-based translation model. This model reached a BLEU score of 15.03% on a test set, while it was limited to 13.27% utilizing the corpus without augmentation.",
        "id": 227231792
      }
    ],
    "negative_ctxs": [
      {
        "title": "Comparing Sanskrit Texts for Critical Editions *",
        "text": "Traditionally Sanskrit is written without blank, sentences can make thousands of characters without any separation. A critical edition takes into account all the different known versions of the same text in order to show the differences between any two distinct versions, in term of words missing, changed or omitted. This paper describes the Sanskrit characteristics that make text comparisons different from other languages, and will present different methods of comparison of Sanskrit texts which can be used for the elaboration of computer assisted critical edition of Sanskrit texts. It describes two sets of methods used to obtain the alignments needed. The first set is using the L.C.S., the second one the global alignment algorithm. One of the methods of the second set uses a classical technique in the field of artificial intelligence, the A* algorithm to obtain the suitable alignment. We conclude by comparing our different results in term of adequacy as well as complexity.",
        "id": 1399747
      },
      {
        "title": "Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access",
        "text": "Most prior work on task-oriented dialogue systems are restricted to a limited coverage of domain APIs, while users oftentimes have domain related requests that are not covered by the APIs. In this paper, we propose to expand coverage of task-oriented dialogue systems by incorporating external unstructured knowledge sources. We define three sub-tasks: knowledge-seeking turn detection, knowledge selection, and knowledge-grounded response generation, which can be modeled individually or jointly. We introduce an augmented version of MultiWOZ 2.1, which includes new out-of-API-coverage turns and responses grounded on external knowledge sources. We present baselines for each sub-task using both conventional and neural approaches. Our experimental results demonstrate the need for further research in this direction to enable more informative conversational systems.",
        "id": 219401758
      },
      {
        "title": "Rapid Adaptation of Neural Machine Translation to New Languages",
        "text": "This paper examines the problem of adapting neural machine translation systems to new, low-resourced languages (LRLs) as effectively and rapidly as possible. We propose methods based on starting with massively multilingual \"seed models\", which can be trained ahead-of-time, and then continuing training on data related to the LRL. We contrast a number of strategies, leading to a novel, simple, yet effective method of \"similar-language regularization\", where we jointly train on both a LRL of interest and a similar high-resourced language to prevent over-fitting to small LRL data. Experiments demonstrate that massively multilingual models, even without any explicit adaptation, are surprisingly effective, achieving BLEU scores of up to 15.5 with no data from the LRL, and that the proposed similarlanguage regularization method improves over other adaptation methods by 1.7 BLEU points average over 4 LRL settings. 1",
        "id": 51976920
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Could you recommend datasets that include SQL annotations over WikiTQ?",
    "positive_ctxs": [
      {
        "title": "On the Potential of Lexico-logical Alignments for Semantic Parsing to SQL Queries",
        "text": "Large-scale semantic parsing datasets annotated with logical forms have enabled major advances in supervised approaches. But can richer supervision help even more? To explore the utility of fine-grained, lexical-level supervision, we introduce SQUALL, a dataset that enriches 11,276 WIKITABLEQUESTIONS English-language questions with manually created SQL equivalents plus alignments between SQL and question fragments. Our annotation enables new training possibilities for encoderdecoder models, including approaches from machine translation previously precluded by the absence of alignments. We propose and test two methods: (1) supervised attention;(2) adopting an auxiliary objective of disambiguating references in the input queries to table columns. In 5-fold cross validation, these strategies improve over strong baselines by 4.4% execution accuracy. Oracle experiments suggest that annotated alignments can support further accuracy gains of up to 23.9%.",
        "id": 225039884
      }
    ],
    "negative_ctxs": [
      {
        "title": "Open Data Vocabularies for Assigning Usage Rights to Translation Memories",
        "text": "An assessment of the intellectual property requirements for data used in machine-aided translation is provided based on a recent EC-funded legal review. This is compared against the capabilities offered by current linked open data standards from the W3C for publishing and sharing translation memories from translation projects, and proposals for adequately addressing the intellectual property needs of stakeholders in translation projects using open data vocabularies are suggested.",
        "id": 37536762
      },
      {
        "title": "臺灣口音中英雙語之多語者影音合成系統 Taiwanese-Accented Mandarin and English Multi-Speaker Talking-Face Synthesis System",
        "text": "This paper proposes a multi-speaker talking-face synthesis system. The system incorporates voice cloning and lipsyncing technology to achieve text-totalking-face generation by acquiring audio and video clips of any speaker and using zero-shot transfer learning. In addition, we used open-source corpora to train several Taiwanese-accented models and proposed using Mandarin Phonetic Symbols (Bopomofo) as the character embedding of the synthesizer to improve the system's ability to synthesize Chinese-English codeswitched sentences. Through our system, users can create rich applications. Also, the research on this technology is novel in the audiovisual speech synthesis field.關鍵字：多語者語音合成、語者驗證、語音 複製、語碼轉換、嘴型同步、人物說話影像",
        "id": 253628242
      },
      {
        "title": "Integrating a Large-scale, Reusable Lexicon with a Natural Language Generator",
        "text": "This paper presents the integration of a largescale, reusable lexicon for generation with the FUF/SURGE unification-based syntactic realizer.The lexicon was combined from multiple existing resources in a semi-automatic process. The integration is a multi-step unification process. This integration allows the reuse of lexical, syntactic, and semantic knowledge encoded in the lexicon in the development of lexical chooser module in a generation system. The lexicon also brings other benefits to a generation system: for example, the ability to generate many lexical and syntactic paraphrases and the ability to avoid non-grammatical output.",
        "id": 1891268
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there commonsense reasoning dataset which generates diverse sentences to describe the relation between concepts?",
    "positive_ctxs": [
      {
        "title": "DimonGen: Diversified Generative Commonsense Reasoning for Explaining Concept Relationships",
        "text": "In this paper, we propose DimonGen, which aims to generate diverse sentences describing concept relationships in various everyday scenarios. To support this, we first create a benchmark dataset for this task by adapting the existing CommonGen dataset. We then propose a two-stage model called MoREE to generate the target sentences. MoREE consists of a mixture of retrievers model that retrieves diverse context sentences related to the given concepts, and a mixture of generators model that generates diverse sentences based on the retrieved contexts. We conduct experiments on the DimonGen task and show that MoREE outperforms strong baselines in terms of both the quality and diversity of the generated sentences. Our results demonstrate that MoREE is able to generate diverse sentences that reflect different relationships between concepts, leading to a comprehensive understanding of concept relationships. 1",
        "id": 254877165
      }
    ],
    "negative_ctxs": [
      {
        "title": "Relation Specific Transformations for Open World Knowledge Graph Completion",
        "text": "We propose an open-world knowledge graph completion model that can be combined with common closed-world approaches (such as ComplEx) and enhance them to exploit text-based representations for entities unseen in training. Our model learns relation-specific transformation functions from text-based embedding space to graph-based embedding space, where the closedworld link prediction model can be applied. We demonstrate state-of-the-art results on common open-world benchmarks and show that our approach benefits from relation-specific transformation functions (RST), giving substantial improvements over a relation-agnostic approach.",
        "id": 227231162
      },
      {
        "title": "OUTCOME-DIRECTED REINFORCEMENT LEARNING BY UNCERTAINTY & TEMPORAL DISTANCE-AWARE CURRICULUM GOAL GENERATION",
        "text": "Current reinforcement learning (RL) often suffers when solving a challenging exploration problem where the desired outcomes or high rewards are rarely observed. Even though curriculum RL, a framework that solves complex tasks by proposing a sequence of surrogate tasks, shows reasonable results, most of the previous works still have difficulty in proposing curriculum due to the absence of a mechanism for obtaining calibrated guidance to the desired outcome state without any prior domain knowledge. To alleviate it, we propose an uncertainty & temporal distance-aware curriculum goal generation method for the outcomedirected RL via solving a bipartite matching problem. It could not only provide precisely calibrated guidance of the curriculum to the desired outcome states but also bring much better sample efficiency and geometry-agnostic curriculum goal proposal capability compared to previous curriculum RL methods. We demonstrate that our algorithm significantly outperforms these prior methods in a variety of challenging navigation tasks and robotic manipulation tasks in a quantitative and qualitative way. 1 * Equal contribution.",
        "id": 256358497
      },
      {
        "title": "Named Entity Recognition in Tweets: An Experimental Study",
        "text": "People tweet more than 100 Million times daily, yielding a noisy, informal, but sometimes informative corpus of 140-character messages that mirrors the zeitgeist in an unprecedented manner. The performance of standard NLP tools is severely degraded on tweets. This paper addresses this issue by re-building the NLP pipeline beginning with part-of-speech tagging, through chunking, to named-entity recognition. Our novel T-NER system doubles F 1 score compared with the Stanford NER system. T-NER leverages the redundancy inherent in tweets to achieve this performance, using LabeledLDA to exploit Freebase dictionaries as a source of distant supervision. LabeledLDA outperforms cotraining, increasing F 1 by 25% over ten common entity types.Our NLP tools are available at: http:// github.com/aritter/twitter_nlp",
        "id": 12861120
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which paper first proposed extracting the pair of target and stance from sentences?",
    "positive_ctxs": [
      {
        "title": "A New Direction in Stance Detection: Target-Stance Extraction in the Wild",
        "text": "Stance detection aims to detect the stance toward a corresponding target.",
        "id": 259370562
      }
    ],
    "negative_ctxs": [
      {
        "title": "Optimal Data Set Selection: An Application to Grapheme-to-Phoneme Conversion",
        "text": "In this paper we introduce the task of unlabeled, optimal, data set selection. Given a large pool of unlabeled examples, our goal is to select a small subset to label, which will yield a high performance supervised model over the entire data set. Our first proposed method, based on the rank-revealing QR matrix factorization, selects a subset of words which span the entire word-space effectively. For our second method, we develop the concept of feature coverage which we optimize with a greedy algorithm. We apply these methods to the task of grapheme-to-phoneme prediction. Experiments over a data-set of 8 languages show that in all scenarios, our selection methods are effective at yielding a small, but optimal set of labelled examples. When fed into a state-of-the-art supervised model for grapheme-to-phoneme prediction, our methods yield average error reductions of 20% over randomly selected examples.",
        "id": 337505
      },
      {
        "title": "DEPAC: a Corpus for Depression and Anxiety Detection from Speech",
        "text": "Mental distress like depression and anxiety contribute to the largest proportion of the global burden of diseases. Automated diagnosis system of such disorders, empowered by recent innovations in Artificial Intelligence, can pave the way to reduce the sufferings of the affected individuals. Development of such systems requires information-rich and balanced corpora. In this work, we introduce a novel mental distress analysis audio dataset DEPAC, labelled based on established thresholds on depression and anxiety standard screening tools. This large dataset comprises multiple speech tasks per individual, as well as relevant demographic information. Alongside, we present a feature set consisting of hand-curated acoustic and linguistic features, which were found effective in identifying signs of mental illnesses in human speech. Finally, we justify the quality and effectiveness of our proposed audio corpus and feature set in predicting depression severity by comparing the performance of baseline machine learning models built on this dataset with baseline models trained on other well-known depression corpora.",
        "id": 250390737
      },
      {
        "title": "Unsupervised Compositionality Prediction of Nominal Compounds",
        "text": "Nominal compounds such as red wine and nut case display a continuum of compositionality, with varying contributions from the components of the compound to its semantics. This article proposes a framework for compound compositionality prediction using distributional semantic models, evaluating to what extent they capture idiomaticity compared to human judgments. For evaluation, we introduce data sets containing human judgments in three languages: English, French, and Portuguese. The results obtained reveal a high agreement between the models and human predictions, suggesting that they are able to incorporate information about idiomaticity. We also present an in-depth evaluation of various factors that can affect prediction, such as model and corpus parameters and compositionality operations. General crosslingual analyses reveal the impact of morphological variation and corpus size in the ability of the model to predict compositionality, and of a uniform combination of the components for best results.",
        "id": 56595638
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Where can I find a detailed discussion on automating the assessment of clarifications in instructional text, including tasks for grading these clarifications as plausible, implausible, or neutral and ranking them on a scale?",
    "positive_ctxs": [
      {
        "title": "SemEval-2022 Task 7: Identifying Plausible Clarifications of Implicit and Underspecified Phrases in Instructional Texts",
        "text": "We describe SemEval-2022 Task 7, a shared task on rating the plausibility of clarifications in English-language instructional texts. The dataset for this task consists of manually clarified how-to guides for which we generated alternative clarifications and collected human plausibility judgements. 1 The task of participating systems was to automatically determine the plausibility of a clarification in the respective context. In total, 21 participants took part in this task, with the best system achieving an accuracy of 68.9%. This report summarizes the results and findings from 8 teams and their system descriptions. Finally, we show in an additional evaluation that predictions by the top participating team make it possible to identify contexts with multiple plausible clarifications with an accuracy of 75.2%.",
        "id": 250390720
      }
    ],
    "negative_ctxs": [
      {
        "title": "EMERGENCE OF GRID-LIKE REPRESENTATIONS BY TRAINING RECURRENT NEURAL NETWORKS TO PERFORM SPATIAL LOCALIZATION",
        "text": "Decades of research on the neural code underlying spatial navigation have revealed a diverse set of neural response properties. The Entorhinal Cortex (EC) of the mammalian brain contains a rich set of spatial correlates, including grid cells which encode space using tessellating patterns. However, the mechanisms and functional significance of these spatial representations remain largely mysterious. As a new way to understand these neural representations, we trained recurrent neural networks (RNNs) to perform navigation tasks in 2D arenas based on velocity inputs. Surprisingly, we find that grid-like spatial response patterns emerge in trained networks, along with units that exhibit other spatial correlates, including border cells and band-like cells. All these different functional types of neurons have been observed experimentally. The order of the emergence of grid-like and border cells is also consistent with observations from developmental studies. Together, our results suggest that grid cells, border cells and others as observed in EC may be a natural solution for representing space efficiently given the predominant recurrent connections in the neural circuits. * equal contribution arXiv:1803.07770v1 [q-bio.NC]",
        "id": 3536139
      },
      {
        "title": "",
        "text": "",
        "id": 232021604
      },
      {
        "title": "Probabilistic Graph Reasoning for Natural Proof Generation",
        "text": "In this paper, we investigate the problem of reasoning over natural language statements. Prior neural based approaches do not explicitly consider the inter-dependency among answers and their proofs. In this paper, we propose PROBR, a novel approach for joint answer prediction and proof generation. PROBR defines a joint probabilistic distribution over all possible proof graphs and answers via an induced graphical model. We then optimize the model using variational approximation on top of neural textual representation. Experiments on multiple datasets under diverse settings (fully supervised, few-shot and zero-shot evaluation) verify the effectiveness of PROBR, e.g., achieving 10%-30% improvement on QA accuracy in few/zero-shot evaluation. Our codes and models can be found at https://github.com/ changzhisun/PRobr/.",
        "id": 235742855
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there a method that measures the information provided in a (model generated) rationale beyond what the original context provided?",
    "positive_ctxs": [
      {
        "title": "REV: Information-Theoretic Evaluation of Free-Text Rationales",
        "text": "Generating free-text rationales is a promising step towards explainable NLP, yet evaluating such rationales remains a challenge. Existing metrics have mostly focused on measuring the association between the rationale and a given label. We argue that an ideal metric should focus on the new information uniquely provided in the rationale that is otherwise not provided in the input or the label. We investigate this research problem from an information-theoretic perspective using conditional V-information (Hewitt et al., 2021). More concretely, we propose a metric called REV (Rationale Evaluation with conditional V-information), to quantify the amount of new, label-relevant information in a rationale beyond the information already available in the input or the label. Experiments across four benchmarks with reasoning tasks, including chain-of-thought, demonstrate the effectiveness of REV in evaluating rationale-label pairs, compared to existing metrics. We further demonstrate REV is consistent with human judgments on rationale evaluations and provides more sensitive measurements of new information in free-text rationales. When used alongside traditional performance metrics, REV provides deeper insights into models' reasoning and prediction processes. . 2015. A large annotated corpus for learning natural language inference. In . 2021. Learning to rationalize for nonmonotonic reasoning with distant supervision.",
        "id": 252816010
      }
    ],
    "negative_ctxs": [
      {
        "title": "Chinese Preposition Selection for Grammatical Error Diagnosis",
        "text": "Misuse of Chinese prepositions is one of common word usage errors in grammatical error diagnosis. In this paper, we adopt the Chinese Gigaword corpus and HSK corpus as L1 and L2 corpora, respectively. We explore gated recurrent neural network model (GRU), and an ensemble of GRU model and maximum entropy language model (GRU-ME) to select the best preposition from 43 candidates for each test sentence. The experimental results show the advantage of the GRU models over simple RNN and n-gram models. We further analyze the effectiveness of linguistic information such as word boundary and part-of-speech tag in this task.This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details:",
        "id": 17778623
      },
      {
        "title": "Shared Task Papers",
        "text": "This paper presents the results of the premier shared task organized alongside the Conference on Machine Translation (WMT) 2018.Participants were asked to build machine translation systems for any of 7 language pairs in both directions, to be evaluated on a test set of news stories. The main metric for this task is human judgment of translation quality. This year, we also opened up the task to additional test suites to probe specific aspects of translation.",
        "id": 53247198
      },
      {
        "title": "N2N Learning: Network to Network Compression via Policy Gradient Reinforcement Learning",
        "text": "While bigger and deeper neural network architectures continue to advance the state-of-the-art for many computer vision tasks, real-world adoption of these networks is impeded by hardware and speed constraints. Conventional model compression methods attempt to address this problem by modifying the architecture manually or using pre-defined heuristics. Since the space of all reduced architectures is very large, modifying the architecture of a deep neural network in this way is a difficult task. In this paper, we tackle this issue by introducing a principled method for learning reduced network architectures in a data-driven way using reinforcement learning. Our approach takes a larger 'teacher' network as input and outputs a compressed 'student' network derived from the 'teacher' network. In the first stage of our method, a recurrent policy network aggressively removes layers from the large 'teacher' model. In the second stage, another recurrent policy network carefully reduces the size of each remaining layer. The resulting network is then evaluated to obtain a reward -a score based on the accuracy and compression of the network. Our approach uses this reward signal with policy gradients to train the policies to find a locally optimal student network. Our experiments show that we can achieve compression rates of more than 10× for models such as ResNet-34 while maintaining similar performance to the input 'teacher' network. We also present a valuable transfer learning result which shows that policies which are pre-trained on smaller 'teacher' networks can be used to rapidly speed up training on larger 'teacher' networks.",
        "id": 13352766
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend research that examines how decoding strategies like top-k impact hallucinatory in generated text?",
    "positive_ctxs": [
      {
        "title": "Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding",
        "text": "Dialogue systems powered by large pretrained language models exhibit an innate ability to deliver fluent and natural-sounding responses. Despite their impressive performance, these models are fitful and can often generate factually incorrect statements impeding their widespread adoption. In this paper, we focus on the task of improving faithfulness and reducing hallucination of neural dialogue systems to known facts supplied by a Knowledge Graph (KG). We propose NEU-RAL PATH HUNTER which follows a generatethen-refine strategy whereby a generated response is amended using the KG. NEURAL PATH HUNTER leverages a separate tokenlevel fact critic to identify plausible sources of hallucination followed by a refinement stage that retrieves correct entities by crafting a query signal that is propagated over a k-hop subgraph. We empirically validate our proposed approach on the OpenDialKG dataset (Moon et al., 2019) against a suite of metrics and report a relative improvement of faithfulness over dialogue responses by 20.35% based on FeQA (Durmus et al., 2020). The code is available at https://github.com/ nouhadziri/Neural-Path-Hunter.",
        "id": 233296059
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 1833004
      },
      {
        "title": "A Computational Approach to Deciphering Unknown Scripts",
        "text": "We propose and evaluate computational techniques for deciphering unknown scripts. We focus on the case in which an unfamiliar script encodes a known language. The decipherment of a brief document or inscription is driven by data about the spoken language. We consider which scripts are easy or hard to decipher, how much data is required, and whether the techniques are robust against language change over time.",
        "id": 12106333
      },
      {
        "title": "Hierarchy Identification for Automatically Generating Table-of-Contents",
        "text": "A table-of-contents (TOC) provides a quick reference to a document's content and structure. We present the first study on identifying the hierarchical structure for automatically generating a TOC using only textual features instead of structural hints e.g. from HTML-tags. We create two new datasets to evaluate our approaches for hierarchy identification. We find that our algorithm performs on a level that is sufficient for a fully automated system. For documents without given segment titles, we extend our work by automatically generating segment titles.We make the datasets and our experimental framework publicly available in order to foster future research in TOC generation.",
        "id": 15256469
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "What sources offer research on maintaining factual accuracy at the entity level in abstractive summary generation?",
    "positive_ctxs": [
      {
        "title": "Entity-level Factual Consistency of Abstractive Text Summarization",
        "text": "A key challenge for abstractive summarization is ensuring factual consistency of the generated summary with respect to the original document. For example, state-of-the-art models trained on existing datasets exhibit entity hallucination, generating names of entities that are not present in the source document. We propose a set of new metrics to quantify the entity-level factual consistency of generated summaries and we show that the entity hallucination problem can be alleviated by simply filtering the training data. In addition, we propose a summary-worthy entity classification task to the training process as well as a joint entity and summary generation approach, which yield further improvements in entity level metrics.",
        "id": 231951460
      }
    ],
    "negative_ctxs": [
      {
        "title": "Offline Sentence Processing Measures for testing Readability with Users",
        "text": "While there has been much work on computational models to predict readability based on the lexical, syntactic and discourse properties of a text, there are also interesting open questions about how computer generated text should be evaluated with target populations. In this paper, we compare two offline methods for evaluating sentence quality, magnitude estimation of acceptability judgements and sentence recall. These methods differ in the extent to which they can differentiate between surface level fluency and deeper comprehension issues. We find, most importantly, that the two correlate. Magnitude estimation can be run on the web without supervision, and the results can be analysed automatically. The sentence recall methodology is more resource intensive, but allows us to tease apart the fluency and comprehension issues that arise.",
        "id": 16215847
      },
      {
        "title": "The application of chordal graphs to inferring phylogenetic trees of languages",
        "text": "Phylogenetic methods are used to build evolutionary trees of languages given character data that may include lexical, phonological, and morphological information. Such data rarely admits a perfect phylogeny. We explore the use of the more permissive conservative Dollo phylogeny as an alternative or complementary approach. We propose a heuristic search algorithm based on the notion of chordal graphs. We test this approach by generating phylogenetic trees from three datasets, and comparing them to those produced by other researchers.",
        "id": 17011784
      },
      {
        "title": "GraphRel: Modeling Text as Relational Graphs for Joint Entity and Relation Extraction",
        "text": "In this paper, we present GraphRel, an end-to-end relation extraction model which uses graph convolutional networks (GCNs) to jointly learn named entities and relations. In contrast to previous baselines, we consider the interaction between named entities and relations via a relation-weighted GCN to better extract relations. Linear and dependency structures are both used to extract both sequential and regional features of the text, and a complete word graph is further utilized to extract implicit features among all word pairs of the text. With the graph-based approach, the prediction for overlapping relations is substantially improved over previous sequential approaches. We evaluate GraphRel on two public datasets: NYT and WebNLG. Results show that GraphRel maintains high precision while increasing recall substantially. Also, GraphRel outperforms previous work by 3.2% and 5.8% (F1 score), achieving a new state-of-the-art for relation extraction.",
        "id": 196211486
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "What is the first paper to address the problem of predicting knowledge graphs whose nodes, links and attributes change with time?",
    "positive_ctxs": [
      {
        "title": "Holistic Prediction on a Time-Evolving Attributed Graph",
        "text": "Graph-based prediction is essential in NLP tasks such as temporal knowledge graph completion. A cardinal question in this field is, how to predict the future links, nodes, and attributes of a time-evolving attributed graph? Unfortunately, existing techniques assume that each link, node, and attribute prediction is independent, and fall short of predicting the appearance of new nodes that were not observed in the past. In this paper, we address two interrelated questions; (1) can we exploit task interdependence to improve prediction accuracy? and(2)can we predict new nodes with their attributes? We propose a unified framework that predicts node attributes and topology changes such as the appearance and disappearance of links and the emergence and loss of nodes. This framework comprises components for independent and interactive prediction and for predicting new nodes. Our experimental study using realworld data confirms that our interdependent prediction framework achieves higher accuracy than methods based on independent prediction.",
        "id": 259370699
      }
    ],
    "negative_ctxs": [
      {
        "title": "MaintNet: A Collaborative Open-Source Library for Predictive Maintenance Language Resources",
        "text": "Maintenance record logbooks are an emerging text type in NLP. An important part of them typically consist of free text with many domain specific technical terms, abbreviations, and nonstandard spelling and grammar. This poses difficulties for NLP pipelines trained on standard corpora. Analyzing and annotating such documents is of particular importance in the development of predictive maintenance systems, which aim to improve operational efficiency, reduce costs, prevent accidents, and save lives. In order to facilitate and encourage research in this area, we have developed MaintNet, a collaborative open-source library of technical and domain-specific language resources. MaintNet provides novel logbook data from the aviation, automotive, and facility maintenance domains along with tools to aid in their (pre-)processing and clustering. Furthermore, it provides a way to encourage discussion on and sharing of new datasets and tools for logbook data analysis.",
        "id": 218889343
      },
      {
        "title": "DIMINISHING RETURN OF VALUE EXPANSION METH- ODS IN MODEL-BASED REINFORCEMENT LEARNING",
        "text": "Model-based reinforcement learning is one approach to increase sample efficiency. However, the accuracy of the dynamics model and the resulting compounding error over modelled trajectories are commonly regarded as key limitations. A natural question to ask is: How much more sample efficiency can be gained by improving the learned dynamics models? Our paper empirically answers this question for the class of model-based value expansion methods in continuous control problems. Value expansion methods should benefit from increased model accuracy by enabling longer rollout horizons and better value function approximations. Our empirical study, which leverages oracle dynamics models to avoid compounding model errors, shows that (1) longer horizons increase sample efficiency, but the gain in improvement decreases with each additional expansion step, and (2) the increased model accuracy only marginally increases the sample efficiency compared to learned models with identical horizons. Therefore, longer horizons and increased model accuracy yield diminishing returns in terms of sample efficiency. These improvements in sample efficiency are particularly disappointing when compared to model-free value expansion methods. Even though they introduce no computational overhead, we find their performance to be on-par with model-based value expansion methods. Therefore, we conclude that the limitation of model-based value expansion methods is not the model accuracy of the learned models. While higher model accuracy is beneficial, our experiments show that even a perfect model will not provide an un-rivalled sample efficiency but that the bottleneck lies elsewhere. Levine. Model-based value estimation for efficient model-free reinforcement learning.",
        "id": 259373058
      },
      {
        "title": "Linguistic annotation of the Spoken Dutch Corpus: If we had to do it all over again",
        "text": "After the successful completion of the Spoken Dutch Corpus (1998 -2003)  the time is ripe to take some time to sit back and reflect on our achievements and the procedures underlying them in order to learn from our experiences. In this paper we will in particular pay attention to issues affecting the levels of linguistic annotation, but some more general issues deserve to be treated as well (bug reporting, consistency). We will try to come up with solutions, but sometimes we want to invite further discussion from other researchers.",
        "id": 3128002
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "What work proposes a model to learn a latent regular cell complex from data?",
    "positive_ctxs": [
      {
        "title": "From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module",
        "text": "Latent Graph Inference (LGI) relaxed the reliance of Graph Neural Networks (GNNs) on a given graph topology by dynamically learning it. However, most of LGI methods assume to have a (noisy, incomplete, improvable, ...) input graph to rewire and can solely learn regular graph topologies. In the wake of the success of Topological Deep Learning (TDL), we study Latent Topology Inference (LTI) for learning higher-order cell complexes (with sparse and not regular topology) describing multi-way interactions between data points. To this aim, we introduce the Differentiable Cell Complex Module (DCM), a novel learnable function that computes cell probabilities in the complex to improve the downstream task. We show how to integrate DCM with cell complex message passing networks layers and train it in a end-to-end fashion, thanks to a two-step inference procedure that avoids an exhaustive search across all possible cells in the input, thus maintaining scalability. Our model is tested on several homophilic and heterophilic graph datasets and it is shown to outperform other state-of-the-art techniques, offering significant improvements especially in cases where an input graph is not provided. * Equal contribution. Corresponding authors,",
        "id": 258887582
      }
    ],
    "negative_ctxs": [
      {
        "title": "Cardinal , nominal or ordinal similarity measures in comparative evaluation of information retrieval process",
        "text": "Similarity measures are used to quantify the resemblance of two sets. Simplest ones are calculated by ratios of the document's number of the compared sets. These measures are simple and usually employed in first steps of evaluation studies, they are called cardinal measures. Others measures compare sets upon the number of common documents they have. They are usually employed in quantitative information retrieval evaluations, some examples are Jaccard, Cosine, Recall or Precision. These measures are called nominal ones. There are more or less adapted in function of the richness of the information system's answer. Indeed, in the past, they were sufficient because answers given by systems were only composed by an unordered set of documents. But usual systems improve the quality or the visibility of there answers by using a relevant ranking or a clustering presentation of documents. In this case, similarity measures aren't adapted. In this paper we present some solutions in the case of totally ordered and partially ordered answer.",
        "id": 3605426
      },
      {
        "title": "Dialogue complexity with portability? Research directions for the Information State approach",
        "text": "We review existing types of dialogue managers (DMs), and propose that the Information State (IS) approach may allow both complexity of dialogue and ease of portability. We discuss implementational drawbacks of the only existing IS DM, and describe our work underway to develop a new DM resolving those drawbacks.",
        "id": 15691543
      },
      {
        "title": "Exploring Weaknesses of VQA Models through Attribution Driven Insights",
        "text": "Deep Neural Networks have been successfully used for the task of Visual Question Answering for the past few years owing to the availability of relevant large scale datasets. However these datasets are created in artificial settings and rarely reflect the real world scenario. Recent research effectively applies these VQA models for answering visual questions for the blind. Despite achieving high accuracy these models appear to be susceptible to variation in input questions.We analyze popular VQA models through the lens of attribution (input's influence on predictions) to gain valuable insights. Further, We use these insights to craft adversarial attacks which inflict significant damage to these systems with negligible change in meaning of the input questions. We believe this will enhance development of systems more robust to the possible variations in inputs when deployed to assist the visually impaired.",
        "id": 219573758
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "What are the key advantages of coupling neural SDEs with neural CDEs for treatment effect estimation over existing baselines?",
    "positive_ctxs": [
      {
        "title": "BAYESIAN NEURAL CONTROLLED DIFFERENTIAL EQUATIONS FOR TREATMENT EFFECT ESTIMATION",
        "text": "Treatment effect estimation in continuous time is crucial for personalized medicine.However, existing methods for this task are limited to point estimates of the potential outcomes, whereas uncertainty estimates have been ignored.Needless to say, uncertainty quantification is crucial for reliable decision-making in medical applications.To fill this gap, we propose a novel Bayesian neural controlled differential equation (BNCDE) for treatment effect estimation in continuous time.In our BNCDE, the time dimension is modeled through a coupled system of neural controlled differential equations and neural stochastic differential equations, where the neural stochastic differential equations allow for tractable variational Bayesian inference.Thereby, for an assigned sequence of treatments, our BNCDE provides meaningful posterior predictive distributions of the potential outcomes.To the best of our knowledge, ours is the first tailored neural method to provide uncertainty estimates of treatment effects in continuous time.As such, our method is of direct practical value for promoting reliable decision-making in medicine.",
        "id": 264490587
      }
    ],
    "negative_ctxs": [
      {
        "title": "Using a Morphological Database to Increase the Accuracy in POS Tagging",
        "text": "We experiment with extending the dictionaries used by three open-source partof-speech taggers, by using data from a large Icelandic morphological database. We show that the accuracy of the taggers can be improved significantly by using the database. The reason is that the unknown word ratio reduces dramatically when adding data from the database to the taggers' dictionaries. For the best performing tagger, the overall tagging accuracy increases from the base tagging result of 92.73% to 93.32%, when the unknown word ratio decreases from 6.8% to 1.1%. When we add reliable frequency information to the tag profiles for some of the words originating from the database, we are able to increase the accuracy further to 93.48% -this is equivalent to 10.3% error reduction compared to the base tagger.",
        "id": 8965880
      },
      {
        "title": "A NOTE ON MOR2H~E STRUCTURE IN GENF~TIVE",
        "text": "In an early model of generative phonology the lexicon of a language contained entries with as few feature specifications as possible in the interest of economy. The blank feature specifications representing both nondistinctive features and those rendered redundant by sequential constraints were filled in by the same 9honological rules. At this point, the concept of ~ rules changing feature values was unclear. When the distinction between rules that fill in blanks and those that change feature values became clear, it was zmbodied in the concept of morpheme structure rules and P rules. The MS rules were further split into feature redundancy (segment structure) rules and sequ~tial constraint rules. The MS component bore a striking resemblence to the earlier \"pkonotactic\" sections of autonomous phonemic analyses, but the claim was made for I~S ~les that they explained what phonotactiee merely described. The MS rules formed a major part of Chomsky's \"readjustment component\" which rendered th~ output of the syntactic component fit to be the input to the phonological component. A fairly current version of ~his model is the following one from Harms' Introduction t__oo Phonological c~)",
        "id": 43449978
      },
      {
        "title": "Picking Apart Story Salads",
        "text": "During natural disasters and conflicts, information about what happened is often confusing, messy, and distributed across many sources. We would like to be able to automatically identify relevant information and assemble it into coherent narratives of what happened. To make this task accessible to neural models, we introduce Story Salads, mixtures of multiple documents that can be generated at scale. By exploiting the Wikipedia hierarchy, we can generate salads that exhibit challenging inference problems. Story salads give rise to a novel, challenging clustering task, where the objective is to group sentences from the same narratives. We demonstrate that simple bag-of-words similarity clustering falls short on this task and that it is necessary to take into account global context and coherence.",
        "id": 53081356
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Is there a parameter-efficient fine-tuning method (PEFT) that allows adjusting the number of optimized parameters to any value, irrespective of the model's architecture or choice of the adapter?",
    "positive_ctxs": [
      {
        "title": "NOLA: NETWORKS AS LINEAR COMBINATION OF LOW RANK RANDOM BASIS",
        "text": "Large Language Models (LLMs) have recently gained popularity due to their impressive few-shot performance across various downstream tasks.However, fine-tuning all parameters and storing a unique model for each downstream task or domain becomes impractical because of the massive size of checkpoints (e.g., 350GB in GPT-3).Current literature, such as LoRA, showcases the potential of lowrank modifications to the original weights of an LLM, enabling efficient adaptation and storage for task-specific models.These methods can reduce the number of parameters needed to fine-tune an LLM by several orders of magnitude.Yet, these methods face two primary limitations: 1) the parameter reduction is lower-bounded by the rank one decomposition, and 2) the extent of reduction is heavily influenced by both the model architecture and the chosen rank.For instance, in larger models, even a rank one decomposition might exceed the number of parameters truly needed for adaptation.In this paper, we introduce NOLA, which overcomes the rank one lower bound present in LoRA.It achieves this by re-parameterizing the low-rank matrices in LoRA using linear combinations of randomly generated matrices (basis) and optimizing the linear mixture coefficients only.This approach allows us to decouple the number of trainable parameters from both the choice of rank and the network architecture.We present adaptation results using GPT-2 and ViT in natural language and computer vision tasks.NOLA performs as well as, or better than models with equivalent parameter counts.Furthermore, we demonstrate that we can halve the parameters in larger models compared to LoRA with rank one, without sacrificing performance.Our code is available here: https://github.com/UCDvision/NOLA* Equal Contribution.",
        "id": 263620510
      }
    ],
    "negative_ctxs": [
      {
        "title": "Minimum Description Length Control",
        "text": "We propose a novel framework for multitask reinforcement learning based on the minimum description length (MDL) principle. In this approach, which we term MDL-control (MDL-C), the agent learns the common structure among the tasks with which it is faced and then distills it into a simpler representation which facilitates faster convergence and generalization to new tasks. In doing so, MDL-C naturally balances adaptation to each task with epistemic uncertainty about the task distribution. We motivate MDL-C via formal connections between the MDL principle and Bayesian inference, derive theoretical performance guarantees, and demonstrate MDL-C's empirical effectiveness on both discrete and high-dimensional continuous control tasks.",
        "id": 250627720
      },
      {
        "title": "AN LR(k ) ERROR DIAGNOSIS AND REC OVERY METHOD",
        "text": "In this paper, a new practical, efficient and language ... independent syntactic error recov ery method for tR( k) parsers is presented. This method is similar to and builds Upon the three-level approach oi' Burke�Fisher [11]. However, it is more time-and space-efficient and fully automatic.",
        "id": 219308174
      },
      {
        "title": "Political Discourse Analysis: A Case Study of Code Mixing and Code Switching in Political Speeches",
        "text": "Political discourse is one of the most interesting data to study power relations in the framework of Critical Discourse Analysis. With the increase in the modes of textual and spoken forms of communication, politicians use language and linguistic mechanisms that contribute significantly in building their relationship with people, especially in a multilingual country like India with many political parties with different ideologies. This paper analyses code-mixing and code-switching in Telugu political speeches to determine the factors responsible for their usage levels in various social settings and communicative contexts. We also compile a detailed set of rules capturing dialectal variations between Standard and Telangana dialects of Telugu.",
        "id": 235097552
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "I'm conducting research on computational humor and looking at various approaches to detect it within texts. What are some articles that explore features like repetition or use language models like GPT-2 for humor recognition?",
    "positive_ctxs": [
      {
        "title": "Humor Recognition and Humor Anchor Extraction",
        "text": "Humor is an essential component in personal communication. How to create computational models to discover the structures behind humor, recognize humor and even extract humor anchors remains a challenge.In this work, we first identify several semantic structures behind humor and design sets of features for each structure, and next employ a computational approach to recognize humor. Furthermore, we develop a simple and effective method to extract anchors that enable humor in a sentence. Experiments conducted on two datasets demonstrate that our humor recognizer is effective in automatically distinguishing between humorous and non-humorous texts and our extracted humor anchors correlate quite well with human annotations.",
        "id": 11128248
      },
      {
        "title": "Uncertainty and Surprisal Jointly Deliver the Punchline: Exploiting Incongruity-Based Features for Humor Recognition",
        "text": "Humor recognition has been widely studied as a text classification problem using data-driven approaches. However, most existing work does not examine the actual joke mechanism to understand humor. We break down any joke into two distinct components: the set-up and the punchline, and further explore the special relationship between them. Inspired by the incongruity theory of humor, we model the setup as the part developing semantic uncertainty, and the punchline disrupting audience expectations. With increasingly powerful language models, we were able to feed the set-up along with the punchline into the GPT-2 language model, and calculate the uncertainty and surprisal values of the jokes. By conducting experiments on the SemEval 2021 Task 7 dataset, we found that these two features have better capabilities of telling jokes from non-jokes, compared with existing baselines.",
        "id": 229349316
      }
    ],
    "negative_ctxs": [
      {
        "title": "Harvesting Multi-Word Expressions from Parallel Corpora",
        "text": "The paper presents a set of approaches to extend the automatically created Slovene wordnet with nominal multiword expressions. In the first approach multiword expressions from Princeton WordNet are translated with a technique that is based on wordalignment and lexicosyntactic patterns. This is followed by extracting new terms from a monolingual corpus using keywordness ranking and contextual patterns. Finally, the multiword expressions are assigned a hypernym and added to our wordnet. Manual evaluation and comparison of the results shows that the translation approach is the most straightforward and accurate. However, it is successfully complemented by the two monolingual approaches which are able to identify more term candidates in the corpus that would otherwise go unnoticed. Some weaknesses of the proposed wordnet extension techniques are also addressed.",
        "id": 12731859
      },
      {
        "title": "Sinica-IASL Chinese Spelling Check System at SIGHAN-7",
        "text": "We developed a Chinese spelling check system for error detection and error correction subtasks in the 2013 SIGHAN-7 Chinese Spelling Check Bake-off. By using the resources of Chinese phonology and orthographic components, our system contains four parts: high confidence pattern matcher, the detection module, the correction module, and the merger. We submitted 2 official runs for both subtasks. The evaluation result show that our system achieved 0.6016 in error detection F-score of subtask 1, and 0.448 in correction accuracy of subtask 2. 1",
        "id": 11928977
      },
      {
        "title": "Transliteration and Alignment of Parallel Texts from Cyrillic to Latin",
        "text": "This article describes a methodology of recovering and preservation of old Romanian texts and problems related to their recognition. Our focus is to create a gold corpus for Romanian language (the novella Sania), for both alphabets used in Transnistria -Cyrillic and Latin. The resource is available for similar researches. This technology is based on transliteration and semiautomatic alignment of parallel texts at the level of letter/lexem/multiwords. We have analysed every text segment present in this corpus and discovered other conventions of writing at the level of transliteration, academic norms and editorial interventions. These conventions allowed us to elaborate and implement some new heuristics that make a correct automatic transliteration process. Sometimes the words of Latin script are modified in Cyrillic script from semantic reasons (for instance, editor's interpretation). Semantic transliteration is seen as a good practice in introducing multiwords from Cyrillic to Latin. Not only does it preserve how a multiwords sound in the source script, but also enables the translator to modify in the original text (here, choosing the most common sense of an expression). Such a technology could be of interest to lexicographers, but also to specialists in computational linguistics to improve the actual transliteration standards.",
        "id": 10202628
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Is there a benchmark designed to assess language models' social biases within question answering frameworks, specifically one encompassing diverse categories of social bias and offering instances of ambiguity and its resolution?",
    "positive_ctxs": [
      {
        "title": "BBQ: A Hand-Built Bias Benchmark for Question Answering",
        "text": "It is well documented that NLP models learn social biases, but little work has been done on how these biases manifest in model outputs for applied tasks like question answering (QA). We introduce the Bias Benchmark for QA (BBQ), a dataset of question sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts. Our task evaluates model responses at two levels: (i) given an under-informative context, we test how strongly responses refect social biases, and (ii) given an adequately informative context, we test whether the model's biases override a correct answer choice. We fnd that models often rely on stereotypes when the context is under-informative, meaning the model's outputs consistently reproduce harmful biases in this setting. Though models are more accurate when the context provides an informative answer, they still rely on stereotypes and average up to 3.4 percentage points higher accuracy when the correct answer aligns with a social bias than when it conficts, with this difference widening to over 5 points on examples targeting gender for most models tested.",
        "id": 239010011
      }
    ],
    "negative_ctxs": [
      {
        "title": "Alignment-based Annotation of Proofreading Texts toward Professional Writing Assistance",
        "text": "This work aims at constructing a corpus to satisfy such requirements to support research towards professional writing assistance. Our corpus is a collection of scientific work written by non-native speakers that has been proofread by native English experts. A new annotation scheme, which is based on word-alignments, is then proposed that is used to capture all types of inarticulations and their corrections including both spelling/grammatical error corrections and paraphrases made by proofreaders. The resulting corpus contains 3,485 pairs of original and revised sentences, of which, 2,516 pairs contain at least one articulation.",
        "id": 16892737
      },
      {
        "title": "",
        "text": "",
        "id": 219309690
      },
      {
        "title": "Translating a Language You Don't Know in the Chinese Room",
        "text": "In a corruption of John Searle's famous AI thought experiment, the Chinese Room (Searle, 1980), we twist its original intent by enabling humans to translate text, e.g. from Uyghur to English, even if they don't have any prior knowledge of the source language. Our enabling tool, which we call the Chinese Room, is equipped with the same resources made available to a machine translation engine. We find that our superior language model and world knowledge allows us to create perfectly fluent and nearly adequate translations, with human expertise required only for the target language. The Chinese Room tool can be used to rapidly create small corpora of parallel data when bilingual translators are not readily available, in particular for low-resource languages.",
        "id": 51871755
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "What methods exist for tailoring news suggestions that consider a user's preferences as well as the current popularity of news stories?",
    "positive_ctxs": [
      {
        "title": "PP-Rec: News Recommendation with Personalized User Interest and Time-aware News Popularity",
        "text": "Personalized news recommendation methods are widely used in online news services. These methods usually recommend news based on the matching between news content and user interest inferred from historical behaviors. However, these methods usually have difficulties in making accurate recommendations to cold-start users, and tend to recommend similar news with those users have read. In general, popular news usually contain important information and can attract users with different interests. Besides, they are usually diverse in content and topic. Thus, in this paper we propose to incorporate news popularity information to alleviate the cold-start and diversity problems for personalized news recommendation. In our method, the ranking score for recommending a candidate news to a target user is the combination of a personalized matching score and a news popularity score. The former is used to capture the personalized user interest in news. The latter is used to measure timeaware popularity of candidate news, which is predicted based on news content, recency, and real-time CTR using a unified framework. Besides, we propose a popularity-aware user encoder to eliminate the popularity bias in user behaviors for accurate interest modeling. Experiments on two real-world datasets show our method can effectively improve the accuracy and diversity for news recommendation.",
        "id": 235294032
      }
    ],
    "negative_ctxs": [
      {
        "title": "Efficient incremental beam-search parsing with generative and discriminative models",
        "text": "Extended Abstract:This talk will present several issues related to incremental (left-to-right) beam-search parsing of natural language using generative or discriminative models, either individually or in combination. The first part of the talk will provide background in incremental top-down and (selective) left-corner beamsearch parsing algorithms, and in stochastic models for such derivation strategies. Next, the relative benefits and drawbacks of generative and discriminative models with respect to heuristic pruning and search will be discussed. A range of methods for using multiple models during incremental parsing will be detailed. Finally, we will discuss the potential for effective use of fast, finite-state processing, e.g. partof-speech tagging, to reduce the parsing search space without accuracy loss. POS-tagging is shown to improve efficiency by as much as 20-25 percent with the same accuracy, largely due to the treatment of unknown words. In contrast, an 'islands-of-certainty' approach, which quickly annotates labeled bracketing over low-ambiguity word sequences, is shown to provide little or no efficiency gain over the existing beam-search.The basic parsing approach that will be described in this talk is stochastic incremental top-down parsing, using a beam-search to prune the search space. Grammar induction occurs from an annotated treebank, and non-local features are extracted from each derivation to enrich the stochastic model. Left-corner grammar and tree transforms can be applied to the treebank or the induced grammar, either fully or selectively, to change the derivation order while retaining the same underlying parsing algorithm. This approach has been shown to be accurate, relatively efficient, and robust using both generative and discriminative models(Roark, 2001;Roark, 2004;Collins and Roark, 2004).",
        "id": 18917446
      },
      {
        "title": "A convex relaxation for weakly supervised relation extractioń",
        "text": "A promising approach to relation extraction, called weak or distant supervision, exploits an existing database of facts as training data, by aligning it to an unlabeled collection of text documents. Using this approach, the task of relation extraction can easily be scaled to hundreds of different relationships. However, distant supervision leads to a challenging multiple instance, multiple label learning problem. Most of the proposed solutions to this problem are based on non-convex formulations, and are thus prone to local minima. In this article, we propose a new approach to the problem of weakly supervised relation extraction, based on discriminative clustering and leading to a convex formulation. We demonstrate that our approach outperforms state-of-the-art methods on the challenging dataset introduced byRiedel et al. (2010).",
        "id": 1849410
      },
      {
        "title": "Published as a conference paper at ICLR 2021 ROBUST REINFORCEMENT LEARNING ON STATE OB- SERVATIONS WITH LEARNED OPTIMAL ADVERSARY",
        "text": "We study the robustness of reinforcement learning (RL) with adversarially perturbed state observations, which aligns with the setting of many adversarial attacks to deep reinforcement learning (DRL) and is also important for rolling out real-world RL agent under unpredictable sensing noise. With a fixed agent policy, we demonstrate that an optimal adversary to perturb state observations can be found, which is guaranteed to obtain the worst case agent reward. For DRL settings, this leads to a novel empirical adversarial attack to RL agents via a learned adversary that is much stronger than previous ones. To enhance the robustness of an agent, we propose a framework of alternating training with learned adversaries (ATLA), which trains an adversary online together with the agent using policy gradient following the optimal adversarial attack framework. Additionally, inspired by the analysis of state-adversarial Markov decision process (SA-MDP), we show that past states and actions (history) can be useful for learning a robust agent, and we empirically find a LSTM based policy can be more robust under adversaries. Empirical evaluations on a few continuous control environments show that ATLA achieves state-of-the-art performance under strong adversaries. Our code is available at https://github.com/huanzhang12/ATLA_robust_RL.arXiv:2101.08452v1 [cs.LG] 21 Jan 2021Published as a conference paper at ICLR 2021 (a) Path in unperturbed environment (found by policy iteration). Agent's reward = +1. Black arrows and numbers show actions and value function of the agent.(b) Path under the optimal adversary. Agent's reward = −∞. Red arrows and numbers show actions and value function of the optimal adversary (Section 3.1).(c) A robust POMDP policy solved by SARSOP(Kurniawati et al., 2008)under the same adversary. This policy is history dependent (Section 3.2).",
        "id": 231662383
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend studies that tackle the issue of popularity bias within news recommendation engines and offer techniques to distinguish between user interests and the popularity of news items?",
    "positive_ctxs": [
      {
        "title": "PP-Rec: News Recommendation with Personalized User Interest and Time-aware News Popularity",
        "text": "Personalized news recommendation methods are widely used in online news services. These methods usually recommend news based on the matching between news content and user interest inferred from historical behaviors. However, these methods usually have difficulties in making accurate recommendations to cold-start users, and tend to recommend similar news with those users have read. In general, popular news usually contain important information and can attract users with different interests. Besides, they are usually diverse in content and topic. Thus, in this paper we propose to incorporate news popularity information to alleviate the cold-start and diversity problems for personalized news recommendation. In our method, the ranking score for recommending a candidate news to a target user is the combination of a personalized matching score and a news popularity score. The former is used to capture the personalized user interest in news. The latter is used to measure timeaware popularity of candidate news, which is predicted based on news content, recency, and real-time CTR using a unified framework. Besides, we propose a popularity-aware user encoder to eliminate the popularity bias in user behaviors for accurate interest modeling. Experiments on two real-world datasets show our method can effectively improve the accuracy and diversity for news recommendation.",
        "id": 235294032
      }
    ],
    "negative_ctxs": [
      {
        "title": "Knowledge Enhanced Fine-Tuning for Better Handling Unseen Entities in Dialogue Generation",
        "text": "Although pre-training models have achieved great success in dialogue generation, their performance drops dramatically when the input contains an entity that does not appear in pretraining and fine-tuning datasets (unseen entity). To address this issue, existing methods leverage an external knowledge base to generate appropriate responses. In real-world scenario, the entity may not be included by the knowledge base or suffer from the precision of knowledge retrieval. To deal with this problem, instead of introducing knowledge base as the input, we force the model to learn a better semantic representation by predicting the information in the knowledge base, only based on the input context. Specifically, with the help of a knowledge base, we introduce two auxiliary training objectives: 1) Interpret Masked Word, which conjectures the meaning of the masked entity given the context; 2) Hypernym Generation, which predicts the hypernym of the entity based on the context. Experiment results on two dialogue corpus verify the effectiveness of our methods under both knowledge available and unavailable settings.",
        "id": 237490367
      },
      {
        "title": "Sharing Copies of Synthetic Clinical Corpora without Physical Distribution - A Case Study to Get Around IPRs and Privacy Constraints Featuring the German JSYNCC Corpus",
        "text": "The legal culture in the European Union imposes almost unsurmountable hurdles to exploit copyright protected language data (in terms of intellectual property rights (IPRs) of media contents) and privacy protected medical health data (in terms of the notion of informational self-determination) as language resources for the NLP community. These juridical constraints have seriously hampered progress in resource-greedy NLP research, in particular for non-English languages in the clinical domain. In order to get around these restrictions, we introduce a novel approach for the creation and re-use of clinical corpora which is based on a two-step workflow. First, we substitute authentic clinical documents by synthetic ones, i.e., made-up reports and case studies written by medical professionals for educational purposes and published in medical e-textbooks. We thus eliminate patients' privacy concerns since no real, concrete individuals are addressed in such narratives. In a second step, we replace physical corpus distribution by sharing software for trustful re-construction of corpus copies. This is achieved by an end-to-end tool suite which extracts well-specified text fragments from e-books and assembles, on demand, identical copies of the same text corpus we defined at our lab at any other site where this software is executed. Thus, we avoid IPR violations since no physical corpus (raw text data) is distributed. As an illustrative case study which is easily portable to other languages we present JSYNCC, the largest and, even more importantly, first publicly available, corpus of German clinical language.",
        "id": 21701790
      },
      {
        "title": "Automatic Assessment of Conceptual Text Complexity Using Knowledge Graphs",
        "text": "Complexity of texts is usually assessed only at the lexical and syntactic levels. Although it is known that conceptual complexity plays a significant role in text understanding, no attempts have been made at assessing it automatically. We propose to automatically estimate the conceptual complexity of texts by exploiting a number of graph-based measures on a large knowledge base. By using a high-quality language learners corpus for English, we show that graph-based measures of individual text concepts, as well as the way they relate to each other in the knowledge graph, have a high discriminative power when distinguishing between two versions of the same text. Furthermore, when used as features in a binary classification task aiming to choose the simpler of two versions of the same text, our measures achieve high performance even in a default setup.This work is licensed under a Creative Commons Attribution 4.0 International License.License details:",
        "id": 52013038
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "In the field of reinforcement learning models for multi-hop reasoning, what issue involves an agent erroneously correlating a successful outcome with irrelevant or coincidental actions, and are there any papers discussing this phenomenon?",
    "positive_ctxs": [
      {
        "title": "From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood",
        "text": "Our goal is to learn a semantic parser that maps natural language utterances into executable programs when only indirect supervision is available: examples are labeled with the correct execution result, but not the program itself. Consequently, we must search the space of programs for those that output the correct result, while not being misled by spurious programs: incorrect programs that coincidentally output the correct result. We connect two common learning paradigms, reinforcement learning (RL) and maximum marginal likelihood (MML), and then present a new learning algorithm that combines the strengths of both. The new algorithm guards against spurious programs by combining the systematic search traditionally employed in MML with the randomized exploration of RL, and by updating parameters such that probability is spread more evenly across consistent programs. We apply our learning algorithm to a new neural semantic parser and show significant gains over existing state-of-theart results on a recent context-dependent semantic parsing task.",
        "id": 9268430
      }
    ],
    "negative_ctxs": [
      {
        "title": "Flat Multi-modal Interaction Transformer for Named Entity Recognition",
        "text": "Multi-modal named entity recognition (MNER) aims at identifying entity spans and recognizing their categories in social media posts with the aid of images. However, in dominant MNER approaches, the interaction of different modalities is usually carried out through the alternation of self-attention and cross-attention or over-reliance on the gating machine, which results in imprecise and biased correspondence between fine-grained semantic units of text and image. To address this issue, we propose a Flat Multi-modal Interaction Transformer (FMIT) for MNER. Specifically, we first utilize noun phrases in sentences and general domain words to obtain visual cues. Then, we transform the fine-grained semantic representation of the vision and text into a unified lattice structure and design a novel relative position encoding to match different modalities in Transformer. Meanwhile, we propose to leverage entity boundary detection as an auxiliary task to alleviate visual bias. Experiments show that our methods achieve the new state-of-the-art performance on two benchmark datasets.",
        "id": 251741236
      },
      {
        "title": "LANGUAGE SYSTEMS INC : DESCRIPTION OF THE DBG SYSTE M AS USED FOR MUC-5 1 INTRODUCTIO N",
        "text": "Language Systems, Inc . (LSI) believes that the best system for producing a complete and accurate automate d analysis of natural language text is an in-depth text understanding system that employs linguistic as wel l as other analytical techniques to interpret the text . Our DBG (Data Base Generation) natural language processing system performs full-scale linguistic analysis of text in order to produce a system-internal textlevel representation of the content of the text . This representation is composed of a set of entity and event frame structures, interrelated to reflect the organization and content of the text . This representation of the text can then be mapped into any data structure required by a downstream application, such as th e templates specified for the MUC-5/Tipster applications . DBG has been designed as a single core syste m for handling texts of different types in different domains for a variety of applications . Application types for which DBG has provided the input include information extraction and database generation tasks suc h as MUC-5, message fusion (the combination of information derived from various kinds of sources, includin g text ; see [1]), and the translation of text into another language using spoken input and output ([2]) .",
        "id": 7259211
      },
      {
        "title": "Finding viable seed URLs for web corpora: A scouting approach and comparative study of available sources",
        "text": "The conventional tools of the \"web as corpus\" framework rely heavily on URLs obtained from search engines. Recently, the corresponding querying process became much slower or impossible to perform on a low budget. I try to find acceptable substitutes, i.e. viable link sources for web corpus construction. To this end, I perform a study of possible alternatives, including social networks as well as the Open Directory Project and Wikipedia. Four different languages (Dutch, French, Indonesian and Swedish) taken as examples show that complementary approaches are needed. My scouting approach using open-source software leads to a URL directory enriched with metadata which may be used to start a web crawl. This is more than a drop-in replacement for existing tools since said metadata enables researchers to filter and select URLs that fit particular needs, as they are classified according to their language, their length and a few other indicators such as host-and markup-based data.",
        "id": 16763441
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Which dataset supports narration generation and temporal localization tasks in Chinese movies?",
    "positive_ctxs": [
      {
        "title": "Movie101: A New Movie Understanding Benchmark",
        "text": "To help the visually impaired enjoy movies, automatic movie narrating systems are expected to narrate accurate, coherent, and role-aware",
        "id": 258832605
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 219310243
      },
      {
        "title": "Group-based Generation of Referring Expressions",
        "text": "Past work of generating referring expressions mainly utilized attributes of objects and binary relations between objects in order to distinguish the target object from others. However, such an approach does not work well when there is no distinctive attribute among objects. To overcome this limitation, this paper proposes a novel generation method utilizing perceptual groups of objects and n-ary relations among them. The evaluation using 18 subjects showed that the proposed method could effectively generate proper referring expressions.",
        "id": 18892448
      },
      {
        "title": "FQuAD: French Question Answering Dataset",
        "text": "Recent advances in the field of language modeling have improved state-of-the-art results on many Natural Language Processing tasks. Among them, Reading Comprehension has made significant progress over the past few years. However, most results are reported in English since labeled resources available in other languages, such as French, remain scarce. In the present work, we introduce the French Question Answering Dataset (FQuAD). FQuAD is a French Native Reading Comprehension dataset of questions and answers on a set of Wikipedia articles that consists of 25,000+ samples for the 1.0 version and 60,000+ samples for the 1.1 version. We train a baseline model which achieves an F1 score of 92.2 and an exact match ratio of 82.1 on the test set. In an effort to track the progress of French Question Answering models we propose a leaderboard and we have made the 1.0 version of our dataset freely available at https://illuin-tech. github.io/FQuAD-explorer/. . 2019. A span-extraction dataset for Chinese machine reading comprehension. In . 2018. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805. Pavel Efimov, Leonid Boytsov, and Pavel Braslavski. 2019. Sberquad -russian reading comprehension dataset: Description and analysis. Matthew Honnibal and Ines Montani. 2017. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. To appear. Ali Kabbadj. 2018. Something new in french text mining and information extraction (universal chatbot): Largest qa french training dataset (110 000+). Guillaume Lample and Alexis Conneau. 2019. Crosslingual language model pretraining. CoRR, abs/1901.07291. . 2019. Mlqa: Evaluating cross-lingual extractive question answering. ArXiv, abs/1910.07475. Seungyoung Lim, Myungji Kim, and Jooyoul Lee. 2019. Korquad1.0: Korean qa dataset for machine reading comprehension. . 2018. Language models are unsupervised multitask learners. CoRR. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable questions for squad. CoRR, abs/1806.03822. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In",
        "id": 211126910
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which paper first investigates the knowledge preferences of LLMs when there are conflicts between the context and the parametric memory?",
    "positive_ctxs": [
      {
        "title": "Adaptive Chameleon or Stubborn Sloth: REVEALING THE BEHAVIOR OF LARGE LANGUAGE MODELS IN KNOWLEDGE CONFLICTS",
        "text": "By providing external information to large language models (LLMs), tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory.However, how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory?We present the first comprehensive and controlled investigation into the behavior of LLMs when encountering knowledge conflicts.We propose a systematic framework to elicit high-quality parametric memory from LLMs and construct the corresponding counter-memory, which enables us to conduct a series of controlled experiments.Our investigation reveals seemingly contradicting behaviors of LLMs.On the one hand, different from prior wisdom, we find that LLMs can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing.On the other hand, LLMs also demonstrate a strong confirmation bias when the external evidence contains some information that is consistent with their parametric memory, despite being presented with conflicting evidence at the same time.These results pose important implications that are worth careful consideration for the further development and deployment of tool-and retrieval-augmented LLMs. 1 * The first two authors contributed equally.Work done during Jian Xie's internship at OSU NLP Group.",
        "id": 263610128
      }
    ],
    "negative_ctxs": [
      {
        "title": "A Dataset for Detecting Humor in Arabic Text",
        "text": "Humor detection is a complex and ambiguous task in natural language processing. This has made automatic humor detection challenging, particularly for languages with limited resources such as Arabic. In this paper, we attempt to solve this task by collecting and annotating Arabic humorous tweets in dialects and Modern Standard Arabic (MSA) text then performing automatic humor detection on the collected data. We experimented on the collected dataset by fine-tuning seven Arabic Pre-Trained language models (PLMs) which are: AraBERTv02, Arabertv02-twitter, QARIB, MarBERT, MARBERTv2, CAMeLBERT-DA, and CAMeLBERT-MIX to establish a baseline classification system. We concluded that CAMeLBERT-DA was the bestperforming model and it achieved an F1score and accuracy of 72.11%.Related workThe task of detecting humor in text has drawn the attention of many researchers in different languages. Although there is no single definition of",
        "id": 256739243
      },
      {
        "title": "Double-Branch Multi-Attention based Graph Neural Network for Knowledge Graph Completion",
        "text": "Graph neural networks (GNNs), which effectively use topological structures in the knowledge graphs (KG) to embed entities and relations in low-dimensional spaces, have shown great power in knowledge graph completion (KGC). KG has abundant global and local structural information, however, many GNN-based KGC models cannot capture these two types of information about the graph structure by designing complex aggregation schemes and are not designed well to learn representations of seen entities with sparse neighborhoods in isolated subgraphs. In this paper, we find that a simple attention-based method can outperform a general GNN-based approach for KGC. We then propose a double-branch multi-attentionbased graph neural network (MA-GNN) to learn more expressive entity representations that contain rich global-local structural information. Specifically, we first explore the graph attention network-based local aggregator to learn entity representations. Furthermore, we propose a snowball local attention mechanism by leveraging the semantic similarity between two-hop neighbors to enrich the entity embedding. Finally, we use Transformer-based selfattention to learn long-range dependence between entities to obtain richer representations with the global graph structure and entity features. Experimental results on five benchmark datasets show that MA-GNN achieves significant improvements over strong baselines for inductive KGC.",
        "id": 259370784
      },
      {
        "title": "Saarland: Vector-based models of semantic textual similarity",
        "text": "This paper describes our system for the Semeval 2012 Sentence Textual Similarity task. The system is based on a combination of few simple vector space-based methods for word meaning similarity. Evaluation results show that a simple combination of these unsupervised data-driven methods can be quite successful. The simple vector space components achieve high performance on short sentences; on longer, more complex sentences, they are outperformed by a surprisingly competitive word overlap baseline, but they still bring improvements over this baseline when incorporated into a mixture model.",
        "id": 7098054
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you suggest research that investigates how many evidence sentences are needed for document-level RE?",
    "positive_ctxs": [
      {
        "title": "Three Sentences Are All You Need: Local Path Enhanced Document Relation Extraction",
        "text": "Document-level Relation Extraction (RE) is a more challenging task than sentence RE as it often requires reasoning over multiple sentences. Yet, human annotators usually use a small number of sentences to identify the relationship between a given entity pair. In this paper, we present an embarrassingly simple but effective method to heuristically select evidence sentences for document-level RE, which can be easily combined with BiLSTM to achieve good performance on benchmark datasets, even better than fancy graph neural network based methods. We have released our code at https://github.com/AndrewZhe/Three-Sentences-Are-All-You-Need.",
        "id": 235313346
      }
    ],
    "negative_ctxs": [
      {
        "title": "Sociolinguistics for Computational Social Science",
        "text": "In recent years, a major growth area in applied natural language processing has been the application of automated techniques to massive datasets in order to answer questions about society, and by extension people. Sociolinguistics, which combines anthropology, statistics and linguistics (e.g.Labov 1994Labov , 2001, studies linguistic data in order to answer key questions about the relationship of language and society. Sociolinguists focus on frequency and patterns in linguistic usage, correlations, strength of factors and significance, which together reveal information about the sex, age, education and occupation of speakers/writers but also their history, culture, place of residence, social relationships and affiliations. The findings arising from this type research offer important insights into the nature of human organizations at the global, national or community level. They also reveal connections and interactions, the convergence and divergence of groups, historical associations and developing trends.",
        "id": 37495699
      },
      {
        "title": "Introducing the CURLICAT Corpora: Seven-language Domain Specific Annotated Corpora from Curated Sources",
        "text": "This article presents the current outcomes of the CURLICAT CEF Telecom project, which aims to collect and deeply annotate a set of large corpora from selected domains. The CURLICAT corpus includes 7 monolingual corpora (Bulgarian, Croatian, Hungarian, Polish, Romanian, Slovak and Slovenian) containing selected samples from respective national corpora. These corpora are automatically tokenized, lemmatized and morphologically analysed and the named entities annotated. The annotations are uniformly provided for each language specific corpus while the common metadata schema is harmonised across the languages. Additionally, the corpora are annotated for IATE terms in all languages. The file format is CoNLL-U Plus format, containing the ten columns specific to the CoNLL-U format and three extra columns specific to our corpora as defined by Varádi et al. (2020). The CURLICAT corpora represent a rich and valuable source not just for training NMT models, but also for further studies and developments in machine learning, cross-lingual terminological data extraction and classification.",
        "id": 250150837
      },
      {
        "title": "Disentangled Learning of Stance and Aspect Topics for Vaccine Attitude Detection in Social Media",
        "text": "Building models to detect vaccine attitudes on social media is challenging because of the composite, often intricate aspects involved, and the limited availability of annotated data. Existing approaches have relied heavily on supervised training that requires abundant annotations and pre-defined aspect categories. Instead, with the aim of leveraging the large amount of unannotated data now available on vaccination, we propose a novel semi-supervised approach for vaccine attitude detection, called VADET. A variational autoencoding architecture based on language models is employed to learn from unlabelled data the topical information of the domain. Then, the model is fine-tuned with a few manually annotated examples of user attitudes. We validate the effectiveness of VADET on our annotated data and also on an existing vaccination corpus annotated with opinions on vaccines. Our results show that VADET is able to learn disentangled stance and aspect topics, and outperforms existing aspect-based sentiment analysis models on both stance detection and tweet clustering. Our source code and dataset are available at http://github. com/somethingx1202/VADet.",
        "id": 248562688
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which paper utilized MMD flows with Riesz kernels to solve Bayesian inverse problems?",
    "positive_ctxs": [
      {
        "title": "Posterior Sampling Based on Gradient Flows of the MMD with Negative Distance Kernel",
        "text": "We propose conditional flows of the maximum mean discrepancy (MMD) with the negative distance kernel for posterior sampling and conditional generative modelling.This MMD, which is also known as energy distance, has several advantageous properties like efficient computation via slicing and sorting.We approximate the joint distribution of the ground truth and the observations using discrete Wasserstein gradient flows and establish an error bound for the posterior distributions.Further, we prove that our particle flow is indeed a Wasserstein gradient flow of an appropriate functional.The power of our method is demonstrated by numerical examples including conditional image generation and inverse problems like superresolution, inpainting and computed tomography in low-dose and limited-angle settings.",
        "id": 263671662
      }
    ],
    "negative_ctxs": [
      {
        "title": "Biomedical Event Extraction with Hierarchical Knowledge Graphs",
        "text": "Biomedical event extraction is critical in understanding biomolecular interactions described in scientific corpus.",
        "id": 222303317
      },
      {
        "title": "Generation of Referring Expression Using Prefix Tree Structure",
        "text": "This paper presents a Prefix Tree (Trie) based model for Generation of Referring Expression (GRE). The existing algorithms in GRE lie in two extremities. Incremental algorithm is simple and speedy but less expressive in nature whereas others are complex and exhaustive but more expressive in nature. Our prefix tree based model not only incorporates all relevant features of GRE (like describing set, generating Boolean and context sensitive description etc.) but also try to attain simplicity and speed properties of Incremental algorithm. Thus this model provides a simple and linguistically rich approach to GRE.",
        "id": 2111529
      },
      {
        "title": "Because Size Does Matter: The Hamburg Dependency Treebank",
        "text": "We present the Hamburg Dependency Treebank (HDT), which to our knowledge is the largest dependency treebank currently available. It consists of genuine dependency annotations, i. e. they have not been transformed from phrase structures. We explore characteristics of the treebank and compare it against others. To exemplify the benefit of large dependency treebanks, we evaluate different parsers on the HDT. In addition, a set of tools will be described which help working with and searching in the treebank.",
        "id": 14586802
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Have any research papers suggested techniques for automatically choosing in-context examples?",
    "positive_ctxs": [
      {
        "title": "Active Example Selection for In-Context Learning",
        "text": "With a handful of demonstration examples, large-scale language models show strong capability to perform various tasks by in-context learning from these examples, without any finetuning. We demonstrate that in-context learning performance can be highly unstable across samples of examples, indicating the idiosyncrasies of how language models acquire information. We formulate example selection for in-context learning as a sequential decision problem, and propose a reinforcement learning algorithm for identifying generalizable policies to select demonstration examples. For GPT-2, our learned policies demonstrate strong abilities of generalizing to unseen tasks in training, with a 5.8% improvement on average. Examples selected from our learned policies can even achieve a small improvement on GPT-3 Ada. However, the improvement diminishes on larger GPT-3 models, suggesting emerging capabilities of large language models. . 2022. We-bGPT: Browser-assisted question-answering with human feedback.",
        "id": 253420743
      }
    ],
    "negative_ctxs": [
      {
        "title": "Enjambment Detection in a Large Diachronic Corpus of Spanish Sonnets Enjambment Detection in a Large Diachronic Corpus of Spanish Sonnets",
        "text": "Enjambment takes place when a syntactic unit is broken up across two lines of poetry, giving rise to different stylistic effects. In Spanish literary studies, there are unclear points about the types of stylistic effects that can arise, and under which linguistic conditions. To systematically gather evidence about this, we developed a system to automatically identify enjambment (and its type) in Spanish. For evaluation, we manually annotated a reference corpus covering different periods. As a scholarly corpus to apply the tool, from public HTML sources we created a diachronic corpus covering four centuries of sonnets (3750 poems), and we analyzed the occurrence of enjambment across stanzaic boundaries in different periods. Besides, we found examples that highlight limitations in current definitions of enjambment.",
        "id": 28722680
      },
      {
        "title": "Error Identification for Machine Translation with Metric Embedding and Attention",
        "text": "Quality Estimation (QE) for Machine Translation has been shown to reach relatively high accuracy in predicting sentence-level scores, relying on pretrained contextual embeddings and human-produced quality scores. However, the lack of explanations along with decisions made by end-to-end neural models makes the results difficult to interpret. Furthermore, word-level annotated datasets are rare due to the prohibitive effort required to perform this task, while they could provide interpretable signals in addition to sentence-level QE outputs. In this paper, we propose a novel QE architecture which tackles both the wordlevel data scarcity and the interpretability limitations of recent approaches. Sentence-level and word-level components are jointly pretrained through an attention mechanism based on synthetic data and a set of MT metrics embedded in a common space. Our approach is evaluated on the Eval4NLP 2021 shared task and our submissions reach the first position in all language pairs. The extraction of metricto-input attention weights show that different metrics focus on different parts of the source and target text, providing strong rationales in the decision-making process of the QE model.",
        "id": 241583432
      },
      {
        "title": "Ask to Understand: Question Generation for Multi-hop Question Answering",
        "text": "Multi-hop Question Answering (QA) requires the machine to answer complex questions by finding scattering clues and reasoning from multiple documents. Graph Network (GN) and Question Decomposition (QD) are two common approaches at present. The former uses the \"black-box\" reasoning process to capture the potential relationship between entities and sentences, thus achieving good performance. At the same time, the latter provides a clear reasoning logical route by decomposing multi-hop questions into simple singlehop sub-questions. In this paper, we propose a novel method to complete multi-hop QA from the perspective of Question Generation (QG). Specifically, we carefully design an endto-end QG module on the basis of a classical QA module, which could help the model understand the context by asking inherently logical sub-questions, thus inheriting interpretability from the QD-based method and showing superior performance. Experiments on the HotpotQA dataset demonstrate that the effectiveness of our proposed QG module, human evaluation further clarifies its interpretability quantitatively, and thorough analysis shows that the QG module could generate better subquestions than QD methods in terms of fluency, consistency, and diversity.\"Educated mind first sign is good at asking questions.\" -Plekhanov G.V.",
        "id": 247518809
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there any works that explores how to achieve balance between representativeness and diversity in chosen samples for few-shot data selection?",
    "positive_ctxs": [
      {
        "title": "Cold-Start Data Selection for Better Few-shot Language Model Fine-tuning: A Prompt-based Uncertainty Propagation Approach",
        "text": "Large Language Models have demonstrated remarkable few-shot performance, but the performance can be sensitive to the selection of few-shot instances. We present PATRON, a prompt-based data selection method for pretrained language model fine-tuning under coldstart scenarios, i.e., no initial labeled data are available. In PATRON, we design (1) a promptbased uncertainty propagation approach to estimate the importance of data points and (2) a partition-then-rewrite (PTR) strategy to promote sample diversity when querying for annotations. Experiments on six text classification datasets show that PATRON outperforms the strongest cold-start data selection baselines by up to 6.9%. Besides, with 128 labels only, PA-TRON achieves 91.0% and 92.1% of the fully supervised performance based on vanilla finetuning and prompt-based learning respectively. Our implementation of PATRON is available at https://github.com/yueyu1030/Patron.",
        "id": 252280753
      }
    ],
    "negative_ctxs": [
      {
        "title": "Efficient Domain Adaptation of Language Models via Adaptive Tokenization",
        "text": "Contextual embedding-based language models trained on large data sets, such as BERT and RoBERTa, provide strong performance across a wide range of tasks and are ubiquitous in modern NLP. It has been observed that fine-tuning these models on tasks involving data from domains different from that on which they were pretrained can lead to suboptimal performance. Recent work has explored approaches to adapt pretrained language models to new domains by incorporating additional pretraining using domain-specific corpora and task data. We propose an alternative approach for transferring pretrained language models to new domains by adapting their tokenizers. We show that domain-specific subword sequences can be efficiently determined directly from divergences in the conditional token distributions of the base and domain-specific corpora. In datasets from four disparate domains, we find adaptive tokenization on a pretrained RoBERTa model provides >97% of the performance benefits of domain specific pretraining. Our approach produces smaller models and less training and inference time than other approaches using tokenizer augmentation. While adaptive tokenization incurs a 6% increase in model parameters in our experimentation, due to the introduction of 10k new domain-specific tokens, our approach, using 64 vCPUs, is 72x faster than further pretraining the language model on domain-specific corpora on 8 TPUs.",
        "id": 237513469
      },
      {
        "title": "Multilingual unsupervised sequence segmentation transfers to extremely low-resource languages",
        "text": "We show that unsupervised sequencesegmentation performance can be transferred to extremely low-resource languages by pretraining a Masked Segmental Language Model (Downey et al., 2021)  multilingually. Further, we show that this transfer can be achieved by training over a collection of low-resource languages that are typologically similar (but phylogenetically unrelated) to the target language. In our experiments, we transfer from a collection of 10 Indigenous American languages (AmericasNLP, Mager et al., 2021)   to K'iche', a Mayan language. We compare our multilingual model to a monolingual (from-scratch) baseline, as well as a model pre-trained on Quechua only. We show that the multilingual pre-trained approach yields consistent segmentation quality across target dataset sizes, exceeding the monolingual baseline in 6/10 experimental settings. Our model yields especially strong results at small target sizes, including a zero-shot performance of 20.6 F1. These results have promising implications for low-resource NLP pipelines involving human-like linguistic units, such as the sparse transcription framework proposed by Bird (2020).",
        "id": 239016040
      },
      {
        "title": "Under review as a conference paper at ICLR 2017 LEARNING RECURRENT REPRESENTATIONS FOR HIERARCHICAL BEHAVIOR MODELING",
        "text": "We propose a framework for detecting action patterns from motion sequences and modeling the sensory-motor relationship of animals, using a generative recurrent neural network. The network has a discriminative part (classifying actions) and a generative part (predicting motion), whose recurrent cells are laterally connected, allowing higher levels of the network to represent high level behavioral phenomena. We test our framework on two types of data, fruit fly behavior and online handwriting. Our results show that 1) taking advantage of unlabeled sequences, by predicting future motion, significantly improves action detection performance when training labels are scarce, 2) the network learns to represent high level phenomena such as writer identity and fly gender, without supervision, and 3) simulated motion trajectories, generated by treating motion prediction as input to the network, look realistic and may be used to qualitatively evaluate whether the model has learnt generative control rules.",
        "id": 16926563
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Has there been any recent work or competitions focused on the development of methods to counteract clickbait through spoiling, such as revealing key information upfront?",
    "positive_ctxs": [
      {
        "title": "SemEval-2023 Task 5: Clickbait Spoiling",
        "text": "In this overview paper, we report on the second PAN Clickbait Challenge hosted as Task 5 at SemEval 2023. The challenge's focus is to better support social media users by automatically generating short spoilers that close the curiosity gap induced by a clickbait post. We organized two subtasks: (1) spoiler type classification to assess what kind of spoiler a clickbait post warrants (e.g., a phrase), and (2) spoiler generation to generate an actual spoiler for a clickbait post. 2 github.com/pan-webis-de/SEMEVAL-2023 2275",
        "id": 259376639
      }
    ],
    "negative_ctxs": [
      {
        "title": "An Evaluation Exercise for Romanian Word Sense Disambiguation",
        "text": "This paper presents the task definition, resources, participating systems, and comparative results for a Romanian Word Sense Disambiguation task, which was organized as part of the SENSEVAL-3 evaluation exercise. Five teams with a total of seven systems were drawn to this task.",
        "id": 752448
      },
      {
        "title": "Using J-K-fold Cross Validation to Reduce Variance When Tuning NLP Models",
        "text": "K-fold cross validation (CV) is a popular method for estimating the true performance of machine learning models, allowing model selection and parameter tuning. However, the very process of CV requires random partitioning of the data and so our performance estimates are in fact stochastic, with variability that can be substantial for natural language processing tasks. We demonstrate that these unstable estimates cannot be relied upon for effective parameter tuning. The resulting tuned parameters are highly sensitive to how our data is partitioned, meaning that we often select sub-optimal parameter choices and have serious reproducibility issues.Instead, we propose to use the less variable J-K-fold CV, in which J independent K-fold cross validations are used to assess performance. Our main contributions are extending J-K-fold CV from performance estimation to parameter tuning and investigating how to choose J and K. We argue that variability is more important than bias for effective tuning and so advocate lower choices of K than are typically seen in the NLP literature, instead use the saved computation to increase J. To demonstrate the generality of our recommendations we investigate a wide range of case-studies: sentiment classification (both general and target-specific), part-of-speech tagging and document classification.",
        "id": 49311178
      },
      {
        "title": "The SuperARV Language Model: Investigating the Effectiveness of Tightly Integrating Multiple Knowledge Sources",
        "text": "A new almost-parsing language model incorporating multiple knowledge sources that is based upon the concept of Constraint Dependency Grammars is presented in this paper. Lexical features and syntactic constraints are tightly integrated into a uniform linguistic structure called a SuperARV that is associated with a word in the lexicon. The Super-ARV language model reduces perplexity and word error rate compared to trigram, part-of-speech-based, and parser-based language models. The relative contributions of the various knowledge sources to the strength of our model are also investigated by using constraint relaxation at the level of the knowledge sources. We have found that although each knowledge source contributes to language model quality, lexical features are an outstanding contributor when they are tightly integrated with word identity and syntactic constraints. Our investigation also suggests possible reasons for the reported poor performance of several probabilistic dependency grammar models in the literature.Johnson(2001)andLafferty et al. (2001)provide insight into why a joint model is superior to a conditional model.Recently, there has been good progress in developing structured models (Chelba, 2000; Charniak, Association for Computational Linguistics.",
        "id": 5820758
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "What are some of the key papers to look at for understanding how attention mechanisms have been used to enhance bidirectional recurrent neural networks in relation classification tasks?",
    "positive_ctxs": [
      {
        "title": "Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification",
        "text": "Relation classification is an important semantic processing task in the field of natural language processing (NLP). State-ofthe-art systems still rely on lexical resources such as WordNet or NLP systems like dependency parser and named entity recognizers (NER) to get high-level features. Another challenge is that important information can appear at any position in the sentence. To tackle these problems, we propose Attention-Based Bidirectional Long Short-Term Memory Networks(Att-BLSTM) to capture the most important semantic information in a sentence. The experimental results on the SemEval-2010 relation classification task show that our method outperforms most of the existing methods, with only word vectors.",
        "id": 9870160
      }
    ],
    "negative_ctxs": [
      {
        "title": "Hybrid Approach to Zero Subject Resolution for multilingual MT -Spanish-to-Korean Cases",
        "text": "The current paper proposes a novel approach to Spanish",
        "id": 5236680
      },
      {
        "title": "Character-level Representations Improve DRS-based Semantic Parsing Even in the Age of BERT",
        "text": "We combine character-level and contextual language model representations to improve performance on Discourse Representation Structure parsing. Character representations can easily be added in a sequence-to-sequence model in either one encoder or as a fully separate encoder, with improvements that are robust to different language models, languages and data sets. For English, these improvements are larger than adding individual sources of linguistic information or adding non-contextual embeddings. A new method of analysis based on semantic tags demonstrates that the character-level representations improve performance across a subset of selected semantic phenomena.",
        "id": 226308548
      },
      {
        "title": "Universal Semantic Parsing",
        "text": "Universal Dependencies (UD) offer a uniform cross-lingual syntactic representation, with the aim of advancing multilingual applications. Recent work shows that semantic parsing can be accomplished by transforming syntactic dependencies to logical forms. However, this work is limited to English, and cannot process dependency graphs, which allow handling complex phenomena such as control. In this work, we introduce UDEPLAMBDA, a semantic interface for UD, which maps natural language to logical forms in an almost language-independent fashion and can process dependency graphs. We perform experiments on question answering against Freebase and provide German and Spanish translations of the WebQuestions and GraphQuestions datasets to facilitate multilingual evaluation. Results show that UDEPLAMBDA outperforms strong baselines across languages and datasets. For English, it achieves a 4.9 F 1 point improvement over the state-of-the-art on Graph-Questions.",
        "id": 10436313
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a study that investigates how a subset with clean, annotated datasets improve denoising methods?",
    "positive_ctxs": [
      {
        "title": "Semi-Supervised Data Programming with Subset Selection",
        "text": "The paradigm of data programming, which uses weak supervision in the form of rules/labelling functions, and semi-supervised learning, which augments small amounts of labelled data with a large unlabelled dataset, have shown great promise in several text classification scenarios. In this work, we argue that by not using any labelled data, data programming based approaches can yield sub-optimal performances, particularly when the labelling functions are noisy. The first contribution of this work is an introduction of a framework, SPEAR which is a semi-supervised data programming paradigm that learns a joint model that effectively uses the rules/labelling functions along with semi-supervised loss functions on the feature space. Next, we also study SPEAR-SS which additionally does subset selection on top of the joint semi-supervised data programming objective and selects a set of examples that can be used as the labelled set by SPEAR. The goal of SPEAR-SS is to ensure that the labelled data can complement the labelling functions, thereby benefiting from both data-programming as well as appropriately selected data for human labelling. We demonstrate that by effectively combining semi-supervision, data-programming, and subset selection paradigms, we significantly outperform the current state-of-the-art on seven publicly available datasets. 1",
        "id": 235421599
      }
    ],
    "negative_ctxs": [
      {
        "title": "AutoLoss: Learning Discrete Schedules for Alternate Optimization",
        "text": "Many machine learning problems involve iteratively and alternately optimizing different task objectives with respect to different sets of parameters. Appropriately scheduling the optimization of a task objective or a set of parameters is usually crucial to the quality of convergence. In this paper, we present AutoLoss, a meta-learning framework that automatically learns and determines the optimization schedule. AutoLoss provides a generic way to represent and learn the discrete optimization schedule from metadata, allows for a dynamic and data-driven schedule in ML problems that involve alternating updates of different parameters or from different loss objectives. We apply AutoLoss on four ML tasks: d-ary quadratic regression, classification using a multi-layer perceptron (MLP), image generation using GANs, and multi-task neural machine translation (NMT). We show that the AutoLoss controller is able to capture the distribution of better optimization schedules that result in higher quality of convergence on all four tasks. The trained AutoLoss controller is generalizable -it can guide and improve the learning of a new task model with different specifications, or on different datasets.",
        "id": 52926194
      },
      {
        "title": "Effective Tag Set Selection in Chinese Word Segmentation via Conditional Random Field Modeling",
        "text": "This paper is concerned with Chinese word segmentation, which is regarded as a character based tagging problem under conditional random field framework. It is different in our method that we consider both feature template selection and tag set selection, instead of feature template focused only method in existing work. Thus, there comes an empirical comparison study of performance among different tag sets in this paper. We show that there is a significant performance difference as different tag sets are selected. Based on the proposed method, our system gives the state-of-the-art performance.",
        "id": 18371469
      },
      {
        "title": "Perturbation CheckLists for Evaluating NLG Evaluation Metrics",
        "text": "Natural Language Generation (NLG) evaluation is a multifaceted task requiring assessment of multiple desirable criteria, e.g., fluency, coherency, coverage, relevance, adequacy, overall quality, etc. Across existing datasets for 6 NLG tasks, we observe that the human evaluation scores on these multiple criteria are often not correlated. For example, there is a very low correlation between human scores on fluency and data coverage for the task of structured data to text generation. This suggests that the current recipe of proposing new automatic evaluation metrics for NLG by showing that they correlate well with scores assigned by humans for a single criteria (overall quality) alone is inadequate. Indeed, our extensive study involving 25 automatic evaluation metrics across 6 different tasks and 18 different evaluation criteria shows that there is no single metric which correlates well with human scores on all desirable criteria, for most NLG tasks. Given this situation, we propose CheckLists for better design and evaluation of automatic metrics. We design templates which target a specific criteria (e.g., coverage) and perturb the output such that the quality gets affected only along this specific criteria (e.g., the coverage drops). We show that existing evaluation metrics are not robust against even such simple perturbations and disagree with scores assigned by humans to the perturbed output. The proposed templates thus allow for a fine-grained assessment of automatic evaluation metrics exposing their limitations and will facilitate better design, analysis and evaluation of such metrics. 1Original sentence: Cameron is the director of Titanic Change names:Kate is the director of Titanic (incorrect)Task Criteria Machine TranslationAdequacy: The generated translation should adequately represent all the information present in the reference.Question GenerationRelevance: Is the question related to the source material they are based upon. Answerability: Is the generated question answerable given the context. Informativeness: The summary should convey the key points of the text. Non-redundancy: The summary should not repeat any points, and ideally have maximal information coverage within the limited text length.Abstractive SummarizationReferential clarity: Any intra-sentence or cross-sentence references in the summary should be unambiguous and within the scope of the summary. Focus: The summary needs to have a focus and all the sentences need to contain information related to this focal point. Structure and Coherence: The summary should be a well-organized and coherent body of informationDialogue GenerationMaking sense: Does the bot say things that don't make sense? Engagingness: Is the dialogue agent enjoyable to talk to? Interestingness: Did you find the bot interesting to talk to? Inquisitivenes: Does the bot ask a good amount of questions? Listening: Does the bot pay attention to what you say? Avoiding Repetition: Does the bot repeat itself? (either within or across utterances) Humanness: Is the conversation with a person or a bot?",
        "id": 237492016
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "Which paper in human motion generation can control the spatial location of any joints of the human with either dense or sparse 3D points?",
    "positive_ctxs": [
      {
        "title": "OMNICONTROL: CONTROL ANY JOINT AT ANY TIME FOR HUMAN MOTION GENERATION",
        "text": "We present a novel approach named OmniControl for incorporating flexible spatial control signals into a text-conditioned human motion generation model based on the diffusion process.Unlike previous methods that can only control the pelvis trajectory, OmniControl can incorporate flexible spatial control signals over different joints at different times with only one model.Specifically, we propose analytic spatial guidance that ensures the generated motion can tightly conform to the input control signals.At the same time, realism guidance is introduced to refine all the joints to generate more coherent motion.Both the spatial and realism guidance are essential and they are highly complementary for balancing control accuracy and motion realism.By combining them, OmniControl generates motions that are realistic, coherent, and consistent with the spatial constraints.Experiments on HumanML3D and KIT-ML datasets show that OmniControl not only achieves significant improvement over state-of-the-art methods on pelvis control but also shows promising results when incorporating the constraints over other joints.Project page: https://neu-vi.github.io/omnicontrol/.",
        "id": 263909429
      }
    ],
    "negative_ctxs": [
      {
        "title": "Large-scale Dictionary Construction via Pivot-based Statistical Machine Translation with Significance Pruning and Neural Network Features",
        "text": "We present our ongoing work on large-scale Japanese-Chinese bilingual dictionary construction via pivot-based statistical machine translation. We utilize statistical significance pruning to control noisy translation pairs that are induced by pivoting. We construct a large dictionary which we manually verify to be of a high quality. We then use this dictionary and a parallel corpus to learn bilingual neural network language models to obtain features for reranking the n-best list, which leads to an absolute improvement of 5% in accuracy when compared to a setting that does not use significance pruning and reranking.",
        "id": 12774566
      },
      {
        "title": "MISGAN: LEARNING FROM INCOMPLETE DATA WITH GENERATIVE ADVERSARIAL NETWORKS",
        "text": "Generative adversarial networks (GANs) have been shown to provide an effective way to model complex distributions and have obtained impressive results on various challenging tasks. However, typical GANs require fully-observed data during training. In this paper, we present a GAN-based framework for learning from complex, high-dimensional incomplete data. The proposed framework learns a complete data generator along with a mask generator that models the missing data distribution. We further demonstrate how to impute missing data by equipping our framework with an adversarially trained imputer. We evaluate the proposed framework using a series of experiments with several types of missing data processes under the missing completely at random assumption. 1 Let x obs denote the observed elements of x, and x mis denote the missing elements according to the mask m. In addition, let θ denote the unknown parameters of the data distribution, and φ denote the unknown parameters for the mask distribution, which are usually assumed to be independent of θ. In the standard maximum likelihood setting, the unknown parameters are estimated by maximizing the 1 Our implementation is available at https://github.com/steveli/misgan 2 The complementm is usually referred to as the missing data indicator in the literature.Published as a conference paper at ICLR 2019 following marginal likelihood, integrating over the unknown missing data values:p(x obs , m) = p θ (x obs , x mis )p φ (m|x obs , x mis )dx mis .Little & Rubin (2014) characterize the missing data mechanism p φ (m|x obs , x mis ) in terms of independence relations between the complete data x = [x obs , x mis ] and the masks m:• Missing completely at random (MCAR): p φ (m|x) = p φ (m),• Missing at random (MAR): p φ (m|x) = p φ (m|x obs ),• Not missing at random (NMAR): m depends on x mis and possibly also x obs .",
        "id": 67856605
      },
      {
        "title": "UIC-NLP at SemEval-2022 Task 5: Exploring Contrastive Learning for Multimodal Detection of Misogynistic Memes",
        "text": "Misogynistic memes are rampant on social media, and often convey their messages using multimodal signals (e.g., images paired with derogatory text or captions). However, to date very few multimodal systems have been leveraged for the detection of misogynistic memes. Recently, researchers have turned to contrastive learning solutions for a variety of problems. Most notably, OpenAI's CLIP model has served as an innovative solution for a variety of multimodal tasks. In this work, we experiment with contrastive learning to address the detection of misogynistic memes within the context of SemEval-2022 Task 5. Although our model does not achieve top results, these experiments provide important exploratory findings for this task. We conduct a detailed error analysis, revealing promising clues and offering a foundation for follow-up work.",
        "id": 250390575
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_acl",
    "question": "Is there a study that shows how to help the demonstration retriever better integrate feedback from LLMs?",
    "positive_ctxs": [
      {
        "title": "Unified Demonstration Retriever for In-Context Learning",
        "text": "In-context learning is a new learning paradigm where a language model conditions on a few input-output pairs (demonstrations) and a test input, and directly outputs the prediction. It has been shown highly dependent on the provided demonstrations and thus promotes the research of demonstration retrieval: given a test input, relevant examples are retrieved from the training set to serve as informative demonstrations for in-context learning. While previous works focus on training task-specific retrievers for several tasks separately, these methods are often hard to transfer and scale on various tasks, and separately trained retrievers incur a lot of parameter storage and deployment cost. In this paper, we propose Unified Demonstration Retriever (UDR), a single model to retrieve demonstrations for a wide range of tasks. To train UDR, we cast various tasks' training signals into a unified list-wise ranking formulation by language model's feedback. Then we propose a multi-task list-wise ranking training framework, with an iterative mining strategy to find high-quality candidates, which can help UDR fully incorporate various tasks' signals. Experiments on 30+ tasks across 13 task families and multiple data domains show that UDR significantly outperforms baselines. Further analyses show the effectiveness of each proposed component and UDR's strong ability in various scenarios including different LMs (1.3B ∼ 175B), unseen datasets, varying demonstration quantities, etc. * Equal Contribution † Corresponding Authors Retriever LM",
        "id": 258557751
      }
    ],
    "negative_ctxs": [
      {
        "title": "Vector-Vector-Matrix Architecture: A Novel Hardware-Aware Framework for Low-Latency Inference in NLP Applications",
        "text": "Deep neural networks have become the standard approach to building reliable Natural Language Processing (NLP) applications, ranging from Neural Machine Translation (NMT) to dialogue systems. However, improving accuracy by increasing the model size requires a large number of hardware computations, which can slow down NLP applications significantly at inference time. To address this issue, we propose a novel vector-vector-matrix architecture (VVMA), which greatly reduces the latency at inference time for NMT. This architecture takes advantage of specialized hardware that has low-latency vector-vector operations and higher-latency vector-matrix operations. It also reduces the number of parameters and FLOPs for virtually all models that rely on efficient matrix multipliers without significantly impacting accuracy. We present empirical results suggesting that our framework can reduce the latency of sequence-to-sequence and Transformer models used for NMT by a factor of four. Finally, we show evidence suggesting that our VVMA extends to other domains, and we discuss novel hardware for its efficient use.",
        "id": 223957092
      },
      {
        "title": "Self-Supervised Learning for Pairwise Data Refinement",
        "text": "Pairwise data automatically constructed from weakly supervised signals has been widely used for training deep learning models. Pairwise datasets such as parallel texts can have uneven quality levels overall, but usually contain data subsets that are more useful as learning examples. We present two methods to refine data that are aimed at obtaining that kind of subsets in a self-supervised way. Our methods are based on iteratively training dualencoder models to compute similarity scores. We evaluate our methods on de-noising parallel texts and training neural machine translation models. We find that: (i) The self-supervised refinement achieves most machine translation gains in the first iteration, but following iterations further improve its intrinsic evaluation. (ii) Machine translations can improve the de-noising performance when combined with selection steps. (iii) Our methods are able to reach the performance of a supervised method. Being entirely self-supervised, our methods are well-suited to handle pairwise data without the need of prior knowledge or human annotations.",
        "id": 227905412
      },
      {
        "title": "The FINITE STRING Newsletter Site Report Controlling Complex Systems of Linguistic Rules",
        "text": "",
        "id": 2666131
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_nonacl",
    "question": "Could you recommend a study that investigates employing graph neural networks to produce replies within multi-party conversational contexts?",
    "positive_ctxs": [
      {
        "title": "HETERMPC: A Heterogeneous Graph Neural Network for Response Generation in Multi-Party Conversations",
        "text": "Recently, various response generation models for two-party conversations have achieved impressive improvements, but less effort has been paid to multi-party conversations (MPCs) which are more practical and complicated. Compared with a two-party conversation where a dialogue context is a sequence of utterances, building a response generation model for MPCs is more challenging, since there exist complicated context structures and the generated responses heavily rely on both interlocutors (i.e., speaker and addressee) and history utterances. To address these challenges, we present HeterMPC, a heterogeneous graph-based neural network for response generation in MPCs which models the semantics of utterances and interlocutors simultaneously with two types of nodes in a graph. Besides, we also design six types of meta relations with node-edge-typedependent parameters to characterize the heterogeneous interactions within the graph. Through multi-hop updating, HeterMPC can adequately utilize the structural knowledge of conversations for response generation. Experimental results on the Ubuntu Internet Relay Chat (IRC) channel benchmark show that HeterMPC outperforms various baseline models for response generation in MPCs.",
        "id": 247476252
      }
    ],
    "negative_ctxs": [
      {
        "title": "4See (Weinreich, 1964), (Fillmore, 1971), (Cruse, 1993), (Pustejovsky, 1995) for interesting accounts on homography/polysemy. See (Bouillon et al., 1992), (Busa, 1996) for accounts on nominals",
        "text": "1In this paper, we address representation issues of Chinese nominals. In particular, we look at lexical rules as a conceptual tool to link forms with the same semantics as is the case between nominalisations and the forms they are derived from. We also address Chinese compounds, illustrating how to recover implicit semantic relations in nominal compounds. Finally, we show how to translate Chinese nontinals within a knowledge-based framework.",
        "id": 17290927
      },
      {
        "title": "Vision and Language Integration: Moving beyond Objects",
        "text": "The last years have seen an explosion of work on the integration of vision and language data. New tasks like Image Captioning and Visual Questions Answering have been proposed and impressive results have been achieved. There is now a shared desire to gain an in-depth understanding of the strengths and weaknesses of those models. To this end, several datasets have been proposed to try and challenge the state-of-the-art. Those datasets, however, mostly focus on the interpretation of objects (as denoted by nouns in the corresponding captions). In this paper, we reuse a previously proposed methodology to evaluate the ability of current systems to move beyond objects and deal with attributes (as denoted by adjectives), actions (verbs), manner (adverbs) and spatial relations (prepositions). We show that the coarse representations given by current approaches are not informative enough to interpret attributes or actions, whilst spatial relations somewhat fare better, but only in attention models.1 The data will be made available at: https://foilunitn.github.io/.",
        "id": 1970133
      },
      {
        "title": "The TechQA Dataset",
        "text": "We introduce TECHQA, a domain-adaptation question answering dataset for the technical support domain. The TECHQA corpus highlights two real-world issues from the automated customer support domain. First, it contains actual questions posed by users on a technical forum, rather than questions generated specifically for a competition or a task. Second, it has a real-world size -600 training, 310 dev, and 490 evaluation question/answer pairs -thus reflecting the cost of creating large labeled datasets with actual data. Hence, TECHQA is meant to stimulate research in domain adaptation rather than as a resource to build QA systems from scratch. TECHQA was obtained by crawling the IB-MDeveloper and DeveloperWorks forums for questions with accepted answers provided in an IBM Technote-a technical document that addresses a specific technical issue. We also release a collection of the 801,998 Technotes available on the web as of April 4, 2019 as a companion resource that can be used to learn representations of the IT domain language.",
        "id": 207847581
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "Where can I find a discourse treebank tailored to Chinese newswire articles that's large enough to make training, development, and test splits?",
    "positive_ctxs": [
      {
        "title": "Building Chinese Discourse Corpus with Connective-driven Dependency Tree Structure",
        "text": "In this paper, we propose a Connectivedriven Dependency Tree (CDT) scheme to represent the discourse rhetorical structure in Chinese language, with elementary discourse units as leaf nodes and connectives as non-leaf nodes, largely motivated by the Penn Discourse Treebank and the Rhetorical Structure Theory. In particular, connectives are employed to directly represent the hierarchy of the tree structure and the rhetorical relation of a discourse, while the nuclei of discourse units are globally determined with reference to the dependency theory. Guided by the CDT scheme, we manually annotate a Chinese Discourse Treebank (CDTB) of 500 documents. Preliminary evaluation justifies the appropriateness of the CDT scheme to Chinese discourse analysis and the usefulness of our manually annotated CDTB corpus.",
        "id": 1781329
      }
    ],
    "negative_ctxs": [
      {
        "title": "Briefly Noted Referential Communication Tasks",
        "text": "",
        "id": 208906
      },
      {
        "title": "TRAINING SOCIALLY ALIGNED LANGUAGE MODELS ON SIMULATED SOCIAL INTERACTIONS",
        "text": "Social alignment in AI systems aims to ensure that these models behave according to established societal values.However, unlike humans, who derive consensus on value judgments through social interaction, current language models (LMs) are trained to rigidly replicate their training corpus in isolation, leading to subpar generalization in unfamiliar scenarios and vulnerability to adversarial attacks.This work presents a novel training paradigm that permits LMs to learn from simulated social interactions.In comparison to existing methodologies, our approach is considerably more scalable and efficient, demonstrating superior performance in alignment benchmarks and human evaluations.This paradigm shift in the training of LMs brings us a step closer to developing AI systems that can robustly and accurately reflect societal norms and values.",
        "id": 264590778
      },
      {
        "title": "Discovering Relations among Named Entities by Detecting Community Structure",
        "text": "This paper proposes a networked data mining method for relations discovery from large corpus. The key idea is representing the named entities pairs and their contexts as the network structure and detecting the communities from the network. Then each community relates to a relation the named entities pairs in the same community have the same relation. Finally, we labeled the relations. Our experiment using the corpus of People's Daily reveals not only that the relations among named entities could be detected with high precision, but also that appropriate labels could be automatically provided for the relations.",
        "id": 14409919
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "manual_iclr",
    "question": "What is the first paper that uses the generalized linear model to analyze multi-neural spike train data?",
    "positive_ctxs": [
      {
        "title": "ONE-HOT GENERALIZED LINEAR MODEL FOR SWITCHING BRAIN STATE DISCOVERY",
        "text": "Exposing meaningful and interpretable neural interactions is critical to understanding neural circuits.Inferred neural interactions from neural signals primarily reflect functional interactions.In a long experiment, subject animals may experience different stages defined by the experiment, stimuli, or behavioral states, and hence functional interactions can change over time.To model dynamically changing functional interactions, prior work employs state-switching generalized linear models with hidden Markov models (i.e., HMM-GLMs).However, we argue they lack biological plausibility, as functional interactions are shaped and confined by the underlying anatomical connectome.Here, we propose a novel prior-informed state-switching GLM.We introduce both a Gaussian prior and a one-hot prior over the GLM in each state.The priors are learnable.We will show that the learned prior should capture the state-constant interaction, shedding light on the underlying anatomical connectome and revealing more likely physical neuron interactions.The state-dependent interaction modeled by each GLM offers traceability to capture functional variations across multiple brain states.Our methods effectively recover true interaction structures in simulated data, achieve the highest predictive likelihood with real neural datasets, and render interaction structures and hidden states more interpretable when applied to real neural data.",
        "id": 264438909
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 227231797
      },
      {
        "title": "Did the Model Understand the Question?",
        "text": "We analyze state-of-the-art deep learning models for three tasks: question answering on (1) images, (2) tables, and (3) passages of text. Using the notion of attribution (word importance), we find that these deep networks often ignore important question terms. Leveraging such behavior, we perturb questions to craft a variety of adversarial examples. Our strongest attacks drop the accuracy of a visual question answering model from 61.1% to 19%, and that of a tabular question answering model from 33.5% to 3.3%. Additionally, we show how attributions can strengthen attacks proposed by Jia and Liang (2017) on paragraph comprehension models. Our results demonstrate that attributions can augment standard measures of accuracy and empower investigation of model performance. When a model is accurate but for the wrong reasons, attributions can surface erroneous logic in the model that indicates inadequacies in the test data.",
        "id": 21673814
      },
      {
        "title": "A Repository of State of the Art and Competitive Baseline Summaries for Generic News Summarization",
        "text": "In the period since 2004, many novel sophisticated approaches for generic multi-document summarization have been developed. Intuitive simple approaches have also been shown to perform unexpectedly well for the task. Yet it is practically impossible to compare the existing approaches directly, because systems have been evaluated on different datasets, with different evaluation measures, against different sets of comparison systems. Here we present a corpus of summaries produced by several state-of-the-art extractive summarization systems or by popular baseline systems. The inputs come from the 2004 DUC evaluation, the latest year in which generic summarization was addressed in a shared task. We use the same settings for ROUGE automatic evaluation to compare the systems directly and analyze the statistical significance of the differences in performance. We show that in terms of average scores the state-of-the-art systems appear similar but that in fact they produce very different summaries. Our corpus will facilitate future research on generic summarization and motivates the need for development of more sensitive evaluation measures and for approaches to system combination in summarization.",
        "id": 16482037
      }
    ]
  },
  {
    "specificity": 1,
    "query_set": "inline_acl",
    "question": "I would like to understand the theoretical basis for using the nuclear norm of a weight matrix as a measure of complexity in linear models for probing tasks. Which paper should I refer to?",
    "positive_ctxs": [
      {
        "title": "Information-Theoretic Probing for Linguistic Structure",
        "text": "The success of neural networks on a diverse set of NLP tasks has led researchers to question how much do these networks actually know about natural language. Probes are a natural way of assessing this. When probing, a researcher chooses a linguistic task and trains a supervised model to predict annotation in that linguistic task from the network's learned representations. If the probe does well, the researcher may conclude that the representations encode knowledge related to the task. A commonly held belief is that using simpler models as probes is better; the logic is that such models will identify linguistic structure, but not learn the task itself. We propose an information-theoretic formalization of probing as estimating mutual information that contradicts this received wisdom: one should always select the highest performing probe one can, even if it is more complex, since it will result in a tighter estimate. The empirical portion of our paper focuses on obtaining tight estimates for how much information BERT knows about parts of speech in a set of five typologically diverse languages that are often underrepresented in parsing research, plus English, totaling six languages. We find BERT accounts for only at most 5% more information than traditional, type-based word embeddings.",
        "id": 215238965
      }
    ],
    "negative_ctxs": [
      {
        "title": "",
        "text": "",
        "id": 164835508
      },
      {
        "title": "Enhanced Multi-Channel Graph Convolutional Network for Aspect Sentiment Triplet Extraction",
        "text": "Aspect Sentiment Triplet Extraction (ASTE) is an emerging sentiment analysis task. Most of the existing studies focus on devising a new tagging scheme that enables the model to extract the sentiment triplets in an end-to-end fashion. However, these methods ignore the relations between words for ASTE task. In this paper, we propose an Enhanced Multi-Channel Graph Convolutional Network model (EMC-GCN) to fully utilize the relations between words. Specifically, we first define ten types of relations for ASTE task, and then adopt a biaffine attention module to embed these relations as an adjacent tensor between words in a sentence. After that, our EMC-GCN transforms the sentence into a multi-channel graph by treating words and the relation adjacent tensor as nodes and edges, respectively. Thus, relationaware node representations can be learnt. Furthermore, we consider diverse linguistic features to enhance our EMC-GCN model. Finally, we design an effective refining strategy on EMC-GCN for word-pair representation refinement, which considers the implicit results of aspect and opinion extraction when determining whether word pairs match or not. Extensive experimental results on the benchmark datasets demonstrate that the effectiveness and robustness of our proposed model, which outperforms state-of-the-art methods significantly.",
        "id": 248780173
      },
      {
        "title": "An annotation scheme and Gold Standard for Dutch-English word alignment",
        "text": "The importance of sentence-aligned parallel corpora has been widely acknowledged. Reference corpora in which sub-sentential translational correspondences are indicated manually are more labour-intensive to create, and hence less wide-spread. Such manually created reference alignments -also called Gold Standards -have been used in research projects to develop or test automatic word alignment systems. In most translations, translational correspondences are rather complex; for example word-by-word correspondences can be found only for a limited number of words. A reference corpus in which those complex translational correspondences are aligned manually is therefore also a useful resource for the development of translation tools and for translation studies. In this paper, we describe how we created a Gold Standard for the Dutch-English language pair. We present the annotation scheme, annotation guidelines, annotation tool and inter-annotator results. To cover a wide range of syntactic and stylistic phenomena that emerge from different writing and translation styles, our Gold Standard data set contains texts from different text types. The Gold Standard will be publicly available as part of the Dutch Parallel Corpus.",
        "id": 9828572
      }
    ]
  }
]