[
  {
    "index": 0,
    "source_corpus_id": 252715594,
    "ref_id": "b57",
    "citation_corpus_id": 238582653,
    "start": 11564,
    "end": 11568,
    "title": "VECTOR-QUANTIZED IMAGE MODELING WITH IM- PROVED VQGAN",
    "abstract": "Pretraining language models with next-token prediction on massive text corpora has delivered phenomenal zero-shot, few-shot, transfer learning and multi-tasking capabilities on both generative and discriminative language tasks. Motivated by this success, we explore a Vector-quantized Image Modeling (VIM) approach that involves pretraining a Transformer to predict rasterized image tokens autoregressively. The discrete image tokens are encoded from a learned Vision-Transformerbased VQGAN (ViT-VQGAN). We first propose multiple improvements over vanilla VQGAN from architecture to codebook learning, yielding better efficiency and reconstruction fidelity. The improved ViT-VQGAN further improves vectorquantized image modeling tasks, including unconditional, class-conditioned image generation and unsupervised representation learning. When trained on Im-ageNet at 256 × 256 resolution, we achieve Inception Score (IS) of 175.1 and Fréchet Inception Distance (FID) of 4.17, a dramatic improvement over the vanilla VQGAN, which obtains 70.6 and 17.04 for IS and FID, respectively. Based on ViT-VQGAN and unsupervised pretraining, we further evaluate the pretrained Transformer by averaging intermediate features, similar to Image GPT (iGPT). This ImageNet-pretrained VIM-L significantly beats iGPT-L on linear-probe accuracy from 60.3% to 73.2% for a similar model size. VIM-L also outperforms iGPT-XL which is trained with extra web image data and larger model size.arXiv:2110.04627v3 [cs.CV] 5 Jun 2022Published as a conference paper at ICLR 2022 Figure 1: Overview of ViT-VQGAN (left) and Vector-quantized Image Modeling (right) for both image generation and image understanding.Remarkable image generation results have been achieved by pre-quantizing images into discrete latent variables and modeling them autoregressively, including VQVAE (Oord et al., 2017), DALL-E (Ramesh et al., 2021)  and VQGAN (Esser et al., 2021). In these approaches, a convolution neural network (CNN) is learned to auto-encode an image and a second stage CNN or Transformer is learned to model the density of encoded latent variables. These have been proved effective for image generation, but few studies have evaluated the learned representation in discriminative tasks (Ramesh et al., 2021; Esser et al., 2021).We explore an approach we refer to as Vector-quantized Image Modeling (VIM) and apply it to both image generation and image understanding tasks. VIM follows a two-stage approach:",
    "prev": "Quantization and Losses:\n\nTo learn a discrete latent space, we quantize our encoder outputs into the entries of a learned codebook via the vector quantization (VQ) objective in VQVAEs [45],\nL VQ = sg(z) − e 2 2 + β z − sg(e) 2 2 ,(1)\nwhere sg(x) ≡ x, and d dx sg(x) ≡ 0 is the stop-gradient operator, β is the commitment loss weight, and e is a codebook vector from codebook E. The index to the codebook vector closest to z is found by i = argmin j z − E j 2 2 .",
    "curr": "In addition to the VQ objective, we adopt the factorized and 2normalized codes from ViT-VQGAN [58] to improve codebook usage and reconstruction quality.",
    "next": "To train our model, we use a combination of L 2 loss, image perceptual loss L IP [20,61], video perceptual loss L VP by using the I3D network [6] as feature extractor, and adversarial loss L Adv with StyleGAN architecture [21]."
  },
  {
    "index": 8,
    "source_corpus_id": 212996548,
    "ref_id": "b9",
    "citation_corpus_id": 52967399,
    "start": 2418,
    "end": 2439,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "INTRODUCTION\n\nTransformer (Vaswani et al., 2017) is widely used in natural language processing due to its high training efficiency and superior capability in capturing long-distance dependencies.",
    "curr": "Building on top of them, modern state-of-the-art models, such as BERT (Devlin et al., 2019), are able to learn powerful language representations from unlabeled text and even surpass the human performance on the challenging question answering task.",
    "next": "However, the good performance comes at a high computational cost."
  },
  {
    "index": 9,
    "source_corpus_id": 202719276,
    "ref_id": "b8",
    "citation_corpus_id": 6706414,
    "start": 2268,
    "end": 2292,
    "title": "Published as a conference paper at ICLR 2015 EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES",
    "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples-inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
    "prev": "INTRODUCTION\n\nDeep learning has achieved a remarkable performance breakthrough on various challenging benchmarks in machine learning fields, such as image classification (Krizhevsky et al., 2012) and speech recognition .",
    "curr": "However, recent studies (Szegedy et al., 2014;Goodfellow et al., 2015) have revealed that deep neural network models are strikingly susceptible to adversarial examples, in which small perturbations around the input are sufficient to mislead the predictions of the target model.",
    "next": "Moreover, such perturbations are almost imperceptible to humans and often transfer across diverse models to achieve black-box attacks (Papernot et al., 2017;Liu et al., 2017)."
  },
  {
    "index": 10,
    "source_corpus_id": 220665539,
    "ref_id": "b22",
    "citation_corpus_id": 5834589,
    "start": 3440,
    "end": 3473,
    "title": "ON LARGE-BATCH TRAINING FOR DEEP LEARNING: GENERALIZATION GAP AND SHARP MINIMA",
    "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say 32-512 data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions-and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap. Published as a conference paper at ICLR 2017 These methods minimize the objective function f by iteratively taking steps of the form: J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. . Weak sharp minima and penalty functions in mathematical programming. PhD thesis, University of Cambridge, 1988.Michael P Friedlander and Mark Schmidt. Hybrid deterministic-stochastic methods for data fitting.",
    "prev": "This question is motivated by the surprising realization over the past decade that deep neural network training can be performed almost entirely with first-order stochastic optimization.",
    "curr": "In fact, empirical evidence supports the hypothesis that the regularizing effect of gradient noise assists model generalization (Keskar et al., 2017;Smith and Le, 2018;Hochreiter and Schmidhuber, 1997).",
    "next": "Stochastic gradient descent variants such as AdaGrad (Duchi et al., 2011) and Adam (Kingma and Ba, 2015) form the core of almost all successful optimization techniques for these models, using small subsets of the data to form the noisy gradient estimates."
  },
  {
    "index": 12,
    "source_corpus_id": 264802502,
    "ref_id": "b11",
    "citation_corpus_id": 28202810,
    "start": 2358,
    "end": 2380,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": " offline RL algorithms conditioned on observation histories suffer from poor sample complexity, in accordance with the above intuition.We then identify sufficient conditions under which offline RL can still be efficient -intuitively, it needs to learn a compact representation of history comprising only features relevant for action selection.We introduce a bisimulation loss that captures the extent to which this happens, and propose that offline RL can explicitly optimize this loss to aid worst-case sample complexity.Empirically, we show that across a variety of tasks either our proposed loss improves performance, or the value of this loss is already minimized as a consequence of standard offline RL, indicating that it correlates well with good performance.",
    "curr": "INTRODUCTION\n\nDeep reinforcement learning (RL) has achieved impressive performance in games (Mnih et al., 2013;Silver et al., 2017;AlphaStar, 2019), robotic locomotion (Schulman et al., 2015;2017), and control (Todorov et al., 2012;Haarnoja et al., 2018).A key challenge in the widespread adoption of RL algorithms is the need for deploying a suboptimal policy in the environment to collect online interactions, which can be detrimental in many applications such as recommender systems (Afsar et al., 2021), healthcare (Shortreed et al., 2011;Wang et al., 2018), and robotics (Kalashnikov et al., 2018).Offline RL aims to learn effective policies entirely from an offline dataset of previously collected demonstrations (Levine et al., 2020), which makes it a promising approach for applications where exploring online from scratch is unsafe or costly.A major reason for the success of offline RL algorithms is their ability to combine components of suboptimal trajectories in the offline dataset using common states, a phenomenon called \"trajectory stitching\" (Fu et al., 2019a;2020).",
    "next": "Most offline RL methods are formulated in a Markov decision process (MDP) where the state is fully observed (Sutton and Barto, 2018).However, in many real-world tasks,"
  },
  {
    "index": 15,
    "source_corpus_id": 253523474,
    "ref_id": "b10",
    "citation_corpus_id": 245906072,
    "start": 3522,
    "end": 3546,
    "title": "Implicit Bias of MSE Gradient Optimization in Underparameterized Neural Networks",
    "abstract": "We study the dynamics of a neural network in function space when optimizing the mean squared error via gradient flow. We show that in the underparameterized regime the network learns eigenfunctions of an integral operator TK∞ determined by the Neural Tangent Kernel (NTK) at rates corresponding to their eigenvalues. For example, for uniformly distributed data on the sphere S d−1 and rotation invariant weight distributions, the eigenfunctions of TK∞ are the spherical harmonics. Our results can be understood as describing a spectral bias in the underparameterized regime. The proofs use the concept of \"Damped Deviations\", where deviations of the NTK matter less for eigendirections with large eigenvalues due to the occurence of a damping factor. Aside from the underparameterized regime, the damped deviations point-of-view can be used to track the dynamics of the empirical risk in the overparameterized setting, allowing us to extend certain results in the literature. We conclude that damped deviations offers a simple and unifying perspective of the dynamics when optimizing the squared error.",
    "prev": "In particular, bounding the smallest eigenvalue of the NTK Gram matrix is a staple technique for establishing convergence guarantees for the optimization (Du et al., 2019a,b;Oymak & Soltanolkotabi, 2020).",
    "curr": "Furthermore, the full spectrum of the NTK Gram matrix governs the dynamics of the empirical risk (Arora et al., 2019b), and the eigenvalues of the associated integral operator characterize the dynamics of the generalization error outside the training set (Bowman & Montufar, 2022;Bowman & Montúfar, 2022).",
    "next": "Moreover, the decay rate of the generalization error for Gaussian process regression using the NTK can be characterized by the decay rate of the spectrum (Caponnetto & De Vito, 2007;Cui et al., 2021;Jin et al., 2022)."
  },
  {
    "index": 23,
    "source_corpus_id": 231632937,
    "ref_id": "b37",
    "citation_corpus_id": 52911937,
    "start": 2092,
    "end": 2114,
    "title": "DIRECTED-INFO GAIL: LEARNING HIERARCHICAL POLICIES FROM UNSEGMENTED DEMONSTRATIONS USING DIRECTED INFORMATION",
    "abstract": "The use of imitation learning to learn a single policy for a complex task that has multiple modes or hierarchical structure can be challenging. In fact, previous work has shown that when the modes are known, learning separate policies for each mode or sub-task can greatly improve the performance of imitation learning. In this work, we discover the interaction between sub-tasks from their resulting stateaction trajectory sequences using a directed graphical model. We propose a new algorithm based on the generative adversarial imitation learning framework which automatically learns sub-task policies from unsegmented demonstrations. Our approach maximizes the directed information flow in the graphical model between sub-task latent variables and their generated trajectories. We also show how our approach connects with the existing Options framework, which is commonly used to learn hierarchical policies. * Denotes equal contribution",
    "prev": "In this paper we ask, how do we learn to decompose the task automatically and utilize the decomposition to solve sparse reward problems?",
    "curr": "Deep RL has made great strides solving a variety of tasks recently, with hierarchical RL (hRL) demonstrating promise in solving such sparse reward tasks (Sharma et al., 2019b;Le et al., 2018;Merel et al., 2019;Ranchod et al., 2015).",
    "next": "In hRL, the task is decomposed into a hierarchy of subtasks, where policies at the top of the hierarchy call upon policies below to perform actions to solve their respective subtasks."
  },
  {
    "index": 24,
    "source_corpus_id": 246904522,
    "ref_id": "b38",
    "citation_corpus_id": 5034059,
    "start": 1944,
    "end": 1964,
    "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    "abstract": "For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusively tailored to a specific task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating and analyzing the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all our tasks yields better results than training a separate model for each task. However, the low absolute performance of our best model indicates the need for improved general NLU systems. son. 2013. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint 1312.3005.",
    "prev": "* Equal contribution.",
    "curr": "INTRODUCTION\n\nOver the past few years, Transformer (Vaswani et al., 2017) has been widely used in various natural language processing (NLP) tasks, including text classification (Wang et al., 2018a), text translation (Ott et al., 2018), question answering (Rajpurkar et al., 2016; and text generation (Brown et al., 2020).",
    "next": "The recent application of Transformer in computer vision (CV) field also demonstrate the potential capacity of Transformer architecture."
  },
  {
    "index": 25,
    "source_corpus_id": 252846609,
    "ref_id": "b18",
    "citation_corpus_id": 52920808,
    "start": 9411,
    "end": 9427,
    "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks",
    "abstract": "One of the mystery in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an m hidden node shallow neural network with ReLU activation and n training data, we show as long as m is large enough and the data is non-degenerate, randomly initialized gradient descent converges a globally optimal solution with a linear convergence rate for the quadratic loss function.Our analysis is based on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods. * Equal Contribution.",
    "prev": "We call the NTK of a finite network with θ chosen at some point during training the network's empirical NTK.",
    "curr": "Although the empirical NTK cannot exactly model the full training dynamics of finite networks, (Du et al., 2018(Du et al., , 2019a give some non-asymptotic guarantees.",
    "next": "Bi-level optimization with NTK: Let (X d , y d ) and (X p , y p ) denote the clean and poison training examples, respectively, (X t , y t ) denote clean test examples, and (X a , y a ) denote test data with the trigger applied and the target label."
  },
  {
    "index": 26,
    "source_corpus_id": 257834209,
    "ref_id": "b41",
    "citation_corpus_id": 3626819,
    "start": 1592,
    "end": 1612,
    "title": "Deep contextualized word representations",
    "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
    "prev": "In our experiments, SPIN reduces memory requirements, improves accuracy across a range of metalearning tasks, and improves state-of-the-art performance on an important practical problem, genotype imputation.",
    "curr": "INTRODUCTION\n\nRecent advances in deep learning have been driven by large-scale parametric models (Krizhevsky et al., 2012;Peters et al., 2018;Devlin et al., 2019;Ramesh et al., 2022).",
    "next": "Modern parametric models rely on large numbers of weights to capture the signal contained in the training set and to facilitate generalization (Frankle & Carbin, 2018;; as a result, they require non-trivial computational resources (Hoffmann et al., 2022), have limited interpretability (Belinkov, 2022), and impose a significant carbon footprint (Bender et al., 2021)."
  },
  {
    "index": 29,
    "source_corpus_id": 263831863,
    "ref_id": "b21",
    "citation_corpus_id": 49411844,
    "start": 2458,
    "end": 2476,
    "title": "DARTS: Differentiable Architecture Search",
    "abstract": "This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques.",
    "prev": "ation is that the model obtained by the proposed inner optimization can mimic the self-supervised target model.To achieve this, we also introduce the MSE between representations of the inner model and the self-supervised target model on the original full dataset for outer optimization.Lastly, assuming that a feature extractor is fixed, we only optimize a linear head on top of the feature extractor, which allows us to reduce the computational cost and obtain a closedform solution of the head with kernel ridge regression.We empirically validate the effectiveness of our method on various applications involving transfer learning.",
    "curr": "INTRODUCTION\n\nAs a consequence of collecting large-scale datasets and recent advances in parallel data processing, deep models have achieved remarkable success in various machine learning problems.However, some applications such as hyperparameter optimization (Franceschi et al., 2017), continual learning (Lopez-Paz & Ranzato, 2017), or neural architecture search (Liu et al., 2019) require repetitive training processes.In such scenarios, it is prohibitively costly to use all the examples from the huge dataset, which motivates the need to compress the full dataset into a small representative set of examples.Recently, many dataset distillation (or condensation) methods (Wang et al., 2018;Zhao et al., 2021;Zhao & Bilen, 2021;Nguyen et al., 2021a;b;Cazenavette et al., 2022;Zhou et al., 2022;Loo et al., 2022;Zhao & Bilen, 2023) have successfully learned a small number of examples on which we can train a model to achieve performance comparable to the one trained on the full dataset.",
    "next": "Despite the recent success of dataset distillation methods, they are not designed to produce a distilled dataset that can be effectively transferred to downstream tasks (Figure 1-(a)).In other words, we may not achieve meaningful performance improvements when pre-training a model on the distilled dataset and fine-tuning it on the target dataset.However, condensing general-purpose datasets"
  },
  {
    "index": 30,
    "source_corpus_id": 5763832,
    "ref_id": "b28",
    "citation_corpus_id": 6628106,
    "start": 20967,
    "end": 20988,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "We use gradient descent with a batch size of 1 robot for optimization, as the problem is not stochastic in nature.",
    "curr": "The parameters are optimized with Adam's rule (Kingma and Ba, 2014) with a learning rate of 0.001.",
    "next": "Every update step with this method takes about 5 s on CPU."
  },
  {
    "index": 32,
    "source_corpus_id": 21850704,
    "ref_id": "b5",
    "citation_corpus_id": 1729177,
    "start": 1836,
    "end": 1854,
    "title": "Hedge Trimmer: A Parse-and-Trim Approach to Headline Generation",
    "abstract": "This paper presents Hedge Trimmer, a HEaDline GEneration system that creates a headline for a newspaper story using linguistically-motivated heuristics to guide the choice of a potential headline. We present feasibility tests used to establish the validity of an approach that constructs a headline by selecting words in order from a story. In addition, we describe experimental results that demonstrate the effectiveness of our linguistically-motivated approach over a HMM-based model, using both human evaluation and automatic metrics for comparing the two approaches.",
    "prev": "There are two prominent types of summarization algorithms.",
    "curr": "First, extractive summarization systems form summaries by copying parts of the input (Neto et al., 2002;Dorr et al., 2003;Nallapati et al., 2017).",
    "next": "Second, abstractive summarization systems generate new phrases, possibly rephrasing or using words that were not in the original text (Chopra et al., 2016;Zeng et al., 2016)."
  },
  {
    "index": 33,
    "source_corpus_id": 239009555,
    "ref_id": "b26",
    "citation_corpus_id": 3536221,
    "start": 3973,
    "end": 3995,
    "title": "MODEL-ENSEMBLE TRUST-REGION POLICY OPTI- MIZATION",
    "abstract": "Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning. However, they tend to suffer from high sample complexity which hinders their use in real-world domains. Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and, to date, it has succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and we show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning than backpropagation through time. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks 1 2 .Published as a conference paper at ICLR 2018 assumption in this approach, henceforth termed vanilla model-based RL, is that with enough data, the learned model will be accurate enough, such that a policy optimized on it will also perform well in the real environment.Although vanilla model-based RL can work well on low-dimensional tasks with relatively simple dynamics, we find that on more challenging continuous control tasks, performance was highly unstable. The reason is that the policy optimization tends to exploit regions where insufficient data is available to train the model, leading to catastrophic failures. Previous work has pointed out this issue as model biasWhile this issue can be regarded as a form of overfitting, we emphasize that standard countermeasures from the supervised learning literature, such as regularization or cross validation, are not sufficient here -supervised learning can guarantee generalization to states from the same distribution as the data, but the policy optimization stage steers the optimization exactly towards areas where data is scarce and the model is inaccurate. This problem is severely aggravated when expressive models such as deep neural networks are employed.",
    "prev": "Model-based RL (MBRL) reduces the amount of data required for policy optimization by approximating the environment with a learned model, which we can use to generate simulated state transitions (Sutton, 1990;Racanière et al., 2017;Moerland et al., 2020).",
    "curr": "While early approaches on lowdimensional tasks by Schneider (1997); Deisenroth & Rasmussen (2011) used probabilistic models with closed-form posteriors, recent methods rely on neural networks to scale to complex tasks on discrete (Kaiser et al., 2020) and continuous (Chua et al., 2018;Kurutach et al., 2018) action spaces.",
    "next": "However, the learned representation of the true environment always remains imperfect, which introduces approximation errors to the RL problem (Atkeson & Santamaria, 1997;Abbeel et al., 2006)."
  },
  {
    "index": 36,
    "source_corpus_id": 261697392,
    "ref_id": "b16",
    "citation_corpus_id": 227209335,
    "start": 2939,
    "end": 2942,
    "title": "SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS",
    "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reversetime SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in diffusion probabilistic modeling and scorebased generative modeling, and allows for new sampling procedures. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, which enables exact likelihood computation, and improved sampling efficiency. In addition, our framework enables conditional generation with an unconditional model, as we demonstrate with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high fidelity generation of 1024ˆ1024 images for the first time from a score-based generative model.",
    "prev": "However, despite their impressive generation quality, these models often suffer from excessive inference time and computational consumption [5,7,8,9,10].",
    "curr": "This can be attributed to the fact that most of these models are either auto-regressive [13,14,15] or diffusion models [16,17].",
    "next": "For instance, Stable Diffusion, even when using a state-of-the-art sampler [18,19,20], typically requires more than 20 steps to generate acceptable images."
  },
  {
    "index": 38,
    "source_corpus_id": 220302524,
    "ref_id": "b1",
    "citation_corpus_id": 3618568,
    "start": 1983,
    "end": 1986,
    "title": "Reading Wikipedia to Answer Open-Domain Questions",
    "abstract": "This paper proposes to tackle opendomain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.",
    "prev": "Introduction\n\nMany language systems rely on text retrieval as their first step to find relevant information.",
    "curr": "For example, search ranking [1], open domain question answering [2], and fact verification [3,4] all first retrieve relevant documents as the input to their later stage reranking, machine reading, and reasoning models.",
    "next": "All these later-stage models enjoy the advancements of deep learning techniques [5,6], while, in contrast, the first stage retrieval still mainly relies on matching discrete bag-of-words [1,2,3,7]."
  },
  {
    "index": 39,
    "source_corpus_id": 252596001,
    "ref_id": "b20",
    "citation_corpus_id": 209439843,
    "start": 6589,
    "end": 6611,
    "title": "MEASURING COMPOSITIONAL GENERALIZATION: A COMPREHENSIVE METHOD ON REALISTIC DATA",
    "abstract": "State-of-the-art machine learning methods exhibit limited compositional generalization. At the same time, there is a lack of realistic benchmarks that comprehensively measure this ability, which makes it challenging to find and evaluate improvements. We introduce a novel method to systematically construct such benchmarks by maximizing compound divergence while guaranteeing a small atom divergence between train and test sets, and we quantitatively compare this method to other approaches for creating compositional generalization benchmarks. We present a large and realistic natural language question answering dataset that is constructed according to this method, and we use it to analyze the compositional generalization ability of three machine learning architectures. We find that they fail to generalize compositionally and that there is a surprisingly strong negative correlation between compound divergence and accuracy. We also demonstrate how our method can be used to create new compositionality benchmarks on top of the existing SCAN dataset, which confirms these findings. Radev. Improving text-to-SQL evaluation methodology. In ACL, 2018. URL http://aclweb.org/anthology/P18-1033.Jerry A Fodor and Zenon W Pylyshyn. Connectionism and cognitive architecture: A critical analysis. Cognition, 28(1-2):3-71, 1988. URL https://pdfs.semanticscholar.org/d806/ 76034bfabfea59f35698af0f715a555fcf50.pdf.",
    "prev": "d and wrote N1 A: that (edited and wrote) (N1) Q: whose N1 was employed by and founded M1 A: whose (N1) (was employed by and founded) (M1) Q: whose N1 married M2 and married N2 A: whose (N1) ((married M2) and (married N2)) Q: that played M2 , played M3 , and played M4 A: that ((played M2) , (played M3) , and (played M4)) Q: that wrote , edited , executive produced , and directed N1 A: that (wrote , edited , executive produced , and directed) (N1) Q: that N3 were written by and art directed by A: that (N3) (were written by and art directed by) Q: Was N1 N2\n\nINTRODUCTION\n\nCompositionality is a key part of human intelligence as it allows us to understand and produce a potentially infinite number of novel combinations of known components (Chomsky, 1957;Montague, 1970;Lake et al., 2017).",
    "curr": "In contrast, standard neural sequence models, transformers and recurrent neural networks, often fail to capture the compositional structure of the problem domain and thus fail to generalize compositionally (Keysers et al., 2020;.",
    "next": "Prior efforts to improve compositional generalization primarily rely on specialized architectures or training procedures (Lake, 2019;Nye et al., 2020;Andreas, 2020;Conklin et al., 2021;Liu et al., 2021)."
  },
  {
    "index": 40,
    "source_corpus_id": 259095643,
    "ref_id": "b15",
    "citation_corpus_id": 10494183,
    "start": 3887,
    "end": 3906,
    "title": "Generating Steganographic Text with LSTMs",
    "abstract": "Motivated by concerns for user privacy, we design a steganographic system (\"stegosystem\") that enables two users to exchange encrypted messages without an adversary detecting that such an exchange is taking place. We propose a new linguistic stegosystem based on a Long Short-Term Memory (LSTM) neural network. We demonstrate our approach on the Twitter and Enron email datasets and show that it yields high-quality steganographic text while significantly improving capacity (encrypted bits per word) relative to the state-of-the-art.",
    "prev": "This can lead to low accuracy or impractical false positive rates that especially impact vulnerable subgroups, such as non-native speakers [Liang et al., 2023].",
    "curr": "One way to enable accurate detection of machine-generated text is through watermarking, where generated text is marked imperceptibly so that its origin can be determined [Atallah et al., 2001, Fang et al., 2017, Kirchenbauer et al., 2023.",
    "next": "Because watermarks rely on subtle patterns in text that are statistically unlikely to be replicated by a human, watermarking enables detectors that achieve high levels of accuracy on relatively short fragments of text."
  },
  {
    "index": 41,
    "source_corpus_id": 247595088,
    "ref_id": "b23",
    "citation_corpus_id": 6628106,
    "start": 20588,
    "end": 20607,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "The goal of our first experiments is to give a broad comparison of the proposed HIGs with commonly used optimizers.",
    "curr": "This includes stochastic gradient descent (SGD), Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), RMSprop , Adam (Kingma & Ba, 2015), and Gauss-Newton (GN) applied to mini batches.",
    "next": "The results are shown in figure 2b where all curves show the best runs for each optimizer with suitable hyperparameters independently selected, as explained in the appendix."
  },
  {
    "index": 42,
    "source_corpus_id": 259342096,
    "ref_id": "b43",
    "citation_corpus_id": 237416585,
    "start": 7349,
    "end": 7352,
    "title": "Published as a conference paper at ICLR 2022 FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS",
    "abstract": "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning-finetuning language models on a collection of datasets described via instructions-substantially improves zeroshot performance on unseen tasks.",
    "prev": "Among these models, transformer-based language models [49] have emerged as the de facto standard for a wide range of NLP tasks, owing to their unparalleled capabilities in capturing complex linguistic patterns and generalizing across diverse contexts.",
    "curr": "One particularly successful paradigm for training such models is instruction-tuning [44,52,4,28,34,38], which enhances their performance on specific tasks by adapting their pre-trained representations to follow natural language instructions.",
    "next": "While the benefits of Large Language Models (LLMs) are indisputable, their rapidly growing size and computational requirements pose significant challenges in terms of training efficiency, memory footprint, and deployment costs."
  },
  {
    "index": 43,
    "source_corpus_id": 21196492,
    "ref_id": "b20",
    "citation_corpus_id": 11816014,
    "start": 4490,
    "end": 4514,
    "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
    "abstract": "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com.",
    "prev": "(2016) show that skip layer connections facilitate signal propagation and alleviate gradient degradation.",
    "curr": "The combination of the deep residual coattention encoder and the mixed objective leads to higher performance across question types, question lengths, and answer lengths on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) compared to our DCN baseline.",
    "next": "The improvement is especially apparent on long questions, which require the model to capture long-range dependencies between the document and the question."
  },
  {
    "index": 45,
    "source_corpus_id": 254926490,
    "ref_id": "b48",
    "citation_corpus_id": 237491751,
    "start": 5512,
    "end": 5531,
    "title": "SITUATEDQA: Incorporating Extra-Linguistic Contexts into QA",
    "abstract": "Answers to the same question may change depending on the extra-linguistic contexts (when and where the question was asked). To study this challenge, we introduce SITUATEDQA, an open-retrieval QA dataset where systems must produce the correct answer to a question given the temporal or geographical context. To construct SITUATEDQA, we first identify such questions in existing QA datasets. We find that a significant proportion of information seeking questions have context-dependent answers (e.g. roughly 16.5% of NQ-Open). For such context-dependent questions, we then crowdsource alternative contexts and their corresponding answers. Our study shows that existing models struggle with producing answers that are frequently updated or from uncommon locations. We further quantify how existing models, which are trained on data collected in the past, fail to generalize to answering questions asked in the present, even when provided with an updated evidence corpus (a roughly 15 point drop in accuracy). Our analysis suggests that open-retrieval QA benchmarks should incorporate extra-linguistic context to stay relevant globally and in the future. Our data, code, and datasheet are available at https: //situatedqa.github.io/.",
    "prev": "Evaluate humans and models on a new benchmark of ambiguously-specified tasks, demonstrating that while pure language models fail to disambiguate the intended task well, sufficiently-large models trained with human feedback data are able to approach or even exceed the performance of our human participants to resolve the ambiguity between tasks 3.",
    "curr": "Show how finetuning on ambiguous in-context prompts and examples can enable traditional language models to surpass the performance of HFD models when evaluated on unseen tasks, providing a promising route towards models that capably manage task ambiguity 2 RELATED WORK\n\n\nAMBIGUITY IN NATURAL LANGUAGE PROCESSING\n\nAmbiguity is a well-studied topic in NLP, with work spanning topics as diverse as search queries (Cronen-Townsend & Croft, 2002;Wang & Agichtein, 2010), question answering (Min et al., 2020;Zhang & Choi, 2021), named entities (Bunescu & Pasca, 2006;Cucerzan, 2007;Dredze et al., 2010), coreference resolution (Webster et al., 2018), machine translation (Stanovsky et al., 2019), and information-seeking dialogues (Aliannejadi et al., 2019;Guo et al., 2021;Aliannejadi et al., 2021;Sun et al., 2022;.",
    "next": "Our work differs from these prior streams of work by studying task ambiguity (Finn et al., 2018;Tamkin et al., 2022c), where the task the agent is being asked to perform is ambiguous, rather than an ambiguous input for a clear task."
  },
  {
    "index": 46,
    "source_corpus_id": 52912260,
    "ref_id": "b22",
    "citation_corpus_id": 14124313,
    "start": 1763,
    "end": 1792,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "ckle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task.Yet, even with such meta-learning, the low-data problem in the novel classification task still remains.In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data.TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results.",
    "curr": "INTRODUCTION\n\nRecent breakthroughs in deep learning (Krizhevsky et al., 2012;Simonyan and Zisserman, 2015;He et al., 2016) highly rely on the availability of large amounts of labeled data.However, this reliance on large data increases the burden of data collection, which hinders its potential applications to the low-data regime where the labeled data is rare and difficult to gather.On the contrary, humans have the ability to recognize new objects after observing only one or few instances (Lake et al., 2011).For example, children can generalize the concept of \"apple\" after given a single instance of it.This significant gap between human and deep learning has reawakened the research interest on few-shot learning (Vinyals et al., 2016;Snell et al., 2017;Finn et al., 2017;Ravi and Larochelle, 2017;Lee and Choi, 2018;Xu et al., 2017;Wang et al., 2018).",
    "next": "Few-shot learning aims to learn a classifier that generalizes well with a few examples of each of these classes.Traditional techniques such as fine-tuning (Jia et al., 2014) that work well with deep learning models would severely overfit on this"
  },
  {
    "index": 47,
    "source_corpus_id": 3535369,
    "ref_id": "b8",
    "citation_corpus_id": 6628106,
    "start": 34220,
    "end": 34239,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "The number of times we train the value network is fixed to 25, while for the control variate, it was chosen to be a hyperparameter.",
    "curr": "All models were trained using ADAM (Kingma & Ba, 2015), with β 1 = 0.9, β 2 = 0.999, and = 1e − 08.",
    "next": "The baseline A2C case has 2 hyperparameters to tune: the learning rate for the optimizer for the policy and value network."
  },
  {
    "index": 49,
    "source_corpus_id": 247595243,
    "ref_id": "b62",
    "citation_corpus_id": 220363897,
    "start": 9302,
    "end": 9320,
    "title": "Meta-Learning Symmetries by Reparameterization",
    "abstract": "Many successful deep learning architectures are equivariant to certain transformations in order to conserve parameters and improve generalization: most famously, convolution layers are equivariant to shifts of the input. This approach only works when practitioners know a priori symmetries of the task and can manually construct an architecture with the corresponding equivariances. Our goal is a general approach for learning equivariances from data, without needing prior knowledge of a task's symmetries or custom task-specific architectures. We present a method for learning and encoding equivariances into networks by learning corresponding parameter sharing patterns from data. Our method can provably encode equivarianceinducing parameter sharing for any finite group of symmetry transformations, and we find experimentally that it can automatically learn a variety of equivariances from symmetries in data. We provide our experiment code and pre-trained models at https://github.com/AllanYangZhou/metalearning-symmetries.",
    "prev": "In our imbalanced setting, generative invariance transfer aims to learn transformations from the head classes that apply to the tail classes of the same dataset, and does not assume the data is paired or grouped.",
    "curr": "There is a general interest in obtaining invariances for machine learning models (Benton et al., 2020;Zhou et al., 2021).",
    "next": "Data augmentation (Beymer & Poggio, 1995;Niyogi et al., 1998) can be used to train classifiers to be invariant to certain hand-picked transformations, but requires the practitioner to know and implement those transformations in advance."
  },
  {
    "index": 50,
    "source_corpus_id": 252683543,
    "ref_id": "b6",
    "citation_corpus_id": 44134226,
    "start": 4073,
    "end": 4091,
    "title": "Hierarchical Neural Story Generation",
    "abstract": "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.",
    "prev": "To avoid this inconsistency, they proposed a self-terminating (ST) language model that uses new parametrization for its classifier rather than usual softmax parametrization.",
    "curr": "They proved that the ST language model is consistent with respect to greedy search, beam search, top-k sampling (Fan et al., 2018) as well as nucleus sampling (Holtzman et al., 2020).",
    "next": "The ST language model increases the termination probability of each sequence monotonically to 1, but this parametrization is not appropriate for learning our natural language."
  },
  {
    "index": 53,
    "source_corpus_id": 202660778,
    "ref_id": "b10",
    "citation_corpus_id": 52920808,
    "start": 8237,
    "end": 8255,
    "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks",
    "abstract": "One of the mystery in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an m hidden node shallow neural network with ReLU activation and n training data, we show as long as m is large enough and the data is non-degenerate, randomly initialized gradient descent converges a globally optimal solution with a linear convergence rate for the quadratic loss function.Our analysis is based on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods. * Equal Contribution.",
    "prev": "After the ICLR conference submission deadline, Yang & Zhang (2019) revised their paper by adding a new variance reduction algorithm that achieves O(H 2 / 3/2 ) sample complexity, which is also worse than our result by a factor of O(H 2 ).",
    "curr": "Apart from the convergence analysis of the general nonconcave performance functions, there has emerged a line of work Liu et al., 2019; that studies the global convergence of (proximal/trust-region) policy optimization with neural network function approximation, which applies the theory of overparameterized neural networks (Du et al., 2019b;a;Allen-Zhu et al., 2019;Zou et al., 2019;Cao & Gu, 2019) to reinforcement learning.",
    "next": "Notation v 2 denotes the Euclidean norm of a vector v ∈ R d and A 2 denotes the spectral norm of a matrix A ∈ R d×d ."
  },
  {
    "index": 54,
    "source_corpus_id": 235293695,
    "ref_id": "b7",
    "citation_corpus_id": 54443381,
    "start": 2354,
    "end": 2377,
    "title": "EFFICIENT LIFELONG LEARNING WITH A-GEM",
    "abstract": "In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC  and other regularizationbased methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency. 1",
    "prev": "Building a system resembling human learning abilities is a deep-rooted desire since sustainable learning over a long-term period is essential for general artificial intelligence.",
    "curr": "In light of this need, continual learning (CL) (Thrun, 1995), or lifelong learning, tackles a learning scenario where a model continuously learns over a sequence of tasks (Kumar & Daume III, 2012;Li & Hoiem, 2016) within a broad research area, such as classification (Kirkpatrick et al., 2017;Chaudhry et al., 2019a), image generation (Zhai et al., 2019), language learning (Li et al., 2019b;Biesialska et al., 2020), clinical application (Lee & Lee, 2020;Lenga et al., 2020), speech recognition (Sadhu & Hermansky, 2020), and federated learning (Yoon et al., 2021).",
    "next": "A well-known challenge for continual learning is catastrophic forgetting (McCloskey & Cohen, 1989), where the continual learner loses the fidelity for past tasks after adapting the previously learned knowledge to future tasks."
  },
  {
    "index": 56,
    "source_corpus_id": 238582772,
    "ref_id": "b22",
    "citation_corpus_id": 108300573,
    "start": 4309,
    "end": 4331,
    "title": "INTERPOLATION-PREDICTION NETWORKS FOR IRREGULARLY SAMPLED TIME SERIES",
    "abstract": "In this paper, we present a new deep learning architecture for addressing the problem of supervised learning with sparse and irregularly sampled multivariate time series. The architecture is based on the use of a semi-parametric interpolation network followed by the application of a prediction network. The interpolation network allows for information to be shared across multiple dimensions of a multivariate time series during the interpolation stage, while any standard deep learning model can be used for the prediction network. This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate. We investigate the performance of this architecture on both classification and regression tasks, showing that our approach outperforms a range of baseline and recently proposed models. 1 1 Our implementation is available at : https://github.com/mlds-lab/interp-net 1 arXiv:1909.07782v1 [cs.LG] 13 Sep 2019Published as a conference paper at ICLR 2019 trends. Similar to the work ofLipton et al. (2016)andChe et al. (2018a), our architecture also explicitly leverages a separate information channel related to patterns of observation times. However, our representation uses a semi-parametric intensity function representation of this information that is more closely related to the work of Lasko (2014) on modeling medical event point processes.Our architecture thus produces three output time series for each input time series: a smooth interpolation modeling broad trends in the input, a short time-scale interpolation modeling transients, and an intensity function modeling local observation frequencies.This work is motivated by problems in the analysis of electronic health records (EHRs)(Marlin et al., 2012;Lipton et al., 2016;Futoma et al., 2017;Che et al., 2018a). It remains rare for hospital systems to capture dense physiological data streams. Instead, it is common for the physiological time series data in electronic health records to be both sparse and irregularly sampled. The additional issue of the lack of alignment in the observation times across physiological variables is also very common.We evaluate the proposed architecture on two datasets for both classification and regression tasks. Our approach outperforms a variety of simple baseline models as well as the basic and advanced GRU models introduced by Che et al. (2018a) across several metrics. We also compare our model with to the Gaussian process adapter (Li & Marlin, 2016) and multi-task Gaussian process RNN classifier(Futoma et al., 2017). Further, we perform full ablation testing of the information channels our architecture can produce to assess their impact on classification and regression performance.",
    "prev": "This decoupled approach does not fully exploit informative missingness patterns or deal with irregular sampling, thus producing suboptimal Previous studies (Wu et al., 2021;Li et al., 2020a;Zhang et al., 2019) have noted that inter-sensor correlations bring rich information in modeling time series.",
    "curr": "However, only few studies consider relational structure of irregularly sampled time series, and those which do have limited ability in capturing inter-sensor connections (Wu et al., 2021;Shukla & Marlin, 2018).",
    "next": "In contrast, we integrate recent advances in graph neural networks to take advantage of relational structure among sensors."
  },
  {
    "index": 59,
    "source_corpus_id": 220302148,
    "ref_id": "b29",
    "citation_corpus_id": 3300937,
    "start": 2398,
    "end": 2401,
    "title": "Learning From Noisy Singly-labeled Data",
    "abstract": "Supervised learning depends on annotated examples, which are taken to be the ground truth. But these labels often come from noisy crowdsourcing platforms, like Amazon Mechanical Turk. Practitioners typically collect multiple labels per example and aggregate the results to mitigate noise (the classic crowdsourcing problem). Given a fixed annotation budget and unlimited unlabeled data, redundant annotation comes at the expense of fewer labeled examples. This raises two fundamental questions: (1) How can we best learn from noisy workers? (2) How should we allocate our labeling budget to maximize the performance of a classifier? We propose a new algorithm for jointly modeling labels and worker quality from noisy crowd-sourced data. The alternating minimization proceeds in rounds, estimating worker quality from disagreement with the current model and then updating the model by optimizing a loss function that accounts for the current estimate of worker quality. Unlike previous approaches, even with only one annotation per example, our algorithm can estimate worker quality. We establish a generalization error bound for models learned with our algorithm and establish theoretically that it's better to label many examples once (vs less multiply) when worker quality is above a threshold. Experiments conducted on both ImageNet (with simulated noisy workers) and MS-COCO (using the real crowdsourced labels) confirm our algorithm's benefits. 1 1. This work was done when the authors were visiting Amazon",
    "prev": "(1)\nWhile ERM is widely used and offers nice statistical properties, it can also perform poorly in practical situations where average performance is not an appropriate surrogate for the objective of interest.",
    "curr": "Significant research has thus been devoted to developing alternatives to traditional ERM for diverse applications, such as learning in the presence of noisy/corrupted data or outliers [25,30], performing classification with imbalanced data [37,38], ensuring that subgroups within a population are treated fairly [36,42,56], or developing solutions with favorable out-of-sample performance [43].",
    "next": "In this paper, we suggest that deficiencies in ERM can be flexibly addressed via a unified framework, tilted empirical risk minimization (TERM)."
  },
  {
    "index": 61,
    "source_corpus_id": 211132990,
    "ref_id": "b32",
    "citation_corpus_id": 3536221,
    "start": 4217,
    "end": 4240,
    "title": "MODEL-ENSEMBLE TRUST-REGION POLICY OPTI- MIZATION",
    "abstract": "Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning. However, they tend to suffer from high sample complexity which hinders their use in real-world domains. Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and, to date, it has succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and we show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning than backpropagation through time. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks 1 2 .Published as a conference paper at ICLR 2018 assumption in this approach, henceforth termed vanilla model-based RL, is that with enough data, the learned model will be accurate enough, such that a policy optimized on it will also perform well in the real environment.Although vanilla model-based RL can work well on low-dimensional tasks with relatively simple dynamics, we find that on more challenging continuous control tasks, performance was highly unstable. The reason is that the policy optimization tends to exploit regions where insufficient data is available to train the model, leading to catastrophic failures. Previous work has pointed out this issue as model biasWhile this issue can be regarded as a form of overfitting, we emphasize that standard countermeasures from the supervised learning literature, such as regularization or cross validation, are not sufficient here -supervised learning can guarantee generalization to states from the same distribution as the data, but the policy optimization stage steers the optimization exactly towards areas where data is scarce and the model is inaccurate. This problem is severely aggravated when expressive models such as deep neural networks are employed.",
    "prev": "A further study confirms that deep ensembles generally achieves the best performance on out-of-distribution uncertainty benchmarks (Ovadia et al., 2019;Gustafsson et al., 2019), compared to other methods such as MC-dropout (Gal & Ghahramani, 2015).",
    "curr": "In other applications such as model-based reinforcement learning (Deisenroth & Rasmussen, 2011;Wang et al., 2019), ensembles of neural networks can be used to estimate model uncertainty, leading to better overall performance (Kurutach et al., 2018).",
    "next": "Despite their success on benchmarks, ensembles are limited in practice due to their expensive computational and memory costs, which increase linearly with the ensemble size in both training and testing."
  },
  {
    "index": 64,
    "source_corpus_id": 259375870,
    "ref_id": "b23",
    "citation_corpus_id": 243865663,
    "start": 12470,
    "end": 12488,
    "title": "Have You Seen That Number? Investigating Extrapolation in Question Answering Models",
    "abstract": "Numerical reasoning in machine reading comprehension (MRC) has shown drastic improvements over the past few years. While the previous models for numerical MRC are able to interpolate the learned numerical reasoning capabilities, it is not clear whether they can perform just as well on numbers unseen in the training dataset. Our work rigorously tests state-of-the-art models on DROP, a numerical MRC dataset, to see if they can handle passages that contain out-of-range numbers. One of the key findings is that the models fail to extrapolate to unseen numbers. Presenting numbers as digit-by-digit input to the model, we also propose the E-digit number form that alleviates the lack of extrapolation in models and reveals the need to treat numbers differently from regular words in the text. Our work provides a valuable insight into the numerical MRC models and the way to represent number forms in MRC. * Equal contribution.",
    "prev": "Our work focuses on decoder-only models since they are well-suited for text generation and are widely used in LLMs (Brown et al., 2020;Touvron et al., 2023;MosaicML, 2023).",
    "curr": "However, encoder-decoder models have also been extensively studied in the literature in the context of learning arithmetic (Kim et al., 2021;Wang et al., 2021).",
    "next": "Qian et al."
  },
  {
    "index": 65,
    "source_corpus_id": 222141728,
    "ref_id": "b2",
    "citation_corpus_id": 52922363,
    "start": 4476,
    "end": 4496,
    "title": "A CONVERGENCE ANALYSIS OF GRADIENT DESCENT FOR DEEP LINEAR NEURAL NETWORKS",
    "abstract": "We analyze speed of convergence to global optimum for gradient descent training a deep linear neural network (parameterized as x → W N W N −1 · · · W 1 x) by minimizing the 2 loss over whitened data. Convergence at a linear rate is guaranteed when the following hold: (i) dimensions of hidden layers are at least the minimum of the input and output dimensions; (ii) weight matrices at initialization are approximately balanced; and (iii) the initial loss is smaller than the loss of any rank-deficient solution. The assumptions on initialization (conditions (ii) and (iii)) are necessary, in the sense that violating any one of them may lead to convergence failure. Moreover, in the important case of output dimension 1, i.e. scalar regression, they are met, and thus convergence to global optimum holds, with constant probability under a random initialization scheme. Our results significantly extend previous analyses, e.g., of deep linear residual networks(Bartlett et al., 2018).",
    "prev": "(2018); Oymak & Soltanolkotabi (2020), and many more).",
    "curr": "There are also convergence results that focus on linear networks, without nonlinear activations (Bartlett et al., 2018;Arora et al., 2019a;Wu et al., 2019;Du & Hu, 2019;Hu et al., 2020).",
    "next": "These results typically focus on the convergence of loss, hence do not address which of the many global minima is reached."
  },
  {
    "index": 68,
    "source_corpus_id": 253080406,
    "ref_id": "b24",
    "citation_corpus_id": 3626819,
    "start": 7137,
    "end": 7158,
    "title": "Deep contextualized word representations",
    "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
    "prev": "Language models.",
    "curr": "Large language models, such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2018), and GPT-2 (Radford et al., 2019), are able to achieve state-of-the-art performance on many standard NLP benchmarks.",
    "next": "More recent works, such as GPT-3 (Brown et al., 2020), PALM (Chowdhery et al., 2022), and Chinchilla (Hoffmann et al., 2022) further enable few-shot learning from textual prompts."
  },
  {
    "index": 69,
    "source_corpus_id": 259833441,
    "ref_id": "b16",
    "citation_corpus_id": 13046179,
    "start": 5144,
    "end": 5148,
    "title": "A BASELINE FOR DETECTING MISCLASSIFIED AND OUT-OF-DISTRIBUTION EXAMPLES IN NEURAL NETWORKS",
    "abstract": "We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.Published as a conference paper at ICLR 2017 one method which outperforms the baseline on some (but not all) tasks. This new method evaluates the quality of a neural network's input reconstruction to determine if an example is abnormal.",
    "prev": "However, in real-world classifier deployment, one may encounter out-of-distribution (OOD) test samples, i.e., samples drawn from some distinct distribution P out = P in (e.g., images of aeroplanes).",
    "curr": "Out-ofdistribution detection is the problem of accurately identifying such OOD samples, and has received considerable study of late [18,30,20,43,23,22,48,51,3,26,52,46,21].",
    "next": "An accurate OOD detector allows one to abstain from making a prediction on OOD samples, rather than making an egregiously incorrect prediction; this yields more reliable and trust-worthy classifiers."
  },
  {
    "index": 75,
    "source_corpus_id": 231918471,
    "ref_id": "b25",
    "citation_corpus_id": 108304275,
    "start": 8791,
    "end": 8815,
    "title": "DISCRIMINATOR-ACTOR-CRITIC: ADDRESSING SAMPLE INEFFICIENCY AND REWARD BIAS IN ADVERSARIAL IMITATION LEARNING",
    "abstract": "We identify two issues with the family of algorithms based on the Adversarial Imitation Learning framework. The first problem is implicit bias present in the reward functions used in these algorithms. While these biases might work well for some environments, they can also lead to sub-optimal behavior in others. Secondly, even though these algorithms can learn from few expert demonstrations, they require a prohibitively large number of interactions with the environment in order to imitate the expert for many real-world applications. In order to address these issues, we propose a new algorithm called Discriminator-Actor-Critic that uses off-policy Reinforcement Learning to reduce policy-environment interaction sample complexity by an average factor of 10. Furthermore, since our reward function is designed to be unbiased, we can apply our algorithm to many problems without making any task-specific adjustments.",
    "prev": "Moving within the maximum-entropy RL framework (Ziebart, 2010;Levine, 2018), Ho & Ermon (2016) noted that the full procedure (RL • IRL) can be interpreted equivalently as the minimisation of some divergence between occupancy measures of the imitator and demonstrator:\n\narg min\nπ {ψ * (ρ π − ρ π D ) − H(π)},(1)\nwith H(π) being the discounted causal entropy (Bloem & Bambos, 2014) of the policy and ψ * the Fenchel conjugate of a chosen regulariser on the form of the reward.",
    "curr": "These are typically optimised in an adversarial fashion (Goodfellow et al., 2014) and given the focus on evaluating ρ π this often requires extensive interaction with the environment, otherwise banking on approximations over a replay buffer (Kostrikov et al., 2018) or a reformulation of the divergence to allow for off-policy evaluation (Kostrikov et al., 2019).",
    "next": "Bear in mind that optimal policies within the maximum-entropy framework are parameterised by a Boltzmann distribution:\nπ(a|s) = exp(Q(s, a)) b∈A exp(Q(s, b)) ,(2)\nwith Q(s, a) the soft Q-function, defined recursively via the soft Bellman-equation:\nQ(s, a) R(s, a) + γE s ∼ρπ soft max a Q(s , a )) ."
  },
  {
    "index": 76,
    "source_corpus_id": 7942973,
    "ref_id": "b1",
    "citation_corpus_id": 6015236,
    "start": 1542,
    "end": 1562,
    "title": "Leveraging Linguistic Structure For Open Domain Information Extraction",
    "abstract": "Relation triples produced by open domaininformation extraction (open IE) systems are useful for question answering, inference, and other IE tasks. Traditionally these are extracted using a large set of patterns; however, this approach is brittle on out-of-domain text and long-range dependencies, and gives no insight into the substructure of the arguments. We replace this large pattern set with a few patterns for canonically structured sentences, and shift the focus to a classifier which learns to extract self-contained clauses from longer sentences. We then run natural logic inference over these short clauses to determine the maximally specific arguments for each candidate triple. We show that our approach outperforms a state-of-the-art open IE system on the end-to-end TAC-KBP 2013 Slot Filling task.",
    "prev": "We also show which hyperparameter choices had a significant effect on parsing accuracy, allowing us to achieve large gains over other graph-based approaches.",
    "curr": "INTRODUCTION\n\nDependency parsers-which annotate sentences in a way designed to be easy for humans and computers alike to understand-have been found to be extremely useful for a sizable number of NLP tasks, especially those involving natural language understanding in some way (Bowman et al., 2016;Angeli et al., 2015;Levy & Goldberg, 2014;Toutanova et al., 2016;Parikh et al., 2015).",
    "next": "However, frequent incorrect parses can severely inhibit final performance, so improving the quality of dependency parsers is needed for the improvement and success of these downstream tasks."
  },
  {
    "index": 80,
    "source_corpus_id": 246996534,
    "ref_id": "b1",
    "citation_corpus_id": 52889459,
    "start": 2032,
    "end": 2051,
    "title": "LARGE SCALE GAN TRAINING FOR HIGH FIDELITY NATURAL IMAGE SYNTHESIS",
    "abstract": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by truncating the latent space. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128×128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.3 and Fréchet Inception Distance (FID) of 9.6, improving over the previous best IS of 52.52 and FID of 18.65.",
    "prev": "* Indicates equal contribution arXiv:2202.08937v2 [cs.LG]\n\nINTRODUCTION\n\nThese days, generative adversarial networks (GANs) (Goodfellow et al., 2014) can successfully approximate the high-dimensional distributions of real images.",
    "curr": "The exceptional quality of the state-ofthe-art GANs (Karras et al., 2020b;Brock et al., 2019) makes them a key ingredient in applications, including semantic editing (Isola et al., 2017;Zhu et al., 2018;Shen et al., 2020;Voynov & Babenko, 2020), image processing (Pan et al., 2020;Ledig et al., 2017;Menon et al., 2020), video generation (Wang et al., 2018a), producing high-quality synthetics (Zhang et al., 2021;Voynov et al., 2020).",
    "next": "To extend the success of GANs to the limited-data regime, it is common to use pretraining, i.e., to initialize the optimization process by the GAN checkpoint pretrained on some large dataset."
  },
  {
    "index": 82,
    "source_corpus_id": 247411320,
    "ref_id": "b29",
    "citation_corpus_id": 6212000,
    "start": 1595,
    "end": 1614,
    "title": "UNDERSTANDING DEEP LEARNING REQUIRES RE- THINKING GENERALIZATION",
    "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.",
    "prev": "Our code is available at: https://github.com/MSU-MLSys-Lab/DeepAA.",
    "curr": "INTRODUCTION\n\n... Data augmentation (DA) is a powerful technique for machine learning since it effectively regularizes the model by increasing the number and the diversity of data points (Goodfellow et al., 2016;Zhang et al., 2017).",
    "next": "A large body of data augmentation transformations has been proposed (Inoue, 2018;Zhang et al., 2018;DeVries & Taylor, 2017;Yun et al., 2019;Hendrycks et al., 2020;Yan et al., 2020) to improve model performance."
  },
  {
    "index": 83,
    "source_corpus_id": 252682980,
    "ref_id": "b13",
    "citation_corpus_id": 28202810,
    "start": 17633,
    "end": 17656,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "Furthermore, plenty of practical algorithms leverage fitted-Q subroutines for updating the critic step (e.g.",
    "curr": "[Schulman et al., 2017, Haarnoja et al., 2018) with different differentiable function choices.",
    "next": "In addition, we also incorporate pessimism for the design."
  },
  {
    "index": 85,
    "source_corpus_id": 249210151,
    "ref_id": "b71",
    "citation_corpus_id": 238419650,
    "start": 2237,
    "end": 2256,
    "title": "GENERATIVE MODELING WITH OPTIMAL TRANSPORT MAPS",
    "abstract": "With the discovery of Wasserstein GANs, Optimal Transport (OT) has become a powerful tool for large-scale generative modeling tasks. In these tasks, OT cost is typically used as the loss for training GANs. In contrast to this approach, we show that the OT map itself can be used as a generative model, providing comparable performance. Previous analogous approaches consider OT maps as generative models only in the latent spaces due to their poor performance in the original high-dimensional ambient space. In contrast, we apply OT maps directly in the ambient space, e.g., a space of high-dimensional images. First, we derive a minmax optimization algorithm to efficiently compute OT maps for the quadratic cost (Wasserstein-2 distance). Next, we extend the approach to the case when the input and output distributions are located in the spaces of different dimensions and derive error bounds for the computed OT map. We evaluate the algorithm on image generation and unpaired image restoration tasks. In particular, we consider denoising, colorization, and inpainting, where the optimality of the restoration map is a desired attribute, since the output (restored) image is expected to be close to the input (degraded) one.Published as a conference paper at ICLR 2022 (a) OT cost as the loss for the generative model. (b) OT map as the generative model. DC Dowson and BV Landau. The fréchet distance between multivariate normal distributions. Journal of multivariate analysis, 12(3):450-455, 1982.Yilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models. arXiv preprint arXiv:1903.08689, 2019.",
    "prev": "Optimal transport (OT) is a powerful framework to solve mass-moving problems for data distributions which finds many applications in machine learning and computer vision (Bonneel & Digne, 2023).Most existing methods to compute OT plans are designed for discrete distributions (Flamary et al., 2021;Peyré et al., 2019;Cuturi, 2013).These methods have good flexibility: they allow to control the properties of the plan via choosing the cost function.However, discrete methods find an optimal matching between two given (train) sets which does not generalize to new (test) data points.This limits the applications of discrete OT plan methods to scenarios when one needs to generate new data, e.g., image-to-image transfer (Zhu et al., 2017).",
    "curr": "Recent works (Rout et al., 2022;Korotin et al., 2023b;2021b;Fan et al., 2021a;Daniels et al., 2021) propose continuous methods to compute OT plans.Thanks to employing neural networks to parameterize OT solutions, the learned transport plan can be used directly as the generative model in data synthesis (Rout et al., 2022) and unpaired learning (Korotin et al., 2023b;Rout et al., 2022;Daniels et al., 2021;Gazdieva et al., 2022).",
    "next": "Existing continuous OT methods mostly focus on classic cost functions such as ℓ 2 (Korotin et al., 2021b;2023b;Fan et al., 2021a;Gazdieva et al., 2022) which estimate the closeness of input and 1 BACKGROUND AND NOTATIONS\n\nIn this section, we provide key concepts of the optimal transport theory.Throughout the paper, we consider compact X = Y ⊂ R D and P, Q ∈ P(X ), P(Y)."
  },
  {
    "index": 86,
    "source_corpus_id": 252668614,
    "ref_id": "b4",
    "citation_corpus_id": 10550488,
    "start": 7484,
    "end": 7504,
    "title": "Findings of the 2015 Workshop on Statistical Machine Translation",
    "abstract": "This paper presents the results of the WMT15 shared tasks, which included a standard news translation task, a metrics task, a tuning task, a task for run-time estimation of machine translation quality, and an automatic post-editing task. This year, 68 machine translation systems from 24 institutions were submitted to the ten translation directions in the standard translation task. An additional 7 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had three subtasks, with a total of 10 teams, submitting 34 entries. The pilot automatic postediting task had a total of 4 teams, submitting 7 entries. ID Institution AALTO Aalto University (Grönroos et al., 2015) ABUMATRAN Abu-MaTran (Rubino et al., 2015) AFRL-MIT-* Air Force Research Laboratory / MIT Lincoln Lab (Gwinnup et al., 2015) CHALMERS Chalmers University of Technology (Kolachina and Ranta, 2015) CIMS University of Stuttgart and Munich (Cap et al., 2015) CMU Carnegie Mellon University CU-CHIMERA Charles University (Bojar and Tamchyna, 2015) CU-TECTO Charles University (Dušek et al., 2015) DFKI Deutsches Forschungszentrum für Künstliche Intelligenz (Avramidis et al., 2015) ILLINOIS University of Illinois (Schwartz et al., 2015) IMS University of Stuttgart (Quernheim, 2015) KIT Karsruhe Institut of Technology (Cho et al., 2015) KIT-LIMSI Karsruhe Institut of Technology / LIMSI (Ha et al., 2015) LIMSI LIMSI (Marie et al., 2015) MACAU University of Macau MONTREAL University of Montreal (Jean et al., 2015) PROMT ProMT RWTH RWTH Aachen (Peter et al., 2015)",
    "prev": "Furthermore, perplexity assigns cnn dailymail even lower scores than the in-domain xsum.",
    "curr": "For translation, the model is trained on WMT15 dataset and evaluated on other WMT test splits (Bojar et al., 2015), OPUS100 (Aulamo & Tiedemann, 2019), and MTNT (Michel & Neubig, 2018).",
    "next": "The in-domain and OOD datasets perplexity densities overlap even more."
  },
  {
    "index": 87,
    "source_corpus_id": 21946795,
    "ref_id": "b23",
    "citation_corpus_id": 9059612,
    "start": 3552,
    "end": 3575,
    "title": "ADVERSARIAL MACHINE LEARNING AT SCALE",
    "abstract": "Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet (Russakovsky  et al., 2014). Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than singlestep attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a \"label leaking\" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.",
    "prev": "showed that adversarially trained models can be made robust to white-box attacks (i.e., with knowledge of the model parameters) if the perturbations computed during training closely maximize the model's loss.",
    "curr": "However, prior attempts at scaling this approach to ImageNet-scale tasks (Deng et al., 2009) have proven unsuccessful (Kurakin et al., 2017b).",
    "next": "It is thus natural to ask whether it is possible, at scale, to achieve robustness against the class of black-box adversaries Towards this goal, Kurakin et al."
  },
  {
    "index": 88,
    "source_corpus_id": 219636462,
    "ref_id": "b14",
    "citation_corpus_id": 3693512,
    "start": 2190,
    "end": 2194,
    "title": "LIFELONG LEARNING WITH DYNAMICALLY EXPAND- ABLE NETWORKS",
    "abstract": "We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets in lifelong learning scenarios on multiple public datasets, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch model with substantially fewer number of parameters.",
    "prev": "At the heart of catastrophic forgetting is the stability-plasticity dilemma [2,3], where a model exhibits high stability on previously trained tasks, but suffers from low plasticity for the integration of new knowledge (and vice-versa).",
    "curr": "Attempts to overcome this challenge in neural network-based CL can be grouped into three main strategies: regularization methods [4][5][6][7][8][9], memory replay [10][11][12][13], and dynamic network architecture [14][15][16].",
    "next": "In particular, regularization methods that control model weights bear the longest history due to its simplicity and efficiency to control the trade-off for a fixed model capacity."
  },
  {
    "index": 90,
    "source_corpus_id": 252907593,
    "ref_id": "b14",
    "citation_corpus_id": 52967399,
    "start": 2581,
    "end": 2602,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Such coordinated representations are particularly useful for tasks such as cross-modal retrieval.",
    "curr": "Another vetted commonly used self-supervised learning framework is Masked Data Modeling (MDM), which learns a meaningful representation with the pretext task of recovering the original inputs or features from the corrupted ones (Devlin et al., 2019).",
    "next": "Particularly, based on the Audio Spectrogram Transformer (Gong et al., 2021a) and Vision Transformer (Dosovitskiy et al., 2020) backbones, the single-modal Masked Auto-Encoder (MAE)  achieved state-of-the-art (SOTA) performance on images and audio tasks (Huang et al., 2022a) individually."
  },
  {
    "index": 91,
    "source_corpus_id": 203593909,
    "ref_id": "b17",
    "citation_corpus_id": 10480989,
    "start": 2616,
    "end": 2638,
    "title": "Language as a Latent Variable: Discrete Generative Models for Sentence Compression",
    "abstract": "In this work we explore deep generative models of text in which the latent representation of a document is itself drawn from a discrete language model distribution. We formulate a variational auto-encoder for inference in this model and apply it to the task of compressing sentences. In this application the generative model first draws a latent summary sentence from a background language model, and then subsequently draws the observed sentence conditioned on this latent summary. In our empirical evaluation we show that generative formulations of both abstractive and extractive compression yield state-of-the-art results when trained on a large amount of supervised data. Further, we explore semi-supervised compression scenarios where we show that it is possible to achieve performance competitive with previously proposed supervised models while training on a fraction of the supervised data.",
    "prev": "In the context of sequence generation problems, semi-supervised approaches have been shown to work well in some cases.",
    "curr": "For example, back-translation (Sennrich et al., 2015) makes use of the monolingual data on the target side to improve machine translation systems, latent variable models  are employed to incorporate unlabeled source data to facilitate sentence compression (Miao & Blunsom, 2016) or code generation (Yin et al., 2018).",
    "next": "In this work, we revisit a much older and simpler semi-supervised method, self-training (ST, Scudder (1965)), where a base model trained with labeled data acts as a \"teacher\" to label the unannotated data, which is then used to augment the original small training set."
  },
  {
    "index": 92,
    "source_corpus_id": 219708742,
    "ref_id": "b2",
    "citation_corpus_id": 52967399,
    "start": 6440,
    "end": 6461,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": " activation functions other than ReLU have appeared in the literature.Johnson (2019) shows that if the activation function ρ is uniformly continuous and can be uniformly approximated by a sequence of one-to-one functions, a width-d x network cannot universally approximate C(K, R).Kidger and Lyons (2020) show that if ρ is continuous, nonpolynomial, and continuously differentiable at at least one point (say z) with ρ (z) = 0, then networks of width d x + d y + 1 with activation ρ are dense in C(K, R dy ).Furthermore, Kidger and Lyons (2020) prove that ReLU networks of width d x + d y + 1 are dense in L p (R dx , R dy ).",
    "curr": "Limitations of prior arts.Note that none of the existing works succeeds in closing the gap between the upper bound (at least d x + d y ) and the lower bound (at most d x + 1).This gap is significant especially for applications with high-dimensional codomains (i.e., large d y ) such as image generation (Kingma and Welling, 2013;Goodfellow et al., 2014), language modeling (Devlin et al., 2019;Liu et al., 2019), and molecule generation (Gómez-Bombarelli et al., 2018;Jin et al., 2018).In the prior arts, the main bottleneck for proving an upper bound below d x + d y is that they maintain all d x neurons to store the input and all d y neurons to construct the function output; this means every layer already requires at least d x + d y neurons.In addition, the proof techniques for the lower bounds only consider the input dimension d x regardless of the output dimension d y .",
    "next": "Summary of results\n\nWe mainly focus on characterizing the minimum width of ReLU networks for universal approximation.Nevertheless, our results are not restricted to ReLU networks; they can be generalized to networks with general activation functions.Our contributions can be summarized as follows."
  },
  {
    "index": 93,
    "source_corpus_id": 220768638,
    "ref_id": "b8",
    "citation_corpus_id": 57189211,
    "start": 5159,
    "end": 5162,
    "title": "REASONING ABOUT PHYSICAL INTERACTIONS WITH OBJECT-ORIENTED PREDICTION AND PLANNING",
    "abstract": "Object-based factorizations provide a useful level of abstraction for interacting with the world. Building explicit object representations, however, often requires supervisory signals that are difficult to obtain in practice. We present a paradigm for learning object-centric representations for physical scene understanding without direct supervision of object properties. Our model, Object-Oriented Prediction and Planning (O2P2), jointly learns a perception function to map from image observations to object representations, a pairwise physics interaction function to predict the time evolution of a collection of objects, and a rendering function to map objects back to pixels. For evaluation, we consider not only the accuracy of the physical predictions of the model, but also its utility for downstream tasks that require an actionable representation of intuitive physics. After training our model on an image prediction task, we can use its learned representations to build block towers more complicated than those observed during training.",
    "prev": "Developing a factorized scene representation has been a core research topic in computer vision for decades.",
    "curr": "Most learning-based prior works are supervised, requiring annotated specification such as segmentations [9], patches [5], or simulation engines [27,10].",
    "next": "These supervised approaches face two challenges."
  },
  {
    "index": 94,
    "source_corpus_id": 84186721,
    "ref_id": "b1",
    "citation_corpus_id": 5590763,
    "start": 33895,
    "end": 33913,
    "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
    "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
    "prev": "At the first time step these messages, m\n\nv , are initialized with the respective atom features shown in Table 4.",
    "curr": "GGNNs then update these messages in a recursive nature:\nm (s) v = GRU   m (s−1) v , i∈Ne1(v) f single m (s−1) i + j∈Ne2(v) f double m (s−1) j + k∈Ne3(v) f triple m (s−1) k  (7)\nWhere GRU is a Gated Recurrent Unit (Cho et al., 2014), the functions N e1 (v), N e2 (v), N e3 (v) index the nodes connected by single, double and triple bonds to node v respectively and f single (·), f double (·) and f triple (·) are linear transformations with learnable parameters.",
    "next": "This process continues for S steps (where we choose S = 4)."
  },
  {
    "index": 95,
    "source_corpus_id": 254535921,
    "ref_id": "b7",
    "citation_corpus_id": 231627730,
    "start": 8406,
    "end": 8424,
    "title": "Preprint, under review. RANDOMIZED ENSEMBLED DOUBLE Q-LEARNING: LEARNING FAST WITHOUT A MODEL",
    "abstract": "Using a high Update-To-Data (UTD) ratio, model-based methods have recently achieved much higher sample efficiency than previous model-free methods for continuous-action DRL benchmarks. In this paper, we introduce a simple modelfree algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show that its performance is just as good as, if not better than, a state-of-the-art modelbased algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this performance using fewer parameters than the model-based method, and with less wall-clock run time. REDQ has three carefully integrated ingredients which allow it to achieve its high performance: (i) a UTD ratio 1; (ii) an ensemble of Q functions; (iii) in-target minimization across a random subset of Q functions from the ensemble. Through carefully designed experiments, we provide a detailed analysis of REDQ and related model-free algorithms. To our knowledge, REDQ is the first successful model-free DRL algorithm for continuous-action spaces using a UTD ratio 1. * Equal contribution, in alphabetical order. † Correspondence to: Keith Ross <keithwross@nyu.edu>. 1 arXiv:2101.05982v1 [cs.LG] 15 Jan 2021 Preprint, under review.zero for most of training, even when the UTD is very high. Furthermore, by adjusting the number of randomly selected Q-functions for in-target minimization, REDQ can control the average Q-function bias. In comparison with standard ensemble averaging and with SAC with a higher UTD, REDQ has much lower std of Q-function bias while maintaining an average bias that is negative but close to zero throughout most of training, resulting in significantly better learning performance. We perform an ablation study, and show that REDQ is very robust to choices of hyperparameters, and can work well with a small ensemble and a small number of Q functions in the in-target minimization. We also provide a theoretical analysis, providing additional insights into REDQ. Finally, we consider combining the REDQ algorithm with an online feature extractor network (OFENet) (Ota et al.,  2020)  to further improve performance, particularly for the more challenging environments Ant and Humanoid. We achieve more than 7x the sample efficiency of SAC to reach a score of 5000 for both Ant and Humanoid. In Humanoid, REDQ-OFE also greatly outperforms MBPO, reaching a score of 5000 at 150K interactions, which is 3x MBPO's score at that point.To ensure our comparisons are fair, and to ensure our results are reproducible(Henderson et al., 2018;Islam et al., 2017;Duan et al., 2016), we provide open source code. For all algorithmic comparisons, we use the same codebase (except for MBPO, for which we use the authors' code).RANDOMIZED ENSEMBLED DOUBLE Q-LEARNING (REDQ)Janner et al.(2019)proposed Model-Based Policy Optimization (MBPO), which was shown to be much more sample efficient than popular model-free algorithms such as SAC and PPO for the MuJoCo environments. MBPO learns a model, and generates \"fake data\" from its model as well as \"real data\" through environment interactions. It then performs parameter updates using both the fake and the real data. One of the distinguishing features of MBPO is that it has a UTD ratio 1 for updating its Q functions, enabling MBPO to achieve high sample efficiency.",
    "prev": "icitly using regularization on the learned Q-values (Kumar et al., 2020).The limitation of existing offline RL approaches is that the derived policies can only act under a fixed degree of conservatism, which is determined by an opaque hyperparameter that scales the estimated epistemic uncertainty, and has to be chosen during offline training.This means the policies will be unable to correct their behavior online, even if it becomes evident from online observations that the estimated value function is too pessimistic or optimistic.",
    "curr": "Our algorithm learns confidence-conditioned Q-values that capture all possible degrees of pessimism by conditioning on the confidence level, modeling epistemic uncertainty as a function of confidence.By doing so, instead of committing to one degree of pessimism, we enable policies that adapt how conservative they should behave using the observations they sees during online evaluation.Our approach is related to ensemble (Agarwal et al., 2020;Lee et al., 2021;Chen et al., 2021;An et al., 2021) approaches in that they also predict multiple Q-values to model epistemic uncertainty.However, existing ensemble methods train individual Q-values on the same objective, and rely on different parameter initializations.In contrast, each of our Q-values captures a different confidence-level.In addition, standard ensemble approaches do not consider adaptive policies.Recently, APE-V proposes using ensembles to learn adaptive policies that condition on belief over which value function is most accurate (Ghosh et al., 2022).Our approach considers a similar strategy for adaptation, but explicitly parameterizes the value function by the confidence level, introducing a novel training objective for this purpose.In our experiments, we compare to a method that adapts APE-V to our discrete-action benchmark tasks.Jiang & Huang (2020); Dai et al.",
    "next": "(2020) propose confidence intervals for policy evaluation at specified confidence-levels.We aim to learn a value function across all confide"
  },
  {
    "index": 97,
    "source_corpus_id": 1880070,
    "ref_id": "b33",
    "citation_corpus_id": 780171,
    "start": 6329,
    "end": 6350,
    "title": "Data-Driven Response Generation in Social Media",
    "abstract": "We present a data-driven approach to generating responses to Twitter status posts, based on phrase-based Statistical Machine Translation. We find that mapping conversational stimuli onto responses is more difficult than translating between languages, due to the wider range of possible responses, the larger fraction of unaligned words/phrases, and the presence of large phrase pairs whose alignment cannot be further decomposed. After addressing these challenges, we compare approaches based on SMT and Information Retrieval in a human evaluation. We show that SMT outperforms IR on this task, and its output is preferred over actual human responses in 15% of cases. As far as we are aware, this is the first work to investigate the use of phrase-based SMT to directly translate a linguistic stimulus into an appropriate response.",
    "prev": "There are many obvious cases where these metrics fail, as they are often incapable of considering the semantic similarity between responses (see Figure 1).",
    "curr": "Despite this, many researchers still use BLEU to evaluate their dialogue models (Ritter et al., 2011;Sordoni et al., 2015b;Li et al., 2015;Galley et al., 2015;Li et al., 2016a), as there are few alternatives available that correlate with human judgements.",
    "next": "While human evaluation should always be used to evaluate dialogue models, it is often too expensive and time-consuming to do this for every model specification (for example, for every combination of model hyperparameters)."
  },
  {
    "index": 98,
    "source_corpus_id": 263829563,
    "ref_id": "b2",
    "citation_corpus_id": 237561567,
    "start": 7856,
    "end": 7873,
    "title": "Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction",
    "abstract": "An essential operation in web corpus construction consists in retaining the desired content while discarding the rest. Another challenge finding one's way through websites. This article introduces a text discovery and extraction tool published under open-source license. Its installation and use is straightforward, notably from Python and on the command-line. The software allows for main text, comments and metadata extraction, while also providing building blocks for web crawling tasks. A comparative evaluation on real-world data also shows its interest as well as the performance of other available solutions.The contributions of this paper are threefold: it references the software, features a benchmark, and provides a meaningful baseline for similar tasks. The tool performs significantly better than other open-source solutions in this evaluation and in external benchmarks.",
    "prev": "he Pile (Gao et al., 2020), and GPT-3 (Brown et al., 2020) introduce various pipelines for extracting quality data from Common Crawl for the purposes of language model training.These pipelines typically consist of three primary steps: text extraction, filtering, and deduplication.",
    "curr": "Text extraction\n\nExtracting plain text from HTML files is a critical step in the creation of Common Crawl-based datasets.The easiest way to extract text from Common Crawl documents is to use the WET corresponding to each webpage, which contains pre-extracted plain text of the webpage.CCNet and C4 both use Common Crawl's WET files.However, the text extracted in WET files may contain too much boilerplate or miss out on important content such as L A T E X equations.It is also possible to extract text directly from the raw HTML found in Common Crawl WARC files.The Pile uses an open source library called jusText (Endrédy & Novák, 2013) to extract text from HTML while RefinedWeb uses a library called Trafilatura (Barbaresi, 2021).These text extraction approaches differ in terms of extraction speed, customization, and their precision and recall for removing boilerplate content.",
    "next": "Filtering The first layer of filtering often involves language identification (Wenzek et al., 2019).Language filtering is used because certain other parts of the pipeline only work for specific languages, and is often done with simple linear classifiers such as from fastText (Joulin et al., 2016).Quality filtering can be done with a combination of perplexity, classifier, and rule-based methods.CCNet uses a 5-gram Kneser-Ney language model implemented in the KenLM library (Heafield, 2011) trained on the target domain.The documents in the dataset are then sorted and filtered by their perplexity under this model.Other datasets such as the one used to train GPT-3 (Brown et al., 2020) use a classifier-based approach.This involves training a classifier on known-high-quality documents, such as those from Wikipedia, as positive examples and unf"
  },
  {
    "index": 100,
    "source_corpus_id": 261696510,
    "ref_id": "b9",
    "citation_corpus_id": 215745286,
    "start": 3723,
    "end": 3744,
    "title": "Unsupervised Commonsense Question Answering with Self-Talk",
    "abstract": "Natural language understanding involves reading between the lines with implicit background knowledge. Current systems either rely on pretrained language models as the sole implicit source of world knowledge, or resort to external knowledge bases (KBs) to incorporate additional relevant knowledge. We propose an unsupervised framework based on self-talk as a novel alternative to multiple-choice commonsense tasks. Inspired by inquiry-based discovery learning (Bruner, 1961), our approach inquires language models with a number of information seeking questions such as \"what is the definition of ...\" to discover additional background knowledge. Empirical results demonstrate that the self-talk procedure substantially improves the performance of zero-shot language model baselines on four out of six commonsense benchmarks, and competes with models that obtain knowledge from external KBs. While our approach improves performance on several benchmarks, the self-talk induced knowledge even when leading to correct answers is not always seen as useful by human judges, raising interesting questions about the inner-workings of pre-trained language models for commonsense reasoning.",
    "prev": "Select Implement def transform_grid(grid): out_grid = np.zeros_like(grid) for col in range(grid.shape[1]): non_zeros = \\ grid[:, col][grid[:, col] != 0] if len(non_zeros) > 0: out_grid[-len(non_zeros):,col]= \\ on_zeros return out_grid def transform_grid(grid): return ...\n\nINTRODUCTION\n\nInductive reasoning -the ability to infer general principles from specific examples and apply them to novel situations -is a core aspect of human intelligence (Peirce, 1868).",
    "curr": "Recently, large-scale pre-trained language models have received significant interest for their performance across a diverse range of reasoning tasks such as commonsense, arithmetic and symbolic reasoning (Rajani et al., 2019;Shwartz et al., 2020;Nye et al., 2021;Wei et al., 2022;Marasović et al., 2021;Lampinen et al., 2022;Zelikman et al., 2022;Zhou et al., 2022).",
    "next": "There has been extensive discussion of language models' impressive \"in-context learning\" capabilities, a form of inductive reasoning."
  },
  {
    "index": 101,
    "source_corpus_id": 252367996,
    "ref_id": "b21",
    "citation_corpus_id": 3488815,
    "start": 2196,
    "end": 2216,
    "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
    "abstract": "Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. * Authors ordered alphabetically.",
    "prev": "In\n\nINTRODUCTION\n\nAs machine learning models are increasingly deployed in security or safety-critical settings, robustness becomes an essential property.",
    "curr": "Adversarial training (Madry et al., 2018) is the state-of-the-art method for improving the adversarial robustness of deep neural networks.",
    "next": "Recent work has made substantial progress in robustness by scaling adversarial training to very large datasets."
  },
  {
    "index": 103,
    "source_corpus_id": 222133031,
    "ref_id": "b9",
    "citation_corpus_id": 6628106,
    "start": 5267,
    "end": 5288,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "This class encompasses many known schemes such as gradient descent with momentum, or full-matrix AdaGrad.",
    "curr": "However, methods such as Adam [Kingma and Ba, 2015] or diagonal AdaGrad [Duchi et al., 2011] are not in this class, as the diagonal re-scaling creates iterates x t outside the span of previous gradients.",
    "next": "Although we will focus on the distance to the solution, the results can be extended to other convergence criteria such as F (x t ) 2 ."
  },
  {
    "index": 104,
    "source_corpus_id": 247451000,
    "ref_id": "b6",
    "citation_corpus_id": 231662264,
    "start": 27174,
    "end": 27194,
    "title": "Published as a conference paper at ICLR 2021 CHARACTERIZING SIGNAL PROPAGATION TO CLOSE THE PERFORMANCE GAP IN UNNORMALIZED RESNETS",
    "abstract": "Batch Normalization is a key component in almost all state-of-the-art image classifiers, but it also introduces practical challenges: it breaks the independence between training examples within a batch, can incur compute and memory overhead, and often results in unexpected bugs. Building on recent theoretical analyses of deep ResNets at initialization, we propose a simple set of analysis tools to characterize signal propagation on the forward pass, and leverage these tools to design highly performant ResNets without activation normalization layers. Crucial to our success is an adapted version of the recently proposed Weight Standardization. Our analysis tools show how this technique preserves the signal in networks with ReLU or Swish activation functions by ensuring that the per-channel activation means do not grow with depth. Across a range of FLOP budgets, our networks attain performance competitive with the state-of-the-art EfficientNets on ImageNet.Published as a conference paper at ICLR 2021 works. Leveraging these SPPs, we show how to design unnormalized ResNets which are constrained to have signal propagation properties similar to batch-normalized ResNets.• We identify a key failure mode in unnormalized ResNets with ReLU or Swish activations and Gaussian weights. Because the mean output of these non-linearities is positive, the squared mean of the hidden activations on each channel grows rapidly as the network depth increases. To resolve this, we propose Scaled Weight Standardization, a minor modification of the recently proposed Weight Standardization (Qiao et al., 2019; Huang et al., 2017b), which prevents the growth in the mean signal, leading to a substantial boost in performance.• We apply our normalization-free network structure in conjunction with Scaled Weight Standardization to ResNets on ImageNet, where we for the first time attain performance which is comparable or better than batch-normalized ResNets on networks as deep as 288 layers.• Finally, we apply our normalization-free approach to the RegNet architecture (Radosavovic  et al., 2020). By combining this architecture with the compound scaling strategy proposed byTan& Le (2019), we develop a class of models without normalization layers which are competitive with the current ImageNet state of the art across a range of FLOP budgets. . Shufflenet v2: Practical guidelines for efficient cnn architecture design. In . Spectral normalization for generative adversarial networks. In ICLR, 2018. Y. Nesterov. A method for unconstrained convex minimization problem with the rate of convergence O(1/k 2 ). Doklady AN USSR, pp. (269), 543-547, 1983. Art B Owen. A robust hybrid of lasso and ridge regression. 2007. . ImageNet large scale visual recognition challenge. IJCV, 115:211-252, 2015. Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in neural information processing systems, pp. 901-909, 2016.",
    "prev": "This suggests that one can compensate for the removal of BN layers, at least in terms of their effect on the behaviour of the network at initialization time, by down-scaling the residual branch of each residual block.",
    "curr": "Arguably, almost all recent work on training deep networks without normalization layers (Zhang et al., 2018;Shao et al., 2020;Bachlechner et al., 2020;Brock et al., 2021a;b) has adopted this idea by introducing multipliers on the residual branches (which may or may not be optimized during training).",
    "next": "In Table 2, we show that one can close most of the gap with standard ResNets by simply adopting the modification in equation 6 without using BN layers."
  },
  {
    "index": 107,
    "source_corpus_id": 264555202,
    "ref_id": "b28",
    "citation_corpus_id": 52115700,
    "start": 30655,
    "end": 30680,
    "title": "Evaluating Theory of Mind in Question Answering",
    "abstract": "We propose a new dataset for evaluating question answering models with respect to their capacity to reason about beliefs. Our tasks are inspired by theory-of-mind experiments that examine whether children are able to reason about the beliefs of others, in particular when those beliefs differ from reality. We evaluate a number of recent neural models with memory augmentation. We find that all fail on our tasks, which require keeping track of inconsistent states of the world; moreover, the models' accuracy decreases notably when random sentences are introduced to the tasks at test. 1",
    "prev": "All these works, however, focus on protecting training data, without considering context, and rely heavily on having a well-defined notion of a single record.While this is ideal for tabular data, it is extremely Preprint   hard to define for language, as drawing borders around a unit of language that needs protection is not always feasible (Brown et al., 2022) and different units might need different levels of protection, based on information type and context.Our work, however, differs from existing literature in two main aspects: (1) we focus on the impact that context has on privacy, and how reasoning about this context is crucial in making judgments when it comes to language, and (2) we shift attention away from training data and towards interactions with the model, as providing lengthy history for the model is becoming more and more relevant.",
    "curr": "Theory of Mind (ToM) and LLMs\n\nThe development of ToM abilities has been a long-standing goal in AI research (Nematzadeh et al., 2018;Le et al., 2019;Sap et al., 2019;Shapira et al., 2023b;Kim et al., 2023).Although qualitative assessments might imply a degree of ToM in LLMs (Whang, 2023), more comprehensive quantitative investigations reveal that LLMs still struggle to reason ToM robustly (Sap et al., 2022;Shapira et al., 2023a;Ullman, 2023;Kim et al., 2023).This might account for the poor performance of LLMs on our benchmark.",
    "next": "Ethics and morality for LLMs Revealing secrets often involves making moral decisions in the real world.Many previous works focus on inferring the morality of the behavior based on textual descriptions Preprint of scenarios (Jiang et al., 2021;Zhou et al., 2023a;Forbes et al., 2020), while more works start to integrate social contexts into the machine morality discourse (Kim et al., 2022b;Pyatkin et al., 2023;Jin et al., 2022)."
  },
  {
    "index": 108,
    "source_corpus_id": 245906072,
    "ref_id": "b17",
    "citation_corpus_id": 6212000,
    "start": 2118,
    "end": 2138,
    "title": "UNDERSTANDING DEEP LEARNING REQUIRES RE- THINKING GENERALIZATION",
    "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.",
    "prev": "This is surprising from an optimization point-of-view because the ERM problem induced by neural networks is nonconvex (Sontag & Sussmann, 1989, 1991 and can even be NP-Complete in certain cases (Blum & Rivest, 1993).",
    "curr": "Perhaps even more surprising is that the discovered solution can generalize even when the network is able to fit arbitrary labels (Zhang et al., 2017), rendering traditional complexity measures such as Rademacher complexity inadequate.",
    "next": "How does deep learning succeed in the face of pathological behavior by the standards of classical optimization and statistical learning theory?"
  },
  {
    "index": 110,
    "source_corpus_id": 263835059,
    "ref_id": "b22",
    "citation_corpus_id": 231934149,
    "start": 5608,
    "end": 5612,
    "title": "TOPOLOGICAL GRAPH NEURAL NETWORKS",
    "abstract": "Graph neural networks (GNNs) are a powerful architecture for tackling graph learning tasks, yet have been shown to be oblivious to eminent substructures such as cycles. We present TOGL, a novel layer that incorporates global topological information of a graph using persistent homology. TOGL can be easily integrated into any type of GNN and is strictly more expressive (in terms the Weisfeiler-Lehman graph isomorphism test) than message-passing GNNs. Augmenting GNNs with TOGL leads to improved predictive performance for graph and node classification tasks, both on synthetic data sets, which can be classified by humans using their topology but not by ordinary GNNs, and on real-world data.",
    "prev": "ts variants [24,26] still being used as static feature descriptors that require domain-specific hyperparameter choices.By contrast, our approach makes the ECT end-to-end trainable, resulting in an efficient and effective shape descriptor that can be integrated into deep learning models.Subsequently, we demonstrate such integrations both on the level of loss terms as well as on the level of novel computational layers.",
    "curr": "In a machine learning context, the choice of model is typically dictated by the type of data.For point clouds, a recent survey [14] outlines a plethora of models for point cloud analysis tasks like classification, many of them being based on learning equivariant functions [41].When additional structure is being present in the form of graphs or meshes, graph neural networks (GNNs) are typically employed for classification tasks [42], with some methods being capable to either learn explicitly on such higher-order domains [3,4,10,15,16] or harness their topological features [23,32].",
    "next": "Mathematical Background\n\nPrior to discussing our method and its implementation, we provide a self-contained description to the Euler Characteristic Transform (ECT).The ECT is often relying on simplicial complexes, the central building blocks in algebraic topology, which are extensively used for calculating homology groups and proving a variety of properties of topological spaces.While numerous variants of simplicial complexes exist, we will focus on those that are embedded in R n .Generally, simplicial complexes are obtained from on a set of points, to which higher-order elements-simplices-such as Figure 1: We construct a simplicial from an image of the MNIST data set (using a Delaunay complex construction on the non-zero pixels).For each choice of direction on S 1 , we obtain a Euler Characteristic Curve.The collection of all these curves constitutes the Euler Characteristic Transform.Existing work typically concatenates all these curves to obtain a static feature vector, where"
  },
  {
    "index": 111,
    "source_corpus_id": 256808748,
    "ref_id": "b26",
    "citation_corpus_id": 210064473,
    "start": 2313,
    "end": 2330,
    "title": "SPACE: UNSUPERVISED OBJECT-ORIENTED SCENE REPRESENTATION VIA SPATIAL ATTENTION AND DECOMPOSITION",
    "abstract": "The ability to decompose complex multi-object scenes into meaningful abstractions like objects is fundamental to achieve higher-level cognition. Previous approaches for unsupervised object-oriented scene representation learning are either based on spatial-attention or scene-mixture approaches and limited in scalability which is a main obstacle towards modeling real-world scenes. In this paper, we propose a generative latent variable model, called SPACE, that provides a unified probabilistic modeling framework that combines the best of spatial-attention and scene-mixture approaches. SPACE can explicitly provide factorized object representations for foreground objects while also decomposing background segments of complex morphology. Previous models are good at either of these, but not both. SPACE also resolves the scalability problems of previous methods by incorporating parallel spatial-attention and thus is applicable to scenes with a large number of objects without performance degradations. We show through experiments on Atari and 3D-Rooms that SPACE achieves the above properties consistently in comparison to SPAIR, IODINE, and GENESIS. Results of our experiments can be found on our project website: https://sites.google.com/view/space-project-page * Visiting Student at Rutgers University. Authors named inside {} equally contributed. Correspondance to",
    "prev": "INTRODUCTION\n\nObjects, and their interactions, are the foundations of human cognition (Spelke & Kinzler, 2007).",
    "curr": "The endowment on making abstractions from perception and organizing them systematically empowers humans the ability to accomplish and generalize across a broad range of tasks, such as scene modeling (Bear et al., 2020), visual reasoning (Yi et al., 2020), and simulating interactions (Bear et al., 2020).",
    "next": "The key to such success lies in the emergence of symbol-like mental representations of object concepts (Whitehead, 1928)."
  },
  {
    "index": 112,
    "source_corpus_id": 53452703,
    "ref_id": "b5",
    "citation_corpus_id": 28971531,
    "start": 2480,
    "end": 2502,
    "title": "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data",
    "abstract": "Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors (Kiros et al., 2015) on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available 1 .",
    "prev": "Perone et al.",
    "curr": "(2018) have shown that the best encoding architectures are based on recurrent neural networks (RNNs) (Conneau et al., 2017;Peters et al., 2018) or the Transformer architecture (Cer et al., 2018).",
    "next": "These techniques are, however, substantially more expensive to train and apply than word embeddings (Hill et al., 2016;Cer et al., 2018)."
  },
  {
    "index": 114,
    "source_corpus_id": 232320210,
    "ref_id": "b4",
    "citation_corpus_id": 51979536,
    "start": 1995,
    "end": 2015,
    "title": "Large-Scale Study of Curiosity-Driven Learning",
    "abstract": "Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the handdesigned extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://pathak22.github. io/large-scale-curiosity/. * Alphabetical ordering; the first three authors contributed equally.Preprint. Work in progress.",
    "prev": "ut also additionally provides a deterministic compressed representation of the input variable, which is useful for inference tasks that require consistent representation.Moreover, it can jointly learn a feature extractor and select features considering each feature dimension's relevance to the target task, which is unattainable by most neural network-based IB methods.We propose an exploration method based on Drop-Bottleneck for reinforcement learning tasks.In a multitude of noisy and reward sparse maze navigation tasks in VizDoom(Kempka et al., 2016)and DM-Lab (Beattie et al., 2016), our exploration method achieves state-of-the-art performance.As a new IB framework, we demonstrate that Drop-Bottleneck outperforms Variational Information Bottleneck (VIB)(Alemi et al., 2017)in multiple aspects including adversarial robustness and dimensionality reduction.",
    "curr": "INTRODUCTION\n\nData with noise or task-irrelevant information easily harm the training of a model; for instance, the noisy-TV problem (Burda et al., 2019a) is one of well-known such phenomena in reinforcement learning.If observations from the environment are modified to contain a TV screen, which changes its channel randomly based on the agent's actions, the performance of curiosity-based exploration methods dramatically degrades (Burda et al., 2019a;b;Kim et al., 2019;Savinov et al., 2019).",
    "next": "The information bottleneck (IB) theory (Tishby et al., 2000;Tishby & Zaslavsky, 2015) provides a framework for dealing with such task-irrelevant information, and has been actively adopted to exploration in reinforcement learning (Kim et al., 2019;Igl et al., 2019).For an input variable X and a target variable Y , the IB theory introduces another variable Z, which is a compressed representation of X.The IB objective trains Z to contain less information about X but more information about Y as possible, where the two are quantified by mutual information terms of I(Z; X) and I(Z; Y ), respectively.IB methods such as Variational Information Bottleneck ("
  },
  {
    "index": 115,
    "source_corpus_id": 219792087,
    "ref_id": "b24",
    "citation_corpus_id": 49411844,
    "start": 2395,
    "end": 2399,
    "title": "DARTS: Differentiable Architecture Search",
    "abstract": "This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques.",
    "prev": "These works usually incur large computation overheads, which make them impractical to use.",
    "curr": "More recent algorithms significantly reduce the search cost including one-shot methods [2,29], a continuous relaxation of the space [25] and network morphisms [5].",
    "next": "In particular, Liu et al."
  },
  {
    "index": 116,
    "source_corpus_id": 247748808,
    "ref_id": "b7",
    "citation_corpus_id": 208857409,
    "start": 2641,
    "end": 2660,
    "title": "YOUR CLASSIFIER IS SECRETLY AN ENERGY BASED MODEL AND YOU SHOULD TREAT IT LIKE ONE",
    "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model.",
    "prev": "INTRODUCTION\n\nAdversarial Training (AT) is one of the most effective approaches developed so far to improve the robustness of deep neural networks (DNNs) (Madry et al., 2018).",
    "curr": "AT solves a minimax optimization problem, with the inner maximization generating adversarial examples by maximizing the classification loss, and the outer minimization finding model parameters by minimizing the loss on adversarial examples generated from the inner maximization (Wang et al., 2019).",
    "next": "Recently, researchers have noticed that such robust classifiers obtained by AT are able to extract features that are perceptually aligned with humans (Engstrom et al., 2019)."
  },
  {
    "index": 119,
    "source_corpus_id": 256868547,
    "ref_id": "b13",
    "citation_corpus_id": 208248131,
    "start": 4276,
    "end": 4295,
    "title": "Published as a conference paper at ICLR 2020 ECONOMY STATISTICAL RECURRENT UNITS FOR INFERRING NONLINEAR GRANGER CAUSALITY",
    "abstract": "Granger causality is a widely-used criterion for analyzing interactions in largescale networks. As most physical interactions are inherently nonlinear, we consider the problem of inferring the existence of pairwise Granger causality between nonlinearly interacting stochastic processes from their time series measurements. Our proposed approach relies on modeling the embedded nonlinearities in the measurements using a component-wise time series prediction model based on Statistical Recurrent Units (SRUs). We make a case that the network topology of Granger causal relations is directly inferrable from a structured sparse estimate of the internal parameters of the SRU networks trained to predict the processes' time series measurements. We propose a variant of SRU, called economy-SRU, which, by design has considerably fewer trainable parameters, and therefore less prone to overfitting. The economy-SRU computes a low-dimensional sketch of its high-dimensional hidden state in the form of random projections to generate the feedback for its recurrent processing. Additionally, the internal weight parameters of the economy-SRU are strategically regularized in a group-wise manner to facilitate the proposed network in extracting meaningful predictive features that are highly time-localized to mimic real-world causal events. Extensive experiments are carried out to demonstrate that the proposed economy-SRU based time series prediction model outperforms the MLP, LSTM and attention-gated CNN-based time series models considered previously for inferring Granger causality. . Towards a rigorous assessment of systems biology models: The DREAM3 challenges. PLOS ONE, 5(2):1-18, Feb 2010. . Earth system modeling 2.0: A blueprint for models that learn from observations and targeted high-resolution simulations. . Scalable matrix-valued kernel learning for high-dimensional nonlinear multivariate regression and Granger causality. In Pan. Estimating brain connectivity with varying-length time lags using a recurrent neural network.",
    "prev": "INTRODUCTION\n\nCausal interpretation of the observed time-series data can help answer fundamental causal questions and advance scientific discoveries in various disciplines such as medical and financial fields.",
    "curr": "To enable causal reasoning and counterfactual prediction, researchers in the past decades have been dedicated to discovering causal graphs from observed time-series and made large progress (Gerhardus & Runge, 2020;Tank et al., 2022;Khanna & Tan, 2020;Wu et al., 2022;Pamfil et al., 2020;Löwe et al., 2022;Runge, 2021).",
    "next": "This task is called causal discovery or causal structure learning, which usually formulates causal relationships as Directed Acyclic Graphs (DAGs)."
  },
  {
    "index": 121,
    "source_corpus_id": 11243593,
    "ref_id": "b10",
    "citation_corpus_id": 14915449,
    "start": 6466,
    "end": 6485,
    "title": "THE GOLDILOCKS PRINCIPLE: READING CHILDREN'S BOOKS WITH EXPLICIT MEMORY REPRESENTATIONS",
    "abstract": "We introduce a new test of how well language models capture meaning in children's books. Unlike standard language modelling benchmarks, it distinguishes the task of predicting syntactic function words from that of predicting lowerfrequency words, which carry greater semantic content. We compare a range of state-of-the-art models, each with a different way of encoding what has been previously read. We show that models which store explicit representations of long-term contexts outperform state-of-the-art neural language models at predicting semantic content words, although this advantage is not observed for syntactic function words. Interestingly, we find that the amount of text encoded in a single memory representation is highly influential to the performance: there is a sweet-spot, not too big and not too small, between single words and full sentences that allows the most meaningful information in a text to be effectively retained and recalled. Further, the attention over such window-based memories can be trained effectively through self-supervision. We then assess the generality of this principle by applying it to the CNN QA benchmark, which involves identifying named entities in paraphrased summaries of news articles, and achieve state-of-the-art performance.",
    "prev": "It is also able to generalize to sequences longer than those seen during training.",
    "curr": "Finally, our model also obtains competitive results on the Childrens Book Test (Hill et al., 2016), and performs best among models that read the text in a single pass before receiving knowledge of the question.",
    "next": "MODEL\n\nOur model is designed to process data in sequential form, and consists of three main parts: an input encoder, a dynamic memory and an output layer, which we now describe in detail."
  },
  {
    "index": 123,
    "source_corpus_id": 3525802,
    "ref_id": "b19",
    "citation_corpus_id": 9615470,
    "start": 7912,
    "end": 7928,
    "title": "A Model of Coherence Based on Distributed Sentence Representation",
    "abstract": "Coherence is what makes a multi-sentence text meaningful, both logically and syntactically. To solve the challenge of ordering a set of sentences into coherent order, existing approaches focus mostly on defining and using sophisticated features to capture the cross-sentence argumentation logic and syntactic relationships. But both argumentation semantics and crosssentence syntax (such as coreference and tense rules) are very hard to formalize. In this paper, we introduce a neural network model for the coherence task based on distributed sentence representation. The proposed approach learns a syntacticosemantic representation for sentences automatically, using either recurrent or recursive neural networks. The architecture obviated the need for feature engineering, and learns sentence representations, which are to some extent able to capture the 'rules' governing coherent sentence structure. The proposed approach outperforms existing baselines and generates the stateof-art performance in standard coherence evaluation tasks 1 .",
    "prev": "Given the first three sentences of a paragraph, choose the next sentence from five sentences later in the paragraph.",
    "curr": "Related to our objective is the local coherence model of Li & Hovy (2014) where a binary classifier is trained to identify coherent/incoherent sentence windows.",
    "next": "In contrast, we only encourage observed contexts to be more plausible than contrastive ones and formulate it as a multi-class classification problem."
  },
  {
    "index": 125,
    "source_corpus_id": 53081529,
    "ref_id": "b42",
    "citation_corpus_id": 11174813,
    "start": 3303,
    "end": 3322,
    "title": "Learning Distributed Representations for Multilingual Text Sequences",
    "abstract": "We propose a novel approach to learning distributed representations of variable-length text sequences in multiple languages simultaneously. Unlike previous work which often derive representations of multi-word sequences as weighted sums of individual word vectors, our model learns distributed representations for phrases and sentences as a whole. Our work is similar in spirit to the recent paragraph vector approach but extends to the bilingual context so as to efficiently encode meaning-equivalent text sequences of multiple languages in the same semantic space. Our learned embeddings achieve state-of-theart performance in the often used crosslingual document classification task (CLDC) with an accuracy of 92.7 for English to German and 91.5 for German to English. By learning text sequence representations as a whole, our model performs equally well in both classification directions in the CLDC task in which past work did not achieve.",
    "prev": "In our experiments on 9 benchmark text classification datasets and 22 textual similarity tasks, the proposed technique consistently matches or outperforms state-of-the-art techniques, with significantly higher accuracy on problems of short length.",
    "curr": "Introduction\n\nText representation plays an important role in many NLP-based tasks such as document classification and clustering (Zhang et al., 2018;Gui et al., 2016Gui et al., , 2014, sense disambiguation (Gong et al., 2017(Gong et al., , 2018a, machine translation (Mikolov et al., 2013b), document matching (Pham et al., 2015), and sequential alignment (Peng et al., 2016(Peng et al., , 2015.",
    "next": "Since there are no explicit features in text, much work has aimed to develop effective text representations."
  },
  {
    "index": 126,
    "source_corpus_id": 203642015,
    "ref_id": "b22",
    "citation_corpus_id": 53015479,
    "start": 2496,
    "end": 2519,
    "title": "PROMP: PROXIMAL META-POLICY SEARCH",
    "abstract": "Credit assignment in Meta-reinforcement learning (Meta-RL) is still poorly understood. Existing methods either neglect credit assignment to pre-adaptation behavior or implement it naively. This leads to poor sample-efficiency during metatraining as well as ineffective task identification strategies. This paper provides a theoretical analysis of credit assignment in gradient-based Meta-RL. Building on the gained insights we develop a novel meta-learning algorithm that overcomes both the issue of poor credit assignment and previous difficulties in estimating meta-policy gradients. By controlling the statistical distance of both pre-adaptation and adapted policies during meta-policy search, the proposed algorithm endows efficient and stable meta-learning. Our approach leads to superior pre-adaptation policy behavior and consistently outperforms previous Meta-RL algorithms in sample-efficiency, wall-clock time, and asymptotic performance. * authors contributed equally to this work arXiv:1810.06784v4 [cs.LG] 11 Feb 2022Published as a conference paper at ICLR 2019We show that the recent formulation introduced by Al-Shedivat et al. (2018)and Stadie et al. (2018)   leads to poor credit assignment, while the MAML formulation(Finn et al., 2017)potentially yields superior meta-policy updates. Second, based on insights from our formal analysis, we highlight both the importance and difficulty of proper meta-policy gradient estimates. In light of this, we propose the low variance curvature (LVC) surrogate objective which yields gradient estimates with a favorable bias-variance trade-off. Finally, building upon the LVC estimator we develop Proximal Meta-Policy Search (ProMP), an efficient and stable meta-learning algorithm for RL. In our experiments, we show that ProMP consistently outperforms previous Meta-RL algorithms in sample-efficiency, wall-clock time, and asymptotic performance.",
    "prev": "However, implementing and running MAML continues to be challenging.",
    "curr": "One major complication is that the standard version of MAML requires estimating second derivatives of the RL reward function, which is difficult when using backpropagation on stochastic policies; indeed, the original implementation of MAML (Finn et al., 2017) did so incorrectly, which spurred the development of unbiased higher-order estimators (DiCE, (Foerster et al., 2018)) and further analysis of the credit assignment mechanism in MAML (Rothfuss et al., 2019).",
    "next": "Another challenge arises from the high variance inherent in policy gradient methods, which can be ameliorated through control variates such as in T-MAML (Liu et al., 2019), through careful adaptive hyperparameter tuning (Behl et al., 2019;Antoniou et al., 2019) and learning rate annealing (Loshchilov & Hutter, 2017)."
  },
  {
    "index": 127,
    "source_corpus_id": 256627797,
    "ref_id": "b10",
    "citation_corpus_id": 202573030,
    "start": 2913,
    "end": 2916,
    "title": "NEURAL OBLIVIOUS DECISION ENSEMBLES FOR DEEP LEARNING ON TABULAR DATA",
    "abstract": "Nowadays, deep neural networks (DNNs) have become the main instrument for machine learning tasks within a wide range of domains, including vision, NLP, and speech. Meanwhile, in an important case of heterogenous tabular data, the advantage of DNNs over shallow counterparts remains questionable. In particular, there is no sufficient evidence that deep learning machinery allows constructing methods that outperform gradient boosting decision trees (GBDT), which are often the top choice for tabular problems. In this paper, we introduce Neural Oblivious Decision Ensembles (NODE), a new deep learning architecture, designed to work with any tabular data. In a nutshell, the proposed NODE architecture generalizes ensembles of oblivious decision trees, but benefits from both end-to-end gradient-based optimization and the power of multi-layer hierarchical representation learning. With an extensive experimental comparison to the leading GBDT packages on a large number of tabular datasets, we demonstrate the advantage of the proposed NODE architecture, which outperforms the competitors on most of the tasks. We open-source the PyTorch implementation of NODE and believe that it will become a universal framework for machine learning on tabular data.",
    "prev": "Instead of using a greedy method [9,10] to search splitting rules, soft trees make decision rules soft and simultaneously update the entire model parameters using the gradient method.",
    "curr": "Soft trees have been actively studied in recent years in terms of predictive performance [7,11,12], interpretability [8,13], and potential techniques in real world applications like pre-training and fine-tuning [14,15].",
    "next": "In addition, a soft tree can be interpreted as a Mixture-of-Experts [16][17][18], a practical technique for balancing computational cost and prediction performance."
  },
  {
    "index": 130,
    "source_corpus_id": 221139843,
    "ref_id": "b44",
    "citation_corpus_id": 7186165,
    "start": 30522,
    "end": 30526,
    "title": "Model-Portability Experiments for Textual Temporal Analysis",
    "abstract": "We explore a semi-supervised approach for improving the portability of time expression recognition to non-newswire domains: we generate additional training examples by substituting temporal expression words with potential synonyms. We explore using synonyms both from WordNet and from the Latent Words Language Model (LWLM), which predicts synonyms in context using an unsupervised approach. We evaluate a state-of-the-art time expression recognition system trained both with and without the additional training examples using data from TempEval 2010, Reuters and Wikipedia. We find that the LWLM provides substantial improvements on the Reuters corpus, and smaller improvements on the Wikipedia corpus. We find that WordNet alone never improves performance, though intersecting the examples from the LWLM and WordNet provides more stable results for Wikipedia.",
    "prev": "A Glossary of Notation\n\nWe provide a glossary of notation used throughout the paper.Total consistency loss (Eq 3) L : X 2 → R A distance function, used for CycleGAN consistency losses λ\n\nHyperparameter controlling the strength of the consistency loss KL(•)\n\nThe KL divergence JS(•)\n\nThe Jensen-Shannon divergence (Definition 2)\nI(•)\nThe Mutual Information\n\n\nB Extended Related Work\n\nWe provide a comprehensive overview of related work and highlight connections to our work below.",
    "curr": "B.1 Overview of Data Augmentation\n\nData augmentation is widely used for improving the aggregate performance of machine learning models in computer vision [46,79], natural language processing [45,71,95] and audio [18,43].The theoretical motivation for data augmentation is largely based on the tangent propagation formalism [19,73,74,76] which expresses the desired invariances induced by a data augmentation as tangent constraints on the directional derivatives of the learned model.",
    "next": "Early work considered augmentations as image defects [5] or stroke warping [90] for character recognition.Since then, augmentation is considered an essential ingredient in computer vision [47,75], with commonly used augmentations including random flips, rotations and crops [31,46,79].Applications of augmentation in computer vision include object detection [23,98] and scene understanding [22] In natural language processing, common data augmentation techniques include back-translation [71,91], synonym or word substitution [25,44,45,83,95], noising [89], grammar induction [39], text editing [85] and other heuristics [20,72].In speech and audio applications, augmentation is also commonly used, through tec"
  },
  {
    "index": 131,
    "source_corpus_id": 258461359,
    "ref_id": "b0",
    "citation_corpus_id": 67855815,
    "start": 2219,
    "end": 2241,
    "title": "Massively Multilingual Neural Machine Translation",
    "abstract": "Multilingual neural machine translation (NMT) enables training a single model that supports translation from multiple source languages into multiple target languages. In this paper, we push the limits of multilingual NMT in terms of the number of languages being used. We perform extensive experiments in training massively multilingual NMT models, translating up to 102 languages to and from English within a single model. We explore different setups for training such models and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages. Our experiments on a large-scale dataset with 102 languages to and from English and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.",
    "prev": "INTRODUCTION\n\nThe rapid development of neural networks opens the path towards the ambitious goal of universal translation that allows converting information between any languages regardless of data modalities (text, audio or video) (Zhang, 2022).",
    "curr": "While the translation for spoken languages (in text and speech) has gained wide attention (Aharoni et al., 2019;Inaguma et al., 2019;Jia et al., 2019), the study of sign language translation (SLT) -a task translating from sign language videos to spoken language texts -still lags behind despite its significance in facilitating the communication between Deaf communities and spoken language communities (Camgoz et al., 2018;.",
    "next": "SLT represents unique challenges: it demands the capability of video understanding and sequence generation."
  },
  {
    "index": 133,
    "source_corpus_id": 239616399,
    "ref_id": "b13",
    "citation_corpus_id": 231933963,
    "start": 4278,
    "end": 4282,
    "title": "OFFLINE MODEL-BASED OPTIMIZATION VIA NOR- MALIZED MAXIMUM LIKELIHOOD ESTIMATION",
    "abstract": "In this work we consider data-driven optimization problems where one must maximize a function given only queries at a fixed set of points. This problem setting emerges in many domains where function evaluation is a complex and expensive process, such as in the design of materials, vehicles, or neural network architectures. Because the available data typically only covers a small manifold of the possible space of inputs, a principal challenge is to be able to construct algorithms that can reason about uncertainty and out-of-distribution values, since a naive optimizer can easily exploit an estimated model to return adversarial inputs. We propose to tackle this problem by leveraging the normalized maximum-likelihood (NML) estimator, which provides a principled approach to handling uncertainty and out-of-distribution inputs. While in the standard formulation NML is intractable, we propose a tractable approximation that allows us to scale our method to high-capacity neural network models. We demonstrate that our method can effectively optimize high-dimensional design problems in a variety of disciplines such as chemistry, biology, and materials engineering.Published as a conference paper at ICLR 2021 vides a flexible approach to constructing conservative and robust estimators using high-dimensional models such as neural networks.",
    "prev": "Such a data-driven approach provides three key benefits: (1) it significantly shortens the recurring cost of running large-scale simulation sweeps, (2) it alleviates the need to explicitly bake in domain knowledge or search space pruning, and (3) it enables data re-use by empowering the designer to optimize accelerators for new unseen applications, by the virtue of effective generalization.",
    "curr": "While data-driven approaches have shown promising results in biology [14,5,57], using offline optimization methods to design accelerators has been challenging Figure 1: Overview of PRIME.",
    "next": "We use a one-time collected dataset of prior accelerator designs, including TPU-style [65], NVDLA-style [42], and ShiDianNao-style [10] accelerators to train a conservative surrogate model, which is used to design accelerators to meet desired goals and constraints."
  },
  {
    "index": 134,
    "source_corpus_id": 220665925,
    "ref_id": "b28",
    "citation_corpus_id": 68137503,
    "start": 2547,
    "end": 2551,
    "title": "FUNCTIONAL VARIATIONAL BAYESIAN NEURAL NETWORKS",
    "abstract": "Variational Bayesian neural networks (BNNs) perform variational inference over weights, but it is difficult to specify meaningful priors and approximate posteriors in a high-dimensional weight space. We introduce functional variational Bayesian neural networks (fBNNs), which maximize an Evidence Lower BOund (ELBO) defined directly on stochastic processes, i.e. distributions over functions. We prove that the KL divergence between stochastic processes equals the supremum of marginal KL divergences over all finite sets of inputs. Based on this, we introduce a practical training objective which approximates the functional ELBO using finite measurement sets and the spectral Stein gradient estimator. With fBNNs, we can specify priors entailing rich structures, including Gaussian processes and implicit stochastic processes. Empirically, we find fBNNs extrapolate well using various structured priors, provide reliable uncertainty estimates, and scale to large datasets. * Equal contribution.Theorem 7. For two stochastic processes P, M on a cylindrical measurable space (Ω T , F T ), the KL divergence of P with respect to M satisfies,where the supremum is over all finite indices subsets T d ⊆ T , and P T d , M T d represent the canonical projection maps π T →T d of P, M , respectively.Proof. Recall that stochastic processes are defined over a cylindrical σ-algebra F T . By Lemma 6, for every set H ∈ F T , the restricted index set τ (H) is countable. Our proof proceeds in two steps:",
    "prev": "The variational approach is limited in posterior expressiveness while the implicit approach is computationally slow and costly in terms of storage.",
    "curr": "Moreover, specifying meaningful priors in parameter space is known to be difficult due to the complex relationship between weights and functions in deep networks [29].",
    "next": "In this paper, we present a Bayesian approach to FSC based on Gaussian processes (GPs) [36] that enables efficient marginalization over functions rather than model parameters."
  },
  {
    "index": 135,
    "source_corpus_id": 263611938,
    "ref_id": "b21",
    "citation_corpus_id": 252683312,
    "start": 8855,
    "end": 8862,
    "title": "OMNIGROK: GROKKING BEYOND ALGORITHMIC DATA",
    "abstract": "Grokking, the unusual phenomenon for algorithmic datasets where generalization happens long after overfitting the training data, has remained elusive. We aim to understand grokking by analyzing the loss landscapes of neural networks, identifying the mismatch between training and test loss landscapes as the cause for grokking. We refer to this as the \"LU mechanism\" because training and test losses (against model weight norm) typically resemble \"L\" and \"U\", respectively. This simple mechanism can nicely explain many aspects of grokking: data size dependence, weight decay dependence, the emergence of representations, etc. Guided by the intuitive picture, we are able to induce grokking on tasks involving images, language and molecules. In the reverse direction, we are able to eliminate grokking for algorithmic datasets. We attribute the dramatic nature of grokking for algorithmic datasets to representation learning.Partial answers to Q1 are provided in recent studies: Liu et al. (2022) attribute grokking to the slow formation of good representations,Thilak et al. (2022)attempts to link grokking to the slingshot mechanism of adaptive optimizers, andBarak et al. (2022)uses Fourier gap to describe hidden progress. This paper aims to understand grokking through the lens of neural loss landscapes. Our landscape analysis is able to explain many aspects of grokking: data size dependence, weight decay dependence, emergence of representations, etc.The paper is organized as follows: In Section 2, we review background on generalization, and introduce the LU mechanism. In Section 3, we show how the LU mechanism leads to grokking for a toy teacher-student setup. In Section 4, we show that the intuition gained from the toy problem can",
    "prev": "[Thi+22] attributed grokking to the slingshot mechanism, which can be measured by the cyclic phase transitions between stable and unstable training regimes.Žunkovič and Ilievski [ŽI22] showed a time separation between achieving zero training error and zero test error in a binary classification task on a linearly separable distribution.",
    "curr": "Liu, Michaud, and Tegmark [LMT23] identified a large initialization scale together with weight decay as a mechanism for grokking.",
    "next": "Barak et al."
  },
  {
    "index": 136,
    "source_corpus_id": 213969759,
    "ref_id": "b14",
    "citation_corpus_id": 52055130,
    "start": 3776,
    "end": 3795,
    "title": "Learning deep representations by mutual information estimation and maximization",
    "abstract": "In this work, we perform unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality of the input to the objective can greatly influence a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and competes with fully-supervised learning on several classification tasks. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation-learning objectives for specific end-goals.IntroductionOne core objective of deep learning is to discover \"good\" representations, and the simple idea explored here is to train a representation-learning function (i.e., an encoder) to maximize the mutual information (MI) between its inputs and outputs. MI is notoriously difficult to compute, particularly in continuous and high-dimensional settings. Fortunately, recent advances enable effective computation of MI between high dimensional input/output pairs of deep neural networks(Belghazi et al., 2018). We leverage MI estimation for representation learning, but we will also show that, depending on the downstream task, maximizing MI between the complete input and the encoder output (i.e., global MI) is often not sufficient for learning useful representations. Rather, structure matters: maximizing the average MI between the representation and local regions of the input can greatly improve the representation's quality for, e.g., classification tasks, while global MI plays a stronger role in the ability to reconstruct the full input given the representation.",
    "prev": "A seminal work is the InfoMax principle (Linsker, 1988), where given an input instance x, the goal of the InfoMax principle is to learn a representation E ψ (x) by maximizing the MI between the input and its representation.",
    "curr": "A growing set of recent works have demonstrated promising empirical performance in unsupervised representation learning via MI maximization (Krause et al., 2010;Hu et al., 2017;Alemi et al., 2018b;Oord et al., 2018;Hjelm et al., 2019).",
    "next": "Another closely related work is the Information Bottleneck method Alemi et al., 2017), where MI is used to limit the contents of representations."
  },
  {
    "index": 137,
    "source_corpus_id": 34984289,
    "ref_id": "b2",
    "citation_corpus_id": 9716222,
    "start": 1855,
    "end": 1881,
    "title": "Training with Exploration Improves a Greedy Stack LSTM Parser",
    "abstract": "We adapt the greedy stack LSTM dependency parser ofDyer et al. (2015)to support a training-with-exploration procedure using dynamic oracles (Goldberg and Nivre, 2013) instead of assuming an error-free action history. This form of training, which accounts for model predictions at training time, improves parsing accuracies. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural network dependency parser.",
    "prev": "* Equal contribution.",
    "curr": "Introduction\n\nRecurrent neural networks (RNNs) have been recently quite successful in structured prediction applications such as machine translation (Sutskever et al., 2014), parsing (Ballesteros et al., 2016) or caption generation (Vinyals et al., 2015).",
    "next": "These models use the same repeated cell (or unit) to output a sequence of tokens one by one."
  },
  {
    "index": 139,
    "source_corpus_id": 235313504,
    "ref_id": "b39",
    "citation_corpus_id": 3488815,
    "start": 2074,
    "end": 2078,
    "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
    "abstract": "Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. * Authors ordered alphabetically.",
    "prev": "Introduction\n\nThis paper considers minimax optimization min x max y f (x, y) in the context of two-player zero-sum games, where the min-player (controlling x) tries to minimize objective f assuming a worst-case opponent (controlling y) that acts so as to maximize it.",
    "curr": "Minimax optimization naturally arises in a variety of important machine learning paradigms, with the most prominent examples being the training of generative adversarial networks (GANs) [20] and adversarially robust models [40].",
    "next": "These applications commonly engage deep neural networks with various techniques such as convolution, recurrent layers, and batch normalization."
  },
  {
    "index": 140,
    "source_corpus_id": 258714845,
    "ref_id": "b24",
    "citation_corpus_id": 59608630,
    "start": 20594,
    "end": 20620,
    "title": "ADAPTIVE POSTERIOR LEARNING: FEW-SHOT LEARNING WITH A SURPRISE-BASED MEMORY MODULE",
    "abstract": "The ability to generalize quickly from few observations is crucial for intelligent systems. In this paper we introduce APL, an algorithm that approximates probability distributions by remembering the most surprising observations it has encountered. These past observations are recalled from an external memory module and processed by a decoder network that can combine information from different memory slots to generalize beyond direct recall. We show this algorithm can perform as well as state of the art baselines on few-shot classification benchmarks with a smaller memory footprint. In addition, its memory compression allows it to scale to thousands of unknown labels. Finally, we introduce a meta-learning reasoning task which is more challenging than direct classification. In this setting, APL is able to generalize with fewer than one example per class via deductive reasoning.",
    "prev": "We refer to Le (2021) for an extensive overview of memory-based architectures.",
    "curr": "Architectures with external memories have also been used for meta-learning (Vinyals et al., 2016;Santoro et al., 2016) and few-shot learning (Munkhdalai and Yu, 2017;Ramalho and Garnelo, 2018;Ma et al., 2021 (Rogers and Hahn, 2010) and key molecular physical descriptors, which were defined by RDKit (Landrum et al., 2006).",
    "next": "While methods would be allowed to use other representations of the input molecules, such as the molecular graph, we used a concatenation of these ECFPs and RDKit-based descriptors."
  },
  {
    "index": 141,
    "source_corpus_id": 222272443,
    "ref_id": "b28",
    "citation_corpus_id": 166228758,
    "start": 3392,
    "end": 3434,
    "title": "Published as a conference paper at ICLR 2020 N-BEATS: NEURAL BASIS EXPANSION ANALYSIS FOR INTERPRETABLE TIME SERIES FORECASTING",
    "abstract": "We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11% over a statistical benchmark and by 3% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.Published as a conference paper at ICLR 2020 inject a suitable inductive bias in the model to make its internal operations more interpretable, in the sense of extracting some explainable driving factors combining to produce a given forecast?SUMMARY OF CONTRIBUTIONSDeep Neural Architecture: To the best of our knowledge, this is the first work to empirically demonstrate that pure DL using no time-series specific components outperforms well-established statistical approaches on M3, M4 and TOURISM datasets (on M4, by 11% over statistical benchmark, by 7% over the best statistical entry, and by 3% over the M4 competition winner). In our view, this provides a long-missing proof of concept for the use of pure ML in TS forecasting and strengthens motivation to continue advancing the research in this area.Interpretable DL for Time Series: In addition to accuracy benefits, we also show that it is feasible to design an architecture with interpretable outputs that can be used by practitioners in very much the same way as traditional decomposition techniques such as the \"seasonality-trend-level\" approach (Cleveland et al., 1990).V. Assimakopoulos and K. Nikolopoulos. The theta model: a decomposition approach to forecasting.",
    "prev": "Model Based (MB) approaches typically rely on partial or ordinary differential equations (PDE/ODE) and stem from a deep understanding of the underlying physical phenomena.",
    "curr": "Machine learning (ML) and deep learning methods are more prior agnostic yet have become state-of-the-art for several spatio-temporal prediction tasks (Shi, Chen, Wang, Yeung, Wong & Woo 2015, Wang, Gao, Long, Wang & Yu 2018, Oreshkin, Carpov, Chapados & Bengio 2020, Donà, Franceschi, Lamprier & Gallinari 2020, and connections have been drawn between deep architectures and numerical ODE solvers, e.g.",
    "next": "neural ODEs (Chen, Rubanova, Bettencourt & Duvenaud 2018, Ayed, de Bézenac, Pajot, Brajard & Gallinari 2019."
  },
  {
    "index": 146,
    "source_corpus_id": 52986403,
    "ref_id": "b20",
    "citation_corpus_id": 6628106,
    "start": 37015,
    "end": 37034,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "A mean pooling layer is used to summarize the LSTM outputs across time, followed by a linear projection layer to predict the posterior mean and log variance.",
    "curr": "C DETAILED EXPERIMENTAL SETUP\n\nThe network is trained using the Adam optimizer (Kingma & Ba, 2015), configured with an initial learning rate 10 −3 , and an exponential decay that halved the learning rate every 12.5k steps, beginning after 50k steps.",
    "next": "Table 6 details the list of prior hyperparameters used for each of the four datasets described in Section 4: multi-speaker English data (multi-spk), noisified multi-speaker English data (noisy-multispk), single-speaker story-telling data (audiobooks), and crowd-sourced audiobook data (crowdsourced)."
  },
  {
    "index": 147,
    "source_corpus_id": 253237975,
    "ref_id": "b7",
    "citation_corpus_id": 848112,
    "start": 4188,
    "end": 4213,
    "title": "TRAINING GANS WITH OPTIMISM",
    "abstract": "We address the issue of limit cycling behavior in training Generative Adversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for training Wasserstein GANs. Recent theoretical results have shown that optimistic mirror decent (OMD) can enjoy faster regret rates in the context of zero-sum games. WGANs is exactly a context of solving a zero-sum game with simultaneous noregret dynamics. Moreover, we show that optimistic mirror decent addresses the limit cycling problem in training WGANs. We formally show that in the case of bi-linear zero-sum games the last iterate of OMD dynamics converges to an equilibrium, in contrast to GD dynamics which are bound to cycle. We also portray the huge qualitative difference between GD and OMD dynamics with toy examples, even when GD is modified with many adaptations proposed in the recent literature, such as gradient penalty or momentum. We apply OMD WGAN training to a bioinformatics problem of generating DNA sequences. We observe that models trained with OMD achieve consistently smaller KL divergence with respect to the true underlying distribution, than models trained with GD variants. Finally, we introduce a new algorithm, Optimistic Adam, which is an optimistic variant of Adam. We apply it to WGAN training on CIFAR10 and observe improved performance in terms of inception score as compared to Adam.",
    "prev": "networks (Arjovsky et al., 2017;Goodfellow et al., 2014), adversarial learning (Goodfellow et al., 2015;Miller et al., 2020), reinforcement learning (Dai et al., 2017;Modi et al., 2021), sharpness-aware minimization (Foret et al., 2021), domain-adversarial training (Ganin et al., 2016), etc.",
    "curr": "Albeit theoretically underexplored, adaptive methods are widely deployed in these applications in combination with popular minimax optimization algorithms such as (stochastic) gradient descent ascent (GDA), extragradient (EG) (Korpelevich, 1976), and optimistic GDA (Popov, 1980;Rakhlin and Sridharan, 2013); see, e.g., (Daskalakis et al., 2018;Gulrajani et al., 2017;Mishchenko et al., 2020;Reisizadeh et al., 2020), just to list a few.",
    "next": "While it seems natural to directly extend adaptive stepsizes to minimax optimization algorithms, a recent work by Yang et al."
  },
  {
    "index": 150,
    "source_corpus_id": 222209080,
    "ref_id": "b18",
    "citation_corpus_id": 1487550,
    "start": 3081,
    "end": 3097,
    "title": "UNSUPERVISED WORD SENSE DISAMBIGUATION RIVALING SUPERVISED METHODS",
    "abstract": "This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints -that words tend to have one sense per discourse and one sense per collocation -exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%.",
    "prev": "Self-training is a common algorithmic paradigm for leveraging unlabeled data with deep networks.",
    "curr": "Self-training methods train a model to fit pseudolabels, that is, predictions on unlabeled data made by a previously-learned model (Yarowsky, 1995;Grandvalet & Bengio, 2005;Lee, 2013).",
    "next": "Recent work also extends these methods to enforce stability of predictions under input transformations such as adversarial perturbations (Miyato et al., 2018) and data augmentation (Xie et al., 2019)."
  },
  {
    "index": 154,
    "source_corpus_id": 238583191,
    "ref_id": "b5",
    "citation_corpus_id": 220249831,
    "start": 23586,
    "end": 23605,
    "title": "Heteroskedastic and Imbalanced Deep Learning with Adaptive Regularization",
    "abstract": "Real-world large-scale datasets are heteroskedastic and imbalanced -labels have varying levels of uncertainty and label distributions are long-tailed. Heteroskedasticity and imbalance challenge deep learning algorithms due to the difficulty of distinguishing among mislabeled, ambiguous, and rare examples. Addressing heteroskedasticity and imbalance simultaneously is under-explored. We propose a data-dependent regularization technique for heteroskedastic datasets that regularizes different regions of the input space differently. Inspired by the theoretical derivation of the optimal regularization strength in a one-dimensional nonparametric classification setting, our approach adaptively regularizes the data points in higher-uncertainty, lower-density regions more heavily. We test our method on several benchmark tasks, including a real-world heteroskedastic and imbalanced dataset, WebVision. Our experiments corroborate our theory and demonstrate a significant improvement over other methods in noise-robust deep learning.",
    "prev": "In Figure \n\n\nImproving SSL on Imbalanced Datasets with Regularization\n\nIn this section, we aim to further improve the performance of SSL to close the gap between imbalanced and balanced datasets.",
    "curr": "Many prior works on imbalanced supervised learning regularize the rare classes more strongly, motivated by the observation that the rare classes suffer from more overfitting [Cao et al., 2019[Cao et al., , 2021.",
    "next": "Inspired by these works, we compute the generalization gaps (i.e., the differences between empirical and validation pre-training losses) on frequent and rare classes for the step-imbalance CIFAR-10 datasets (where 5 classes are frequent class with 5000 examples per class and the rest are rare with 50 examples per class)."
  },
  {
    "index": 157,
    "source_corpus_id": 222125116,
    "ref_id": "b9",
    "citation_corpus_id": 5590763,
    "start": 2351,
    "end": 2355,
    "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
    "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
    "prev": "A variety of approaches have been suggested to mitigate the exploding and vanishing gradient problem.",
    "curr": "These include adding gating mechanisms to the RNN in order to control the flow of information in the network, leading to architectures such as long short-term memory (LSTM) [21] and gated recurring units (GRU) [10], that can overcome the vanishing gradient problem on account of the underlying additive structure.",
    "next": "However, the gradients might still explode and learning very long term dependencies remains a challenge [30]."
  },
  {
    "index": 158,
    "source_corpus_id": 4679427,
    "ref_id": "b1",
    "citation_corpus_id": 18828233,
    "start": 2297,
    "end": 2322,
    "title": "TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.",
    "prev": "*\n\nIntroduction\n\nIn order to learn Generative Adversarial Networks (Goodfellow et al., 2014), it is now well established that the generator should mimic the distribution of real data, in the sense of a certain discrepancy measure.",
    "curr": "Discrepancies between distributions that measure the goodness of the fit of the neural generator to the real data distribution has been the subject of many recent studies (Arjovsky & Bottou, 2017;Nowozin et al., 2016;Kaae Sønderby et al., 2017;Mao et al., 2017;Gulrajani et al., 2017;, most of which focus on training stability.",
    "next": "In terms of data modalities, most success was booked in plausible natural image generation after the introduction of Deep Convolutional Generative Adversarial Networks (DCGAN) (Radford et al., 2015)."
  },
  {
    "index": 159,
    "source_corpus_id": 219558760,
    "ref_id": "b36",
    "citation_corpus_id": 52895589,
    "start": 1628,
    "end": 1644,
    "title": "HOW POWERFUL ARE GRAPH NEURAL NETWORKS?",
    "abstract": "Graph Neural Networks (GNNs) for representation learning of graphs broadly follow a neighborhood aggregation framework, where the representation vector of a node is computed by recursively aggregating and transforming feature vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs in capturing different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.",
    "prev": "Under review.",
    "curr": "Introduction\n\nGraph neural networks (GNNs) (Scarselli et al., 2008;Micheli, 2009) have seen growing popularity over the last few years (Duvenaud et al., 2015;Hamilton et al., 2017;Xu et al., 2019).",
    "next": "Many domains can be naturally represented as graphs."
  },
  {
    "index": 160,
    "source_corpus_id": 88514953,
    "ref_id": "b17",
    "citation_corpus_id": 16209268,
    "start": 3397,
    "end": 3421,
    "title": "Published as a conference paper at ICLR 2015 QUALITATIVELY CHARACTERIZING NEURAL NETWORK OPTIMIZATION PROBLEMS",
    "abstract": "Training neural networks involves solving large-scale non-convex optimization problems. This task has long been believed to be extremely difficult, with fear of local minima and other obstacles motivating a variety of schemes to improve optimization, such as unsupervised pretraining. However, modern neural networks are able to achieve negligible training error on complex tasks, using only direct training with stochastic gradient descent. We introduce a simple analysis technique to look for evidence that such networks are overcoming local optima. We find that, in fact, on a straight path from initialization to solution, a variety of state of the art neural networks never encounter any significant obstacles.",
    "prev": "Ideally, we would like to quantify the probability to converge to a local minimum as a function of the error at this minimum, where the probability is taken with the respect to the randomness of the initialization of the weights, the data and SGD.",
    "curr": "Specifically, we would like to know, under which conditions this probability is very small if the error is high, as was observed empirically (e.g., (Dauphin et al., 2014;Goodfellow et al., 2015)).",
    "next": "However, this seems to be a daunting task for realistic MNNs, since it requires a characterization of the sizes and distributions of the basins of attraction for all local minima."
  },
  {
    "index": 163,
    "source_corpus_id": 263671510,
    "ref_id": "b31",
    "citation_corpus_id": 247595263,
    "start": 10710,
    "end": 10730,
    "title": "SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS",
    "abstract": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",
    "prev": "We also investigated using only GSM8K to create difficult problems, but we found that the new problems were too similar to the original ones, and the large gap to MATH still exists (more information can be found in Appendix C).",
    "curr": "Self-distillation.Given that we do not have ground truth answers for the new problems, we then generate n different LCE solutions as depicted in (Wang et al., 2023a) for each new problem with our initial MathCoder models, keeping only those solutions for which all n answers match (n is set to 3 in this paper), thus ensuring our dataset's quality.We use MathCoder-Initial here because it demonstrates the potential for effective model distillation using a model much weaker than the powerful closed-source models.As MathCoder-Initial already has an accuracy of 77.3% on GSM8K and 44.0% on MATH, it is plausible that distilling it can produce good results.It also reduces the cost compared to using GPT-4.Some examples can be found in Appendix A.",
    "next": "Combining the new data D 1 with the seed data D 0 yields the MathCodeInstruct dataset D = {D 0 , D 1 }.We fine-tune the base Llama-2 (Touvron et al., 2023) and CodeLlama (Rozière et al., 2023) models using MathCodeInstruct to derive our final MathCoder models.For clarity, we refer to the supervised fine-tuning of base Llama-2 as \"MathCoder-L\" and that of CodeLlama as \"MathCoder-CL\", as shown in Fig."
  },
  {
    "index": 165,
    "source_corpus_id": 52978527,
    "ref_id": "b53",
    "citation_corpus_id": 27494814,
    "start": 10231,
    "end": 10249,
    "title": "To prune, or not to prune: exploring the efficacy of pruning for model compression",
    "abstract": "Model pruning seeks to induce sparsity in a deep neural network's various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports(Han et al., 2015a;Narang et al., 2017)prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model's dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy.",
    "prev": "Frankle & Carbin (2018) hypothesizes that certain connections, together with their randomly initialized weights, is particularly effective for training, and a pruning algorithm can help find such sub-networks.",
    "curr": "Zhu & Gupta (2018) shows that training a small-dense model cannot achieve the same accuracy as a pruned large-sparse model with identical memory footprint.",
    "next": "In this work, we reveal a different and rather surprising characteristic of network pruning methods: fine-tuning the pruned model with inherited weights is no better than training it from scratch."
  },
  {
    "index": 168,
    "source_corpus_id": 250627720,
    "ref_id": "b15",
    "citation_corpus_id": 231847016,
    "start": 6650,
    "end": 6671,
    "title": "DISCOVERING A SET OF POLICIES FOR THE WORST CASE REWARD",
    "abstract": "We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, setmax policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is a policy iteration algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite. . Fast reinforcement learning with generalized policy updates. . Diversity is all you need: Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018. Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval research logistics quarterly, 3(1-2):95-110, 1956. Dan Garber and Elad Hazan. A linearly convergent conditional gradient algorithm with applications to online and stochastic optimization. arXiv preprint arXiv:1301.4666, 2013. Reinforcement learning with soft state aggregation. In Advances in neural information processing systems, pp. 361-368, 1995. Nathan Sprague and Dana Ballard. Multiple-goal reinforcement learning with modular sarsa (0). 2003.",
    "prev": "Multiple tasks In standard multitask RL, there is a (possibly infinite) set of tasks (MDPs) M = {M }, usually presented to the agent by sampling from some task distribution P M ∈ P(M).",
    "curr": "Typical objectives include finding either a single policy or a set of policies which maximize worstor average-case value: max π min M ∈M V π M (Zahavy et al., 2021) or max π E P M V π M (Moskovitz et al., 2022a).",
    "next": "When the emphasis is on decreasing the required sample complexity of learning new tasks, a useful metric is cumulative regret: the agent's total shortfall across training compared to an optimal agent."
  },
  {
    "index": 169,
    "source_corpus_id": 256615813,
    "ref_id": "b27",
    "citation_corpus_id": 3144218,
    "start": 6513,
    "end": 6535,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "Bounding Shape Mask R- CNN (Kang et al., 2020) improves performance on object detection and instance segmentation by its bounding shape mask branch.",
    "curr": "BCNet (Ke et al., 2021) uses two GCN (Welling & Kipf, 2016) layers to detect overlapping instances.",
    "next": "Although these algorithms have yielded promising results, they are still restricted in the low-resolution mask representation and thus do not generate high-quality masks."
  },
  {
    "index": 171,
    "source_corpus_id": 263609164,
    "ref_id": "b22",
    "citation_corpus_id": 219530969,
    "start": 33805,
    "end": 33830,
    "title": "Deployment-Efficient Reinforcement Learning via Model-Based Offline Optimization",
    "abstract": "Most reinforcement learning (RL) algorithms assume online access to the environment, in which one may readily interleave updates to the policy with experience collection using that policy. However, in many real-world applications such as health, education, dialogue agents, and robotics, the cost or potential risk of deploying a new data-collection policy is high, to the point that it can become prohibitive to update the data-collection policy more than a few times during learning. With this view, we propose a novel concept of deployment efficiency, measuring the number of distinct data-collection policies that are used during policy learning. We observe that naïvely applying existing model-free offline RL algorithms recursively does not lead to a practical deployment-efficient and sample-efficient algorithm. We propose a novel model-based algorithm, Behavior-Regularized Model-ENsemble (BREMEN) that can effectively optimize a policy offline using 10-20 times fewer data than prior works. Furthermore, the recursive application of BREMEN is able to achieve impressive deployment efficiency while maintaining the same or better sample efficiency, learning successful policies from scratch on simulated robotic environments with only 5-10 deployments, compared to typical values of hundreds to millions in standard RL baselines. Codes and pre-trained models are available at https://github.com/matsuolab/BREMEN. * Equal contribution.Preprint. Under review.",
    "prev": "minimising the number of times the policy used changes from one episode to the next.These works are not directly comparable because they study regret-minimisation for finite-horizon MDPs and we study BPI and PE in the discounted setting.Nevertheless, there are works on tabular MDPs (Qiao et al., 2022;Bai et al., 2019;Zhang et al., 2020), linear MDPs (Gao et al., 2021;Wang et al., 2021;Qiao & Wang, 2023) and MDPs with a linear representation for the action values (Qiao et al., 2023).",
    "curr": "The multi-batch learning model has been studied extensively for bandit algorithms (Perchet et al., 2015;Jun et al., 2016;Gao et al., 2019;Esfandiari et al., 2021;Duchi et al., 2018;Han et al., 2020;Ruan et al., 2021).In RL, it has been studied in the regret-minimisation setting for finite-horizon tabular (Zihan et al., 2022) and linear MDPs (Wang et al., 2021) and MDPs under general function approximation (Xiong et al., 2023).A closely related notion is deployment efficiency (Matsushima et al., 2021), which constrains batches to be of a fixed size consisting of trajectories from a single policy.In finite-horizon linear MDPs, it has been shown that BPI can be solved to arbitrary accuracy with a number of deployments independent of the dimension d (Huang et al., 2022;Qiao & Wang, 2023) where the deployed policy is a finite mixture of deterministic policies.Our results suggest that infinite-horizon discounted MDPs under more general linear representation of action-values are fundamentally harder since the number of deployments must scale with dimension.",
    "next": "The policy finetuning setting assumes access to an offline dataset that can be complemented with online trajectories (Xie et al., 2021) but is different from our setting since there is no adaptivity constraint in the online algorithm, i.e."
  },
  {
    "index": 172,
    "source_corpus_id": 44096233,
    "ref_id": "b23",
    "citation_corpus_id": 2100831,
    "start": 5974,
    "end": 5998,
    "title": "MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text",
    "abstract": "We present MCTest, a freely available set of stories and associated questions intended for research on the machine comprehension of text. Previous work on machine comprehension (e.g., semantic modeling) has made great strides, but primarily focuses either on limited-domain datasets, or on solving a more restricted goal (e.g., open-domain relation extraction). In contrast, MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension. Reading comprehension can test advanced abilities such as causal reasoning and understanding the world, yet, by being multiple-choice, still provide a clear metric. By being fictional, the answer typically can be found only in the story itself. The stories and questions are also carefully limited to those a young child would understand, reducing the world knowledge that is required for the task. We present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions. By screening workers (with grammar tests) and stories (with grading), we have ensured that the data is the same quality as another set that we manually edited, but at one tenth the editing cost. By being open-domain, yet carefully restricted, we hope MCTest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text.",
    "prev": "Third, we demonstrate that on synthetic data, DSMN achieves superior performance for answering questions that require visual thinking.",
    "curr": "Related Work\n\nNatural language datasets for QA: Several natural language QA datasets have been proposed to test AI systems on various reasoning abilities (Levesque et al., 2011;Richardson et al., 2013).",
    "next": "Our work differs from them in two key aspects: first, we use synthetic data instead of natural data; and second, we specialize in geometrical reasoning instead of general language understanding."
  },
  {
    "index": 174,
    "source_corpus_id": 53729760,
    "ref_id": "b19",
    "citation_corpus_id": 205514,
    "start": 2521,
    "end": 2543,
    "title": "DEEP MULTI-SCALE VIDEO PREDICTION BEYOND MEAN SQUARE ERROR",
    "abstract": "Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics. This is why pixel-space video prediction may be viewed as a promising avenue for unsupervised feature learning. In addition, while optical flow has been a very studied problem in computer vision for a long time, future frame prediction is rarely approached. Still, many vision applications could benefit from the knowledge of the next frames of videos, that does not require the complexity of tracking every pixel trajectory. In this work, we train a convolutional network to generate future frames given an input sequence. To deal with the inherently blurry predictions obtained from the standard Mean Squared Error (MSE) loss function, we propose three different and complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function. We compare our predictions to different published results based on recurrent neural networks on the UCF101 dataset.",
    "prev": "INTRODUCTION\n\nGenerative Adversarial Networks (GANs) (Goodfellow et al., 2014) have been able to produce photorealistic images, often indistinguishable from real images.",
    "curr": "This remarkable ability has powered many real-world applications ranging from visual recognition (Wang et al., 2017), to image manipulation (Isola et al., 2017;Zhu et al., 2017), to video prediction (Mathieu et al., 2016).",
    "next": "Since its invention in 2014, many GAN variants have been proposed (Radford et al., 2016;Zhang et al., 2018), often producing more realistic and diverse samples with better training stability."
  },
  {
    "index": 175,
    "source_corpus_id": 220769181,
    "ref_id": "b10",
    "citation_corpus_id": 204512179,
    "start": 1924,
    "end": 1928,
    "title": "LEARNING NEARLY DECOMPOSABLE VALUE FUNC- TIONS VIA COMMUNICATION MINIMIZATION",
    "abstract": "Reinforcement learning encounters major challenges in multi-agent settings, such as scalability and non-stationarity. Recently, value function factorization learning emerges as a promising way to address these challenges in collaborative multi-agent systems. However, existing methods have been focusing on learning fully decentralized value function, which are not efficient for tasks requiring communication. To address this limitation, this paper presents a novel framework for learning nearly decomposable value functions with communication, with which agents act on their own most of the time but occasionally send messages to other agents in order for effective coordination. This framework hybridizes value function factorization learning and communication learning by introducing two information-theoretic regularizers. These regularizers are maximizing mutual information between decentralized Q functions and communication messages while minimizing the entropy of messages between agents. We show how to optimize these regularizers in a way that is easily integrated with existing value function factorization methods such as QMIX. Finally, we demonstrate that, on the StarCraft unit micromanagement benchmark, our framework significantly outperforms baseline methods and allows to cut off more than 80% communication without sacrificing the performance. The video of our experiments is available at",
    "prev": "Demonstrative videos are available at https\n\nIntroduction\n\nCooperative multi-agent reinforcement learning (MARL) has achieved great progress in recent years [1][2][3][4][5][6][7].",
    "curr": "Advances in valued-based MARL [8][9][10][11] contribute significantly to the progress, achieving state-of-the-art performance on challenging tasks, such as StarCraft II micromanagement [12].",
    "next": "However, these value-based methods present a major challenge for stability and convergence in multi-agent settings [13], which is further exacerbated in continuous action spaces."
  },
  {
    "index": 177,
    "source_corpus_id": 222341655,
    "ref_id": "b32",
    "citation_corpus_id": 199543783,
    "start": 24270,
    "end": 24291,
    "title": "Behaviour Suite for Reinforcement Learning",
    "abstract": "This paper introduces the Behaviour Suite for Reinforcement Learning, or bsuite for short. bsuite is a collection of carefully-designed experiments that investigate core capabilities of reinforcement learning (RL) agents with two objectives. First, to collect clear, informative and scalable problems that capture key issues in the design of general and efficient learning algorithms. Second, to study agent behaviour through their performance on these shared benchmarks. To complement this effort, we open source github.com/deepmind/bsuite, which automates evaluation and analysis of any agent on bsuite. This library facilitates reproducible and accessible research on the core issues in RL, and ultimately the design of superior learning algorithms. Our code is Python, and easy to use within existing projects. We include examples with OpenAI Baselines, Dopamine as well as new reference implementations. Going forward, we hope to incorporate more excellent experiments from the research community, and commit to a periodic review of bsuite from a committee of prominent researchers.",
    "prev": "it requires two independent samples of s starting from the same (s, a) due to the inner conditional expectation (Baird, 1995), whereas IV regression methods do not suffer from this issue.",
    "curr": "We evaluate DFIV on three BSuite (Osband et al., 2019) tasks: catch, mountain car, and cartpole.",
    "next": "See Section E.6.1 for a description of those tasks."
  },
  {
    "index": 178,
    "source_corpus_id": 252110923,
    "ref_id": "b9",
    "citation_corpus_id": 210839350,
    "start": 5670,
    "end": 5673,
    "title": "DD-PPO: LEARNING NEAR-PERFECT POINTGOAL NAVIGATORS FROM 2.5 BILLION FRAMES",
    "abstract": "We present Decentralized Distributed Proximal Policy Optimization (DD-PPO), a method for distributed reinforcement learning in resource-intensive simulated environments.DD-PPO is distributed (uses multiple machines), decentralized (lacks a centralized server), and synchronous (no computation is ever 'stale'), making it conceptually simple and easy to implement.In our experiments on training virtual robots to navigate in Habitat-Sim(Savva et al., 2019), DD-PPO exhibits near-linear scaling -achieving a speedup of 107x on 128 GPUs over a serial implementation.We leverage this scaling to train an agent for 2.5 Billion steps of experience (the equivalent of 80 years of human experience) -over 6 months of GPU-time training in under 3 days of wall-clock time with 64 GPUs.This massive-scale training not only sets the state of art on Habitat Autonomous Navigation Challenge 2019, but essentially 'solves' the task -near-perfect autonomous navigation in an unseen environment without access to a map, directly from an RGB-D camera and a GPS+Compass sensor.Fortuitously, error vs computation exhibits a power-law-like distribution; thus, 90% of peak performance is obtained relatively early (at 100 million steps) and relatively cheaply (under 1 day with 8 GPUs).Finally, we show that the scene understanding and navigation policies learned can be transferred to other navigation tasks -the analog of 'ImageNet pre-training + task-specific fine-tuning' for embodied AI.Our model outperforms ImageNet pre-trained CNNs on these transfer tasks and can serve as a universal resource (all models and code are publicly available).",
    "prev": "Second, we study how to translate the start of manipulation skills to the navigation reward, which is used to train the navigation skill to connect manipulation skills.",
    "curr": "Note that the goal position in mobile manipulation plays a very different role from that in point-goal [9,10] navigation.",
    "next": "On the one hand, the position of a target object (e.g., on the table or in the fridge) is often not directly navigable; on the other hand, a navigable position close to the goal position can be infeasible due to kinematic and collision constraints."
  },
  {
    "index": 179,
    "source_corpus_id": 211069439,
    "ref_id": "b2",
    "citation_corpus_id": 11212020,
    "start": 2487,
    "end": 2510,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "INTRODUCTION\n\nText sequence transduction systems convert a given text sequence from one domain to another.",
    "curr": "These techniques can be applied to a wide range of natural language processing applications such as machine translation (Bahdanau et al., 2015), summarization (Rush et al., 2015), and dialogue response generation (Zhao et al., 2017).",
    "next": "In many cases, however, parallel corpora for the task at hand are scarce."
  },
  {
    "index": 181,
    "source_corpus_id": 232105052,
    "ref_id": "b44",
    "citation_corpus_id": 44131019,
    "start": 3421,
    "end": 3439,
    "title": "Scaling Neural Machine Translation",
    "abstract": "Sequence to sequence learning models still require several days to reach state of the art performance on large benchmark datasets using a single machine. This paper shows that reduced precision and large batch training can speedup training by nearly 5x on a single 8-GPU machine with careful tuning and implementation. 1 On WMT'14 English-German translation, we match the accuracy ofVaswani et al. (2017)in under 5 hours when training on 8 GPUs and we obtain a new state of the art of 29.3 BLEU after training for 85 minutes on 128 GPUs. We further improve these results to 29.8 BLEU by training on the much larger Paracrawl dataset. On the WMT'14 English-French task, we obtain a state-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.",
    "prev": "tion, and explore its application in transformers.RFA can be used as a drop-in replacement for conventional softmax attention and offers a straightforward way of learning with recency bias through an optional gating mechanism.Experiments on language modeling and machine translation demonstrate that RFA achieves similar or better performance compared to strong transformer baselines.In the machine translation experiment, RFA decodes twice as fast as a vanilla transformer.Compared to existing efficient transformer variants, RFA is competitive in terms of both accuracy and efficiency on three long text classification datasets.Our analysis shows that RFA's efficiency gains are especially notable on long sequences, suggesting that RFA will be particularly useful in tasks that require working with large inputs, fast decoding speed, or low memory footprints.",
    "curr": "INTRODUCTION\n\nTransformer architectures (Vaswani et al., 2017) have achieved tremendous success on a variety of sequence modeling tasks (Ott et al., 2018;Radford et al., 2018;Parmar et al., 2018;Devlin et al., 2019;Parisotto et al., 2020, inter alia).Under the hood, the key component is attention (Bahdanau et al., 2015), which models pairwise interactions of the inputs, regardless of their distances from each other.This comes with quadratic time and memory costs, making the transformers computationally expensive, especially for long sequences.A large body of research has been devoted to improving their time and memory efficiency (Tay et al., 2020c).Although better asymptotic complexity and prominent gains for long sequences have been achieved (Lee et al., 2019;Child et al., 2019;Beltagy et al., 2020, inter alia), in practice, many existing approaches are less well-suited for moderatelength ones: the additional computation steps required by some approaches can overshadow the time and memory they save (Kitaev et al., 2020;Wang et al., 2020;Roy et al., 2020, inter alia).",
    "next": "This work proposes random feature attention (RFA), an efficient atte"
  },
  {
    "index": 182,
    "source_corpus_id": 257102434,
    "ref_id": "b15",
    "citation_corpus_id": 13046179,
    "start": 10346,
    "end": 10372,
    "title": "A BASELINE FOR DETECTING MISCLASSIFIED AND OUT-OF-DISTRIBUTION EXAMPLES IN NEURAL NETWORKS",
    "abstract": "We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.Published as a conference paper at ICLR 2017 one method which outperforms the baseline on some (but not all) tasks. This new method evaluates the quality of a neural network's input reconstruction to determine if an example is abnormal.",
    "prev": "This function should induce a partial order over instances in X .",
    "curr": "The most common and well-known κ function for a classification model f (with softmax at its last layer) is its softmax response values -κ(x,ŷ|f ) f (x)ŷ (Cordella et al., 1995;De Stefano et al., 2000) -which is also widely accepted as a baseline in the OOD literature (Hendrycks & Gimpel, 2017;Hendrycks et al., 2021;Berger et al., 2021;Shalev et al., 2018).",
    "next": "While this is the primary κ we evaluate for the sake of simplicity, various other κ functions, which are also utilized for OOD detection, exist."
  },
  {
    "index": 185,
    "source_corpus_id": 225067229,
    "ref_id": "b32",
    "citation_corpus_id": 216553145,
    "start": 2479,
    "end": 2498,
    "title": "Multi-Domain Dialogue Acts and Response Co-Generation",
    "abstract": "Generating fluent and informative responses is of critical importance for task-oriented dialogue systems. Existing pipeline approaches generally predict multiple dialogue acts first and use them to assist response generation. There are at least two shortcomings with such approaches. First, the inherent structures of multi-domain dialogue acts are neglected. Second, the semantic associations between acts and responses are not taken into account for response generation. To address these issues, we propose a neural co-generation model that generates dialogue acts and responses concurrently. Unlike those pipeline approaches, our act generation module preserves the semantic structures of multi-domain dialogue acts and our response generation module dynamically attends to different acts as needed. We train the two modules jointly using an uncertainty loss to adjust their task weights adaptively. Extensive experiments are conducted on the largescale MultiWOZ dataset and the results show that our model achieves very favorable improvement over several state-of-the-art models in both automatic and human evaluations.",
    "prev": "Human evaluations show that COCO-generated conversations perfectly reflect the underlying user goal with more than 95% accuracy and are as human-like as the original conversations, further strengthening its reliability and promise to be adopted as part of the robustness evaluation of DST models.",
    "curr": "INTRODUCTION\n\nTask-oriented dialogue (TOD) systems have recently attracted growing attention and achieved substantial progress (Zhang et al., 2019b;Peng et al., 2020;Wang et al., 2020b;a), partly made possible by the construction of large-scale datasets (Budzianowski et al., 2018;Byrne et al., 2019;Rastogi et al., 2019).",
    "next": "Dialogue state tracking (DST) is a backbone of TOD systems, where it is responsible for extracting the user's goal represented as a set of slot-value pairs (e.g., (area, center), (food, British)), as illustrated in the upper part of Figure 1."
  },
  {
    "index": 186,
    "source_corpus_id": 215814169,
    "ref_id": "b24",
    "citation_corpus_id": 59310641,
    "start": 7411,
    "end": 7414,
    "title": "PAY LESS ATTENTION WITH LIGHTWEIGHT AND DYNAMIC CONVOLUTIONS",
    "abstract": "Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU. 1 * Work done during an internship at Facebook. 1 Code and pre-trained models available at",
    "prev": "[23] for a review of different pruning strategies.",
    "curr": "Others have worked on lightweight architectures, by modifying existing models [24,25,26] or developing new networks, such as MobileNet [16], ShuffleNet [15], and EfficientNet [4] in vision.",
    "next": "Finally, knowledge distillation [6] has been applied to sentence representation [13,27,28,29,30], to reduce the size of a BERT model [31]."
  },
  {
    "index": 187,
    "source_corpus_id": 264426451,
    "ref_id": "b38",
    "citation_corpus_id": 227209335,
    "start": 2332,
    "end": 2350,
    "title": "SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS",
    "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reversetime SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in diffusion probabilistic modeling and scorebased generative modeling, and allows for new sampling procedures. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, which enables exact likelihood computation, and improved sampling efficiency. In addition, our framework enables conditional generation with an unconditional model, as we demonstrate with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high fidelity generation of 1024ˆ1024 images for the first time from a score-based generative model.",
    "prev": "ombined with better hyperparameter tuning, these modifications enable consistency models to achieve FID scores of 2.51 and 3.25 on CIFAR-10 and ImageNet 64 ˆ64 respectively in a single sampling step.These scores mark a 3.5ˆand 4ˆimprovement compared to prior consistency training approaches.Through two-step sampling, we further reduce FID scores to 2.24 and 2.77 on these two datasets, surpassing those obtained via distillation in both one-step and two-step settings, while narrowing the gap between consistency models and other state-of-the-art generative models.",
    "curr": "INTRODUCTION\n\nConsistency models (Song et al., 2023) are an emerging family of generative models that produce high-quality samples using a single network evaluation.Unlike GANs (Goodfellow et al., 2014), consistency models are not trained with adversarial optimization and thus sidestep the associated training difficulty.Compared to score-based diffusion models (Sohl-Dickstein et al., 2015;Song & Ermon, 2019;2020;Ho et al., 2020;Song et al., 2021), consistency models do not require numerous sampling steps to generate high-quality samples.They are trained to generate samples in a single step, but still retain important advantages of diffusion models, such as the flexibility to exchange compute for sample quality through multistep sampling, and the ability to perform zero-shot data editing.",
    "next": "We can train consistency models using either consistency distillation (CD) or consistency training (CT).The former requires pre-training a diffusion model and distilling the knowledge therein into a consistency model.The latter allows us to train consistency models directly from data, establishing them as an independent family of generative models.Previous work (Song et al., 2023) demonstrates that CD significantly outperforms CT.However, CD adds computational overhead to the training process since it requires learning a separate diffusion model.Additionally, distillation limits the sample quality of the consistency model to that of the diffu"
  },
  {
    "index": 191,
    "source_corpus_id": 263829348,
    "ref_id": "b12",
    "citation_corpus_id": 209315300,
    "start": 2766,
    "end": 2786,
    "title": "REFORMER: THE EFFICIENT TRANSFORMER",
    "abstract": "Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(L 2 ) to O(L log L), where L is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of N times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.Published as a conference paper at ICLR 2020We introduce the Reformer model which solves these problems using the following techniques:• Reversible layers, first introduced in Gomez et al.(2017), enable storing only a single copy of activations in the whole model, so the N factor disappears. • Splitting activations inside feed-forward layers and processing them in chunks removes the d f f factor and saves memory inside feed-forward layers.",
    "prev": "INTRODUCTION\n\nTime series forecasting, i.e., predicting future data based on historical observations, has broad realworld applications, such as health, transportation, finance and so on.",
    "curr": "In the past decade, numerous deep neural network architectures have been applied to time series modeling, including convolutional neural networks (CNN) (Bai et al., 2018), recurrent neural networks (RNN) (Siami-Namini et al., 2018), graph neural networks (GNN) (Li et al., 2018;Cao et al., 2020), and Transformers (Liu et al., 2021;Wu et al., 2021;Zhou et al., 2021;Wu et al., 2023;Zhou et al., 2022;Woo et al., 2022;Kitaev et al., 2020;Nie et al., 2023), leading to state-of-the-arts results.",
    "next": "While achieving strong prediction performance, the previous works on time series mostly benefit from the advance in sequence modeling (from RNN and GNN, to transformers) that captures temporal dependencies but overlooks a series of intricate patterns within time series data, such as seasonality, trend, and residual."
  },
  {
    "index": 194,
    "source_corpus_id": 235313882,
    "ref_id": "b19",
    "citation_corpus_id": 153313159,
    "start": 9071,
    "end": 9092,
    "title": "Sparse Sequence-to-Sequence Models",
    "abstract": "Sequence-to-sequence models are a powerful workhorse of NLP. Most variants employ a softmax transformation in both their attention mechanism and output layer, leading to dense alignments and strictly positive output probabilities. This density is wasteful, making models less interpretable and assigning probability mass to many implausible outputs. In this paper, we propose sparse sequence-to-sequence models, rooted in a new family of α-entmax transformations, which includes softmax and sparsemax as particular cases, and is sparse for any α > 1. We provide fast algorithms to evaluate these transformations and their gradients, which scale well for large vocabulary sizes. Our models are able to produce sparse alignments and to assign nonzero probability to a short list of plausible outputs, sometimes rendering beam search exact. Experiments on morphological inflection and machine translation reveal consistent gains over dense models. the encodings [h 1 , . . . , h J ], using s t as a query vector. This is done by computing token-level scores z j := s t W (z) h j , then taking a weighted averageπ j h j , where π := softmax(z).(1) the anonymous reviewers, for helpful discussion and feedback.",
    "prev": "(2019) replace F c (x) as a weighted sum of features:\nF c (x) = D j=1 x j entmax α (F c ) j = x · entmax α (F c ).",
    "curr": "(2)\nHere F c ∈ R D are the logits for which features to choose, and entmax α (Peters et al., 2019) is the entmax transformation which works like a sparse version of softmax such that the sum of the output equals to 1.",
    "next": "They also replace the I with entmoid which works like a sparse sigmoid that has output values between 0 and 1."
  },
  {
    "index": 198,
    "source_corpus_id": 253255190,
    "ref_id": "b2",
    "citation_corpus_id": 54203451,
    "start": 3304,
    "end": 3326,
    "title": "SYSTEMATIC GENERALIZATION: WHAT IS REQUIRED AND CAN IT BE LEARNED?",
    "abstract": "Numerous models for grounded language understanding have been recently proposed, including (i) generic models that can be easily adapted to any given task with little adaptation and (ii) intuitively appealing modular models that require background knowledge to be instantiated. We compare both types of models in how much they lend themselves to a particular form of systematic generalization. Using a synthetic VQA test, we evaluate which models are capable of reasoning about all possible object pairs after training on only a small subset of them. Our findings show that the generalization of modular models is much more systematic and that it is highly sensitive to the module layout, i.e. to how exactly the modules are connected. We furthermore investigate if modular models that generalize well could be made more end-to-end by learning their layout and parametrization. We find that end-to-end methods from prior work often learn a wrong layout and a spurious parametrization that do not facilitate systematic generalization. Our results suggest that, in addition to modularity, systematic generalization in language understanding may require explicit regularizers or priors.",
    "prev": "In practice, when trained on language data, do transformers instead constrain their computation to look equivalent to a tree-structured bottom-up computation?",
    "curr": "While generalization tests on benchmarks (Lake & Baroni, 2018;Bahdanau et al., 2019;Hupkes et al., 2019;Kim & Linzen, 2020, among others) assess if a transformer's behavior is aligned with tree-like models, they do not measure if the transformer's computation is tree-structured, largely because model behavior on benchmarks could entirely be due to orthogonal properties of the dataset (Patel et al., 2022).",
    "next": "Thus, to understand if transformers implement tree-structured computations, the approach we take is based on directly approximating them with a separate, tree-structured computation."
  },
  {
    "index": 200,
    "source_corpus_id": 229923250,
    "ref_id": "b1",
    "citation_corpus_id": 11212020,
    "start": 2344,
    "end": 2367,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "arXiv, 2020.",
    "curr": "INTRODUCTION\n\nSequence-to-Sequence (Seq2Seq) learning  has advanced the state of the art in various natural language processing (NLP) tasks, such as machine translation (Bahdanau et al., 2015;Vaswani et al., 2017;Wu et al., 2019), text summarization (Wang et al., 2019b;Zhang et al., 2020), and grammatical error correction (Kiyono et al., 2019;Kaneko et al., 2020).",
    "next": "Seq2Seq models are generally implemented with an encoder-decoder framework, in which a multi-layer encoder summarizes a source sequence into a sequence of representation and another multi-layer decoder produces the target sequence conditioned on the encoded representation."
  },
  {
    "index": 202,
    "source_corpus_id": 219969405,
    "ref_id": "b19",
    "citation_corpus_id": 71145737,
    "start": 2905,
    "end": 2909,
    "title": "Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples",
    "abstract": "Few-shot classification refers to learning a classifier for new classes given only a few examples. While a plethora of models have emerged to tackle this recently, we find the current procedure and datasets that are used to systematically assess progress in this setting lacking. To address this, we propose META-DATASET: a new benchmark for training and evaluating few-shot classifiers that is large-scale, consists of multiple datasets, and presents more natural and realistic tasks. The aim is to measure the ability of state-ofthe-art models to leverage diverse sources of data to achieve higher generalization, and to evaluate that generalization ability in a more challenging setting. We additionally measure robustness of current methods to variations in the number of available examples and the number of classes. Finally our extensive empirical evaluation leads us to identify weaknesses in Prototypical Networks and MAML, two popular few-shot classification methods, and to propose a new method, Proto-MAML, which achieves improved performance on our benchmark.",
    "prev": "Recently, Triantafillou et al.",
    "curr": "[20] proposed a benchmark for multi-domain few-shot classification, Meta-Dataset, and highlighted some of the challenges that current methods face when training data is heterogeneous.",
    "next": "Crucially, they found that methods which trained on all available domains would normally obtain improved performance on some domains at the expense of others."
  },
  {
    "index": 203,
    "source_corpus_id": 59279266,
    "ref_id": "b3",
    "citation_corpus_id": 17682909,
    "start": 4146,
    "end": 4166,
    "title": "Spectral Networks and Deep Locally Connected Networks on Graphs",
    "abstract": "Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for lowdimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.",
    "prev": "Spectral methods leverage graph Fourier transform to convert signals defined in vertex domain into spectral domain, e.g., the space spanned by the eigenvectors of the graph Laplacian matrix, and then filter is defined in spectral domain, maintaining the weight sharing property of CNN.",
    "curr": "As the pioneering work of spectral methods, spectral CNN (Bruna et al., 2014) exploited graph data with the graph Fourier transform to implement convolution operator using convolution theorem.",
    "next": "Some subsequent works make spectral methods spectrum-free (Defferrard et al., 2016;Kipf & Welling, 2017;Khasanova & Frossard, 2017), achieving locality in spatial domain and avoiding high computational cost of the eigendecomposition of Laplacian matrix."
  },
  {
    "index": 204,
    "source_corpus_id": 252815987,
    "ref_id": "b13",
    "citation_corpus_id": 3480671,
    "start": 6503,
    "end": 6520,
    "title": "NON-AUTOREGRESSIVE NEURAL MACHINE TRANSLATION",
    "abstract": "Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English-German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English-Romanian.",
    "prev": "For example, including an extra mathematical symbol can push everything one line further down.",
    "curr": "Some datasets also have long-term symbolic dependencies, which may be difficult for non-sequential models to handle, analogous to some of the challenges observed in nonautoregressive machine translation (Gu et al., 2018).",
    "next": "Generation with Diffusion Models Denoising diffusion probabilistic models (DDPM) (Ho et al., 2020) parameterize a probabilistic distribution P (y 0 |x) as a Markov chain P (y t−1 |y t ) with an initial distribution P (y T )."
  },
  {
    "index": 205,
    "source_corpus_id": 209314627,
    "ref_id": "b0",
    "citation_corpus_id": 3651422,
    "start": 47962,
    "end": 47982,
    "title": "Workshop track -ICLR 2018 UNCERTAINTY ESTIMATION VIA STOCHASTIC BATCH NORMALIZATION",
    "abstract": "In this work, we investigate Batch Normalization technique and propose its probabilistic interpretation. We propose a probabilistic model and show that Batch Normalization maximazes the lower bound of its marginalized log-likelihood. Then, according to the new probabilistic model, we design an algorithm which acts consistently during train and test. However, inference becomes computationally inefficient. To reduce memory and computational cost, we propose Stochastic Batch Normalization -an efficient approximation of proper inference procedure. This method provides us with a scalable uncertainty estimation technique. We demonstrate the performance of Stochastic Batch Normalization on popular architectures (including deep convolutional architectures: VGG-like and ResNets) for MNIST and CIFAR-10 datasets.",
    "prev": "It does not provide a consistent ranking of different ensembling techniques.",
    "curr": "(\nSrivastava et al., 2014;Gal & Ghahramani, 2016), variational inference(Blundell et al., 2015;  Kingma et al., 2015; Louizos & Welling, 2017), batch normalization(Ioffe & Szegedy, 2015; Teye  et al., 2018;Atanov et al., 2019), Laplace approximation(Ritter et al., 2018)  and many more.Snapshot-based methods aim to obtain sets of weights for deep learning models and then to average the predictions across these weights.",
    "next": "The weights can be trained independently (e.g."
  },
  {
    "index": 207,
    "source_corpus_id": 222208810,
    "ref_id": "b12",
    "citation_corpus_id": 969555,
    "start": 13384,
    "end": 13406,
    "title": "Pointing the Unknown Words",
    "abstract": "The problem of rare and unknown words is an important issue that can potentially effect the performance of many NLP systems, including both traditional countbased and deep learning models. We propose a novel way to deal with the rare and unseen words for the neural network models with attention. Our model uses two softmax layers in order to predict the next word in conditional language models: one of the softmax layers predicts the location of a word in the source sentence, and the other softmax layer predicts a word in the shortlist vocabulary. The decision of which softmax layer to use at each timestep is adaptively made by an MLP which is conditioned on the context. We motivate this work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the environment when the name of an object is not known. Using our proposed model, we observe improvements in two tasks, neural machine translation on the Europarl English to French parallel corpora and text summarization on the Gigaword dataset.",
    "prev": "The decoder generates an action sentence a t token-by-token to interact with the game.",
    "curr": "The encoder and decoder are based on a Transformer Seq2Seq model with pointer softmax mechanism (Gulcehre et al., 2016 When playing a game, an agent might get stuck at certain states due to various failures (e.g., action is grammatically incorrect, wrong object name).",
    "next": "The observation for a failed action does not contain any useful feedback, so a fully deterministic model tends to produce the same (wrong) action repeatedly."
  },
  {
    "index": 211,
    "source_corpus_id": 260126025,
    "ref_id": "b39",
    "citation_corpus_id": 249538336,
    "start": 7152,
    "end": 7156,
    "title": "ESCHER: ESCHEWING IMPORTANCE SAMPLING IN GAMES BY COMPUTING A HISTORY VALUE FUNCTION TO ESTIMATE REGRET",
    "abstract": "Recent techniques for approximating Nash equilibria in very large games leverage neural networks to learn approximately optimal policies (strategies). One promising line of research uses neural networks to approximate counterfactual regret minimization (CFR) or its modern variants. DREAM, the only current CFR-based neural method that is model free and therefore scalable to very large games, trains a neural network on an estimated regret target that can have extremely high variance due to an importance sampling term inherited from Monte Carlo CFR (MCCFR). In this paper we propose an unbiased model-free method that does not require any importance sampling. Our method, ESCHER, is principled and is guaranteed to converge to an approximate Nash equilibrium with high probability. We show that the variance of the estimated regret of ESCHER is orders of magnitude lower than DREAM and other baselines. We then show that ESCHER outperforms the prior state of the art-DREAM and neural fictitious self play (NFSP)-on a number of games and the difference becomes dramatic as game size increases. In the very large game of dark chess, ESCHER is able to beat DREAM and NFSP in a head-to-head competition over 90% of the time.",
    "prev": "There are a number of related deep reinforcement learning methods for two-player zero-sum games.",
    "curr": "CFR-based techniques such as Deep CFR [4], DREAM [62], and ESCHER [40], use deep reinforcement learning to approximate CFR.",
    "next": "Policy-gradient techniques such as RPG [60], NeuRD [23], Friction-FoReL [53; 52], and MMD [59], approximate Nash equilibrium via modified actor-critic algorithms."
  },
  {
    "index": 213,
    "source_corpus_id": 226237047,
    "ref_id": "b24",
    "citation_corpus_id": 40100965,
    "start": 1791,
    "end": 1812,
    "title": "Universal Language Model Fine-tuning for Text Classification",
    "abstract": "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100× more data. We opensource our pretrained models and code 1 .",
    "prev": "We also demonstrate that the new objective leads to models that are more robust to different levels of noise in the training data, and can generalize better to related tasks with limited labeled task data.",
    "curr": "INTRODUCTION\n\nState-of-the-art for most existing natural language processing (NLP) classification tasks is currently achieved by systems that are first pre-trained on auxiliary language modeling tasks and then fine-tuned on the task of interest with cross-entropy loss (Radford et al., 2019;Howard & Ruder, 2018;Liu et al., 2019;Devlin et al., 2019).",
    "next": "Although commonly used, cross-entropy loss -the KL-divergence between one-hot vectors of labels and the distribution of model's output logits -has several shortcomings."
  },
  {
    "index": 214,
    "source_corpus_id": 52900371,
    "ref_id": "b10",
    "citation_corpus_id": 65455367,
    "start": 29042,
    "end": 29061,
    "title": "ON THE CONVERGENCE OF ADAM AND BEYOND",
    "abstract": "Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSPROP, ADAM, ADADELTA, NADAM are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where ADAM does not converge to the optimal solution, and describe the precise problems with the previous analysis of ADAM algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with \"long-term memory\" of past gradients, and propose new variants of the ADAM algorithm which not only fix the convergence issues but often also lead to improved empirical performance.",
    "prev": "The figure 2 depicts trajectories for one particular seed, while the figure 3 plots means and error bars over these distinct seeds.",
    "curr": "C ADDITIONAL SIMULATIONS C.1 ADAM VERSUS AMSGRAD Plotted in the figure S1 are the comparisons between Adam (Kingma & Ba, 2014) and AMS-Grad (J. Reddi et al., 2018) algorithms with the default hyperparameters α = 10 −3 , (β 1 , β 2 ) = (0.9, 0.999), and = 10 −8 .",
    "next": "The AMSGrad algorithm marginally outperforms the Adam algorithm for the tasks at hand and thus the results with the AMSGrad are presented in the main text."
  },
  {
    "index": 215,
    "source_corpus_id": 226254365,
    "ref_id": "b22",
    "citation_corpus_id": 202712906,
    "start": 2743,
    "end": 2746,
    "title": "Published as a conference paper at ICLR 2020 RAPID LEARNING OR FEATURE REUSE? TOWARDS UNDERSTANDING THE EFFECTIVENESS OF MAML",
    "abstract": "An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML's popularity, a fundamental open question remains -is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse, with the meta-initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of the underlying neural network. ANIL matches MAML's performance on benchmark few-shot image classification and RL and offers computational improvements over MAML. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly. * Equal contribution arXiv:1909.09157v2 [cs.LG] . Recasting gradientbased meta-learning as hierarchical bayes. arXiv preprint arXiv:1801.08930, 2018.James Harrison, Apoorva Sharma, and Marco Pavone. Meta-learning priors for efficient online bayesian regression. arXiv preprint arXiv:1807.08912, 2018. . Gradient-based meta-learning with learned layerwise metric and subspace. arXiv preprint arXiv:1801.05558, 2018.",
    "prev": "Introduction\n\nTraining, regularizing, and understanding complex neural network models is challenging.",
    "curr": "There remain central open questions on making training faster and more data-efficient [11,23,24], ensuring better generalisation [32] and improving transparency and robustness [2,20].",
    "next": "A promising approach for addressing these questions is learning to teach [35], in which learned auxiliary information about a task is provided to a neural network to inform the training process and help downstream objectives."
  },
  {
    "index": 216,
    "source_corpus_id": 245836975,
    "ref_id": "b25",
    "citation_corpus_id": 5959482,
    "start": 3603,
    "end": 3625,
    "title": "Efficient Estimation of Word Representations in Vector Space",
    "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities. arXiv:1301.3781v3 [cs.CL] 7 Sep 2013 1 The test set is available at www.fit.vutbr.cz/˜imikolov/rnnlm/word-test.v1.txt 2",
    "prev": "Zero-shot methods, on the other hand, commonly leverage word embeddings to discover or generate related features between seen and unseen classes (Bucher et al., 2019;Gu et al., 2020) without the need for additional annotations.",
    "curr": "Existing works in this space use standard word embeddings (Mikolov et al., 2013) and focus on the image encoder.",
    "next": "In this work, we present a simple approach to leveraging modern language models to increase the flexibility and generality of semantic segmentation models."
  },
  {
    "index": 218,
    "source_corpus_id": 52890982,
    "ref_id": "b12",
    "citation_corpus_id": 3338083,
    "start": 1887,
    "end": 1912,
    "title": "Published as a conference paper at ICLR 2018 SYNTHESIZING REALISTIC NEURAL POPULATION ACTIVITY PATTERNS USING GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "The ability to synthesize realistic patterns of neural activity is crucial for studying neural information processing. Here we used the Generative Adversarial Networks (GANs) framework to simulate the concerted activity of a population of neurons. We adapted the Wasserstein-GAN variant to facilitate the generation of unconstrained neural population activity patterns while still benefiting from parameter sharing in the temporal domain. We demonstrate that our proposed GAN, which we termed Spike-GAN, generates spike trains that match accurately the first-and second-order statistics of datasets of tens of neurons and also approximates well their higher-order statistics. We applied Spike-GAN to a real dataset recorded from salamander retina and showed that it performs as well as state-ofthe-art approaches based on the maximum entropy and the dichotomized Gaussian frameworks. Importantly, Spike-GAN does not require to specify a priori the statistics to be matched by the model, and so constitutes a more flexible method than these alternative approaches. Finally, we show how to exploit a trained Spike-GAN to construct 'importance maps' to detect the most relevant statistical structures present in a spike train. Spike-GAN provides a powerful, easy-to-use technique for generating realistic spiking neural activity and for describing the most relevant features of the large-scale neural population recordings studied in modern systems neuroscience.",
    "prev": "However, audio signals have high temporal resolution, and strategies that learn such a latent representation must perform effectively in high dimensions.",
    "curr": "Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) are one such unsupervised strategy for mapping low-dimensional latent vectors to high-dimensional data.",
    "next": "The potential advantages of GAN-based approaches to audio synthesis are numerous."
  },
  {
    "index": 220,
    "source_corpus_id": 256416405,
    "ref_id": "b21",
    "citation_corpus_id": 3488815,
    "start": 4363,
    "end": 4383,
    "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
    "abstract": "Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. * Authors ordered alphabetically.",
    "prev": "• We compare interpretations of the proof features for standard DNNs and state-of-the-art robustly trained DNNs for the MNIST and CIFAR10 datasets.",
    "curr": "We observe that the proof features corresponding to the standard networks rely on spurious input features while the proofs of adversarially trained DNNs (Madry et al., 2018) filter out some of the spurious features.",
    "next": "In contrast, the networks trained with certifiable training  produce proofs that do not rely on any spurious features but they also miss out on some meaningful features."
  },
  {
    "index": 221,
    "source_corpus_id": 249097375,
    "ref_id": "b6",
    "citation_corpus_id": 211146562,
    "start": 6732,
    "end": 6749,
    "title": "Published as a conference paper at ICLR 2020 DIVIDEMIX: LEARNING WITH NOISY LABELS AS SEMI-SUPERVISED LEARNING",
    "abstract": "Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at",
    "prev": "Semi-supervised learning is also popular and effective on learning with noisy labels in recent years.",
    "curr": "Some works [Li et al., 2020a, Nguyen et al., 2020 first perform clustering on the sample loss and divide the samples into clean ones and noisy ones.",
    "next": "Then drop the labels of the \"noisy samples\" and perform semi-supervised learning on all the samples."
  },
  {
    "index": 224,
    "source_corpus_id": 246607791,
    "ref_id": "b6",
    "citation_corpus_id": 213896662,
    "start": 4842,
    "end": 4858,
    "title": "Published as a conference paper at ICLR 2020 GRADIENTS AS FEATURES FOR DEEP REPRESENTATION LEARNING",
    "abstract": "We address the challenging problem of deep representation learning -the efficient adaption of a pre-trained deep network to different tasks. Specifically, we propose to explore gradient-based features. These features are gradients of the model parameters with respect to a task-specific loss given an input sample. Our key innovation is the design of a linear model that incorporates both gradient and activation of the pre-trained network. We demonstrate that our model provides a local linear approximation to an underlying deep model, and discuss important theoretical insights. Moreover, we present an efficient algorithm for the training and inference of our model without computing the actual gradients. Our method is evaluated across a number of representation-learning tasks on several datasets and using different network architectures. Strong results are obtained in all settings, and are well-aligned with our theoretical insights 1 .arXiv:2004.05529v1 [cs.LG] 12 Apr 2020Published as a conference paper at ICLR 2020 is evaluated across tasks, datasets and network architectures and compared against a set of baseline methods. We observe empirically that our model with the gradient features outperforms the traditional activation-based logistic regressor by a significant margin in all settings. Moreover, our model compares favorably against fine-tuning of network parameters.Our main contributions are thus summarized as follows.• We propose a novel representation-learning method. At the core of our method lies in a linear model that builds on gradients of model parameters as the feature representation. • From a theoretical perspective, we claim that our linear model provides a local approximation of fine-tuning an underlying deep model. From a practical perspective, we devise an efficient and scalable algorithm for the training and inference of our method. • We demonstrate strong results of our method across various representation-learning tasks, different network architectures and several datasets. Furthermore, these empirical results are wellaligned with our theoretical insight.",
    "prev": "Furthermore, although that GANs and VAEs are known to be able to generate high-quality samples from the data distribution, there is no strong evidence that they encode explicit layerwise representations to similar quality as in supervised learning models, which implies that there does not exist a natural way to explicitly extract a representation from intermediate layer activations in unsupervisedly pre-trained generative models.",
    "curr": "Additionally, layer activations alone do not suffice to reach the full power of learned representations hidden in neural network models, as shown in recent works (Mu et al., 2020) that incorporating additional gradients-based features into representation leads to substantial improvement over solely using activations-based features.",
    "next": "In light of these constraints, we are interested in the question: is there a principled method for representation extraction beyond layer activations?"
  },
  {
    "index": 225,
    "source_corpus_id": 257079072,
    "ref_id": "b19",
    "citation_corpus_id": 30535508,
    "start": 15896,
    "end": 15917,
    "title": "LEARNING SPARSE NEURAL NETWORKS THROUGH L 0 REGULARIZATION",
    "abstract": "We propose a practical method for L 0 norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of L 0 regularization. However, since the L 0 norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected L 0 norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the hard concrete distribution for the gates, which is obtained by \"stretching\" a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.",
    "prev": "As opposed to when using a straight through estimator (Qiao et al., 2021), non-zero gradients are ensured during the backward pass.",
    "curr": "To overcome training limitations, we use a hard concrete distribution (Qiao et al., 2021;Louizos et al., 2018).",
    "next": "It rescales the weights and the random variable introduced during training prevents from obtaining local minima (Appendix B)."
  },
  {
    "index": 226,
    "source_corpus_id": 262083735,
    "ref_id": "b9",
    "citation_corpus_id": 212747810,
    "start": 20442,
    "end": 20466,
    "title": "Calibration of Pre-trained Transformers",
    "abstract": "Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT (Devlin et al., 2019)  and RoBERTa  (Liu et al., 2019)  in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging outof-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pretrained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5× lower;(2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain. 1",
    "prev": "onstructed by the meteorological community.Hallenbeck (1920), for example, presents the performance of a certain rain forecasting method by aggregating results over 6 months into a table: Among the days forecast to have between 10%−20% chance of rain, the table records the true fraction of days which were rainy -and similarly for every forecast interval.This early account of calibration already applies the practice of binning-discretizing predictions into bins, and estimating frequencies conditional on each bin.Plots of these tables turned into binned reliability diagrams (Murphy and Winkler, 1977;DeGroot and Fienberg, 1983), which was recently popularized in the machine learning community by a series of works including Zadrozny and Elkan (2001); Niculescu-Mizil and Caruana (2005); Guo et al.",
    "curr": "(2017).Binned reliability diagrams continue to be used in studies of calibration in machine learning, including in the GPT-4 tech report (Guo et al., 2017;Nixon et al., 2019;Minderer et al., 2021;Desai and Durrett, 2020;?",
    "next": ";OpenAI, 2023)."
  },
  {
    "index": 227,
    "source_corpus_id": 253107476,
    "ref_id": "b20",
    "citation_corpus_id": 3144218,
    "start": 2476,
    "end": 2498,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "* Equal contribution.",
    "curr": "INTRODUCTION\n\nDeep learning has made tremendous advances in the past decade, leading to state-of-the-art performance on various learning tasks such as computer vision (He et al., 2016), natural language processing (Devlin et al., 2019) and graph learning (Kipf & Welling, 2017).",
    "next": "While some progress has been made regarding the theoretical understanding of these deep models (Arora et al., 2018;Bartlett et al., 2019;Neyshabur et al., 2015;Dziugaite & Roy, 2017), the considered settings are unfortunately often very restrictive and the insights made are only qualitative or very loose."
  },
  {
    "index": 231,
    "source_corpus_id": 29154793,
    "ref_id": "b12",
    "citation_corpus_id": 26100519,
    "start": 8200,
    "end": 8204,
    "title": "DEEP VOICE 3: 2000-SPEAKER NEURAL TEXT-TO-SPEECH",
    "abstract": "We present Deep Voice 3, a fully-convolutional attention-based neural textto-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural speech synthesis systems in naturalness while training ten times faster. We scale Deep Voice 3 to data set sizes unprecedented for TTS, training on more than eight hundred hours of audio from over two thousand speakers. In addition, we identify common error modes of attention-based speech synthesis networks, demonstrate how to mitigate them, and compare several different waveform synthesis methods. We also describe how to scale inference to ten million queries per day on one single-GPU server.2. We show that our architecture trains quickly and scales to the LibriSpeech dataset(Panayotov et al., 2015), which consists of nearly 820 hours of audio data from 2484 speakers.3. We demonstrate that we can generate monotonic attention behavior, avoiding error modes commonly occurred in speech synthesis. 4. We compare the quality of several waveform synthesis methods for a single speaker, including WORLD (Morise et al., 2016), Griffin-Lim (Griffin & Lim, 1984), and WaveNet (Oord et al., 2016). 5. We describe the implementation of an inference kernel for Deep Voice 3, which can serve up to ten million queries per day on one single-GPU server. * Authors listed in reverse alphabetical order. † These authors contributed to this work while members of Baidu Research. Under review as a conference paper at ICLR 2018 2 RELATED WORK Our work builds upon the state-of-the-art in neural speech synthesis and attention-based sequenceto-sequence learning. Several recent works tackle the problem of synthesizing speech with neural networks, including Deep Voice 1 (Arık et al., 2017), Deep Voice 2 (Arık et al., 2017), Tacotron (Wang et al., 2017), Char2Wav (Sotelo et al., 2017), VoiceLoop (Taigman et al., 2017), SampleRNN (Mehri et al., 2017), and WaveNet (Oord et al., 2016). Deep Voice 1 & 2 retain the traditional structure of TTS pipelines, separating grapheme-to-phoneme conversion, duration and frequency prediction, and waveform synthesis. In contrast to Deep Voice 1 & 2, Deep Voice 3 employs an attention-based sequenceto-sequence model, yielding a more compact architecture. Similar to Deep Voice 3, Tacotron and Char2Wav are the two proposed sequence-to-sequence models for neural TTS. Tacotron is a neural text-to-spectrogram conversion model, used with Griffin-Lim for spectrogram-to-waveform synthesis. Char2Wav predicts the parameters of WORLD vocoder (Morise et al., 2016) and uses a Sam-pleRNN conditioned upon WORLD parameters for waveform generation. In contrast to Char2Wav and Tacotron, Deep Voice 3 avoids Recurrent Neural Networks (RNNs) 1 to speed up training and alleviates several challenging error modes that attention models fall into. Thus, Deep Voice 3 makes attention-based TTS feasible for a production TTS system with no compromise on accuracy. Finally, WaveNet and SampleRNN are proposed as neural vocoder models for waveform synthesis. It is also worth noting that there are numerous alternatives for high-quality hand-engineered vocoders in the literature, such as STRAIGHT(Kawahara et al., 1999), Vocaine (Agiomyrgiannakis, 2015), and WORLD (Morise et al., 2016). Deep Voice 3 adds no novel vocoder, but has the potential to be integrated with different waveform synthesis methods with slight modifications of its architecture.",
    "prev": "In [12], the authors have used it for denoising waveforms by predicting the middle ground-truth sample from its noisy input support.",
    "curr": "Recent contributions in Text-To-Speech(TTS) [13,14] have successfully conditioned wavenet on linguistic and acoustic features to obtain state of the art performance.",
    "next": "In our encoder-decoder architecture, we use WaveNet as the output of the decoder, and backpropagate through it down to the encoder."
  },
  {
    "index": 232,
    "source_corpus_id": 247244739,
    "ref_id": "b28",
    "citation_corpus_id": 604334,
    "start": 1399,
    "end": 1402,
    "title": "Intriguing properties of neural networks",
    "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
    "prev": "E for details).",
    "curr": "Today's best-performing classifiers are vulnerable to adversarial attacks [17,46] and exhibit high robust error : for many inputs, their predictions change under adversarial perturbations, even though the true class stays the same.",
    "next": "For example, in image classification tasks, we distinguish between two categories of such attacks that are contentpreserving [16] (or consistent [38]) if their strength is limited -perceptible and imperceptible perturbations."
  },
  {
    "index": 236,
    "source_corpus_id": 258686472,
    "ref_id": "b42",
    "citation_corpus_id": 208910151,
    "start": 6960,
    "end": 6979,
    "title": "Optimism in Reinforcement Learning with Generalized Linear Function Approximation",
    "abstract": "We design a new provably efficient algorithm for episodic reinforcement learning with generalized linear function approximation. We analyze the algorithm under a new expressivity assumption that we call \"optimistic closure,\" which is strictly weaker than assumptions from prior analyses for the linear setting. With optimistic closure, we prove that our algorithm enjoys a regret bound ofÕ(where d is the dimensionality of the state-action features and T is the number of episodes. This is the first statistically and computationally efficient algorithm for reinforcement learning with generalized linear functions.",
    "prev": "Related Work\n\nRL with linear function approximation To make MDPs with large state space amenable for provable RL, there has been an explosion of works relying on MDP classes with various linear structures (Jiang et al., 2017;Sun et al., 2019;Du et al., 2021;Jin et al., 2021).",
    "curr": "Among different assumptions made in recent work Wang et al., 2020b;Jin et al., 2020b;Du et al., 2019;Zanette et al., 2020;Ayoub et al., 2020;Jia et al., 2020;Weisz et al., 2021;Zhou et al., 2021;He et al., 2022b;Zhou and Gu, 2022;He et al., 2022a), we consider the linear mixture MDP setting Ayoub et al., 2020;Zhou et al., 2021;Zhang et al., 2021a;He et al., 2022b), where the transition kernel is a linear combination of d given models.",
    "next": "More specifically, we focus on the adversarial linear mixture MDP of He et al."
  },
  {
    "index": 237,
    "source_corpus_id": 264812826,
    "ref_id": "b4",
    "citation_corpus_id": 52967399,
    "start": 2040,
    "end": 2061,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "xpressiveness limitations.We show that despite the continuous embedding space being more expressive than the discrete token space, soft-prompting and prefix-tuning are strictly less expressive than full fine-tuning, even with the same number of learnable parameters.Concretely, context-based fine-tuning cannot change the relative attention pattern over the content and can only bias the outputs of an attention layer in a fixed direction.This suggests that while techniques like prompting, in-context learning, soft prompting, and prefixtuning can effectively elicit skills present in the pretrained model, they cannot learn novel tasks that require new attention patterns.",
    "curr": "INTRODUCTION\n\nLanguage model advances are largely driven by larger models and more training data (Kaplan et al., 2020;Rae et al., 2021).Training cutting-edge models is out of reach for most academic researchers, small enterprises, and individuals, and it has become common to instead fine-tune open-source pretrained models (Devlin et al., 2019;Min et al., 2021).Yet, due to escalating computational demands, even fine-tuning of the larger models has become prohibitively expensive (Lialin et al., 2023).",
    "next": "As a result, there is an acute need for more efficient fine-tuning methods, either by sparsely modifying the parameters of the model or modifying its input context.Examples of the first type include adapter modules which introduce a few trainable layers to modify the behaviour of the frozen pretrained network (Rebuffi et al., 2017;Houlsby et al., 2019;Hu et al., 2023).One can also use low-rank updates, which also results in a reduced number of trainable parameters (Hu et al., 2021)."
  },
  {
    "index": 241,
    "source_corpus_id": 254854553,
    "ref_id": "b31",
    "citation_corpus_id": 3488815,
    "start": 3190,
    "end": 3210,
    "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
    "abstract": "Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. * Authors ordered alphabetically.",
    "prev": "Among a variety of robustness evaluation methods, gradient-based adversarial attack generation makes a tremendous success in the computer vision (CV) domain (Croce & Hein, 2020;Dong et al., 2020).",
    "curr": "For example, the projected gradient descent (PGD)-based methods have been widely used to benchmark the adversarial robustness of CV models (Madry et al., 2018;Zhang et al., 2019b;Shafahi et al., 2019;Wong et al., 2020;Zhang et al., 2019a;Athalye et al., 2018).",
    "next": "However, in the natural language processing (NLP) area, the predominant robustness evaluation tool belongs to query-based attack generation methods Garg & Ramakrishnan, 2020;Li et al., 2019), which do not make the full use of gradient information."
  },
  {
    "index": 242,
    "source_corpus_id": 252531169,
    "ref_id": "b0",
    "citation_corpus_id": 11212020,
    "start": 1690,
    "end": 1713,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "INTRODUCTION\n\nSummarization is the process of condensing a source text into a shorter version while preserving its information content.",
    "curr": "Thanks to neural encoder-decoder models (Bahdanau et al., 2015;Sutskever et al., 2014), Transformer-based architectures (Vaswani et al., 2017), and large-scale pretraining (Liu & Lapata, 2019;Zhang et al., 2020;Lewis et al., 2020), the past few years have witnessed a huge leap forward in summarization technology.",
    "next": "Abstractive methods fluently paraphrase the main content of the input, using a vocabulary different from the original document, while extractive approaches are less creative -they produce summaries by identifying and subsequently concatenating the most important sentences in a document -but manage to avoid hallucinations, false statements and inconsistencies."
  },
  {
    "index": 243,
    "source_corpus_id": 245837268,
    "ref_id": "b18",
    "citation_corpus_id": 13046179,
    "start": 2607,
    "end": 2633,
    "title": "A BASELINE FOR DETECTING MISCLASSIFIED AND OUT-OF-DISTRIBUTION EXAMPLES IN NEURAL NETWORKS",
    "abstract": "We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.Published as a conference paper at ICLR 2017 one method which outperforms the baseline on some (but not all) tasks. This new method evaluates the quality of a neural network's input reconstruction to determine if an example is abnormal.",
    "prev": "Since there is little prior warning of phase transitions, they pose a challenge to monitoring the safety of ML systems.",
    "curr": "Spurred by this challenge, we propose an anomaly detection task (Hendrycks & Gimpel, 2017;Tack et al., 2020): Can we detect when the true reward starts to drop, while maintaining a low false positive rate in benign cases?",
    "next": "We instantiate our proposed task, POLYNOMALY, for the traffic and COVID environments (Section 5)."
  },
  {
    "index": 245,
    "source_corpus_id": 20472740,
    "ref_id": "b51",
    "citation_corpus_id": 1957433,
    "start": 9627,
    "end": 9652,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "Embedding Layer converts each word or phrase to a vector representation and construct the representation matrix for sentences.",
    "curr": "In embedding layer, a model can map tokens to vectors with the pre-trained word representation such as GloVe (Pennington et al., 2014), word2Vec (Mikolov et al., 2013) and fasttext (Joulin et al., 2016).",
    "next": "It can also utilize the preprocessing tool, e.g."
  },
  {
    "index": 247,
    "source_corpus_id": 247594724,
    "ref_id": "b0",
    "citation_corpus_id": 67915085,
    "start": 1475,
    "end": 1478,
    "title": "STOCHASTIC OPTIMIZATION OF SORTING NETWORKS VIA CONTINUOUS RELAXATIONS",
    "abstract": "Sorting input objects is an important step in many machine learning pipelines. However, the sorting operator is non-differentiable with respect to its inputs, which prohibits end-to-end gradient-based optimization. In this work, we propose NeuralSort, a general-purpose continuous relaxation of the output of the sorting operator from permutation matrices to the set of unimodal row-stochastic matrices, where every row sums to one and has a distinct arg max. This relaxation permits straight-through optimization of any computational graph involve a sorting operation. Further, we use this relaxation to enable gradient-based stochastic optimization over the combinatorially large space of permutations by deriving a reparameterized gradient estimator for the Plackett-Luce family of distributions over permutations. We demonstrate the usefulness of our framework on three tasks that require learning semantic orderings of high-dimensional objects, including a fully differentiable, parameterized extension of the k-nearest neighbors algorithm. ;Chierichetti et al., 2018). By learning distributions over unobserved permutations, we can account for the uncertainty in these permutations in a principled manner. However, the challenge with stochastic optimization over discrete distributions lies in gradient estimation with respect to the distribution parameters. Vanilla REINFORCE estimators are impractical for most cases, or necessitate custom control variates for low-variance gradient estimation (Glasserman, 2013).In this regard, we consider the Plackett-Luce (PL) family of distributions over permutations (Plackett, 1975; Luce, 1959). A common modeling choice for ranking models, the PL distribution is parameterized by n scores, with its support defined over the symmetric group consisting of n! permutations. We derive a reparameterizable sampler for stochastic optimization with respect to this distribution, based on Gumbel perturbations to the n (log-)scores. However, the reparameterized sampler requires sorting these perturbed scores, and hence the gradients of a downstream learning objective with respect to the scores are not defined. By using NeuralSort instead, we can approximate the objective and obtain well-defined reparameterized gradient estimates for stochastic optimization.Finally, we apply NeuralSort to tasks that require us to learn semantic orderings of complex, highdimensional input data. First, we consider sorting images of handwritten digits, where the goal is to learn to sort images by their unobserved labels. Our second task extends the first one to quantile regression, where we want to estimate the median (50-th percentile) of a set of handwritten numbers. In addition to identifying the index of the median image in the sequence, we need to learn to map the inferred median digit to its scalar representation. In the third task, we propose an algorithm that learns a basis representation for the k-nearest neighbors (kNN) classifier in an end-to-end procedure. Because the choice of the k nearest neighbors requires a non-differentiable sorting, we use NeuralSort to obtain an approximate, differentiable surrogate. On all tasks, we observe significant empirical improvements due to NeuralSort over the relevant baselines and competing relaxations to permutation matrices.PRELIMINARIESAn n-dimensional permutation z = [z 1 , z 2 , . . . , z n ] T is a list of unique indices {1, 2, . . . , n}. Every permutation z is associated with a permutation matrix P z ∈ {0, 1} n×n with entries given as: P z [i, j] = 1 if j = z i 0 otherwise. multimodal graph matching: Sparse coding meets graph matching. In Advances in Neural Information Processing Systems, 2013.",
    "prev": "INTRODUCTION\n\nRecently, the idea of end-to-end training of neural networks with ordering supervision via continuous relaxation of the sorting function has been presented by Grover et al.",
    "curr": "[1].",
    "next": "The idea of ordering supervision is that the ground truth order of some samples is known while their absolute values remain unsupervised."
  },
  {
    "index": 249,
    "source_corpus_id": 1859294,
    "ref_id": "b10",
    "citation_corpus_id": 5590763,
    "start": 1836,
    "end": 1840,
    "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
    "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
    "prev": "Such success has been enabled by the appearance of larger datasets, more powerful computing resources and improved architectures and training algorithms.",
    "curr": "Gated units, such as the Long Short-Term Memory [24] (LSTM) and the Gated Recurrent Unit [11] (GRU), were designed to deal with the vanishing gradients problem commonly found in RNNs [8].",
    "next": "These architectures have become popularized thanks to their impressive results in a variety of tasks such as machine translation [5], language modeling [53] or speech recognition [19]."
  },
  {
    "index": 252,
    "source_corpus_id": 9655643,
    "ref_id": "b5",
    "citation_corpus_id": 5590763,
    "start": 1799,
    "end": 1816,
    "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
    "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
    "prev": "* Work done at Google Brain.",
    "curr": "INTRODUCTION\n\nSequence-to-sequence (seq2seq) models (Sutskever et al., 2014;Cho et al., 2014) with attention  have been successfully applied to many applications including machine translation (Luong et al., 2015;Jean et al., 2015), parsing (Vinyals et al., 2015a), image captioning (Vinyals et al., 2015b;Xu et al., 2015) and Automatic Speech Recognition (ASR) (Chan et al., 2016;Bahdanau et al., 2016a).",
    "next": "Previous work has assumed a fixed deterministic decomposition for each output sequence."
  },
  {
    "index": 254,
    "source_corpus_id": 8728609,
    "ref_id": "b35",
    "citation_corpus_id": 1957433,
    "start": 2450,
    "end": 2474,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "1\n\nINTRODUCTION\n\nWord embeddings-continuous-valued vector representations of words-are an almost ubiquitous component of recent natural language processing (NLP) research.",
    "curr": "Word embeddings can be learned using spectral methods (Deerwester et al., 1990) or, more commonly in recent work, via neural networks (Bengio et al., 2003;Mnih & Hinton, 2007;Mikolov et al., 2013;Pennington et al., 2014).",
    "next": "Word embeddings can also be composed to form embeddings of phrases, sentences, or documents Kiros et al., 2015;Wieting et al., 2016;Iyyer et al., 2015)."
  },
  {
    "index": 257,
    "source_corpus_id": 49907212,
    "ref_id": "b12",
    "citation_corpus_id": 9059612,
    "start": 2991,
    "end": 2998,
    "title": "ADVERSARIAL MACHINE LEARNING AT SCALE",
    "abstract": "Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet (Russakovsky  et al., 2014). Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than singlestep attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a \"label leaking\" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.",
    "prev": "Introduction\n\nRecent research has shown that neural networks exhibit significant vulnerability to adversarial examples, or slightly perturbed inputs designed to fool the network prediction.",
    "curr": "This vulnerability is present in a wide range of settings, from situations in which inputs are fed directly to classifiers [SZS + 14, CMV + 16] to highly variable real-world environments [KGB16,AEIK18].",
    "next": "Researchers have developed a host of methods to construct such attacks [GSS15, MFF, CW17, MMS + 18], most of which correspond to first order (i.e., gradient based) methods."
  },
  {
    "index": 260,
    "source_corpus_id": 52895589,
    "ref_id": "b14",
    "citation_corpus_id": 3292002,
    "start": 2047,
    "end": 2071,
    "title": "GRAPH ATTENTION NETWORKS",
    "abstract": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-theart results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a proteinprotein interaction dataset (wherein test graphs remain unseen during training).",
    "prev": "INTRODUCTION\n\nLearning with graph structured data, such as molecules, social, biological, and financial networks, requires effective representation of their graph structure (Hamilton et al., 2017b).",
    "curr": "Recently, there has been a surge of interest in Graph Neural Network (GNN) approaches for representation learning of graphs (Li et al., 2016;Hamilton et al., 2017a;Kipf & Welling, 2017;Velickovic et al., 2018;Xu et al., 2018).",
    "next": "GNNs broadly follow a recursive neighborhood aggregation (or message passing) scheme, where each node aggregates feature vectors of its neighbors to compute its new feature vector (Gilmer et al., 2017;Xu et al., 2018)."
  },
  {
    "index": 261,
    "source_corpus_id": 222379753,
    "ref_id": "b34",
    "citation_corpus_id": 49868626,
    "start": 8978,
    "end": 8996,
    "title": "META-LEARNING WITH LATENT EMBEDDING OPTIMIZATION",
    "abstract": "Gradient-based meta-learning techniques are both widely applicable and proficient at solving challenging few-shot learning and fast adaptation problems. However, they have practical difficulties when operating on high-dimensional parameter spaces in extreme low-data regimes. We show that it is possible to bypass these limitations by learning a data-dependent latent generative representation of model parameters, and performing gradient-based meta-learning in this lowdimensional latent space. The resulting approach, latent embedding optimization (LEO), decouples the gradient-based adaptation procedure from the underlying high-dimensional space of model parameters. Our evaluation shows that LEO can achieve state-of-the-art performance on the competitive miniImageNet and tieredImageNet few-shot classification tasks. Further analysis indicates LEO is able to capture uncertainty in the data, and can perform adaptation more effectively by optimizing in latent space. arXiv:1807.05960v3 [cs.LG]",
    "prev": "For example, they may assume that features that are discriminative for the base classes are also discriminative for the novel classes, suggesting a metric learning-based approach Qi et al., 2018;Snell et al., 2017;Vinyals et al., 2016;Sung et al., 2018;Hou et al., 2019).",
    "curr": "Alternatively, they may assume that model initializations that lead to rapid convergence on the base classes are also good initializations for the novel classes (Finn et al., 2017;Ravi & Larochelle, 2017;Nichol & Schulman;Rusu et al., 2019;.",
    "next": "Other methods assume that modes of intra-class variation are shared, suggesting the possibility of learned, class-agnostic augmentation policies (Hariharan & Girshick, 2017;Chen et al., 2019b)."
  },
  {
    "index": 263,
    "source_corpus_id": 263334074,
    "ref_id": "b88",
    "citation_corpus_id": 259370805,
    "start": 3118,
    "end": 3137,
    "title": "DT-Solver: Automated Theorem Proving with Dynamic-Tree Sampling Guided by Proof-level Value Function",
    "abstract": "Recent advances in neural theorem-proving resort to large language models and tree searches. When proving a theorem, a language model advises single-step actions based on the current proving state and the tree search finds a sequence of correct steps using actions given by the language model. However, prior works often conduct constant computation efforts for each proving state while ignoring that the hard states often need more exploration than easy states. Moreover, they evaluate and guide the proof search solely depending on the current proof state instead of considering the whole proof trajectory as human reasoning does. Here, to accommodate general theorems, we propose a novel Dynamic-Tree Driven Theorem Solver (DT-Solver) by guiding the search procedure with state confidence and proof-level values. Specifically, DT-Solver introduces a dynamic-tree Monte-Carlo search algorithm, which dynamically allocates computing budgets for different state confidences, guided by a new proof-level value function to discover proof states that require substantial exploration. Experiments on two popular theorem-proving datasets, PISA and Mathlib, show significant performance gains by our DT-Solver over the state-of-the-art approaches, with a 6.65% improvement on average in terms of success rate. And especially under low computing resource settings (11.03% improvement on average).",
    "prev": " further bridges the gap between human proofs and formal proofs by making it easier to impute missing steps.LEGO-Prover advances the stateof-the-art pass rate on miniF2F-valid (48.0% to 57.0%) and miniF2F-test (45.5% to 50.0%).During the proving process, LEGO-Prover also manages to generate over 20,000 skills (theorems/lemmas) and adds them to the growing library.Our ablation study indicates that these newly added skills are indeed helpful for proving theorems, resulting in an improvement from a success rate of 47.1% to 50.4%.We also release our code and all the generated skills.",
    "curr": "1\n\nINTRODUCTION\n\nThe automation of formal reasoning tasks, such as theorem proving and mathematical proof formalization, represents a formidable challenge and an active area of research within the domain of artificial intelligence (Polu & Sutskever, 2020a;Han et al., 2022;Jiang et al., 2022a;First et al., 2023;Bansal et al., 2019;Lample et al., 2022;Jiang et al., 2022b;2021;Zhao et al., 2023;Yang et al., 2023;Wang et al., 2023b;Liu et al., 2023).The process of formalizing mathematical proofs typically relies on human experts to transcribe intricate mathematical concepts into structured formal languages verifiable by interactive theorem prover like Lean (de Moura et al., 2015) or Isabelle (Paulson, 1994).This process, while robust, is often labor-intensive and demands a high level of expertise.",
    "next": "In the past few years, large language models (LLMs) have emerged as a promising avenue, with their capacity to process and produce human-like text, opening doors to the idea of LLM-based neural LEGO Prover (* lemma 1."
  },
  {
    "index": 264,
    "source_corpus_id": 238198403,
    "ref_id": "b3",
    "citation_corpus_id": 221447287,
    "start": 4451,
    "end": 4471,
    "title": "WAVEGRAD: ESTIMATING GRADIENTS FOR WAVEFORM GENERATION",
    "abstract": "This paper introduces WaveGrad, a conditional model for waveform generation through estimating gradients of the data density. This model is built on the prior work on score matching and diffusion probabilistic models. It starts from Gaussian white noise and iteratively refines the signal via a gradient-based sampler conditioned on the mel-spectrogram. WaveGrad is non-autoregressive, and requires only a constant number of generation steps during inference. It can use as few as 6 iterations to generate high fidelity audio samples. WaveGrad is simple to train, and implicitly optimizes for the weighted variational lower-bound of the log-likelihood. Empirical experiments reveal WaveGrad to generate high fidelity audio samples matching a strong likelihood-based autoregressive baseline with less sequential operations. * Work done during an internship at Google Brain. † Equal contribution.",
    "prev": "Among other popular solutions of the disentanglement problem one can mention applying vector quantization technique to the content information (Wu et al., 2020;Wang et al., 2021), utilizing features of Variational AutoEncoders (Luong & Tran, 2021; The model we propose in this paper solves the disentanglement problem by employing the encoder predicting \"average voice\": it is trained to transform mel features corresponding to each phoneme into mel features corresponding to this phoneme averaged across a large multi-speaker dataset.",
    "curr": "As for decoder, in our VC model, it is designed as a part of a Diffusion Probabilistic Model (DPM) since this class of generative models has shown very good results in speech-related tasks like raw waveform generation (Chen et al., 2021a;Kong et al., 2021) and mel feature generation (Popov et al., 2021;Jeong et al., 2021).",
    "next": "However, this decoder choice poses a problem of slow inference because DPM forward pass scheme is iterative and to obtain high-quality results it is typically necessary to run it for hundreds of iterations (Ho et al., 2020;Nichol & Dhariwal, 2021)."
  },
  {
    "index": 268,
    "source_corpus_id": 264490454,
    "ref_id": "b9",
    "citation_corpus_id": 246285344,
    "start": 1626,
    "end": 1649,
    "title": "NATURAL LANGUAGE DESCRIPTIONS OF DEEP VISUAL FEATURES",
    "abstract": "Some neurons in deep networks specialize in recognizing highly specific perceptual, structural, or semantic features of inputs. In computer vision, techniques exist for identifying neurons that respond to individual concept categories like colors, textures, and object classes. But these techniques are limited in scope, labeling only a small subset of neurons and behaviors in any network. Is a richer characterization of neuron-level computation possible? We introduce a procedure (called MILAN, for mutual-information-guided linguistic annotation of neurons) that automatically labels neurons with open-ended, compositional, natural language descriptions. Given a neuron, MILAN generates a description by searching for a natural language string that maximizes pointwise mutual information with the image regions in which the neuron is active. MILAN produces fine-grained descriptions that capture categorical, relational, and logical structure in learned features. These descriptions obtain high agreement with human-generated feature descriptions across a diverse set of model architectures and tasks, and can aid in understanding and controlling learned models. We highlight three applications of natural language neuron descriptions. First, we use MILAN for analysis, characterizing the distribution and importance of neurons selective for attribute, category, and relational information in vision models. Second, we use MILAN for auditing, surfacing neurons sensitive to human faces in datasets designed to obscure them. Finally, we use MILAN for editing, improving robustness in an image classifier by deleting neurons sensitive to text features spuriously correlated with class labels. 1",
    "prev": "iently large model from the Pythia and LLaMA families.Using causal interventions, we show that LMs' internal activations represent binding information by attaching binding ID vectors to corresponding entities and attributes.We further show that binding ID vectors form a continuous subspace, in which distances between binding ID vectors reflect their discernability.Overall, our results uncover interpretable strategies in LMs for representing symbolic knowledge in-context, providing a step towards understanding general in-context reasoning in large-scale LMs.",
    "curr": "INTRODUCTION\n\nModern language models (LMs) excel at many reasoning benchmarks, suggesting that they can perform general purpose reasoning across many domains.However, the mechanisms that underlie LM reasoning remain largely unknown (Räuker et al., 2023).The deployment of LMs in society has led to calls to better understand these mechanisms (Hendrycks et al., 2021), so as to know why they work and when they fail (Mu & Andreas, 2020;Hernandez et al., 2021;Vig et al., 2020b).",
    "next": "In this work, we seek to understand binding, a foundational skill that underlies reasoning.How humans solve binding, i.e."
  },
  {
    "index": 269,
    "source_corpus_id": 52893515,
    "ref_id": "b26",
    "citation_corpus_id": 4429876,
    "start": 3650,
    "end": 3654,
    "title": "Global optimality conditions for deep neural networks",
    "abstract": "We study the error landscape of deep linear and nonlinear neural networks with square error loss. We build on the recent results in the literature and present necessary and sufficient conditions for a critical point of the empirical risk function to be a global minimum in the deep linear network case. Our simple conditions can also be used to determine whether a given critical point is a global minimum or a saddle point. We further extend these results to deep nonlinear neural networks and prove similar sufficient conditions for global optimality in the function space.",
    "prev": "There is a large and rapidly expanding literature of optimization of neural networks.",
    "curr": "Some works focus on the loss surface [1,10,14,[16][17][18][19][20][21][24][25][26][27][28], while others study the convergence of gradient-based methods for optimizing this loss [3,6,22].",
    "next": "In particular, our focus is on the loss surface itself, independent of any algorithmic concerns; this is reflected in the works summarized below."
  },
  {
    "index": 270,
    "source_corpus_id": 51926976,
    "ref_id": "b14",
    "citation_corpus_id": 8820379,
    "start": 1483,
    "end": 1502,
    "title": "Summarizing Source Code using a Neural Attention Model",
    "abstract": "High quality source code is often paired with high level summaries of the computation it performs, for example in code documentation or in descriptions posted in online forums. Such summaries are extremely useful for applications such as code search but are expensive to manually author, hence only done for a small fraction of all code that is produced. In this paper, we present the first completely datadriven approach for generating high level summaries of source code. Our model, CODE-NN , uses Long Short Term Memory (LSTM) networks with attention to produce sentences that describe C# code snippets and SQL queries. CODE-NN is trained on a new corpus that is automatically collected from StackOverflow, which we release. Experiments demonstrate strong performance on two tasks: (1) code summarization, where we establish the first end-to-end learning results and outperform strong baselines, and (2) code retrieval, where our learned model improves the state of the art on a recently introduced C# benchmark by a large margin.",
    "prev": "Our model significantly outperforms previous models that were specifically designed for programming languages, as well as state-of-the-art NMT models.",
    "curr": "INTRODUCTION\n\nModeling the relation between source code and natural language can be used for automatic code summarization (Allamanis et al., 2016), documentation (Iyer et al., 2016), retrieval (Allamanis et al., 2015b), and even generation (Balog et al., 2016;Rabinovich et al., 2017;Yin and Neubig, 2017;Devlin et al., 2017;Murali et al., 2017).",
    "next": "In this work, we consider the general problem of generating a natural language sequence from a given snippet of source code."
  },
  {
    "index": 272,
    "source_corpus_id": 65455367,
    "ref_id": "b3",
    "citation_corpus_id": 6628106,
    "start": 2638,
    "end": 2657,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "This problem is especially exacerbated in high dimensional problems arising in deep learning.",
    "curr": "To tackle this issue, several variants of ADAGRAD, such as RMSPROP (Tieleman & Hinton, 2012), ADAM (Kingma & Ba, 2015), ADADELTA (Zeiler, 2012), NADAM (Dozat, 2016), etc, have been proposed which mitigate the rapid decay of the learning rate using the exponential moving averages of squared past gradients, essentially limiting the reliance of the update to only the past few gradients.",
    "next": "While these algorithms have been successfully employed in several practical applications, they have also been observed to not converge in some other settings."
  },
  {
    "index": 273,
    "source_corpus_id": 3300406,
    "ref_id": "b8",
    "citation_corpus_id": 7774489,
    "start": 2328,
    "end": 2350,
    "title": "Published as a conference paper at ICLR 2017 STOCHASTIC NEURAL NETWORKS FOR HIERARCHICAL REINFORCEMENT LEARNING",
    "abstract": "Deep reinforcement learning has achieved many impressive results in recent years. However, tasks with sparse rewards or long horizons continue to pose significant challenges. To tackle these important problems, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks. Our approach brings together some of the strengths of intrinsic motivation and hierarchical methods: the learning of useful skill is guided by a single proxy reward, the design of which requires very minimal domain knowledge about the downstream tasks. Then a high-level policy is trained on top of these skills, providing a significant improvement of the exploration and allowing to tackle sparse rewards in the downstream tasks. To efficiently pre-train a large span of skills, we use Stochastic Neural Networks combined with an information-theoretic regularizer. Our experiments 1 show 2 that this combination is effective in learning a wide span of interpretable skills in a sample-efficient way, and can significantly boost the learning performance uniformly across a wide range of downstream tasks.",
    "prev": "This problem is known as the problem of option discovery.",
    "curr": "Option discovery has received ample attention over many years, with varied solutions being proposed (e.g., Bacon et al., 2017;Ş imsek & Barto, 2004;Daniel et al., 2016;Florensa et al., 2017;Konidaris & Barto, 2009;Mankowitz et al., 2016;McGovern & Barto, 2001).",
    "next": "Recently, Machado et al."
  },
  {
    "index": 274,
    "source_corpus_id": 219956317,
    "ref_id": "b25",
    "citation_corpus_id": 6628106,
    "start": 21833,
    "end": 21844,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "Acceleration\n\nAs the last comparison, we discuss the combination of compression and acceleration/momentum.",
    "curr": "This setting is very important to consider as essentially all state-of-the-art methods for training deep learning models, including Adam [Kingma andBa, 2015, Reddi et al., 2018], rely on the use of momentum in one form or another.",
    "next": "One can treat the unbiased compressed gradient as a stochastic gradient [Gorbunov et al., 2020] and the theory for momentum SGD [Yang et al., 2016, Gadat et al., 2018, Loizou and Richtárik, 2017 would be applicable with an extra smoothness assumption."
  },
  {
    "index": 275,
    "source_corpus_id": 3720457,
    "ref_id": "b8",
    "citation_corpus_id": 6628106,
    "start": 2779,
    "end": 2798,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "A related hyperparameter is momentum; typically fixed to a reasonable value such as 0.9, careful tuning can also give significant performance gains (Sutskever et al., 2013).",
    "curr": "While optimizers such as Adam (Kingma & Ba, 2015) are often described as adapting coordinate-specific learning rates, in fact they also have global learning rate and momentum hyperparameters analogously to SGD, and tuning at least the learning rate can be important to good performance.",
    "next": "In light of this, it is not surprising that there have been many attempts to adapt learning rates, either online during optimization (Schraudolph, 1999;Schaul et al., 2013), or offline by fitting a learning rate schedule (Maclaurin et al., 2015)."
  },
  {
    "index": 277,
    "source_corpus_id": 257405190,
    "ref_id": "b79",
    "citation_corpus_id": 232269984,
    "start": 7844,
    "end": 7848,
    "title": "LARGE SCALE IMAGE COMPLETION VIA CO-MODUL- ATED GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "Numerous task-specific variants of conditional generative adversarial networks have been developed for image completion. Yet, a serious limitation remains that all existing algorithms tend to fail when handling large-scale missing regions. To overcome this challenge, we propose a generic new approach that bridges the gap between image-conditional and recent modulated unconditional generative architectures via co-modulation of both conditional and stochastic style representations. Also, due to the lack of good quantitative metrics for image completion, we propose the new Paired/Unpaired Inception Discriminative Score (P-IDS/U-IDS), which robustly measures the perceptual fidelity of inpainted images compared to real images via linear separability in a feature space. Experiments demonstrate superior performance in terms of both quality and diversity over state-of-the-art methods in free-form image completion and easy generalization to image-to-image translation. Code is available at https://github.com/zsyzzsoft/co-mod-gan.",
    "prev": "Besides, Zhao et al.",
    "curr": "[80] propose a novel architecture, bridging the gap between image-conditional and unconditional generation, improving free-form large-scale image completion.",
    "next": "There are also attempts to study the progressive generation."
  },
  {
    "index": 278,
    "source_corpus_id": 3502468,
    "ref_id": "b5",
    "citation_corpus_id": 5273326,
    "start": 20511,
    "end": 20533,
    "title": "Published as a conference paper at ICLR 2016 FAST AND ACCURATE DEEP NETWORK LEARNING BY EXPONENTIAL LINEAR UNITS (ELUS)",
    "abstract": "We introduce the \"exponential linear unit\" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PRe-LUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10% classification error for a single crop, single model network.",
    "prev": "FearNet was implemented in Tensorflow.",
    "curr": "For mPFC and BLA, each fully connected layer uses an exponential linear unit activation function (Clevert et al., 2016).",
    "next": "The output of the encoder also connects to a softmax output layer."
  },
  {
    "index": 279,
    "source_corpus_id": 235899205,
    "ref_id": "b26",
    "citation_corpus_id": 9586240,
    "start": 14851,
    "end": 14873,
    "title": "Annotated Gigaword",
    "abstract": "We have created layers of annotation on the English Gigaword v.5 corpus to render it useful as a standardized corpus for knowledge extraction and distributional semantics. Most existing large-scale work is based on inconsistent corpora which often have needed to be re-annotated by research teams independently, each time introducing biases that manifest as results that are only comparable at a high level. We provide to the community a public reference set based on current state-of-the-art syntactic analysis and coreference resolution, along with an interface for programmatic access. Our goal is to enable broader involvement in large-scale knowledge-acquisition efforts by researchers that otherwise may not have had the ability to produce such a resource on their own.",
    "prev": "summary/article).",
    "curr": "We provide an example of auto-prompting for a sample from the Gigaword summarization dataset (Napoles et al., 2012) with the respective masking in Figure 2 .",
    "next": "For our generation experiments, we denote HTLM-Auto-NS (not-sized) as the auto-prompt without using size hints, where HTLM-Auto-S uses the size hints based policy described in the previous section."
  },
  {
    "index": 280,
    "source_corpus_id": 252668463,
    "ref_id": "b2",
    "citation_corpus_id": 6021932,
    "start": 5171,
    "end": 5174,
    "title": "IN SEARCH OF THE REAL INDUCTIVE BIAS: ON THE ROLE OF IMPLICIT REGULARIZATION IN DEEP LEARNING",
    "abstract": "We present experiments demonstrating that some other form of capacity control, different from network size, plays a central role in learning multi-layer feedforward networks. We argue, partially through analogy to matrix factorization, that this is an inductive bias that can help shed light on deep learning.",
    "prev": "We summarize our contributions as follows:\n\nIntroduction\n\nThough neural networks (NNs) have experienced extraordinary success, understanding the generalizability of NNs and successfully using them in real-world contexts still faces a number of obstacles [1,2].",
    "curr": "It is a well-known enigma, for instance, why such NNs generalize well and do not suffer from overfitting [3,4,5].",
    "next": "Recent research on the loss landscape of NNs seeks to reduce these obstacles."
  },
  {
    "index": 281,
    "source_corpus_id": 173990564,
    "ref_id": "b13",
    "citation_corpus_id": 6706414,
    "start": 22678,
    "end": 22682,
    "title": "Published as a conference paper at ICLR 2015 EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES",
    "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples-inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
    "prev": "Consider a worst-case scenario, in which an adversary can arbitrarily alter the output of the NN with minuscule changes to the input.",
    "curr": "It is well-known that there are NNs for which this is possible [14].",
    "next": "In this case, with the help of the I/O kernel, the model becomes highly uncertain with respect to the output kernel."
  },
  {
    "index": 283,
    "source_corpus_id": 236170938,
    "ref_id": "b31",
    "citation_corpus_id": 184486852,
    "start": 4552,
    "end": 4569,
    "title": "Causal Discovery with Reinforcement Learning",
    "abstract": "Discovering causal structure among a set of variables is a fundamental problem in many empirical sciences. Traditional score-based casual discovery methods rely on various local heuristics to search for a directly acyclic graph (DAG) according to a predefined score function. While these methods, e.g., greedy equivalence search (GES), may have attractive results with infinite samples and certain model assumptions, they are less satisfactory in practice due to finite data and possible violation of assumptions. Motivated by recent advances in neural combinatorial optimization, we propose to use reinforcement learning (RL) to search for the DAG with the best scoring. Our encoder-decoder model takes observable data as input and generates graph adjacency matrices that are used to compute corresponding rewards. The reward incorporates both the predefined score function and two penalty terms for enforcing acyclicity. In contrast with typical RL applications where the goal is to learn a policy, we use RL as a search strategy and our final output would be the graph, among all graphs generated during training, that achieves the best reward. We conduct experiments on both synthetic and real data, and show that the proposed approach not only has an improved search ability but also allows for a flexible score function under the acyclicity constraint.Preprint. Under review.",
    "prev": "Current methods are typically applied to a few dozens of variables and cannot scale so well, which is imperative for modern applications like learning causal relations with gene editing interventions (Dixit et al., 2016;Macosko et al., 2015).",
    "curr": "A promising new direction for scaling up DAG discovery methods are continuous-optimization methods (Zheng et al., 2018;2020;Zhu et al., 2020;Ke et al., 2019;Brouillard et al., 2020;Yu et al., 2019).",
    "next": "In contrast to score-based and constrained-based (Peters et al., 2017;Guo et al., 2020) methods, continuous-optimization methods reinterpret the search over discrete graph topologies as a continuous problem with neural networks as function approximators, for which efficient solvers are amenable."
  },
  {
    "index": 284,
    "source_corpus_id": 231648113,
    "ref_id": "b37",
    "citation_corpus_id": 12713052,
    "start": 2086,
    "end": 2102,
    "title": "NEURAL ARCHITECTURE SEARCH WITH REINFORCEMENT LEARNING",
    "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214. * Work done as a member of the Google Brain Residency program (g.co/brainresidency.) Under review as a conference paper at ICLR 2017 neural network can be typically specified by a variable-length string. It is therefore possible to use a recurrent network -the controller -to generate such string. Training the network specified by the string -the \"child network\" -on the real data will result in an accuracy on a validation set. Using this accuracy as the reward signal, we can compute the policy gradient to update the controller. As a result, in the next iteration, the controller will give higher probabilities to architectures that receive high accuracies. In other words, the controller will learn to improve its search over time.Our experiments show that Neural Architecture Search can design good models from scratch, an achievement considered not possible with other methods. On image recognition with CIFAR-10, Neural Architecture Search can find a novel ConvNet model that is better than most human-invented architectures. Our CIFAR-10 model achieves a 3.65 test set error, while being 1.05x faster than the current best model. On language modeling with Penn Treebank, Neural Architecture Search can design a novel recurrent cell that is also better than previous RNN and LSTM architectures. The cell that our model found achieves a test set perplexity of 62.4 on the Penn Treebank dataset, which is 3.6 perplexity better than the previous state-of-the-art.",
    "prev": "INTRODUCTION\n\nInstead of manually designing neural networks, neural architecture search (NAS) algorithms are used to automatically discover the best ones (Tan & Le, 2019a;Bender et al., 2018).",
    "curr": "Early work by Zoph & Le (2017) proposed using a reinforcement learning (RL) controller that constructs candidate architectures, these are evaluated and then feedback is provided to the controller based on the performance of the candidate.",
    "next": "One major problem with this basic NAS methodology is that each evaluation is very costly -typically on the order of hours or days to train a single neural network fully."
  },
  {
    "index": 286,
    "source_corpus_id": 202573030,
    "ref_id": "b25",
    "citation_corpus_id": 153313159,
    "start": 4662,
    "end": 4683,
    "title": "Sparse Sequence-to-Sequence Models",
    "abstract": "Sequence-to-sequence models are a powerful workhorse of NLP. Most variants employ a softmax transformation in both their attention mechanism and output layer, leading to dense alignments and strictly positive output probabilities. This density is wasteful, making models less interpretable and assigning probability mass to many implausible outputs. In this paper, we propose sparse sequence-to-sequence models, rooted in a new family of α-entmax transformations, which includes softmax and sparsemax as particular cases, and is sparse for any α > 1. We provide fast algorithms to evaluate these transformations and their gradients, which scale well for large vocabulary sizes. Our models are able to produce sparse alignments and to assign nonzero probability to a short list of plausible outputs, sometimes rendering beam search exact. Experiments on morphological inflection and machine translation reveal consistent gains over dense models. the encodings [h 1 , . . . , h J ], using s t as a query vector. This is done by computing token-level scores z j := s t W (z) h j , then taking a weighted averageπ j h j , where π := softmax(z).(1) the anonymous reviewers, for helpful discussion and feedback.",
    "prev": "Furthermore, NODE allows constructing multi-layer architectures, which resembles \"deep\" GBDT that is trained end-to-end, which was never proposed before.",
    "curr": "Besides the usage of oblivious decision tables, another important design choice is the recent entmax transformation (Peters et al., 2019), which effectively performs a \"soft\" splitting feature choice in decision trees inside the NODE architecture.",
    "next": "As discussed in the following sections, these design choices are critical to obtain state-of-the-art performance."
  },
  {
    "index": 289,
    "source_corpus_id": 13206339,
    "ref_id": "b25",
    "citation_corpus_id": 6018348,
    "start": 2374,
    "end": 2392,
    "title": "Distant Supervision for Relation Extraction with an Incomplete Knowledge Base",
    "abstract": "Distant supervision, heuristically labeling a corpus using a knowledge base, has emerged as a popular choice for training relation extractors. In this paper, we show that a significant number of \"negative\" examples generated by the labeling process are false negatives because the knowledge base is incomplete. Therefore the heuristic for generating negative examples has a serious flaw. Building on a state-of-the-art distantly-supervised extraction algorithm, we proposed an algorithm that learns from only positive and unlabeled labels at the pair-of-entity level. Experimental results demonstrate its advantage over existing algorithms.",
    "prev": "We are interested in automated reasoning on large knowledge bases (KB) with rich and diverse semantics (Suchanek et al., 2007;Bollacker et al., 2008;Carlson et al., 2010).",
    "curr": "KBs are highly incomplete (Min et al., 2013), and facts not directly stored in a KB can often be inferred from those that are, creating exciting opportunities and challenges for automated reasoning.",
    "next": "For example, consider the small knowledge graph in figure 1."
  },
  {
    "index": 290,
    "source_corpus_id": 247058691,
    "ref_id": "b12",
    "citation_corpus_id": 12639289,
    "start": 2684,
    "end": 2714,
    "title": "Recurrent Continuous Translation Models",
    "abstract": "We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.",
    "prev": "This search problem is a hard combinatorial optimization problem, and as a result, constraints are frequently imposed on the structure of the model to make solving or approximating the search problem easier.",
    "curr": "In neural machine translation, an autoregressive factorization of the output probability distribution is widely used (Kalchbrenner & Blunsom, 2013;Sutskever et al., 2014;Vaswani et al., 2017), and a variety of conditional independence assumptions are made in other model classes from statistical translation models (Brown et al., 1993;Koehn et al., 2003) to non-autoregressive neural models (Lee et al., 2018).",
    "next": "Although these assumptions enable fast and accurate approximations to the search problem with simple and efficient algorithms (e.g., beam search), which can be crucial for efficient production applications, they limit the form of the models and thereby restricting the kinds of architectures that can be used to address observed model failures."
  },
  {
    "index": 293,
    "source_corpus_id": 3481593,
    "ref_id": "b10",
    "citation_corpus_id": 17597823,
    "start": 22717,
    "end": 22736,
    "title": "SwissCheese at SemEval-2016 Task 4: Sentiment Classification Using an Ensemble of Convolutional Neural Networks with Distant Supervision",
    "abstract": "In this paper, we propose a classifier for predicting message-level sentiments of English micro-blog messages from Twitter. Our method builds upon the convolutional sentence embedding approach proposed by (Severyn and Moschitti, 2015a; Severyn and Moschitti, 2015b). We leverage large amounts of data with distant supervision to train an ensemble of 2-layer convolutional neural networks whose predictions are combined using a random forest classifier. Our approach was evaluated on the datasets of the SemEval-2016 competition (Task 4) outperforming all other approaches for the Message Polarity Classification task.",
    "prev": "Each training sample x consists of a sentence s and its sentiment labelỹ.",
    "curr": "The student for the sentiment classification task is a convolutional model which has been shown to perform best on the dataset we used (Deriu et al., 2017;Severyn & Moschitti, 2015a;b;Deriu et al., 2016).",
    "next": "The first layer of the network learns the function ψ(.)"
  },
  {
    "index": 295,
    "source_corpus_id": 257219926,
    "ref_id": "b0",
    "citation_corpus_id": 246867209,
    "start": 11467,
    "end": 11490,
    "title": "Published as a conference paper at ICLR 2022 PROSPECT PRUNING: FINDING TRAINABLE WEIGHTS AT INITIALIZATION USING META-GRADIENTS",
    "abstract": "Pruning neural networks at initialization would enable us to find sparse models that retain the accuracy of the original network while consuming fewer computational resources for training and inference. However, current methods are insufficient to enable this optimization and lead to a large degradation in model performance. In this paper, we identify a fundamental limitation in the formulation of current methods, namely that their saliency criteria look at a single step at the start of training without taking into account the trainability of the network. While pruning iteratively and gradually has been shown to improve pruning performance, explicit consideration of the training stage that will immediately follow pruning has so far been absent from the computation of the saliency criterion. To overcome the short-sightedness of existing methods, we propose Prospect Pruning (ProsPr), which uses meta-gradients through the first few steps of optimization to determine which weights to prune. ProsPr combines an estimate of the higherorder effects of pruning on the loss and the optimization trajectory to identify the trainable sub-network. Our method achieves state-of-the-art pruning performance on a variety of vision classification tasks, with less data and in a single shot compared to existing pruning-at-initialization methods. Our code is available online at https://github.com/",
    "prev": "We refer to a variant of sparsify-during-training as early pruning here, which only exerts pruning to network in the early stage of training.",
    "curr": "It includes pruning at initialization, e.g., GraSP , SynFlow (Tanaka et al., 2020), SBP-SR (Hayou et al., 2021), ProsPr (Alizadeh et al., 2022), and the conventional early pruning methods which stop pruning after several epochs of training (You et al., 2020;Liu et al., 2021b;Rachwan et al., 2022).",
    "next": "Most of these works are inspired by the discovery of Lottery Ticket Hypothesis (LTH) (Frankle & Carbin, 2019) or SNIP (Lee et al., 2019), if not both."
  },
  {
    "index": 297,
    "source_corpus_id": 202750230,
    "ref_id": "b35",
    "citation_corpus_id": 44131019,
    "start": 1691,
    "end": 1708,
    "title": "Scaling Neural Machine Translation",
    "abstract": "Sequence to sequence learning models still require several days to reach state of the art performance on large benchmark datasets using a single machine. This paper shows that reduced precision and large batch training can speedup training by nearly 5x on a single 8-GPU machine with careful tuning and implementation. 1 On WMT'14 English-German translation, we match the accuracy ofVaswani et al. (2017)in under 5 hours when training on 8 GPUs and we obtain a new state of the art of 29.3 BLEU after training for 85 minutes on 128 GPUs. We further improve these results to 29.8 BLEU by training on the much larger Paracrawl dataset. On the WMT'14 English-French task, we obtain a state-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.",
    "prev": "Moreover, we show that our approach leads to small BERT-like models of higher quality compared to training from scratch or using distillation.",
    "curr": "INTRODUCTION\n\nTransformer architectures (Vaswani et al., 2017) have become the dominant architecture in natural language processing, with state-of-the-art performance across a variety of tasks, including machine translation (Vaswani et al., 2017;Ott et al., 2018), language modeling Baevski & Auli, 2018) and sentence representation (Devlin et al., 2018;.",
    "next": "Each of its layers contains millions of parameters accessed during the forward pass, making it computationally demanding in terms of memory and latency during both training and inference."
  },
  {
    "index": 300,
    "source_corpus_id": 261530381,
    "ref_id": "b1",
    "citation_corpus_id": 208076137,
    "start": 2431,
    "end": 2455,
    "title": "GENERATIVE MODELS FOR EFFECTIVE ML ON PRIVATE, DECENTRALIZED DATASETS",
    "abstract": "To improve real-world applications of machine learning, experienced modelers develop intuition about their datasets, their models, and how the two interact. Manual inspection of raw data-of representative samples, of outliers, of misclassifications-is an essential tool in a) identifying and fixing problems in the data, b) generating new modeling hypotheses, and c) assigning or refining human-provided labels. However, manual data inspection is problematic for privacy-sensitive datasets, such as those representing the behavior of realworld individuals. Furthermore, manual data inspection is impossible in the increasingly important setting of federated learning, where raw examples are stored at the edge and the modeler may only access aggregated outputs such as metrics or model parameters. This paper demonstrates that generative models-trained using federated methods and with formal differential privacy guarantees-can be used effectively to debug many commonly occurring data issues even when the data cannot be directly inspected. We explore these methods in applications to text with differentially private federated RNNs and to images using a novel algorithm for differentially private federated GANs.Published as a conference paper at ICLR 2020 McMahan et al., 2017), which is the motivation and focus of this work-the data cannot be inspected. In FL, raw data remains distributed across a fleet of devices, such as mobile phones, while an orchestrating server coordinates training of a shared global model. Only the final model parameters and statistics are gathered and made available to the modeler. 1 How can a modeler effectively debug when training data is privacy sensitive or decentralized? This paper demonstrates that the novel application of auxiliary models, namely privacy-preserving generative models, can stand in for direct data examination during the process of debugging data errors during inference or training. By combining ideas from deep generative models, FL, and user-level differential privacy (DP), we show how some needs traditionally met with data inspection can instead be met by generating synthetic examples from a privacy-preserving federated generative model. These examples could be representative of all or a subset of the non-inspectable data, while at the same time preserving the privacy of individuals. Our contributions include:• Identifying key challenges in implementing end-to-end workflows with non-inspectable data, e.g., for debugging a 'primary' ML model used in a mobile application. • Proposing a methodology that allows (sufficiently powerful) 'auxiliary' generative models to resolve these challenges. • Demonstrating how privacy preserving federated generative models-RNNs for text and GANs for images-can be trained to high enough fidelity to discover introduced data errors matching those encountered in real world scenarios. This requires a novel adaption of generative adversarial networks (GANs) to the federated setting with user-level DP guarantees.",
    "prev": "nciples of FL.We propose a novel method, Federated Orthogonal Training (FOT), to overcome these drawbacks and address the global catastrophic forgetting in CFL.Our algorithm extracts the global input subspace of each layer for old tasks and modifies the aggregated updates of new tasks such that they are orthogonal to the global principal subspace of old tasks for each layer.This decreases the interference between tasks, which is the main cause for forgetting.We empirically show that FOT outperforms state-of-the-art continual learning methods in the CFL setting, achieving an average accuracy gain of up to 15% with 27% lower forgetting while only incurring a minimal computation and communication cost.",
    "curr": "INTRODUCTION\n\nFederated learning (FL) is a decentralized training solution born from the need of keeping the local data of clients private to train a global model (McMahan et al., 2017).Most of the FL works focus on the global learning of a single task (Hard et al., 2018;Yang et al., 2021;Augenstein et al., 2020).However, in real life, new tasks might arrive to the clients over time while previous data disappear due to storage limitations.For instance, assume a malware classifier is trained over multiple FL clients.The emergence of new malware families (new tasks) is inevitable, making the update of the classifier a necessity.Another real-life scenario can be the emergence of new viruses in some clients due to epidemics.The global model also has to learn to classify these new viruses (new tasks) (Yoon et al., 2021).In both of these scenarios, the model should not forget its prior knowledge while learning new tasks.",
    "next": "Continual Learning (CL) addresses this issue in centralized machine learning (ML), the problem of learning sequentially arrived tasks without forgetting (Kirkpatrick et al., 2017).Learning a global model while new tasks appear in the clients in an online manner is a problem of Continual Federated Learning (CFL) (Ma et al., 2022).An ideal CFL algorithm solving global catastrophic fo"
  },
  {
    "index": 301,
    "source_corpus_id": 235606307,
    "ref_id": "b7",
    "citation_corpus_id": 233386821,
    "start": 3669,
    "end": 3686,
    "title": "Published as a conference paper at ICLR 2021 CAPTURING LABEL CHARACTERISTICS IN VAEs",
    "abstract": "We present a principled approach to incorporating labels in variational autoencoders (VAEs) that captures the rich characteristic information associated with those labels. While prior work has typically conflated these by learning latent variables that directly correspond to label values, we argue this is contrary to the intended effect of supervision in VAEs-capturing rich label characteristics with the latents. For example, we may want to capture the characteristics of a face that make it look young, rather than just the age of the person. To this end, we develop the characteristic capturing VAE (CCVAE), a novel VAE model and concomitant variational objective which captures label characteristics explicitly in the latent space, eschewing direct correspondences between label values and latents. Through judicious structuring of mappings between such characteristic latents and labels, we show that the CCVAE can effectively learn meaningful representations of the characteristics of interest across a variety of supervision schemes. In particular, we show that the CCVAE allows for more effective and more general interventions to be performed, such as smooth traversals within the characteristics for a given label, diverse conditional generation, and transferring characteristics across datapoints 1 . * work done while at Oxford † equal contribution 1 Link to code: https://github.com/thwjoy/ccvae",
    "prev": "Prior approaches typically combine information through explicit specification as products (Wu & Goodman, 2018), mixtures (Shi et al., 2019), combinations of such (Sutter et al., 2021), or through additional regularisers on the representations (Suzuki et al., 2016;Sutter et al., 2020).",
    "curr": "Here, we explore an alternative approach that leverages advances in semi-supervised VAEs (Siddharth et al., 2017; Joy et al., 2021) to repurpose existing regularisation in the VAE framework as an implicit means by which information is combined across modalities (see Figure 1).",
    "next": "We develop a novel formulation for multimodal VAEs that views the combination of information through a semi-supervised lens, as mutual supervision between modalities."
  },
  {
    "index": 303,
    "source_corpus_id": 214002473,
    "ref_id": "b31",
    "citation_corpus_id": 76667896,
    "start": 2633,
    "end": 2650,
    "title": "UNSUPERVISED DISCOVERY OF PARTS, STRUCTURE, AND DYNAMICS",
    "abstract": "Humans easily recognize object parts and their hierarchical structure by watching how they move; they can then predict how each part moves in the future. In this paper, we propose a novel formulation that simultaneously learns a hierarchical, disentangled object representation and a dynamics model for object parts from unlabeled videos. Our Parts, Structure, and Dynamics (PSD) model learns to, first, recognize the object parts via a layered image representation; second, predict hierarchy via a structural descriptor that composes low-level concepts into a hierarchical structure; and third, model the system dynamics by predicting the future. Experiments on multiple real and synthetic datasets demonstrate that our PSD model works well on all three tasks: segmenting object parts, building their hierarchical structure, and capturing their motion distributions.",
    "prev": "INTRODUCTION\n\nSystem identification or physical parameter estimation is commonly required for control or state estimation for physical modelling, and typically relies on dedicated sensing equipment and carefully constructed experiments.",
    "curr": "Current machine learning approaches to physical modeling from video either require training by supervised regression from video to object coordinates before estimating explicit physics (Watters et al., 2017;Wu et al., 2017b;Belbute-Peres et al., 2018), or are able to discover and segment objects from video in an unsupervised manner, but do not naturally integrate with a physics engine for long-term predictions or generation of interpretable locations and physical parameters for physical reasoning (Xu et al., 2019;van Steenkiste et al., 2018).",
    "next": "In this work, we bridge the gap between unsupervised discovery of objects from video and learning the physical dynamics of a system, by learning unknown physical parameters and explicit trajectory coordinates."
  },
  {
    "index": 305,
    "source_corpus_id": 220793552,
    "ref_id": "b6",
    "citation_corpus_id": 54101493,
    "start": 3214,
    "end": 3217,
    "title": "IMAGENET-TRAINED CNNS ARE BIASED TOWARDS TEXTURE; INCREASING SHAPE BIAS IMPROVES ACCURACY AND ROBUSTNESS",
    "abstract": "Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNettrained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on 'Stylized-ImageNet', a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation. de Beeck. Deep neural networks as a computational model for human shape sensitivity. DiCarlo. Performance-optimized hierarchical models predict neural responses in higher visual cortex.",
    "prev": "While we are collecting datasets with millions of training samples, DNNs are still vulnerable to domain shift, small perturbations, and adversarial examples to which humans are remarkably robust [20,4].",
    "curr": "Recent research has shown that neural networks tend to use superficial features rather than global shape information for prediction even when trained on large scale datasets such as ImageNet [7].",
    "next": "These superficial features can be local textures or even patterns imperceptible to humans but detectable to the DNNs, as is the case for adversarial examples [11]."
  },
  {
    "index": 306,
    "source_corpus_id": 256105351,
    "ref_id": "b2",
    "citation_corpus_id": 67855617,
    "start": 3840,
    "end": 3857,
    "title": "ROTATE: KNOWLEDGE GRAPH EMBEDDING BY RELA- TIONAL ROTATION IN COMPLEX SPACE",
    "abstract": "We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction.Published as a conference paper at ICLR 2019ModelScore FunctionSE (Bordes et al., 2011)− Wr,1h − Wr,2t h, t ∈ R k , Wr,· ∈ R k×k TransE(Bordes et al., 2013)− h + r − t h, r, t ∈ R k TransX − gr,1(h) + r − gr,2(t) h, r, t ∈ R k DistMult(Yang et al., 2014)r, h, t h, r, t ∈ R k ComplEx(Trouillon et al., 2016)Re( r, h, t ) h, r, t ∈ C k HolE(Nickel et al., 2016)r, h ⊗ t h, r, t ∈ R k ConvE(Dettmers et al., 2017)σ (vec(σ([r, h]   * Ω))W ), t h, r, t ∈ R k RotatE − h • r − t 2 h, r, t ∈ C k , |ri| = 1",
    "prev": "This issue is well known as the Open World Assumption (OWA) (Ji et al., 2021).",
    "curr": "Representation learning methods are employed to mitigate the incompleteness issue by learning representations from the observed KG triples and generalizing them to unseen triples (Bordes et al., 2013;Trouillon et al., 2016;Sun et al., 2018;Zhang et al., 2019;Chami et al., 2020).",
    "next": "When considering logical queries over incomplete knowledge graphs, the query answering models are required to not only predict the unseen knowledge but also execute logical operators, such as conjunction, disjunction, and negation (Ren & Leskovec, 2020;Wang et al., 2021b)."
  },
  {
    "index": 308,
    "source_corpus_id": 259075184,
    "ref_id": "b25",
    "citation_corpus_id": 245704504,
    "start": 3001,
    "end": 3020,
    "title": "SDEDIT: GUIDED IMAGE SYNTHESIS AND EDITING WITH STOCHASTIC DIFFERENTIAL EQUATIONS",
    "abstract": "Guided image synthesis enables everyday users to create and edit photo-realistic images with minimum effort. The key challenge is balancing faithfulness to the user inputs (e.g., hand-drawn colored strokes) and realism of the synthesized images. Existing GAN-based methods attempt to achieve such balance using either conditional GANs or GAN inversions, which are challenging and often require additional training data or loss functions for individual applications. To address these issues, we introduce a new image synthesis and editing method, Stochastic Differential Editing (SDEdit), based on a diffusion model generative prior, which synthesizes realistic images by iteratively denoising through a stochastic differential equation (SDE). Given an input image with user guide in a form of manipulating RGB pixels, SDEdit first adds noise to the input, then subsequently denoises the resulting image through the SDE prior to increase its realism. SDEdit does not require task-specific training or inversions and can naturally achieve the balance between realism and faithfulness. SDEdit outperforms state-of-the-art GAN-based methods by up to 98.09% on realism and 91.72% on overall satisfaction scores, according to a human perception study, on multiple tasks, including stroke-based image synthesis and editing as well as image compositing.",
    "prev": " in various applications, including but not limited to object detection (Misra et al., 2021;Liu et al., 2021b;Wang et al., 2022a), segmentation (Qian et al., 2022;Tang et al., 2022;Zhao et al., 2021), and tracking (Qi et al., 2020;Zheng et al., 2021;Shan et al., 2021).",
    "curr": "Despite the significant advances in 3D point cloud processing, acquiring task-specific 3D annotations is a highly expensive and severely limited process due to the geometric complexity.The shortage of data annotations highlights the need for adapting pre-training paradigms.Instead of training the deep network from randomly initialized weights, prior work suggests that pre-training the network on a relevant but different pre-task and later fine-tuning the weights using task-specific labels often leads to superior performance.In natural language processing and 2D vision, pre-trained models are the backbones of many exciting applications, such as real-time chatbots (Touvron et al., 2023;OpenAI, 2023) and graphic designers (Meng et al., 2021;Wang et al., 2022b).However, pre-training on point clouds has yet to demonstrate a universal performance improvement.From-scratch training remains a common practice in 3D vision.",
    "next": "Initial attempts towards 3D point-cloud pre-training primarily leverage contrastive learning (Chopra et al., 2005), especially when the point clouds are collected from indoor scenes (Xie et al., 2020;Rao et al., 2021;Liu et al., 2021a;Zhang et al., 2021;Chen et al., 2022).However, the broad application of contrastive learning-based pre-training techniques is impeded by the requirement of large batch sizes and the necessity to carefully define positive and negative pairs.In contrast to natural language processing and 2D vision, pre-training on 3D point clouds presents two unique challenges.First, the data is extremely scarce, even without annotations.Public 3D datasets are orders of magnitude smaller than 2D image datasets.Second, the lack of data annotations necessitates 3D pre-training methods to adhere to t"
  },
  {
    "index": 310,
    "source_corpus_id": 258048805,
    "ref_id": "b16",
    "citation_corpus_id": 8968704,
    "start": 2926,
    "end": 2942,
    "title": "A Neural Representation of Sketch Drawings",
    "abstract": "We present sketch-rnn, a recurrent neural network (RNN) able to construct stroke-based drawings of common objects. The model is trained on a dataset of human-drawn images representing many different classes. We outline a framework for conditional and unconditional sketch generation, and describe new robust training methods for generating coherent sketch drawings in a vector format.",
    "prev": "Recently however, with the advent of large-scale datasets, generative modelling of chirographic data started to gain traction.",
    "curr": "Specifically, models have been trained on generic doodles/drawings data (Ha & Eck, 2018), or more \"specialized\" entities like fonts (Lopes et al., 2019), diagrams (Gervais et al., 2020;, SVG Icons (Carlier et al., 2020) etc.",
    "next": "Building unconditional neural generative models not only allows understanding the distribution of chirographic data but also enables further downstream tasks (e.g."
  },
  {
    "index": 311,
    "source_corpus_id": 221971169,
    "ref_id": "b18",
    "citation_corpus_id": 202539918,
    "start": 2223,
    "end": 2243,
    "title": "Mogrifier LSTM",
    "abstract": "Many advances in Natural Language Processing have been based upon more expressive models for how inputs interact with the context in which they occur. Recurrent networks, which have enjoyed a modicum of success, still lack the generalization and systematicity ultimately required for modelling language. In this work, we propose an extension to the venerable Long Short-Term Memory in the form of mutual gating of the current input and the previous output. This mechanism affords the modelling of a richer space of interactions between inputs and their context. Equivalently, our model can be viewed as making the transition function given by the LSTM context-dependent. Experiments demonstrate markedly improved generalization on language modelling in the range of 3-4 perplexity points on Penn Treebank and Wikitext-2, and 0.01-0.05 bpc on four character-based datasets. We establish a new state of the art on all datasets with the exception of Enwik8, where we close a large gap between the LSTM and Transformer models.",
    "prev": "For example, syntactic effects evolve at the timescale of words, whereas semantics, emotions, and narratives can evolve at much longer timescales of tens to hundreds or thousands of words.",
    "curr": "The importance of long timescale information is evident in results showing that neural networks have outperformed classical n-gram models on many language modeling benchmarks (Melis et al., 2019;Krause et al., 2019;Dai et al., 2019).",
    "next": "This difference is attributed to these networks' ability to capture long timescale dependencies that that are impossible for n-gram models."
  },
  {
    "index": 313,
    "source_corpus_id": 252715881,
    "ref_id": "b22",
    "citation_corpus_id": 235614244,
    "start": 9912,
    "end": 9916,
    "title": "IS ATTENTION BETTER THAN MATRIX DECOMPOSITION?",
    "abstract": "As an essential ingredient of modern deep learning, attention mechanism, especially self-attention, plays a vital role in the global correlation discovery. However, is hand-crafted attention irreplaceable when modeling the global context? Our intriguing finding is that self-attention is not better than the matrix decomposition (MD) model developed 20 years ago regarding the performance and computational cost for encoding the long-distance dependencies. We model the global context issue as a low-rank recovery problem and show that its optimization algorithms can help design global information blocks. This paper then proposes a series of Hamburgers, in which we employ the optimization algorithms for solving MDs to factorize the input representations into sub-matrices and reconstruct a low-rank embedding. Hamburgers with different MDs can perform favorably against the popular global context module self-attention when carefully coping with gradients back-propagated through MDs. Comprehensive experiments are conducted in the vision tasks where it is crucial to learn the global context, including semantic segmentation and image generation, demonstrating significant improvements over self-attention and its variants. Code is available.",
    "prev": "In terms of training implicit models, [25] also proposed a novel gradient estimate called phantom gradient which relies on fixed-point unrolling and a Neumann series to provide a new update direction; computation of precise gradient is forgone.",
    "curr": "Implicit models have also been extended to more complex learning frameworks, such as attention mechanisms [24] and Graph Neural Networks [30].",
    "next": "Preliminary: Differentiable optimization layers\n\nWe consider a parameterized convex optimization problems with polyhedral constraints:\nmin x f (x; θ) s.t."
  },
  {
    "index": 314,
    "source_corpus_id": 52894384,
    "ref_id": "b5",
    "citation_corpus_id": 17263016,
    "start": 4997,
    "end": 5017,
    "title": "Low Resource Dependency Parsing: Cross-lingual Parameter Sharing in a Neural Network Parser",
    "abstract": "Training a high-accuracy dependency parser requires a large treebank. However, these are costly and time-consuming to build. We propose a learning method that needs less data, based on the observation that there are underlying shared structures across languages. We exploit cues from a different source language in order to guide the learning process. Our model saves at least half of the annotation effort to reach the same accuracy compared with using the purely supervised method.",
    "prev": "RELATED WORK\n\nMulti-task learning.",
    "curr": "The dominant approach to multi-task learning is to have a model that shares parameters in a soft (Duong et al., 2015;Yang & Hospedales, 2017) or hard way (Caruana, 1993).",
    "next": "Soft sharing refers to each task having independent weights that are constrained to be similar (e.g."
  },
  {
    "index": 317,
    "source_corpus_id": 257631995,
    "ref_id": "b22",
    "citation_corpus_id": 3687922,
    "start": 2887,
    "end": 2909,
    "title": "SEMI-PARAMETRIC TOPOLOGICAL MEMORY FOR NAVIGATION",
    "abstract": "We introduce a new memory architecture for navigation in previously unseen environments, inspired by landmark-based navigation in animals. The proposed semiparametric topological memory (SPTM) consists of a (non-parametric) graph with nodes corresponding to locations in the environment and a (parametric) deep network capable of retrieving nodes from the graph based on observations. The graph stores no metric information, only connectivity of locations corresponding to the nodes. We use SPTM as a planning module in a navigation system. Given only 5 minutes of footage of a previously unseen maze, an SPTM-based navigation agent can build a topological map of the environment and use it to confidently navigate towards goals. The average success rate of the SPTM agent in goal-directed navigation across test environments is higher than the best-performing baseline by a factor of three.",
    "prev": "This implies that a goal-conditioned policy is replaceable by a policy conditioned on a \"subgoal\" existing between the goal and the agent.",
    "curr": "Based on this insight, researchers have investigated graph-based planning to construct a goal-reaching path by (a) proposing a series of subgoals and (b) executing policies conditioned on the nearest subgoal (Savinov et al., 2018;Eysenbach et al., 2019;Huang et al., 2019).",
    "next": "Since the nearby subgoals are easier to reach than the faraway goal, such planning improves the success ratio of the agent reaching the target-goal during sample collection."
  },
  {
    "index": 318,
    "source_corpus_id": 231839495,
    "ref_id": "b28",
    "citation_corpus_id": 3162051,
    "start": 3130,
    "end": 3150,
    "title": "mixup: BEYOND EMPIRICAL RISK MINIMIZATION",
    "abstract": "Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks. * Alphabetical order.",
    "prev": "To improve the generalization performance of the neural networks, a body of research has been proposed to develop regularizers based on priors or to augment the training data with task-dependent transforms (Bishop, 2006;Cubuk et al., 2019).",
    "curr": "Recently, a new taskindependent data augmentation technique, called mixup, has been proposed (Zhang et al., 2018).",
    "next": "The original mixup, called Input Mixup, linearly interpolates a given pair of input data and can be easily applied to various data and tasks, improving the generalization performance and robustness of neural networks."
  },
  {
    "index": 320,
    "source_corpus_id": 67856680,
    "ref_id": "b1",
    "citation_corpus_id": 2129889,
    "start": 1851,
    "end": 1874,
    "title": "Learning End-to-End Goal-Oriented Dialog",
    "abstract": "End-to-end dialog systems, in which all components are learnt simultaneously, have recently obtained encouraging successes. However these were mostly on conversations related to chit-chat with no clear objective and for which evaluation is difficult. This paper proposes a set of tasks to test the capabilities of such systems on goal-oriented dialogs, where goal completion ensures a well-defined measure of performance. Built in the context of restaurant reservation, our tasks require to manipulate sentences and symbols, in order to properly conduct conversations, issue API calls and use the outputs of such calls. We show that an end-to-end dialog system based on Memory Networks can reach promising, yet imperfect, performance and learn to perform non-trivial operations. We confirm those results by comparing our system to a hand-crafted slot-filling baseline on data from the second Dialog State Tracking Challenge(Henderson et al., 2014a).Currently, the most useful applications of dialog systems are goal-oriented and transactional: the system is expected to understand a user request and complete a related task with a clear goal within",
    "prev": "Based on our results, we argue that AQM+ is a general task-oriented dialog algorithm that can be applied for non-yes-or-no responses.",
    "curr": "INTRODUCTION\n\nRecent advances in deep learning have led an end-to-end neural approach to task-oriented dialog problems that can reduce a laborious labeling task on states and intents (Bordes & Weston, 2017).",
    "next": "Many researchers have applied sequence-to-sequence models (Vinyals & Le, 2015) that are trained in a supervised learning (SL) and a reinforcement learning (RL) fashion to generate an appropriate sentence for the task."
  },
  {
    "index": 323,
    "source_corpus_id": 222380524,
    "ref_id": "b13",
    "citation_corpus_id": 49411844,
    "start": 7315,
    "end": 7332,
    "title": "DARTS: Differentiable Architecture Search",
    "abstract": "This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques.",
    "prev": "AutoML is a long-pursued target of machine learning (He et al., 2019).",
    "curr": "Recently a sub-field of AutoML, neural architecture search (NAS), has attracted much attention due to its success in automating the process of neural network architecture design (Zoph & Le, 2017;Pham et al., 2018;Liu et al., 2018).",
    "next": "As an essential element, loss function has also raised the interest of researchers to automate its design process."
  },
  {
    "index": 325,
    "source_corpus_id": 238419701,
    "ref_id": "b17",
    "citation_corpus_id": 3568073,
    "start": 5182,
    "end": 5203,
    "title": "PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION",
    "abstract": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at 1024 2 . We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CELEBA dataset.",
    "prev": "Existing generative models are mostly designed to synthesize images of fixed sizes.",
    "curr": "A few methods (Karras et al., 2018; have been recently developed to train latent generative models on high-resolution images, up to 1024×1024 pixels.",
    "next": "However, latent generative models generate images from dense latent vectors that require synthesizing all structural contents at once."
  },
  {
    "index": 326,
    "source_corpus_id": 209202457,
    "ref_id": "b14",
    "citation_corpus_id": 108304275,
    "start": 2334,
    "end": 2357,
    "title": "DISCRIMINATOR-ACTOR-CRITIC: ADDRESSING SAMPLE INEFFICIENCY AND REWARD BIAS IN ADVERSARIAL IMITATION LEARNING",
    "abstract": "We identify two issues with the family of algorithms based on the Adversarial Imitation Learning framework. The first problem is implicit bias present in the reward functions used in these algorithms. While these biases might work well for some environments, they can also lead to sub-optimal behavior in others. Secondly, even though these algorithms can learn from few expert demonstrations, they require a prohibitively large number of interactions with the environment in order to imitate the expert for many real-world applications. In order to address these issues, we propose a new algorithm called Discriminator-Actor-Critic that uses off-policy Reinforcement Learning to reduce policy-environment interaction sample complexity by an average factor of 10. Furthermore, since our reward function is designed to be unbiased, we can apply our algorithm to many problems without making any task-specific adjustments.",
    "prev": "This realization is at the heart of imitation learning (Ho & Ermon, 2016;Pomerleau, 1989), in which one aims to learn a behavior policy from a set of expert demonstrations -logged experience data of a near-optimal policy interacting with the environment -without explicit knowledge of rewards.",
    "curr": "Distribution matching via adversarial learning, or Adversarial Imitation Learning (AIL), has recently become a popular approach for imitation learning (Ho & Ermon, 2016;Fu et al., 2017;Ke et al., 2019;Kostrikov et al., 2019).",
    "next": "These methods interpret the states and actions provided in the expert demonstrations as a finite sample from a target distribution."
  },
  {
    "index": 328,
    "source_corpus_id": 258999368,
    "ref_id": "b12",
    "citation_corpus_id": 6628106,
    "start": 25083,
    "end": 25102,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "(2020).",
    "curr": "All models were trained for 200 epochs using Adam (Kingma & Ba, 2015) with the standard hyperparameters of β 1 = 0.9, β 2 = 0.999, a learning rate of 0.001, and a batch size of 500 on a single A5000 GPU using PyTorch (Paszke et al., 2019).",
    "next": "We did not use any additional training heuristics such as Dropout (Srivastava et al., 2014)."
  },
  {
    "index": 329,
    "source_corpus_id": 248239812,
    "ref_id": "b18",
    "citation_corpus_id": 233714270,
    "start": 28337,
    "end": 28354,
    "title": "Published as a conference paper at ICLR 2021 LATENT SKILL PLANNING FOR EXPLORATION AND TRANSFER",
    "abstract": "To quickly solve new tasks in complex environments, intelligent agents need to build up reusable knowledge. For example, a learned world model captures knowledge about the environment that applies to new tasks. Similarly, skills capture general behaviors that can apply to new tasks. In this paper, we investigate how these two approaches can be integrated into a single reinforcement learning agent. Specifically, we leverage the idea of partial amortization for fast adaptation at test time. For this, actions are produced by a policy that is learned over time while the skills it conditions on are chosen using online planning. We demonstrate the benefits of our design decisions across a suite of challenging locomotion tasks and demonstrate improved sample efficiency in single tasks as well as in transfer from one task to another, as compared to competitive baselines. Videos are available at: https://sites.google.com/view/latent-skill-planning/ * Kevin and Homanga contributed equally to this work.arXiv:2011.13897v2 [cs.LG] 2 May 2021Published as a conference paper at ICLR 2021 search procedure such as Cross-Entropy Method (CEM; Rubinstein, 1997) on the learned models to determine which action to execute next. Since rollouts from the learned dynamics and reward models are not executed in the actual environment during training, these learned models are sometimes also referred to as imagination models(Hafner et al., 2018;. Fully amortized methods such as Dreamer(Hafner et al., 2019), train a reactive policy with many rollouts from the imagination model. They then execute the resulting policy in the environment.",
    "prev": "RELATED WORKS\n\nVisual model-based RL.",
    "curr": "Recent developments in video prediction and contrastive learning have enabled learning of world-models from images (Watter et al., 2015;Babaeizadeh et al., 2017;Hafner et al., 2019a;Ha & Schmidhuber, 2018;Hafner et al., 2019b;Xie et al., 2020).",
    "next": "All of these approaches learn latent representations through reconstruction objectives that are amenable for planning."
  },
  {
    "index": 330,
    "source_corpus_id": 219721308,
    "ref_id": "b13",
    "citation_corpus_id": 15329665,
    "start": 9319,
    "end": 9323,
    "title": "Discovering Diverse and Salient Threads in Document Collections",
    "abstract": "We propose a novel probabilistic technique for modeling and extracting salient structure from large document collections. As in clustering and topic modeling, our goal is to provide an organizing perspective into otherwise overwhelming amounts of information. We are particularly interested in revealing and exploiting relationships between documents. To this end, we focus on extracting diverse sets of threads-singlylinked, coherent chains of important documents. To illustrate, we extract research threads from citation graphs and construct timelines from news articles. Our method is highly scalable, running on a corpus of over 30 million words in about four minutes, more than 75 times faster than a dynamic topic model. Finally, the results from our model more closely resemble human news summaries according to several metrics and are also preferred by human judges.",
    "prev": "As in prior DPP work, our decomposition has inner dimension K that could be as large as M , but is usually much smaller in practice.",
    "curr": "Our algorithms work well for modest values of K. In cases where the natural K is larger (e.g., natural language processing applications where K might be the number of words), random projections can often be used to significantly reduce K [14].",
    "next": "New kernel decomposition and scalable learning\n\nPrior work on NDPPs proposed a maximum likelihood estimation (MLE) learning algorithm [13]."
  },
  {
    "index": 331,
    "source_corpus_id": 204734215,
    "ref_id": "b4",
    "citation_corpus_id": 52889459,
    "start": 20843,
    "end": 20862,
    "title": "LARGE SCALE GAN TRAINING FOR HIGH FIDELITY NATURAL IMAGE SYNTHESIS",
    "abstract": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by truncating the latent space. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128×128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.3 and Fréchet Inception Distance (FID) of 9.6, improving over the previous best IS of 52.52 and FID of 18.65.",
    "prev": "(2019) attempt to solve the undesirable convergence issue of GDA by exploiting curvature information, but they focus on simultaneous game on finding local Nash and it is unclear how to extend their algorithm to sequential games.",
    "curr": "For GAN training, there is a rich literature on different strategies to make the GAN-game welldefined, e.g., by adding instance noise (Salimans et al., 2016), by using different objectives (Nowozin et al., 2016;Gulrajani et al., 2017;Mao et al., 2017) or by tweaking the architectures (Radford et al., 2015;Brock et al., 2019).",
    "next": "While these strategies try to make the overall optimization problem easily, our work deals with a specific optimization problem and convergence issues arise in theory and in practice; hence our algorithm is orthogonal to these work."
  },
  {
    "index": 334,
    "source_corpus_id": 234790212,
    "ref_id": "b26",
    "citation_corpus_id": 1957433,
    "start": 25519,
    "end": 25544,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "Scene Prior (SP) (Yang et al., 2018) learns a graph neural network from the FastText database (Joulin et al., 2016) and leverages the scene prior knowledge and category relationships for navigation.",
    "curr": "Word Embedding (WE) uses GloVe embedding (Pennington et al., 2014) to indicate the target category rather than detection.",
    "next": "The association between object appearances and GloVe embeddings is learned through trail and error."
  },
  {
    "index": 335,
    "source_corpus_id": 236965836,
    "ref_id": "b7",
    "citation_corpus_id": 28202810,
    "start": 25760,
    "end": 25783,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "SQIL (Reddy et al., 2020) denotes the SQIL heuristic.",
    "curr": "In order to ensure a fair comparison among algorithms, we re-implemented all of them using the same basis RL algorithm, SAC (Haarnoja et al., 2018).",
    "next": "All imitation learning agents were run for 500000 environment interactions."
  },
  {
    "index": 336,
    "source_corpus_id": 247518687,
    "ref_id": "b3",
    "citation_corpus_id": 231592851,
    "start": 3189,
    "end": 3215,
    "title": "Published as a conference paper at ICLR 2021 ESTIMATING AND EVALUATING REGRESSION PREDIC- TIVE UNCERTAINTY IN DEEP OBJECT DETECTORS",
    "abstract": "Predictive uncertainty estimation is an essential next step for the reliable deployment of deep object detectors in safety-critical tasks. In this work, we focus on estimating predictive distributions for bounding box regression output with variance networks. We show that in the context of object detection, training variance networks with negative log likelihood (NLL) can lead to high entropy predictive distributions regardless of the correctness of the output mean. We propose to use the energy score as a non-local proper scoring rule and find that when used for training, the energy score leads to better calibrated and lower entropy predictive distributions than NLL. We also address the widespread use of non-proper scoring metrics for evaluating predictive distributions from deep object detectors by proposing an alternate evaluation approach founded on proper scoring rules. Using the proposed evaluation tools, we show that although variance networks can be used to produce high quality predictive distributions, adhoc approaches used by seminal object detectors for choosing regression targets during training do not provide wide enough data support for reliable variance learning. We hope that our work helps shift evaluation in probabilistic object detection to better align with predictive uncertainty evaluation in other machine learning domains. Code for all models, evaluation, and datasets is available at: https://github.com/asharakeh/probdet.git. distance-sensitive proper scoring rule based on energy statistics(Székely & Rizzo, 2013), as an alternative for training variance networks. We show that predictive distributions learnt with the energy score are lower entropy, better calibrated, and of higher quality when evaluated using proper scoring rules.Pitfalls of EvaluationWe address the widespread use of non-proper scoring rules for evaluating probabilistic object detectors by providing evaluation tools based on well established proper scoring rules(Gneiting & Raftery, 2007)that are only minimized if the estimated predictive distribution is equal to the true target distribution, for both classification and regression. Using the proposed tools, we benchmark probabilistic extensions of three common object detection architectures on in-distribution, shifted, and out-of-distribution data. Our results show that variance networks can differentiate between in-distribution, shifted, and out-of-distribution data using their predictive entropy. We find that ad-hoc approaches used by seminal object detectors for choosing their regression targets during training do not provide a wide enough data support for reliable learning in variance networks. Finally, we provide clear recommendations in Sec. 5 to avoid the pitfalls described above.",
    "prev": "On the other hand, quantifying aleatoric uncertainty enables learning of dynamics models of stochastic processes (e.g.",
    "curr": "for model-based or offline reinforcement learning) (Chua et al., 2018;Yu et al., 2020), improves performance in semantic segmentation, depth regression and object detection (Kendall & Gal, 2017;Harakeh & Waslander, 2021), and allows for risk-sensitive decision making (Dabney et al., 2018;Vlastelica et al., 2021).",
    "next": "We examine a common approach for quantifying aleatoric uncertainty in neural network regression."
  },
  {
    "index": 337,
    "source_corpus_id": 247594344,
    "ref_id": "b8",
    "citation_corpus_id": 208309981,
    "start": 13058,
    "end": 13076,
    "title": "Published as a conference paper at ICLR 2021 HOW MUCH OVER-PARAMETERIZATION IS SUFFI- CIENT TO LEARN DEEP RELU NETWORKS?",
    "abstract": "A recent line of research on deep learning focuses on the extremely overparameterized setting, and shows that when the network width is larger than a high degree polynomial of the training sample size n and the inverse of the target error ´1 , deep neural networks learned by (stochastic) gradient descent enjoy nice optimization and generalization guarantees. Very recently, it is shown that under certain margin assumptions on the training data, a polylogarithmic width condition suffices for two-layer ReLU networks to converge and generalize (Ji and Telgarsky, 2020). However, whether deep neural networks can be learned with such a mild over-parameterization is still an open question. In this work, we answer this question affirmatively and establish sharper learning guarantees for deep ReLU networks trained by (stochastic) gradient descent. In specific, under certain assumptions made in previous work, our optimization and generalization guarantees hold with network width polylogarithmic in n and ´1 . Our results push the study of over-parameterized deep neural networks towards more practical settings. * Equal contribution.",
    "prev": "REGRET ANALYSIS\n\nIn this section, we provide finite time regret analysis of NPR, where the time horizon T is set beforehand.",
    "curr": "The theoretical analysis is built on the recent studies about the generalization of DNN models Chen et al., 2020;Daniely, 2017;Arora et al., 2019), which illustrate that with (stochastic) gradient descent, the learned parameters of a DNN locate in a particular regime with the generalization error being characterized by the best function in the corresponding neural tangent kernel (NTK) space (Jacot et al., 2018).",
    "next": "We leave the background of NTK in the appendix and focus on the key steps of our proof in this section."
  },
  {
    "index": 339,
    "source_corpus_id": 5034059,
    "ref_id": "b27",
    "citation_corpus_id": 3626819,
    "start": 3984,
    "end": 4005,
    "title": "Deep contextualized word representations",
    "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
    "prev": "We find that unified multi-task trained models slightly outperform comparable models trained on each task separately.",
    "curr": "Our best multitask model makes use of ELMo (Peters et al., 2018), a recently proposed pre-training technique.",
    "next": "However, this model still achieves a fairly low absolute score, indicating room for improved general NLU systems."
  },
  {
    "index": 341,
    "source_corpus_id": 233204603,
    "ref_id": "b37",
    "citation_corpus_id": 224705241,
    "start": 19699,
    "end": 19703,
    "title": "PSEUDOSEG: DESIGNING PSEUDO LABELS FOR SEMANTIC SEGMENTATION",
    "abstract": "Recent advances in semi-supervised learning (SSL) demonstrate that a combination of consistency regularization and pseudo-labeling can effectively improve image classification accuracy in the low-data regime. Compared to classification, semantic segmentation tasks require much more intensive labeling costs. Thus, these tasks greatly benefit from data-efficient training methods. However, structured outputs in segmentation render particular difficulties (e.g., designing pseudo-labeling and augmentation) to apply existing SSL strategies. To address this problem, we present a simple and novel re-design of pseudo-labeling to generate well-calibrated structured pseudo labels for training with unlabeled or weakly-labeled data. Our proposed pseudo-labeling strategy is network structure agnostic to apply in a one-stage consistency training framework. We demonstrate the effectiveness of the proposed pseudo-labeling strategy in both low-data and high-data regimes. Extensive experiments have validated that pseudo labels generated from wisely fusing diverse sources and strong data augmentation are crucial to consistency training for segmentation. The source code is available at https://github.com/googleinterns/wss.",
    "prev": "In fully supervised learning, we simply applied ReCo on top of standard supervised learning.",
    "curr": "[38].",
    "next": "The percentage and the number of labelled data used are listed in the first row."
  },
  {
    "index": 342,
    "source_corpus_id": 208202124,
    "ref_id": "b9",
    "citation_corpus_id": 12730344,
    "start": 2902,
    "end": 2927,
    "title": "An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks",
    "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models \"forget\" how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting. We find that it is always best to train using the dropout algorithmthe dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoff curve between these two extremes. We find that different tasks and relationships between tasks result in very different rankings of activation function performance. This suggests that the choice of activation function should always be cross-validated.",
    "prev": "Excessive adaptation could lead to inadvertent forgetting of how to perform earlier tasks.",
    "curr": "Indeed, catastrophic forgetting is one of the main pathologies in continual learning (McCloskey and Cohen, 1989;Ratcliff, 1990;Robins, 1993Robins, , 1995French, 1999;Pape et al., 2011;Goodfellow et al., 2014a;Achille et al., 2018;Kemker et al., 2018;Kemker and Kanan, 2018;Diaz-Rodriguez et al., 2018;Zeno et al., 2018;Ahn et al., 2019;Parisi et al., 2019;Pfulb and Gepperth, 2019;Rajasegaran et al., 2019).",
    "next": "Many approaches to continual learning employ an architecture which is divided a priori into (i) a slowly evolving, global part; and (ii) a quickly evolving, task-specific, local part."
  },
  {
    "index": 343,
    "source_corpus_id": 257365037,
    "ref_id": "b21",
    "citation_corpus_id": 3366315,
    "start": 3577,
    "end": 3598,
    "title": "Published as a conference paper at ICLR 2018 SPECTRAL NORMALIZATION FOR GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. The code with Chainer (Tokui et al., 2015), generated images and pretrained models are available at https://github.com/pfnet-research/sngan_ projection.Published as a conference paper at ICLR 2018• Lipschitz constant is the only hyper-parameter to be tuned, and the algorithm does not require intensive tuning of the only hyper-parameter for satisfactory performance. • Implementation is simple and the additional computational cost is small.",
    "prev": " in our original ICLR paper are not accurate.We elaborate on the issue and provides some fix in Appendix D.\n\nINTRODUCTION\n\nThe robustness of deep neural networks is nowadays a great challenge to establish confidence in their decisions for real-life applications.Addressing this challenge requires guarantees on the stability of the prediction, with respect to adversarial attacks.In this context, the Lipschitz constant of neural networks is a key property at the core of many recent advances.Along with the margin of the classifier, this property allows us to certify the robustness against worst-case adversarial perturbations.This certification is based on a sphere of stability within which the decision remains the same for any perturbation inside the sphere (Tsuzuku et al., 2018).",
    "curr": "The design of 1-Lipschitz layers provides a successful approach to enforce this property for the whole neural network.For this purpose, many different techniques have been devised such as spectral normalization (Miyato et al., 2018;Farnia et al., 2019), orthogonal parameterization (Trockman et al., 2021;Li et al., 2019;Singla et al., 2021;Yu et al., 2022;Xu et al., 2022), Convex Potential Layers (CPL) (Meunier et al., 2022), and Almost-Orthogonal-Layers (AOL) (Prach et al., 2022).While all these techniques share the same goal, their motivations, and derivations can greatly differ, delivering different solutions.Nevertheless, their raw experimental comparison fails to really gain insight into their peculiar performance, soundness, and in the end their possible complementarity.Therefore a question acts as a barrier for an in-depth analysis and future development:\n\nAre there common principles underlying the developments of 1-Lipschitz Layers?",
    "next": "In this paper, we propose a novel perspective to answer this question based on a unified Semidefinite Programming (SDP) approach.We introduce a common algebraic condition underlying various types of methods like spectral normalization, orthogonality-based methods, AOL, and CPL"
  },
  {
    "index": 345,
    "source_corpus_id": 238354021,
    "ref_id": "b23",
    "citation_corpus_id": 12663716,
    "start": 7909,
    "end": 7932,
    "title": "PIXELCNN++: IMPROVING THE PIXELCNN WITH DISCRETIZED LOGISTIC MIXTURE LIKELIHOOD AND OTHER MODIFICATIONS",
    "abstract": "PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.",
    "prev": "Order Agnostic ARDMs The main difficulty of parameterizing an autoregressive model from an engineering perspective, is the need to enforce the triangular or causal dependence.",
    "curr": "Especially for 2D signals, this triangular dependence is difficult to enforce for arbitrary orders (Jain et al., 2020) and tedious design is needed for multi-scale architectures (Salimans et al., 2017).",
    "next": "To relax this requirement, we take inspiration from modern diffusion-based generative models."
  },
  {
    "index": 346,
    "source_corpus_id": 218720006,
    "ref_id": "b10",
    "citation_corpus_id": 28202810,
    "start": 5107,
    "end": 5130,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "We then show that if we define the trust-region w.r.t.",
    "curr": "the uniform policy, instead of the old one, our off-policy MDPO coincides with the popular soft actor-critic (SAC) algorithm (Haarnoja et al., 2018).",
    "next": "We discuss this connection in detail (Section 4.2) and empirically compare these algorithms using the same set of continuous control problems (Section 5.4)."
  },
  {
    "index": 347,
    "source_corpus_id": 247618912,
    "ref_id": "b13",
    "citation_corpus_id": 3633127,
    "start": 67142,
    "end": 67164,
    "title": "Published as a conference paper at ICLR 2018 CGANS WITH PROJECTION DISCRIMINATOR",
    "abstract": "We propose a novel, projection based way to incorporate the conditional information into the discriminator of GANs that respects the role of the conditional information in the underlining probabilistic model. This approach is in contrast with most frameworks of conditional GANs used in application today, which use the conditional information by concatenating the (embedded) conditional vector to the feature vectors. With this modification, we were able to significantly improve the quality of the class conditional image generation on ILSVRC2012 (Im-ageNet) 1000-class image dataset from the current state-of-the-art result, and we achieved this with a single pair of a discriminator and a generator. We were also able to extend the application to super-resolution and succeeded in producing highly discriminative super-resolution images. This new structure also enabled high quality category transformation based on parametric functional transformation of conditional batch normalization layers in the generator. The code with Chainer (Tokui et al., 2015), generated images and pretrained models are available at https://github.com/pfnet-research/sngan_projection.",
    "prev": "A potential downside to modeling latent factors in generative models is a decrease in image quality of generated samples that has been noted when disentanglement terms are added(Burgess et al., 2018; Khrulkov et al., 2021).",
    "curr": "Prior work has studied how to integrate additional information into GAN training, in particular ground truth class labels(Mirza & Osindero, 2014;Salimans et al., 2016;Odena, 2016;Odena et al., 2017;  Brock et al., 2019;Thekumparampil et al., 2018;Miyato & Koyama, 2018; Lučić et al., 2019), also considering noisy scenarios(Kaneko et al., 2019).",
    "next": "However, in the programmatic weak supervision setting, having multiple noisy sources of imperfect labels that include abstains present large hurdles to similar conditional modeling."
  },
  {
    "index": 348,
    "source_corpus_id": 238857286,
    "ref_id": "b25",
    "citation_corpus_id": 14124313,
    "start": 13094,
    "end": 13122,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "The pixel-wise loss L pixel combines 1 and 2 distance as:\nL pixel (p,p) = λ 1 1 (p,p) + λ 2 2 (p,p),(9)\nwhere λ 1 and λ 2 are two hyper-parameters to control the relative importance.",
    "curr": "For the perceptual loss L perceptual , we extract features from conv5 in a VGG-16 network (Simonyan & Zisserman, 2015) pretrained on ImageNet (Krizhevsky et al., 2012) and combine the 1 and 2 distance as:\nL perceptual (p,p) = λ 3 1 (φ(p), φ(p)) + λ 4 2 (φ(p), φ(p)),(10)\nwhere φ(·) represents the output of conv5 in the VGG-16 network, and λ 3 and λ 4 are two hyperparameters.",
    "next": "Compared to the pixel-wise loss, the perceptual loss is better to capture the region-wise structure, which reflects the waveform coherence."
  },
  {
    "index": 349,
    "source_corpus_id": 259287063,
    "ref_id": "b46",
    "citation_corpus_id": 213729382,
    "start": 2381,
    "end": 2398,
    "title": "Published as a conference paper at ICLR 2020 DYNAMIC MODEL PRUNING WITH FEEDBACK",
    "abstract": "Deep neural networks often have millions of parameters. This can hinder their deployment to low-end devices, not only due to high memory requirements but also because of increased latency at inference. We propose a novel model compression method that generates a sparse trained model without additional overhead: by allowing (i) dynamic allocation of the sparsity pattern and (ii) incorporating feedback signal to reactivate prematurely pruned weights we obtain a performant sparse model in one single training pass (retraining is not needed, but can further improve the performance). We evaluate our method on CIFAR-10 and ImageNet, and show that the obtained sparse models can reach the state-of-the-art performance of dense models. Moreover, their performance surpasses that of models generated by all previously proposed pruning schemes. arXiv:2006.07253v1 [cs.LG] 12 Jun 2020 2 RELATED WORK Previous works on obtaining pruned networks can (loosely) be divided into three main categories.Pruning after training. Training approaches to obtain sparse networks usually include a three stage pipeline-training of a dense model, one-shot pruning and fine-tuning-e.g., (Han et al.",
    "prev": "weight decay yields models suitable for averaging, sharing identical sparse connectivity by design.Averaging these models significantly enhances generalization and OOD performance over their individual counterparts.Building on this, we introduce SPARSE MODEL SOUPS (SMS), a novel method for merging sparse models by initiating each prune-retrain cycle with the averaged model from the previous phase.SMS preserves sparsity, exploits sparse network benefits, is modular and fully parallelizable, and substantially improves IMP's performance.We further demonstrate that SMS can be adapted to enhance state-of-the-art pruning-during-training approaches.",
    "curr": "INTRODUCTION\n\nState-of-the-art Neural Network architectures typically rely on extensive over-parameterization with millions or billions of parameters (Zhang et al., 2016).In consequence, these models have significant memory requirements and the training and inference process is computationally demanding.However, recent work (e.g.Han et al., 2015;Lin et al., 2020;Renda et al., 2020;Zimmer et al., 2022) has demonstrated that these resource demands can be significantly reduced by pruning the model, i.e., removing redundant structures such as individual parameters or groups thereof.The resulting sparse models demand considerably less storage and floating-point operations (FLOPs) during inference, while retaining performance comparable to dense models.",
    "next": "A different line of research has shown that the performance of a predictor can be significantly enhanced by leveraging multiple models, instead of selecting the best one on a hold-out validation dataset and discarding the rest.Such ensembles combine the predictions of m ∈ N individually trained models by averaging their output predictions (Ganaie et al., 2021;Mehrtash et al., 2020;Chandak et al., 2023;Fort et al., 2019).Prediction ensembles have been shown to improve the predictive performance and positively impact predictive uncertainty metrics such as calibration, out-of-distribution generalizatio"
  },
  {
    "index": 350,
    "source_corpus_id": 249097923,
    "ref_id": "b16",
    "citation_corpus_id": 3526769,
    "start": 6705,
    "end": 6724,
    "title": "MITIGATING ADVERSARIAL EFFECTS THROUGH RAN- DOMIZATION",
    "abstract": "Convolutional neural networks have demonstrated their powerful ability on various tasks in recent years. However, they are extremely vulnerable to adversarial examples. I.e., clean images, with imperceptible perturbations added, can easily cause convolutional neural networks to fail. In this paper, we propose to utilize randomization to mitigate adversarial effects. Specifically, we use two randomization operations: random resizing, which resizes the input images to a random size, and random padding, which pads zeros around the input images in a random manner. Extensive experiments demonstrate that the proposed randomization method is very effective at defending against both single-step and iterative attacks. Our method also enjoys the following advantages: 1) no additional training or fine-tuning, 2) very few additional computations, 3) compatible with other adversarial defense methods. By combining the proposed randomization method with an adversarially trained model, it achieves a normalized score of 0.924 (ranked No.2 among 107 defense teams) in the NIPS 2017 adversarial examples defense challenge, which is far better than using adversarial training alone with a normalized score of 0.773 (ranked No.56). The code is public available at https: /. R-fcn: Object detection via region-based fully convolutional networks.",
    "prev": "In addition to the attention terms, AoA and ATA also include the typical attack losses, e.g., logit output in their objective functions, and use a hyperparameter to balance the two terms.",
    "curr": "Adversarial Perturbations (TAP) (Zhou et al., 2018), Activation attack (AA) (Inkawhich et al., 2019) and Intermediate Level Attack (ILA) (Huang et al., 2019), which all directly maximize the distance between feature maps of benign images and adversarial examples, also belong to this category.",
    "next": "TAP and AA generate adversarial examples by employing multi-layer and single-layer feature maps respectively."
  },
  {
    "index": 351,
    "source_corpus_id": 252668422,
    "ref_id": "b52",
    "citation_corpus_id": 244954585,
    "start": 2842,
    "end": 2868,
    "title": "REPRESENTATIONS OF THE HIPPOCAMPAL FORMATION",
    "abstract": "Many deep neural network architectures loosely based on brain networks have recently been shown to replicate neural firing patterns observed in the brain.One of the most exciting and promising novel architectures, the Transformer neural network, was developed without the brain in mind.In this work, we show that transformers, when equipped with recurrent position encodings, replicate the precisely tuned spatial representations of the hippocampal formation; most notably place and grid cells.Furthermore, we show that this result is no surprise since it is closely related to current hippocampal models from neuroscience.We additionally show the transformer version offers dramatic performance gains over the neuroscience version.This work continues to bind computations of artificial and brain networks, offers a novel understanding of the hippocampal-cortical interaction, and suggests how wider cortical areas may perform complex tasks beyond current neuroscience models such as language comprehension.",
    "prev": "Transformer, originally designed for natural language processing (Vaswani et al., 2017), has flourished for various tasks in computer vision, including image classification (Dosovitskiy et al., 2020;Yuan et al., 2021a), object detection (Carion et al., 2020;Zhu et al., 2020;, semantic segmentation Yuan et al., 2021b) and low-level image processing (Chen et al., 2021).",
    "curr": "Self-attention, the key part of Transformer, selectively focuses on information of interest, and is also an important feature of the human biological system (Whittington et al., 2022;Caucheteux & King, 2022).",
    "next": "Intuitively, it is intriguing to explore applying self-attention in SNNs for more advanced deep learning, considering the biological properties of the two mechanisms."
  },
  {
    "index": 355,
    "source_corpus_id": 3525232,
    "ref_id": "b13",
    "citation_corpus_id": 12639289,
    "start": 2536,
    "end": 2566,
    "title": "Recurrent Continuous Translation Models",
    "abstract": "We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.",
    "prev": "Neural network architectures lie at the heart of a variety of applications.",
    "curr": "They are practically ubiquitous across vision tasks (LeCun et al., 1995;Krizhevsky et al., 2012;Simonyan & Zisserman, 2014) and natural language understanding, from machine translation (Kalchbrenner & Blunsom, 2013;Sutskever et al., 2014;Bahdanau et al., 2014) to textual entailment (Bowman et al., 2015;Rocktäschel et al., 2015) via sentiment analysis (Socher et al., 2013;Kalchbrenner et al., 2014) and reading comprehension Hill et al., 2015;Rajpurkar et al., 2016).",
    "next": "They have been used to synthesise programs (Ling et al., 2016;Parisotto et al., 2016;Devlin et al., 2017) or internalise algorithms (Graves et al., 2016;Joulin & Mikolov, 2015;Kaiser & Sutskever, 2015;Reed & De Freitas, 2015)."
  },
  {
    "index": 357,
    "source_corpus_id": 220280819,
    "ref_id": "b24",
    "citation_corpus_id": 14717992,
    "start": 1630,
    "end": 1633,
    "title": "REINFORCEMENT LEARNING WITH UNSUPERVISED AUXILIARY TASKS",
    "abstract": "Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-theart on Atari, averaging 880% expert human performance, and a challenging suite of first-person, three-dimensional Labyrinth tasks leading to a mean speedup in learning of 10× and averaging 87% expert human performance on Labyrinth.Natural and artificial agents live in a stream of sensorimotor data. At each time step t, the agent receives observations o t and executes actions a t . These actions influence the future course of the sensorimotor stream. In this paper we develop agents that learn to predict and control this stream, by solving a host of reinforcement learning problems, each focusing on a distinct feature of the sensorimotor stream. Our hypothesis is that an agent that can flexibly control its future experiences will also be able to achieve any goal with which it is presented, such as maximising its future rewards.The classic reinforcement learning paradigm focuses on the maximisation of extrinsic reward. However, in many interesting domains, extrinsic rewards are only rarely observed. This raises questions of what and how to learn in their absence. Even if extrinsic rewards are frequent, the sensorimotor stream contains an abundance of other possible learning targets. Traditionally, unsupervised learning attempts to reconstruct these targets, such as the pixels in the current or subsequent frame. It is typically used to accelerate the acquisition of a useful representation. In contrast, our learning objective is to predict and control features of the sensorimotor stream, by treating them as pseudorewards for reinforcement learning. Intuitively, this set of tasks is more closely matched with the agent's long-term goals, potentially leading to more useful representations.Consider a baby that learns to maximise the cumulative amount of red that it observes. To correctly predict the optimal value, the baby must understand how to increase \"redness\" by various means, including manipulation (bringing a red object closer to the eyes); locomotion (moving in front of a red object); and communication (crying until the parents bring a red object). These behaviours are likely to recur for many other goals that the baby may subsequently encounter. No understanding of these behaviours is required to simply reconstruct the redness of current or subsequent images.Our architecture uses reinforcement learning to approximate both the optimal policy and optimal value function for many different pseudo-rewards. It also makes other auxiliary predictions that serve to focus the agent on important aspects of the task. These include the long-term goal of predicting cumulative extrinsic reward as well as short-term predictions of extrinsic reward. To learn more efficiently, our agents use an experience replay mechanism to provide additional updates * Joint first authors. Ordered alphabetically by first name.",
    "prev": "Introduction\n\nReinforcement learning (RL) provides a general solution for an agent to solve a given task by learning from its own experiences of interacting with the environment.",
    "curr": "In various domains of careful task designs and informative rewards, such as game playing [3,25,37] and robotics [1,11], RL algorithms have achieved significant progress on solving challenging sequential decision making problems.",
    "next": "However, the effectiveness of such a trial-and-error learning paradigm relies on the agent's ability of exploring the task environment and collecting informative experiences."
  },
  {
    "index": 359,
    "source_corpus_id": 3529936,
    "ref_id": "b28",
    "citation_corpus_id": 11758569,
    "start": 2445,
    "end": 2449,
    "title": "UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks -demonstrating their applicability as general image representations.arXiv:1511.06434v2 [cs.LG] 7 Jan 2016Under review as a conference paper at ICLR 2016• We show that the generators have interesting vector arithmetic properties allowing for easy manipulation of many semantic qualities of generated samples.",
    "prev": "These are omnipresent in scientific and engineering research involving data analysis, for instance ecology, climate science and geography, where simulators are used to fit real-world observations to produce forecasting results.",
    "curr": "Within the machine learning community there is a recent interest in a specific type of implicit models, generative adversarial networks (GANs) [10], which has been shown to be one of the most successful approaches to image and text generation [29,49,2,5].",
    "next": "Very recently, implicit distributions have also been considered as approximate posterior distributions for Bayesian inference, e.g."
  },
  {
    "index": 360,
    "source_corpus_id": 232478335,
    "ref_id": "b28",
    "citation_corpus_id": 67856276,
    "start": 1815,
    "end": 1832,
    "title": "MULTILINGUAL NEURAL MACHINE TRANSLATION WITH KNOWLEDGE DISTILLATION",
    "abstract": "Multilingual machine translation, which translates multiple languages with a single model, has attracted much attention due to its efficiency of offline training and online serving. However, traditional multilingual translation usually yields inferior accuracy compared with the counterpart using individual models for each language pair, due to language diversity and model capacity limitations. In this paper, we propose a distillation-based approach to boost the accuracy of multilingual machine translation. Specifically, individual models are first trained and regarded as teachers, and then the multilingual model is trained to fit the training data and match the outputs of individual models simultaneously through knowledge distillation. Experiments on IWSLT, WMT and Ted talk translation datasets demonstrate the effectiveness of our method. Particularly, we show that one model is enough to handle multiple languages (up to 44 languages in our experiment), with comparable or even better accuracy than individual models. * Authors contribute equally to this work.",
    "prev": "Finally, we broadly discuss several circumstances wherein label smoothing will indeed lose its effectiveness.",
    "curr": "INTRODUCTION\n\nLabel smoothing (Szegedy et al., 2016) and knowledge distillation (Hinton et al., 2015) are two commonly recognized techniques in training deep neural networks and have been applied in many state-of-the-art models, such as language translation (Vaswani et al., 2017;Tan et al., 2019;Zhou et al., 2020), image classification  and speech recognition (Chiu et al., 2018;Pereyra et al., 2017;Chorowski & Jaitly, 2017).",
    "next": "Recently a large body of studies is focusing on exploring the underlying relationships between these two methods, for instance, Müller et al."
  },
  {
    "index": 362,
    "source_corpus_id": 208857488,
    "ref_id": "b45",
    "citation_corpus_id": 3531730,
    "start": 17563,
    "end": 17588,
    "title": "A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks",
    "abstract": "We present a generalization bound for feedforward neural networks in terms of the product of the spectral norms of the layers and the Frobenius norm of the weights. The generalization bound is derived using a PAC-Bayes analysis.",
    "prev": "Experiments\n\n\nOverparamterized LQR\n\nWe first analyze the case of the LQR as a surrogate for what may occur in deep RL, which has been done before for various topics such as sample complexity (Dean et al., 2019) and model-based RL .",
    "curr": "This is analogous to analyzing linear/logistic regression (Kakade et al., 2008;McAllester, 2003) as a surrogate to understanding extensions to deep SL techniques (Neyshabur et al., 2018a;Bartlett et al., 2017).",
    "next": "In particular, this has numerous benefits -the cost (negative of reward) function is deterministic, and allows exact gradient descent (i.e."
  },
  {
    "index": 364,
    "source_corpus_id": 245117682,
    "ref_id": "b22",
    "citation_corpus_id": 28202810,
    "start": 2632,
    "end": 2655,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "INTRODUCTION\n\nReinforcement learning (RL) has suffered for years from a curse of poor sample complexity.",
    "curr": "State-ofthe-art model-free reinforcement learning algorithms routinely take tens of thousands of sampled transitions to solve very simple tasks and millions to solve moderately complex ones (Haarnoja et al., 2018;Lillicrap et al., 2015).",
    "next": "The current best model-based reinforcement learning (MBRL) algorithms are better, requiring thousands of samples for simple problems and hundreds of thousands of samples for harder ones (Chua et al., 2018)."
  },
  {
    "index": 365,
    "source_corpus_id": 256390058,
    "ref_id": "b10",
    "citation_corpus_id": 3484654,
    "start": 21628,
    "end": 21647,
    "title": "RECASTING GRADIENT-BASED META-LEARNING AS HIERARCHICAL BAYES",
    "abstract": "Meta-learning allows an intelligent agent to leverage prior learning episodes as a basis for quickly improving performance on a novel task. Bayesian hierarchical modeling provides a theoretical framework for formalizing meta-learning as inference for a set of parameters that are shared across tasks. Here, we reformulate the model-agnostic meta-learning algorithm (MAML) of Finn et al.(2017)as a method for probabilistic inference in a hierarchical Bayesian model. In contrast to prior methods for meta-learning via hierarchical Bayes, MAML is naturally applicable to complex function approximators through its use of a scalable gradient descent procedure for posterior inference. Furthermore, the identification of MAML as hierarchical Bayes provides a way to understand the algorithm's operation as a meta-learning procedure, as well as an opportunity to make use of computational strategies for efficient inference. We use this opportunity to propose an improvement to the MAML algorithm that makes use of techniques from approximate inference and curvature estimation.",
    "prev": "Meta learning is a learning framework that aims to adapt or generalize well on new tasks.",
    "curr": "There are three approaches in meta learning: metric-based (Koch et al., 2015;Vinyals et al., 2016;Sung et al., 2018;Snell et al., 2017), model-based (Santoro et al., 2016Munkhdalai & Yu, 2017;Grant et al., 2018) and optimization-based (Finn et al., 2017;Nichol et al., 2018).",
    "next": "Neural processes (NPs) is the model-based meta learning with stochasticity."
  },
  {
    "index": 366,
    "source_corpus_id": 67856276,
    "ref_id": "b26",
    "citation_corpus_id": 1998416,
    "start": 1685,
    "end": 1705,
    "title": "Effective Approaches to Attention-based Neural Machine Translation",
    "abstract": "An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker. 1",
    "prev": "* Authors contribute equally to this work.",
    "curr": "INTRODUCTION\n\nNeural Machine Translation (NMT) has witnessed rapid development in recent years (Bahdanau et al., 2015;Luong et al., 2015b;Wu et al., 2016;Gehring et al., 2017;Vaswani et al., 2017;Guo et al., 2018;Shen et al., 2018), including advanced model structures (Gehring et al., 2017;Vaswani et al., 2017; and human parity achievements .",
    "next": "While conventional NMT can well handle single pair translation, training a separate model for each language pair is resource consuming, considering there are thousands of languages in the world 1 ."
  },
  {
    "index": 367,
    "source_corpus_id": 235293986,
    "ref_id": "b8",
    "citation_corpus_id": 56657912,
    "start": 26709,
    "end": 26739,
    "title": "BENCHMARKING NEURAL NETWORK ROBUSTNESS TO COMMON CORRUPTIONS AND PERTURBATIONS",
    "abstract": "In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, IMAGENET-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called IMAGENET-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.",
    "prev": "Response to gradual domain shift.",
    "curr": "In order to assess how well models can cope with a gradual transition from their native domain, we evaluate their ECE performance on data perturbed by 19 types of corruption (Hendrycks & Dietterich, 2019) at five different severity levels.",
    "next": "Figure 3 depicts the performance of models averaged across all corruptions."
  },
  {
    "index": 371,
    "source_corpus_id": 235417023,
    "ref_id": "b37",
    "citation_corpus_id": 604334,
    "start": 28490,
    "end": 28511,
    "title": "Intriguing properties of neural networks",
    "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
    "prev": "Before that we review adversarial attacks widely used for evaluating and improving robustness.",
    "curr": "Adversarial attack\n\nSince the realization of the adversarial example phenomenon (Biggio et al., 2013;Szegedy et al., 2014), tons of adversarial attacks have been proposed (Moosavi-Dezfooli et al., 2016;Goodfellow et al., 2014;Carlini & Wagner, 2017;Dong et al., 2018;Tu et al., 2019;Madry et al., 2017).",
    "next": "Among these attacks, PGD attack (Madry et al., 2017) is one of the most commonly used attacks."
  },
  {
    "index": 372,
    "source_corpus_id": 49473438,
    "ref_id": "b25",
    "citation_corpus_id": 11758569,
    "start": 5993,
    "end": 5996,
    "title": "UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks -demonstrating their applicability as general image representations.arXiv:1511.06434v2 [cs.LG] 7 Jan 2016Under review as a conference paper at ICLR 2016• We show that the generators have interesting vector arithmetic properties allowing for easy manipulation of many semantic qualities of generated samples.",
    "prev": "Various ideas have been proposed to further improve the quality of the learned distributions and the stability of the training.",
    "curr": "(See e.g., [1,24,15,26,33,28,16,9,38] and the reference therein.)",
    "next": "However, understanding of GANs is still in its infancy."
  },
  {
    "index": 373,
    "source_corpus_id": 54101493,
    "ref_id": "b1",
    "citation_corpus_id": 68222714,
    "start": 5709,
    "end": 5732,
    "title": "APPROXIMATING CNNS WITH BAG-OF-LOCAL- FEATURES MODELS WORKS SURPRISINGLY WELL ON IMAGENET",
    "abstract": "Deep Neural Networks (DNNs) excel on many complex perceptual tasks but it has proven notoriously difficult to understand how they reach their decisions. We here introduce a high-performance DNN architecture on ImageNet whose decisions are considerably easier to explain. Our model, a simple variant of the ResNet-50 architecture called BagNet, classifies an image based on the occurrences of small local image features without taking into account their spatial ordering. This strategy is closely related to the bag-of-feature (BoF) models popular before the onset of deep learning and reaches a surprisingly high accuracy on ImageNet (87.6% top-5 for 33 × 33 px features and Alexnet performance for 17 × 17 px features). The constraint on local features makes it straight-forward to analyse how exactly each part of the image influences the classification. Furthermore, the BagNets behave similar to state-of-the art deep neural networks such as VGG-16, ResNet-152 or DenseNet-169 in terms of feature sensitivity, error distribution and interactions between image parts. This suggests that the improvements of DNNs over previous bag-of-feature classifiers in the last few years is mostly achieved by better fine-tuning rather than by qualitatively different decision strategies.",
    "prev": "On the other hand, some rather disconnected findings point to an important role of object textures for CNN object recognition.",
    "curr": "CNNs can still classify texturised images perfectly well, even if the global shape structure is completely destroyed Brendel & Bethge, 2019).",
    "next": "Conversely, standard CNNs are bad at recognising object sketches where object shapes are preserved yet all texture cues are missing (Ballester & de Araújo, 2016)."
  },
  {
    "index": 379,
    "source_corpus_id": 236912505,
    "ref_id": "b34",
    "citation_corpus_id": 3075448,
    "start": 2712,
    "end": 2715,
    "title": "HIGH-DIMENSIONAL CONTINUOUS CONTROL USING GENERALIZED ADVANTAGE ESTIMATION",
    "abstract": "Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(λ). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.Published as a conference paper at ICLR 2016 λ ∈ [0, 1], the generalized advantage estimator (GAE). Related methods have been proposed in the context of online actor-critic methods(Kimura & Kobayashi, 1998;Wawrzyński, 2009). We provide a more general analysis, which is applicable in both the online and batch settings, and discuss an interpretation of our method as an instance of reward shaping(Ng et al., 1999), where the approximate value function is used to shape the reward.We present experimental results on a number of highly challenging 3D locomotion tasks, where we show that our approach can learn complex gaits using high-dimensional, general purpose neural network function approximators for both the policy and the value function, each with over 10 4 parameters. The policies perform torque-level control of simulated 3D robots with up to 33 state dimensions and 10 actuators.The contributions of this paper are summarized as follows:1. We provide justification and intuition for an effective variance reduction scheme for policy gradients, which we call generalized advantage estimation (GAE). While the formula has been proposed in prior work(Kimura & Kobayashi, 1998;Wawrzyński, 2009), our analysis is novel and enables GAE to be applied with a more general set of algorithms, including the batch trust-region algorithm we use for our experiments. 2. We propose the use of a trust region optimization method for the value function, which we find is a robust and efficient way to train neural network value functions with thousands of parameters. 3. By combining(1)and(2)above, we obtain an algorithm that empirically is effective at learning neural network policies for challenging control tasks. The results extend the state of the art in using reinforcement learning for high-dimensional continuous control. Videos are available at https://sites.google.com/site/gaepapersupp.",
    "prev": "The value function is learned using experience of the agent acting in the environment via temporal difference (TD) methods [39], which regress the value function to a target based on a combination of groundtruth returns achieved in the environment and the approximate value function itself.",
    "curr": "The need to bootstrap learning of the value function on its own estimates is known to lead to difficulties in practice, where one must achieve a careful balance between bias and variance [15,49,35,23].",
    "next": "If a slight imbalance arises, the consequences can be disastrous for learning [45,46,41]."
  },
  {
    "index": 380,
    "source_corpus_id": 261823054,
    "ref_id": "b54",
    "citation_corpus_id": 204509033,
    "start": 23353,
    "end": 23357,
    "title": "UNDERSTANDING THE LIMITATIONS OF VARIATIONAL MUTUAL INFORMATION ESTIMATORS",
    "abstract": "Variational approaches based on neural networks are showing promise for estimating mutual information (MI) between high dimensional variables. However, they can be difficult to use in practice due to poorly understood bias/variance tradeoffs. We theoretically show that, under some conditions, estimators such as MINE exhibit variance that could grow exponentially with the true amount of underlying MI. We also empirically demonstrate that existing estimators fail to satisfy basic self-consistency properties of MI, such as data processing and additivity under independence. Based on a unified perspective of variational approaches, we develop a new estimator that focuses on variance reduction. Empirical results on standard benchmark tasks demonstrate that our proposed estimator exhibits improved biasvariance trade-offs on standard benchmark tasks.",
    "prev": "Models\n\nWe evaluate representations obtained using correlation maximization methods based either on linear projections (TICA) (43) or non-linear encoders (VAMPNet) (39)  in Appendix E.1.We use a conditional Flow++ architecture (25) to model the variational transition distribution q ϕ (z t |z t−τ ).This is because of the modeling flexibility, the tractability of the likelihood, and the possibility of directly sampling to unfold latent simulations.Multi-layer perceptrons (MLPs) are used to model q ψ (y t |z t ), mapping the representations z t into the logits of a categorical distribution over the target y t .For all objectives, we use the same encoder, transition, and predictive architectures.",
    "curr": "Quantitative evaluation We estimate the autoinformation of the representations AI([z t ] T t=s ; τ ) at several lag time τ using SMILE (54) and we measure the amount of information that the representations contain about the targets of interest I(z t ; y t ) by difference of discrete entropies: H(y t ) − H(y t |z t ) (47; 54; 41).These measurements aim to validate the theory presented in Section 2 and better characterize the representations obtained with the different models.",
    "next": "Following the graphical model in Figure 2, given an initial system state x s of a test trajectory [x t ] T t=s and the sequence of corresponding targets [y t ] T t=s , we use the trained encoder, transition, and prediction models to unfold trajectories [ỹ s+kτ ] K k=1 ∼ q LS ([y s+kτ ] K k=1 |x s ) that cover the same temporal span as the test trajectory (K = ⌊(T − s)/τ ⌋).Similarly to previous work (4), for evaluation purposes, we consider only discrete targets y t so that we can estimate the marginal and transition probabilities for the ground truth and unfolded trajectories by counting the frequency of each state and the corresponding transition m"
  },
  {
    "index": 381,
    "source_corpus_id": 225067567,
    "ref_id": "b23",
    "citation_corpus_id": 40100965,
    "start": 2198,
    "end": 2220,
    "title": "Universal Language Model Fine-tuning for Text Classification",
    "abstract": "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100× more data. We opensource our pretrained models and code 1 .",
    "prev": "2  We focus on encoder-only models, and do not consider encoder-decoder models like T5 (Raffel et al.,  2020)  where none of the embedding matrices are discarded after pre-training.",
    "curr": "Output embeddings may also be\n\nINTRODUCTION\n\nThe performance of models in natural language processing (NLP) has dramatically improved in recent years, mainly driven by advances in transfer learning from large amounts of unlabeled data (Howard & Ruder, 2018;Devlin et al., 2019).",
    "next": "The most successful paradigm consists of pre-training a large Transformer (Vaswani et al., 2017) model with a self-supervised loss and fine-tuning it on data of a downstream task (Ruder et al., 2019)."
  },
  {
    "index": 383,
    "source_corpus_id": 259108558,
    "ref_id": "b35",
    "citation_corpus_id": 174802916,
    "start": 2625,
    "end": 2629,
    "title": "Published as a conference paper at ICLR 2020 SCALING AUTOREGRESSIVE VIDEO MODELS",
    "abstract": "Due to the statistical complexity of video, the high degree of inherent stochasticity, and the sheer amount of data, generating natural video remains a challenging task. State-of-the-art video generation models often attempt to address these issues by combining sometimes complex, usually video-specific neural network architectures, latent variable models, adversarial training and a range of other methods. Despite their often high complexity, these approaches still fall short of generating high quality video continuations outside of narrow domains and often struggle with fidelity. In contrast, we show that conceptually simple autoregressive video generation models based on a three-dimensional self-attention mechanism achieve competitive results across multiple metrics on popular benchmark datasets, for which they produce continuations of high fidelity and realism. We also present results from training our models on Kinetics, a large scale action recognition dataset comprised of YouTube videos exhibiting phenomena such as camera movement, complex object interactions and diverse human movement. While modeling these phenomena consistently remains elusive, we hope that our results, which include occasional realistic continuations encourage further research on comparatively complex, large scale datasets such as Kinetics.Published as a conference paper at ICLR 2020We obtain strong results on popular benchmarks (Section 4.2, Appendix A) and produce high fidelity video continuations on the BAIR robot pushing dataset(Ebert et al., 2017)exhibiting plausible object interactions. Furthermore, our model achieves an almost 50% reduction in perplexity compared to prior work on autoregressive models on another robot pushing dataset.Finally, we apply our models to down-sampled videos from the Kinetics-600 dataset(Section 4.3). While modeling the full range of Kinetics-600 videos still poses a major challenge, we see encouraging video continuations for a more limited subset, namely cooking videos. These feature camera movement, complex object interactions and still cover diverse subjects.We hope that these initial results will encourage future video generation work to evaluate models on more challenging datasets such as Kinetics.",
    "prev": "These include generating the same token or series of token repeatedly, or generating gibberish outputs.",
    "curr": "This phenomenon of degeneration for longer continuations has also been observed in autoregressive models for video [36].",
    "next": "This problem of neural text degeneration has been linked to the training objective for LLMs, which trains a conditional distribution for the next token given a (partial) sentence [10]."
  },
  {
    "index": 384,
    "source_corpus_id": 211003696,
    "ref_id": "b14",
    "citation_corpus_id": 8393918,
    "start": 3806,
    "end": 3822,
    "title": "Published as a conference paper at ICLR 2016 GATED GRAPH SEQUENCE NEURAL NETWORKS",
    "abstract": "Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks(Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be described as abstract data structures.Published as a conference paper at ICLR 2016 to learn features on the graph that encode the partial output sequence that has already been produced (e.g., the path so far if outputting a path) and that still needs to be produced (e.g., the remaining path). We will show how the GNN framework can be adapted to these settings, leading to a novel graph-based neural network model that we call Gated Graph Sequence Neural Networks (GGS-NNs).We illustrate aspects of this general model in experiments on bAbI tasks  and graph algorithm learning tasks that illustrate the capabilities of the model. We then present an application to the verification of computer programs. When attempting to prove properties such as memory safety (i.e., that there are no null pointer dereferences in a program), a core problem is to find mathematical descriptions of the data structures used in a program. FollowingBrockschmidt et al. (2015), we have phrased this as a machine learning problem where we will learn to map from a set of input graphs, representing the state of memory, to a logical description of the data structures that have been instantiated. Whereas Brockschmidt et al.(2015)relied on a large amount of hand-engineering of features, we show that the system can be replaced with a GGS-NN at no cost in accuracy.",
    "prev": "Also, logic rules can only cover a small part of the possible combinations of knowledge graph relations, hence limiting the application of models that are purely based on logic rules.",
    "curr": "Graph neural networks (GNNs) have recently gained increasing popularity for addressing many graph related problems effectively (Dai et al., 2016;Li et al., 2016;Kipf & Welling, 2017;Schlichtkrull et al., 2018).",
    "next": "GNN-based methods typically require sufficient labeled instances on specific end tasks to achieve good performance, however, knowledge graphs have the long-tail nature (Xiong et al., 2018), i.e., a large portion the relations in only are a few triples."
  },
  {
    "index": 386,
    "source_corpus_id": 252967887,
    "ref_id": "b8",
    "citation_corpus_id": 28202810,
    "start": 6326,
    "end": 6349,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "PRELIMINARIES\n\nSoft Actor-Critic.",
    "curr": "In this paper, our approach is built on Soft Actor-Critic (SAC) (Haarnoja et al., 2018).",
    "next": "SAC is a stable off-policy actor-critic algorithm based on the maximum entropy reinforcement learning framework, in which the actor maximizes both the returns and the entropy."
  },
  {
    "index": 387,
    "source_corpus_id": 249538510,
    "ref_id": "b2",
    "citation_corpus_id": 202749904,
    "start": 5114,
    "end": 5137,
    "title": "HIGH FIDELITY SPEECH SYNTHESIS WITH ADVERSARIAL NETWORKS",
    "abstract": "Generative adversarial networks have seen rapid development in recent years and have led to remarkable improvements in generative modelling of images. However, their application in the audio domain has received limited attention, and autoregressive models, such as WaveNet, remain the state of the art in generative modelling of audio signals such as human speech. To address this paucity, we introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech. Our architecture is composed of a conditional feed-forward generator producing raw speech audio, and an ensemble of discriminators which operate on random windows of different sizes. The discriminators analyse the audio both in terms of general realism, as well as how well the audio corresponds to the utterance that should be pronounced. To measure the performance of GAN-TTS, we employ both subjective human evaluation (MOS -Mean Opinion Score), as well as novel quantitative metrics (Fréchet DeepSpeech Distance and Kernel DeepSpeech Distance), which we find to be well correlated with MOS. We show that GAN-TTS is capable of generating high-fidelity speech with naturalness comparable to the state-of-the-art models, and unlike autoregressive models, it is highly parallelisable thanks to an efficient feed-forward generator. Listen to GAN-TTS reading this abstract at https://storage.googleapis.com/ deepmind-media/research/abstract.wav.",
    "prev": "INTRODUCTION\n\nDeep generative models have demonstrated noticeable successes for modeling raw audio.",
    "curr": "The successful methods include, autoregressive models (van den Oord et al., 2016;Mehri et al., 2017;Kalchbrenner et al., 2018), flow-based models (van den Oord et al., 2018;Ping et al., 2019;Prenger et al., 2019;Kim et al., 2019;Ping et al., 2020;Lee et al., 2020), GAN-based models Kumar et al., 2019;Bińkowski et al., 2020;Yamamoto et al., 2020;Kong et al., 2020), and diffusion models (Kong et al., 2021;Chen et al., 2021;Lee et al., 2022).",
    "next": "Among these methods, GAN-based vocoders (e.g., Kong et al., 2020) can generate high-fidelity raw audio conditioned on mel spectrogram, while synthesizing hundreds of times faster than real-time on a single GPU."
  },
  {
    "index": 388,
    "source_corpus_id": 258331519,
    "ref_id": "b12",
    "citation_corpus_id": 28202810,
    "start": 12124,
    "end": 12147,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "We do not include HalfCheetah-v3 since the simulator does not terminate even when the robot has already fallen over, so there is no standard way to determine catastrophic failures.",
    "curr": "We first select three popular algorithms, including Soft Actor-Critic (SAC) (Haarnoja et al., 2018), Twin Delayed DDPG Table 1: Failure rates (%) of relay-evaluation using states generated by stranger agents trained with 4 algorithms, reported in the 4 rows for each environment.",
    "next": "The \"Reference\" column shows the failure rate of the stranger agents, serving as the baseline failure rate for these controllable states."
  },
  {
    "index": 391,
    "source_corpus_id": 220302636,
    "ref_id": "b6",
    "citation_corpus_id": 52055130,
    "start": 6736,
    "end": 6756,
    "title": "Learning deep representations by mutual information estimation and maximization",
    "abstract": "In this work, we perform unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality of the input to the objective can greatly influence a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and competes with fully-supervised learning on several classification tasks. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation-learning objectives for specific end-goals.IntroductionOne core objective of deep learning is to discover \"good\" representations, and the simple idea explored here is to train a representation-learning function (i.e., an encoder) to maximize the mutual information (MI) between its inputs and outputs. MI is notoriously difficult to compute, particularly in continuous and high-dimensional settings. Fortunately, recent advances enable effective computation of MI between high dimensional input/output pairs of deep neural networks(Belghazi et al., 2018). We leverage MI estimation for representation learning, but we will also show that, depending on the downstream task, maximizing MI between the complete input and the encoder output (i.e., global MI) is often not sufficient for learning useful representations. Rather, structure matters: maximizing the average MI between the representation and local regions of the input can greatly improve the representation's quality for, e.g., classification tasks, while global MI plays a stronger role in the ability to reconstruct the full input given the representation.",
    "prev": "Related Work\n\nContrastive loss Our work aims to encourage data-efficient multimodal generative-model learning using a popular representation learning metric-contrastive loss (Hadsell et al., 2006;Weinberger and Saul, 2009).",
    "curr": "Although learning robust generalisable representations have been an important desideratum in generative-model learning (Bengio et al., 2013;Tenenbaum and Freeman, 2000), most applications of contrastive methods skip the generative component, and directly learn representations for specific tasks, as seen in contrastive predictive coding for time series data (van den Oord et al., 2018), image classification (Hénaff et al., 2019), noise contrastive estimation for vector embeddings of words (Mnih and Kavukcuoglu, 2013), as well as a range of frameworks such as DIM (Hjelm et al., 2019), MoCo (He et al., 2019), SimCLR (Chen et al., 2020) for more general visual-representation learning.",
    "next": "These approaches, while effective for the particular task at hand, fall short in learning representations that are generalisable to different tasks."
  },
  {
    "index": 392,
    "source_corpus_id": 252716080,
    "ref_id": "b9",
    "citation_corpus_id": 56895453,
    "start": 1430,
    "end": 1449,
    "title": "LEARNING WHEN TO COMMUNICATE AT SCALE IN MULTIAGENT COOPERATIVE AND COMPETITIVE TASKS",
    "abstract": "Learning when to communicate and doing that effectively is essential in multi-agent tasks. Recent works show that continuous communication allows efficient training with back-propagation in multiagent scenarios, but have been restricted to fullycooperative tasks. In this paper, we present Individualized Controlled Continuous Communication Model (IC3Net) which has better training efficiency than simple continuous communication model, and can be applied to semi-cooperative and competitive settings along with the cooperative settings. IC3Net controls continuous communication with a gating mechanism and uses individualized rewards for each agent to gain better performance and scalability while fixing credit assignment issues. Using variety of tasks including StarCraft BroodWars TM explore and combat scenarios, we show that our network yields improved performance and convergence rates than the baselines as the scale increases. Our results convey that IC3Net agents learn when to communicate based on the scenario and profitability.Published as a conference paper at ICLR 2019Teaching agents how to communicate makes it is unnecessary to hand code the communication protocol with expert knowledge(Sukhbaatar et al., 2016)(Kottur et al., 2017. While the content of communication is important, it is also important to know when to communicate either to increase scalability and performance or to increase competitive edge. For example, a prey needs to learn when to communicate to avoid communicating its location with predators.Sukhbaatar et al. (2016)showed that agents communicating through a continuous vector are easier to train and have a higher information throughput than communication based on discrete symbols. Their continuous communication is differentiable, so it can be trained efficiently with back-propagation. However, their model assumes full-cooperation between agents and uses average global rewards. This restricts the model from being used in mixed or competitive scenarios as full-cooperation involves sharing hidden states to everyone; exposing everything and leading to poor performance by all agents as shown by our results. Furthermore, the average global reward for all agents makes the credit assignment problem even harder and difficult to scale as agents don't know their individual contributions in mixed or competitive scenarios where they want themselves to succeed before others.To solve above mentioned issues, we make the following contributions:",
    "prev": "Self-play (Samuel, 1967;Tesauro, 1995) has not only demonstrated the ability to abstract highdimensional state spaces as typified by AlphaGo (Silver et al., 2017), but also improved exploration coverage in partially observable environments.",
    "curr": "Communication (Sukhbaatar et al., 2016;Singh et al., 2019) exchanges their internal representations such as explored observation and hidden state in RNNs.",
    "next": "Evolutionary learning is expected to be a general framework for creating superhuman AIs as such learning can generate a high-level abstract representation without any bias in supervision."
  },
  {
    "index": 394,
    "source_corpus_id": 237353080,
    "ref_id": "b6",
    "citation_corpus_id": 215814169,
    "start": 2957,
    "end": 2974,
    "title": "Training with Quantization Noise for Extreme Model Compression",
    "abstract": "We tackle the problem of producing compact models, maximizing their accuracy for a given model size. A standard solution is to train networks with Quantization Aware Training [1], where the weights are quantized during training and the gradients approximated with the Straight-Through Estimator[2]. In this paper, we extend this approach to work with extreme compression methods where the approximations introduced by STE are severe. Our proposal is to only quantize a different random subset of weights during each forward, allowing for unbiased gradients to flow through the other weights. Controlling the amount of noise and its form allows for extreme compression rates while maintaining the performance of the original model. As a result we establish new state-of-the-art compromises between accuracy and model size both in natural language processing and image classification. For example, applying our method to state-of-the-art Transformer and ConvNet architectures, we can achieve 82.5% accuracy on MNLI by compressing RoBERTa to 14 MB and 80.0% top-1 accuracy on ImageNet by compressing an EfficientNet-B3 to 3.3 MB. * Equal",
    "prev": "Another solution is to compress a model with small accuracy degradation so that it takes less storage and reduces System-on-Chip (SoC) memory bandwidth utilization, which can minimize power-consumption and latency.",
    "curr": "To this end, various DNN compression techniques have been proposed (Wang et al., 2019b;Dong et al., 2020;Park et al., 2018;Rastegari et al., 2016;Fan et al., 2021;Stock et al., 2020;Zhou et al., 2019;Park et al., 2019;Yu et al., 2018;Polino et al., 2018).",
    "next": "Among them, weight-clustering/sharing (Han et al., 2016;Wu et al., 2018;Ullrich et al., 2017;Stock et al., 2020) has shown a high DNN compression ratio where weights are clustered into a few shareable weight values (or centroids) based on k-means clustering."
  },
  {
    "index": 395,
    "source_corpus_id": 204008396,
    "ref_id": "b40",
    "citation_corpus_id": 1957433,
    "start": 2782,
    "end": 2806,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "1 Published as a conference paper at ICLR 2020The joint vocabulary is composed of three disjoint sets:is the shared vocabulary set and V i J is the set of tokens that appear in the ith language only.",
    "curr": "Note that a key difference of existing supervised joint training methods is that embeddings corresponding to V s J are not shared between E L1 and E L2 , meaning that they are disjoint, as in alignment methods.DISCUSSIONWhile alignment methods have had great success, there are still some critical downsides, among which we stress the following points:\n\nINTRODUCTION\n\nContinuous word representations (Mikolov et al., 2013a;Pennington et al., 2014;Bojanowski et al., 2017) have become ubiquitous across a wide range of NLP tasks.",
    "next": "In particular, methods for crosslingual word embeddings (CLWE) have proven a powerful tool for cross-lingual transfer for downstream tasks, such as text classification (Klementiev et al., 2012), dependency parsing (Ahmad et al., 2019), named entity recognition (NER) (Xie et al., 2018;Chen et al., 2019), natural language inference , language modeling (Adams et al., 2017), and machine translation (MT) (Zou et al., 2013;Artetxe et al., 2018b;."
  },
  {
    "index": 396,
    "source_corpus_id": 219708931,
    "ref_id": "b0",
    "citation_corpus_id": 53388625,
    "start": 1969,
    "end": 1994,
    "title": "THE LOTTERY TICKET HYPOTHESIS: FINDING SPARSE, TRAINABLE NEURAL NETWORKS",
    "abstract": "Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard technique for pruning weights naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the lottery ticket hypothesis: dense, randomly-initialized feed-forward networks contain subnetworks (winning tickets) that-when trained in isolation-arrive at comparable test accuracy in a comparable number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Furthermore, the winning tickets we find above that size learn faster than the original network and exhibit higher test accuracy. arXiv:1803.03635v4 [cs.LG] 27 Nov 2018 1. Randomly initialize a neural network f (x; θ 0 ) (where θ 0 ∼ D θ ). 2. Train the network for j iterations, reaching parameters θ j . 3. Prune s% of the parameters, creating a mask m where P m = (100 − s)%. 4. To extract the winning ticket, reset the remaining parameters to their values in θ 0 , creating the untrained network f (x; m θ 0 ).If dense networks contain winning tickets and pruning reveals them, then the network f (x; m θ 0 )when trained for j iterations-will reach similar accuracy to f (x; θ j ) at least as quickly, and m will be too sparse for a randomly-reinitialized or randomly-sparsified network to do the same.Results. We identify winning tickets in a fully-connected architecture for MNIST and convolutional architectures for CIFAR10 across several optimization strategies (SGD, momentum, and Adam) with techniques like dropout, weight decay, and batchnorm. In deeper networks, our pruning-based strategy for finding winning tickets is sensitive to the learning rate: it requires warmup to find winning tickets at higher learning rates. The winning tickets we find are 10-20% (or less) of the size of the Anonymous. Gradient descent provably optimizes over-parameterized neural networks. Dally. Dsd: Regularizing deep neural networks with dense-sparse-dense training flow. Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size. arXiv preprint arXiv:1602.07360, 2016.",
    "prev": "Introduction\n\nThe majority of pruning algorithms for Deep Neural Networks require training the dense models and often fine-tuning sparse sub-networks in order to obtain their pruned counterparts.",
    "curr": "In Frankle and Carbin [2019], the authors provide empirical evidence that there exist sparse sub-networks that can be trained from scratch to achieve similar performance as their dense counterparts.",
    "next": "However, their method to find such sub-networks requires training the full-sized model and intermediate sub-networks, making the process much more expensive."
  },
  {
    "index": 397,
    "source_corpus_id": 249191769,
    "ref_id": "b7",
    "citation_corpus_id": 203641746,
    "start": 3128,
    "end": 3130,
    "title": "Beyond Linearization: On Quadratic and Higher-Order Approximation of Wide Neural Networks",
    "abstract": "Recent theoretical work has established connections between over-parametrized neural networks and linearized models governed by the Neural Tangent Kernels (NTKs). NTK theory leads to concrete convergence and generalization results, yet the empirical performance of neural networks are observed to exceed their linearized models, suggesting insufficiency of this theory.Towards closing this gap, we investigate the training of over-parametrized neural networks that are beyond the NTK regime yet still governed by the Taylor expansion of the network. We bring forward the idea of randomizing the neural networks, which allows them to escape their NTK and couple with quadratic models. We show that the optimization landscape of randomized two-layer networks are nice and amenable to escaping-saddle algorithms. We prove concrete generalization and expressivity results on these randomized networks, which leads to sample complexity bounds (of learning certain simple functions) that match the NTK and can in addition be better by a dimension factor when mild distributional assumptions are present. We demonstrate that our randomization technique can be generalized systematically beyond the quadratic case, by using it to find networks that are coupled with higher-order terms in their Taylor series. * Salesforce Research. yu.bai@salesforce.com † Princeton University. jasonlee@princeton.edu design. arXiv preprint arXiv:1711.00501, 2017. . Characterizing implicit bias in terms of optimization geometry. arXiv preprint arXiv:1802.08246, 2018a.Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent on linear convolutional networks. In Advances in Neural Information Processing Systems, pages 9461-9471, 2018b.Benjamin D Haeffele and René Vidal. Global optimality in tensor factorization, deep learning, and beyond. arXiv preprint arXiv:1506.07540, 2015.Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231, 2016. . Kernel and deep regimes in overparametrized models. arXiv preprint arXiv:1906.05827, 2019.Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding neural networks. arXiv preprint arXiv:1904.00687, 2019. gradient descent optimizes over-parameterized deep ReLU networks. arXiv preprint arXiv:1811.08888, 2018.",
    "prev": "To alleviate the non-convexity of neural network optimization, recent studies focus on over-parameterization as one of the promising approaches.",
    "curr": "Indeed, it is fully exploited by (i) Neural Tangent Kernel (NTK) [27,5,6,18,53, 60] and (ii) mean field analysis [36,15,31,50,14,46].",
    "next": "In the setting of NTK, a relatively large-scale initialization is considered."
  },
  {
    "index": 398,
    "source_corpus_id": 256598122,
    "ref_id": "b0",
    "citation_corpus_id": 15978939,
    "start": 6162,
    "end": 6184,
    "title": "Domain Adaptation with Structural Correspondence Learning",
    "abstract": "Discriminative learning methods are widely used in natural language processing. These methods work best when their training and test data are drawn from the same distribution. For many NLP tasks, however, we are confronted with new domains in which labeled data is scarce or non-existent. In such cases, we seek to adapt existing models from a resourcerich source domain to a resource-poor target domain. We introduce structural correspondence learning to automatically induce correspondences among features from different domains. We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger.",
    "prev": "Domain Adaptation Typically domain adaptations consider data distributions and deviations to search for mappings aligning domains.",
    "curr": "Early literature suggested that assuming data drawn from certain probabilities (Blitzer et al., 2006) can be used to model and compensate for the domain mismatch.",
    "next": "Some studies then looked for theoretical arguments when a successful adaptation can be yielded."
  },
  {
    "index": 399,
    "source_corpus_id": 203592088,
    "ref_id": "b17",
    "citation_corpus_id": 3366315,
    "start": 4120,
    "end": 4140,
    "title": "Published as a conference paper at ICLR 2018 SPECTRAL NORMALIZATION FOR GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. The code with Chainer (Tokui et al., 2015), generated images and pretrained models are available at https://github.com/pfnet-research/sngan_ projection.Published as a conference paper at ICLR 2018• Lipschitz constant is the only hyper-parameter to be tuned, and the algorithm does not require intensive tuning of the only hyper-parameter for satisfactory performance. • Implementation is simple and the additional computational cost is small.",
    "prev": "As such, the distribution of training images can be estimated precisely without supervision.",
    "curr": "Recent interests in generative models pertain to their training stability Gulrajani et al., 2017;Miyato et al., 2018) and improvement in quality and diversity (Karras et al., 2018;Brock et al., 2019;Karras et al., 2019).",
    "next": "Furthermore, methods to learn 3D representation from 2D images by constructing a generative model conditioned on camera parameters have been proposed (Shen et al., 2018;Sitzmann et al., 2019;Nguyen-Phuoc et al., 2019)."
  },
  {
    "index": 401,
    "source_corpus_id": 243847413,
    "ref_id": "b6",
    "citation_corpus_id": 6628106,
    "start": 15985,
    "end": 16004,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "We then approximate the integral over the δ-ball via Monte Carlo integration by sampling k points x 1 , ..., x k uniformly in the ball to obtain the objective:\n1 k k i=1 log p(g|x i ) + log p(x i ).",
    "curr": "(3)\nFinally, as the objective is differentiable, we can use gradient-based optimizer such as Adam (Kingma & Ba, 2015), and obtain the attack in Algorithm 1.",
    "next": "Fig."
  },
  {
    "index": 403,
    "source_corpus_id": 56657849,
    "ref_id": "b17",
    "citation_corpus_id": 6628106,
    "start": 37305,
    "end": 37326,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "For external competitor methods, we used the hyperparameters from the respective publications where applicable and otherwise the default parameters from their packages.",
    "curr": "The models were implemented in TensorFlow (Abadi et al., 2016) and optimized using Adam (Kingma and Ba, 2015).",
    "next": "C CLUSTERING PERFORMANCE MEASURES\n\nGiven that one of our most interesting tasks at hand is the clustering of data, we need some performance measures to objectively compare the quality of this clustering with other methods."
  },
  {
    "index": 404,
    "source_corpus_id": 49876500,
    "ref_id": "b13",
    "citation_corpus_id": 8968704,
    "start": 3164,
    "end": 3167,
    "title": "A Neural Representation of Sketch Drawings",
    "abstract": "We present sketch-rnn, a recurrent neural network (RNN) able to construct stroke-based drawings of common objects. The model is trained on a dataset of human-drawn images representing many different classes. We outline a framework for conditional and unconditional sketch generation, and describe new robust training methods for generating coherent sketch drawings in a vector format.",
    "prev": "From another point of view, it also indicates that the autoencoder has uncovered some structure about the data and has captured it in its latent space.",
    "curr": "These characteristics have led interpolations to be a commonly reported experimental result in studies about autoencoders [5,11,14,26,27,32] and latent-variable generative models in general [10,30,38].",
    "next": "The connection between interpolation and a \"flat\" data manifold has also been explored in the context of unsupervised representation learning [3] and regularization [40]."
  },
  {
    "index": 408,
    "source_corpus_id": 263909446,
    "ref_id": "b40",
    "citation_corpus_id": 227209335,
    "start": 1900,
    "end": 1918,
    "title": "SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS",
    "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reversetime SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in diffusion probabilistic modeling and scorebased generative modeling, and allows for new sampling procedures. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, which enables exact likelihood computation, and improved sampling efficiency. In addition, our framework enables conditional generation with an unconditional model, as we demonstrate with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high fidelity generation of 1024ˆ1024 images for the first time from a score-based generative model.",
    "prev": "s generalize DDIM, mapping the reverse diffusion dynamics to a more amenable space for sampling.In contrast, splitting-based integrators, commonly used in molecular dynamics, reduce the numerical simulation error by cleverly alternating between numerical updates involving the data and auxiliary variables.After extensively studying these methods empirically and theoretically, we present a hybrid method that leads to the best-reported performance for diffusion models in augmented spaces.Applied to Phase Space Langevin Diffusion[Pandey & Mandt, 2023]on CIFAR-10, our deterministic and stochastic samplers achieve FID scores of 2.11 and 2.36 in only 100 network function evaluations (NFE) as compared to 2.57 and 2.63 for the best-performing baselines, respectively.Our code and model checkpoints will be made publicly available at https://github.com/mandt-lab/PSLD.",
    "curr": "INTRODUCTION\n\nScore-based Generative models (or Diffusion models) (Sohl-Dickstein et al., 2015;Song & Ermon, 2019;Ho et al., 2020;Song et al., 2020) have demonstrated impressive performance on various tasks, such as image and video synthesis (Dhariwal & Nichol, 2021;Ho et al., 2022a;Rombach et al., 2022;Ramesh et al., 2022;Saharia et al., 2022a;Yang et al., 2022;Ho et al., 2022b;Harvey et al., 2022), image super-resolution (Saharia et al., 2022b), and audio and speech synthesis (Chen et al., 2021;Lam et al., 2021).",
    "next": "However, high-quality sample generation in standard diffusion models requires hundreds to thousands of expensive score function evaluations.While there have been recent advances in improving the sampling efficiency (Song et al., 2021;Lu et al., 2022;Zhang & Chen, 2023), most of these efforts have been focused towards a specific family of models that perform diffusion in the data space (Song et al., 2020;Karras et al., 2022).Interestingly, recent work (Dockhorn et al., 2022b;Pandey & Mandt, 2023;Singhal et al., 2023) indicates that performing diffusion in a joint space, where the data space is augmented with auxiliary v"
  },
  {
    "index": 410,
    "source_corpus_id": 264825556,
    "ref_id": "b34",
    "citation_corpus_id": 246823323,
    "start": 4366,
    "end": 4387,
    "title": "Published as a conference paper at ICLR 2022 LEARNING FAST SAMPLERS FOR DIFFUSION MODELS BY DIFFERENTIATING THROUGH SAMPLE QUALITY",
    "abstract": "Diffusion models have emerged as an expressive family of generative models rivaling GANs in sample quality and autoregressive models in likelihood scores. Standard diffusion models typically require hundreds of forward passes through the model to generate a single high-fidelity sample. We introduce Differentiable Diffusion Sampler Search (DDSS): a method that optimizes fast samplers for any pre-trained diffusion model by differentiating through sample quality scores. We present Generalized Gaussian Diffusion Models (GGDM), a family of flexible non-Markovian samplers for diffusion models. We show that optimizing the degrees of freedom of GGDM samplers by maximizing sample quality scores via gradient descent leads to improved sample quality. Our optimization procedure backpropagates through the sampling process using the reparametrization trick and gradient rematerialization. DDSS achieves strong results on unconditional image generation across various datasets (e.g., FID scores on LSUN church 128x128 of 11.6 with only 10 inference steps, and 4.82 with 20 steps, compared to 51.1 and 14.9 with strongest DDPM/DDIM baselines). Our method is compatible with any pre-trained diffusion model without fine-tuning or re-training required. * Work done as part of the Google AI Residency.",
    "prev": "n training, it is shown to empirically generate samples of comparable quality to the original model (Salimans & Ho, 2022;Meng et al., 2023).Unfortunately, the GPU time required to distill a model is comparable to the training time of the original model Salimans & Ho (2022), which is often considerable.",
    "curr": "(ii) Dedicated solvers: where the specific structure of the ODE is used to design a more efficient solver (Song et al., 2020a;Lu et al., 2022a;b) and/or employ a suitable solver family from the literature of numerical analysis (Zhang & Chen, 2022;Zhang et al., 2023).The main benefit of this approach is two-fold: First, it is consistent, i.e., as the number of steps (NFE) increases, the samples converge to those of the pre-trained model.Second, it does not require further training/fine-tuning of the pre-trained model, consequently avoiding long additional training times and access to training data.Related to our approach, some works have tried to learn an ODE solver within a certain class (Watson et al., 2021;Duan et al., 2023); however, they do not guarantee consistency and usually introduce moderate improvements over generic dedicated solvers.In this paper, we introduce Bespoke solvers, a framework for learning consistent ODE solvers custom-tailored to pre-trained flow models.The main motivation for Bespoke solvers is that different models exhibit sampling paths with different characteristics, leading to local truncation errors that are specific to each instance of a trained model.A key observation of this paper is that optimizing a solver for a particular model can significantly improve quality of samples for low NFE compared to existing dedicated solvers.Furthermore, Bespoke solvers use a very small number of learnable parameters and consequently are efficient to train.For example, we have trained n ∈ {5, 8, 10} steps Bespoke solvers for a pretrained ImageNet-64×64 flow model with {40, 64, 80} learnable parameters (resp.",
    "next": ")producing images with Fréchet Inception Distances (FID) of 2.2, 1."
  },
  {
    "index": 412,
    "source_corpus_id": 256827824,
    "ref_id": "b53",
    "citation_corpus_id": 5834589,
    "start": 25088,
    "end": 25092,
    "title": "ON LARGE-BATCH TRAINING FOR DEEP LEARNING: GENERALIZATION GAP AND SHARP MINIMA",
    "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say 32-512 data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions-and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap. Published as a conference paper at ICLR 2017 These methods minimize the objective function f by iteratively taking steps of the form: J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. . Weak sharp minima and penalty functions in mathematical programming. PhD thesis, University of Cambridge, 1988.Michael P Friedlander and Mark Schmidt. Hybrid deterministic-stochastic methods for data fitting.",
    "prev": "Figure 4 shows that for some rules, i.e.Trimmed Mean, the presence of even a single Byzantine worker has a catastrophic impact.For other rules, as the number of Byzantine workers increases, filtering out the outliers becomes more challenging because the amount of noise increases.Regardless, FA remains more robust compared to other approaches.",
    "curr": "Marginal utility of larger batch sizes under a fixed noise level:\n\nWe empirically verified the batch size required to identify our optimal Y * -the FA matrix at each iteration.In particular, we fixed the noise level to f = 3 Byzantine workers and varied batch sizes.We show the results in Figure 5.Our results indicate that, in cases where a larger batch size is a training requirement, FA achieves a significantly better accuracy compared to the existing state of the art aggregators.This may be useful in some large scale vision applications, see [54,55] for more details.Empirically, we can already see that our spectral relaxation to identify gradient subspace is effective in practice in all our experiments.",
    "next": "Tolerance to communication loss:\n\nTo analyze the effect of unreliable communication channels between the workers and the parameter server on convergence, we design an experiment where the physical link between some of the workers and the parameter server randomly drops a percentage of packets.Here, we set the loss rate of three links to 10% i.e., there are 3 Byzantine workers in our setting.The loss is introduced using the netem queuing discipline in Linux designed to emulate the properties of wide area networks [56].The two main takeaways in Figure 6a are:\n\n1."
  },
  {
    "index": 413,
    "source_corpus_id": 259375820,
    "ref_id": "b11",
    "citation_corpus_id": 241035330,
    "start": 2653,
    "end": 2671,
    "title": "An Explanation of In-context Learning as Implicit Bayesian Inference",
    "abstract": "Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning 1 . Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.",
    "prev": "Introduction\n\nLarge language models (LLMs) demonstrate the surprising ability of in-context learning, where an LLM \"learns\" to solve a task by conditioning on a prompt containing input-output exemplars [Brown et al., 2020, Lieber et al., 2021, Radford et al., 2019, Wang and Komatsuzaki, 2021.",
    "curr": "Recent works have advanced the understanding of in-context learning via empirical analysis [Min et al., 2022, Akyürek et al., 2023, von Oswald et al., 2022, Dai et al., 2023, but theoretical analysis remains limited [Xie et al., 2022].",
    "next": "A recent line of work [Garg et al., 2022, Akyürek et al., 2023, von Oswald et al., 2022, Dai et al., 2023 empirically finds that transformers can be trained to implement algorithms that solve linear regression problems in-context."
  },
  {
    "index": 414,
    "source_corpus_id": 246485738,
    "ref_id": "b12",
    "citation_corpus_id": 235293778,
    "start": 30919,
    "end": 30944,
    "title": "Published as a conference paper at ICLR 2021 UNSUPERVISED REPRESENTATION LEARNING FOR TIME SERIES WITH TEMPORAL NEIGHBORHOOD CODING",
    "abstract": "Time series are often complex and rich in information but sparsely labeled and therefore challenging to model. In this paper, we propose a self-supervised framework for learning generalizable representations for non-stationary time series. Our approach, called Temporal Neighborhood Coding (TNC), takes advantage of the local smoothness of a signal's generative process to define neighborhoods in time with stationary properties. Using a debiased contrastive objective, our framework learns time series representations by ensuring that in the encoding space, the distribution of signals from within a neighborhood is distinguishable from the distribution of non-neighboring signals. Our motivation stems from the medical field, where the ability to model the dynamic nature of time series data is especially valuable for identifying, tracking, and predicting the underlying patients' latent states in settings where labeling data is practically impossible. We compare our method to recently developed unsupervised representation learning approaches and demonstrate superior performance on clustering and classification tasks for multiple datasets. *",
    "prev": "We ran the code from their open source repository 4 as is, hyper-parameters used are all defaults as suggested in their paper.",
    "curr": "TNC (Tonekaboni et al., 2021) TNC proposes a self-supervised framework for learning generalizable representations for non-stationary time series.",
    "next": "They make use of the augmented Dickey-Fuller test for stationarity to ensure positive samples come from the a neighborhood of similar signals."
  },
  {
    "index": 418,
    "source_corpus_id": 53034786,
    "ref_id": "b42",
    "citation_corpus_id": 990233,
    "start": 2951,
    "end": 2972,
    "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    "abstract": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.",
    "prev": "RNNs explicitly imposes a chain structure on the data.",
    "curr": "This chain structure may seem at odds with the latent non-sequential structure of language and poses several difficulties for the processing of natural language data with deep learning methods, such as capturing long-term dependencies (Bengio et al., 2009), achieving good generalization (Bowman et al., 2015), handling negation (Socher et al., 2013), etc.",
    "next": "Meanwhile, some evidence exists that an RNN with sufficient capacity has the potential to encode such a tree structure implicitly ."
  },
  {
    "index": 419,
    "source_corpus_id": 256827026,
    "ref_id": "b7",
    "citation_corpus_id": 52967399,
    "start": 2468,
    "end": 2471,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Notably, on the MSRVTT retrieval task, UniAdapter achieves 49.7% recall@1 with only 2.2% tunable model parameters, outperforming the latest competitors by 2.0%.",
    "curr": "The code and models are available at https\n\nIntroduction\n\nThe pretrain-finetune paradigm has achieved great success in natural language processing (NLP) [8,4], computer vision (CV) [41], and multimodal modeling [37,22], where models are first pre-trained with large-scale data, and then fully fine-tuned for each downstream task.",
    "next": "Recent research further finds that fine-tuning/adapting a foundation model to a new modality by introducing additional trainable modules significantly outperforms previous works, such as temporal modeling modules [13,23,33] for image-to-video transferring (see Figure 1 (a))."
  },
  {
    "index": 421,
    "source_corpus_id": 252715598,
    "ref_id": "b3",
    "citation_corpus_id": 11212020,
    "start": 2344,
    "end": 2367,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "Code is publicly available.",
    "curr": "INTRODUCTION\n\nThe vision community has witnessed the prevalence of self-attention (Bahdanau et al., 2015) and Transformers (Vaswani et al., 2017).",
    "next": "The success of Transformers in natural language processing motivates the creation of their variants for vision recognition."
  },
  {
    "index": 423,
    "source_corpus_id": 237372712,
    "ref_id": "b18",
    "citation_corpus_id": 5034059,
    "start": 1429,
    "end": 1448,
    "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    "abstract": "For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusively tailored to a specific task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating and analyzing the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all our tasks yields better results than training a separate model for each task. However, the low absolute performance of our best model indicates the need for improved general NLU systems. son. 2013. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint 1312.3005.",
    "prev": "INTRODUCTION\n\nShared benchmarks and datasets have historically played a crucial role in driving advances in largescale applications of deep learning, e.g.",
    "curr": "in computer vision (Deng et al., 2009) and natural language processing (Wang et al., 2019;Rajpurkar et al., 2016;Paperno et al., 2016).",
    "next": "Neural theorem proving is a rapidly developing area which aims to apply techniques from deep learning to interactive theorem proving."
  },
  {
    "index": 425,
    "source_corpus_id": 259187750,
    "ref_id": "b11",
    "citation_corpus_id": 232185279,
    "start": 12657,
    "end": 12661,
    "title": "Published as a conference paper at ICLR 2021 FAIR MIXUP: FAIRNESS VIA INTERPOLATION",
    "abstract": "Training classifiers under fairness constraints such as group fairness, regularizes the disparities of predictions between the groups. Nevertheless, even though the constraints are satisfied during training, they might not generalize at evaluation time. To improve the generalizability of fair classifiers, we propose fair mixup, a new data augmentation strategy for imposing the fairness constraint. In particular, we show that fairness can be achieved by regularizing the models on paths of interpolated samples between the groups. We use mixup, a powerful data augmentation strategy to generate these interpolates. We analyze fair mixup and empirically show that it ensures a better generalization for both accuracy and fairness measurement in tabular, vision, and language benchmarks. The code is available at https://github.com/chingyaoc/fair-mixup. * Work done during an internship at IBM Research AI",
    "prev": "In particular, we consider the following three types of in-processing methods.",
    "curr": "Gap Regularization [12] simplifies optimization by offering a smooth approximation to real loss functions, which are often non-convex or difficult to optimize directly.",
    "next": "This approach includes DiffDP, DiffEodd, and DiffEopp."
  },
  {
    "index": 429,
    "source_corpus_id": 238419003,
    "ref_id": "b2",
    "citation_corpus_id": 52156206,
    "start": 8524,
    "end": 8556,
    "title": "Gromov-Wasserstein Alignment of Word Embedding Spaces",
    "abstract": "Cross-lingual or cross-domain correspondences play key roles in tasks ranging from machine translation to transfer learning. Recently, purely unsupervised methods operating on monolingual embeddings have become effective alignment tools. Current state-of-theart methods, however, involve multiple steps, including heuristic post-hoc refinement strategies. In this paper, we cast the correspondence problem directly as an optimal transport (OT) problem, building on the idea that word embeddings arise from metric recovery algorithms. Indeed, we exploit the Gromov-Wasserstein distance that measures how similarities between pairs of words relate across languages. We show that our OT objective can be estimated efficiently, requires little or no tuning, and results in performance comparable with the state-of-the-art in various unsupervised word translation tasks.",
    "prev": "Finally, other domain adaptation and transfer learning settings use Gromov-Wasserstein variants, e.g.",
    "curr": "for transfer between word embedding spaces (Alvarez-Melis & Jaakkola, 2018) and image spaces (Vayer et al., 2020b).",
    "next": "PRELIMINARIES\n\nMetric Markov Decision Process."
  },
  {
    "index": 432,
    "source_corpus_id": 254877694,
    "ref_id": "b8",
    "citation_corpus_id": 220055784,
    "start": 4552,
    "end": 4574,
    "title": "SenSeI: Sensitive Set Invariance for Enforcing Individual Fairness",
    "abstract": "In this paper, we cast fair machine learning as invariant machine learning. We first formulate a version of individual fairness that enforces invariance on certain sensitive sets. We then design a transport-based regularizer that enforces this version of individual fairness and develop an algorithm to minimize the regularizer efficiently. Our theoretical results guarantee the proposed approach trains certifiably fair ML models. Finally, in the experimental studies we demonstrate improved fairness metrics in comparison to several recent fair training procedures on three ML tasks that are susceptible to algorithmic bias.",
    "prev": "Then, we use active learning and crowdworker judgments to identify pairs that deserve similar treatment according to human intuition.",
    "curr": "In the context of text classification, most existing works have cast similarity in terms of word replacement (Dixon et al., 2018;Garg et al., 2019;Yurochkin & Sun, 2021;Liang et al., 2020).",
    "next": "Given a sentence s, a similar sentence s is generated by replacing each word in s, that belongs to a list of words A j indicative of a demographic group j, by a word from list A j , indicative of another demographic group j = j."
  },
  {
    "index": 435,
    "source_corpus_id": 253098972,
    "ref_id": "b5",
    "citation_corpus_id": 165163607,
    "start": 2447,
    "end": 2473,
    "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions",
    "abstract": "In this paper we study yes/no questions that are naturally occurring -meaning that they are generated in unprompted and unconstrained settings. We build a reading comprehension dataset, BoolQ, of such questions, and show that they are unexpectedly challenging. They often query for complex, non-factoid information, and require difficult entailment-like inference to solve. We also explore the effectiveness of a range of transfer learning baselines. We find that transferring from entailment data is more effective than transferring from paraphrase or extractive QA data, and that it, surprisingly, continues to be very beneficial even when starting from massive pre-trained language models such as BERT. Our best method trains BERT on MultiNLI and then re-trains it on our train set. It achieves 80.4% accuracy compared to 90% accuracy of human annotators (and 62% majority-baseline), leaving a significant gap for future work.",
    "prev": "We conduct experiments across a diverse set of eight NLP tasks using models of different scales (T5-{base, large, XL}) and find that SESoM consistently outperforms the existing models of the same as well as larger parametric scale by a large margin.",
    "curr": "INTRODUCTION\n\nRecent few years have witnessed the great success of large pre-trained language models (PLM) (Kenton & Toutanova, 2019;Liu et al., 2019;Radford et al., 2019;Raffel et al., 2020;Brown et al., 2020).",
    "next": "The size of pre-trained models which can easily go to billions of parameters (Brown et al., 2020;Raffel et al., 2020), however, hinder their real-world deployments and applications."
  },
  {
    "index": 437,
    "source_corpus_id": 254535963,
    "ref_id": "b8",
    "citation_corpus_id": 21529792,
    "start": 7416,
    "end": 7433,
    "title": "Published as a conference paper at ICLR 2018 LEARNING ROBUST REWARDS WITH ADVERSARIAL INVERSE REINFORCEMENT LEARNING",
    "abstract": "Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually specified reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose AIRL, a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation. We demonstrate that AIRL is able to recover reward functions that are robust to changes in dynamics, enabling us to learn policies even under significant variation in the environment seen during training. Our experiments show that AIRL greatly outperforms prior methods in these transfer settings.",
    "prev": "te that the same finding holds even with real humans.Overall, our results suggest an optimistic perspective on the framework of reward learning, and that efforts in improving human models will further enhance the quality of the inferred rewards.",
    "curr": "RELATED WORK\n\nInverse reinforcement learning (IRL) aims to use expert demonstrations, often from a human, to infer a reward function (Ng & Russel, 2000;Ziebart et al., 2008).Maximum-entropy (MaxEnt) IRL is a popular IRL framework that models the demonstrator as noisily optimal, maximizing reward while also randomising actions as much as possible (Ziebart et al., 2008;2010).This is equivalent to modeling humans as Boltzmann rational.MaxEnt IRL is preferred in practice over Bayesian IRL (Ramachandran & Amir, 2007), which learns a posterior over reward functions rather than a point estimate, due to better scaling in high-dimensional environments (Wulfmeier et al., 2015).More recently, Guided Cost Learning (Finn et al., 2016) and Adversarial IRL (Fu et al., 2018) learn reward functions more robust to environment changes, but build off similar modeling assumptions as MaxEnt IRL.Gleave & Toyer (2022) connected MaxEnt IRL to maximum likelihood estimation (MLE), which is the framework that we consider in this work.One of the challenges with IRL is that rewards are not always uniquely identified from expert demonstrations (Cao et al., 2021;Kim et al., 2021).Since identifiability is orthogonal to the main message of our work-sensitivity to misspecified human models-we assume that the dataset avoids this ambiguity.",
    "next": "Recent IRL algorithms attempt to account for possible irrationalities in the expert (Evans et al., 2016;Reddy et al., 2018;Shah et al., 2019).Reddy et al."
  },
  {
    "index": 440,
    "source_corpus_id": 258865444,
    "ref_id": "b14",
    "citation_corpus_id": 248987078,
    "start": 3398,
    "end": 3420,
    "title": "Context Matters for Image Descriptions for Accessibility: Challenges for Referenceless Evaluation Metrics",
    "abstract": "Few images on the Web receive alt-text descriptions that would make them accessible to blind and low vision (BLV) users. Imagebased NLG systems have progressed to the point where they can begin to address this persistent societal problem, but these systems will not be fully successful unless we evaluate them on metrics that guide their development correctly. Here, we argue against current referenceless metrics -those that don't rely on human-generated ground-truth descriptions -on the grounds that they do not align with the needs of BLV users. The fundamental shortcoming of these metrics is that they do not take context into account, whereas contextual information is highly valued by BLV users. To substantiate these claims, we present a study with BLV participants who rated descriptions along a variety of dimensions. An in-depth analysis reveals that the lack of context-awareness makes current referenceless metrics inadequate for advancing image accessibility. As a proof-of-concept, we provide a contextual version of the referenceless metric CLIPScoreSculpture GazeboImage descriptionA freestanding, open, hexagonal gazebo with a dome-like roof in an idyllic park area.",
    "prev": "onto the users, who frequently either for convenience or lack of awareness simply do not choose to do so (Gleason et al., 2019).We find that as many as 98% of images uploaded to Twitter even after the widespread feature rollout do not have alt-text, to say nothing of the quality of those that do.When a screen reader encounters such an image, it will simply say \"Image\", leaving the user with no meaningful information about what the image is actually of.Even when images on Twitter do have accompanying user-written alt-text, the quality is inconsistent as not all users are well informed regarding best practices.",
    "curr": "Note that while there is a long line of existing research on the broader task of captioning images more generally, alt-text generation for social media is a special case of this task, which in turn comes with its own challenges.Well written alt-text is generally more explicitly descriptive than a high level caption and may emphasize specific details in the image based on context (Kreiss et al., 2022a).See Figure 1 for an example.Furthermore, the distribution of image types on Twitter differs substantially from those found in traditional captioning datasets, and may contain digital artwork, promotional graphics, or screenshots containing text.An additional challenge is that Twitter users are not well trained in the practice of writing alt-text, and therefore native \"gold\" examples can vary  ()   Alt-text: Delicious pink margarita sits on a table full of salsa and chips\n\nCaption: having cocktail at person who wants to join ?",
    "next": "Our Dataset Conceptual Captions\n\nFigure 1: Left: An image that requires textual context to write accurate alt-text for.Without conditioning on the tweet text, the election flyers are indistinguishable from books to a traditional captioning system.Right: Two similar images from our dataset and Conceptual Captions with their gold labels.The alt-text for the first image is literally descriptive while the second is more colloquial."
  },
  {
    "index": 441,
    "source_corpus_id": 257687492,
    "ref_id": "b26",
    "citation_corpus_id": 52908393,
    "start": 27139,
    "end": 27164,
    "title": "CEM-RL: Combining evolutionary and gradient-based methods for policy search",
    "abstract": "Deep neuroevolution and deep reinforcement learning (deep RL) algorithms are two popular approaches to policy search. The former is widely applicable and rather stable, but suffers from low sample efficiency. By contrast, the latter is more sample efficient, but the most sample efficient variants are also rather unstable and highly sensitive to hyper-parameter setting. So far, these families of methods have mostly been compared as competing tools. However, an emerging approach consists in combining them so as to get the best of both worlds. Two previously existing combinations use either an ad hoc evolutionary algorithm or a goal exploration process together with the Deep Deterministic Policy Gradient (ddpg) algorithm, a sample efficient off-policy deep RL algorithm. In this paper, we propose a different combination scheme using the simple cross-entropy method (cem) and Twin Delayed Deep Deterministic policy gradient (td3), another off-policy deep RL algorithm which improves over ddpg. We evaluate the resulting method, cem-rl, on a set of benchmarks classically used in deep RL. We show that cem-rl benefits from several advantages over its competitors and offers a satisfactory trade-off between performance and sample efficiency.",
    "prev": "While PBT-MAP-ELITES and PBT use similar strategies to update the population of agents, PBT only seeks the highest-performing agent by extracting the best one from the final population while PBT-MAP-ELITES aims to find a diverse collection of high-performing agents.",
    "curr": "Several methods such as CERL, ERL, and CEM-RL (Pourchot & Sigaud, 2019;Khadka & Tumer, 2018;Khadka et al., 2019) combine ES algorithms with PBRL methods to improve the asymptotic performance and sample efficiency of standard RL methods.",
    "next": "Other methods, such as DvD (Parker-Holder et al., 2020) and P3S-TD3 (Jung et al., 2020), train populations of agents and add terms in their loss functions to encourage the agents to explore different regions of the state-action space but always with the end goal of maximizing the performance of the best agent in the population."
  },
  {
    "index": 442,
    "source_corpus_id": 3497822,
    "ref_id": "b4",
    "citation_corpus_id": 51559,
    "start": 3308,
    "end": 3311,
    "title": "QUASI-RECURRENT NEURAL NETWORKS",
    "abstract": "Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep's computation on the previous timestep's output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks. * Equal contribution",
    "prev": "Bradbury et al.",
    "curr": "[5], Kalchbrenner et al.",
    "next": "[12], Gehring et al."
  },
  {
    "index": 444,
    "source_corpus_id": 27494814,
    "ref_id": "b24",
    "citation_corpus_id": 2973141,
    "start": 6807,
    "end": 6824,
    "title": "Compression of Neural Machine Translation Models via Pruning",
    "abstract": "Neural Machine Translation (NMT), like many other deep learning domains, typically suffers from over-parameterization, resulting in large storage sizes. This paper examines three simple magnitude-based pruning schemes to compress NMT models, namely class-blind, class-uniform, and class-distribution, which differ in terms of how pruning thresholds are computed for the different classes of weights in the NMT architecture. We demonstrate the efficacy of weight pruning as a compression technique for a state-of-the-art NMT system. We show that an NMT model with over 200 million parameters can be pruned by 40% with very little performance loss as measured on the WMT'14 English-German translation task. This sheds light on the distribution of redundancy in the NMT architecture. Our main result is that with retraining, we can recover and even surpass the original performance with an 80%-pruned model.",
    "prev": "In Optimal Brain Surgeon (Hassibi et al., 1993), the saliency for each parameter was computed using the inverse Hessian matrix, and the low-saliency weights are pruned and all other weights in the network are updated using the second-order information.",
    "curr": "More recently, magnitude-based weight pruning methods have become popular techniques for network pruning (Han et al., 2015b,a;See et al., 2016;Narang et al., 2017).",
    "next": "Magnitude-based weight pruning techniques are computationally efficient, scaling to large networks and datasets."
  },
  {
    "index": 448,
    "source_corpus_id": 235417313,
    "ref_id": "b0",
    "citation_corpus_id": 235614315,
    "start": 4834,
    "end": 4852,
    "title": "Published as a conference paper at ICLR 2021 FEDERATED LEARNING BASED ON DYNAMIC REGULARIZATION",
    "abstract": "We propose a novel federated learning method for distributively training neural network models, where the server orchestrates cooperation between a subset of randomly chosen devices in each round. We view Federated Learning problem primarily from a communication perspective and allow more device level computations to save transmission costs. We point out a fundamental dilemma, in that the minima of the local-device level empirical loss are inconsistent with those of the global empirical loss. Different from recent prior works, that either attempt inexact minimization or utilize devices for parallelizing gradient computation, we propose a dynamic regularizer for each device at each round, so that in the limit the global and device solutions are aligned. We demonstrate both through empirical results on real and synthetic data as well as analytical results that our scheme leads to efficient training, in both convex and non-convex settings, while being fully agnostic to device heterogeneity and robust to large number of devices, partial participation and unbalanced data.",
    "prev": "arXiv preprint arXiv:\n\nINTRODUCTION\n\nFederated learning (FL) (McMahan et al., 2017), which is a distributed learning framework with personalized data, has become an attractive field of research.",
    "curr": "From the early days of this field, improving a single global model across devices has been the main objective (Zhao et al., 2018;Duan et al., 2019;Li et al., 2018;Acar et al., 2021), where the global model suffers from data heterogeneity.",
    "next": "Many researchers have recently focused on multiple personalized models by leveraging data heterogeneity across devices as a blessing in disguise (Chen et al., 2018;Dinh et al., 2021;Zhang et al., 2020;Fallah et al., 2020;Shamsian et al., 2021;Smith et al., 2017)."
  },
  {
    "index": 449,
    "source_corpus_id": 220249871,
    "ref_id": "b3",
    "citation_corpus_id": 213152193,
    "start": 9504,
    "end": 9524,
    "title": "ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS",
    "abstract": "Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK]  and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.",
    "prev": "(2017), a hand-craft positional encoding based on sinusoid function is proposed.",
    "curr": "But learnable positional encoding, i.e., treating p i as parameters, is often used in the recent works Devlin et al., 2018;Clark et al., 2019b).",
    "next": "Relative Positional Encoding."
  },
  {
    "index": 450,
    "source_corpus_id": 60440615,
    "ref_id": "b24",
    "citation_corpus_id": 16631020,
    "start": 1779,
    "end": 1798,
    "title": "Transfer Learning for Low-Resource Neural Machine Translation",
    "abstract": "The encoder-decoder framework for neural machine translation (NMT) has been shown effective in large data scenarios, but is much less effective for low-resource languages. We present a transfer learning method that significantly improves BLEU scores across a range of low-resource languages. Our key idea is to first train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training. Using our transfer learning method we improve baseline NMT models by an average of 5.6 BLEU on four low-resource language pairs. Ensembling and unknown word replacement add another 2 BLEU which brings the NMT performance on low-resource machine translation close to a strong syntax based machine translation (SBMT) system, exceeding its performance on one language pair. Additionally, using the transfer learning model for re-scoring, we can improve the SBMT system by an average of 1.3 BLEU, improving the state-of-the-art on low-resource machine translation.",
    "prev": "Experiments on a standard dataset of four low-resource languages show consistent improvements over strong multilingual NMT baselines, with gains of up to 2 BLEU on one of the tested languages, achieving the new state-of-the-art on all four language pairs 1 .",
    "curr": "INTRODUCTION\n\nMultilingual Neural Machine Translation (NMT) has shown great potential both in creating parameter-efficient MT systems for many languages (Johnson et al., 2016), and in improving translation quality of low-resource languages (Zoph et al., 2016;Firat et al., 2016;Gu et al., 2018;Neubig & Hu, 2018;Nguyen & Chiang, 2018).",
    "next": "Despite the success of multilingual NMT, it remains a research question how to represent the words from multiple languages in a way that is both parameter efficient and conducive to cross-lingual generalization."
  },
  {
    "index": 451,
    "source_corpus_id": 252762329,
    "ref_id": "b14",
    "citation_corpus_id": 238418995,
    "start": 7526,
    "end": 7544,
    "title": "LARGE LEARNING RATE TAMES HOMOGENEITY: CONVERGENCE AND BALANCING EFFECT",
    "abstract": "Recent empirical advances show that training deep models with large learning rate often improves generalization performance. However, theoretical justifications on the benefits of large learning rate are highly limited, due to challenges in analysis. In this paper, we consider using Gradient Descent (GD) with a large learning rate on a homogeneous matrix factorization problem, i.e., min X,Y A − XY 2 F . We prove a convergence theory for constant large learning rates well beyond 2/L, where L is the largest eigenvalue of Hessian at the initialization. Moreover, we rigorously establish an implicit bias of GD induced by such a large learning rate, termed 'balancing', meaning that magnitudes of X and Y at the limit of GD iterations will be close even if their initialization is significantly unbalanced. Numerical experiments are provided to support our theory.Published as a conference paper at ICLR 2022 Problem (1) possesses several intriguing properties. Firstly, the objective function is non-convex, and critical points are either global minima or saddles (see e.g.,Baldi & Hornik (1989); Li et al. (2019b); Valavi et al. (2020a); Chen et al. (2018)). Secondly, problem (1) is homogeneous in X and Y , meaning that rescaling X, Y to aX, a −1 Y for any a = 0 will not change the objective's value. This property is shared by commonly used ReLU neural networks. A direct consequence of homogeneity is that global minima of (1) are non-isolated and can be unbounded. The curvatures at these global minima are highly dependent on the magnitudes of X, Y . When X, Y have comparable magnitudes, the largest eigenvalue of Hessian is small, and this corresponds to a flat minimum; on the contrary, unbalanced X and Y give a sharp minimum. Last but not the least, the homogeneity impairs smoothness conditions of (1), rendering the gradient being not Lipschitz continuous unless X, Y are bounded. See a formal discussion in Section 2.Existing approaches for solving (1) often uses explicit regularization(Ge et al., 2017;Tu et al., 2016;Cabral et al., 2013;Li et al., 2019a), or infinitesimal (or diminishing) learning rates for controlling the magnitudes of X, Y (Du et al., 2018; Ye & Du, 2021). In this paper, we go beyond the scope of aforementioned works, and analyze GD with a large learning rate for solving (1). In particular, we allow the learning rate h to be as large as approximately 4/L (see more explanation in Section 2), where L denotes the largest eigenvalue of Hessian at GD initialization. In connection to empirical observations, we provide positive answers to the following two questions:Does GD with large learning rate converge at least for some cases of (1)? Does larger learning rate biases toward flatter minima (i.e., X, Y with comparable magnitudes)?We remark that having a learning rate h ≈ 4/L is far beyond the commonly analyzed regime in optimization. Even for globally L-smooth objective, traditional theory requires h < 2/L for GD convergence and h = 1/L is optimal for convex functions(Boyd et al., 2004), not to mention that our problem (1) is never globally L-smooth due to homogeneity. Modified equation provides a tool for probing intermediate learning rates (see Hairer et al. (2006, Chapter 9) for a general review, and Kong & Tao (2020, Appendix A) for the specific setup of GD), but the learning rate here is too large for modified equation to work (see Appendix C). In fact, besides blowing up, GD with large learning rate may have a zoology of limiting behaviors (see e.g., Appendix B for convergence to periodic orbits under our setup, and Kong & Tao (2020) for convergence to chaotic attractors).Our analyses (of convergence and balancing) leverage various mathematical tools, including a proper partition of state space and its dynamical transition (specifically invented for this problem), stability theory of discrete time dynamical systems(Alligood et al., 1996), and geometric measure theory(Federer, 2014).The rest of the paper is organized as: Section 2 provides the background of studying (1) and discusses related works; Section 3 presents convergence and balancing results for scalar factorization problems; Section 4 generalizes the theory to rank-1 matrix approximation; Section 5 studies problem (1) with arbitrary A and its arbitrary-rank approximation; Section 6 summarizes the paper and discusses broadly related topics and future directions.",
    "prev": "They show that the loss can non-monotonically decrease even when the sharpness λ > 2/η.",
    "curr": "The non-monotone property of the loss has also been observed in many other settings (Jastrzebski et al., 2020;Xing et al., 2018;Lewkowycz et al., 2020;Wang et al., 2022;Arora et al., 2018;Li et al., 2022a).",
    "next": "Recently several works try to understand the mechanism behind EoS with different loss functions under various assumptions (Ahn et al., 2022;Ma et al., 2022;Lyu et al., 2022;Li et al., 2022b)."
  },
  {
    "index": 452,
    "source_corpus_id": 253098739,
    "ref_id": "b33",
    "citation_corpus_id": 68137503,
    "start": 3821,
    "end": 3838,
    "title": "FUNCTIONAL VARIATIONAL BAYESIAN NEURAL NETWORKS",
    "abstract": "Variational Bayesian neural networks (BNNs) perform variational inference over weights, but it is difficult to specify meaningful priors and approximate posteriors in a high-dimensional weight space. We introduce functional variational Bayesian neural networks (fBNNs), which maximize an Evidence Lower BOund (ELBO) defined directly on stochastic processes, i.e. distributions over functions. We prove that the KL divergence between stochastic processes equals the supremum of marginal KL divergences over all finite sets of inputs. Based on this, we introduce a practical training objective which approximates the functional ELBO using finite measurement sets and the spectral Stein gradient estimator. With fBNNs, we can specify priors entailing rich structures, including Gaussian processes and implicit stochastic processes. Empirically, we find fBNNs extrapolate well using various structured priors, provide reliable uncertainty estimates, and scale to large datasets. * Equal contribution.Theorem 7. For two stochastic processes P, M on a cylindrical measurable space (Ω T , F T ), the KL divergence of P with respect to M satisfies,where the supremum is over all finite indices subsets T d ⊆ T , and P T d , M T d represent the canonical projection maps π T →T d of P, M , respectively.Proof. Recall that stochastic processes are defined over a cylindrical σ-algebra F T . By Lemma 6, for every set H ∈ F T , the restricted index set τ (H) is countable. Our proof proceeds in two steps:",
    "prev": "To address these shortcomings, we take a new approach to formulating the meta-learning problem and represent prior knowledge in a novel way.",
    "curr": "We build on recent advances in functional approximate inference for BNNs that perform Bayesian inference in the function space rather than in the parameter space of neural networks (Wang et al., 2018;Sun et al., 2019).",
    "next": "When viewing the BNN prior and posterior as stochastic processes, the perfect Bayesian prior is the (true) data-generating stochastic process itself."
  },
  {
    "index": 453,
    "source_corpus_id": 251710555,
    "ref_id": "b6",
    "citation_corpus_id": 52967399,
    "start": 2072,
    "end": 2093,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Compared to Linformer, the accuracy can be as much as 12% higher while using similar FLOPs in the attention layer.",
    "curr": "INTRODUCTION\n\nSelf attention layer is the key component of Transformers (Vaswani et al., 2017), enabling them to achieve state of the art performance across tasks in Natural Language Processing (Devlin et al., 2019;Raffel et al., 2019) and Computer Vision (Dosovitskiy et al., 2021).",
    "next": "Attention computation scales quadratically (n 2 ) with the input sequence length (n), making it a key bottleneck in scaling Transformers to long inputs."
  },
  {
    "index": 457,
    "source_corpus_id": 248266388,
    "ref_id": "b9",
    "citation_corpus_id": 14717992,
    "start": 1555,
    "end": 1575,
    "title": "REINFORCEMENT LEARNING WITH UNSUPERVISED AUXILIARY TASKS",
    "abstract": "Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-theart on Atari, averaging 880% expert human performance, and a challenging suite of first-person, three-dimensional Labyrinth tasks leading to a mean speedup in learning of 10× and averaging 87% expert human performance on Labyrinth.Natural and artificial agents live in a stream of sensorimotor data. At each time step t, the agent receives observations o t and executes actions a t . These actions influence the future course of the sensorimotor stream. In this paper we develop agents that learn to predict and control this stream, by solving a host of reinforcement learning problems, each focusing on a distinct feature of the sensorimotor stream. Our hypothesis is that an agent that can flexibly control its future experiences will also be able to achieve any goal with which it is presented, such as maximising its future rewards.The classic reinforcement learning paradigm focuses on the maximisation of extrinsic reward. However, in many interesting domains, extrinsic rewards are only rarely observed. This raises questions of what and how to learn in their absence. Even if extrinsic rewards are frequent, the sensorimotor stream contains an abundance of other possible learning targets. Traditionally, unsupervised learning attempts to reconstruct these targets, such as the pixels in the current or subsequent frame. It is typically used to accelerate the acquisition of a useful representation. In contrast, our learning objective is to predict and control features of the sensorimotor stream, by treating them as pseudorewards for reinforcement learning. Intuitively, this set of tasks is more closely matched with the agent's long-term goals, potentially leading to more useful representations.Consider a baby that learns to maximise the cumulative amount of red that it observes. To correctly predict the optimal value, the baby must understand how to increase \"redness\" by various means, including manipulation (bringing a red object closer to the eyes); locomotion (moving in front of a red object); and communication (crying until the parents bring a red object). These behaviours are likely to recur for many other goals that the baby may subsequently encounter. No understanding of these behaviours is required to simply reconstruct the redness of current or subsequent images.Our architecture uses reinforcement learning to approximate both the optimal policy and optimal value function for many different pseudo-rewards. It also makes other auxiliary predictions that serve to focus the agent on important aspects of the task. These include the long-term goal of predicting cumulative extrinsic reward as well as short-term predictions of extrinsic reward. To learn more efficiently, our agents use an experience replay mechanism to provide additional updates * Joint first authors. Ordered alphabetically by first name.",
    "prev": "* Correspondence to clare.",
    "curr": "INTRODUCTION\n\nDeep reinforcement learning has achieved remarkable successes in a variety of tasks (Mnih et al., 2015;Moravčík et al., 2017;Silver et al., 2017;Abreu et al., 2019), but its impressive performance is mirrored by its brittleness and sensitivity to seemingly innocuous design choices (Henderson et al., 2018).",
    "next": "In sparse-reward environments in particular, even different random seeds of the same algorithm can attain dramatically different performance outcomes."
  },
  {
    "index": 458,
    "source_corpus_id": 221447287,
    "ref_id": "b36",
    "citation_corpus_id": 49882757,
    "start": 2654,
    "end": 2672,
    "title": "ClariNet: Parallel Wave Generation in End-to-End Text-to-Speech",
    "abstract": "In this work, we propose an alternative solution for parallel wave generation by WaveNet. In contrast to parallel WaveNet (Oord et al., 2018), we distill a Gaussian inverse autoregressive flow from the autoregressive WaveNet by minimizing a novel regularized KL divergence between their highly-peaked output distributions. Our method computes the KL divergence in closed-form, which simplifies the training algorithm and provides very efficient distillation. In addition, we propose the first text-to-wave neural architecture for speech synthesis, which is fully convolutional and enables fast end-to-end training from scratch. It significantly outperforms the previous pipeline that connects a text-to-spectrogram model to a separately trained WaveNet (Ping et al., 2018). We also successfully distill a parallel waveform synthesizer conditioned on the hidden representation in this end-to-end model. 2 * These authors contributed equally to this work. Correspondence to <weiping.thu@gmail.com>. Our method is named after the musical instrument clarinet, whose sound resembles human voice.2 Audio samples are in https://clarinet-demo.github.io/ arXiv:1807.07281v2 [cs.CL] 30 Jul 2018",
    "prev": "However, autoregressive models require a large number of sequential computational steps to generate one audio sample, which makes it challenging to deploy these models in real-world production applications, such as digital voice assistants on smart speakers, even using specialized hardware designed for neural networks.",
    "curr": "There has been a plethora of research into non-autoregressive models for audio generation, including normalizing flow (NF) models like inverse autoregressive flow (IAF) Ping et al., 2019), generative flow (Glow) (Prenger et al., 2019;Kim et al., 2019), and continuous normalizing flow (CNF) Wu & Ling, 2020), implicit generative models like generative adversarial network (GAN) (Donahue et al., 2018;Engel et al., 2019;Yamamoto et al., 2020;Bikowski et al., 2020;McCarthy & Ahmed, 2020) and energy score (Gritsenko et al., 2020), variational models like variational auto-encoder (VAE) (Peng et al., 2020), models inspired by digital signal processing (Ai & Ling, 2020;Engel et al., 2020), and those by the speech production mechanism (Juvela et al., 2019;Wang et al., 2020).",
    "next": "Although these models improve inference speed thanks to their architectures offering less sequential operations, they often yield lower quality samples than the autoregressive models."
  },
  {
    "index": 460,
    "source_corpus_id": 48361056,
    "ref_id": "b7",
    "citation_corpus_id": 848112,
    "start": 4514,
    "end": 4539,
    "title": "TRAINING GANS WITH OPTIMISM",
    "abstract": "We address the issue of limit cycling behavior in training Generative Adversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for training Wasserstein GANs. Recent theoretical results have shown that optimistic mirror decent (OMD) can enjoy faster regret rates in the context of zero-sum games. WGANs is exactly a context of solving a zero-sum game with simultaneous noregret dynamics. Moreover, we show that optimistic mirror decent addresses the limit cycling problem in training WGANs. We formally show that in the case of bi-linear zero-sum games the last iterate of OMD dynamics converges to an equilibrium, in contrast to GD dynamics which are bound to cycle. We also portray the huge qualitative difference between GD and OMD dynamics with toy examples, even when GD is modified with many adaptations proposed in the recent literature, such as gradient penalty or momentum. We apply OMD WGAN training to a bioinformatics problem of generating DNA sequences. We observe that models trained with OMD achieve consistently smaller KL divergence with respect to the true underlying distribution, than models trained with GD variants. Finally, we introduce a new algorithm, Optimistic Adam, which is an optimistic variant of Adam. We apply it to WGAN training on CIFAR10 and observe improved performance in terms of inception score as compared to Adam.",
    "prev": "Searching for a more stable function family (Radford et al., 2015) and utilizing better objectives Gulrajani et al., 2017) have been explored for training and instability issues.",
    "curr": "For tackling non-convergence, regularizing objective functions (Mescheder et al., 2017;Salimans et al., 2016), two time-scale techniques (Heusel et al., 2017) and extra-gradient (optimistic) methods (Daskalakis et al., 2018;Mertikopoulos et al., 2019) have been studied.",
    "next": "One reason for non-convergence is cycling around an optimal solution (Mertikopoulos et al., 2018;, or even slow outward spiraling (Bailey & Piliouras, 2018)."
  },
  {
    "index": 463,
    "source_corpus_id": 231719730,
    "ref_id": "b5",
    "citation_corpus_id": 213152193,
    "start": 20388,
    "end": 20408,
    "title": "ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS",
    "abstract": "Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK]  and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.",
    "prev": "Unlike the training datasets, the evaluation document was translated to plain L A T E X manually using the PDF as a reference, in order to avoid possible spurious patterns in automatically expanded sT E X.",
    "curr": "S T E X-ANNOTATING WITH MACHINE LEARNING AS AN NMT TASK\n\nIn the course of our experiments, we considered our disambiguation task as a machine translation (NMT) problem, the models for which have been proven to be quite effective even beyond natural language translations (Clark et al., 2020).",
    "next": "In fact, the autoformalization projects mentiond in Section 3, which are spiritually closest to our task, all used NMT models with positive results."
  },
  {
    "index": 465,
    "source_corpus_id": 58014184,
    "ref_id": "b2",
    "citation_corpus_id": 11212020,
    "start": 10881,
    "end": 10904,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "The queried values are invariant to the ordering of the key-value pairs; this permutation invariance property of attention is key in its application to NPs.",
    "curr": "The idea of using a differentiable addressing mechanism that can be learned from the data has been applied successfully in various areas of Deep Learning, namely handwriting generation and recognition (Graves, 2012) and neural machine translation (Bahdanau et al., 2015).",
    "next": "More recently, there has been work employing self-attention (where keys and queries are identical) to give expressive sequence-to-sequence mappings in natural language processing (Vaswani et al., 2017) and image modelling (Parmar et al., 2018)."
  },
  {
    "index": 470,
    "source_corpus_id": 264406180,
    "ref_id": "b6",
    "citation_corpus_id": 229923720,
    "start": 17860,
    "end": 17880,
    "title": "Transformer Feed-Forward Layers Are Key-Value Memories",
    "abstract": "Feed-forward layers constitute two-thirds of a transformer model's parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformerbased language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys' input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model's layers via residual connections to produce the final output distribution.",
    "prev": "Mean ablating the 1st or 2nd head slightly increased the average loss for BA questions from 0.05 to 0.08, whereas ablating the 3rd head substantially increased the loss to 3.7, confirming that the 3rd head is doing the BA task.",
    "curr": "The MLP can be thought of as a \"key-value pair\" memory [Meng et al., 2022, Geva et al., 2021] that can hold many bigrams and trigrams.We claim our MLP pulls together the two-state 1st head result, the tri-state 2nd head result and the ten-state 3rd head result value, treating them as a trigram with 60 (2 x 3 x 10) possible keys.For each digit, the MLP has memorised the mapping of these 60 keys to the 60 correct digit answers (0 to 9).We haven't proven this experimentally.Our MLP is sufficiently large to store this many mappings with zero interference between mappings [Elhage et al., 2022].",
    "next": "Despite being feasible, the model does not calculate the task MC1 in layers 7 to 11.Instead it completes each digit calculation in 1 layer, possibly because there are training optimisation benefits in generating a \"compact\" algorithm."
  },
  {
    "index": 471,
    "source_corpus_id": 59413817,
    "ref_id": "b12",
    "citation_corpus_id": 3286670,
    "start": 1973,
    "end": 1995,
    "title": "EXPRESSIVE POWER OF RECURRENT NEURAL NET- WORKS",
    "abstract": "Deep neural networks are surprisingly efficient at solving practical tasks, but the theory behind this phenomenon is only starting to catch up with the practice. Numerous works show that depth is the key to this efficiency. A certain class of deep convolutional networks -namely those that correspond to the Hierarchical Tucker (HT) tensor decomposition -has been proven to have exponentially higher expressive power than shallow networks. I.e. a shallow network of exponential width is required to realize the same score function as computed by the deep architecture. In this paper, we prove the expressive power theorem (an exponential lower bound on the width of the equivalent shallow network) for a class of recurrent neural networks -ones that correspond to the Tensor Train (TT) decomposition. This means that even processing an image patch by patch with an RNN can be exponentially more efficient than a (shallow) convolutional network with one hidden layer. Using theoretical results on the relation between the tensor decompositions we compare expressive powers of the HT-and TT-Networks. We also implement the recurrent TT-Networks and provide numerical evidence of their expressivity.",
    "prev": "The ability of these neural networks to efficiently represent a rich class of functions with a relatively small number of parameters is often referred to as depth efficiency, and the theory behind this phenomenon is not yet fully understood.",
    "curr": "A recent line of work Khrulkov et al., 2018;Cohen et al., 2018) focuses on comparing various deep learning architectures in terms of their expressive power.",
    "next": "It was shown in  that ConvNets with product pooling are exponentially more expressive than shallow networks, that is there exist functions realized by ConvNets which require an exponentially large number of parameters in order to be realized by shallow nets."
  },
  {
    "index": 473,
    "source_corpus_id": 238419359,
    "ref_id": "b5",
    "citation_corpus_id": 189762527,
    "start": 2107,
    "end": 2129,
    "title": "COMET : Commonsense Transformers for Automatic Knowledge Graph Construction",
    "abstract": "We present the first comprehensive study on automatic knowledge base construction for two prevalent commonsense knowledge graphs: ATOMIC (Sap et al., 2019) and Con-ceptNet (Speer et al., 2017). Contrary to many conventional KBs that store knowledge with canonical templates, commonsense KBs only store loosely structured open-text descriptions of knowledge. We posit that an important step toward automatic commonsense completion is the development of generative models of commonsense knowledge, and propose COMmonsEnse Transformers (COMET ) that learn to generate rich and diverse commonsense descriptions in natural language. Despite the challenges of commonsense modeling, our investigation reveals promising results when implicit knowledge from deep pre-trained language models is transferred to generate explicit knowledge in commonsense knowledge graphs. Empirical results demonstrate that COMET is able to generate novel knowledge that humans rate as high quality, with up to 77.5% (ATOMIC) and 91.7% (ConceptNet) precision at top 1, which approaches human performance for these resources. Our findings suggest that using generative commonsense models for automatic commonsense KB completion could soon be a plausible alternative to extractive methods.",
    "prev": "Such knowledge can be implicitly encoded or explicitly stored in structured knowledge graphs (KGs).",
    "curr": "Large pre-trained language models (Devlin et al., 2018;Radford et al., 2018;Brown et al., 2020) are found to be effective in learning broad and rich implicit knowledge (Petroni et al., 2019;Bosselut et al., 2019;Talmor et al., 2020) and thus demonstrate much success for QA tasks.",
    "next": "Nevertheless, pretrained LMs struggle a lot with structured reasoning such as handling negation (Ribeiro et al., 2020;Yasunaga et al., 2021)."
  },
  {
    "index": 475,
    "source_corpus_id": 244714829,
    "ref_id": "b10",
    "citation_corpus_id": 85501317,
    "start": 27512,
    "end": 27529,
    "title": "Cyclical Annealing Schedule: A Simple Approach to Mitigating KL Vanishing",
    "abstract": "Variational autoencoders (VAEs) with an autoregressive decoder have been applied for many natural language processing (NLP) tasks. The VAE objective consists of two terms, (i) reconstruction and (ii) KL regularization, balanced by a weighting hyper-parameter β. One notorious training difficulty is that the KL term tends to vanish. In this paper we study scheduling schemes for β, and show that KL vanishing is caused by the lack of good latent codes in training the decoder at the beginning of optimization. To remedy this, we propose a cyclical annealing schedule, which repeats the process of increasing β multiple times. This new procedure allows the progressive learning of more meaningful latent codes, by leveraging the informative representations of previous cycles as warm re-starts. The effectiveness of cyclical annealing is validated on a broad range of NLP tasks, including language modeling, dialog response generation and unsupervised language pre-training.",
    "prev": "EXPERIMENTS\n\n\nEXPERIMENTAL SETTINGS\n\nWe train the conditional VAE described in Section 4.2 and all implicit functions using the Adam solver (Kingma & Ba, 2014) with a learning rate of 5 × 10 −5 .",
    "curr": "To balance the KL-divergence and the reconstruction objectives in the VAE, we adopt the Cyclical Annealing Schedule introduced in (Fu et al., 2019).",
    "next": "We evaluate the proposed environment field on agent navigation in 2D mazes (Section 5.2) and compare with the VIN (Tamar et al., 2016)."
  },
  {
    "index": 476,
    "source_corpus_id": 253510295,
    "ref_id": "b2",
    "citation_corpus_id": 53729760,
    "start": 3778,
    "end": 3780,
    "title": "GAN DISSECTION: VISUALIZING AND UNDERSTANDING GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, they have not been well visualized or understood. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models. In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts using a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. We examine the contextual relationship between these units and their surroundings by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in a scene. We provide open source interpretation tools to help researchers and practitioners better understand their GAN models * . * Interactive demos, video, code, and data are available at GitHub and gandissect.",
    "prev": "6\n\nINTRODUCTION\n\nSemantic image synthesis refers to generating photo-realistic images conditioned on pixel-level semantic labels.",
    "curr": "This task has a wide range of applications such as image editing and content generation (Chen & Koltun, 2017;Isola et al., 2017;Guo et al., 2022;Gu et al., 2019;Bau et al., 2019a;b;Liu et al., 2019;Qi et al., 2018;Jiang et al., 2020).",
    "next": "Although existing methods conducted interesting explorations, we still observe unsatisfactory aspects, mainly in the generated local structures and details, as well as small-scale objects, which we believe are mainly due to three reasons: 1) Conventional methods (Park et al., 2019;Wang et al., 2018;Liu et al., 2019) generally take the semantic label map as input directly."
  },
  {
    "index": 477,
    "source_corpus_id": 252873224,
    "ref_id": "b22",
    "citation_corpus_id": 189898036,
    "start": 2669,
    "end": 2675,
    "title": "Gradient Descent Maximizes the Margin of Homogeneous Neural Networks",
    "abstract": "Recent works on implicit regularization have shown that gradient descent converges to the max-margin direction for logistic regression with one-layer or multi-layer linear networks. In this paper, we generalize this result to homogeneous neural networks, including fully-connected and convolutional neural networks with ReLU or LeakyReLU activations. In particular, we study the gradient flow (gradient descent with infinitesimal step size) optimizing the logistic loss or cross-entropy loss of any homogeneous model (possibly non-smooth), and show that if the training loss decreases below a certain threshold, then we can define a smoothed version of the normalized margin which increases over time. We also formulate a natural constrained optimization problem related to margin maximization, and prove that both the normalized margin and its smoothed version converge to the objective value at a KKT point of the optimization problem. Furthermore, we extend the above results to a large family of loss functions. We conduct several experiments to justify our theoretical finding on MNIST and CIFAR-10 datasets. For gradient descent with constant learning rate, we observe that the normalized margin indeed keeps increasing after the dataset is fitted, but the speed is very slow. However, if we schedule the learning rate more carefully, we can observe a more rapid growth of the normalized margin. Finally, as margin is closely related to robustness, we discuss potential benefits of training longer for improving the robustness of the model. model fits the training data perfectly). For example, given a Convolutional Neural Network (CNN) that has achieved 100% training accuracy, one can easily make the cross-entropy loss arbitrarily small by scaling up the weight and bias parameters (W , b) at the last layer, i.e., transforming (W , b) to (cW , cb) for large enough c > 0. This means that, similar to linear logistic regression, CNNs also have some parameters whose scale does not matter, and hence a promising and meaningful research direction is to study whether their convergent direction maximizes the margin. In general, we observe that the following three properties that are usually satisfied by modern deep neural networks:1. Partial Homogeneity. The output of the neural network is (positively) homogeneous with respect to a part of its parameters (e.g., the parameters at the last linear layer);2. Separability. The training set is separable by the neural network for some set of parameters, i.e., the neural network has sufficient representation power to achieve 100% training accuracy (this is true for state-of-the-art CNNs for image classification, and many of them even have enough capacity to fit randomly labeled data easily [Zhang et al., 2017]);3. No finite minima on the loss function. The loss function used to measure the similarity between the network output and ground-truth is lower bounded by a constant (e.g., 0) but it does not have finite minima (e.g., exponential loss, logistic loss, cross-entropy loss).Simplifications. For simplicity and ease of presentation, we make the following simplifications. First, as the most prominent examples of homogeneous neural networks are all non-smooth (e.g., ReLU networks), we turn to analyze the case of training neural networks by gradient flow (more precisely, subgradient flow in Clarke's sense).Second, we ensure Separability as follows: we assume that after time t 0 , the training loss is smaller than a threshold, and the threshold here is chosen to be so small that the training accuracy is guaranteed to be 100% (e.g., for the logistic loss and cross-entropy loss, the threshold can be set to ln 2). In this paper, we focus on analyzing the behavior of the network after t 0 .",
    "prev": "For gradient flow, we consider the standard leaky ReLU activation, φ(z) = max(γz, z).",
    "curr": "Our starting point in this setting is recent work by Lyu and Li [LL20] and Ji and Telgarsky [JT20] that show that, provided the network interpolates the training data at some time, gradient flow on homogeneous networks, such as two-layer leaky ReLU networks, converges (in direction) to a network that satisfies the Karush-Kuhn-Tucker (KKT) conditions for the margin-maximization problem,\nmin W 1 2 W 2 F s.t.",
    "next": "∀i ∈ [n], y i f (x i ; W ) ≥ 1 ."
  },
  {
    "index": 478,
    "source_corpus_id": 14298291,
    "ref_id": "b21",
    "citation_corpus_id": 806709,
    "start": 3895,
    "end": 3916,
    "title": "Semantic Compositionality through Recursive Matrix-Vector Spaces",
    "abstract": "Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.",
    "prev": "(2014), who focus on learning expression representations to aid the search for computationally efficient identities.",
    "curr": "They use recursive neural networks (TREENN) 1 (Socher et al., 2012) for modeling homogenous, single-variable polynomial expressions.",
    "next": "While they present impressive results, we find that the TREENN model fails when applied to more complex symbolic polynomial and boolean expressions."
  },
  {
    "index": 479,
    "source_corpus_id": 228705808,
    "ref_id": "b12",
    "citation_corpus_id": 208248131,
    "start": 2018,
    "end": 2037,
    "title": "Published as a conference paper at ICLR 2020 ECONOMY STATISTICAL RECURRENT UNITS FOR INFERRING NONLINEAR GRANGER CAUSALITY",
    "abstract": "Granger causality is a widely-used criterion for analyzing interactions in largescale networks. As most physical interactions are inherently nonlinear, we consider the problem of inferring the existence of pairwise Granger causality between nonlinearly interacting stochastic processes from their time series measurements. Our proposed approach relies on modeling the embedded nonlinearities in the measurements using a component-wise time series prediction model based on Statistical Recurrent Units (SRUs). We make a case that the network topology of Granger causal relations is directly inferrable from a structured sparse estimate of the internal parameters of the SRU networks trained to predict the processes' time series measurements. We propose a variant of SRU, called economy-SRU, which, by design has considerably fewer trainable parameters, and therefore less prone to overfitting. The economy-SRU computes a low-dimensional sketch of its high-dimensional hidden state in the form of random projections to generate the feedback for its recurrent processing. Additionally, the internal weight parameters of the economy-SRU are strategically regularized in a group-wise manner to facilitate the proposed network in extracting meaningful predictive features that are highly time-localized to mimic real-world causal events. Extensive experiments are carried out to demonstrate that the proposed economy-SRU based time series prediction model outperforms the MLP, LSTM and attention-gated CNN-based time series models considered previously for inferring Granger causality. . Towards a rigorous assessment of systems biology models: The DREAM3 challenges. PLOS ONE, 5(2):1-18, Feb 2010. . Earth system modeling 2.0: A blueprint for models that learn from observations and targeted high-resolution simulations. . Scalable matrix-valued kernel learning for high-dimensional nonlinear multivariate regression and Granger causality. In Pan. Estimating brain connectivity with varying-length time lags using a recurrent neural network.",
    "prev": "INTRODUCTION\n\nGranger causality (GC) (Granger, 1969) is a popular practical approach for the analysis of multivariate time series and has become instrumental in exploratory analysis (McCracken, 2016) in various disciplines, such as neuroscience (Roebroeck et al., 2005), economics (Appiah, 2018), and climatology (Charakopoulos et al., 2018).",
    "curr": "Recently, the focus of the methodological research has been on inferring GC under nonlinear dynamics (Tank et al., 2018;Nauta et al., 2019;Wu et al., 2020;Khanna & Tan, 2020;Löwe et al., 2020), causal structures varying across replicates (Löwe et al., 2020), and unobserved confounding (Nauta et al., 2019;Löwe et al., 2020).",
    "next": "To the best of our knowledge, the latest powerful techniques for inferring GC do not target the effect sign detection (see Section 2.1 for a formal definition) or exploration of effect variability with time and, thus, have limited interpretability."
  },
  {
    "index": 481,
    "source_corpus_id": 256846551,
    "ref_id": "b19",
    "citation_corpus_id": 67855860,
    "start": 2396,
    "end": 2418,
    "title": "Attention is not Explanation",
    "abstract": "Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful \"explanations\" for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do. Code to reproduce all experiments is available at https://github.com/successar/ AttentionExplanation.",
    "prev": "Some recent works in the field of question-answering (QA) have demonstrated that language models can bypass some of these issues and learn to reason directly over natural language , allowing for more flexible and adaptable reasoning capabilities.",
    "curr": "Another advantage of performing multi-step reasoning over natural language is that it allows for more inspectable outputs, improving the explainability of models that are otherwise regarded as black box systems (Jain & Wallace, 2019;Rajani et al., 2019a;Danilevsky et al., 2020).",
    "next": "Despite the recent progress, we notice that there is still a gap in resources for training and evaluating general reasoning capabilities over natural language."
  },
  {
    "index": 482,
    "source_corpus_id": 252683429,
    "ref_id": "b0",
    "citation_corpus_id": 245334722,
    "start": 3939,
    "end": 3958,
    "title": "TRANSFORMERS CAN DO BAYESIAN INFERENCE",
    "abstract": "Currently, it is hard to reap the benefits of deep learning for Bayesian methods, which allow the explicit specification of prior knowledge and accurately capture model uncertainty. We present Prior-Data Fitted Networks (PFNs). PFNs leverage large-scale machine learning techniques to approximate a large set of posteriors. The only requirement for PFNs to work is the ability to sample from a prior distribution over supervised learning tasks (or functions). Our method restates the objective of posterior approximation as a supervised classification problem with a set-valued input: it repeatedly draws a task (or function) from the prior, draws a set of data points and their labels from it, masks one of the labels and learns to make probabilistic predictions for it based on the set-valued input of the rest of the data points. Presented with a set of samples from a new supervised learning task as input, PFNs make probabilistic predictions for arbitrary other data points in a single forward propagation, having learned to approximate Bayesian inference. We demonstrate that PFNs can near-perfectly mimic Gaussian processes and also enable efficient Bayesian inference for intractable problems, with over 200-fold speedups in multiple setups compared to current methods. We obtain strong results in very diverse areas such as Gaussian process regression, Bayesian neural networks, classification for small tabular data sets, and few-shot image classification, demonstrating the generality of PFNs. Code and trained PFNs are released at https://github. com/automl/TransformersCanDoBayesianInference.",
    "prev": "Instead, we replace this step by performing a single forward pass with a large Transformer that has been pre-trained to solve artificially generated classification tasks from a tabular dataset prior.",
    "curr": "Our method builds on Prior-Data Fitted Networks (PFNs; Müller et al., 2022; see Section 2), which learn the training and prediction algorithm itself.",
    "next": "PFNs approximate Bayesian inference given any prior one can sample from and approximate the posterior predictive distribution (PPD) directly."
  },
  {
    "index": 483,
    "source_corpus_id": 53094405,
    "ref_id": "b15",
    "citation_corpus_id": 11445252,
    "start": 8928,
    "end": 8952,
    "title": "LEARNING FEATURES OF MUSIC FROM SCRATCH",
    "abstract": "We introduce a new large-scale music dataset, MusicNet, to serve as a source of supervision and evaluation of machine learning methods for music research. Mu-sicNet consists of hundreds of freely-licensed classical music recordings by 10 composers, written for 11 instruments, together with instrument/note annotations resulting in over 1 million temporal labels on 34 hours of chamber music performances under various studio and microphone conditions. We define a multi-label classification task to predict notes in musical recordings, along with an evaluation protocol. We benchmark several machine learning architectures for this task: i) learning from \"hand-crafted\" spectrogram features; ii) end-to-end learning with a neural net; iii) end-to-end learning with a convolutional neural net. We show that several end-to-end learning proposals outperform approaches based on learning from hand-crafted audio features.",
    "prev": "Repertoire is mostly classical, including composers from the 17 th to early 20 th century.",
    "curr": "MusicNet (Thickstun et al., 2017) contains recordings of human performances, but separatelysourced scores.",
    "next": "As discussed in , the alignment between audio and score is not fully accurate."
  },
  {
    "index": 485,
    "source_corpus_id": 252693111,
    "ref_id": "b26",
    "citation_corpus_id": 6628106,
    "start": 17840,
    "end": 17858,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "We resize the training batch to 32 for image SR, and 8 for image denoising and JPEG CAR in order to make a fair comparison.",
    "curr": "We choose ADAM Kingma & Ba (2015) to optimize our ART model with β 1 = 0.9, β 2 = 0.999, and zero weight decay.",
    "next": "The initial learning rate is set as 2×10 −4 and is reduced by half as the training iteration reaches a certain number."
  },
  {
    "index": 490,
    "source_corpus_id": 254044229,
    "ref_id": "b14",
    "citation_corpus_id": 225039882,
    "start": 3225,
    "end": 3251,
    "title": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. 1 arXiv:2010.11929v2 [cs.CV] 3 Jun 2021Published as a conference paper at ICLR 2021 inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.However, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks.RELATED WORKTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global selfattention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.",
    "prev": "Recently, Saunshi et al.",
    "curr": "(2022) argue that the pre-training loss is not the only contributor to the performance of self-supervised learning, and that previous works which view neural networks as a black box cannot tell apart the differences in downstream performance between architectures (e.g., ResNet (He et al., 2015) vs vision transformers (Dosovitskiy et al., 2020)).",
    "next": "Furthermore, selfsupervised learning with an appropriate architecture can possibly work under more general condi-tions and/or with fewer pre-training data than predicted by these results on general architecture."
  },
  {
    "index": 494,
    "source_corpus_id": 231648391,
    "ref_id": "b11",
    "citation_corpus_id": 13123084,
    "start": 8985,
    "end": 9005,
    "title": "Published as a conference paper at ICLR 2017 TEMPORAL ENSEMBLING FOR SEMI-SUPERVISED LEARNING",
    "abstract": "In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce self-ensembling, where we form a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the (non-augmented) classification error rate from 18.44% to 7.05% in SVHN with 500 labels and from 18.63% to 16.55% in CIFAR-10 with 4000 labels, and further to 5.12% and 12.16% by enabling the standard augmentations. We additionally obtain a clear improvement in CIFAR-100 classification accuracy by using random images from the Tiny Images dataset as unlabeled extra inputs during training. Finally, we demonstrate good tolerance to incorrect labels.",
    "prev": "Another technique is a consistency regularization which states that realistic perturbations of input examples from unlabeled dataset should not significantly change the output of a neural network.",
    "curr": "Consistency regularization is used in Π-model (Laine & Aila, 2017) and further improved by Temporal Ensembling (Laine & Aila, 2017) which maintains an exponential moving average prediction for each training example and Mean Teacher (Tarvainen & Valpola, 2017) that averages model weights instead of model predictions.",
    "next": "Recent methods UDA (Xie et al., 2019), ReMixMatch  and FixMatch ) use a combination of consistency loss, pseudo-labeling and advanced augmentation techniques in addition to color perturbations and spatial transformations."
  },
  {
    "index": 497,
    "source_corpus_id": 252683376,
    "ref_id": "b0",
    "citation_corpus_id": 49310753,
    "start": 24596,
    "end": 24621,
    "title": "Published as a conference paper at ICLR 2018 MAXIMUM A POSTERIORI POLICY OPTIMISATION",
    "abstract": "We introduce a new algorithm for reinforcement learning called Maximum aposteriori Policy Optimisation (MPO) based on coordinate ascent on a relativeentropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings. , et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.",
    "prev": "(Ahmed et al., 2018), and stronger robustness (Eysenbach & Levine, 2022).",
    "curr": "Generally, policies optimizing the MaxEnt RL objective sample actions that are proportional to the exponentiated reward, and alternatively can be viewed as a noise injection procedure for better exploration (Attias, 2003;Ziebart, 2010;Haarnoja et al., 2017;Nachum et al., 2017;Levine, 2018;Abdolmaleki et al., 2018;Haarnoja et al., 2018b;Vieillard et al., 2020b;Pan et al., 2022;2023;Lahlou et al., 2023).",
    "next": "However, this noise injection is commonly done directly in action space, leading to only local perturbations, whereas we inject noise through a nonlinear mapping."
  },
  {
    "index": 498,
    "source_corpus_id": 15816492,
    "ref_id": "b21",
    "citation_corpus_id": 12639289,
    "start": 1364,
    "end": 1394,
    "title": "Recurrent Continuous Translation Models",
    "abstract": "We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.",
    "prev": "Experimental results on abstractive sentence summarisation, morphological inflection, and machine translation show that noisy channel models outperform direct models, and that they significantly benefit from increased amounts of unpaired output data that direct models cannot easily use.",
    "curr": "INTRODUCTION\n\nRecurrent neural network sequence to sequence models (Kalchbrenner & Blunsom, 2013;Sutskever et al., 2014;Bahdanau et al., 2015) are excellent models of p(output sequence y | input sequence x), provided sufficient input-output (x, y) pairs are available for estimating their parameters.",
    "next": "However, in many domains, vastly more unpaired output examples are available than input-output pairs (e.g., transcribed speech is relatively rare although non-spoken texts are abundant; Swahili-English translations are rare although English texts are abundant; etc.)."
  },
  {
    "index": 499,
    "source_corpus_id": 265038424,
    "ref_id": "b14",
    "citation_corpus_id": 3366315,
    "start": 2403,
    "end": 2423,
    "title": "Published as a conference paper at ICLR 2018 SPECTRAL NORMALIZATION FOR GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. The code with Chainer (Tokui et al., 2015), generated images and pretrained models are available at https://github.com/pfnet-research/sngan_ projection.Published as a conference paper at ICLR 2018• Lipschitz constant is the only hyper-parameter to be tuned, and the algorithm does not require intensive tuning of the only hyper-parameter for satisfactory performance. • Implementation is simple and the additional computational cost is small.",
    "prev": "The original GAN is, however, highly unstable and often suffers from mode collapse.",
    "curr": "Much of recent researches has focused on improving the stability of GANs (Radford et al., 2015;Heusel et al., 2017;Miyato et al., 2018;Karras et al., 2018).",
    "next": "On the theoretical aspect, Nagarajan & Kolter (2017) proved that gradient based training of the original GAN is locally stable."
  },
  {
    "index": 500,
    "source_corpus_id": 220525352,
    "ref_id": "b5",
    "citation_corpus_id": 28202810,
    "start": 2288,
    "end": 2290,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "The success of various off-policy RL algorithms largely attributes to the use of experience replay [5-7, 15, 16].",
    "curr": "However, most off-policy RL algorithms usually adopt a random sampling [5,6,16], which treats all past experiences equally.",
    "next": "It is questionable whether this simple strategy would always choose the most effective samples for training."
  },
  {
    "index": 502,
    "source_corpus_id": 261214750,
    "ref_id": "b2",
    "citation_corpus_id": 215416146,
    "start": 2521,
    "end": 2524,
    "title": "S2ORC: The Semantic Scholar Open Research Corpus",
    "abstract": "We introduce S2ORC, 1 a large corpus of 81.1M English-language academic papers spanning many academic disciplines. The corpus consists of rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open access papers. Full text is annotated with automaticallydetected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects. In S2ORC, we aggregate papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date. We hope this resource will facilitate research and development of tools and tasks for text mining over academic text. * denotes equal contribution 1 Instructions for access to the data and model are available at https://github.com/allenai/s2orc/.2 https://arxiv.org Figure 1: Inline citations and references to figures and tables are annotated in S2ORC's structured full text. Citations are linked to bibliography entries, which are linked to other papers in S2ORC. Figure and table references are linked to their captions.3 https://www.ncbi.nlm.nih.gov/pmc 4 https://citeseerx.ist.psu.edu 5 https://www.aclweb.org/anthology 6 http://aan.how/",
    "prev": "The information of millions of academic papers can not be fully accessed because they are locked behind an unreadable format.",
    "curr": "Existing corpora, such as the S2ORC dataset [3], capture the text of 12M 2 papers using GROBID [4], but are missing meaningful representations of the mathematical equations.",
    "next": "To this end, we introduce Nougat, a transformer based model that can convert images of document pages to formatted markup text."
  },
  {
    "index": 503,
    "source_corpus_id": 247594694,
    "ref_id": "b6",
    "citation_corpus_id": 222272028,
    "start": 29905,
    "end": 29928,
    "title": "LINEAR MODE CONNECTIVITY IN MULTITASK AND CONTINUAL LEARNING",
    "abstract": "Continual (sequential) training and multitask (simultaneous) training are often attempting to solve the same overall objective: to find a solution that performs well on all considered tasks. The main difference is in the training regimes, where continual learning can only have access to one task at a time, which for neural networks typically leads to catastrophic forgetting. That is, the solution found for a subsequent task does not perform well on the previous ones anymore. However, the relationship between the different minima that the two training regimes arrive at is not well understood. What sets them apart? Is there a local structure that could explain the difference in performance achieved by the two different schemes? Motivated by recent work showing that different minima of the same task are typically connected by very simple curves of low error, we investigate whether multitask and continual solutions are similarly connected. We empirically find that indeed such connectivity can be reliably achieved and, more interestingly, it can be done by a linear path, conditioned on having the same initialization for both. We thoroughly analyze this observation and discuss its significance for the continual learning process. Furthermore, we exploit this finding to propose an effective algorithm that constrains the sequentially learned minima to behave as the multitask solution. We show that our method outperforms several state of the art continual learning algorithms on various vision benchmarks 1 . * Equal contribution 1 The code is available at: https://github.com/imirzadeh/MC-SGD arXiv:2010.04495v1 [cs.LG]",
    "prev": "More implementation details are in Appendix C.4.",
    "curr": "Moreover, we also conduct the visualization of relative loss change g over a linear path like (Mirzadeh et al., 2020) in Appendix C.4.",
    "next": "The loss change of SSL is slower than that of SL along the linear interpolation path, demonstrating the flatter minima of SSL."
  },
  {
    "index": 505,
    "source_corpus_id": 256105083,
    "ref_id": "b23",
    "citation_corpus_id": 218487034,
    "start": 1176,
    "end": 1197,
    "title": "On Faithfulness and Factuality in Abstractive Summarization",
    "abstract": "It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.",
    "prev": "Introduction\n\nLarge language models, often trained with billions of parameters, have achieved impressive performance in recent years (Raffel et al., 2019) and are used in a wide variety of natural language generation tasks.",
    "curr": "However, their output is sometimes undesirable, with hallucinated content (Maynez et al., 2020;Filippova, 2020), and much work remains to fully understand their properties.",
    "next": "In many applications, such as healthcare, question-answering systems, or customer service, incorrect predictions are particularly costly and must be avoided."
  },
  {
    "index": 506,
    "source_corpus_id": 260316137,
    "ref_id": "b16",
    "citation_corpus_id": 53112107,
    "start": 11078,
    "end": 11099,
    "title": "QUASI-HYPERBOLIC MOMENTUM AND ADAM FOR DEEP LEARNING",
    "abstract": "Momentum-based acceleration of stochastic gradient descent (SGD) is widely used in deep learning. We propose the quasi-hyperbolic momentum algorithm (QHM) as an extremely simple alteration of momentum SGD, averaging a plain SGD step with a momentum step. We describe numerous connections to and identities with other algorithms, and we characterize the set of two-state optimization algorithms that QHM can recover. Finally, we propose a QH variant of Adam called QHAdam, and we empirically demonstrate that our algorithms lead to significantly improved training in a variety of settings, including a new state-of-theart result on WMT16 EN-DE. We hope that these empirical results, combined with the conceptual and practical simplicity of QHM and QHAdam, will spur interest from both practitioners and researchers. PyTorch code is immediately available. 1 . We also offer evidence that momentum often yields negligible improvement over plain SGD.We emphasize QHM and QHAdam's efficiency and conceptual simplicity. QHM has no extra overhead vs. Nesterov's accelerated gradient, and QHAdam has very little overhead vs. Adam. Also, both algorithms are easily understood as an interpolation between two other well-known algorithms, so they are accessible to practitioners and can be tuned starting with existing practical intuitions. We believe that this contributes strongly to the algorithms' practical promise.PRELIMINARIESWe begin with notation and a brief review of stochastic gradient descent (SGD) and momentum.Primitives In this paper, θ ∈ R p denotes a vector of model parameters. L(θ) : R p → R denotes a loss function to be minimized via θ.L(θ) : R p → R denotes an approximator of the loss function (e.g. over a minibatch). ∇L denotes the gradient of function L. Unless otherwise specified, all vector operations are element-wise. We use g, a, s, v, w ∈ R p as auxiliary buffers, and g is typically the \"momentum buffer\". θ,L(·), and all buffers are subscriptable by t, the optimization step.Optimization algorithms We consider optimization algorithms that perform a sequence of steps (indexed by t), updating θ at each step towards minimizing L(θ). For brevity, we write algorithms as \"update rules\", which describe the algorithm's behavior during a single step t, rather than as full pseudocode. Update rules take this basic form (optionally with one or more auxiliary steps):Plain SGD The SGD algorithm, parameterized by learning rate α ∈ R, uses the update rule:Momentum The momentum algorithm, parameterized by α ∈ R and β ∈ R, uses the update rule:",
    "prev": "They also argued that momentum helps generalization from the lens of stability analysis for finite number of iterates, when the loss function is Lipschitz.",
    "curr": "Under the formulation of quasi-hyperbolic momentum [Ma and Yarats, 2019], Gitman et al.",
    "next": "[2019] proposed another unified analysis for momentum methods, in terms of the asymptotic and local convergence as well as the stability."
  },
  {
    "index": 507,
    "source_corpus_id": 102481167,
    "ref_id": "b3",
    "citation_corpus_id": 1844940,
    "start": 1790,
    "end": 1807,
    "title": "MAKING NEURAL PROGRAMMING ARCHITECTURES GENERALIZE VIA RECURSION",
    "abstract": "Empirically, neural networks that attempt to learn programs from data have exhibited poor generalizability. Moreover, it has traditionally been difficult to reason about the behavior of these models beyond a certain level of input complexity. In order to address these issues, we propose augmenting neural architectures with a key abstraction: recursion. As an application, we implement recursion in the Neural Programmer-Interpreter framework on four tasks: grade-school addition, bubble sort, topological sort, and quicksort. We demonstrate superior generalizability and interpretability with small amounts of training data. Recursion divides the problem into smaller pieces and drastically reduces the domain of each neural network component, making it tractable to prove guarantees about the overall system's behavior. Our experience suggests that in order for neural architectures to robustly learn program semantics, it is necessary to incorporate a concept like recursion.Published as a conference paper at ICLR 2017 reduce the complexity of programs. We find that recursion makes it easier for the network to learn the right program and generalize to unknown situations. Recursion enables provable guarantees on neural programs' behavior without needing to exhaustively enumerate all possible inputs to the programs. This paper is the first (to our knowledge) to investigate the important problem of provable generalization properties of neural programs. As an application, we incorporate recursion into the Neural Programmer-Interpreter architecture and consider four sample tasks: grade-school addition, bubble sort, topological sort, and quicksort. Empirically, we observe that the learned recursive programs solve all valid inputs with 100% accuracy after training on a very small number of examples, out-performing previous generalization results. Given verification sets that cover all the base cases and reduction rules, we can provide proofs that these learned programs generalize perfectly. This is the first time one can provide provable guarantees of perfect generalization for neural programs.",
    "prev": "INTRODUCTION\n\nAdvances in machine learning and the availability of large corpora of source code have led to growing interest in the development of neural representations of programs for performing program analyses.",
    "curr": "In particular, different representations based on token sequences (Gupta et al., 2017;Bhatia et al., 2018), program parse trees (Piech et al., 2015;Mou et al., 2016), program traces (Reed & de Freitas, 2015;Cai et al., 2017;Wang et al., 2018), and graphs (Allamanis et al., 2018) have been proposed for a variety of tasks including repair (Devlin et al., 2017b;Allamanis et al., 2018), optimization (Bunel et al., 2017), and synthesis (Parisotto et al., 2017;Devlin et al., 2017a).",
    "next": "In recent work, Allamanis et al."
  },
  {
    "index": 510,
    "source_corpus_id": 202750253,
    "ref_id": "b28",
    "citation_corpus_id": 53015027,
    "start": 7048,
    "end": 7051,
    "title": "ADAPTIVITY OF DEEP RELU NETWORK FOR LEARN- ING IN BESOV AND MIXED SMOOTH BESOV SPACES: OPTIMAL RATE AND CURSE OF DIMENSIONALITY",
    "abstract": "Deep learning has shown high performances in various types of tasks from visual recognition to natural language processing, which indicates superior flexibility and adaptivity of deep learning. To understand this phenomenon theoretically, we develop a new approximation and estimation error analysis of deep learning with the ReLU activation for functions in a Besov space and its variant with mixed smoothness. The Besov space is a considerably general function space including the Hölder space and Sobolev space, and especially can capture spatial inhomogeneity of smoothness. Through the analysis in the Besov space, it is shown that deep learning can achieve the minimax optimal rate and outperform any nonadaptive (linear) estimator such as kernel ridge regression, which shows that deep learning has higher adaptivity to the spatial inhomogeneity of the target function than other estimators such as linear ones. In addition to this, it is shown that deep learning can avoid the curse of dimensionality if the target function is in a mixed smooth Besov space. We also show that the dependency of the convergence rate on the dimensionality is tight due to its minimax optimality. These results support high adaptivity of deep learning and its superior ability as a feature extractor.",
    "prev": "Due to its significant importance in wide range of application areas, its theoretical analysis is also getting much important.",
    "curr": "For example, it has been known that the deep neural network has universal approximation capability [13,24,44] and its expressive power grows up in an exponential order against the number of layers [37,8,12,11,42,47].",
    "next": "However, theoretical understandings are still lacking in several important issues."
  },
  {
    "index": 512,
    "source_corpus_id": 247613305,
    "ref_id": "b52",
    "citation_corpus_id": 216562627,
    "start": 7606,
    "end": 7626,
    "title": "Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels",
    "abstract": "We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to regularize the value function. Existing model-free approaches, such as Soft Actor-Critic (SAC)[20], are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based[21,31,22]methods and recently proposed contrastive learning[42]. Our approach can be combined with any model-free reinforcement learning algorithm, requiring only minor modifications. An implementation can be found at https://sites.google.com/view/data-regularized-q.IntroductionSample-efficient deep reinforcement learning (RL) algorithms capable of directly training from image pixels would open up many real-world applications in control and robotics. However, simultaneously training a convolutional encoder alongside a policy network is challenging when given limited environment interaction, strong correlation between samples and a typically sparse reward signal. Naive attempts to use a large capacity encoder result in severe over-fitting (seeFigure 1a) and smaller encoders produce impoverished representations that limit task performance.Limited supervision is a common problem across AI and a number of approaches are adopted: (i) pretraining with self-supervised learning (SSL), followed by standard supervised learning; (ii) supervised learning with an additional auxiliary loss and (iii) supervised learning with data augmentation. SSL approaches are highly effective in the large data regime, e.g. in domains such as vision [7, 23] and NLP[12,13]where large (unlabeled) datasets are readily available. However, in RL this is not the case: an off-policy RL agent is trained on a replay buffer that grows as the agent interacts with the environment. But in a sample efficient regime the buffer may only hold 10 4 -10 5 transitions from a few hundred trajectories, limiting the effectiveness of SSL methods.A wide range of auxiliary loss functions have been proposed to augment supervised objectives, e.g. weight regularization, noise injection [25], or some form of online SSL objective. In RL, * Equal contribution. Author ordering determined by coin flip. Both authors are corresponding.Preprint. Under review.",
    "prev": "Data augmentation for RL.",
    "curr": "In the context of RL, data augmentation has been widely investigated for improving data-efficiency Yarats et al., 2021), or RL generalization (Cobbe et al., 2019;Lee et al., 2019).",
    "next": "For example, RAD  demonstrated that data augmentation, such as random crop, can improve both data-efficiency and generalization of RL algorithms."
  },
  {
    "index": 515,
    "source_corpus_id": 231879989,
    "ref_id": "b40",
    "citation_corpus_id": 231933751,
    "start": 8827,
    "end": 8844,
    "title": "VA-RED 2 : VIDEO ADAPTIVE REDUNDANCY REDUC- TION",
    "abstract": "Performing inference on deep learning models for videos remains a challenge due to the large amount of computational resources required to achieve robust recognition. An inherent property of real-world videos is the high correlation of information across frames which can translate into redundancy in either temporal or spatial feature maps of the models, or both. The type of redundant features depends on the dynamics and type of events in the video: static videos have more temporal redundancy while videos focusing on objects tend to have more channel redundancy. Here we present a redundancy reduction framework, termed VA-RED 2 , which is input-dependent. Specifically, our VA-RED 2 framework uses an input-dependent policy to decide how many features need to be computed for temporal and channel dimensions. To keep the capacity of the original model, after fully computing the necessary features, we reconstruct the remaining redundant features from those using cheap linear operations. We learn the adaptive policy jointly with the network weights in a differentiable way with a shared-weight mechanism, making it highly efficient. Extensive experiments on multiple video datasets and different visual tasks show that our framework achieves 20% − 40% reduction in computation (FLOPs) when compared to state-of-the-art methods without any performance loss.",
    "prev": "Conditional Computation.",
    "curr": "Many conditional computation methods have been recently proposed with the goal of improving computational efficiency (Bengio et al., 2015;Veit & Belongie, 2018;Wang et al., 2018b;Graves, 2016;Meng et al., 2020;Pan et al., 2021).",
    "next": "Several works have been Figure 1: A conceptual view for adaptive temporal fusion."
  },
  {
    "index": 520,
    "source_corpus_id": 245502568,
    "ref_id": "b22",
    "citation_corpus_id": 231807280,
    "start": 3252,
    "end": 3259,
    "title": "UNDERSTANDING THE ROLE OF IMPORTANCE WEIGHT- ING FOR DEEP LEARNING",
    "abstract": "The recent paper byByrd & Lipton (2019), based on empirical observations, raises a major concern on the impact of importance weighting for the over-parameterized deep learning models. They observe that as long as the model can separate the training data, the impact of importance weighting diminishes as the training proceeds. Nevertheless, there lacks a rigorous characterization of this phenomenon. In this paper, we provide formal characterizations and theoretical justifications on the role of importance weighting with respect to the implicit bias of gradient descent and margin-based learning theory. We reveal both the optimization dynamics and generalization performance under deep learning models. Our work not only explains the various novel phenomenons observed for importance weighting in deep learning, but also extends to the studies where the weights are being optimized as part of the model, which applies to a number of topics under active research.",
    "prev": "In particular, Byrd and Lipton [BL19] empirically showed that when no regularization is used, overparameterized linear and nonlinear models trained with the importance weighted cross-entropy loss ignore the importance weights.",
    "curr": "Xu, Ye, and Ruan [XYR20] followed up and provided a theoretical justification for this observation in overparameterized linear and non-linear models.",
    "next": "To build intuition about why importance weighting fails, consider linear classifiers as an example."
  },
  {
    "index": 522,
    "source_corpus_id": 249431648,
    "ref_id": "b61",
    "citation_corpus_id": 6212000,
    "start": 2505,
    "end": 2509,
    "title": "UNDERSTANDING DEEP LEARNING REQUIRES RE- THINKING GENERALIZATION",
    "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.",
    "prev": "While significant efforts have been devoted in understanding and correcting biases in classical models such as logistic regressions and supported vector machines (SVM), see, e.g., [1,29], those derived tools are far less effective on modern over-parameterized models such as neural networks (NN).",
    "curr": "Over-parameterization can lead to poor generalization, as extensive efforts in both theoretical and empirical studies have exhibited ( [62,50,4]).",
    "next": "Furthermore, in large models, it is also difficult for measures of fairness (such as equalized odds to be introduced shortly) to generalize, as shown in Fig."
  },
  {
    "index": 523,
    "source_corpus_id": 1463401,
    "ref_id": "b5",
    "citation_corpus_id": 5590763,
    "start": 2354,
    "end": 2371,
    "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
    "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
    "prev": "The remarkable recent successes of the deep convolutional neural networks are particularly based on this ability to learn hierarchical representation for spatial data (Krizhevsky et al., 2012).",
    "curr": "For modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances (Mikolov et al., 2010;Graves, 2013;Cho et al., 2014;Vinyals et al., 2015).",
    "next": "However, unlike the spatial data, learning both hierarchical and temporal representation has been among the long-standing challenges of RNNs in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data (Schmidhuber, 1991;Mozer, 1993;El Hihi & Bengio, 1995;Lin et al., 1996;Koutník et al., 2014)."
  },
  {
    "index": 524,
    "source_corpus_id": 3535069,
    "ref_id": "b3",
    "citation_corpus_id": 19435386,
    "start": 3234,
    "end": 3236,
    "title": "A Corpus of Natural Language for Visual Reasoning",
    "abstract": "We present a new visual reasoning language dataset, containing 92,244 pairs of examples of natural statements grounded in synthetic images with 3,962 unique sentences. We describe a method of crowdsourcing linguistically-diverse data, and present an analysis of our data. The data demonstrates a broad set of linguistic phenomena, requiring visual and set-theoretic reasoning. We experiment with various models, and show the data presents a strong challenge for future research.",
    "prev": "Machine understanding of this structured visual information could assist human analysts in extracting knowledge from the vast documentation produced by modern science.",
    "curr": "Thus motivated, and inspired by recent research in Visual Question Answering (VQA) [1,2] and relational reasoning [3,4], we introduce FigureQA.",
    "next": "FigureQA is a corpus of over one million question-answer pairs grounded in over 100, 000 figures, devised to study aspects of comprehension and reasoning in machines."
  },
  {
    "index": 525,
    "source_corpus_id": 3703428,
    "ref_id": "b22",
    "citation_corpus_id": 6628106,
    "start": 15185,
    "end": 15203,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "We even show that the use of particular representation learning algorithms such as VAEs in the IMGEP-UGL architecture can produce exploration dynamics that match the one using engineered representations.",
    "curr": "Secondary contributions of this article:\n\n• We show that the IMGEP-UGL architecture can be successfully implemented (in terms of exploration efficiency) using various unsupervised learning algorithms for the goal space learning component: AutoEncoders (AEs) (Bourlard & Kamp, 1988), Variational AE (VAE) (Rezende et al., 2014;Kingma & Ba, 2015), VAE with Normalizing Flow (Rezende & Mohamed, 2015), Isomap (Tenenbaum et al., 2000), PCA (Pearson, 1901), and we quantitatively compare their performances in terms of exploration dynamics of the associated IMGEP-UGL architecture.",
    "next": "• We show that specifying more embedding dimensions than needed to capture the phenomenon manifold does not deteriorate the performance of these unsupervised learning algorithms."
  },
  {
    "index": 526,
    "source_corpus_id": 257255036,
    "ref_id": "b51",
    "citation_corpus_id": 227209335,
    "start": 2032,
    "end": 2052,
    "title": "SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS",
    "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reversetime SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in diffusion probabilistic modeling and scorebased generative modeling, and allows for new sampling procedures. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, which enables exact likelihood computation, and improved sampling efficiency. In addition, our framework enables conditional generation with an unconditional model, as we demonstrate with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high fidelity generation of 1024ˆ1024 images for the first time from a score-based generative model.",
    "prev": "INTRODUCTION\n\nDiffusion probabilistic modeling has quickly become a central approach for learning data distributions, obtaining impressive empirical results across multiple domains like images (Nichol & Dhariwal, 2021), videos (Ho et al., 2022) or even 3D geometry (Luo & Hu, 2021).",
    "curr": "In particular, Denoising Diffusion Probabilistic Models (often referred to as DDPMs or diffusion generative models) (Ho et al., 2020;Nichol & Dhariwal, 2021) and their continuous-time extension (Song et al., 2021b) both present a training objective that is more stable than precursors like generative adversarial nets (GANs) (Goodfellow et al., 2014) or energy-based models (EBMs) (Du et al., 2020).",
    "next": "In addition, diffusion generative models have shown to empirically outperform GANs in the image domain (Dhariwal & Nichol, 2021) and to suffer less from mode-seeking pathologies during training (Kodali et al., 2017)."
  },
  {
    "index": 527,
    "source_corpus_id": 7902640,
    "ref_id": "b21",
    "citation_corpus_id": 16299141,
    "start": 15676,
    "end": 15698,
    "title": "Pointer Sentinel Mixture Models",
    "abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus. 1",
    "prev": "For machine translation, we use the extended DSL and construct candidate architectures incrementally using the RL generator without a ranking function.",
    "curr": "LANGUAGE MODELING USING RANDOM SEARCH WITH A RANKING FUNCTION\n\nFor evaluating architectures found during architecture search, we use the WikiText-2 dataset (Merity et al., 2017b).",
    "next": "When evaluating a proposed novel RNN cell c, we construct a two layer c-RNN with a 200 unit hidden size."
  },
  {
    "index": 528,
    "source_corpus_id": 263909107,
    "ref_id": "b66",
    "citation_corpus_id": 3693334,
    "start": 1863,
    "end": 1880,
    "title": "Visualizing the Loss Landscape of Neural Nets",
    "abstract": "Neural network training relies on our ability to find \"good\" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and wellchosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple \"filter normalization\" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.",
    "prev": " analyze the connection between posterior quality and uncertainty quantification, delve into the impact of modes on the posterior, and explore methods for visualizing the posterior.Moreover, we uncover weight-space symmetries as a critical aspect for understanding the posterior.To this extent, we develop an in-depth assessment of the impact of both permutation and scaling symmetries that tend to obfuscate the Bayesian posterior.While the first type of transformation is known for duplicating modes, we explore the relationship between the latter and L2 regularization, challenging previous misconceptions.Finally, to help the community improve our understanding of the Bayesian posterior, we will release shortly the first large-scale checkpoint dataset, including thousands of real-world models, along with our codes.",
    "curr": "INTRODUCTION\n\nDespite substantial advancements in deep learning, Deep Neural Networks (DNNs) remain black box models.Various studies have sought to explore DNN loss landscapes (Li et al., 2018;Fort & Jastrzebski, 2019;Fort & Scherlis, 2019;Liu et al., 2022) to achieve a deeper understanding of these models.Recent works have, for instance, unveiled the interconnection of the modes obtained with Stochastic Gradient Descent (SGD) via narrow pathways that link pairs of modes, or through tunnels that connect multiple modes simultaneously (Garipov et al., 2018;Draxler et al., 2018).This mode connectivity primarily arises from scale and permutation invariances, which imply that numerous weights can represent the same exact function (e.g., Entezari et al.",
    "next": "(2022)).Several studies have delved into the relationship between these symmetries and the characteristics of the loss landscape (Entezari et al., 2022;Neyshabur et al., 2015;Brea et al., 2019).Our work investigates the connections between these symmetries and the distribution of DNN weights, a crucial aspect for uncertainty quantification.As shown in Figure 1, it is apparent that these symmetries also exert influence on DNN poster"
  },
  {
    "index": 531,
    "source_corpus_id": 209316097,
    "ref_id": "b1",
    "citation_corpus_id": 7771402,
    "start": 2049,
    "end": 2073,
    "title": "Abstract Meaning Representation for Sembanking",
    "abstract": "We describe Abstract Meaning Representation (AMR), a semantic representation language in which we are writing down the meanings of thousands of English sentences. We hope that a sembank of simple, whole-sentence semantic structures will spur new work in statistical natural language understanding and generation, like the Penn Treebank encouraged work on statistical parsing. This paper gives an overview of AMR and tools associated with it.",
    "prev": "So far, most of the existing methods focus on building statistical associations between textual inputs and semantic representations, e.g.",
    "curr": "using first-order logic (Manning et al., 1999) or other types of representations such as abstract meaning representation (Banarescu et al., 2013).",
    "next": "Recently, grounded language learning has gradually attracted attention in various domains, inspired by the hypothesis that early language learning was focused on problemsolving (Kirby & Hurford, 2002)."
  },
  {
    "index": 532,
    "source_corpus_id": 222133372,
    "ref_id": "b50",
    "citation_corpus_id": 3366315,
    "start": 16630,
    "end": 16634,
    "title": "Published as a conference paper at ICLR 2018 SPECTRAL NORMALIZATION FOR GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. The code with Chainer (Tokui et al., 2015), generated images and pretrained models are available at https://github.com/pfnet-research/sngan_ projection.Published as a conference paper at ICLR 2018• Lipschitz constant is the only hyper-parameter to be tuned, and the algorithm does not require intensive tuning of the only hyper-parameter for satisfactory performance. • Implementation is simple and the additional computational cost is small.",
    "prev": "Compared with the conditions stated in Section 2 (namely, that it is locally homogeneous with a symmetric Jacobian), our requirement on the denoiser is milder.",
    "curr": "One can train a nonexpansive D σ by constraining the Lipschitz constant of D σ via the spectral normalization, which is an active area of research in deep learning [51][52][53].",
    "next": "We can now state the theorems on ASYNC-RED."
  },
  {
    "index": 534,
    "source_corpus_id": 240419913,
    "ref_id": "b47",
    "citation_corpus_id": 56475856,
    "start": 2418,
    "end": 2421,
    "title": "LEARNING TO ADAPT IN DYNAMIC, REAL-WORLD ENVIRONMENTS THROUGH META-REINFORCEMENT LEARNING",
    "abstract": "Although reinforcement learning methods can achieve impressive results in simulation, the real world presents two major challenges: generating samples is exceedingly expensive, and unexpected perturbations or unseen situations cause proficient but specialized policies to fail at test time. Given that it is impractical to train separate policies to accommodate all situations the agent may see in the real world, this work proposes to learn how to quickly and effectively adapt online to new tasks. To enable sample-efficient learning, we consider learning online adaptation in the context of model-based reinforcement learning. Our approach uses meta-learning to train a dynamics model prior such that, when combined with recent data, this prior can be rapidly adapted to the local context. Our experiments demonstrate online adaptation for continuous control tasks on both simulated and real-world agents. We first show simulated agents adapting their behavior online to novel terrains, crippled body parts, and highly-dynamic environments. We also illustrate the importance of incorporating online adaptation into autonomous agents that operate in the real world by applying our method to a real dynamic legged millirobot. We demonstrate the agent's learned ability to quickly adapt online to a missing leg, adjust to novel terrains and slopes, account for miscalibration or errors in pose estimation, and compensate for pulling payloads. 2 1 * Equal Contribution 2 Videos available at: https://sites.",
    "prev": "While plenty of other work in model-based RL does measure generalization [e.g.",
    "curr": "18,48,70,76], each approach is typically evaluated on a bespoke task, making it difficult to ascertain the state of generalization in model-based RL more broadly.",
    "next": "Model-free RL, like model-based RL, has also suffered from both the \"train=test\" paradigm and a lack of standardization around how to measure generalization."
  },
  {
    "index": 536,
    "source_corpus_id": 262825568,
    "ref_id": "b48",
    "citation_corpus_id": 252715691,
    "start": 1769,
    "end": 1787,
    "title": "GLM-130B: AN OPEN BILINGUAL PRE-TRAINED MODEL",
    "abstract": "We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and disconvergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B-the largest Chinese language model-across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization, without quantization aware training and with almost no performance loss, making it the first among 100B-scale models. More importantly, the property allows its effective inference on 4×RTX 3090 (24G) or 8×RTX 2080 Ti (11G) GPUs, the most ever affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at https://github.comIn this work, we introduce the pre-training of a 100B-scale model-GLM-130B, in terms of engineering efforts, model design choices, training strategies for efficiency and stability, and quantization for affordable inference. As it has been widely realized that it is computationally unaffordable to empirically enumerate all possible designs for training 100B-scale LLMs, we present not only the successful part for training GLM-130B but also many of the failed options and lessons learned. Particularly, the training stability is the decisive factor in the success of training models of such a scale. Different from practices such as manually adjusting learning rates in OPT-175B and using embedding norm in the sacrifice of performance in BLOOM-176B, we experiment with various options and find the strategy of embedding gradient shrink can significantly stabilize the training of GLM-130B.Specifically, GLM-130B is a bilingual (English and Chinese) bidirectional dense model with 130 billion parameters, pre-trained over 400 billion tokens on a cluster of 96 NVIDIA DGX-A100 (8×40G) GPU nodes between May 6 and July 3, 2022. Instead of using the GPT-style architecture, we adopt the General Language Model (GLM) algorithm (Du et al., 2022)  to leverage its bidirectional attention advantage and autoregressive blank infilling objective.Table 1summarizes the comparison between GLM-130B, GPT-3 and another two open-source efforts-OPT-175B and BLOOM-176B, as well as PaLM 540B (Chowdhery et al., 2022)-a 4× larger model-as a reference.Altogether, the conceptual uniqueness and engineering efforts enable GLM-130B to exhibit performance that surpasses the level of GPT-3 on a wide range of benchmarks (in total 112 tasks) and also outperforms PaLM 540B in many cases, while outperformance over GPT-3 has not been observed in OPT-175B and BLOOM-176B (Cf.Figure 1 (a)). For zero-shot performance, GLM-130B is better than GPT-3 175B (+5.0%), OPT-175B (+6.5%), and BLOOM-176B (+13.0%) on LAMBADA (Paperno et al., 2016), and achieves 3× better performance than GPT-3 on Big-bench-lite (Srivastava  et al., 2022). For the 5-shot MMLU (Hendrycks et al., 2021) tasks, it is better than GPT-3 175B (+0.9%) and BLOOM-176B (+12.7%). As a bilingual LLM also in Chinese, it offers significantly better results than ERNIE TITAN 3.0 260B (Wang et al., 2021)-the largest Chinese LLM-on 7 zero-shot CLUE (Xu et al., 2020) datasets (+24.26%) and 5 zero-shot FewCLUE  ones (+12.75%). Importantly, as summarized inFigure 1 (b), GLM-130B as an open model is associated with significantly less bias and generation toxicity than its 100B-scale counterparts.Finally, we design GLM-130B to empower as many people as possible to conduct 100B-scale LLM studies. First, instead of using 175B+ parameters as OPT and BLOOM, the 130B size is decided because such a size supports inference on a single A100 (8×40G) server. Second, to further lower the GPU requirements, we quantize GLM-130B into INT4 precision without quantization aware training while OPT and BLOOM can only reach INT8. Due to a unique property of the GLM architecture, GLM-130B's INT4 quantization introduces negligible performance degradation, e.g., -0.74% on LAMBADA and even +0.05% on MMLU, making it still better than the uncompressed GPT-3. This enables GLM-130B's fast inference with performance guarantee on a server of 4×RTX 3090 (24G) or 8×RTX 2080 Ti (11G), the most ever affordable GPU required for using 100B-scale LLMs to date.We open-source the model checkpoints, code, training logs, related toolkits, and lessons learned.THE DESIGN CHOICES OF GLM-130BThe architecture of a machine learning model defines its inductive bias. However, it has been realized that it is computationally unaffordable to explore various architectural designs for LLMs. We introduce and explain the unique design choices of GLM-130B. 2.1 GLM-130B'S ARCHITECTURE GLM as Backbone. Most recent 100B-scale LLMs, such as GPT-3, PaLM, OPT, and BLOOM, follow the traditional GPT-style (Radford et al., 2019) architecture of decoder-only autoregressive language modeling. In GLM-130B, we instead make an attempt to explore the potential of a bidirectional GLM-General Language Model (Du et al., 2022)-as its backbone.GLM is a transformer-based language model that leverages autoregressive blank infilling as its training objective. Briefly, for a text sequence x = [x 1 , · · · , x n ], text spans {s 1 , · · · , s m } are sampled from it, each of which s i denotes a span of consecutive tokens [s i,1 , · · · , s i,li ] and is replaced (i.e., corrupted) with a single mask token to form x corrupt . The model is asked to recover them autoregressively. To allow interactions between corrupted spans, their visibility to each other is decided by a He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters.",
    "prev": "-rank adaptation (QA-LoRA) algorithm.The motivation lies in the imbalanced degrees of freedom of quantization and adaptation, and the solution is to use group-wise operators which increase the degree of freedom of quantization meanwhile decreasing that of adaptation.QA-LoRA is easily implemented with a few lines of code, and it equips the original LoRA with two-fold abilities: (i) during fine-tuning, the LLM's weights are quantized (e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the LLM and auxiliary weights are naturally integrated into a quantized model without loss of accuracy.We apply QA-LoRA to the LLaMA and LLaMA2 model families and validate its effectiveness in different fine-tuning datasets and downstream scenarios.Code will be made available at https://github.com/yuhuixu1993/qa-lora.",
    "curr": "INTRODUCTION\n\nRecently, large language models (LLMs) (Brown et al., 2020;Scao et al., 2022;Zhang et al., 2022;Touvron et al., 2023a;Chowdhery et al., 2022;OpenAI, 2023;Zeng et al., 2023) have shown unprecedented performance across a wide range of language understanding tasks (Wei et al., 2022a) and served as the foundation of state-of-the-art chat systems (Bubeck et al., 2023).The diversity of real-world applications calls for a pipeline in which LLMs can be fine-tuned to fit different scenarios and quantized to be deployed onto edge devices (e.g., mobile phones), and the key issue is to get rid of the heavy computational burden brought by the large number of parameters of LLMs.",
    "next": "There are two lines of research for this purpose.The first one is parameter-efficient fine-tuning (PEFT) (Houlsby et al., 2019;Li & Liang, 2021;Liu et al., 2021;He et al., 2022;Hu et al., 2021) which introduced a small number of learnable parameters while keeping most pre-trained parameters unchanged.Among them, low-rank adaptation (LoRA) (Hu et al., 2021), a popular PEFT algorithm, proposed to fine-tune low-rank matrices to complement the pre-trained weights.Despite the comparable performanc"
  },
  {
    "index": 537,
    "source_corpus_id": 210164865,
    "ref_id": "b48",
    "citation_corpus_id": 26238954,
    "start": 3616,
    "end": 3635,
    "title": "Under review as a conference paper at ICLR 2018 BREAKING THE SOFTMAX BOTTLENECK: A HIGH-RANK RNN LANGUAGE MODEL",
    "abstract": "We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. 1 * Equal contribution. Ordering determined by dice rolling. 1 Code is available at httpsUnder review as a conference paper at ICLR 2018 matrices that have much larger normalized singular values and thus much higher rank than Softmax and other baselines on real-world datasets.We evaluate our proposed approach on standard language modeling benchmarks. MoS substantially improves over the current state-of-the-art results on benchmarks, by up to 3.6 points in terms of perplexity, reaching perplexities 47.69 on Penn Treebank and 40.68 on WikiText-2. We further apply MoS to a dialog dataset and show improved performance over Softmax and other baselines.Our contribution is two-fold. First, we identify the Softmax bottleneck by formulating language modeling as a matrix factorization problem. Second, we propose a simple and effective method that substantially improves over the current state-of-the-art results.",
    "prev": "AR models based on recurrent (G)NNs can be viewed as special instantiations of SSMs in which the state transitions are restricted to being deterministic (Fraccaro, 2018, Section 4.2).",
    "curr": "Despite their simplicity, it has been pointed out that their modeling capability is bottlenecked by the deterministic state transitions (Chung et al., 2015;Fraccaro et al., 2016) and the oversimplified observation distributions (Yang et al., 2018).",
    "next": "In this study, we make the following contributions: (i) We propose the relational state-space model (R-SSM), a novel hierarchical deep SSM that simulates the stochastic state transitions of interacting objects with GNNs, extending GNN-based dynamics modeling to challenging stochastic multi-object systems."
  },
  {
    "index": 538,
    "source_corpus_id": 252715691,
    "ref_id": "b9",
    "citation_corpus_id": 237416585,
    "start": 11827,
    "end": 11845,
    "title": "Published as a conference paper at ICLR 2022 FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS",
    "abstract": "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning-finetuning language models on a collection of datasets described via instructions-substantially improves zeroshot performance on unseen tasks.",
    "prev": "To improve FFNs in Transformer, we pick GLU with the GeLU (Hendrycks & Gimpel, 2016) activation as the replacement.",
    "curr": "2.2 GLM-130B'S PRE-TRAINING SETUP Inspired by recent works (Aribandi et al., 2022;Wei et al., 2022a;Sanh et al., 2022), the GLM-130B pre-training objective includes not only the self-supervised GLM autoregressive blank infilling) but also multi-task learning for a small portion of tokens.",
    "next": "This is expected to help boost its downstream zero-shot performance."
  },
  {
    "index": 541,
    "source_corpus_id": 52892477,
    "ref_id": "b1",
    "citation_corpus_id": 900029,
    "start": 1151,
    "end": 1172,
    "title": "NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 20-28, Deep Neural Network Language Models",
    "abstract": "In recent years, neural network language models (NNLMs) have shown success in both peplexity and word error rate (WER) compared to conventional n-gram language models. Most NNLMs are trained with one hidden layer. Deep neural networks (DNNs) with more hidden layers have been shown to capture higher-level discriminative information about input features, and thus produce better networks. Motivated by the success of DNNs in acoustic modeling, we explore deep neural network language models (DNN LMs) in this paper. Results on a Wall Street Journal (WSJ) task demonstrate that DNN LMs offer improvements over a single hidden layer NNLM. Furthermore, our preliminary results are competitive with a model M language model, considered to be one of the current state-of-the-art techniques for language modeling.",
    "prev": "On the WIKITEXT-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the BILLION WORD benchmark, we achieve 23.02 perplexity.",
    "curr": "1\n\nINTRODUCTION\n\nLanguage modeling is a basic task in natural language processing, with many applications such as speech recognition (Arisoy et al., 2012) and statistical machine translation (Schwenk et al., 2012;Vaswani et al., 2013;Baltescu & Blunsom, 2015).",
    "next": "Recently, much progress has been made by neural methods (Bengio et al., 2003;Mikolov et al., 2010) based on LSTMs (Józefowicz et al., 2016), gated convolutional networks (Dauphin et al., 2017) and self-attentional networks (Al-Rfou et al., 2018)."
  },
  {
    "index": 543,
    "source_corpus_id": 247596648,
    "ref_id": "b24",
    "citation_corpus_id": 3144218,
    "start": 8028,
    "end": 8049,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "For generalizing across parametric variations of a single morphology, various approaches have been proposed like using a learned hardware embedding (Chen et al., 2018), meta-learning for policy adaptation (Al-Shedivat et al., 2017;Ghadirzadeh et al., 2021), kinematics randomization (Exarchos et al., 2020), and dynamics randomization (Peng et al., 2018).",
    "curr": "In case of multiple different morphologies, one approach to tackle the challenge of differences in action and state spaces is to leverage Graph Neural Networks (Scarselli et al., 2008;Kipf & Welling, 2017;.",
    "next": "Wang et al."
  },
  {
    "index": 546,
    "source_corpus_id": 219530969,
    "ref_id": "b2",
    "citation_corpus_id": 3517962,
    "start": 2457,
    "end": 2460,
    "title": "Published as a conference paper at ICLR 2018 DISTRIBUTED DISTRIBUTIONAL DETERMINISTIC POLICY GRADIENTS",
    "abstract": "This work adopts the very successful distributional perspective on reinforcement learning and adapts it to the continuous control setting. We combine this within a distributed framework for off-policy learning in order to develop what we call the Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG. We also combine this technique with a number of additional, simple improvements such as the use of N -step returns and prioritized experience replay. Experimentally we examine the contribution of each of these individual components, and show how they interact, as well as their combined contributions. Our results show that across a wide variety of simple control tasks, difficult manipulation tasks, and a set of hard obstacle-based locomotion tasks the D4PG algorithm achieves state of the art performance.arXiv:1804.08617v1 [cs.LG] 23 Apr 2018Published as a conference paper at ICLR 2018 experience, which we implement using the ApeX framework(Horgan et al., 2018). This results in significant savings in terms of wall-clock time for difficult control tasks. We will also introduce a number of small improvements to the DDPG algorithm, and in our experiments will show the individual contributions of each component. Finally, this algorithm, which we call the Distributed Distributional DDPG algorithm (D4PG), obtains state-of-the-art performance across a wide variety of control tasks, including hard manipulation and locomotion tasks.",
    "prev": "Under review.",
    "curr": "Introduction\n\nReinforcement learning (RL) algorithms have recently demonstrated impressive success in learning behaviors for a variety of sequential decision-making tasks [3,24,42].",
    "next": "Virtually all of these demonstrations have relied on highly-frequent online access to the environment, with the RL algorithms often interleaving each update to the policy with additional experience collection of that policy acting in the environment."
  },
  {
    "index": 548,
    "source_corpus_id": 259096096,
    "ref_id": "b37",
    "citation_corpus_id": 213695995,
    "start": 2481,
    "end": 2484,
    "title": "Published as a conference paper at ICLR 2020 FEATURE INTERACTION INTERPRETABILITY: A CASE FOR EXPLAINING AD-RECOMMENDATION SYSTEMS VIA NEURAL INTERACTION DETECTION",
    "abstract": "Recommendation is a prevalent application of machine learning that affects many users; therefore, it is important for recommender models to be accurate and interpretable. In this work, we propose a method to both interpret and augment the predictions of black-box recommender systems. In particular, we propose to interpret feature interactions from a source recommender model and explicitly encode these interactions in a target recommender model, where both source and target models are black-boxes. By not assuming the structure of the recommender system, our approach can be used in general settings. In our experiments, we focus on a prominent use of machine learning recommendation: ad-click prediction. We found that our interaction interpretations are both informative and predictive, e.g., significantly outperforming existing recommender models. What's more, the same approach to interpret interactions can provide new insights into domains even beyond recommendation, such as text and image classification.",
    "prev": "Introduction\n\nA core research question in multimodal learning is to understand the nature of multimodal interactions across modalities in the context of a task: the emergence of new task-relevant information during learning from both modalities that was not present in either modality alone [6,65].",
    "curr": "In settings where labeled multimodal data is abundant, the study of multimodal interactions has inspired advances in theoretical analysis [1,43,66,84,94] and representation learning [51, 76,91,104] in language and vision [2], multimedia [9], healthcare [53], and robotics [57].",
    "next": "In this paper, we study the problem of interaction quantification in a setting where there is only unlabeled multimodal data D M = {(x 1 , x 2 )} but some labeled unimodal data D i = {(x i , y)} collected separately for each modality."
  },
  {
    "index": 549,
    "source_corpus_id": 248377437,
    "ref_id": "b12",
    "citation_corpus_id": 28202810,
    "start": 22619,
    "end": 22642,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "2, the learner policy is trained with a MaxEnt RL algorithm with per-timestep entropy-regularized reward given by f ω (s t , a t ) − α log π(a t |s t ), where α is the entropy coefficient.",
    "curr": "In our experiments, we use the clipped-ratio PPO algorithm (Schulman et al., 2017) and adaptively tune α as suggested in prior work (Haarnoja et al., 2018).",
    "next": "RELATED WORK\n\nThere is a vast amount of literature on IL since it is a powerful framework to train agents to perform complex behaviors without a reward specification."
  },
  {
    "index": 551,
    "source_corpus_id": 227054461,
    "ref_id": "b9",
    "citation_corpus_id": 8968704,
    "start": 2176,
    "end": 2192,
    "title": "A Neural Representation of Sketch Drawings",
    "abstract": "We present sketch-rnn, a recurrent neural network (RNN) able to construct stroke-based drawings of common objects. The model is trained on a dataset of human-drawn images representing many different classes. We outline a framework for conditional and unconditional sketch generation, and describe new robust training methods for generating coherent sketch drawings in a vector format.",
    "prev": "Sketching is often used during brainstorming to help the creative process, and is a popular creative activity in itself.",
    "curr": "Sketch-related AI so far has primarily focused on mimicking the human ability to perceive rich visual information from simple line drawings (Yu et al., 2015;Li et al., 2018) and to generate minimal depictions that capture the salient aspects of our visual world (Ha & Eck, 2018;Isola et al., 2017).",
    "next": "Most existing datasets contain sketches drawn by humans to realistically mimic common objects (Eitz et al., 2012;Sangkloy et al., 2016;Jongejan et al., 2016;Wang et al., 2019)."
  },
  {
    "index": 552,
    "source_corpus_id": 220713471,
    "ref_id": "b6",
    "citation_corpus_id": 53018855,
    "start": 5740,
    "end": 5743,
    "title": "DISCRIMINATOR REJECTION SAMPLING",
    "abstract": "We propose a rejection sampling scheme using the discriminator of a GAN to approximately correct errors in the GAN generator distribution. We show that under quite strict assumptions, this will allow us to recover the data distribution exactly. We then examine where those strict assumptions break down and design a practical algorithm-called Discriminator Rejection Sampling (DRS)-that can be used on real data-sets. Finally, we demonstrate the efficacy of DRS on a mixture of Gaussians and on the SAGAN model, state-of-the-art in the image generation task at the time of developing this work. On ImageNet, we train an improved baseline that increases the Inception Score from 52.52 to 62.36 and reduces the Fréchet Inception Distance from 18.65 to 14.79. We then use DRS to further improve on this baseline, improving the Inception Score to 76.08 and the FID to 13.75.",
    "prev": "In contrast, we rely on private GAN to reduce the support to only contain the set of privately generated samples, which makes PGB tractable even for high-dimensional data.",
    "curr": "We also provide an extension of the PGB method by incorporating the technique of discriminator rejection sampling [7,35].",
    "next": "We leverage the fact that the distinguisher's equilibrium strategy, which is a mixture of discriminators, can often accurately predict which samples are unlikely and thus can be used as a rejection sampler."
  },
  {
    "index": 557,
    "source_corpus_id": 263620293,
    "ref_id": "b65",
    "citation_corpus_id": 3352260,
    "start": 5477,
    "end": 5499,
    "title": "Jointly Learning to Construct and Control Agents using Deep Reinforcement Learning",
    "abstract": "The physical design of a robot and the policy that controls its motion are inherently coupled, and should be determined according to the task and environment. In an increasing number of applications, data-driven and learningbased approaches, such as deep reinforcement learning, have proven effective at designing control policies. For most tasks, the only way to evaluate a physical design with respect to such control policies is empirical-i.e., by picking a design and training a control policy for it. Since training these policies is timeconsuming, it is computationally infeasible to train separate policies for all possible designs as a means to identify the best one. In this work, we address this limitation by introducing a method that performs simultaneous joint optimization of the physical design and control network. Our approach maintains a distribution over designs and uses reinforcement learning to optimize a control policy to maximize expected reward over the design distribution. We give the controller access to design parameters to allow it to tailor its policy to each design in the distribution. Throughout training, we shift the distribution towards higher-performing designs, eventually converging to a design and control policy that are jointly optimal. We evaluate our approach in the context of legged locomotion, and demonstrate that it discovers novel designs and walking gaits, outperforming baselines in both performance and efficiency.",
    "prev": "Via our detailed ablation study, we construct ChemRLformer and find that pretraining on aligned datasets can significantly improve performance across all molecular design tasks, even exceeding the performance of agents pretrained on 100 times larger datasets.We also show that targeted algorithmic design, such as hill-climbing in the replay buffer and regularization, further increases the performance of ChemRLformer.To the best of our knowledge, ChemRLformer is the largest analysis of text-based RL methods for molecule discovery.",
    "curr": "[Yu et al., 2018, Schaff et al., 2019].Other methods have explicitly included the design process in the RL loop by training design problems together [Chen et al., 2021a, Ha, 2019, Luck et al., 2020, Kumar et al., 2022] with most prior work focusing on robot and agent design, not molecular design.Our molecular design work creates an autoregressive structure that grows the size of the state as the agent acts in the environment.",
    "next": "Molecular Discovery Using Sequence-Based Methods: Sequence-based methods treat molecular design as a sequence of tokens that get concatenated in order.Generative models for sequence-based methods span a diverse range, including variational autoencoders (VAEs) [Gómez-Bombarelli et al., 2018, Alperstein et al., 2019], recurrent neural networks (RNNs) [Gupta et al., 2018, Bjerrum and Threlfall, 2017, Grisoni et al., 2020, Flam-Shepherd et al., 2022] and transformer models [Wang et al., 2019, Fabian et al., 2020, Edwards et al., 2022a, Zeng et al., 2022, Taylor et al., 2022].The general procedure for all"
  },
  {
    "index": 558,
    "source_corpus_id": 240354066,
    "ref_id": "b12",
    "citation_corpus_id": 219965819,
    "start": 2855,
    "end": 2858,
    "title": "LIPSCHITZ RECURRENT NEURAL NETWORKS",
    "abstract": "Differential equations are a natural choice for modeling recurrent neural networks because they can be viewed as dynamical systems with a driving input. In this work, we propose a recurrent unit that describes the hidden state's evolution with two parts: a well-understood linear component plus a Lipschitz nonlinearity. This particular functional form simplifies stability analysis, which enables us to provide an asymptotic stability guarantee. Further, we demonstrate that Lipschitz recurrent units are more robust with respect to perturbations. We evaluate our approach on a range of benchmark tasks, and we show it outperforms existing recurrent units.",
    "prev": "Since LRDs are perhaps the foremost challenge for sequence models, all standard model families such as continuous-time models (CTMs), RNNs, CNNs, and Transformers include many specialized variants designed to address them.",
    "curr": "Modern examples include orthogonal and Lipschitz RNNs [1,13] to combat vanishing gradients, dilated convolutions to increase context size [3,28], and an increasingly vast family of efficient Transformers that reduce the quadratic dependence on sequence length [8,22].",
    "next": "Despite being designed for LRDs, these solutions still perform poorly on challenging benchmarks such as LRA [40] or raw audio classification [18]."
  },
  {
    "index": 559,
    "source_corpus_id": 108306725,
    "ref_id": "b9",
    "citation_corpus_id": 6715185,
    "start": 35139,
    "end": 35165,
    "title": "Published as a conference paper at ICLR 2016 NEURAL PROGRAMMER: INDUCING LATENT PROGRAMS WITH GRADIENT DESCENT",
    "abstract": "Deep neural networks have achieved impressive supervised classification performance in many tasks including image recognition, speech recognition, and sequence to sequence learning. However, this success has not been translated to applications like question answering that may involve complex arithmetic and logic reasoning. A major limitation of these models is in their inability to learn even simple arithmetic and logic operations. For example, it has been shown that neural networks fail to learn to add two binary numbers reliably. In this work, we propose Neural Programmer, a neural network augmented with a small set of basic arithmetic and logic operations that can be trained end-to-end using backpropagation. Neural Programmer can call these augmented operations over several steps, thereby inducing compositional programs that are more complex than the built-in operations. The model learns from a weak supervision signal which is the result of execution of the correct program, hence it does not require expensive annotation of the correct program itself. The decisions of what operations to call, and what data segments to apply to are inferred by Neural Programmer. Such decisions, during training, are done in a differentiable fashion so that the entire network can be trained jointly by gradient descent. We find that training the model is difficult, but it can be greatly improved by adding random noise to the gradient. On a fairly complex synthetic table-comprehension dataset, traditional recurrent networks and attentional models perform poorly while Neural Programmer typically obtains nearly perfect accuracy. * Work done during an internship at Google.",
    "prev": "Neural Turing Machine (NTM) (Graves et al., 2014; enables general-purpose neural problem solving such as sorting by introducing an external memory that mimics the execution of Turing Machine.",
    "curr": "Neural program induction and synthesis (Neelakantan et al., 2016;Reed & De Freitas, 2016;Kaiser & Sutskever, 2016;Parisotto et al., 2017;Devlin et al., 2017;Bunel et al., 2018;Sun et al., 2018) are recently introduced to solve problems by synthesizing computer programs with neural augmentations.",
    "next": "Some works tackle the issue of the systematical generalization by introducing extra supervision (Cai et al., 2017)."
  },
  {
    "index": 560,
    "source_corpus_id": 257039090,
    "ref_id": "b12",
    "citation_corpus_id": 173990158,
    "start": 7470,
    "end": 7488,
    "title": "Multimodal Transformer for Unaligned Multimodal Language Sequences",
    "abstract": "Human language is often multimodal, which comprehends a mixture of natural language, facial gestures, and acoustic behaviors. However, two major challenges in modeling such multimodal human language time-series data exist: 1) inherent data non-alignment due to variable sampling rates for the sequences from each modality; and 2) long-range dependencies between elements across modalities. In this paper, we introduce the Multimodal Transformer (MulT) to generically address the above issues in an end-to-end manner without explicitly aligning the data. At the heart of our model is the directional pairwise crossmodal attention, which attends to interactions between multimodal sequences across distinct time steps and latently adapt streams from one modality to another. Comprehensive experiments on both aligned and non-aligned multimodal time-series show that our model outperforms state-of-the-art methods by a large margin. In addition, empirical analysis suggests that correlated crossmodal signals are able to be captured by the proposed crossmodal attention mechanism in MulT.",
    "prev": "These datasets enable us to train a multimodal model y = f (x 1 , x 2 ; θ) which we are interested in visualizing.",
    "curr": "Modern parameterizations of multimodal models f are typically black-box neural networks, such as multimodal transformers (Hendricks et al., 2021;Tsai et al., 2019) and pretrained models (Li et al., 2019;Lu et al., 2019).",
    "next": "How can we visualize and understand the internal modeling of multimodal information and interactions in these models?"
  },
  {
    "index": 561,
    "source_corpus_id": 52055130,
    "ref_id": "b2",
    "citation_corpus_id": 18828233,
    "start": 41405,
    "end": 41429,
    "title": "TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.",
    "prev": "(13)\nSome intuition might help understand why this might work.",
    "curr": "As discussed in Arjovsky & Bottou (2017), if P and Q θ have support on a low-dimensional manifolds on X , unless they are perfectly aligned, there exists a discriminator that will be able to perfectly distinguish between samples coming from P and Q θ , which means that U ψ,P and U ψ,Q must also be disjoint.",
    "next": "However, to train the generator, U ψ,P and U ψ,Q need to share support on Y in order to ensure stable and non-zero gradients for the generator."
  },
  {
    "index": 562,
    "source_corpus_id": 263608118,
    "ref_id": "b24",
    "citation_corpus_id": 253098274,
    "start": 2169,
    "end": 2187,
    "title": "SUPER-NATURALINSTRUCTIONS: Generalization via Declarative Instructions on 1600+ NLP Tasks",
    "abstract": "How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce SUPER-NATURALINSTRUCTIONS, 1 a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructionstraining models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones. Furthermore, we build Tk-INSTRUCT, a transformer model trained to follow a variety of incontext instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-INSTRUCT outperforms existing instruction-following models such as Instruct-GPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models. 2",
    "prev": "es the attention matrix with linear complexity via kernel-based linear attention, then creates a sparse approximation to the full attention matrix with a top-k selection to perform a sparse attention operation.For language modeling tasks (Wikitext2), previous linear and sparse attention methods show a roughly two-fold worse perplexity scores over the quadratic OPT-125M baseline, while SEA achieves an even better perplexity than OPT-125M, using roughly half as much memory as OPT-125M.Moreover, SEA maintains an interpretable attention matrix and can utilize knowledge distillation to lower the complexity of existing pretrained transformers.We believe that our work will have a large practical impact, as it opens the possibility of running large transformers on resource-limited devices with less memory.",
    "curr": "INTRODUCTION\n\nThe transformer (Vaswani et al., 2017) architecture has revolutionized various fields of artificial intelligence, such as natural language understanding (Touvron et al., 2023;Wang et al., 2022) and computer vision (Dosovitskiy et al., 2021) due to its ability to learn pairwise relationships between all T tokens in a given sequence (O(T 2 )).This has ushered in the era of large transformer-based foundation models with impressive generalization abilities (Brown et al., 2020;Chiang et al., 2023).However, since the transformer's attention mechanism comes with a quadratic space and time complexity, it becomes untenable for handling long sequences which is essential for tasks such as dialogue generation (Chen et al., 2023).To overcome this limitation, previous works have suggested approaches with linear complexity by using static or dynamic sparse attention patterns (Beltagy et al., 2020;Zaheer et al., 2020;Tay et al., 2020a;Kitaev et al., 2020;Tay et al., 2020b;Liu et al., 2021), or by replacing quadratic attention with kernel or low-rank approximations (Choromanski et al., 2021;Chen et al., 2021;Qin et al., 2022).",
    "next": "However, despite their promising aspects, previous linear attent"
  },
  {
    "index": 564,
    "source_corpus_id": 260887189,
    "ref_id": "b7",
    "citation_corpus_id": 258461287,
    "start": 3078,
    "end": 3097,
    "title": "Can Large Language Models Be an Alternative to Human Evaluation?",
    "abstract": "Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms. Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided. In this paper, we explore if such an ability of the LLMs can be used as an alternative to human evaluation. We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation. We use human evaluation and LLM evaluation to evaluate the texts in two NLP tasks: openended story generation and adversarial attacks. We show that the result of LLM evaluation is consistent with the results obtained by expert human evaluation: the texts rated higher by human experts are also rated higher by the LLMs. We also find that the results of LLM evaluation are stable over different formatting of the task instructions and the sampling algorithm used to generate the answer. We are the first to show the potential of using LLMs to assess the quality of texts and discuss the limitations and ethical considerations of LLM evaluation. , et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. , et al. 2022. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.",
    "prev": "1\n\nINTRODUCTION\n\nThe emergence of Large Language Models (LLMs) has played a pivotal role in driving the advancement of Artificial Intelligence (AI) systems.",
    "curr": "Noteworthy LLMs like ChatGPT (OpenAI, 2023a;b), Claude2 (Anthropic, 2023), Bard (Google, 2023), and Llama2 (Touvron et al., 2023a) have demonstrated their advanced capability to perform innovative applications, ranging from tool utilization, supplementing human evaluations, to stimulating human interactive behaviors Bubeck et al., 2023;Schick et al., 2023;Chiang & Lee, 2023;Park et al., 2023).",
    "next": "The outstanding competencies have fueled their widespread deployment, while the progression is shadowed by a significant challenge: ensuring the safety and reliability of the responses."
  },
  {
    "index": 565,
    "source_corpus_id": 51969884,
    "ref_id": "b4",
    "citation_corpus_id": 5590763,
    "start": 28291,
    "end": 28309,
    "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
    "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
    "prev": "−E(y, h; x) = f i xiw x if j yjw y jf k h k w h kf + k w h k h k + j w y j yj = x T W x • y T W y • h T W h 1 + h T w h + y T w y(21)\nSetting aside of bias terms, the I × J × K parameter tensor of unfactored Higher-Order Boltzmann Machines is replaced with three matrices, W x ∈ R I×F , W y ∈ R J×F , and W h ∈ R K×F .",
    "curr": "D.3 MULTIPLICATIVE INTEGRATION WITH RECURRENT NEURAL NETWORKS\n\nMost of recurrent neural networks, including vanilla RNNs, Long Short Term Memory networks (Hochreiter & Schmidhuber, 1997) and Gated Recurrent Units (Cho et al., 2014), share a common expression as follows:\nφ(Wx + Uh + b)(22)\nwhere φ is a non-linear function, W ∈ R d×n , x ∈ R n , U ∈ R d×m , h ∈ R m , and b ∈ R d is a bias vector.",
    "next": "Note that, usually, x is an input state vector and h is an hidden state vector in recurrent neural networks."
  },
  {
    "index": 566,
    "source_corpus_id": 256846836,
    "ref_id": "b24",
    "citation_corpus_id": 222140788,
    "start": 6355,
    "end": 6375,
    "title": "Published as a conference paper at ICLR 2021 DENOISING DIFFUSION IMPLICIT MODELS",
    "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples 10× to 50× faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error.",
    "prev": "Different works also studied different approximations of the unknown q(z t−1 |z t , z 0 ) used to perform sampling.",
    "curr": "For example, denoising diffusion implicit model (DDIM) (Song et al., 2021a) first computed a predicted clean data point\nz 0 = z t − ( √ 1 − α t ) θ (z t , t) √ α t ,(3)\nand sample z t−1 from q(z t−1 |z t ,ẑ 0 ) by replacing unknown z 0 withẑ 0 .",
    "next": "On the other hand, while the details of individual sampling methods vary, all sampling methods produce z t−1 based on current sample z t , current time step t and a predicted noiseˆ ."
  },
  {
    "index": 568,
    "source_corpus_id": 264288947,
    "ref_id": "b27",
    "citation_corpus_id": 254877603,
    "start": 2411,
    "end": 2432,
    "title": "When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories",
    "abstract": "Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the difficulty of encoding a wealth of world knowledge in their parameters. This paper aims to understand LMs' strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments on two open-domain entity-centric QA datasets: POPQA, our new dataset with 14k questions about long-tail entities, and EntityQuestions, a widely used opendomain QA dataset. We find that LMs struggle with less popular factual knowledge, and that retrieval augmentation helps significantly in these cases. Scaling, on the other hand, mainly improves memorization of popular knowledge, and fails to appreciably improve memorization of factual knowledge in the long tail. Based on those findings, we devise a new method for retrieval augmentation that improves performance and reduces inference costs by only retrieving non-parametric memories when necessary. 1  , et al.  2016. MS MARCO: A human generated machine reading comprehension dataset. . 2022. Quantifying memorization across neural language models. mented language models. . 2022. Large language models struggle to learn long-tail knowledge.",
    "prev": "1 1 Our code and trained models are available at https://selfrag.github.io/.",
    "curr": "INTRODUCTION\n\nState-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023;Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022).Retrieval-Augmented Generation (RAG) methods (Figure 1 left; Lewis et al.",
    "next": "2020;Guu et al."
  },
  {
    "index": 570,
    "source_corpus_id": 252873467,
    "ref_id": "b14",
    "citation_corpus_id": 233289412,
    "start": 3502,
    "end": 3523,
    "title": "Editing Factual Knowledge in Language Models",
    "abstract": "The factual knowledge acquired during pretraining and stored in the parameters of Language Models (LMs) can be useful in downstream tasks (e.g., question answering or textual inference). However, some facts can be incorrectly induced or become obsolete over time. We present KNOWLEDGEEDITOR, a method which can be used to edit this knowledge and, thus, fix 'bugs' or unexpected predictions without the need for expensive retraining or fine-tuning. Besides being computationally efficient, KNOWLEDGEEDITOR does not require any modifications in LM pretraining (e.g., the use of meta-learning). In our approach, we train a hyper-network with constrained optimization to modify a fact without affecting the rest of the knowledge; the trained hyper-network is then used to predict the weight update at test time. We show KNOWL-EDGEEDITOR's efficacy with two popular architectures and knowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and ii) a sequence-to-sequence BART model for question answering. With our method, changing a prediction on the specific wording of a query tends to result in a consistent change in predictions also for its paraphrases. We show that this can be further encouraged by exploiting (e.g., automatically-generated) paraphrases during training. Interestingly, our hyper-network can be regarded as a 'probe' revealing which components need to be changed to manipulate factual knowledge; our analysis shows that the updates tend to be concentrated on a small subset of components. 1",
    "prev": "To that end, several knowledge-editing methods have been proposed to insert new memories directly into specific model parameters.",
    "curr": "The approaches include constrained fine-tuning (Zhu et al., 2020), hypernetwork knowledge editing (De Cao et al., 2021;Hase et al., 2021;Mitchell et al., 2021;, and rank-one model editing (Meng et al., 2022).",
    "next": "However, this body of work is typically limited to updating at most a few dozen facts; a recent study evaluates on a maximum of 75 (Mitchell et al., 2022) whereas others primarily focus on single-edit cases."
  },
  {
    "index": 573,
    "source_corpus_id": 71145737,
    "ref_id": "b17",
    "citation_corpus_id": 49868626,
    "start": 27345,
    "end": 27363,
    "title": "META-LEARNING WITH LATENT EMBEDDING OPTIMIZATION",
    "abstract": "Gradient-based meta-learning techniques are both widely applicable and proficient at solving challenging few-shot learning and fast adaptation problems. However, they have practical difficulties when operating on high-dimensional parameter spaces in extreme low-data regimes. We show that it is possible to bypass these limitations by learning a data-dependent latent generative representation of model parameters, and performing gradient-based meta-learning in this lowdimensional latent space. The resulting approach, latent embedding optimization (LEO), decouples the gradient-based adaptation procedure from the underlying high-dimensional space of model parameters. Our evaluation shows that LEO can achieve state-of-the-art performance on the competitive miniImageNet and tieredImageNet few-shot classification tasks. Further analysis indicates LEO is able to capture uncertainty in the data, and can perform adaptation more effectively by optimizing in latent space. arXiv:1807.05960v3 [cs.LG]",
    "prev": "That said, in the past two years, there have been several other methods proposed for few-shot learning.",
    "curr": "Some bear similarity with MAML and correspond to a meta-learner trained to quickly adapt its parameters to various novel tasks (Ravi & Larochelle, 2017;Munkhdalai & Yu, 2017;Rusu et al., 2019;Yoon et al., 2018).",
    "next": "Others relate to Prototypical Networks by learning a data representation as well as a compact representation for a classifier of data under that representation (Bertinetto et al., 2019;Gidaris & Komodakis, 2018;Oreshkin et al., 2018;Gidaris & Komodakis, 2018)."
  },
  {
    "index": 574,
    "source_corpus_id": 238634584,
    "ref_id": "b30",
    "citation_corpus_id": 211842237,
    "start": 5855,
    "end": 5879,
    "title": "Published as a conference paper at ICLR 2020 DIRECTIONAL MESSAGE PASSING FOR MOLECULAR GRAPHS",
    "abstract": "Graph neural networks have recently achieved great successes in predicting quantum mechanical properties of molecules. These models represent a molecule as a graph using only the distance between atoms (nodes). They do not, however, consider the spatial direction from one atom to another, despite directional information playing a central role in empirical potentials for molecules, e.g. in angular potentials. To alleviate this limitation we propose directional message passing, in which we embed the messages passed between atoms instead of the atoms themselves. Each message is associated with a direction in coordinate space. These directional message embeddings are rotationally equivariant since the associated directions rotate with the molecule. We propose a message passing scheme analogous to belief propagation, which uses the directional information by transforming messages based on the angle between them. Additionally, we use spherical Bessel functions and spherical harmonics to construct theoretically well-founded, orthogonal representations that achieve better performance than the currently prevalent Gaussian radial basis representations while using fewer than 1 /4 of the parameters. We leverage these innovations to construct the directional message passing neural network (DimeNet). DimeNet outperforms previous GNNs on average by 76 % on MD17 and by 31 % on QM9. Our implementation is available online. 1 1 https://www.daml.in.tum.de/dimenet arXiv:2003.03123v2 [cs.LG] 5 Apr 2022Published as a conference paper at ICLR 2020 embeddings are equivariant with respect to the above transformations since the directions move with the molecule. Hence, they preserve the relative directional information between neighboring atoms. We propose to let message embeddings interact based on the distance between atoms and the angle between directions. Both distances and angles are invariant to translation, rotation, and inversion of the molecule, as required. Additionally, we show that the distance and angle can be jointly represented in a principled and effective manner by using spherical Bessel functions and spherical harmonics. We leverage these innovations to construct the directional message passing neural network (DimeNet). DimeNet can learn both molecular properties and atomic forces. It is twice continuously differentiable and solely based on the atom types and coordinates, which are essential properties for performing molecular dynamics simulations. DimeNet outperforms previous GNNs on average by 76 % on MD17 and by 31 % on QM9. Our paper's main contributions are:",
    "prev": "The Open Catalyst Project (OCP) provides a platform for comparing different architectures by predicting energies and forces from the periodic structure of catalytic surfaces (Chanussot et al., 2021).",
    "curr": "Our encoder and decoder PGNNs directly use GNN architectures developed for the OCP (Klicpera et al., 2020b;Godwin et al., 2021), which are also closely related to SE(3) equivariant networks (Thomas et al., 2018;Fuchs et al., 2020).",
    "next": "Quantum mechanical search of stable materials."
  },
  {
    "index": 576,
    "source_corpus_id": 3518190,
    "ref_id": "b0",
    "citation_corpus_id": 11212020,
    "start": 1453,
    "end": 1475,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores up to 32.8, without using even a single parallel sentence at training time.",
    "curr": "INTRODUCTION\n\nThanks to recent advances in deep learning (Sutskever et al., 2014;Bahdanau et al., 2015) and the availability of large-scale parallel corpora, machine translation has now reached impressive performance on several language pairs .",
    "next": "However, these models work very well only when provided with massive amounts of parallel data, in the order of millions of parallel sentences."
  },
  {
    "index": 577,
    "source_corpus_id": 239009574,
    "ref_id": "b22",
    "citation_corpus_id": 213085920,
    "start": 3232,
    "end": 3236,
    "title": "Published as a conference paper at ICLR 2020 STRATEGIES FOR PRE-TRAINING GRAPH NEURAL NETWORKS",
    "abstract": "Many applications of machine learning require a model to make accurate predictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that naïve strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4% absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction. * Equal contribution. Project website, data and code: Attribute Masking Supervised Attribute Prediction Structural Similarity Prediction Structure prediction Context Prediction (b) Categorization of our pre-training methods Graph space Node space Graph embeddings Node embeddings Linear classifier Figure 1: (a.i) When only node-level pre-training is used, nodes of different shapes (semantically different nodes) can be well separated, however, node embeddings are not composable, and thus resulting graph embeddings (denoted by their classes, + and −) that are created by pooling node-level embeddings are not separable. (a.ii) With graph-level pre-training only, graph embeddings are well separated, however the embeddings of individual nodes do not necessarily capture their domainspecific semantics. (a.iii) High-quality node embeddings are such that nodes of different types are well separated, while at the same time, the embedding space is also composable. This allows for accurate and robust representations of entire graphs and enables robust transfer of pre-trained models to a variety of downstream tasks. (b) Categorization of pre-training methods for GNNs. Crucially, our methods, i.e., Context Prediction, Attribute Masking, and graph-level supervised pre-training (Supervised Attribute Prediction) enable both node-level and graph-level pre-training.matter of increasing the number of labeled pre-training datasets that are from the same domain as the downstream task. Instead, it requires substantial domain expertise to carefully select examples and target labels that are correlated with the downstream task of interest. Otherwise, the transfer of knowledge from related pre-training tasks to a new downstream task can harm generalization, which is known as negative transfer(Rosenstein et al., 2005)and significantly limits the applicability and reliability of pre-trained models.Present work. Here, we focus on pre-training as an approach to transfer learning in Graph Neural Networks (GNNs)(Kipf & Welling, 2017;Hamilton et al., 2017a;Ying et al., 2018b;Xu et al., 2019;2018)for graph-level property prediction. Our work presents two key contributions.(1) We conduct the first systematic large-scale investigation of strategies for pre-training GNNs. For that, we build two large new pre-training datasets, which we share with the community: a chemistry dataset with 2 million graphs and a biology dataset with 395K graphs. We also show that large domain-specific datasets are crucial to investigate pre-training and that existing downstream benchmark datasets are too small to evaluate models in a statistically reliable way.(2) We develop an effective pretraining strategy for GNNs and demonstrate its effectiveness and its ability for out-of-distribution generalization on hard transfer-learning problems.",
    "prev": "Falling broadly into the second category, our paper studies self-supervised molecular representation learning by leveraging the consistency between 3D geometry and 2D topology.",
    "curr": "Motivated by the prominent success of the pretraining-finetuning pipeline [17], unsupervisedly pretrained graph neural networks for molecules yields promising performance on downstream tasks and becomes increasingly popular [42,54,82,90,103,104].",
    "next": "The key to pre-training lies in finding an effective proxy task (i.e., training objective) to leverage the power of large unlabeled datasets."
  },
  {
    "index": 578,
    "source_corpus_id": 241035330,
    "ref_id": "b11",
    "citation_corpus_id": 2381275,
    "start": 2402,
    "end": 2424,
    "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context *",
    "abstract": "We introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAM-BADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA exemplifies a wide range of linguistic phenomena, and that none of several state-ofthe-art language models reaches accuracy above 1% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the development of new models capable of genuine understanding of broad context in natural language text. * Denis and Germán share first authorship. Marco, Gemma, and Raquel share senior authorship.",
    "prev": "Introduction\n\nLarge language models (LMs) such as GPT-3 (Brown et al., 2020, Lieber et al., 2021, Radford et al., 2019, Wang and Komatsuzaki, 2021 are pretrained on massive text corpora to predict the next word given previous words.",
    "curr": "They demonstrate the surprising ability to do in-context learning, where an LM \"learns\" to do a task simply by conditioning on a prompt containing input-output pairs, achieving SOTA results on LAMBADA (Paperno et al., 2016) and TriviaQA (Joshi et al., 2017) tasks (18% and 3% over previous SOTA (Brown et al., 2020)).",
    "next": "For example, consider the task of predicting nationalities from names."
  },
  {
    "index": 582,
    "source_corpus_id": 263834782,
    "ref_id": "b6",
    "citation_corpus_id": 52967399,
    "start": 2301,
    "end": 2322,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Henceforth, we will refer to this masked language model as the auxiliary model 1 .",
    "curr": "This pre-training task, known as replaced token detection (RTD), has shown great advantages in training and data efficiency compared to other pre-training tasks such as masked language modeling (MLM) (Devlin et al., 2019).",
    "next": "ELECTRAstyle pre-training and its variations have been increasingly popular in advancing natural language understanding capabilities (Meng et al., 2021;Chi et al., 2021;Meng et al., 2022;He et al., 2021;."
  },
  {
    "index": 585,
    "source_corpus_id": 238419211,
    "ref_id": "b19",
    "citation_corpus_id": 216867841,
    "start": 2364,
    "end": 2383,
    "title": "Named Entity Recognition without Labelled Data: A Weak Supervision Approach",
    "abstract": "Named Entity Recognition (NER) performance often degrades rapidly when applied to target domains that differ from the texts observed during training. When in-domain labelled data is available, transfer learning techniques can be used to adapt existing NER models to the target domain. But what should one do when there is no hand-labelled data for the target domain? This paper presents a simple but powerful approach to learn NER models in the absence of labelled data through weak supervision. The approach relies on a broad spectrum of labelling functions to automatically annotate texts from the target domain. These annotations are then merged together using a hidden Markov model which captures the varying accuracies and confusions of the labelling functions. A sequence labelling model can finally be trained on the basis of this unified annotation. We evaluate the approach on two English datasets (CoNLL 2003 and news articles from Reuters and Bloomberg) and demonstrate an improvement of about 7 percentage points in entity-level F 1 scores compared to an out-of-domain neural NER model.",
    "prev": "s.Moreover, we provide a theoretically-principled test of the distinguishability of PLRM for unseen labels, along with a generalization bound.On both image and text classification tasks as well as an industrial advertising application, we demonstrate the advantages of PLRM by outperforming baselines by a margin of 2%-9%.",
    "curr": "INTRODUCTION\n\nOne of the greatest bottlenecks of using modern machine learning models is the need for substantial amounts of manually-labeled training data.In real-world applications, such manual annotations are typically time-consuming, labor-intensive and static.To reduce the efforts of annotation, researchers have proposed Weak Supervision (WS) frameworks (Ratner et al., 2016;2018;2019;Fu et al., 2020) for synthesizing labels from multiple weak supervision sources, e.g., heuristics, knowledge bases, or pre-trained classifiers.These frameworks have been widely applied on various machine learning tasks (Dunnmon et al., 2020;Fries et al., 2021;Safranchik et al., 2020;Lison et al., 2020;Zhou et al., 2020;Hooper et al., 2021;Zhan et al., 2019;Varma et al., 2019) and industrial data (Bach et al., 2019).Among them, data programming (Ratner et al., 2016), one representative example that generalizes many approaches in the literature, represents weak supervision sources as labeling functions (LFs) and synthesizes training labels using Probabilistic Graphical Model (PGM).",
    "next": "Given both the increasing popularity of WS and the general increase in open-source availability of machine learning models and tools, there is a rising tide of available supervision sources that WS frameworks and practitioners could potentially leverage, including pre-trained machine learning models or prediction APIs (Chen et al., 2020;d'Andrea & Mintz, 2019;Yao et al., 2017).However, existing WS frameworks only utilize weak supervision sources with the same label space as the target task.This incompatibility largely limits the scope of usable sources, necessitating manual effort from domain experts to pr"
  },
  {
    "index": 588,
    "source_corpus_id": 248887351,
    "ref_id": "b16",
    "citation_corpus_id": 9192723,
    "start": 13304,
    "end": 13323,
    "title": "Constructing Datasets for Multi-hop Reading Comprehension Across Documents",
    "abstract": "Most Reading Comprehension methods limit themselves to queries which can be answered using a single sentence, paragraph, or document. Enabling models to combine disjoint pieces of textual evidence would extend the scope of machine comprehension methods, but currently there exist no resources to train and test this capability. We propose a novel task to encourage the development of models for text understanding across multiple documents and to investigate the limits of existing methods. In our task, a model learns to seek and combine evidence -effectively performing multi-hop (alias multi-step) inference. We devise a methodology to produce datasets for this task, given a collection of query-answer pairs and thematically linked documents. Two datasets from different domains are induced, 1 and we identify potential pitfalls and devise circumvention strategies. We evaluate two previously proposed competitive models and find that one can integrate information across documents. However, both models struggle to select relevant information, as providing documents guaranteed to be relevant greatly improves their performance. While the models outperform several strong baselines, their best accuracy reaches 42.9% compared to human performance at 74.0% -leaving ample room for improvement.",
    "prev": "To this end, we evaluated LLMs from the Gopher family in a 5-shot 1 setting on a larger set of 50 tasks that touch on different aspects of logical reasoning and vary in terms of the number of reasoning steps required, presence or absence of negation, whether the relevant context information was provided, and whether the model is required to evaluate the accuracy of multiple choices or generate the answer among others.",
    "curr": "The additional tasks were collected from six sources: bAbI (Weston et al., 2015), BigBench (Ghazal et al., 2017), AAC (Betz et al., 2020), Jeopardy (Tunguz, 2019), Proof Writer (Tafjord et al., 2020) and 2WikiMultiHop (Welbl et al., 2018) (see Fig.",
    "next": "S5a in Supplementary Information for raw results)."
  },
  {
    "index": 590,
    "source_corpus_id": 263310823,
    "ref_id": "b3",
    "citation_corpus_id": 225039882,
    "start": 25972,
    "end": 25973,
    "title": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. 1 arXiv:2010.11929v2 [cs.CV] 3 Jun 2021Published as a conference paper at ICLR 2021 inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.However, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks.RELATED WORKTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global selfattention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.",
    "prev": "t is located with respect to the rest of the training set.This allows us to answer the question: Which data record S has the most significant influence and at which query Q?",
    "curr": "Proposition 4.1 (First-order optimality condition for influence).Consider a dataset D, a differing point S, and a kernel function K satisfying the same conditions of Theorem 3.1.Then, we have that\n∇ Q M (Q) | Q=S = −α −2 (1 − K SD M −1 D K DS )(y S − K SD M −1 D y D ) 2 KSD M −1 D K DS , where M (Q) is the mean distance LOOD in C.1, K SD , K DS , K DD are kernel matrices as defined in Section 2, KSD = ∂ ∂Q K QD | Q=S , M D = K DD + σ 2 I, and α = 1 − K SD M −1 D K DS + σ 2 .",
    "next": "Our first observation from Proposition 4.1 is that when S is far from the dataset D in the sense that K SD ≈ 0, the mean distance LOOD gradient ∇ Q M (Q) | Q=S is close to 0, i.e., the differing data S is a stationary point for the mean distance LOOD objective.We also analyze the second-order Hessian condition of mean distance LOOD in Lemma G. 4, and prove that the differing data S has locally maximal influence, as long as S far from the remaining training dataset D, and the data labels are bounded (which is the case in a classification task).In the opposite case where S is close to D in the sense that y S ≈ K SD M −1 D y D (i.e., S is perfectly predicted by the GP trained on remaining dataset D), then the gradient\n∇ Q M (Q) | Q=S of mean distance LOOD is also close to zero."
  },
  {
    "index": 591,
    "source_corpus_id": 3524184,
    "ref_id": "b2",
    "citation_corpus_id": 11383178,
    "start": 7570,
    "end": 7590,
    "title": "IMPORTANCE WEIGHTED AUTOENCODERS",
    "abstract": "The variational autoencoder (VAE; ) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.",
    "prev": "(2015)).",
    "curr": "MARGINAL LOG-LIKELIHOOD ESTIMATION\n\nWe use two bounds to estimate the marginal log-likelihood of a model: IWAE (Burda et al., 2016) and AIS (Neal, 2001).",
    "next": "Here we describe the IWAE bound."
  },
  {
    "index": 593,
    "source_corpus_id": 237292758,
    "ref_id": "b24",
    "citation_corpus_id": 6628106,
    "start": 1545,
    "end": 1565,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "This suggests that the inferior generalization performance of Adam is fundamentally tied to the nonconvex landscape of deep learning optimization.",
    "curr": "Introduction\n\nAdaptive gradient methods (Duchi et al., 2011;Hinton et al., 2012;Kingma and Ba, 2015;Reddi et al., 2018) such as Adam are very popular optimizers for training deep neural networks.",
    "next": "By adjusting the learning rate coordinate-wisely based on historical gradient information, they are known to be able to automatically choose appropriate learning rates to achieve fast convergence in training."
  },
  {
    "index": 595,
    "source_corpus_id": 254408767,
    "ref_id": "b33",
    "citation_corpus_id": 239768373,
    "start": 33117,
    "end": 33135,
    "title": "Published as a conference paper at ICLR 2022 LEARNING WITH NOISY LABELS REVISITED: A STUDY USING REAL-WORLD HUMAN ANNOTATIONS",
    "abstract": "Existing research on learning with noisy labels mainly focuses on synthetic label noise. The synthetic noise, though has clean structures which greatly enabled statistical analyses, often fails to model the real-world noise patterns. The recent literature has observed several efforts to offer real-world noisy datasets, e.g., Food-101N, WebVision, and Clothing1M. Yet the existing efforts suffer from two caveats: firstly, the lack of ground-truth verification makes it hard to theoretically study the property and treatment of real-world label noise. Secondly, these efforts are often of large scales, which may result in unfair comparisons of robust methods within reasonable and accessible computation power. To better understand real-world label noise, it is important to establish controllable, easy-to-use and moderate-sized real-world noisy datasets with both ground-truth and noisy labels. This work presents two new benchmark datasets, which we name as CIFAR-10N, CIFAR-100N (jointly we call them CIFAR-N), equipping the training datasets of CIFAR-10 and CIFAR-100 with human-annotated real-world noisy labels we collected from Amazon Mechanical Turk. We quantitatively and qualitatively show that realworld noisy labels follow an instance-dependent pattern rather than the classically assumed and adopted ones (e.g., class-dependent label noise). We then initiate an effort to benchmarking a subset of the existing solutions using CIFAR-10N and CIFAR-100N. We further proceed to study the memorization of correct and wrong predictions, which further illustrates the difference between human noise and class-dependent synthetic noise. We show indeed the real-world noise patterns impose new and outstanding challenges as compared to synthetic label noise. These observations require us to rethink the treatment of noisy labels, and we hope the availability of these two datasets would facilitate the development and evaluation of future learning with noisy label solutions. The corresponding datasets and the leaderboard are available at http://noisylabels.com.",
    "prev": "We can observe that models in Region 1 have the highest test accuracies in each dataset.",
    "curr": "Computation (a) The Clothing-1M dataset (b) The Animal-10N dataset (c) The CIFAR-10N dataset Selection (d) The Clothing-1M dataset (e) The Animal-10N dataset (f) The CIFAR-10N dataset Evaluation (g) The Clothing-1M dataset (h) The Animal-10N dataset (i) The CIFAR-10N dataset\n\nExperiments on Real-world Datasets with noisy labels\n\nTo evaluate the performance of our approach on real-world datasets, we have conducted additional experiments on the Clothing-1M dataset [Xiao et al., 2015], which is a dataset with 1M images of clothes, on the Animal-10N dataset [Song et al., 2019], which is a dataset with 50k images of animals and on the CIFAR-10N dataset [Wei et al., 2022], which is the CIFAR-10 dataset with human-annotated noisy labels obtained from Amazon Mechanical Turk.",
    "next": "In the Clothing-1M dataset, the images have been labeled from the texts that accompany them, hence there are both clean and noisy labels in the set, and in the Animal-10N dataset, the images have been gathered and labeled from search engines."
  },
  {
    "index": 597,
    "source_corpus_id": 256358777,
    "ref_id": "b13",
    "citation_corpus_id": 3144218,
    "start": 1370,
    "end": 1392,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "These new theoretical results are supported by corresponding numerical investigations.",
    "curr": "INTRODUCTION\n\nGraph Convolutional Networks (GCNs) (Kipf & Welling, 2017;Hammond et al., 2011;Defferrard et al., 2016) generalize Euclidean convolutional networks to the graph setting by replacing convolutional filters by functional calculus filters; i.e.",
    "next": "scalar functions applied to a suitably chosen graph-shift-oprator capturing the geometry of the underlying graph."
  },
  {
    "index": 598,
    "source_corpus_id": 252683295,
    "ref_id": "b3",
    "citation_corpus_id": 247475874,
    "start": 2131,
    "end": 2150,
    "title": "E-KAR : A Benchmark for Rationalizing Natural Language Analogical Reasoning",
    "abstract": "The ability to recognize analogies is fundamental to human cognition. Existing benchmarks to test word analogy do not reveal the underneath process of analogical reasoning of neural models. Holding the belief that models capable of reasoning should be right for the right reasons, we propose a first-of-its",
    "prev": "1 Code and datasets are available in https://github.com/zjunlp/MKG_Analogy.",
    "curr": "INTRODUCTION\n\nAnalogical reasoning -the ability to perceive and use relational similarity between two situations or events -holds an important place in human cognition (Johnson-Laird, 2006;Bengio et al., 2021;Chen et al., 2022a) and can provide back-end support for various fields such as education (Thagard, 1992), creativity (Goel, 1997), thus appealing to the AI community.",
    "next": "Early, Mikolov et al."
  },
  {
    "index": 599,
    "source_corpus_id": 53514358,
    "ref_id": "b2",
    "citation_corpus_id": 30745030,
    "start": 2650,
    "end": 2672,
    "title": "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data",
    "abstract": "Neural networks exhibit good generalization behavior in the over-parameterized regime, where the number of network parameters exceeds the number of observations. Nonetheless, current generalization bounds for neural networks fail to explain this phenomenon. In an attempt to bridge this gap, we study the problem of learning a two-layer over-parameterized neural network, when the data is generated by a linearly separable function. In the case where the network has Leaky ReLU activations, we provide both optimization and generalization guarantees for overparameterized networks. Specifically, we prove convergence rates of SGD to a global minimum and provide generalization guarantees for this global minimum that are independent of the network size. Therefore, our result clearly shows that the use of SGD for optimization both finds a global minimum, and avoids overfitting despite the high capacity of the model. This is the first theoretical demonstration that SGD can avoid overfitting, when learning over-specified neural network classifiers.",
    "prev": "Yet these models have achieved state-ofthe-art generalization error on many real-world tasks.",
    "curr": "This observation has spurred an active line of research (Soudry et al., 2018;Brutzkus et al., 2018;Li & Liang, 2018) that has tried to understand what properties are possessed by stochastic gradient descent (SGD) training of deep networks that allows these networks to generalize well.",
    "next": "One particularly promising line of work in this area (Neyshabur et al., 2017;Arora et al., 2018) has been bounds that utilize the noise-resilience of deep networks on training data i.e., how much the training loss of the network changes with noise injected into the parameters, or roughly, how wide is the training loss minimum."
  },
  {
    "index": 600,
    "source_corpus_id": 244714571,
    "ref_id": "b15",
    "citation_corpus_id": 52920808,
    "start": 17523,
    "end": 17540,
    "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks",
    "abstract": "One of the mystery in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an m hidden node shallow neural network with ReLU activation and n training data, we show as long as m is large enough and the data is non-degenerate, randomly initialized gradient descent converges a globally optimal solution with a linear convergence rate for the quadratic loss function.Our analysis is based on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods. * Equal Contribution.",
    "prev": "The first part of Assumption 4.1 assures that H is non-singular and that the input data lies in the unit sphere S d−1 .",
    "curr": "Such assumption is commonly made in overparameterized neural network literature (Arora et al., 2019;Du et al., 2019b;a;Cao & Gu, 2019).",
    "next": "The non-singularity is satisfied when e.g."
  },
  {
    "index": 602,
    "source_corpus_id": 221041408,
    "ref_id": "b2",
    "citation_corpus_id": 213152193,
    "start": 9171,
    "end": 9191,
    "title": "ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS",
    "abstract": "Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK]  and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.",
    "prev": "(2019) observes that parameters in different layers have similar attention distribution, and propose a parameter distillation method from shallow layers to deep layers.",
    "curr": "Another notable work is ELECTRA (Clark et al., 2019), which develops a new task using one discriminator and one generator.",
    "next": "The generator corrupts the sentence, and the discriminator is trained to predict whether each word in the corrupted sentence is replaced or not."
  },
  {
    "index": 603,
    "source_corpus_id": 247476419,
    "ref_id": "b8",
    "citation_corpus_id": 49667762,
    "start": 2546,
    "end": 2568,
    "title": "UNIVERSAL TRANSFORMERS",
    "abstract": "Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset. * Equal contribution, alphabetically by last name. † Work performed while at Google Brain.",
    "prev": "One solution is cross-layer parameter sharing, which reduces the memory needed to store parameters, which can also reduce the cost of communicating model updates in distributed training (Lan et al., 2020;Jaegle et al., 2021) and federated learning (Konečný et al., 2016;McMahan et al., 2017), as the model is smaller, and can help avoid overfitting (Jaegle et al., 2021).",
    "curr": "However, prior work in parameter sharing (e.g., Dehghani et al., 2019;Savarese & Maire, 2019;Lan et al., 2020;Jaegle et al., 2021) has two significant limitations.",
    "next": "First, they rely on suboptimal hand-crafted techniques for deciding where and how sharing occurs."
  },
  {
    "index": 604,
    "source_corpus_id": 247597138,
    "ref_id": "b0",
    "citation_corpus_id": 61153666,
    "start": 6808,
    "end": 6813,
    "title": "SECTOR: A Neural Model for Coherent Topic Segmentation and Classification",
    "abstract": "When searching for information, a human reader first glances over a document, spots relevant sections, and then focuses on a few sentences for resolving her intention. However, the high variance of document structure complicates the identification of the salient topic of a given section at a glance. To tackle this challenge, we present SECTOR, a model to support machine reading systems by segmenting documents into coherent sections and assigning topic labels to each section. Our deep neural network architecture learns a latent topic embedding over the course of a document. This can be leveraged to classify local topics from plain text and segment a document at topic shifts. In addition, we contribute WikiSection, a publicly available data set with 242k labeled sections in English and German from two distinct domains: diseases and cities. From our extensive evaluation of 20 architectures, we report a highest score of 71.6% F1 for the segmentation and classification of 30 topics from the English city domain, scored by our SECTOR long short-term memory model with Bloom filter embeddings and bidirectional segmentation. This is a significant improvement of 29.5 points F1 over state-of-the-art CNN classifiers with baseline segmentation. 1 Our source code is available under the Apache License 2.0 at https://github.com/sebastianarnold/ SECTOR.",
    "prev": "Forcing autoregressive models to generate longer texts exacerbates this incoherence because the models struggle to extrapolate beyond their expected text end point.",
    "curr": "Prior work has tried to address the problem of generating globally coherent text with planning-based approaches (Puduppully et al., 2019;Moryossef et al., 2019;Fan et al., 2019;Kiddon et al., 2016).",
    "next": "However, planning-based approaches rely on domain-specific heuristics for capturing text structure and dynamics."
  },
  {
    "index": 605,
    "source_corpus_id": 252683988,
    "ref_id": "b44",
    "citation_corpus_id": 23892230,
    "start": 2156,
    "end": 2178,
    "title": "Challenges in Data-to-Document Generation",
    "abstract": "Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of database records. In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this task. In particular, we introduce a new, large-scale corpus of data records paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods. Experiments show that these models produce fluent text, but fail to convincingly approximate humangenerated documents. Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copy-and reconstructionbased extensions lead to noticeable improvements.",
    "prev": "With SLiC, we exceed or match SOTA results on a wide range of generation tasks spanning abstractive summarization, question generation, abstractive question answering and data-to-text generation, even with modest-sized models.",
    "curr": "INTRODUCTION\n\nConditional language generation aims to generate natural language text based on input context, and includes many useful and hard tasks such as abstractive summarization (Mani, 2001;Nenkova and McKeown, 2011), generative question answering (Bajaj et al., 2016), question generation (Zhou et al., 2017) and data-to-text (Wiseman et al., 2017;Gardent et al., 2017) tasks.",
    "next": "Pretraining large Transformer encoder-decoder models and fine-tuning them on downstream tasks is the common paradigm to address these tasks (Raffel et al., 2020;Lewis et al., 2019;Tay et al., 2022;Zhang et al., 2019a)."
  },
  {
    "index": 606,
    "source_corpus_id": 263608698,
    "ref_id": "b8",
    "citation_corpus_id": 215717103,
    "start": 6356,
    "end": 6375,
    "title": "Collecting Highly Parallel Data for Paraphrase Evaluation",
    "abstract": "A lack of standard datasets and evaluation metrics has prevented the field of paraphrasing from making the kind of rapid progress enjoyed by the machine translation community over the last 15 years. We address both problems by presenting a novel data collection framework that produces highly parallel text data relatively inexpensively and on a large scale. The highly parallel nature of this data allows us to use simple n-gram comparisons to measure both the semantic adequacy and lexical dissimilarity of paraphrase candidates. In addition to being simple and efficient to compute, experiments show that these metrics correlate highly with human judgments.",
    "prev": "age), and AL (audio-language) data pairs.The videos of previous datasets are always truncated segments from long videos (Miech et al., 2019;Xue et al., 2022), resulting in fragmented semantics.To avoid this problem, we construct our video-text pairs from short videos with complete stories.To ensure the quality of the central language modality, we perform multi-view text generation and enhancement on VIDAL-10M.",
    "curr": "The proposed LanguageBind ensures that we can extend vision-language to multiple (N) modalities, and our dataset VIDAL-10M benefits more downstream tasks beyond VL tasks, including video retrieval (Luo et al., 2022), depth classification (Cao et al., 2017), infrared classification (Baffa & Lattari, 2018) and audio classification (Palanisamy et al., 2020).In text-to-video zero-shot retrieval based on CLIP-Large, LanguageBind achieves state-of-the-art (SOTA) performance on four datasets, surpassing InterVideo (Wang et al., 2022c) by 1.9% on MSR-VTT (Xu et al., 2016), 8.8% on MSVD (Chen & Dolan, 2011), 6.3% on DiDeMo (Anne Hendricks et al., 2017), and 4.4% on ActivityNet (Caba Heilbron et al., 2015).For zero-shot classification on depth and infrared data, LanguageBind achieves a substantial performance advantage over ImageBind.LanguageBind attains top-1 accuracy of 87.2% and 65.1% on LLVIP and NYU-D, respectively, outperforming Im-ageBind by 23.8% and 11.1%.For zero-shot audio classification tasks, LanguageBind outperforms ImageBind with a 22.9% higher top-1 accuracy on the ESC50 dataset.",
    "next": "We summarize our primary contributions as follows:\n\n• We propose LanguageBind, the langauge-based multi-modal pretraining approach.During the pretraining process, all modalities gradually align with the language modality through contrastive learning, and these modalities are unified within a shared embedding space."
  },
  {
    "index": 608,
    "source_corpus_id": 247595075,
    "ref_id": "b21",
    "citation_corpus_id": 4606753,
    "start": 2916,
    "end": 2937,
    "title": "NEURAL-GUIDED DEDUCTIVE SEARCH FOR REAL- TIME PROGRAM SYNTHESIS FROM EXAMPLES",
    "abstract": "Synthesizing user-intended programs from a small number of input-output examples is a challenging problem with several important applications like spreadsheet manipulation, data wrangling and code refactoring. Existing synthesis systems either completely rely on deductive logic techniques that are extensively handengineered or on purely statistical models that need massive amounts of data, and in general fail to provide real-time synthesis on challenging benchmarks. In this work, we propose Neural Guided Deductive Search (NGDS), a hybrid synthesis technique that combines the best of both symbolic logic techniques and statistical models. Thus, it produces programs that satisfy the provided specifications by construction and generalize well on unseen examples, similar to data-driven systems. Our technique effectively utilizes the deductive search framework to reduce the learning problem of the neural component to a simple supervised learning setup. Further, this allows us to both train on sparingly available real-world data and still leverage powerful recurrent neural network encoders. We demonstrate the effectiveness of our method by evaluating on real-world customer scenarios by synthesizing accurate programs with up to 12× speed-up compared to state-ofthe-art systems.",
    "prev": "First, learning to search works best when it exploits the symbolic scaffolding of existing search algorithms already proven useful for the problem domain.",
    "curr": "For example, AlphaGo exploits Monte Carlo Tree Search (Silver et al., 2016), while NGDS exploits top-down deductive search (Kalyan et al., 2018).",
    "next": "We engineer CROSSBEAM around bottom-up enumerative search (Udupa et al., 2013), a backbone of several successful recent program synthesis algorithms (Shi et al., 2020a;Odena et al., 2021;Barke et al., 2020)."
  },
  {
    "index": 610,
    "source_corpus_id": 245769552,
    "ref_id": "b20",
    "citation_corpus_id": 52967399,
    "start": 9232,
    "end": 9253,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Clusters of signalprocessing-based acoustic features, e.g., Mel-frequency cepstral coefficients (MFCC), exhibit nontrivial correlations with the inherent acoustic units of speech inputs.",
    "curr": "Using (A 1:T , z a 1:T ) pairs, the second step learns new feature representations by minimizing a masked prediction loss, similar to masked language modeling in BERT (Devlin et al., 2019).",
    "next": "The pressure to predict cluster assignments of masked audio regions forces the model to learn good local acoustic representations for unmasked regions and long-range temporal dependencies between latent features."
  },
  {
    "index": 612,
    "source_corpus_id": 252967802,
    "ref_id": "b9",
    "citation_corpus_id": 257232596,
    "start": 5074,
    "end": 5095,
    "title": "Published as a conference paper at ICLR 2023 FROM t-SNE TO UMAP WITH CONTRASTIVE LEARNING",
    "abstract": "Neighbor embedding methods t-SNE and UMAP are the de facto standard for visualizing high-dimensional datasets. Motivated from entirely different viewpoints, their loss functions appear to be unrelated. In practice, they yield strongly differing embeddings and can suggest conflicting interpretations of the same data. The fundamental reasons for this and, more generally, the exact relationship between t-SNE and UMAP have remained unclear. In this work, we uncover their conceptual connection via a new insight into contrastive learning methods. Noisecontrastive estimation can be used to optimize t-SNE, while UMAP relies on negative sampling, another contrastive method. We find the precise relationship between these two contrastive methods and provide a mathematical characterization of the distortion introduced by negative sampling. Visually, this distortion results in UMAP generating more compact embeddings with tighter clusters compared to t-SNE. We exploit this new conceptual connection to propose and implement a generalization of negative sampling, allowing us to interpolate between (and even extrapolate beyond) t-SNE and UMAP and their respective embeddings. Moving along this spectrum of embeddings leads to a trade-off between discrete / local and continuous / global structures, mitigating the risk of over-interpreting ostensible features of any single embedding. We provide a PyTorch implementation.",
    "prev": "A number of algorithms based on that framework, such as LargeVis (Tang et al., 2016), UMAP (McInnes et al., 2018), and TriMap (Amid & Warmuth, 2019) have been developed and got widespread adoption in recent years in a variety of application fields.",
    "curr": "All of them are closely related to SNE (Böhm et al., 2022;Damrich et al., 2023) and rely on the kNN graph of the data.",
    "next": "NE algorithms have been used to visualize latent representations of neural networks trained in a supervised setting (e.g."
  },
  {
    "index": 613,
    "source_corpus_id": 52877454,
    "ref_id": "b2",
    "citation_corpus_id": 4630420,
    "start": 6172,
    "end": 6201,
    "title": "DEEP GAUSSIAN EMBEDDING OF GRAPHS: UNSUPERVISED INDUCTIVE LEARNING VIA RANKING",
    "abstract": "Methods that learn representations of nodes in a graph play a critical role in network analysis since they enable many downstream learning tasks. We propose Graph2Gauss -an approach that can efficiently learn versatile node embeddings on large scale (attributed) graphs that show strong performance on tasks such as link prediction and node classification. Unlike most approaches that represent nodes as point vectors in a low-dimensional continuous space, we embed each node as a Gaussian distribution, allowing us to capture uncertainty about the representation. Furthermore, we propose an unsupervised method that handles inductive learning scenarios and is applicable to different types of graphs: plain/attributed, directed/undirected. By leveraging both the network structure and the associated node attributes, we are able to generalize to unseen nodes without additional training. To learn the embeddings we adopt a personalized ranking formulation w.r.t. the node distances that exploits the natural ordering of the nodes imposed by the network structure. Experiments on real world networks demonstrate the high performance of our approach, outperforming state-of-the-art network embedding methods on several different tasks. Additionally, we demonstrate the benefits of modeling uncertainty -by analyzing it we can estimate neighborhood diversity and detect the intrinsic latent dimensionality of a graph.",
    "prev": "Contrastive methods are central to many popular word-embedding methods (Collobert & Weston, 2008;Mnih & Kavukcuoglu, 2013;Mikolov et al., 2013), but they are found in many unsupervised algorithms for learning representations of graphstructured input as well.",
    "curr": "There are many ways to score a representation, but in the graph literature the most common techniques use classification (Perozzi et al., 2014;Grover & Leskovec, 2016;Kipf & Welling, 2016b;Hamilton et al., 2017b), though other scoring functions are used (Duran & Niepert, 2017;Bojchevski & Günnemann, 2018).",
    "next": "DGI is also contrastive in this respect, as our objective is based on classifying local-global pairs and negative-sampled counterparts."
  },
  {
    "index": 615,
    "source_corpus_id": 209832425,
    "ref_id": "b8",
    "citation_corpus_id": 54443381,
    "start": 5528,
    "end": 5551,
    "title": "EFFICIENT LIFELONG LEARNING WITH A-GEM",
    "abstract": "In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC  and other regularizationbased methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency. 1",
    "prev": "Regularization and replay approaches fix the model architecture before training and prevent catastrophic forgetting by regularizing the change of a specific set of weights or replaying previously learned data.",
    "curr": "Hybrids of replay and regularization also exist, such as Gradient Episodic Memory (GEM) (Lopez-Paz & Ranzato, 2017;Chaudhry et al., 2019a).",
    "next": "On the other hand, methods based on expansion add new network components to learn new data."
  },
  {
    "index": 616,
    "source_corpus_id": 13019454,
    "ref_id": "b17",
    "citation_corpus_id": 629094,
    "start": 10109,
    "end": 10130,
    "title": "Word representations: A simple and general method for semi-supervised learning",
    "abstract": "If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize. com/projects/wordreprs/",
    "prev": "E x ∈ R |V |×E is the embedding matrix, where |V | is the number of unique events (the vocabulary size) and E is the embedding dimension.",
    "curr": "The use of embedding provides a dense representation for an event that improves learning (Turian et al., 2010).",
    "next": "Through training, the embedding vector of an event encodes its meaning relative to other events."
  },
  {
    "index": 621,
    "source_corpus_id": 258999337,
    "ref_id": "b16",
    "citation_corpus_id": 6628106,
    "start": 32876,
    "end": 32895,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "o the points within its intermediate predicted mask from the previous layer.Then in the click-to-click attention module, each click query self-attends to each other to realize inter-query communications (Eq.2).To make the point features click-aware, we let the point features cross-attend to the click queries in a scene-toclick attention module (Eq.3).In equations ( 1) to (3), we omit the layer normalization and dropout for simplicity.W Q , W K , W V are learnable weights for query, key and value as in the standard attention mechanism (Vaswani et al., 2017).In all attention modules, we add the positional part to their respective keys/queries.",
    "curr": "Q l+1 c = softmax W Q (Q l c + Q p ) • W K (F l c + F p ) + H √ D • W V F l c + Q l c\n(1)\nQ l+1 c = softmax W Q (Q l c + Q p ) • W K (Q l c + Q p √ D • W V Q l c + Q l c\n(2)    Training.We set the λ CE = 1 and the λ Dice = 2 in the loss function.The loss is applied to every intermediate layer of the click attention module.We use the Adam optimizer (Kingma & Ba, 2015) with a weight decay factor 1e-4.We train the model on ScanNet40 for 1100 epochs with an initial learning rate 1e-4, which is decayed by 0.1 after 1000 epochs.Due to the smaller data size, we train the model on ScanNet20 for 850 epochs with an initial learning rate 1e-4, which is decayed by 0.1 after 800 epochs.We use a single TITAN RTX GPU with 24GB memory for training.",
    "next": "F l+1 c = softmax W Q (F l c + F p ) • W K (Q l c + Q p ) √ D • W V Q l c + F l c(3)\n\nB ADDITIONAL RESULTS\n\nB.1 ABLATION ON QUERY FUSION STRATEGY In our query fusion module, we apply a per-point max operation to aggregate click-specific masks to region-specific masks (Fig."
  },
  {
    "index": 622,
    "source_corpus_id": 225076227,
    "ref_id": "b24",
    "citation_corpus_id": 52901777,
    "start": 7946,
    "end": 7968,
    "title": "Bias-Reduced Uncertainty Estimation for Deep Neural Classifiers",
    "abstract": "We consider the problem of uncertainty estimation in the context of (non-Bayesian) deep neural classification. In this context, all known methods are based on extracting uncertainty signals from a trained network optimized to solve the classification problem at hand. We demonstrate that such techniques tend to introduce biased estimates for instances whose predictions are supposed to be highly confident. We argue that this deficiency is an artifact of the dynamics of training with SGD-like optimizers, and it has some properties similar to overfitting. Based on this observation, we develop an uncertainty estimation algorithm that selectively estimates the uncertainty of highly confident points, using earlier snapshots of the trained model, before their estimates are jittered (and way before they are ready for actual classification). We present extensive experiments indicating that the proposed algorithm provides uncertainty estimates that are consistently better than all known methods.",
    "prev": "RELATED WORK\n\nSelective classification.",
    "curr": "Abstaining from uncertain predictions is a classic idea (Chow, 1957;Hellman, 1970), and uncertainty estimation is an active area of research, from the popular approach of using softmax probabilities (Geifman & El-Yaniv, 2017) to more sophisticated methods using dropout (Gal & Ghahramani, 2016), ensembles (Lakshminarayanan et al., 2017), or training snapshots (Geifman et al., 2018).",
    "next": "Others incorporate abstention into model training (Bartlett & Wegkamp, 2008;Geifman & El-Yaniv, 2019;Feng et al., 2019) and learn to abstain on examples human experts are more likely to get correct (Raghu et al., 2019;Mozannar & Sontag, 2020;De et al., 2020)."
  },
  {
    "index": 626,
    "source_corpus_id": 3292002,
    "ref_id": "b23",
    "citation_corpus_id": 3144218,
    "start": 5643,
    "end": 5664,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "(2016) proposed to approximate the filters by means of a Chebyshev expansion of the graph Laplacian, removing the need to compute the eigenvectors of the Laplacian and yielding spatially localized filters.",
    "curr": "Finally, Kipf & Welling (2017) simplified the previous method by restricting the filters to operate in a 1-step neighborhood around each node.",
    "next": "However, in all of the aforementioned spectral approaches, the learned filters depend on the Laplacian eigenbasis, which depends on the graph structure."
  },
  {
    "index": 627,
    "source_corpus_id": 202888950,
    "ref_id": "b16",
    "citation_corpus_id": 3507990,
    "start": 21092,
    "end": 21110,
    "title": "Published as a conference paper at ICLR 2018 META-LEARNING FOR SEMI-SUPERVISED FEW-SHOT CLASSIFICATION",
    "abstract": "In few-shot classification, we are interested in learning algorithms that train a classifier from only a handful of labeled examples. Recent progress in few-shot classification has featured meta-learning, in which a parameterized model for a learning algorithm is defined and trained on episodes representing different classification problems, each with a small labeled training set and its corresponding test set. In this work, we advance this few-shot classification paradigm towards a scenario where unlabeled examples are also available within each episode. We consider two situations: one where all unlabeled examples are assumed to belong to the same set of classes as the labeled examples of the episode, as well as the more challenging situation where examples from other distractor classes are also provided. To address this paradigm, we propose novel extensions of Prototypical Networks (Snell et al., 2017) that are augmented with the ability to use unlabeled examples when producing prototypes. These models are trained in an end-to-end way on episodes, to learn to leverage the unlabeled examples successfully. We evaluate these methods on versions of the Omniglot and miniImageNet benchmarks, adapted to this new framework augmented with unlabeled examples. We also propose a new split of ImageNet, consisting of a large set of classes, with a hierarchical structure. Our experiments confirm that our Prototypical Networks can learn to improve their predictions due to unlabeled examples, much like a semi-supervised algorithm would. * Equal contribution. 1 See the following blog post for an overview:",
    "prev": "Detailed description of the architecture can be found in the appendix.",
    "curr": "Experiments are performed on three data sets: Omniglot (Lake et al., 2015), miniImageNet (Vinyals et al., 2016), and tieredImageNet (Ren et al., 2018).",
    "next": "We followed standard data processing procedures which are detailed in the appendix."
  },
  {
    "index": 628,
    "source_corpus_id": 91175758,
    "ref_id": "b34",
    "citation_corpus_id": 13751870,
    "start": 23320,
    "end": 23332,
    "title": "A Call for Clarity in Reporting BLEU Scores",
    "abstract": "The field of machine translation faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to \"the\" BLEU score, BLEU is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for usersupplied reference processing, and provide a new tool, SACREBLEU, 1 to facilitate this. . 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. ArXiv eprints, abs/1609.08144.",
    "prev": "If implementation details can cause the scores produced by a metric to vary significantly, this conflates comparison of when different implementations are used to compare different methods.",
    "curr": "This has caused issues in benchmarking machine translation (Post, 2018) and music information retrieval (Raffel et al., 2014).",
    "next": "This problem is particularly pronounced for NNDs because they require implementing a neural network architecture and training scheme, and the use of different software frameworks or even driver versions can cause results to vary (Henderson et al., 2018;Oliver et al., 2018)."
  },
  {
    "index": 629,
    "source_corpus_id": 257405483,
    "ref_id": "b6",
    "citation_corpus_id": 52967399,
    "start": 1699,
    "end": 1720,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "INTRODUCTION\n\nHyperparameter optimization (HPO) and neural architecture search (NAS) yield state-of-the-art models, but often are a very costly endeavor, especially when working with large datasets and models.",
    "curr": "For example, using the results of (Sharir et al., 2020) we can estimate that evaluating 50 configurations for a 340-million-parameter BERT model (Devlin et al., 2019) on the 15GB Wikipedia and Book corpora would cost around $500,000.",
    "next": "To make HPO and NAS more efficient, researchers explored how we can learn from cheaper evaluations (e.g."
  },
  {
    "index": 632,
    "source_corpus_id": 204905143,
    "ref_id": "b16",
    "citation_corpus_id": 3633127,
    "start": 1617,
    "end": 1639,
    "title": "Published as a conference paper at ICLR 2018 CGANS WITH PROJECTION DISCRIMINATOR",
    "abstract": "We propose a novel, projection based way to incorporate the conditional information into the discriminator of GANs that respects the role of the conditional information in the underlining probabilistic model. This approach is in contrast with most frameworks of conditional GANs used in application today, which use the conditional information by concatenating the (embedded) conditional vector to the feature vectors. With this modification, we were able to significantly improve the quality of the class conditional image generation on ILSVRC2012 (Im-ageNet) 1000-class image dataset from the current state-of-the-art result, and we achieved this with a single pair of a discriminator and a generator. We were also able to extend the application to super-resolution and succeeded in producing highly discriminative super-resolution images. This new structure also enabled high quality category transformation based on parametric functional transformation of conditional batch normalization layers in the generator. The code with Chainer (Tokui et al., 2015), generated images and pretrained models are available at https://github.com/pfnet-research/sngan_projection.",
    "prev": "Moreover, Our consistency regularized GAN (CR-GAN) improves stateof-the-art FID scores for conditional generation from 14.73 to 11.67 on CIFAR-10 and from 8.73 to 6.66 on ImageNet-2012.",
    "curr": "INTRODUCTION\n\nGenerative Adversarial Networks (GANs) (Goodfellow et al., 2014) have recently demonstrated impressive results on image-synthesis benchmarks Zhang et al., 2017;Miyato & Koyama, 2018;Zhang et al., 2018;Brock et al., 2018;Karras et al., 2019).",
    "next": "In the original setting, GANs are composed of two neural networks trained with competing goals: the generator is trained to synthesize realistic samples to fool the discriminator and the discriminator is trained to distinguish real samples from fake ones produced by the generator."
  },
  {
    "index": 635,
    "source_corpus_id": 85449634,
    "ref_id": "b41",
    "citation_corpus_id": 5592690,
    "start": 1968,
    "end": 1988,
    "title": "MACHINE COMPREHENSION USING MATCH-LSTM AND ANSWER POINTER",
    "abstract": "Machine comprehension of text is an important problem in natural language processing. A recently released dataset, the Stanford Question Answering Dataset (SQuAD), offers a large number of real questions and their answers created by humans through crowdsourcing. SQuAD provides a challenging testbed for evaluating machine comprehension algorithms, partly because compared with previous datasets, in SQuAD the answers do not come from a small set of candidate answers and they have variable lengths. We propose an end-to-end neural architecture for the task. The architecture is based on match-LSTM, a model we proposed previously for textual entailment, and Pointer Net, a sequence-to-sequence model proposed byVinyals et al. (2015)to constrain the output tokens to be from the input sequences. We propose two ways of using Pointer Net for our task. Our experiments show that both of our two models substantially outperform the best results obtained byRajpurkar et al. (2016)using logistic regression and manually crafted features.",
    "prev": "A lot of effort has been put into designing sophisticated neural MRC architectures for reading short context (e.g.",
    "curr": "a single paragraph), with much success (Wang & Jiang, 2017;Seo et al., 2017;Xiong et al., 2017;Wang et al., 2018c;Yu et al., 2018, inter alia).",
    "next": "However, the performance of such systems degrades significantly when combined with a retriever in open domain settings."
  },
  {
    "index": 636,
    "source_corpus_id": 11324902,
    "ref_id": "b24",
    "citation_corpus_id": 14124313,
    "start": 22935,
    "end": 22962,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "All evaluations are repeated 10 times based on different shuffles of the data, and the mean accuracies and standard deviations are analyzed.",
    "curr": "Office: Since the office dataset is rather small with only 2817 images in its largest domain, we use the latent representations of the convolution neural network VGG16 of Simonyan & Zisserman (2014).",
    "next": "In particular we train a classifier with one hidden layer, 256 hidden nodes and sigmoid activation function on top of the output of the first dense layer in the network."
  },
  {
    "index": 637,
    "source_corpus_id": 57759353,
    "ref_id": "b10",
    "citation_corpus_id": 7034786,
    "start": 9408,
    "end": 9433,
    "title": "NEURAL PROGRAMMER-INTERPRETERS",
    "abstract": "We propose the neural programmer-interpreter (NPI): a recurrent and compositional neural network that learns to represent and execute programs. NPI has three learnable components: a task-agnostic recurrent core, a persistent key-value program memory, and domain-specific encoders that enable a single NPI to operate in multiple perceptually diverse environments with distinct affordances. By learning to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability compared to sequence-tosequence LSTMs. The program memory allows efficient learning of additional tasks by building on existing programs. NPI can also harness the environment (e.g. a scratch pad with read-write pointers) to cache intermediate results of computation, lessening the long-term memory burden on recurrent hidden units. In this work we train the NPI with fully-supervised execution traces; each program has example sequences of calls to the immediate subprograms conditioned on the input. Rather than training on a huge number of relatively weak labels, NPI learns from a small number of rich examples. We demonstrate the capability of our model to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models. Furthermore, a single NPI learns to execute these programs and all 21 associated subprograms.Published as a conference paper at ICLR 2016 INPUT GOTO KEY ARG END h M key M prog GOTO() HGOTO() LGOTO() VGOTO() LGOTO() ACT(LEFT) DGOTO() ACT(DOWN)",
    "prev": "(2017) Learning to execute programs.",
    "curr": "Neural Program Interpreters (NPI) have been extensively studied for programs that abstract and execute tasks such as sorting, shape manipulation, and grade-school arithmetic (Reed & De Freitas, 2016;Cai et al., 2017;Bošnjak et al., 2017).",
    "next": "In NPI (Reed & De Freitas, 2016), the key insight is that a program execution trace can be decomposed into predefined operations that are more primitive; and at each step, an NPI learns to predict what operation to take next depending on the general environment, domain specific state , and previous actions."
  },
  {
    "index": 639,
    "source_corpus_id": 235368204,
    "ref_id": "b16",
    "citation_corpus_id": 4722462,
    "start": 2279,
    "end": 2296,
    "title": "EMERGENT TRANSLATION IN MULTI-AGENT COMMUNICATION",
    "abstract": "While most machine translation systems to date are trained on large parallel corpora, humans learn language in a different way: by being grounded in an environment and interacting with other humans. In this work, we propose a communication game where two agents, native speakers of their own respective languages, jointly learn to solve a visual referential task. We find that the ability to understand and translate a foreign language emerges as a means to achieve shared goals. The emergent translation is interactive and multimodal, and crucially does not require parallel corpora, but only monolingual, independent text and corresponding images. Our proposed translation model achieves this by grounding the source and target languages into a shared visual modality, and outperforms several baselines on both word-level and sentence-level translation tasks. Furthermore, we show that agents in a multilingual community learn to translate better and faster than in a bilingual communication setting.",
    "prev": "The success of deep learning (DL) models on complicated cognitive tasks (Krizhevsky et al., 2012;LeCun et al., 2015;Silver et al., 2016) then inspired researchers to apply DL-based models to language games to investigate the agents' ability to invent communication protocols without preset rules (e.g.",
    "curr": "Lee et al., 2018;Lazaridou et al., 2018).",
    "next": "In the existing works (e.g."
  },
  {
    "index": 640,
    "source_corpus_id": 236318292,
    "ref_id": "b25",
    "citation_corpus_id": 108300573,
    "start": 3303,
    "end": 3306,
    "title": "INTERPOLATION-PREDICTION NETWORKS FOR IRREGULARLY SAMPLED TIME SERIES",
    "abstract": "In this paper, we present a new deep learning architecture for addressing the problem of supervised learning with sparse and irregularly sampled multivariate time series. The architecture is based on the use of a semi-parametric interpolation network followed by the application of a prediction network. The interpolation network allows for information to be shared across multiple dimensions of a multivariate time series during the interpolation stage, while any standard deep learning model can be used for the prediction network. This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate. We investigate the performance of this architecture on both classification and regression tasks, showing that our approach outperforms a range of baseline and recently proposed models. 1 1 Our implementation is available at : https://github.com/mlds-lab/interp-net 1 arXiv:1909.07782v1 [cs.LG] 13 Sep 2019Published as a conference paper at ICLR 2019 trends. Similar to the work ofLipton et al. (2016)andChe et al. (2018a), our architecture also explicitly leverages a separate information channel related to patterns of observation times. However, our representation uses a semi-parametric intensity function representation of this information that is more closely related to the work of Lasko (2014) on modeling medical event point processes.Our architecture thus produces three output time series for each input time series: a smooth interpolation modeling broad trends in the input, a short time-scale interpolation modeling transients, and an intensity function modeling local observation frequencies.This work is motivated by problems in the analysis of electronic health records (EHRs)(Marlin et al., 2012;Lipton et al., 2016;Futoma et al., 2017;Che et al., 2018a). It remains rare for hospital systems to capture dense physiological data streams. Instead, it is common for the physiological time series data in electronic health records to be both sparse and irregularly sampled. The additional issue of the lack of alignment in the observation times across physiological variables is also very common.We evaluate the proposed architecture on two datasets for both classification and regression tasks. Our approach outperforms a variety of simple baseline models as well as the basic and advanced GRU models introduced by Che et al. (2018a) across several metrics. We also compare our model with to the Gaussian process adapter (Li & Marlin, 2016) and multi-task Gaussian process RNN classifier(Futoma et al., 2017). Further, we perform full ablation testing of the information channels our architecture can produce to assess their impact on classification and regression performance.",
    "prev": "The main challenges in dealing with such data include the presence of variable time gaps between the observation time points, partially observed feature vectors caused by the lack of temporal alignment across different dimensions, as well as different data cases, and variable numbers of observations across dimensions and data cases.",
    "curr": "Significant recent work has focused on developing specialized models and architectures to address these challenges in modeling irregularly sampled multivariate time series [17,18,19,9,3,26,24,12,16,27,7,30,14].",
    "next": "Recently, Shukla and Marlin [27] introduced the Multi-Time Attention Network (mTAN) model, a variational autoencoder (VAE) architecture for continuous-time interpolation of irregularly sampled time series."
  },
  {
    "index": 642,
    "source_corpus_id": 52903499,
    "ref_id": "b29",
    "citation_corpus_id": 4429876,
    "start": 4428,
    "end": 4432,
    "title": "Global optimality conditions for deep neural networks",
    "abstract": "We study the error landscape of deep linear and nonlinear neural networks with square error loss. We build on the recent results in the literature and present necessary and sufficient conditions for a critical point of the empirical risk function to be a global minimum in the deep linear network case. Our simple conditions can also be used to determine whether a given critical point is a global minimum or a saddle point. We further extend these results to deep nonlinear neural networks and prove similar sufficient conditions for global optimality in the function space.",
    "prev": "Bridging this gap between theory and practice is a very active area of research.",
    "curr": "There have been many attempts to understand why optimization works well for neural networks, by studying the loss surface [1,10,12,17,18,21,23,24,[26][27][28][29][30] and the role of (stochastic) gradient-based methods [4,8,25].",
    "next": "Besides nonconvexity, for ReLU networks significant additional challenges in the analysis arise due to nondifferentiability, and obtaining a precise understanding of the nondifferentiable points is still elusive."
  },
  {
    "index": 644,
    "source_corpus_id": 53215593,
    "ref_id": "b3",
    "citation_corpus_id": 11212020,
    "start": 12131,
    "end": 12154,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "(1) Our key intuition is that the inverse dynamics model should attend to the most relevant part of the observation, which is controllable by the agent, to be able to classify the actions.",
    "curr": "We determine whether each region in a H × W grid is controllable, or in other words, useful for predicting the agent's action, by using a spatial attention mechanism (Bahdanau et al., 2015;Xu et al., 2015).",
    "next": "An overview of the model is shown in Figure 1."
  },
  {
    "index": 645,
    "source_corpus_id": 220347587,
    "ref_id": "b36",
    "citation_corpus_id": 196178484,
    "start": 5174,
    "end": 5178,
    "title": "Self-Attentive, Multi-Context One-Class Classification for Unsupervised Anomaly Detection on Text",
    "abstract": "There exist few text-specific methods for unsupervised anomaly detection, and for those that do exist, none utilize pre-trained models for distributed vector representations of words. In this paper we introduce a new anomaly detection method-Context Vector Data Description (CVDD)-which builds upon word embedding models to learn multiple sentence representations that capture multiple semantic contexts via the self-attention mechanism. Modeling multiple contexts enables us to perform contextual anomaly detection of sentences and phrases with respect to the multiple themes and concepts present in an unlabeled text corpus. These contexts in combination with the self-attention weights make our method highly interpretable. We demonstrate the effectiveness of CVDD quantitatively as well as qualitatively on the wellknown Reuters, 20 Newsgroups, and IMDB Movie Reviews datasets.",
    "prev": "These methods attempt to separate nominal samples from anomalies in an unsupervised manner by concentrating nominal data in feature space while mapping anomalies to distant locations [36,6].",
    "curr": "In the domain of NLP, DSVDD [36] has been successfully applied to text, which yields a form of interpretation using attention mechanisms [37].",
    "next": "For images, Kauffmann et al."
  },
  {
    "index": 647,
    "source_corpus_id": 264128269,
    "ref_id": "b19",
    "citation_corpus_id": 21850704,
    "start": 2428,
    "end": 2449,
    "title": "A Deep Reinforced Model for Abstractive Summarization",
    "abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. However, for longer documents and summaries, these models often include repetitive and incoherent phrases. We introduce a neural network model with intra-attention and a new training method. This method combines standard supervised word prediction and reinforcement learning (RL). Models trained only with the former often exhibit \"exposure bias\" -they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, a 5.7 absolute points improvement over previous state-of-the-art models. It also performs well as the first abstractive model on the New York Times corpus. Human evaluation also shows that our model produces higher quality summaries.",
    "prev": "xplanation for why Goodhart's law occurs in Markov decision processes.We use these theoretical insights to propose an optimal early stopping method that provably avoids the aforementioned pitfall and derive theoretical regret bounds for this method.Moreover, we derive a training method that maximises worst-case reward, for the setting where there is uncertainty about the true reward function.Finally, we evaluate our early stopping method experimentally.Our results support a foundation for a theoretically-principled study of reinforcement learning under reward misspecification.",
    "curr": "INTRODUCTION\n\nTo solve a problem using Reinforcement Learning (RL), it is necessary first to formalise that problem using a reward function (Sutton & Barto, 2018).However, due to the complexity of many real-world tasks, it is exceedingly difficult to directly specify a reward function that fully captures the task in the intended way.However, misspecified reward functions will often lead to undesirable behaviour (Paulus et al., 2018;Ibarz et al., 2018;Knox et al., 2023;Pan et al., 2021).This makes designing good reward functions a major obstacle to using RL in practice, especially for safety-critical applications.",
    "next": "An increasingly popular solution is to learn reward functions from mechanisms such as human or automated feedback (e.g.Christiano et al., 2017;Ng & Russell, 2000).However, this approach comes with its own set of challenges: the right data can be difficult to collect (e.g.Paulus et al., 2018), and it is often challenging to interpret it correctly (e.g.Mindermann & Armstrong, 2018;Skalse & Abate, 2023).Moreover, optimising a policy against a learned reward model effectively constitutes a distributional shift (Gao et al., 2023); i.e., even if a reward function is accurate under the training distribution, it may fail to induce desirable behaviour from the RL agent."
  },
  {
    "index": 649,
    "source_corpus_id": 247476256,
    "ref_id": "b33",
    "citation_corpus_id": 15197911,
    "start": 3725,
    "end": 3741,
    "title": "PUSHING THE BOUNDARIES OF BOUNDARY DETEC- TION USING DEEP LEARNING",
    "abstract": "In this work we show that adapting Deep Convolutional Neural Network training to the task of boundary detection can result in substantial improvements over the current state-of-the-art in boundary detection. Our contributions consist firstly in combining a careful design of the loss for boundary detection training, a multi-resolution architecture and training with external data to improve the detection accuracy of the current state of the art. When measured on the standard Berkeley Segmentation Dataset, we improve theoptimal dataset scale F-measure from 0.780 to 0.808 -while human performance is at 0.803. We further improve performance to 0.813 by combining deep learning with grouping, integrating the Normalized Cuts technique within a deep network. We also examine the potential of our boundary detector in conjunction with the task of semantic segmentation and demonstrate clear improvements over state-ofthe-art systems. Our detector is fully integrated in the popular Caffe framework and processes a 320x420 image in less than a second.",
    "prev": "Such a strategy has been successful in other dense prediction tasks (Ronneberger et al., 2015) as well.",
    "curr": "Others propose different loss functions (Kokkinos, 2016;Kervadec et al., 2019) to address class imbalance.",
    "next": "Despite the improvements, we identify two issues regarding crisp boundary detection."
  },
  {
    "index": 650,
    "source_corpus_id": 220055921,
    "ref_id": "b36",
    "citation_corpus_id": 7034786,
    "start": 9963,
    "end": 9967,
    "title": "NEURAL PROGRAMMER-INTERPRETERS",
    "abstract": "We propose the neural programmer-interpreter (NPI): a recurrent and compositional neural network that learns to represent and execute programs. NPI has three learnable components: a task-agnostic recurrent core, a persistent key-value program memory, and domain-specific encoders that enable a single NPI to operate in multiple perceptually diverse environments with distinct affordances. By learning to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability compared to sequence-tosequence LSTMs. The program memory allows efficient learning of additional tasks by building on existing programs. NPI can also harness the environment (e.g. a scratch pad with read-write pointers) to cache intermediate results of computation, lessening the long-term memory burden on recurrent hidden units. In this work we train the NPI with fully-supervised execution traces; each program has example sequences of calls to the immediate subprograms conditioned on the input. Rather than training on a huge number of relatively weak labels, NPI learns from a small number of rich examples. We demonstrate the capability of our model to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models. Furthermore, a single NPI learns to execute these programs and all 21 associated subprograms.Published as a conference paper at ICLR 2016 INPUT GOTO KEY ARG END h M key M prog GOTO() HGOTO() LGOTO() VGOTO() LGOTO() ACT(LEFT) DGOTO() ACT(DOWN)",
    "prev": "Our proposed framework bears some resemblance to SPIRAL [7]; however, SPIRAL does not provide a density function, and it has no hierarchical structure, limiting its applications to image reconstruction and unconditional generation.",
    "curr": "Neuro-symbolic modeling has become an active area of research, with applications to learning inputoutput programs [37,16,3,34], question answering [44,29] and image description [4].",
    "next": "GNS modeling distinguishes from prior work through its focus on hybrid generative modeling, combining both structured program execution and neural networks directly in the probabilistic generative process."
  },
  {
    "index": 652,
    "source_corpus_id": 202712898,
    "ref_id": "b2",
    "citation_corpus_id": 14124313,
    "start": 1541,
    "end": 1568,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "Nonetheless, these architectures may not necessarily lead to better generalization performance compared with other candidate architectures in the same search space, and therefore further improvement is possible by revising existing NAS algorithms.",
    "curr": "INTRODUCTION\n\nVarious neural network architectures (Krizhevsky et al., 2012;Simonyan & Zisserman, 2015;He et al., 2016;Huang et al., 2017) have been devised over the past decades, achieving superhuman performance for a wide range of tasks.",
    "next": "Designing these neural networks typically takes substantial efforts from domain experts by trial and error."
  },
  {
    "index": 654,
    "source_corpus_id": 13807351,
    "ref_id": "b45",
    "citation_corpus_id": 252796,
    "start": 5644,
    "end": 5665,
    "title": "Building a Large Annotated Corpus of English: The Penn Treebank",
    "abstract": "There is a growing consensus that significant, rapid progress can be made in both text understanding and spoken language understanding by investigating those phenomena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large corpora. Such corpora are beginning to serve as important research tools for investigators in natural language processing, speech recognition, and integrated spoken language systems, as well as in theoretical linguistics. Annotated corpora promise to be valuable for enterprises as diverse as the automatic construction of statistical models for the grammar of the written and the colloquial spoken language, the development of explicit formal theories of the differing grammars of writing and speech, the investigation of prosodic phenomena in speech, and the evaluation and comparison of the adequacy of parsing models.In this paper, we review our experience with constructing one such large annotated corpus--the Penn Treebank, a corpus 1 consisting of over 4.5 million words of American English. During the first three-year phase of the Penn Treebank Project (1989)(1990)(1991)(1992), this corpus has been annotated for part-of-speech (POS) information. In addition, over half of it has been annotated for skeletal syntactic structure. These materials are available to members of the Linguistic Data Consortium; for details, see Section 5.1.The paper is organized as follows. Section 2 discusses the POS tagging task. After outlining the considerations that informed the design of our POS tagset and presenting the tagset itself, we describe our two-stage tagging process, in which text is first assigned POS tags automatically and then corrected by human annotators. Section 3 briefly presents the results of a comparison between entirely manual and semi-automated tagging, with the latter being shown to be superior on three counts: speed, consistency, and accuracy. In Section 4, we turn to the bracketing task. Just as with the tagging task, we have partially automated the bracketing task: the output of",
    "prev": "Actively biasing the optimization towards wide valleys in the energy landscape results in better generalization error.",
    "curr": "We present experimental results on fully-connected and convolutional neural networks (CNNs) on the MNIST and CIFAR-10 (Krizhevsky, 2009) datasets and recurrent neural networks (RNNs) on the Penn Tree Bank dataset (PTB) (Marcus et al., 1993) and character-level text prediction.",
    "next": "Our experiments show that Entropy-SGD scales to deep networks used in practice, obtains comparable generalization error as competitive baselines and also trains much more quickly than SGD (we get a 2x speed-up over SGD on RNNs)."
  },
  {
    "index": 655,
    "source_corpus_id": 3470596,
    "ref_id": "b39",
    "citation_corpus_id": 11445252,
    "start": 29665,
    "end": 29689,
    "title": "LEARNING FEATURES OF MUSIC FROM SCRATCH",
    "abstract": "We introduce a new large-scale music dataset, MusicNet, to serve as a source of supervision and evaluation of machine learning methods for music research. Mu-sicNet consists of hundreds of freely-licensed classical music recordings by 10 composers, written for 11 instruments, together with instrument/note annotations resulting in over 1 million temporal labels on 34 hours of chamber music performances under various studio and microphone conditions. We define a multi-label classification task to predict notes in musical recordings, along with an evaluation protocol. We benchmark several machine learning architectures for this task: i) learning from \"hand-crafted\" spectrogram features; ii) end-to-end learning with a neural net; iii) end-to-end learning with a convolutional neural net. We show that several end-to-end learning proposals outperform approaches based on learning from hand-crafted audio features.",
    "prev": "The nature of an audio signal allows one to exploit complex operations as presented earlier in the paper.",
    "curr": "The experiments were performed on the MusicNet dataset [Thickstun et al., 2016].",
    "next": "For computational efficiency we resampled the original input from 44.1kHz to 11kHz using the algorithm described in Smith [2002]."
  },
  {
    "index": 656,
    "source_corpus_id": 222066778,
    "ref_id": "b2",
    "citation_corpus_id": 3708505,
    "start": 5522,
    "end": 5539,
    "title": "DEEP NEURAL NETWORKS AS GAUSSIAN PROCESSES",
    "abstract": "A deep fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP) in the limit of infinite network width. This correspondence enables exact Bayesian inference for neural networks on regression tasks by means of straightforward matrix computations. For single hiddenlayer networks, the covariance function of this GP has long been known. Recently, kernel functions for multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified the correspondence between using these kernels as the covariance function for a GP and performing fully Bayesian prediction with a deep neural network. In this work, we derive this correspondence and develop a computationally efficient pipeline to compute the covariance functions. We then use the resulting GP to perform Bayesian inference for deep neural networks on MNIST and CIFAR-10. We find that the GP-based predictions are competitive and can outperform neural networks trained with stochastic gradient descent. We observe that the trained neural network accuracy approaches that of the corresponding GP-based computation with increasing layer width, and that the GP uncertainty is strongly correlated with prediction error. We connect our observations to the recent development of signal propagation in random neural networks. * Both authors contributed equally to this work. † Work done as a member of the Google Brain Residency program (g.co/brainresidency). 1 Throughout this paper, we assume the conditions on the parameter distributions and nonlinearities are such that the Central Limit Theorem will hold; for instance, that the weight variance is scaled inversely proportional to the layer width.",
    "prev": "Related work.",
    "curr": "Kernels for deep learning were originally derived by Neal (1996) for shallow networks, and later for deep networks (Cho & Saul, 2009;Daniely et al., 2016;Lee et al., 2018;Matthews et al., 2018).",
    "next": "Smola et al."
  },
  {
    "index": 657,
    "source_corpus_id": 250243645,
    "ref_id": "b46",
    "citation_corpus_id": 6628106,
    "start": 21932,
    "end": 21934,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "Starting with Figure 1 We then plot the MI accuracy and precision as a function of time, both before injection and also for ten epochs after.",
    "curr": "We observe that MI is impossible until the injection step, after which precision remains very 46 47 48 49 50 51 52 53 54 55 56 57 58 59  high for many epochs, taking 10 epochs to decay to roughly 65%, which is permitted by DP 2 with ε ≈ 0.6.",
    "next": "This demonstrates that forgetting does occur for this setting."
  },
  {
    "index": 659,
    "source_corpus_id": 249848252,
    "ref_id": "b11",
    "citation_corpus_id": 196831582,
    "start": 2748,
    "end": 2751,
    "title": "Published as a conference paper at ICLR 2020 ON THE \"STEERABILITY\" OF GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "An open secret in contemporary machine learning is that many models work beautifully on standard benchmarks but fail to generalize outside the lab. This has been attributed to biased training data, which provide poor coverage over real world events. Generative models are no exception, but recent advances in generative adversarial networks (GANs) suggest otherwise -these models can now synthesize strikingly realistic and diverse images. Is generative modeling of photos a solved problem? We show that although current GANs can fit standard datasets very well, they still fall short of being comprehensive models of the visual manifold. In particular, we study their ability to fit simple transformations such as camera movements and color changes. We find that the models reflect the biases of the datasets on which they are trained (e.g., centered objects), but that they also exhibit some capacity for generalization: by \"steering\" in latent space, we can shift the distribution while still creating realistic images. We hypothesize that the degree of distributional shift is related to the breadth of the training data distribution. Thus, we conduct experiments to quantify the limits of GAN transformations and introduce techniques to mitigate the problem. Code is released on our project page:",
    "prev": "Under review.",
    "curr": "Introduction\n\nGenerative Adversarial Networks (GANs) [8] have achieved significant advancement over the past several years, enabling many computer vision tasks such as image manipulation [17,12,28,9,2], domain translation [11,34,6,7,20,18], and image or video generation [19,21,15,16,13,14,29,32,30,31].",
    "next": "In addition, the resolution and quality of images synthesized by generative models have seen rapid improvement recently in terms of quantitative metrics [3,10,27]."
  },
  {
    "index": 660,
    "source_corpus_id": 222125236,
    "ref_id": "b21",
    "citation_corpus_id": 207870430,
    "start": 1696,
    "end": 1721,
    "title": "Published as a conference paper at ICLR 2020 GENERALIZATION THROUGH MEMORIZATION: NEAREST NEIGHBOR LANGUAGE MODELS",
    "abstract": "We introduce kNN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a k-nearest neighbors (kNN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong WIKITEXT-103 LM, with neighbors drawn from the original training set, our kNN-LM achieves a new stateof-the-art perplexity of 15.79 -a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail. . Pointer sentinel mixture models. ICLR, 2017. Tomáš Mikolov, Martin Karafiát, Lukáš Burget, JanČernockỳ, and Sanjeev Khudanpur. Recurrent neural network based language model. In Eleventh annual conference of the international speech communication association, 2010. A. Emin Orhan. A simple cache model for image recognition. In NeurIPS, 2018. Nicolas Papernot and Patrick McDaniel. Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning. arXiv preprint arXiv:1803.04765, 2018.Ofir Press and Lior Wolf. Using the output embedding to improve language models. In ICLR, 2017. machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015. Catanzaro. Megatron-lm: Training multi-billion parameter language models using gpu model parallelism. arXiv preprint arXiv:1909.08053, 2019.",
    "prev": "rity search.This approach requires no additional training and scales to give the decoder direct access to billions of examples at test time, resulting in a highly expressive model that consistently improves performance across many settings.Simply adding nearest neighbor search improves a state-of-the-art German-English translation model by 1.5 BLEU.kNN-MT allows a single model to be adapted to diverse domains by using a domain-specific datastore, improving results by an average of 9.2 BLEU over zero-shot transfer, and achieving new state-of-the-art results-without training on these domains.A massively multilingual model can also be specialized for particular language pairs, with improvements of 3 BLEU for translating from English into German and Chinese.Qualitatively, kNN-MT is easily interpretable; it combines source and target context to retrieve highly relevant examples.",
    "curr": "INTRODUCTION\n\nNon-parametric methods have recently been successfully applied to tasks such as language modeling (Khandelwal et al., 2020) and question answering (Guu et al., 2020;Lewis et al., 2020).They allow models that are (1) expressive, because they can use an arbitrary amount of data at test time; (2) adaptable, because predictions can be controlled by changing the datastore, and (3) interpretable, because the data used to make the prediction can be directly inspected.We introduce kNN-MT, a simple non-parametric method for machine translation (MT) using nearest neighbor retrieval.kNN-MT can be added to any pre-trained neural translation model without further training, and significantly improves performance for in-domain, out-of-domain, and multi-lingual evaluations.",
    "next": "More specifically, kNN-MT interpolates the target-token softmax distribution from a neural MT model with a multinomial generated using nearest neighbor search over examples cached in a data store.The cache is over translation contexts (i.e."
  },
  {
    "index": 661,
    "source_corpus_id": 85543148,
    "ref_id": "b37",
    "citation_corpus_id": 14124313,
    "start": 7304,
    "end": 7332,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "It is challenging due to the complexity of features in high-dimensional space (Krizhevsky et al., 2012), the large intra-class variation and inter-class similarity across categories in benchmarks (Deng et al., 2009;Tsung-Yi Lin, 2015).",
    "curr": "Thanks to the development of deep networks structure (Simonyan & Zisserman, 2015;He et al., 2016) and modern GPU hardware acceleration, this community has witnessed a great bloom in both performance and efficiency.",
    "next": "The detection of small objects is addressed in concurrent literature mainly through two manners."
  },
  {
    "index": 663,
    "source_corpus_id": 251953412,
    "ref_id": "b39",
    "citation_corpus_id": 201646309,
    "start": 3249,
    "end": 3275,
    "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    "abstract": "BERT(Devlin et al., 2018)and RoBERTa (Liu  et al., 2019)  has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods. 1",
    "prev": "As there are millions to billions of documents in a collection, efficiency is the most fundamental prerequisite for large-scale retrieval.",
    "curr": "To this end, query-agnostic document representations (i.e., indexing the collection independently) and lightweight relevance metrics (e.g., cosine similarity, dot-product) have become the common practices to meet the prerequisite -usually achieved by a two-tower structure (Reimers & Gurevych, 2019), a.k.a., bi-encoder and dual-encoder, in representation learning literature.",
    "next": "Besides the prevalent 'dense-vector retrieval' paradigm that encodes both queries and documents in the same low-dimension, real-valued latent semantic space , another retrieval paradigm, 'lexicon-weighting retrieval', aims to leverage weighted sparse representation in vocabulary space (Formal et al., 2021a;."
  },
  {
    "index": 666,
    "source_corpus_id": 3473900,
    "ref_id": "b16",
    "citation_corpus_id": 5959482,
    "start": 1479,
    "end": 1501,
    "title": "Efficient Estimation of Word Representations in Vector Space",
    "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities. arXiv:1301.3781v3 [cs.CL] 7 Sep 2013 1 The test set is available at www.fit.vutbr.cz/˜imikolov/rnnlm/word-test.v1.txt 2",
    "prev": "INTRODUCTION\n\nLearning feature representations of natural data such as text and images has become increasingly important for understanding real-world concepts.",
    "curr": "These representations are useful for many tasks, ranging from semantic understanding of words and sentences (Mikolov et al., 2013;Kiros et al., 2015), image caption generation (Vinyals et al., 2015), textual entailment prediction (Rocktäschel et al., 2015), to language communication with robots (Bisk et al., 2016).",
    "next": "Meaningful representations of text and images capture visual-semantic information, such as hierarchical structure where certain entities are abstractions of others."
  },
  {
    "index": 668,
    "source_corpus_id": 225103201,
    "ref_id": "b8",
    "citation_corpus_id": 24029589,
    "start": 9634,
    "end": 9654,
    "title": "The Relationship Between High-Dimensional Geometry and Adversarial Examples",
    "abstract": "Machine learning models with very low test error have been shown to be consistently vulnerable to small, adversarially chosen perturbations of the input. We hypothesize that this counterintuitive behavior is a result of the high-dimensional geometry of the data manifold, and explore this hypothesis on a simple highdimensional dataset. For this dataset we show a fundamental bound relating the classification error rate to the average distance to the nearest misclassification, which is independent of the model. We train different neural network architectures on this dataset and show their error sets approach this theoretical bound. As a result of the theory, the vulnerability of machine learning models to small adversarial perturbations is a logical consequence of the amount of test error observed. We hope that our theoretical analysis of this foundational synthetic case will point a way forward to explore how the geometry of complex real-world data sets leads to adversarial examples.",
    "prev": "Finally, we also go beyond the max-margin settings in these works to analyze the dynamics of a finite-time gradient-descent trained classifier.",
    "curr": "We also refer the reader to an orthogonal line of theory work (Fawzi et al., 2018;Gilmer et al., 2018) that has investigated the failure of standard classifiers in the adversarial perturbation setting.",
    "next": "Algorithms for OoD generalization."
  },
  {
    "index": 670,
    "source_corpus_id": 212874725,
    "ref_id": "b3",
    "citation_corpus_id": 22163777,
    "start": 11270,
    "end": 11273,
    "title": "Boosting the Actor with Dual Critic",
    "abstract": "This paper proposes a new actor-critic-style algorithm called Dual Actor-Criticor Dual-AC. It is derived in a principled way from the Lagrangian dual form of the Bellman optimality equation, which can be viewed as a two-player game between the actor and a critic-like function, which is named as dual critic. Compared to its actor-critic relatives, Dual-AC has the desired property that the actor and dual critic are updated cooperatively to optimize the same objective function, providing a more transparent way for learning the critic that is directly related to the objective function of the actor. We then provide a concrete algorithm that can effectively solve the minimax optimization problem, using techniques of multi-step bootstrapping, path regularization, and stochastic dual ascent algorithm. We demonstrate that the proposed algorithm achieves the state-of-the-art performances across several benchmarks. * The first two authors equally contributed.",
    "prev": "Therefore, B π describes how visitation flows from (ξ, α) to (s, a) and hence, we call it the backward flow operator.",
    "curr": "Note that similar forms of B π have appeared in the literature, usually used to encode constraints in a dual linear program for an MDP [4,37,38].",
    "next": "However, the application of B π for the off-policy estimation problem as considered here appears new to the best of our knowledge."
  },
  {
    "index": 674,
    "source_corpus_id": 263620583,
    "ref_id": "b38",
    "citation_corpus_id": 57825721,
    "start": 29466,
    "end": 29486,
    "title": "ON THE TURING COMPLETENESS OF MODERN NEURAL NETWORK ARCHITECTURES",
    "abstract": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.",
    "prev": "(2022) for a survey).Since it is infeasible to precisely define real-world data and tasks, several works have studied it in stylized well-defined settings (Chan et al., 2022;Hahn & Goyal, 2023;Xie et al., 2021).Garg et al.",
    "curr": "(2022) presented a meta-learning-like Transformers and Sequence Models.The analysis of the capabilities and limitations of recurrent architectures dates back to a few decades ago (Kolen & Kremer, 2001).Given the recent success of Transformers, several works have sought to investigate their theoretical expressiveness (Pérez et al., 2019;Merrill et al., 2022;Chiang & Cholak, 2022;Hahn, 2020;Yun et al., 2020;Liu et al., 2022) as well as their empirical capabilities (Bhattamishra et al., 2023;2020b;Ebrahimi et al., 2020) and limitations (Bhattamishra et al., 2020a;Chiang & Cholak, 2022).Delétang et al.",
    "next": "( 2022) conduct a comprehensive study of the performance of various sequence models such as Transformers and RNNs on formal language tasks.While most of these prior works focus on classification or related tasks, our work complements these as we conduct a comprehensive study on in-context learning tasks."
  },
  {
    "index": 675,
    "source_corpus_id": 53483414,
    "ref_id": "b13",
    "citation_corpus_id": 9059612,
    "start": 2706,
    "end": 2728,
    "title": "ADVERSARIAL MACHINE LEARNING AT SCALE",
    "abstract": "Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet (Russakovsky  et al., 2014). Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than singlestep attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a \"label leaking\" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.",
    "prev": "Various defensive methods have been proposed to mitigate the effect of the adversarial examples.",
    "curr": "Adversarial training which augments the training set with adversarial examples shows good defensive performance in terms of white box attacks (Kurakin et al., 2017;.",
    "next": "Apart from adversarial training, there are many other defensive approaches including defensive distillation (Papernot et al., 2016b), using randomization at inference time (Xie et al., 2018), and thermometer encoding (Buckman et al., 2018), etc."
  },
  {
    "index": 679,
    "source_corpus_id": 220265948,
    "ref_id": "b33",
    "citation_corpus_id": 3524184,
    "start": 20908,
    "end": 20912,
    "title": "INFERENCE SUBOPTIMALITY IN VARIATIONAL AUTOENCODERS",
    "abstract": "Amortized inference has led to efficient approximate inference for large datasets. The quality of posterior inference is largely determined by two factors: a) the ability of the variational distribution to model the true posterior and b) the capacity of the recognition network to generalize inference over all datapoints. We analyze approximate inference in variational autoencoders in terms of these factors. We find that suboptimal inference is often due to amortizing inference rather than the limited complexity of the approximating distribution. We show that this is due partly to the generator learning to accommodate the choice of approximation. Furthermore, we show that the parameters used to increase the expressiveness of the approximation play a role in generalizing inference rather than simply improving the complexity of the approximation.",
    "prev": "Figure 3  We evaluate the accuracy of mean reconstruction at different ratios of observed entries by measuring the peak signal to noise ratio (PSNR) between the ground truth data and mean recoveries ( Figure  3(b)).",
    "curr": "To evaluate probabilistic performance we approximately measure the likelihood assigned by the recovered posteriors to the ground truth data through a reconstruction ELBO, by training a new inference function with the clean ground truths, but leaving the posterior fixed, as is common for evaluating ELBOs in unsupervised settings [34,35,11].",
    "next": "Results are shown in figure 3(c)."
  },
  {
    "index": 680,
    "source_corpus_id": 51942590,
    "ref_id": "b14",
    "citation_corpus_id": 1428702,
    "start": 10417,
    "end": 10421,
    "title": "Learning Word Vectors for Sentiment Analysis",
    "abstract": "Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term-document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.",
    "prev": "It just sucks.",
    "curr": "( ) This sentence is contained in a movie review from the IMDB movie data set [15], and it is classified as negative sentiment by a machine learning model to be discussed in the sequel.",
    "next": "Now suppose we wish to quantify the importance of feature \"not\" in prediction."
  },
  {
    "index": 681,
    "source_corpus_id": 212633677,
    "ref_id": "b27",
    "citation_corpus_id": 5959482,
    "start": 10709,
    "end": 10713,
    "title": "Efficient Estimation of Word Representations in Vector Space",
    "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities. arXiv:1301.3781v3 [cs.CL] 7 Sep 2013 1 The test set is available at www.fit.vutbr.cz/˜imikolov/rnnlm/word-test.v1.txt 2",
    "prev": ", x n ) in one go.",
    "curr": "An embedding is a mapping from plain input, for example words or characters, to a high dimensional vector, for which learning algorithms and toolkits exists, e.g., word2vec [26].",
    "next": "Given the encoders output z = (z 0 , ."
  },
  {
    "index": 683,
    "source_corpus_id": 249375359,
    "ref_id": "b1",
    "citation_corpus_id": 52922363,
    "start": 2697,
    "end": 2717,
    "title": "A CONVERGENCE ANALYSIS OF GRADIENT DESCENT FOR DEEP LINEAR NEURAL NETWORKS",
    "abstract": "We analyze speed of convergence to global optimum for gradient descent training a deep linear neural network (parameterized as x → W N W N −1 · · · W 1 x) by minimizing the 2 loss over whitened data. Convergence at a linear rate is guaranteed when the following hold: (i) dimensions of hidden layers are at least the minimum of the input and output dimensions; (ii) weight matrices at initialization are approximately balanced; and (iii) the initial loss is smaller than the loss of any rank-deficient solution. The assumptions on initialization (conditions (ii) and (iii)) are necessary, in the sense that violating any one of them may lead to convergence failure. Moreover, in the important case of output dimension 1, i.e. scalar regression, they are met, and thus convergence to global optimum holds, with constant probability under a random initialization scheme. Our results significantly extend previous analyses, e.g., of deep linear residual networks(Bartlett et al., 2018).",
    "prev": "From the theoretical perspective, understanding the roles of nonlinearity in deep neural networks is one critical part of understanding how modern deep models work.",
    "curr": "Currently, most works focus on linear variants of deep models (Jacot et al., 2018;Arora et al., 2019a;Kawaguchi, 2016;Jing et al., 2022;Tian et al., 2021;Wang et al., 2021).",
    "next": "When nonlinearity is involved, deep models are often treated as richer families of black-box functions than linear ones (Arora et al., 2019b;HaoChen et al., 2021)."
  },
  {
    "index": 684,
    "source_corpus_id": 257365130,
    "ref_id": "b13",
    "citation_corpus_id": 102350747,
    "start": 5336,
    "end": 5357,
    "title": "Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Autoencoders",
    "abstract": "We introduce deep inside-outside recursive autoencoders (DIORA), a fully-unsupervised method for discovering syntax that simultaneously learns representations for constituents within the induced tree. Our approach predicts each word in an input sentence conditioned on the rest of the sentence and uses inside-outside dynamic programming to consider all possible binary trees over the sentence. At test time the CKY algorithm extracts the highest scoring parse. DIORA achieves a new state-of-the-art F1 in unsupervised binary constituency parsing (unlabeled) in two benchmark datasets, WSJ and MultiNLI.",
    "prev": "PRELIMINARY\n\n\nESSENTIAL PROPERTIES OF STRUCTURED LANGUAGE MODELS\n\nStructured language models feature combining the powerful representation of neural networks with syntax structures.",
    "curr": "Though many attempts have been made about structured language models (Kim et al., 2019;Drozdov et al., 2019;Shen et al., 2021), three prerequisites need to be met before a model is selected as the backbone of our method.",
    "next": "Firstly, it should have the ability to learn reasonable syntax structure in an unsupervised manner."
  },
  {
    "index": 685,
    "source_corpus_id": 244117004,
    "ref_id": "b71",
    "citation_corpus_id": 52895589,
    "start": 17109,
    "end": 17125,
    "title": "HOW POWERFUL ARE GRAPH NEURAL NETWORKS?",
    "abstract": "Graph Neural Networks (GNNs) for representation learning of graphs broadly follow a neighborhood aggregation framework, where the representation vector of a node is computed by recursively aggregating and transforming feature vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs in capturing different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.",
    "prev": "RELATED WORK\n\nGeometric Deep Learning.",
    "curr": "Graph Neural Networks (GNNs) are becoming the de facto choice for learning with graph data (Bruna et al., 2013;Defferrard et al., 2016;Kipf and Welling, 2016;Gilmer et al., 2017;Xu et al., 2018;.",
    "next": "Motivated by symmetries naturally occurring in different data types, architectures are tailored to explicitly incorporate such properties (Cohen and Welling, 2016a;Thomas et al., 2018;Fuchs et al., 2020;Finzi et al., 2020;Eismann et al., 2020;Satorras et al., 2021)."
  },
  {
    "index": 686,
    "source_corpus_id": 233033761,
    "ref_id": "b9",
    "citation_corpus_id": 54101493,
    "start": 3683,
    "end": 3705,
    "title": "IMAGENET-TRAINED CNNS ARE BIASED TOWARDS TEXTURE; INCREASING SHAPE BIAS IMPROVES ACCURACY AND ROBUSTNESS",
    "abstract": "Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNettrained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on 'Stylized-ImageNet', a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation. de Beeck. Deep neural networks as a computational model for human shape sensitivity. DiCarlo. Performance-optimized hierarchical models predict neural responses in higher visual cortex.",
    "prev": "First, obtaining photorealistic appearance features at the micro-level, such as texture and illumination, is challenging due to the limits of simulation complexity and rendering granularity.",
    "curr": "Without special treatment, CNNs tend to be biased towards textures (Geirhos et al., 2019) and suffer from badly learned representations on synthetic data.",
    "next": "Second, the common lack of texture and shape variations on synthetic images often leads to collapsed and trivial representations without any diversity."
  },
  {
    "index": 687,
    "source_corpus_id": 235367997,
    "ref_id": "b20",
    "citation_corpus_id": 52967399,
    "start": 26528,
    "end": 26549,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Training: There are two steps of training involved for B2F: 1) model agnostic autoregressive BSEQ prediction task to pre-train BSEQENC; 2) model-specific training for BFRP.",
    "curr": "Autoregressive BSEQ prediction: Pre-training on auxiliary tasks to improve the quality of latent embedding is a well-known technique for deep learning methods (Devlin et al., 2019;Radford et al., 2018).",
    "next": "We pre-train BSEQENC to predict the next values of backfill sequences {x (t r +1) t ,i } i∈F ."
  },
  {
    "index": 688,
    "source_corpus_id": 220514300,
    "ref_id": "b32",
    "citation_corpus_id": 990233,
    "start": 5556,
    "end": 5560,
    "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    "abstract": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.",
    "prev": "Structured Convolutional Models\n\nWe address neural models designed to learn from (multi-)relational, structured data, while exploiting some form of parameter sharing, as \"structured convolutional models\" 1 .",
    "curr": "Examples of such models include Recursive Neural Networks [33,23], Graph Neural Networks [16,28], and various Relational Neural Networks [34,26,5,31], designed for regular trees, graphs, and general relational structures, respectively.",
    "next": "They all utilize the idea of convolution (templating), where the same parameterized pattern is carried over different subparts of the data (representation) with the same local structure."
  },
  {
    "index": 691,
    "source_corpus_id": 4117071,
    "ref_id": "b27",
    "citation_corpus_id": 11758569,
    "start": 8398,
    "end": 8420,
    "title": "UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks -demonstrating their applicability as general image representations.arXiv:1511.06434v2 [cs.LG] 7 Jan 2016Under review as a conference paper at ICLR 2016• We show that the generators have interesting vector arithmetic properties allowing for easy manipulation of many semantic qualities of generated samples.",
    "prev": "However there is an assumption that the class conditional distributions share some commonality and so information can be transferred from the source domain to the one-shot target domain.",
    "curr": "Generative Adversarial Networks (GAN) , and specifically Deep Convolutional GANs (DCGAN) (Radford et al., 2015) use of the ability to discriminate between true and generated examples as an objective, GAN approaches can learn complex joint densities.",
    "next": "Recent improvements in the optimization process  have reduced some of the failure modes of the GAN learning process."
  },
  {
    "index": 693,
    "source_corpus_id": 255749563,
    "ref_id": "b62",
    "citation_corpus_id": 239616181,
    "start": 3323,
    "end": 3343,
    "title": "ILLITERATE DALL-E LEARNS TO COMPOSE",
    "abstract": "Although DALL·E has shown an impressive ability of composition-based systematic generalization in image generation, it requires the dataset of text-image pairs and the compositionality is provided by the text. In contrast, object-centric representation models like the Slot Attention model learn composable representations without the text prompt. However, unlike DALL·E its ability to systematically generalize for zero-shot generation is significantly limited. In this paper, we propose a simple but novel slot-based autoencoding architecture, called SLATE 1 , for combining the best of both worlds: learning object-centric representations that allows systematic generalization in zero-shot image generation without text. As such, this model can also be seen as an illiterate DALL·E model. Unlike the pixel-mixture decoders of existing object-centric representation models, we propose to use the Image GPT decoder conditioned on the slots for capturing complex interactions among the slots and pixels. In experiments, we show that this simple and easy-to-implement architecture not requiring a text prompt achieves significant improvement in in-distribution and out-of-distribution (zero-shot) image generation and qualitatively comparable or better slot-attention structure than the models based on mixture decoders. https://sites.google.com/view/slate-autoencoder",
    "prev": "Yet it is quite elusive what should be the appropriate structure and granularity of these tokens to support systematic generalization and how to obtain them, particularly in the unsupervised setting where the model should learn this ability only by observing.",
    "curr": "In visual scenes, binding has recently been pursued by object-centric learning methods through the spatial binding approach (Locatello et al., 2020;Singh et al., 2022a).",
    "next": "Spatial binding aims to divide a scene spatially into smaller areas so that each area contains a meaningful entity like an object."
  },
  {
    "index": 695,
    "source_corpus_id": 36060542,
    "ref_id": "b21",
    "citation_corpus_id": 1450294,
    "start": 3838,
    "end": 3842,
    "title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps",
    "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [5], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks[13].",
    "prev": "A third weakness, that both cases share, is inherited by the way in which they generate spatial filter-wise responses, i.e.",
    "curr": "either through deconvolutionbased heatmaps [22,23,28] or by up-scaling the activation maps at a given layer/filter to the image space [2,30].",
    "next": "On the one hand, deconvolution-based methods are able to produce heatmaps with high level of detail from any filter in the network."
  },
  {
    "index": 700,
    "source_corpus_id": 258833682,
    "ref_id": "b54",
    "citation_corpus_id": 604334,
    "start": 1738,
    "end": 1742,
    "title": "Intriguing properties of neural networks",
    "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
    "prev": "Under review.",
    "curr": "Introduction\n\nDeep Neural Network (DNN) has been shown to exhibit susceptibility to adversarial attacks [55], wherein intentionally crafted imperceptible perturbations introduced into the original input cause the model's predictions to be altered.",
    "next": "Among various defense methods [32,37,60,9,42], Adversarial Training (AT) [39] stands out as one of the most effective techniques [3,59] to enhance DNN's adversarial robustness."
  },
  {
    "index": 704,
    "source_corpus_id": 222272028,
    "ref_id": "b5",
    "citation_corpus_id": 54443381,
    "start": 7151,
    "end": 7174,
    "title": "EFFICIENT LIFELONG LEARNING WITH A-GEM",
    "abstract": "In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC  and other regularizationbased methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency. 1",
    "prev": "With the trending popularity of deep learning, continual learning has gained a critical importance because the catastrophic forgetting problem imposes key challenges to deploy deep learning models in various applications (e.g Lange et al., 2019;.",
    "curr": "A growing body of research has attempted to tackle this problem in recent years (e.g Parisi et al., 2018;Toneva et al., 2018;Nguyen et al., 2019;Farajtabar et al., 2019;Hsu et al., 2018;Rusu et al., 2016;Kirkpatrick et al., 2017;Zenke et al., 2017;Shin et al., 2017;Rolnick et al., 2018;Lopez-Paz & Ranzato, 2017;Chaudhry et al., 2018b;Riemer et al., 2018;Wallingford et al., 2020).",
    "next": "Among these works, our proposed MC-SGD bares most similarities to rehearsal based methods such us (e.g."
  },
  {
    "index": 705,
    "source_corpus_id": 252408526,
    "ref_id": "b35",
    "citation_corpus_id": 247628243,
    "start": 3094,
    "end": 3098,
    "title": "EVALUATING DISTRIBUTIONAL DISTORTION IN NEURAL LANGUAGE MODELING",
    "abstract": "A fundamental characteristic of natural language is the high rate at which speakers produce novel expressions. Because of this novelty, a heavy-tail of rare events accounts for a significant amount of the total probability mass of distributions in language(Baayen, 2001). Standard language modeling metrics such as perplexity quantify the performance of language models (LM) in aggregate. As a result, we have relatively little understanding of whether neural LMs accurately estimate the probability of sequences in this heavy-tail of rare events. To address this gap, we develop a controlled evaluation scheme which uses generative models trained on natural data as artificial languages from which we can exactly compute sequence probabilities. Training LMs on generations from these artificial languages, we compare the sequence-level probability estimates given by LMs to the true probabilities in the target language. Our experiments reveal that LSTM and Transformer language models (i) systematically underestimate the probability of sequences drawn from the target language, and (ii) do so more severely for lessprobable sequences. Investigating where this probability mass went, (iii) we find that LMs tend to overestimate the probability of ill-formed (perturbed) sequences. In addition, we find that this underestimation behaviour (iv) is weakened, but not eliminated by greater amounts of training data, and (v) is exacerbated for target distributions with lower entropy.",
    "prev": "Prior work has developed a rough taxonomy of data properties, or metadata which different examples might exhibit, including but not limited to: noisy [68,71,62,63], atypical [25,10,21,60], challenging [24,3,8,49,2], prototypical or core subset selection [49,55,56,27] and out-of-distribution This approach can bring to light biases, mislabelled examples, and other dataset issues.",
    "curr": "[36].",
    "next": "While important progress has been made on some of these metadata categories individually, these categories are typically addressed in isolation reflecting an overly strong assumption that only one, known issue is at play in a given dataset."
  },
  {
    "index": 707,
    "source_corpus_id": 247222761,
    "ref_id": "b26",
    "citation_corpus_id": 3507990,
    "start": 25089,
    "end": 25107,
    "title": "Published as a conference paper at ICLR 2018 META-LEARNING FOR SEMI-SUPERVISED FEW-SHOT CLASSIFICATION",
    "abstract": "In few-shot classification, we are interested in learning algorithms that train a classifier from only a handful of labeled examples. Recent progress in few-shot classification has featured meta-learning, in which a parameterized model for a learning algorithm is defined and trained on episodes representing different classification problems, each with a small labeled training set and its corresponding test set. In this work, we advance this few-shot classification paradigm towards a scenario where unlabeled examples are also available within each episode. We consider two situations: one where all unlabeled examples are assumed to belong to the same set of classes as the labeled examples of the episode, as well as the more challenging situation where examples from other distractor classes are also provided. To address this paradigm, we propose novel extensions of Prototypical Networks (Snell et al., 2017) that are augmented with the ability to use unlabeled examples when producing prototypes. These models are trained in an end-to-end way on episodes, to learn to leverage the unlabeled examples successfully. We evaluate these methods on versions of the Omniglot and miniImageNet benchmarks, adapted to this new framework augmented with unlabeled examples. We also propose a new split of ImageNet, consisting of a large set of classes, with a hierarchical structure. Our experiments confirm that our Prototypical Networks can learn to improve their predictions due to unlabeled examples, much like a semi-supervised algorithm would. * Equal contribution. 1 See the following blog post for an overview:",
    "prev": "Furthermore to ensure that T > 0, we parametrized it with an exponential activation.",
    "curr": "FEW-SHOT IMAGE CLASSIFICATION\n\nWe evaluate COMLN on two standard few-shot image classification benchmarks: the miniImageNet (Vinyals et al., 2016) and the tieredImageNet datasets (Ren et al., 2018), both datasets being derived from ILSVRC-2012 (Russakovsky et al., 2015).",
    "next": "The process for creating tasks follows the standard procedure from the few-shot classification literature (Santoro et al., 2016), with distinct classes between the different splits."
  },
  {
    "index": 708,
    "source_corpus_id": 218889280,
    "ref_id": "b6",
    "citation_corpus_id": 848112,
    "start": 2128,
    "end": 2131,
    "title": "TRAINING GANS WITH OPTIMISM",
    "abstract": "We address the issue of limit cycling behavior in training Generative Adversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for training Wasserstein GANs. Recent theoretical results have shown that optimistic mirror decent (OMD) can enjoy faster regret rates in the context of zero-sum games. WGANs is exactly a context of solving a zero-sum game with simultaneous noregret dynamics. Moreover, we show that optimistic mirror decent addresses the limit cycling problem in training WGANs. We formally show that in the case of bi-linear zero-sum games the last iterate of OMD dynamics converges to an equilibrium, in contrast to GD dynamics which are bound to cycle. We also portray the huge qualitative difference between GD and OMD dynamics with toy examples, even when GD is modified with many adaptations proposed in the recent literature, such as gradient penalty or momentum. We apply OMD WGAN training to a bioinformatics problem of generating DNA sequences. We observe that models trained with OMD achieve consistently smaller KL divergence with respect to the true underlying distribution, than models trained with GD variants. Finally, we introduce a new algorithm, Optimistic Adam, which is an optimistic variant of Adam. We apply it to WGAN training on CIFAR10 and observe improved performance in terms of inception score as compared to Adam.",
    "prev": "The interactions between learning agents make for vastly more complex mechanics: naively applying gradient descent on each loss is known to diverge even in simple bilinear games.",
    "curr": "Related Work A large number of methods have been proposed recently to tackle the question of local convergence, or global convergence in the convex setting: adaptations of single-loss algorithms such as Extragradient (EG) [2] and Optimistic Mirror Descent (OMD) [7], Consensus Optimization (CO) for GAN training [20], Competitive Gradient Descent (CGD) based on solving a bilinear approximation of the loss functions [26], Local Symplectic Surgery (LSS) for finding local Nash equilibria [19], Symplectic Gradient Adjustment (SGA) based on a novel decomposition of game mechanics [4,16], and opponent-shaping algorithms including Learning with Opponent-Learning Awareness (LOLA) [10] and its convergent counterpart, Stable Opponent Shaping (SOS) [17].",
    "next": "Each has shown promising theoretical implications and empirical results, but none offers insight into global convergence in the non-convex setting, which includes the vast majority of machine learning applications."
  },
  {
    "index": 709,
    "source_corpus_id": 249209577,
    "ref_id": "b2",
    "citation_corpus_id": 219558760,
    "start": 10125,
    "end": 10144,
    "title": "On the Bottleneck of Graph Neural Networks and its Practical Implications",
    "abstract": "Graph neural networks (GNNs) were shown to effectively learn from highly structured data containing elements (nodes) with relationships (edges) between them. GNN variants differ in how each node in the graph absorbs the information flowing from its neighbor nodes. In this paper, we highlight an inherent problem in GNNs: the mechanism of propagating information between neighbors creates a bottleneck when every node aggregates messages from its neighbors. This bottleneck causes the over-squashing of exponentially-growing information into fixed-size vectors. As a result, the graph fails to propagate messages flowing from distant nodes and performs poorly when the prediction task depends on long-range information. We demonstrate that the bottleneck hinders popular GNNs from fitting the training data. We show that GNNs that absorb incoming edges equally, like GCN and GIN, are more susceptible to over-squashing than other GNN types. We further show that existing, extensively-tuned, GNN-based models suffer from over-squashing and that breaking the bottleneck improves state-of-the-art results without any hyperparameter tuning or additional weights.Preprint. Under review.",
    "prev": "Modern approaches implement variations of this idea as differentiable neural architectures (Gori et al., 2005;Scarselli et al., 2008;Kipf & Welling, 2017;Gilmer et al., 2017).",
    "curr": "This allows to express more elaborate forms of propagation (Li et al., 2018;Alon & Yahav, 2021) and aggregation (Wu et al., 2019;Xu et al., 2019;Li et al., 2016), including attention-based mechanisms (Veličković et al., 2018;Brody et al., 2022).",
    "next": "Nonetheless, a key result by Wu et al."
  },
  {
    "index": 711,
    "source_corpus_id": 247292293,
    "ref_id": "b47",
    "citation_corpus_id": 3292002,
    "start": 25827,
    "end": 25852,
    "title": "GRAPH ATTENTION NETWORKS",
    "abstract": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-theart results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a proteinprotein interaction dataset (wherein test graphs remain unseen during training).",
    "prev": "3(c), we compare the test accuracy averaged on eight graphs when using different GNNs e.g.",
    "curr": "GCN, SGC (Wu et al., 2019) and GAT (Velickovic et al., 2018), for data generation (See Appendix G for more results).",
    "next": "The results verify that our approach achieves consistently superior performance in different cases."
  },
  {
    "index": 713,
    "source_corpus_id": 245334722,
    "ref_id": "b6",
    "citation_corpus_id": 6628106,
    "start": 32657,
    "end": 32676,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "The only exception of this is the fine-tuning done for the few-shot learning experiment in Section 7.",
    "curr": "We use Adam (Kingma & Ba, 2015) and cosine decay (Loshchilov & Hutter, 2016) with warmup.",
    "next": "For all experiments we used a embedding size of 512, only for few-shot classification we used 1024."
  },
  {
    "index": 719,
    "source_corpus_id": 208268589,
    "ref_id": "b26",
    "citation_corpus_id": 5037032,
    "start": 7227,
    "end": 7247,
    "title": "ZERO-SHOT VISUAL IMITATION",
    "abstract": "The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both what and how to imitate.We pursue an alternative paradigm wherein an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss.In our framework, the role of the expert is only to communicate the goals (i.e., what to imitate) during inference.The learned policy is then employed to mimic the expert (i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task.Our method is \"zero-shot\" in the sense that the agent never has access to expert actions during training or for the task demonstration at inference.We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen office environments with a TurtleBot.Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance.Videos, models, and more details are available at https://pathak22.github.io/zeroshot-imitation/. * Denotes equal contribution.",
    "prev": "• Combine the local alignment and global alignment components into a reinforcement learning framework by a regularized policy update objective.",
    "curr": "RELATED WORK\n\nImitation learning is widely used in solving complicated tasks where pure reinforcement learning might suffer from high sample complexity, like robotics control (Le et al., 2017;Ye & Alterovitz, 2017;Pathak et al., 2018), autonomous vehicle (Fu et al.",
    "next": ";Pomerleau, 1989), and playing video game Pohlen et al., 2018;Aytar et al., 2018a)."
  },
  {
    "index": 720,
    "source_corpus_id": 252280667,
    "ref_id": "b1",
    "citation_corpus_id": 7167114,
    "start": 67362,
    "end": 67382,
    "title": "DEEP VARIATIONAL INFORMATION BOTTLENECK",
    "abstract": "We present a variational approximation to the information bottleneck ofTishby et al. (1999). This variational approach allows us to parameterize the information bottleneck model using a neural network and leverage the reparameterization trick for efficient training. We call this method \"Deep Variational Information Bottleneck\", or Deep VIB. We show that models trained with the VIB objective outperform those that are trained with other forms of regularization, in terms of generalization performance and robustness to adversarial attack.",
    "prev": "GWAE (GMP), AUC=1.0000.",
    "curr": "Figure 3 :\n3The ROC curves of the OoD detection inMNIST (LeCun et al., 1998) against Omniglot (Lake et al., 2015\n\nFrom\nthe Information Bottleneck (IB)(Tishby et al., 1999)  point of view, the β-VAE objective is re-interpreted as the following optimization problem(Alemi et al., 2018;Achille & Soatto, 2018): maximize θ,φ I φ (z; y)\n\n\nZhao et al., 2019)  is an extension of VAE to prevent posterior collapse by the retention of data information in the latent variables.",
    "next": "The InfoVAE objective is the sum of the ELBO and the inference model mutual information I φ in Eq."
  },
  {
    "index": 722,
    "source_corpus_id": 258480276,
    "ref_id": "b14",
    "citation_corpus_id": 238583580,
    "start": 12824,
    "end": 12840,
    "title": "TOWARDS A UNIFIED VIEW OF PARAMETER-EFFICIENT TRANSFER LEARNING",
    "abstract": "Fine-tuning large pretrained language models on downstream tasks has become the de-facto learning paradigm in NLP.However, conventional approaches finetune all the parameters of the pretrained model, which becomes prohibitive as the model size and the number of tasks grow.Recent work has proposed a variety of parameter-efficient transfer learning methods that only fine-tune a small number of (extra) parameters to attain strong performance.While effective, the critical ingredients for success and the connections among the various methods are poorly understood.In this paper, we break down the design of state-of-the-art parameter-efficient transfer learning methods and present a unified framework that establishes connections between them.Specifically, we re-frame them as modifications to specific hidden states in pretrained models, and define a set of design dimensions along which different methods vary, such as the function to compute the modification and the position to apply the modification.Through comprehensive empirical studies across machine translation, text summarization, language understanding, and text classification benchmarks, we utilize the unified view to identify important design choices in previous methods.Furthermore, our unified framework enables the transfer of design elements across different approaches, and as a result we are able to instantiate new parameter-efficient fine-tuning methods that tune less parameters than previous methods while being more effective, achieving comparable results to fine-tuning all parameters on all four tasks. 1 * Equal Contribution.Order determined by random dice rolling.",
    "prev": "SegGPT (Wang et al., 2023) introduce a robust in-context learning paradigm and can segment any images by a given image-mask prompt.SEEM (Zou et al., 2023) further presents a general segmentation model prompted by multi-modal references, e.g., language and audio, incorporating versatile semantic knowledge.In this study, we introduce a new task termed personalized object segmentation, and annotate a new dataset PerSeg for evaluation.Instead of developing large segmentation models, our goal is to personalize them to segment user-provided objects in any poses or scenes.We propose two approaches, PerSAM and PerSAM-F, which efficiently customize SAM for personalized segmentation.",
    "curr": "Parameter-efficient Fine-tuning.Directly tuning the entire foundation models on downstream tasks can be computationally expensive and memory-intensive, posing challenges for resourceconstrained applications.To address this issue, recent works have focused on developing parameterefficient methods (Sung et al., 2022;He et al., 2022;Rebuffi et al., 2017;Qin & Eisner, 2021) to freeze the weights of foundation models and append small-scale modules for fine-tuning.Prompt tuning (Lester et al., 2021;Zhou et al., 2022;Jia et al., 2022;Liu et al., 2021) suggests using learnable soft prompts alongside frozen models to perform specific downstream tasks, achieving more competitive performance with scale and robust domain transfer compared to full model tuning.",
    "next": "Low-Rank Adaption (LoRA) (Hu et al., 2021;Cuenca & Paul, 2023;Zhang et al., 2023b;Hedegaard et al., 2022) injects trainable rank decomposition matrices concurrently to each pre-trained weight, which significantly reduces the number of learnable parameters required for downstream tasks.Adapters (Houlsby et al., 2019;Pfeiffer et al., 2020;Lin et al., 2020;Chen et al., 2022) are designed to be inserted between layers of the original transformer, introducing lightweight MLPs for feature transformation.Different from existing works, we adopt a more efficient adaption met"
  },
  {
    "index": 724,
    "source_corpus_id": 2721941,
    "ref_id": "b0",
    "citation_corpus_id": 748227,
    "start": 2947,
    "end": 2968,
    "title": "Generating Sentences from a Continuous Space",
    "abstract": "The standard recurrent neural network language model (rnnlm) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an rnn-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.",
    "prev": "We find particularly promising the opportunity to leverage deep generative models for search in high-dimensional discrete spaces (Gómez-Bombarelli et al., 2016b;Kusner et al., 2017).",
    "curr": "Discrete search is at the heart of problems in drug discovery (Gómez-Bombarelli et al., 2016a), natural language processing (Bowman et al., 2016;Guimaraes et al., 2017), and symbolic regression (Kusner et al., 2017).",
    "next": "The application of deep modeling to search involves 'lifting' the search from the discrete space to a continuous space, via an autoencoder (Rumelhart et al., 1985)."
  },
  {
    "index": 727,
    "source_corpus_id": 261100669,
    "ref_id": "b6",
    "citation_corpus_id": 246652381,
    "start": 2548,
    "end": 2550,
    "title": "BACKDOOR DEFENSE VIA DECOUPLING THE TRAIN- ING PROCESS",
    "abstract": "Recent studies have revealed that deep neural networks (DNNs) are vulnerable to backdoor attacks, where attackers embed hidden backdoors in the DNN model by poisoning a few training samples. The attacked model behaves normally on benign samples, whereas its prediction will be maliciously changed when the backdoor is activated. We reveal that poisoned samples tend to cluster together in the feature space of the attacked DNN model, which is mostly due to the endto-end supervised training paradigm. Inspired by this observation, we propose a novel backdoor defense via decoupling the original end-to-end training process into three stages. Specifically, we first learn the backbone of a DNN model via self-supervised learning based on training samples without their labels. The learned backbone will map samples with the same ground-truth label to similar locations in the feature space. Then, we freeze the parameters of the learned backbone and train the remaining fully connected layers via standard training with all (labeled) training samples. Lastly, to further alleviate side-effects of poisoned samples in the second stage, we remove labels of some 'low-credible' samples determined based on the learned model and conduct a semi-supervised fine-tuning of the whole model. Extensive experiments on multiple benchmark datasets and DNN models verify that the proposed defense is effective in reducing backdoor threats while preserving high accuracy in predicting benign samples. Our code is available at https://github.comPublished as a conference paper at ICLR 2022 door trigger (dubbed poisoned samples) tend to cluster together in the feature space. We reveal that this phenomenon is mostly due to the end-to-end supervised training paradigm. Specifically, the excessive learning capability allows DNNs to learn features about the backdoor trigger, while the DNNs can shrink the distance between poisoned samples in the feature space and connect the learned trigger-related features with the target label by the end-to-end supervised training. Based on this understanding, we propose to decouple the end-to-end training process for the backdoor defense. Specifically, we treat the DNNs as two disjoint parts, including a feature extractor (i.e., backbone) and a simple classifier (i.e., the remaining fully connected layers). We first learn the purified feature extractor via self-supervised learning (Kolesnikov et al., 2019; Chen et al., 2020a; Jing & Tian,  2020)  with unlabeled training samples (obtained by removing their labels), and then learn the simple classifier via standard supervised training process based on the learned feature extractor and all training samples. The strong data augmentations involved in the self-supervised learning damage trigger patterns, making them unlearnable during representation learning; and the decoupling process further disconnects trigger patterns and the target label. Accordingly, hidden backdoors cannot be successfully created even the model is trained on the poisoned dataset based on our defense.Moreover, we further reveal that the representation of poisoned samples generated by the purified extractor is significantly different from those generated by the extractor learned with standard training process. Specifically, the poisoned sample lies closely to samples with its ground-truth label instead of the target label. This phenomenon makes the training of the simple classifier similar to label-noise learning (Wang et al., 2019b; Ma et al., 2020; Berthon et al., 2021). As such, we first filter high-credible training samples (i.e., training samples that are most probably to be benign) and then use those samples as labeled samples and the remaining part to form unlabeled samples to fine-tune the whole model via semi-supervised learning (Rasmus et al., 2015;Berthelot et al., 2019;Sohn et al., 2020). This approach is to further reduce the adverse effects of poisoned samples.The main contributions of this paper are three-fold. (1) We reveal that the backdoor is embedded in the feature space, which is mostly due to the end-to-end supervised training paradigm.(2) Based on our understanding, we propose a decoupling-based backdoor defense (DBD) to alleviate the threat of poisoning-based backdoor attacks. (3) Experiments on classical benchmark datasets are conducted, which verify the effectiveness of our defense.",
    "prev": "del, our defense, BaDExpert (Backdoor Input Detection with Backdoor Expert), effectively mitigates 17 SOTA backdoor attacks while minimally impacting clean utility.The effectiveness of BaDExpert has been verified on multiple datasets (CIFAR10, GTSRB, and ImageNet) across multiple model architectures (ResNet, VGG,\n\nIntroduction\n\nA prominent security concern of deep neural networks (DNNs) is the threat of backdoor attacks [1,2], wherein an adversary embeds hidden behaviors (backdoors) into a model through techniques such as data poisoning [3] or weights tampering [4].During inference, such a backdoor remains dormant when processing benign inputs but can be activated by trigger-planted backdoor samples devised by attackers.Upon activation, the compromised model produces anomalous outputs, which could lead to severe security breaches.",
    "curr": "The existing literature has extensively explored defensive strategies against backdoor attacks, with a significant focus on development-stage defenses [5,6,7,8].These defenses are operated before and during the model training process, primarily targeting data-poisoning-based attacks [3].",
    "next": "In this work, we rather focus on post-development defenses that operate after the model development [9,10,11,12].Given an arbitrary model that may potentially be backdoored, post-development defenses tackle the challenge of secure deployment head-on, without knowing how the model was generated.Implementing such defenses faces non-trivial technical challenges.From a methodological point of view, these defenses do not have access to the training dataset or information about training dynamics (such as gradient updates or loss information) and thus forfeit rich information that could aid in system defense.For example, approaches that directly analyze poisoned datasets [5,8] or the backdoor training dynamics [6,7] cannot be applied."
  },
  {
    "index": 733,
    "source_corpus_id": 13046179,
    "ref_id": "b7",
    "citation_corpus_id": 6706414,
    "start": 1596,
    "end": 1621,
    "title": "Published as a conference paper at ICLR 2015 EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES",
    "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples-inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
    "prev": "INTRODUCTION\n\nWhen machine learning classifiers are employed in real-world tasks, they tend to fail when the training and test distributions differ.",
    "curr": "Worse, these classifiers often fail silently by providing highconfidence predictions while being woefully incorrect (Goodfellow et al., 2015;Amodei et al., 2016).",
    "next": "Classifiers failing to indicate when they are likely mistaken can limit their adoption or cause serious accidents."
  },
  {
    "index": 734,
    "source_corpus_id": 254877510,
    "ref_id": "b6",
    "citation_corpus_id": 52967399,
    "start": 1882,
    "end": 1903,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "g data sets to obtain a single model that performs well both across all data set domains and can generalize on out-ofdomain data.We propose a dataless knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models.Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling.Further, we find that our method is a promising alternative to multi-task learning that can preserve or sometimes improve over the individual models without access to the training data.Finally, model merging is more efficient than training a multi-task model, thus making it applicable to a wider set of scenarios.",
    "curr": "1\n\nINTRODUCTION\n\nThe dominant paradigm for solving NLP tasks ranging from classification to sequence tagging involves fine-tuning a pretrained language model (PLM) using task-specific labeled data (Devlin et al., 2019;He et al., 2021).This results in specialized models that are explicitly trained to run inference over a single domain and task.Multi-task learning has shown that leveraging information across domains or tasks can be beneficial if the data sets, data set size and algorithms are well selected (Phang et al., 2018;Pruksachatkun et al., 2020;Poth et al., 2021;Weller et al., 2022).Combining knowledge of multiple data sets in a single model can lead to better overall performance on in-domain data (Poth et al., 2021), can better generalize on out-of-domain data (Wang et al., 2020b) and results in a model that is more practical and parameter efficient than maintaining specialized models.",
    "next": "However, the multi-task learning setup suffers from two practical limitations.First, the training process requires access to the original labeled data, which may not be realistic as annotated data may be private to the agent fine-tuning the model which can happen in order to ensure data or annotation privacy o"
  },
  {
    "index": 735,
    "source_corpus_id": 13298214,
    "ref_id": "b13",
    "citation_corpus_id": 8395799,
    "start": 12275,
    "end": 12299,
    "title": "Language Understanding for Text-based Games using Deep Reinforcement Learning",
    "abstract": "In this paper, we consider the task of learning control policies for text-based games. In these games, all interactions in the virtual world are through text and the underlying state is not observed. The resulting language barrier makes such environments challenging for automatic game players. We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback. This framework enables us to map text descriptions into vector representations that capture the semantics of the game states. We evaluate our approach on two game worlds, comparing against baselines using bag-ofwords and bag-of-bigrams for state representations. Our algorithm outperforms the baselines on both worlds demonstrating the importance of learning expressive representations. 1",
    "prev": "Recent work has developed on-policy RL methods such as advantage actor-critic that use asynchronous training of multiple agents in parallel (Mnih et al., 2016).",
    "curr": "Recurrent networks have also been successfully incorporated to enable state disambiguation in partially observable environments (Koutnik et al., 2013;Hausknecht & Stone, 2015;Mnih et al., 2016;Narasimhan et al., 2015).",
    "next": ", 1999)) to transfer between navigation tasks."
  },
  {
    "index": 736,
    "source_corpus_id": 1257772,
    "ref_id": "b0",
    "citation_corpus_id": 11212020,
    "start": 1804,
    "end": 1826,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.",
    "curr": "INTRODUCTION\n\nRecent advances in machine learning and deep neural networks enabled researchers to solve multiple important practical problems like image, video, text classification and others (Krizhevsky et al., 2012;Hinton et al., 2012;Bahdanau et al., 2015).",
    "next": "However, machine learning models are often vulnerable to adversarial manipulation of their input intended to cause incorrect classification (Dalvi et al., 2004)."
  },
  {
    "index": 738,
    "source_corpus_id": 259274820,
    "ref_id": "b0",
    "citation_corpus_id": 235436185,
    "start": 25378,
    "end": 25381,
    "title": "BEIT: BERT Pre-Training of Image Transformers",
    "abstract": "We introduce a self-supervised vision representation model BEIT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT [DCLT19] developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e., image patches (such as 16×16 pixels), and visual tokens (i.e., discrete tokens). We first \"tokenize\" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEIT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods.",
    "prev": "is typically utilized for downstream initialization.",
    "curr": "Except for the common supervised pre-training [16,10,24], contrastive learning (CL) [4,14,6,12] and masked image modeling (MIM) [1,44,13] dominate the recent research.",
    "next": "The former is achieved by pulling close the features of two different augment views of the input image."
  },
  {
    "index": 739,
    "source_corpus_id": 265037895,
    "ref_id": "b14",
    "citation_corpus_id": 3366315,
    "start": 2403,
    "end": 2423,
    "title": "Published as a conference paper at ICLR 2018 SPECTRAL NORMALIZATION FOR GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. The code with Chainer (Tokui et al., 2015), generated images and pretrained models are available at https://github.com/pfnet-research/sngan_ projection.Published as a conference paper at ICLR 2018• Lipschitz constant is the only hyper-parameter to be tuned, and the algorithm does not require intensive tuning of the only hyper-parameter for satisfactory performance. • Implementation is simple and the additional computational cost is small.",
    "prev": "The original GAN is, however, highly unstable and often suffers from mode collapse.",
    "curr": "Much of recent researches has focused on improving the stability of GANs (Radford et al., 2015;Heusel et al., 2017;Miyato et al., 2018;Karras et al., 2018).",
    "next": "On the theoretical aspect, Nagarajan & Kolter (2017) proved that gradient based training of the original GAN is locally stable."
  },
  {
    "index": 740,
    "source_corpus_id": 56895453,
    "ref_id": "b33",
    "citation_corpus_id": 2705742,
    "start": 8430,
    "end": 8449,
    "title": "Learning Language Games through Interaction",
    "abstract": "We introduce a new language learning setting relevant to building adaptive natural language interfaces. It is inspired by Wittgenstein's language games: a human wishes to accomplish some task (e.g., achieving a certain configuration of blocks), but can only communicate with a computer, who performs the actual actions (e.g., removing all red blocks). The computer initially knows nothing about language and therefore must learn it from scratch through interaction, while the human adapts to the computer's capabilities. We created a game called SHRDLURN in a blocks world and collected interactions from 100 people playing it. First, we analyze the humans' strategies, showing that using compositionality and avoiding synonyms correlates positively with task performance. Second, we compare computer strategies, showing that modeling pragmatics on a semantic parsing model accelerates learning for more strategic players.",
    "prev": "However, they have non-symmetric addition of agents in communication channel and are restricted to only cooperative scenarios.",
    "curr": "In contrast, a lot of work has focused on understanding agents' communication content; mostly in discrete settings with two agents (Wang et al., 2016;Havrylov & Titov, 2017;Kottur et al., 2017;Lazaridou et al., 2016;.",
    "next": "Lazaridou et al."
  },
  {
    "index": 741,
    "source_corpus_id": 252917667,
    "ref_id": "b6",
    "citation_corpus_id": 231698861,
    "start": 7500,
    "end": 7503,
    "title": "Published as a conference paper at ICLR 2021 LEARNING PARAMETRISED GRAPH SHIFT OPERATORS",
    "abstract": "In many domains data is currently represented as graphs and therefore, the graph representation of this data becomes increasingly important in machine learning. Network data is, implicitly or explicitly, always represented using a graph shift operator (GSO) with the most common choices being the adjacency, Laplacian matrices and their normalisations. In this paper, a novel parametrised GSO (PGSO) is proposed, where specific parameter values result in the most commonly used GSOs and message-passing operators in graph neural network (GNN) frameworks. The PGSO is suggested as a replacement of the standard GSOs that are used in state-of-the-art GNN architectures and the optimisation of the PGSO parameters is seamlessly included in the model training. It is proved that the PGSO has real eigenvalues and a set of real eigenvectors independent of the parameter values and spectral bounds on the PGSO are derived. PGSO parameters are shown to adapt to the sparsity of the graph structure in a study on stochastic blockmodel networks, where they are found to automatically replicate the GSO regularisation found in the literature. On several real-world datasets the accuracy of state-of-theart GNN architectures is improved by the inclusion of the PGSO in both nodeand graph-classification tasks. * Equal contribution.Published as a conference paper at ICLR 2021On different tasks and datasets, the choice between the different representations encoded by the different graph shift operator matrices has shown to be a consequential decision. Due to the past successful approaches that use different GSOs for different tasks and datasets, it is natural to assume that there is no single optimal representation for all scenarios. Finding an optimal representation of network data could contribute positively to a range of learning tasks such as node and graph classification or community detection. Fundamental to this search is an answer to Question 1. In addition, we pose the following second research question.Question 2: Can we learn such an optimal representation to encode graph structure in a numerically stable and computationally efficient way?The utilisation of a GSO as a topology representation is currently a hand-engineered choice of normalised variants of the adjacency matrix. Thus, the learnable representation of node interactions is transferred into either convolutional filters(Kipf & Welling, 2017;Hamilton et al., 2017a)or attention weights(Veličković et al., 2018), keeping the used GSO constant. In this work, we suggest a parametrisation of the GSO. Specific parameter values in our proposed parametrised (and differentiable) GSO result in the most commonly used GSOs, namely the adjacency, unnormalised Laplacian and both normalised Laplacian matrices, and GNN aggregation functions, e.g., the averaging and summation message passing operations. The beauty of this innovation is that it can be seamlessly included in both message passing and convolutional GNN architectures. Optimising the operator parameters will allow us to find answers to our two research questions.",
    "prev": "Together with Dasoulas et al.",
    "curr": "[7] and Sahbi [33], our work can be listed as a theoretically grounded way to learn the GSO.",
    "next": "Justified by formulas grounded in Monte-Carlo analysis, we show how to compensate for the nonuniformity in the sampling when computing non-uniform geometric GSOs."
  },
  {
    "index": 745,
    "source_corpus_id": 259212224,
    "ref_id": "b5",
    "citation_corpus_id": 208547755,
    "start": 2027,
    "end": 2030,
    "title": "Published as a conference paper at ICLR 2020 DREAM TO CONTROL: LEARNING BEHAVIORS BY LATENT IMAGINATION",
    "abstract": "Learned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.",
    "prev": "Introduction\n\nThe ability to predict the outcome of an agent's action over long horizons is a crucial unresolved challenge in Reinforcement Learning (RL) [1][2][3][4].",
    "curr": "This is especially important in model-based RL and planning, where deriving a policy from the learned dynamics models allows one to efficiently accomplish a wide variety of tasks in an environment [5][6][7][8][9].",
    "next": "In fact, state-of-the-art model-free techniques also rely on dynamics models to learn a better representation for downstream value prediction tasks [30]."
  },
  {
    "index": 749,
    "source_corpus_id": 239016655,
    "ref_id": "b2",
    "citation_corpus_id": 231648113,
    "start": 23035,
    "end": 23062,
    "title": "ZERO-COST PROXIES FOR LIGHTWEIGHT NAS",
    "abstract": "Neural Architecture Search (NAS) is quickly becoming the standard methodology to design neural network models. However, NAS is typically compute-intensive because multiple models need to be evaluated before choosing the best one. To reduce the computational power and time needed, a proxy task is often used for evaluating each model instead of full training. In this paper, we evaluate conventional reduced-training proxies and quantify how well they preserve ranking between multiple models during search when compared with the rankings produced by final trained accuracy. We propose a series of zero-cost proxies, based on recent pruning literature, that use just a single minibatch of training data to compute a model's score. Our zero-cost proxies use 3 orders of magnitude less computation but can match and even outperform conventional proxies. For example, Spearman's rank correlation coefficient between final validation accuracy and our best zero-cost proxy on NAS-Bench-201 is 0.82, compared to 0.61 for EcoNAS (a recently proposed reduced-training proxy). Finally, we use these zerocost proxies to enhance existing NAS search algorithms such as random search, reinforcement learning, evolutionary search and predictor-based search. For all search methodologies and across three different NAS datasets, we are able to significantly improve sample efficiency, and thereby decrease computation, by using our zero-cost proxies. For example on NAS-Bench-101, we achieved the same accuracy 4× quicker than the best previous result.",
    "prev": "Experimental setup details are included in Appendix A.3.",
    "curr": "To align with the experimental setup of prior work (Abdelfattah et al., 2021b;Mellor et al., 2021), we use two criteria to evaluate the correlations between different metrics and test accuracies across approximately 20k networks:\n\nSpearman's ρ (Daniel et al., 1990) characterizes the monotonic relationships between two variables.",
    "next": "The correlation score is restricted in range [-1, 1], where ρ = 1 denotes a perfect positive monotonic relationship and ρ = −1 denotes a perfect negative monotonic relationship."
  },
  {
    "index": 752,
    "source_corpus_id": 232290577,
    "ref_id": "b43",
    "citation_corpus_id": 196183669,
    "start": 10921,
    "end": 10940,
    "title": "Generating Fluent Adversarial Examples for Natural Languages",
    "abstract": "Efficiently building an adversarial attacker for natural language processing (NLP) tasks is a real challenge. Firstly, as the sentence space is discrete, it is difficult to make small perturbations along the direction of gradients. Secondly, the fluency of the generated examples cannot be guaranteed. In this paper, we propose MHA, which addresses both problems by performing Metropolis-Hastings sampling, whose proposal is designed with the guidance of gradients. Experiments on IMDB and SNLI show that our proposed MHA outperforms the baseline model on attacking capability. Adversarial training with MHA also leads to better robustness and performance.",
    "prev": "Different from this work, MARS is built upon the general MCMC sampling framework, which allows further enhancement with adaptive proposal learning to edit molecular graphs efficiently.",
    "curr": "Actually, generating instances from a discrete space with MCMC sampling methods is previously employed in various other applications, e.g., generating natural language sentences under various constraints Zhang et al., 2019;.",
    "next": "PROPOSED MARS APPROACH\n\nIn this section, we present the MArkov moleculaR Sampling method (MARS) for multi-objective molecular design."
  },
  {
    "index": 753,
    "source_corpus_id": 43964415,
    "ref_id": "b10",
    "citation_corpus_id": 38796293,
    "start": 1781,
    "end": 1784,
    "title": "Published as a conference paper at ICLR 2018 DEEP GRADIENT COMPRESSION: REDUCING THE COMMUNICATION BANDWIDTH FOR DISTRIBUTED TRAINING",
    "abstract": "Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD are redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during this compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270× to 600× without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.",
    "prev": "Communication has been reported to be a major bottleneck for many large scale deep learning applications, see e.g.",
    "curr": "[3,11,24,31].",
    "next": "Mini-batch parallel SGD addresses this issue by increasing compute before communication."
  },
  {
    "index": 754,
    "source_corpus_id": 251765117,
    "ref_id": "b27",
    "citation_corpus_id": 218487034,
    "start": 3201,
    "end": 3222,
    "title": "On Faithfulness and Factuality in Abstractive Summarization",
    "abstract": "It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.",
    "prev": "respects; for example, they are not able to retroactively modify or refine their own outputs.",
    "curr": "Beyond that, they are hard to control (Korbak et al., 2022) and verifying their outputs is challenging as they often hallucinate content (Maynez et al., 2020;Shuster et al., 2021;Nakano et al., 2021) and lack the ability to explain their intentions.",
    "next": "All of this makes it very difficult for humans to collaborate with such models for writing coherent, factual texts."
  },
  {
    "index": 755,
    "source_corpus_id": 235254358,
    "ref_id": "b43",
    "citation_corpus_id": 52895589,
    "start": 2361,
    "end": 2377,
    "title": "HOW POWERFUL ARE GRAPH NEURAL NETWORKS?",
    "abstract": "Graph Neural Networks (GNNs) for representation learning of graphs broadly follow a neighborhood aggregation framework, where the representation vector of a node is computed by recursively aggregating and transforming feature vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs in capturing different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.",
    "prev": "In a GNN, each node iteratively updates its state by interacting with its neighbors.",
    "curr": "GNN variants (Wu et al., 2019;Xu et al., 2019;Li et al., 2016) mostly differ in how each node aggregates and combines the representations of its neighbors with its own.",
    "next": "pioneered the use of attention-based neighborhood aggregation, in one of the most common GNN variants -Graph Attention Network (GAT)."
  },
  {
    "index": 759,
    "source_corpus_id": 264289264,
    "ref_id": "b44",
    "citation_corpus_id": 247446904,
    "start": 9874,
    "end": 9893,
    "title": "Published as a conference paper at ICLR 2023 PROTEIN REPRESENTATION LEARNING BY GEOMETRIC STRUCTURE PRETRAINING",
    "abstract": "Learning effective protein representations is critical in a variety of tasks in biology such as predicting protein function or structure. Existing approaches usually pretrain protein language models on a large number of unlabeled amino acid sequences and then finetune the models with some labeled data in downstream tasks. Despite the effectiveness of sequence-based approaches, the power of pretraining on known protein structures, which are available in smaller numbers only, has not been explored for protein property prediction, though protein structures are known to be determinants of protein function. In this paper, we propose to pretrain protein representations according to their 3D structures. We first present a simple yet effective encoder to learn the geometric features of a protein. We pretrain the protein graph encoder by leveraging multiview contrastive learning and different self-prediction tasks. Experimental results on both function prediction and fold classification tasks show that our proposed pretraining methods outperform or are on par with the state-of-the-art sequence-based methods, while using much less pretraining data. Our implementation is available at https://github.com/ DeepGraphLearning/GearNet.",
    "prev": "mising methods in the realm of protein design, which encompasses protein diffusion (Yim et al., 2023) and inverse folding (Gao et al., 2022a;Jendrusch et al., 2021;Wu et al., 2021;Ovchinnikov & Huang, 2021;Dauparas et al., 2022;Ingraham et al., 2019;Hsu et al., 2022;Gao et al., 2023b;Derevyanko et al., 2018).Specifically, a protein diffusion model first generates the backbone structure of a protein, followed by an inverse folding network that designs the corresponding sequence for this backbone.The feasibility of both these steps has been experimentally validated through cryo-electron microscopy (Watson et al., 2023;Dauparas et al., 2022), marking a significant breakthrough in the field of protein design.However, while protein diffusion methods based on frame representation achieve significant success, in these methods, atom representation is absent, rendering previous general purpose encoders unusable.",
    "curr": "GENERAL PURPOSE ENCODER\n\nIn the past, numerous encoders (Hermosilla et al., 2020;Zhang et al., 2022;Hermosilla & Ropinski, 2022;Veličković et al., 2017;Baldassarre et al., 2021;Li et al., 2022;Gao et al., 2022b;Shroff et al., 2019;Dumortier et al., 2022;McPartlon et al., 2022;Cao et al., 2021;Anishchenko et al., 2021;Karimi et al., 2020;Zhang et al., 2020;Wang et al., 2022b;Derevyanko et al., 2018) have been proposed for tasks such as model quality assessment (Townshend et al., 2021) and fold classification (Hou et al., 2018), where atomic information is available.However, these methods are not suitable for protein design tasks where atomic representations of proteins are unavailable.For instance, GVP (Jing et al., 2020) transforms input atomic coordinates into vector and scalars variables as the network input, facilitating the model's SE(3) invariance.Meanwhile, Wang et al.",
    "next": "(2022a); Jin et al."
  },
  {
    "index": 760,
    "source_corpus_id": 244527086,
    "ref_id": "b17",
    "citation_corpus_id": 3566136,
    "start": 12140,
    "end": 12169,
    "title": "Published as a conference paper at ICLR 2018 RELATIONAL NEURAL EXPECTATION MAXIMIZATION: UNSUPERVISED DISCOVERY OF OBJECTS AND THEIR INTERACTIONS",
    "abstract": "Common-sense physical reasoning is an essential ingredient for any intelligent agent operating in the real-world. For example, it can be used to simulate the environment, or to infer the state of parts of the world that are currently unobserved. In order to match real-world conditions this causal knowledge must be learned without access to supervised data. To address this problem we present a novel method that learns to discover objects and model their physical interactions from raw visual images in a purely unsupervised fashion. It incorporates prior knowledge about the compositional nature of human perception to factor interactions between object-pairs and learn efficiently. On videos of bouncing balls we show the superior modelling capabilities of our method compared to other unsupervised neural approaches that do not incorporate such prior knowledge. We demonstrate its ability to handle occlusion and show that it can extrapolate learned knowledge to scenes with different numbers of objects. * Work performed while at IDSIA.",
    "prev": "We minimize the pixel-wise squared reconstruction error (averaged over the batch), summed over both the temporal and spatial dimensions:\nL rec = T t=1 y t − y true t 2 .",
    "curr": "(4)\n\nRELATED WORK\n\nObject-centric representation learning There is a rich literature on learning object representations from static scenes (Greff et al., 2016;Eslami et al., 2016;Greff et al., 2017;2019;Burgess et al., 2019;Engelcke et al., 2020;Crawford & Pineau, 2019;Lin et al., 2020;Locatello et al., 2020;Du et al., 2021a) or videos (van Steenkiste et al., 2018;Kosiorek et al., 2018;Stelzner et al., 2019;Kipf et al., 2020;Crawford & Pineau, 2020;Creswell et al., 2021) without explicit supervision.",
    "next": "PSGNet (Bear et al., 2020) learns to decompose static images or individual frames from a video into hierarchical scene graphs using motion information estimated from neighboring video frames."
  },
  {
    "index": 761,
    "source_corpus_id": 258762142,
    "ref_id": "b10",
    "citation_corpus_id": 251252882,
    "start": 9708,
    "end": 9711,
    "title": "Prompt-to-Prompt Image Editing with Cross Attention Control",
    "abstract": "Recent large-scale text-driven synthesis models have attracted much attention thanks to their remarkable capabilities of generating highly diverse images that follow given text prompts. Such text-based synthesis methods are particularly appealing to humans who are used to verbally describe their intent. Therefore, it is only natural to extend the text-driven image synthesis to text-driven image editing. Editing is challenging for these generative models, since an innate property of an editing technique is to preserve most of the original image, while in the text-based models, even a small modification of the text prompt often leads to a completely different outcome. State-of-the-art methods mitigate this by requiring the users to provide a spatial mask to localize the edit, hence, ignoring the original structure and content within the masked region. In this paper, we pursue an intuitive prompt-toprompt editing framework, where the edits are controlled by text only. To this end, we analyze a text-conditioned model in depth and observe that the cross-attention layers are the key to controlling the relation between the spatial layout of the image to each word in the prompt. With this observation, we present several applications which monitor the image synthesis by editing the textual prompt only. This includes localized editing by replacing a word, global editing by adding a specification, and even delicately controlling the extent to which a word is reflected in the image. We present our results over diverse images and prompts, demonstrating high-quality synthesis and fidelity to the edited prompts.",
    "prev": "This leads to the emergence of the recently popular topic, subject-driven text-to-image generation [8,27].",
    "curr": "Text-Guided Image Editing Text-guided image editing [2,3,11,15,17,20] aims to edit an input image according to the given textual descriptions.",
    "next": "SDEdit [20] and Blended-Diffusion [1] blend the noisy input to the generated image in the diffusion denoising process."
  },
  {
    "index": 764,
    "source_corpus_id": 108296236,
    "ref_id": "b42",
    "citation_corpus_id": 1957433,
    "start": 2187,
    "end": 2212,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "INTRODUCTION\n\nLearned embeddings form the basis for many state-of-the-art learning systems.",
    "curr": "Word embeddings like word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), fastText (Bojanowski et al., 2017), and ELMo (Peters et al., 2018) are ubiquitous in natural language processing, where they are used for tasks like machine translation (Neubig et al., 2018), while graph embeddings (Nickel et al., 2016) like node2vec (Grover & Leskovec, 2016) are used to represent knowledge graphs and pre-trained image models (Simon et al., 2016) appear in many computer vision pipelines.",
    "next": "An effective embedding should capture the semantic structure of the data with high fidelity, in a way that is amenable to downstream tasks."
  },
  {
    "index": 768,
    "source_corpus_id": 245704504,
    "ref_id": "b9",
    "citation_corpus_id": 13890001,
    "start": 18181,
    "end": 18200,
    "title": "Published as a conference paper at ICLR 2017 NEURAL PHOTO EDITING WITH INTROSPECTIVE AD- VERSARIAL NETWORKS",
    "abstract": "The increasingly photorealistic sample quality of generative image models suggests their feasibility in applications beyond image generation. We present the Neural Photo Editor, an interface that leverages the power of generative neural networks to make large, semantically coherent changes to existing images. To tackle the challenge of achieving accurate reconstructions without loss of feature quality, we introduce the Introspective Adversarial Network, a novel hybridization of the VAE and GAN. Our model efficiently captures long-range dependencies through use of a computational block based on weight-shared dilated convolutions, and improves generalization performance with Orthogonal Regularization, a novel weight regularization method. We validate our contributions on CelebA, SVHN, and CIFAR-100, and produce samples and reconstructions with high visual fidelity.",
    "prev": "GANs inversion and editing.",
    "curr": "Another mainstream approach to image editing involves GAN inversion (Zhu et al., 2016;Brock et al., 2017), where the input is first projected into the latent space of an unconditional GAN before synthesizing a new image from the modified latent code.",
    "next": "Several methods have been proposed in this direction, including fine-tuning network weights for each image (Bau et al., 2019a;Pan et al., 2020;Roich et al., 2021), choosing better or multiple layers to project and edit (Abdal et al., 2019;Gu et al., 2020;Wu et al., 2021), designing better encoders (Richardson et al., 2021;Tov et al., 2021), modeling image corruption and transformations (Anirudh et al., 2020;Huh et al., 2020), and discovering meaningful latent directions (Shen et al., 2020;Goetschalckx et al., 2019;Jahanian et al., 2020;Härkönen et al., 2020)."
  },
  {
    "index": 771,
    "source_corpus_id": 220265858,
    "ref_id": "b38",
    "citation_corpus_id": 6359641,
    "start": 40818,
    "end": 40822,
    "title": "Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism",
    "abstract": "We propose multi-way, multilingual neural machine translation. The proposed approach enables a single neural translation model to translate between multiple languages, with a number of parameters that grows only linearly with the number of languages. This is made possible by having a single attention mechanism that is shared across all language pairs. We train the proposed multiway, multilingual model on ten language pairs from WMT'15 simultaneously and observe clear performance improvements over models trained on only one language pair. In particular, we observe that the proposed model significantly improves the translation quality of low-resource language pairs.",
    "prev": "As a result, the run-time overhead is typically negligible, even for convolutional networks where masking and padding are heavily used.",
    "curr": "We chose multilingual neural machine translation (MT) [39,40,41] to validate our design for efficient training with GShard.",
    "next": "Multilingual MT, which is an inherently multi-task learning problem, aims at building a single neural network for the goal of translating multiple language pairs simultaneously."
  },
  {
    "index": 773,
    "source_corpus_id": 3047732,
    "ref_id": "b34",
    "citation_corpus_id": 3281198,
    "start": 3782,
    "end": 3809,
    "title": "A UNIFIED PERSPECTIVE ON MULTI-DOMAIN AND MULTI-TASK LEARNING",
    "abstract": "In this paper, we provide a new neural-network based perspective on multi-task learning (MTL) and multi-domain learning (MDL). By introducing the concept of a semantic descriptor, this framework unifies MDL and MTL as well as encompassing various classic and recent MTL/MDL algorithms by interpreting them as different ways of constructing semantic descriptors. Our interpretation provides an alternative pipeline for zero-shot learning (ZSL), where a model for a novel class can be constructed without training data. Moreover, it leads to a new and practically relevant problem setting of zero-shot domain adaptation (ZSDA), which is the analogous to ZSL but for novel domains: A model for an unseen domain can be generated by its semantic descriptor. Experiments across this range of problems demonstrate that our framework outperforms a variety of alternatives.",
    "prev": "For example, one may want simultaneously predict a person's age (task one: multi-class classification or regression) as well as identify their gender (task two: binary classification) from a face image.",
    "curr": "Multi-Domain Learning: Each \"task\" corresponds to a dataset [Yang and Hospedales, 2015].",
    "next": "For example, one jointly can train a multi-class object recognition model for images captured by an HD camera (task/domain one) and for those captured by a webcam (task/domain two)."
  },
  {
    "index": 774,
    "source_corpus_id": 233474778,
    "ref_id": "b15",
    "citation_corpus_id": 14337532,
    "start": 27296,
    "end": 27323,
    "title": "SGDR: STOCHASTIC GRADIENT DESCENT WITH WARM RESTARTS",
    "abstract": "Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradientbased optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at",
    "prev": "Our proposed principles are preferable guidance for designing an efficient module for temporal modeling.",
    "curr": "We use SGD with momentum 0.9 and cosine learning rate schedule (Loshchilov & Hutter, 2017) to train the entire network.",
    "next": "The first 10 epochs are used for warm-up (Goyal et al., 2017a) to overcome early optimization difficulty."
  },
  {
    "index": 775,
    "source_corpus_id": 247778993,
    "ref_id": "b4",
    "citation_corpus_id": 54458698,
    "start": 1952,
    "end": 1978,
    "title": "Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning",
    "abstract": "Text-based adventure games provide a platform on which to explore reinforcement learning in the context of a combinatorial action space, such as natural language. We present a deep reinforcement learning architecture that represents the game state as a knowledge graph which is learned during exploration. This graph is used to prune the action space, enabling more efficient exploration. The question of which action to take can be reduced to a question-answering task, a form of transfer learning that pre-trains certain parts of our architecture. In experiments using the TextWorld framework, we show that our proposed technique can learn a control policy faster than baseline alternatives. We have also open-sourced our code at https://github.com/rajammanabrolu/KG-DQN.",
    "prev": "Our experiments show that the proposed approach consistently improves existing methods, obtains good out-of-distribution generalization, and achieves new state-of-the-art results on widely used environments.",
    "curr": "INTRODUCTION\n\nText-based games (TBGs) have emerged as key benchmarks for studying how reinforcement learning (RL) agents can tackle the challenges of grounded language understanding, partial observability, large action spaces, and out-of-distribution generalization Ammanabrolu & Riedl, 2019).",
    "next": "While we have indeed made some progress on these fronts in recent years Adhikari et al., 2020;Murugesan et al., 2021b;a), these agents are still very inefficient and suffer from insufficient generalization to novel environments."
  },
  {
    "index": 779,
    "source_corpus_id": 14711954,
    "ref_id": "b2",
    "citation_corpus_id": 3130692,
    "start": 5513,
    "end": 5535,
    "title": "Learning to Compose Neural Networks for Question Answering",
    "abstract": "We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural module network, achieves state-of-theart results on benchmark datasets in both visual and structured domains.",
    "prev": "We show that our approach substantially outperforms purely unsupervised methods that do not provide the learner with any task-specific guidance about how hierarchies should be deployed, and further that the specific use of sketches to parameterize modular subpolicies makes better use of sketches than conditioning on them directly.",
    "curr": "The present work may be viewed as an extension of recent approaches for learning compositional deep architectures from structured program descriptors (Andreas et al., 2016;Reed & de Freitas, 2016).",
    "next": "Here we focus on learning in interactive environments."
  },
  {
    "index": 781,
    "source_corpus_id": 247849778,
    "ref_id": "b15",
    "citation_corpus_id": 236459945,
    "start": 2063,
    "end": 2081,
    "title": "Enhancing Content Preservation in Text Style Transfer Using Reverse Attention and Conditional Layer Normalization",
    "abstract": "Text style transfer aims to alter the style (e.g., sentiment) of a sentence while preserving its content. A common approach is to map a given sentence to content representation that is free of style, and the content representation is fed to a decoder with a target style. Previous methods in filtering style completely remove tokens with style at the token level, which incurs the loss of content information. In this paper, we propose to enhance content preservation by implicitly removing the style information of each token with reverse attention, and thereby retain the content. Furthermore, we fuse content information when building the target style representation, making it dynamic with respect to the content. Our method creates not only styleindependent content representation, but also content-dependent style representation in transferring style. Empirical results show that our method outperforms the state-of-the-art baselines by a large margin in terms of content preservation. In addition, it is also competitive in terms of style transfer accuracy and fluency.",
    "prev": "INTRODUCTION\n\nText style transfer (TST) models learn how to transfer the style of text from source to target while preserving the style-independent content (John et al., 2019;Fu et al., 2018).",
    "curr": "Existing TST methods perform well when transferring simple styles, such as sentiment; however, they tend to do a poor job on more abstract and subtle styles, such as formality and political stance (Lee et al., 2021;Fu et al., 2019b).",
    "next": "The lack of parallel datasets is one of the main bottlenecks for text style transfer tasks."
  },
  {
    "index": 783,
    "source_corpus_id": 12130431,
    "ref_id": "b18",
    "citation_corpus_id": 14124313,
    "start": 2824,
    "end": 2851,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "We believe that our method sheds new insights on how to make deep CNNs to be applicable on mobile or embedded devices.",
    "curr": "The code is available at https://github.com\n\nINTRODUCTION\n\nDeep convolutional neural networks (CNNs) have demonstrated record breaking results on a variety of computer vision tasks such as image classification (Krizhevsky et al., 2012;Simonyan & Zisserman, 2015), face recognition (Taigman et al., 2014;Sun et al., 2014), semantic segmentation (Long et al., 2015;Chen et al., 2015a) and object detection (Girshick, 2015;Ren et al., 2015).",
    "next": "Regardless of the availability of significantly improved training resources such as abundant annotated data, powerful computational platforms and diverse training frameworks, the promising results of deep CNNs are mainly attributed to the large number of learnable parameters, ranging from tens of millions to even hundreds of millions."
  },
  {
    "index": 785,
    "source_corpus_id": 204824061,
    "ref_id": "b3",
    "citation_corpus_id": 52967399,
    "start": 1224,
    "end": 1244,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "* Work done during an internship at Facebook AI Research.",
    "curr": "INTRODUCTION\n\nThe size of modern neural sequence models (Gehring et al., 2017;Vaswani et al., 2017;Devlin et al., 2019) can amount to billions of parameters (Radford et al., 2019).",
    "next": "For example, the winning entry of the WMT'19 news machine translation task in English-German used an ensemble totaling two billion parameters ."
  },
  {
    "index": 786,
    "source_corpus_id": 238419267,
    "ref_id": "b6",
    "citation_corpus_id": 11212020,
    "start": 8264,
    "end": 8287,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "THE PROBABILISTIC ATTENTION MECHANISM\n\nWe now give a high-level explanation of our results; the detailed formulations are in Section 2.",
    "curr": "Introduced in (Bahdanau et al., 2015) and later used to define the transformer architecture (Vaswani et al., 2017), in the NLP context, attention maps a matrix of queries Q, a matrix of keys K, and a matrix of values V to the quantity Softmax(QK )V , where the softmax function (defined below) is applied row-wise to QK .",
    "next": "Just as the authors of (Petersen & Voigtlaender, 2020;Zhou, 2020) focus on the simplified versions of practically implementable ConvNets in the study of approximation theory of deep ConvNets (e.g."
  },
  {
    "index": 787,
    "source_corpus_id": 88517649,
    "ref_id": "b8",
    "citation_corpus_id": 52889459,
    "start": 2223,
    "end": 2226,
    "title": "LARGE SCALE GAN TRAINING FOR HIGH FIDELITY NATURAL IMAGE SYNTHESIS",
    "abstract": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by truncating the latent space. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128×128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.3 and Fréchet Inception Distance (FID) of 9.6, improving over the previous best IS of 52.52 and FID of 18.65.",
    "prev": "By capturing the mechanisms behind the data generation process, one can reason about data probabilistically, access and traverse the low-dimensional manifold the data is assumed to live on, and ultimately generate new data.",
    "curr": "It is therefore not surprising that learning generative models has gained momentum in applications like chemistry [16,25], NLP [8,46] and computer vision [9,48].",
    "next": "Variational Autoencoders (VAEs) [27,38] allow for a principled probabilistic way to model high-dimensional distributions."
  },
  {
    "index": 789,
    "source_corpus_id": 252118863,
    "ref_id": "b12",
    "citation_corpus_id": 52967399,
    "start": 6941,
    "end": 6945,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Early practices revolve around contrastive learning [6-8, 18, 20] where the model output features of images transformed by different data augmentations are pulled together.",
    "curr": "With the development of Masked Language Modeling (MLM) in language pre-training [13], researchers also introduce the training strategy of masked reconstruction to visual pre-training.",
    "next": "BEiT [3] uses the DALL-E [32] to encode an image patch as the target for model reconstruction."
  },
  {
    "index": 793,
    "source_corpus_id": 204907203,
    "ref_id": "b7",
    "citation_corpus_id": 52967399,
    "start": 2655,
    "end": 2676,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Besides theft of intellectual property, extracted models may leak sensitive information about the training data (Tramèr et al., 2016) or be used to generate adversarial examples that evade the model served by the API (Papernot et al., 2017).",
    "curr": "With the recent success of contextualized pretrained representations for transfer learning, NLP models created by finetuning ELMo  and BERT (Devlin et al., 2019) have become increasingly popular .",
    "next": "Contextualized pretrained representations boost performance and reduce sample complexity (Yogatama et al., 2019), and typically require only a shallow task-specific network-sometimes just a single layer as in BERT."
  },
  {
    "index": 794,
    "source_corpus_id": 238408158,
    "ref_id": "b39",
    "citation_corpus_id": 3587087,
    "start": 1885,
    "end": 1904,
    "title": "Collective Entity Disambiguation with Structured Gradient Tree Boosting",
    "abstract": "We present a gradient-tree-boosting-based structured learning model for jointly disambiguating named entities in a document. Gradient tree boosting is a widely used machine learning algorithm that underlies many topperforming natural language processing systems. Surprisingly, most works limit the use of gradient tree boosting as a tool for regular classification or regression problems, despite the structured nature of language. To the best of our knowledge, our work is the first one that employs the structured gradient tree boosting (SGTB) algorithm for collective entity disambiguation. By defining global features over previous disambiguation decisions and jointly modeling them with local features, our system is able to produce globally optimized entity assignments for mentions in a document. Exact inference is prohibitively expensive for our globally normalized model. To solve this problem, we propose Bidirectional Beam Search with Gold path (BiBSG), an approximate inference algorithm that is a variant of the standard beam search algorithm. BiBSG makes use of global information from both past and future to perform better local search. Experiments on standard benchmark datasets show that SGTB significantly improves upon published results. Specifically, SGTB outperforms the previous state-of-the-art neural system by near 1% absolute accuracy on the popular AIDA-CoNLL dataset. 1",
    "prev": "INTRODUCTION\n\nWe consider the most general form of entity linking (EL) in which a system, given a document, must both extract entity mentions and link the mentions to their corresponding entries in a knowledge base (KB).",
    "curr": "EL is a foundational building block in automatic text understanding with applications to question answering (QA) (Ferrucci, 2012), information retrieval (Xiong et al., 2017;Hasibi et al., 2016;Balog et al., 2013;Reinanda et al., 2015), and commercial recommendation systems (Yang et al., 2018;Slawski, 2015).",
    "next": "The output space in EL is intractably large."
  },
  {
    "index": 795,
    "source_corpus_id": 254096162,
    "ref_id": "b18",
    "citation_corpus_id": 235313882,
    "start": 10712,
    "end": 10731,
    "title": "NODE-GAM: NEURAL GENERALIZED ADDITIVE MODEL FOR INTERPRETABLE DEEP LEARNING",
    "abstract": "Deployment of machine learning models in real high-risk settings (e.g. healthcare) often depends not only on the model's accuracy but also on its fairness, robustness, and interpretability. Generalized Additive Models (GAMs) are a class of interpretable models with a long history of use in these high-risk domains, but they lack desirable features of deep learning such as differentiability and scalability. In this work, we propose a neural GAM (NODE-GAM) and neural GA 2 M (NODE-GA 2 M) that scale well and perform better than other GAMs on large datasets, while remaining interpretable compared to other ensemble and deep learning models. We demonstrate that our models find interesting patterns in the data. Lastly, we show that we improve model accuracy via self-supervised pre-training, an improvement that is not possible for non-differentiable GAMs.",
    "prev": "By using neural networks to learn the transformations from raw feature values to effects, Neural Additive Models (NAMs) maintain the interpretability of GAMs with more expressive effects (Agarwal et al., 2021).",
    "curr": "Pairwise interactions between effects can be included for more complex models (Yang et al., 2021;Chang et al., 2022), though complications arise involving degeneracy and the number of effect terms to inspect grows rapidly.",
    "next": "Regularization can be used to encourage sparsity in the ML model so that fewer components-weights in the network (Ng, 2004) or even features (Lemhadri et al., 2021)-conspire to produce the final prediction."
  },
  {
    "index": 796,
    "source_corpus_id": 257482484,
    "ref_id": "b13",
    "citation_corpus_id": 233169183,
    "start": 3858,
    "end": 3877,
    "title": "PLASTICINELAB: A SOFT-BODY MANIPULATION BENCHMARK WITH DIFFERENTIABLE PHYSICS",
    "abstract": "Simulated virtual environments serve as one of the main driving forces behind developing and evaluating skill learning algorithms. However, existing environments typically only simulate rigid body physics. Additionally, the simulation process usually does not provide gradients that might be useful for planning and control optimizations. We introduce a new differentiable physics benchmark called PasticineLab, which includes a diverse collection of soft body manipulation tasks. In each task, the agent uses manipulators to deform the plasticine into a desired configuration. The underlying physics engine supports differentiable elastic and plastic deformation using the DiffTaichi system, posing many underexplored challenges to robotic agents. We evaluate several existing reinforcement learning (RL) methods and gradient-based methods on this benchmark. Experimental results suggest that 1) RL-based approaches struggle to solve most of the tasks efficiently; 2) gradient-based approaches, by optimizing open-loop control sequences with the built-in differentiable physics engine, can rapidly find a solution within tens of iterations, but still fall short on multi-stage tasks that require long-term planning. We expect that PlasticineLab will encourage the development of novel algorithms that combine differentiable physics and RL for more complex physics-based skill learning tasks. PlasticineLab is publicly available 1 . * This work was done during an internship at the MIT-IBM Watson AI Lab.",
    "prev": "INTRODUCTION\n\nDeformable object manipulation (DOM) is a crucial area of research with broad applications, from household (Maitin-Shepard et al., 2010;Miller et al., 2011;Ma et al., 2022) to industrial settings (Miller et al., 2012;.",
    "curr": "To aid in algorithm development and prototyping, several DOM benchmarks Huang et al., 2021) have been developed using deformable object simulators.",
    "next": "However, the high dimensional state and action spaces remain a significant challenge to DOM."
  },
  {
    "index": 798,
    "source_corpus_id": 252918265,
    "ref_id": "b34",
    "citation_corpus_id": 3725815,
    "start": 6802,
    "end": 6821,
    "title": "Self-Attention with Relative Position Representations",
    "abstract": "Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al.(2017)achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graphlabeled inputs.",
    "prev": "(2006) (CTC) loss, identical SpecAugment (Park et al., 2019) parameters, and Adagrad optimizer (Duchi et al., 2011).",
    "curr": "The acoustic model is the same transformer architecture that was introduced in slimIPL, except that we encode positions with either absolute sinusoidal positional embedding (Vaswani et al., 2017) or the recently proposed CAPE (Likhomanenko et al., 2021b) instead of relative positional embedding (Shaw et al., 2018).",
    "next": "This allows us to speed up training (by 2-3x) and decrease the memory footprint significantly."
  },
  {
    "index": 799,
    "source_corpus_id": 240070972,
    "ref_id": "b30",
    "citation_corpus_id": 13900194,
    "start": 5143,
    "end": 5162,
    "title": "Certifying Some Distributional Robustness with Principled Adversarial Training",
    "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations. By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.* Equal contribution 1 Note that z, u ∈ R m , so trivially the dimensionality of the solution grows polynomially.2We assume that X is a subset of normed vector space.",
    "prev": "The classic threat model of adversarial robustness considers an inductive setting where a model is learned at the training time and fixed, and then at the test time, an attacker attempts to thwart the fixed model with adversarially perturbed input.",
    "curr": "This gives rise to the adversarial training Sinha et al., 2018;Carmon et al., 2019) to enhance adversarial robustness.",
    "next": "Going beyond the inductive threat model, there has been emerging interest in using transductive learning (Vapnik, 1998) 1 for adversarial robustness (Goldwasser et al., 2020;Wu et al., 2020b;Wang et al., 2021)."
  },
  {
    "index": 800,
    "source_corpus_id": 235731899,
    "ref_id": "b9",
    "citation_corpus_id": 3708505,
    "start": 2281,
    "end": 2299,
    "title": "DEEP NEURAL NETWORKS AS GAUSSIAN PROCESSES",
    "abstract": "A deep fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP) in the limit of infinite network width. This correspondence enables exact Bayesian inference for neural networks on regression tasks by means of straightforward matrix computations. For single hiddenlayer networks, the covariance function of this GP has long been known. Recently, kernel functions for multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified the correspondence between using these kernels as the covariance function for a GP and performing fully Bayesian prediction with a deep neural network. In this work, we derive this correspondence and develop a computationally efficient pipeline to compute the covariance functions. We then use the resulting GP to perform Bayesian inference for deep neural networks on MNIST and CIFAR-10. We find that the GP-based predictions are competitive and can outperform neural networks trained with stochastic gradient descent. We observe that the trained neural network accuracy approaches that of the corresponding GP-based computation with increasing layer width, and that the GP uncertainty is strongly correlated with prediction error. We connect our observations to the recent development of signal propagation in random neural networks. * Both authors contributed equally to this work. † Work done as a member of the Google Brain Residency program (g.co/brainresidency). 1 Throughout this paper, we assume the conditions on the parameter distributions and nonlinearities are such that the Central Limit Theorem will hold; for instance, that the weight variance is scaled inversely proportional to the layer width.",
    "prev": "Neal (1996) showed that a shallow but infinitely-wide Bayesian Neural Network (BNN) with random weights and biases corresponds to a GP.",
    "curr": "This result was extended to fully-connected deep neural networks of any depth (Lee et al., 2018;Matthews et al., 2018), which are shown to converge to GPs as the width grows.",
    "next": "Similar results were later obtained for deep Convolutional Neural Networks (CNNs) (Novak et al., 2018;Garriga-Alonso et al., 2019) and attention networks (Hron et al., 2020)."
  },
  {
    "index": 801,
    "source_corpus_id": 54203451,
    "ref_id": "b7",
    "citation_corpus_id": 20472740,
    "start": 2023,
    "end": 2042,
    "title": "NATURAL LANGUAGE INFERENCE OVER INTERACTION SPACE",
    "abstract": "Natural Language Inference (NLI) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis. We introduce Interactive Inference Network (IIN), a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. One instance of such architecture, Densely Interactive Inference Network (DIIN), demonstrates the state-of-the-art performance on large scale NLI copora and large-scale NLI alike corpus. It's noteworthy that DIIN achieve a greater than 20% error reduction on the challenging Multi-Genre NLI (MultiNLI; Williams et al. 2017) dataset with respect to the strongest published system.",
    "prev": "INTRODUCTION\n\nIn recent years, neural network based models have become the workhorse of natural language understanding and generation.",
    "curr": "They empower industrial systems in machine translation (Wu et al., 2016) and text generation (Kannan et al., 2016), also showing state-of-the-art performance on numerous benchmarks including Recognizing Textual Entailment (RTE) (Gong et al., 2017), Visual Question Answering (VQA) (Jiang et al., 2018), and Reading Comprehension (Wang et al., 2018).",
    "next": "Despite these successes, a growing body of literature suggests that these approaches do not generalize outside of the specific distributions on which they are trained, something that is necessary for a language understanding system to be widely deployed in the real world."
  },
  {
    "index": 802,
    "source_corpus_id": 221761540,
    "ref_id": "b26",
    "citation_corpus_id": 211146562,
    "start": 3602,
    "end": 3619,
    "title": "Published as a conference paper at ICLR 2020 DIVIDEMIX: LEARNING WITH NOISY LABELS AS SEMI-SUPERVISED LEARNING",
    "abstract": "Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at",
    "prev": "While increasing the size of data does improve the model's robustness to noise, our method can substantially boost the representation learning performance by addressing noise.",
    "curr": "There exists a large body of literature on learning with label noise (Jiang et al., 2018;Han et al., 2018;Guo et al., 2018;Tanaka et al., 2018;Arazo et al., 2019;Li et al., 2020a).",
    "next": "However, existing methods have several limitations that make them less effective for webly-supervised representation learning."
  },
  {
    "index": 803,
    "source_corpus_id": 238408412,
    "ref_id": "b10",
    "citation_corpus_id": 52967399,
    "start": 2423,
    "end": 2444,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "7a.",
    "curr": "INTRODUCTION\n\nMultilingual language models (Devlin et al., 2019;Conneau & Lample, 2019;Conneau et al., 2020;Lewis et al., 2020a;Xue et al., 2021) have achieved impressive performance on a variety of multilingual natural language processing (NLP) tasks.",
    "next": "Training a model with multiple languages jointly can be understood as a multi-task learning (MTL) problem where each language serves as a distinct task to be learned (Wang et al., 2021)."
  },
  {
    "index": 804,
    "source_corpus_id": 263909090,
    "ref_id": "b19",
    "citation_corpus_id": 253244266,
    "start": 11341,
    "end": 11358,
    "title": "A Systematic Investigation of Commonsense Knowledge in Large Language Models",
    "abstract": "Language models (LMs) trained on large amounts of data (e.g., Brown et al., 2020;Patwary et al., 2021)have shown impressive performance on many NLP tasks under the zeroshot and few-shot setup. Here we aim to better understand the extent to which such models learn commonsense knowledge -a critical component of many NLP applications. We conduct a systematic and rigorous zero-shot and few-shot commonsense evaluation of large pretrained LMs, where we: (i) carefully control for the LMs' ability to exploit potential surface cues and annotation artefacts, and (ii) account for variations in performance that arise from factors that are not related to commonsense knowledge. Our findings highlight the limitations of pre-trained LMs in acquiring commonsense knowledge without task-specific supervision; furthermore, using larger models or few-shot evaluation are insufficient to achieve human-level commonsense performance.",
    "prev": "Take the \"Take nap\" task as an example, the robot needs to understand that napping can be done on a bed, and the bed is typically located in a bedroom.",
    "curr": "Many works hold the belief that LLMs trained on large-scale data encode commonsense knowledge about the real-world (Davison et al., 2019;Li et al., 2022b;Bian et al., 2023).",
    "next": "Recently, several studies have investigated the integration of LLMs into task planning, which aims to address language ambiguities and provide robots with background knowledge Li et al., 2022a;Ahn et al., 2022)."
  },
  {
    "index": 806,
    "source_corpus_id": 256358781,
    "ref_id": "b28",
    "citation_corpus_id": 236170938,
    "start": 2281,
    "end": 2300,
    "title": "EFFICIENT NEURAL CAUSAL DISCOVERY WITHOUT ACYCLICITY CONSTRAINTS",
    "abstract": "Learning the structure of a causal graphical model using both observational and interventional data is a fundamental problem in many scientific fields. A promising direction is continuous optimization for score-based methods, which, however, require constrained optimization to enforce acyclicity or lack convergence guarantees. In this paper, we present ENCO, an efficient structure learning method for directed, acyclic causal graphs leveraging observational and interventional data. ENCO formulates the graph search as an optimization of independent edge likelihoods, with the edge orientation being modeled as a separate parameter. Consequently, we provide for ENCO convergence guarantees when interventions on all variables are available, without having to constrain the score function with respect to acyclicity. In experiments, we show that ENCO can efficiently recover graphs with hundreds of nodes, an order of magnitude larger than what was previously possible, while handling deterministic variables and discovering latent confounders. * Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.In this work, we address both problems. By modeling the orientation of an edge as a separate parameter, we can define the score function without any acyclicity constraints or regularizers. This allows for unbiased low-variance gradient estimators that scale learning to much larger graphs. Yet, if we are able to intervene on all variables, the proposed optimization is guaranteed to converge to the correct, acyclic graph. Importantly, since such interventions might not always be available, we show that our algorithm performs robustly even when intervening on fewer variables and having small sample sizes. We call our method ENCO for Efficient Neural Causal Discovery.We make the following four contributions. Firstly, we propose ENCO, a causal structure learning method for observational and interventional data using continuous optimization. Different from recent methods, ENCO models the edge orientation as a separate parameter. Secondly, we derive unbiased, low-variance gradient estimators, which is crucial for scaling up the model to large numbers of variables. Thirdly, we show that ENCO is guaranteed to converge to the correct causal graph if interventions on all variables are available, despite not having any acyclicity constraints. Yet, we show in practice that the algorithm works on partial intervention sets as well. Fourthly, we extend ENCO to detecting latent confounders. In various experimental settings, ENCO recovers graphs accurately, making less than one error on graphs with 1,000 variables in less than nine hours of computation.",
    "prev": "lems in protein-signaling and transcriptional network discovery, that our approach lies on the Pareto frontier of two key metrics, the SID and SHD.",
    "curr": "INTRODUCTION\n\nIn many domains, including cell biology (Sachs et al., 2005), finance (Sanford & Moosa, 2012), and genetics (Zhang et al., 2013), the data generating process is thought to be represented by an underlying directed acylic graph (DAG).Many models rely on DAG assumptions, e.g., causal modeling uses DAGs to model distribution shifts, ensure predictor fairness among subpopulations, or learn agents more sample-efficiently (Kaddour et al., 2022).A key question, with implications ranging from better modeling to causal discovery, is how to recover this unknown DAG from observed data alone.While there are methods for identifying the underlying DAG if given additional interventional data (Eberhardt, 2007;Hauser & Bühlmann, 2014;Shanmugam et al., 2015;Kocaoglu et al., 2017;Brouillard et al., 2020;Addanki et al., 2020;Squires et al., 2020;Lippe et al., 2022), it is not always practical or ethical to obtain such data (e.g., if one aims to discover links between dietary choices and deadly diseases).",
    "next": "Learning DAGs from observational data alone is fundamentally difficult for two reasons."
  },
  {
    "index": 807,
    "source_corpus_id": 252531622,
    "ref_id": "b15",
    "citation_corpus_id": 220347682,
    "start": 16537,
    "end": 16566,
    "title": "In Search of Lost Domain Generalization",
    "abstract": "The goal of domain generalization algorithms is to predict well on distributions different from those seen during training. While a myriad of domain generalization algorithms exist, inconsistencies in experimental conditions-datasets, architectures, and model selection criteria-render fair and realistic comparisons difficult. In this paper, we are interested in understanding how useful domain generalization algorithms are in realistic settings. As a first step, we realize that model selection is non-trivial for domain generalization tasks. Contrary to prior work, we argue that domain generalization algorithms without a model selection strategy should be regarded as incomplete. Next, we implement DOMAINBED, a testbed for domain generalization including seven multi-domain datasets, nine baseline algorithms, and three model selection criteria. We conduct extensive experiments using DO-MAINBED and find that, when carefully implemented, empirical risk minimization shows state-of-the-art performance across all datasets. Looking forward, we hope that the release of DOMAINBED, along with contributions from fellow researchers, will streamline reproducible and rigorous research in domain generalization. * Alphabetical order, equal contribution.Preprint. Under review.",
    "prev": "As mentioned before, time series OOD algorithms are currently underexplored and there are only two recent strong approaches for comparison: GILE (Qian et al., 2021) and AdaRNN (Du et al., 2021).",
    "curr": "5 We further compare with 7 general OOD methods 6 from DomainBed (Gulrajani & Lopez-Paz, 2021): ERM, DANN (Ganin et al., 2016), CORAL (Sun & Saenko, 2016), Mixup (Zhang et al., 2018), GroupDRO (Sagawa et al., 2020), RSC (Huang et al., 2020), and ANDMask (Parascandolo et al., 2021).",
    "next": "More details of these methods are in Sec."
  },
  {
    "index": 808,
    "source_corpus_id": 52135921,
    "ref_id": "b26",
    "citation_corpus_id": 11217889,
    "start": 3258,
    "end": 3283,
    "title": "Under review as a conference paper at ICLR 2018 CERTIFIED DEFENSES AGAINST ADVERSARIAL EX- AMPLES",
    "abstract": "While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no attack that perturbs each pixel by at most = 0.1 can cause more than 35% test error.",
    "prev": "It has been shown time and again ) that basically all defenses previously proposed did not increase model robustness but prevented existing attacks from finding minimal adversarial examples, the most common reason being masking of the gradients on which most attacks rely.",
    "curr": "The few verifiable defenses can only guarantee robustness within a small linear regime around the data points (Hein & Andriushchenko, 2017;Raghunathan et al., 2018).",
    "next": "The only defense currently considered effective  is a particular type of adversarial training ."
  },
  {
    "index": 810,
    "source_corpus_id": 259252546,
    "ref_id": "b3",
    "citation_corpus_id": 225039882,
    "start": 8853,
    "end": 8856,
    "title": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. 1 arXiv:2010.11929v2 [cs.CV] 3 Jun 2021Published as a conference paper at ICLR 2021 inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.However, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks.RELATED WORKTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global selfattention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.",
    "prev": ",32].Pioneering deep global motion deblurring works utilize CNNs as basic layers and achieve promising improvements in image quality.Among them, DeepDeblur [16], a multi-scale convolutional neural network, performs residual blocks to increase convergence speed.DeblurGAN [8] and DeblurGAN-v2 [9] introduce GANs and a perceptual loss to improve subjective quality.HINet [2] applies Instance Normalization to boost performance.Recently, a CNN-based local motion deblurring method, LBAG [10], bridges the gap between global and local motion deblurring by inserting gate modules at the end of MIMO-UNet architecture [3].It predicts differentiable blur masks to reduce sharp backgrounds from modifications and guide the network to deblur locally.Although the performance is significantly improved, CNN-based methods suffer from the content-independent interactions between images and convolution kernels, as well as the limitations of long-range dependency modeling.",
    "curr": "Given the Vision Transformer's (ViT) [4] ability to capture long-range dependencies, its application to global deblurring tasks has seen a surge of interest.For example, Uformer [24] employs windowbased self-attention with a learnable multi-scale restoration modulator to capture both local and global dependencies.Restormer [28] utilizes multi-head attention and a feed-forward network to achieve long-range pixel interactions.In this paper, we build a Transformer-based local motion deblurring framework, LMD-ViT, that adaptively selects windows relevant to blurry regions for window-based self-attention and feed-forward operations, simultaneously benefiting from long-range modeling.",
    "next": "Vision Transformer acceleration.Transformers have proven valuable in deblurring tasks, yet their direct application in local motion deblurring for high-resolution images presents challenges concerning computational efficiency.To solve the heavy computation problem of global self-attention in Transformers, researchers have presented several technique"
  },
  {
    "index": 811,
    "source_corpus_id": 108296442,
    "ref_id": "b8",
    "citation_corpus_id": 13276568,
    "start": 2297,
    "end": 2319,
    "title": "Learning language through pictures",
    "abstract": "We propose IMAGINET, a model of learning visually grounded representations of language from coupled textual and visual input. The model consists of two Gated Recurrent Unit networks with shared word embeddings, and uses a multi-task objective by receiving a textual description of a scene and trying to concurrently predict its visual representation and the next word in the sentence. Mimicking an important aspect of human language learning, it acquires meaning representations for individual words from descriptions of visual scenes. Moreover, it learns to effectively use sequential structure in semantic interpretation of multi-word phrases.",
    "prev": "It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "curr": "INTRODUCTION\n\nHumans are capable of learning visual concepts by jointly understanding vision and language (Fazly et al., 2010;Chrupała et al., 2015;Gauthier et al., 2018).",
    "next": "Consider the example shown in Figure 1-I."
  },
  {
    "index": 812,
    "source_corpus_id": 54477714,
    "ref_id": "b13",
    "citation_corpus_id": 8495258,
    "start": 2224,
    "end": 2245,
    "title": "A Decomposable Attention Model for Natural Language Inference",
    "abstract": "We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.",
    "prev": "To generate a coherent piece, a model needs to reference elements that came before, sometimes in the distant past, repeating, varying, and further developing them to create contrast and surprise.",
    "curr": "Intuitively, self-attention (Parikh et al., 2016) appears to be a good match for this task.",
    "next": "Self-attention over its own previous outputs allows an autoregressive model to access any part of the previously generated output at every step of generation."
  },
  {
    "index": 814,
    "source_corpus_id": 57189211,
    "ref_id": "b26",
    "citation_corpus_id": 3566136,
    "start": 26642,
    "end": 26670,
    "title": "Published as a conference paper at ICLR 2018 RELATIONAL NEURAL EXPECTATION MAXIMIZATION: UNSUPERVISED DISCOVERY OF OBJECTS AND THEIR INTERACTIONS",
    "abstract": "Common-sense physical reasoning is an essential ingredient for any intelligent agent operating in the real-world. For example, it can be used to simulate the environment, or to infer the state of parts of the world that are currently unobserved. In order to match real-world conditions this causal knowledge must be learned without access to supervised data. To address this problem we present a novel method that learns to discover objects and model their physical interactions from raw visual images in a purely unsupervised fashion. It incorporates prior knowledge about the compositional nature of human perception to factor interactions between object-pairs and learn efficiently. On videos of bouncing balls we show the superior modelling capabilities of our method compared to other unsupervised neural approaches that do not incorporate such prior knowledge. We demonstrate its ability to handle occlusion and show that it can extrapolate learned knowledge to scenes with different numbers of objects. * Work performed while at IDSIA.",
    "prev": "The insight that static observations are physically stable configurations of objects has been leveraged to improve 3D scene understanding algorithms.",
    "curr": "There has also been much attention on inferring object-factorized, or otherwise disentangled, representations of images (Eslami et al., 2016;Greff et al., 2017;van Steenkiste et al., 2018).",
    "next": "In contrast to works which aim to discover objects in a completely unsupervised manner, we focus on using object representations learned with minimal supervision, in the form of segmentation masks, for downstream tasks."
  },
  {
    "index": 818,
    "source_corpus_id": 2703040,
    "ref_id": "b19",
    "citation_corpus_id": 3144218,
    "start": 4782,
    "end": 4803,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "This process is then repeated multiple times like in classical CNNs to build a deep graph representation.",
    "curr": "Other notable works on graph neural networks include (Li et al., 2015;Schütt et al., 2017;Battaglia et al., 2016;Kipf & Welling, 2017).",
    "next": "Very recently, (Gilmer et al., 2017) showed that many of these approaches can be seen to be specific instances of a general message passing formalism, and coined the term message passing neural networks (MPNNs) to refer to them collectively."
  },
  {
    "index": 819,
    "source_corpus_id": 250334789,
    "ref_id": "b32",
    "citation_corpus_id": 3526391,
    "start": 2934,
    "end": 2953,
    "title": "ENHANCING THE RELIABILITY OF OUT-OF-DISTRIBUTION IMAGE DETECTION IN NEURAL NETWORKS",
    "abstract": "We consider the problem of detecting out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between in-and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach (Hendrycks & Gimpel, 2017) by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet (applied to CIFAR-10 and Tiny-ImageNet) when the true positive rate is 95%.A seemingly straightforward approach of detecting out-of-distribution images is to enlarge the training set of both in-and out-of-distribution examples. However, the number of out-of-distribution examples can be infinitely many, making the re-training approach computationally expensive and intractable. Moreover, to ensure that a neural network accurately classifies in-distribution samples into correct classes while correctly detecting out-of-distribution samples, one might need to employ exceedingly large neural network architectures, which further complicates the training process.Hendrycks & Gimpel (2017) proposed a baseline method to detect out-of-distribution examples without further re-training networks. The method is based on an observation that a well-trained neural network tends to assign higher softmax scores to in-distribution examples than out-of-distribution examples. In this paper, we go further. We observe that after using temperature scaling in the softmax function(Hinton et al., 2015;Pereyra et al., 2017)  and adding small controlled perturbations to inputs, . Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. 2015.",
    "prev": "INTRODUCTION\n\nLearning in the presence of Out-Of-Distribution (OOD) data has been a challenging task in machine learning, as the deployed classifier tends to fail if the unseen data drawn from unknown distributions are not properly handled (Hendrycks & Gimpel, 2017;Pan & Yang, 2009).",
    "curr": "Such a critical problem ubiquitously exists when deep models meet domain shift (Ganin et al., 2016;Tzeng et al., 2017) and unseen-class data (Hendrycks & Gimpel, 2017;Scheirer et al., 2012), which has drawn a lot of attention in some important fields such as OOD detection (Hein et al., 2019;Hendrycks & Gimpel, 2017;Lee et al., 2018;Liang et al., 2018;2023a;, Open-Set Domain Adaptation (DA) Saito et al., 2018), and Open-Set Semi-Supervised Learning (SSL) (Huang et al., 2021b;a;Oliver et al., 2018;Saito et al., 2021;.",
    "next": "In the above fields, OOD data can be divided into two types, namely benign OOD data 1 and malign OOD data."
  },
  {
    "index": 820,
    "source_corpus_id": 246634143,
    "ref_id": "b38",
    "citation_corpus_id": 14124313,
    "start": 4373,
    "end": 4401,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "• Experimental results show that our efficient models outperform the previous efficient models and yield faster model speeds with further robustness ( §5).",
    "curr": "INTRODUCTION\n\nImage classification has been advanced with deep convolutional neural networks (Simonyan & Zisserman, 2015;Huang et al., 2017;He et al., 2016b) with the common design paradigm of the network building blocks with trainable spatial convolutions inside.",
    "next": "Such trainable layers with learnable parameters effectively grasp attentive signals to distinguish input but are computationally heavy."
  },
  {
    "index": 821,
    "source_corpus_id": 252070677,
    "ref_id": "b37",
    "citation_corpus_id": 2131938,
    "start": 2278,
    "end": 2282,
    "title": "Information Extraction over Structured Data: Question Answering with Freebase",
    "abstract": "Answering natural language questions using the Freebase knowledge base has recently been explored as a platform for advancing the state of the art in open domain semantic parsing. Those efforts map questions to sophisticated meaning representations that are then attempted to be matched against viable answer candidates in the knowledge base. Here we show that relatively modest information extraction techniques, when paired with a webscale corpus, can outperform these sophisticated approaches by roughly 34% relative gain.",
    "prev": "INTRODUCTION\n\nKnowledge graphs (KGs) represent a large amount of entities and their relationships via a collection of factual triplets, (ℎ, , ), where each triplet expresses the relationship between a head entity ℎ and a tail entity .",
    "curr": "Large-scale KGs [1,19,26,32] can provide powerful knowledge inference capabilities for many intelligent applications, including question answering [38], web search [8] and recommendation systems [33].",
    "next": "As KGs are often built semiautomatically from unstructured data, it has become a well-known fact that most existing KGs are far from complete [6]."
  },
  {
    "index": 822,
    "source_corpus_id": 54472058,
    "ref_id": "b1",
    "citation_corpus_id": 11212020,
    "start": 1821,
    "end": 1843,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "INTRODUCTION\n\nDue to the power law distribution of word frequencies, rare words are extremely common in any language (Zipf, 1935).",
    "curr": "Yet, the majority of language generation tasks-including machine translation (Sutskever et al., 2014;Bahdanau et al., 2014;Luong et al., 2015), summarization (Rush et al., 2015;See et al., 2017;Paulus et al., 2018), dialogue generation (Vinyals & Le, 2015), question answering (Yin et al., 2015), speech recognition (Graves et al., 2013;Xiong et al., 2017), and others-generate words by sampling from a multinomial distribution over a closed output vocabulary.",
    "next": "This is done by computing scores for each candidate word and normalizing them to probabilities using a softmax layer."
  },
  {
    "index": 823,
    "source_corpus_id": 260497139,
    "ref_id": "b0",
    "citation_corpus_id": 18828233,
    "start": 5003,
    "end": 5005,
    "title": "TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.",
    "prev": "Negative Logarithm Alternative It is worth mentioning that in the above formulation we adopt − log(D r (x)) as an alternative of log(1−D r (x)) for the generator's loss [12].",
    "curr": "The rationale is that, when the discriminator perfectly distinguishes fake samples, the log(1−D r (x)) loss function of the generator may suffer from the gradient vanishing problem [12,1].",
    "next": "The motivation of using the − log(D r (x)) is that: though giving a difference gradient scale, it always preserves the same gradient direction as log(1−D r (x))."
  },
  {
    "index": 826,
    "source_corpus_id": 56895416,
    "ref_id": "b13",
    "citation_corpus_id": 49411844,
    "start": 3015,
    "end": 3033,
    "title": "DARTS: Differentiable Architecture Search",
    "abstract": "This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques.",
    "prev": "However, as NAS is modeled as a Markov Decision Process, credits are assigned to structural decisions with temporal-difference (TD) learning (Sutton et al., 1998), whose efficiency and interpretability suffer from delayed rewards (Arjona-Medina et al., 2018).",
    "curr": "To get rid of the architecture sampling process, DARTS (Liu et al., 2019) proposes deterministic attention on operations to analytically calculate expectation at each layer and remove operations with relatively weak attention after the convergence of the parent network.",
    "next": "Due to the pervasive non-linearity in neural operations, the performance of the child network is always inconsistent and parameter retraining comes up as necessary."
  },
  {
    "index": 827,
    "source_corpus_id": 235390444,
    "ref_id": "b13",
    "citation_corpus_id": 4986726,
    "start": 2046,
    "end": 2071,
    "title": "CENSORING REPRESENTATIONS WITH AN ADVERSARY",
    "abstract": "In practice, there are often explicit constraints on what representations or decisions are acceptable in an application of machine learning. For example it may be a legal requirement that a decision must not favour a particular group. Alternatively it can be that that representation of data must not have identifying information. We address these two related issues by learning flexible representations that minimize the capability of an adversarial critic. This adversary is trying to predict the relevant sensitive variable from the representation, and so minimizing the performance of the adversary ensures there is little or no information in the representation about the sensitive variable. We demonstrate this adversarial approach on two problems: making decisions free from discrimination and removing private information from images. We formulate the adversarial model as a minimax problem, and optimize that minimax objective using a stochastic gradient alternate min-max optimizer. We demonstrate the ability to provide discriminant free representations for standard test problems, and compare with previous state of the art methods for fairness, showing statistically significant improvement across most cases. The flexibility of this method is shown via a novel problem: removing annotations from images, from separate training examples of annotated and unannotated images, and with no a priori knowledge of the form of annotation provided to the model.",
    "prev": "Concretely, the goal is to ensure that representations have two properties: (i) they are informative for various prediction tasks of interest, (ii) sensitive attributes of the original data (e.g., race) cannot be recovered from the representations.",
    "curr": "Perhaps the most prominent approach for learning fair representations is adversarial training (Edwards & Storkey, 2016;Madras et al., 2018;Xie et al., 2017;Song et al., 2019;Roy & Boddeti, 2019), which jointly trains an encoder trying to transform data into a fair representation with an adversary attempting to recover sensitive attributes from the representation.",
    "next": "However, several recent lines of work (Feng et al., 2019;Moyer et al., 2018;Elazar & Goldberg, 2018;Xu et al., 2020;Gupta et al., 2021;Song & Shmatikov, 2020) have noticed that these approaches do not produce truly fair representations: stronger adversaries can in fact recover sensitive attributes."
  },
  {
    "index": 828,
    "source_corpus_id": 222291521,
    "ref_id": "b18",
    "citation_corpus_id": 3144218,
    "start": 3067,
    "end": 3071,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "For example, in drug discovery, when viewing molecules as graphs with atoms as nodes and chemical bonds as edges, biochemists are interested in identifying the subgraphs that mostly represent certain properties of the molecules, namely the functional groups [17,11].",
    "curr": "In graph representation learning, the predictive subgraph highlights the vital substructure for graph classification, and provides an alternative way for yielding graph representation besides mean/sum aggregation [19,30,32] and pooling aggregation [35,21,4].",
    "next": "In graph attack and defense, it is vital to purify a perturbed graph and mine the robust structures for classification [16]."
  },
  {
    "index": 829,
    "source_corpus_id": 13352766,
    "ref_id": "b2",
    "citation_corpus_id": 1740355,
    "start": 8386,
    "end": 8429,
    "title": "Under review as a conference paper at ICLR 2017 DESIGNING NEURAL NETWORK ARCHITECTURES USING REINFORCEMENT LEARNING",
    "abstract": "At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We propose a meta-modeling approach based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using Q-learning with angreedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks. , et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015. A high-throughput screening approach to discovering good forms of biologically inspired visual representation.",
    "prev": "This allows our approach to find the opti-mal architecture for the given dataset and constraints instead of being limited to that of the original model.",
    "curr": "Knowledge Distillation: Knowledge distillation is the task of training a smaller network (a \"student\") to mimic a \"teacher\" network, performing comparably to the input network (a \"teacher\") (Bucilu, Caruana, and Niculescu-Mizil 2006), (Ba and Caruana 2014), (Hinton, Vinyals, and Dean 2015), (Romero et al.",
    "next": "2014), (Urban et al."
  },
  {
    "index": 830,
    "source_corpus_id": 222208633,
    "ref_id": "b20",
    "citation_corpus_id": 3608234,
    "start": 5721,
    "end": 5740,
    "title": "Published as a conference paper at ICLR 2018 GENERATING WIKIPEDIA BY SUMMARIZING LONG SEQUENCES",
    "abstract": "We show that generating English Wikipedia articles can be approached as a multidocument summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoderdecoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations. * Joint first-authors. Ordered randomly. † Work done as a member of the Google Brain Residency (g.co/brainresidency)",
    "prev": "The most straightforward paradigm is restricting the attention pattern to be fixed local windows.",
    "curr": "Most works (Liu et al., 2018a;Parmar et al., 2018;Child et al., 2019;Huang et al., 2019;Ho et al., 2019;Hu et al., 2019;Parmar et al., 2019;Qiu et al., 2019;Beltagy et al., 2020;Zaheer et al., 2020) follow this paradigm.",
    "next": "Although restricting the attention pattern to a local neighborhood can decrease the complexity, it loses global information."
  },
  {
    "index": 832,
    "source_corpus_id": 231847109,
    "ref_id": "b9",
    "citation_corpus_id": 220249831,
    "start": 4943,
    "end": 4962,
    "title": "Heteroskedastic and Imbalanced Deep Learning with Adaptive Regularization",
    "abstract": "Real-world large-scale datasets are heteroskedastic and imbalanced -labels have varying levels of uncertainty and label distributions are long-tailed. Heteroskedasticity and imbalance challenge deep learning algorithms due to the difficulty of distinguishing among mislabeled, ambiguous, and rare examples. Addressing heteroskedasticity and imbalance simultaneously is under-explored. We propose a data-dependent regularization technique for heteroskedastic datasets that regularizes different regions of the input space differently. Inspired by the theoretical derivation of the optimal regularization strength in a one-dimensional nonparametric classification setting, our approach adaptively regularizes the data points in higher-uncertainty, lower-density regions more heavily. We test our method on several benchmark tasks, including a real-world heteroskedastic and imbalanced dataset, WebVision. Our experiments corroborate our theory and demonstrate a significant improvement over other methods in noise-robust deep learning.",
    "prev": "Robust SSL (Oliver et al., 2018;Guo Labeled set Unlabeled set \"elephant\" \"cheetah\" \"octopus\" class 1 class 2 class 3 seen classes novel classes + seen classes seen class seen class seen class novel class novel class novel class\n\nOpen-world SSL Figure 1: In the open-world SSL, the unlabeled dataset may contain classes that have never been encountered in the labeled set.",
    "curr": "Given unlabeled test set, the model needs to either assign instances to one of the classes previously seen in the labeled set, or form a novel class and assign instances to Chen et al., 2020b;Guo et al., 2020;Yu et al., 2020) assumes class distribution mismatch between labeled and unlabeled data, but in this setting the model only needs to be able to recognize (reject) instances belonging to novel classes in the unlabeled data as out-of-distribution instances.",
    "next": "In contrast, instead of rejecting instances belonging to novel classes, open-world SSL aims at discovering individual novel classes and then assigning instances to them."
  },
  {
    "index": 835,
    "source_corpus_id": 222140859,
    "ref_id": "b9",
    "citation_corpus_id": 52967399,
    "start": 2031,
    "end": 2052,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "INTRODUCTION\n\nSelf-supervised representation learning pre-trains good feature extractors from massive unlabeled data, which show promising transferability to various downstream tasks.",
    "curr": "Recent success includes large-scale pre-trained language models (e.g., BERT, RoBERTa, and GPT-3 (Devlin et al., 2019;Liu et al., 2019;Brown et al., 2020)), which have advanced state of the art over a wide range of NLP tasks such as NLI and QA, even surpassing human performance.",
    "next": "Specifically, in the computer vision domain, many studies have shown that self-supervised representation learning is essentially solving the problem of maximizing the mutual information (MI) I(X; T ) between the input X and the representation T (van den Oord et al., 2018;Belghazi et al., 2018;Hjelm et al., 2019;."
  },
  {
    "index": 837,
    "source_corpus_id": 235669672,
    "ref_id": "b23",
    "citation_corpus_id": 1487550,
    "start": 19543,
    "end": 19559,
    "title": "UNSUPERVISED WORD SENSE DISAMBIGUATION RIVALING SUPERVISED METHODS",
    "abstract": "This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints -that words tend to have one sense per discourse and one sense per collocation -exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%.",
    "prev": "We use 5 iterations, t 1 = 0.8, and t 2 = 1.2.",
    "curr": "• Self-training (Yarowsky, 1995;McClosky et al., 2006).",
    "next": "A classical semi-supervised method -each iteration, we train on pseudo-labeled data (initialized to be the original labeled dataset) and add highly confident predictions to the training set using the prediction as the label."
  },
  {
    "index": 838,
    "source_corpus_id": 256697539,
    "ref_id": "b17",
    "citation_corpus_id": 238354201,
    "start": 24567,
    "end": 24592,
    "title": "MOBILEVIT: LIGHT-WEIGHT, GENERAL-PURPOSE, AND MOBILE-FRIENDLY VISION TRANSFORMER",
    "abstract": "Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision transformers (ViTs) have been adopted. Unlike CNNs, ViTs are heavyweight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. Mobile-ViT presents a different perspective for the global processing of information with transformers. Our results show that MobileViT significantly outperforms CNNand ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters, which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7% more accurate than MobileNetv3 for a similar number of parameters. Our source code is open-source and available at: https://github.com/apple/ml-cvnets.",
    "prev": "We tested the inference time and peak memory consumption on CPU (Intel(R) Xeon(R) Gold 6246R CPU with 3.40 GHz clock), GPU (Nvidia A100), and on CPU with XNNPACK optimization.",
    "curr": "For comparison, we also tested LSTM and MobileViT (Mehta & Rastegari, 2022) networks.",
    "next": "MODELS AND TRAINING PROCESS\n\nGhostNet Our classification network consist of 3 ghost bottlenecks and ends with a convolutional layer and average pooling."
  },
  {
    "index": 847,
    "source_corpus_id": 5037032,
    "ref_id": "b32",
    "citation_corpus_id": 14724343,
    "start": 202899,
    "end": 202920,
    "title": "THIRD-PERSON IMITATION LEARNING",
    "abstract": "Reinforcement learning (RL) makes it possible to train agents capable of achieving sophisticated goals in complex and uncertain environments. A key difficulty in reinforcement learning is specifying a reward function for the agent to optimize. Traditionally, imitation learning in RL has been used to overcome this problem. Unfortunately, hitherto imitation learning methods tend to require that demonstrations are supplied in the first-person: the agent is provided with a sequence of states and a specification of the actions that it should have taken. While powerful, this kind of imitation learning is limited by the relatively hard problem of collecting first-person demonstrations. Humans address this problem by learning from third-person demonstrations: they observe other humans perform tasks, infer the task, and accomplish the same task themselves. In this paper, we present a method for unsupervised third-person imitation learning. Here third-person refers to training an agent to correctly achieve a simple goal in a simple environment when it is provided a demonstration of a teacher achieving the same goal but from a different viewpoint; and unsupervised refers to the fact that the agent receives only these third-person demonstrations, and is not provided a correspondence between teacher states and student states. Our methods primary insight is that recent advances from domain confusion can be utilized to yield domain agnostic features which are crucial during the training process. To validate our approach, we report successful experiments on learning from third-person demonstrations in a pointmass domain, a reacher domain, and inverted pendulum.",
    "prev": "(2017) observe a sequence of images from the expert demonstration for performing rope manipulations.Sermanet et al.",
    "curr": "(2017;2018) imitate humans with robots by self-supervised learning but require expert supervision at training time.Third person imitation learning (Stadie et al., 2017) and the concurrent work of imitation-from-observation (Liu et al., 2018) learn to translate expert observations into agent observations such that they can do policy optimization to minimize the distance between the agent trajectory and the translated demonstration, but they require demonstrations for learning.Visual servoing is a standard problem in robotics (Koichi & Tom, 1993) that seeks to take actions that align the agent's observation with a target configuration of carefully-designed visual features (Wilson et al., 1996;Yoshimi & Allen, 1994) or raw pixel intensities (Caron et al., 2013).Classical methods rely on fixed features or policies, but more recently end-to-end learning has improved results (Lampe & Riedmiller, 2013;Lee et al., 2017).et al.",
    "next": "(1995) jointly learn forward and inverse dynamics model but do not optimize for consistency between the forward and inverse dynamics.We empirically show that learning models by our forward consistency loss significantly improves task p"
  },
  {
    "index": 850,
    "source_corpus_id": 232233563,
    "ref_id": "b17",
    "citation_corpus_id": 28202810,
    "start": 1892,
    "end": 1914,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "Although RL has achieved impressive achievements on various tasks (Silver et al., 2017;Mnih et al., 2015;Berner et al., 2019), it is very expensive to provide dense rewards for every task we want the robot to learn.",
    "curr": "Intrinsically motivated reinforcement learning encourages the agent to explore by providing an \"internal motivation\" instead, such as curiosity (Schmidhuber, 1991;Pathak et al., 2017;Burda et al., 2018), diversity (Gregor et al., 2016;Haarnoja et al., 2018;Eysenbach et al., 2019) and empowerment (Klyubin et al., 2005;Salge et al., 2014;Mohamed & Rezende, 2015).",
    "next": "Those internal motivations can be computed on the fly when the agent is interacting with the environment, without any human engineered reward."
  },
  {
    "index": 853,
    "source_corpus_id": 211082893,
    "ref_id": "b29",
    "citation_corpus_id": 205514,
    "start": 3422,
    "end": 3444,
    "title": "DEEP MULTI-SCALE VIDEO PREDICTION BEYOND MEAN SQUARE ERROR",
    "abstract": "Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics. This is why pixel-space video prediction may be viewed as a promising avenue for unsupervised feature learning. In addition, while optical flow has been a very studied problem in computer vision for a long time, future frame prediction is rarely approached. Still, many vision applications could benefit from the knowledge of the next frames of videos, that does not require the complexity of tracking every pixel trajectory. In this work, we train a convolutional network to generate future frames given an input sequence. To deal with the inherently blurry predictions obtained from the standard Mean Squared Error (MSE) loss function, we propose three different and complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function. We compare our predictions to different published results based on recurrent neural networks on the UCF101 dataset.",
    "prev": "Videos of real-world interactions are plentiful and readily available, and a large generative model can be trained on large unlabeled datasets containing many video sequences, thereby learning about a wide range of real-world phenoma.",
    "curr": "Such a model could be useful for learning representations for further downstream tasks (Mathieu et al., 2016), or could even be used directly in applications where predicting the future enables effective decision making and control, such as robotics (Finn et al., 2016).",
    "next": "A central challenge in video prediction is that the future is highly uncertain: a short sequence of observations of the present can imply many possible futures."
  },
  {
    "index": 855,
    "source_corpus_id": 51952942,
    "ref_id": "b1",
    "citation_corpus_id": 65455367,
    "start": 8889,
    "end": 8892,
    "title": "ON THE CONVERGENCE OF ADAM AND BEYOND",
    "abstract": "Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSPROP, ADAM, ADADELTA, NADAM are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where ADAM does not converge to the optimal solution, and describe the precise problems with the previous analysis of ADAM algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with \"long-term memory\" of past gradients, and propose new variants of the ADAM algorithm which not only fix the convergence issues but often also lead to improved empirical performance.",
    "prev": "The most well-known first-order algorithms are gradient descent (GD) for deterministic optimization [6,7] and stochastic gradient descent (SGD) for stochastic optimization [8,9], where the former determines ∆ t using the full (batch) gradient of an objective function, and the latter uses a simpler but more computationally-efficient stochastic (unbiased) gradient estimate.",
    "curr": "• Different from recently developed theoretical analysis of AMSGrad [2], which has been focused on diminishing momentum controlling parameter, our convergence analysis is applicable to the more popular constant momentum parameter setting.",
    "next": "• Our work provides theoretical support for a generic class of adaptive momentum based methods, including existing algorithms such as Adam, AMSGrad, AdaGrad, as well as their new variants such as AdaGrad with momentum."
  },
  {
    "index": 856,
    "source_corpus_id": 244709059,
    "ref_id": "b28",
    "citation_corpus_id": 225094135,
    "start": 49901,
    "end": 49923,
    "title": "Published as a conference paper at ICLR 2021 LEARNING TO REPRESENT ACTION VALUES AS A HYPERGRAPH ON THE ACTION VERTICES",
    "abstract": "Action-value estimation is a critical component of many reinforcement learning (RL) methods whereby sample complexity relies heavily on how fast a good estimator for action value can be learned. By viewing this problem through the lens of representation learning, good representations of both state and action can facilitate action-value estimation. While advances in deep learning have seamlessly driven progress in learning state representations, given the specificity of the notion of agency to RL, little attention has been paid to learning action representations. We conjecture that leveraging the combinatorial structure of multi-dimensional action spaces is a key ingredient for learning good representations of action. To test this, we set forth the action hypergraph networks framework-a class of functions for learning action representations in multi-dimensional discrete action spaces with a structural inductive bias. Using this framework we realise an agent class based on a combination with deep Q-networks, which we dub hypergraph Q-networks. We show the effectiveness of our approach on a myriad of domains: illustrative prediction problems under minimal confounding effects, Atari 2600 games, and discretised physical control benchmarks.Published as a conference paper at ICLR 2021Our results advocate for the general usefulness of leveraging the combinatorial structure of multidimensional discrete action spaces, especially in problems with larger action spaces.",
    "prev": "This paradigm transforms the value decomposition to the structured prediction problem.",
    "curr": "A future work is integrating prior knowledge of the decomposition structure as many previous works for structured prediction (Chen et al., 2020;Tavakoli et al., 2021).",
    "next": "Figure 1 :\n1Learning curves on a suite of MuJoCo benchmark tasks with episodic rewards."
  },
  {
    "index": 857,
    "source_corpus_id": 251253049,
    "ref_id": "b58",
    "citation_corpus_id": 222140788,
    "start": 8761,
    "end": 8780,
    "title": "Published as a conference paper at ICLR 2021 DENOISING DIFFUSION IMPLICIT MODELS",
    "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples 10× to 50× faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error.",
    "prev": "(2021) improve inversion by conditioning the denoising process on noised low-pass filter data from the target image.",
    "curr": "demonstrate that the DDIM (Song et al., 2020) sampling process can be inverted in a closed-form manner, extracting a latent noise map that will produce a given real image.",
    "next": "In DALL-E 2 (Ramesh et al., 2022), they build on this method and demonstrate that it can be used to induce changes in the image, such as cross-image interpolations or semantic editing."
  },
  {
    "index": 858,
    "source_corpus_id": 52947979,
    "ref_id": "b12",
    "citation_corpus_id": 17306137,
    "start": 2373,
    "end": 2396,
    "title": "PIXELVAE: A LATENT VARIABLE MODEL FOR NATURAL IMAGES",
    "abstract": "Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and model global structure well but have difficulty capturing small details. PixelCNN models details very well, but lacks a latent code and is difficult to scale for capturing large structures. We present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. Our model requires very few expensive autoregressive layers compared to PixelCNN and learns latent codes that are more compressed than a standard VAE while still capturing most non-trivial structure. Finally, we extend our model to a hierarchy of latent variables at different scales. Our model achieves state-of-the-art performance on binarized MNIST, competitive performance on 64 × 64 ImageNet, and high-quality samples on the LSUN bedrooms dataset.",
    "prev": "Implementation of DReG estimators and code to reproduce experiments: sites.google.com/view/ dregs.",
    "curr": "INTRODUCTION\n\nFollowing the influential work by (Kingma & Welling, 2013;Rezende et al., 2014), deep generative models with latent variables have been widely used to model data such as natural images (Rezende & Mohamed, 2015;Gulrajani et al., 2016), speech and music time-series (Chung et al., 2015;Fraccaro et al., 2016;Krishnan et al., 2015), and video (Babaeizadeh et al., 2017;Ha & Schmidhuber, 2018;Denton & Fergus, 2018).",
    "next": "The power of these models lies in combining learned nonlinear function approximators with a principled probabilistic approach, resulting in expressive models that can capture complex distributions."
  },
  {
    "index": 859,
    "source_corpus_id": 262824542,
    "ref_id": "b30",
    "citation_corpus_id": 3608234,
    "start": 6444,
    "end": 6448,
    "title": "Published as a conference paper at ICLR 2018 GENERATING WIKIPEDIA BY SUMMARIZING LONG SEQUENCES",
    "abstract": "We show that generating English Wikipedia articles can be approached as a multidocument summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoderdecoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations. * Joint first-authors. Ordered randomly. † Work done as a member of the Google Brain Residency (g.co/brainresidency)",
    "prev": "Experimental methodology\n\nThis section details our experimental set-up (Section 2.1) and useful tools employed by our analysis: (i) measuring the relationship between learning rate and loss across scales (Section 2.2) and (ii) examining scaling trends for model characteristics (Section 2.3).",
    "curr": "Experimental set-up\n\nWe train small Transformer models [45] with a similar experimental set-up as GPT-2 [38] implemented in Flax [20]: the models are decoder-only [31] and trained with an auto-regressive loss (refer to Section A for more infrastructure details).While we experimentally manipulate many of the following hyperparameters, this section provides their default values, which we use unless otherwise specified.",
    "next": "By default, we use AdamW [33] with β 1 = 0.9, β 2 = 0.95, ϵ = 1e-8, and gradient clipping at global norm 1.The default warmup is 5e3 steps, and the default number of total steps is 1e5.We use a linear schedule for warmup and and a cosine-decay [32] schedule for the remainder, with minimum learning rate 1e-5.We use an independent weight decay of 1e-4 and auxiliary z-loss [6] with coefficient 1e-4.Sections 3.2.2 and 3.1.2respectively provide additional information and ablations on decoupled weight decay and z-loss."
  },
  {
    "index": 861,
    "source_corpus_id": 255186293,
    "ref_id": "b8",
    "citation_corpus_id": 218486942,
    "start": 28207,
    "end": 28229,
    "title": "GoEmotions: A Dataset of Fine-Grained Emotions",
    "abstract": "Understanding emotion expressed in language has a wide range of applications, from building empathetic chatbots to detecting harmful online behavior. Advancement in this area can be improved using large-scale datasets with a fine-grained typology, adaptable to multiple downstream tasks. We introduce GoEmotions, the largest manually annotated dataset of 58k English Reddit comments, labeled for 27 emotion categories or Neutral. We demonstrate the high quality of the annotations via Principal Preserved Component Analysis. We conduct transfer learning experiments with existing emotion benchmarks to show that our dataset generalizes well to other domains and different emotion taxonomies. Our BERTbased model achieves an average F1-score of .46 across our proposed taxonomy, leaving much room for improvement. 1",
    "prev": "CIFAR-100 is a single-label image recognition task, which we perform in the challenging zero-shot classification setting using CLIP (Radford et al., 2021).",
    "curr": "In addition, we conduct a natural language processing experiment using the Go Emotions (Demszky et al., 2020) dataset and a fine-tuned BERT model (Devlin et al., 2018), where the goal is to recognize emotion in text and a single instance may have multiple labels.",
    "next": "Finally, we evaluate the interaction between our method and subgroup fairness using the UCI Nursery dataset (Dua and Graff, 2017), where applicants are ranked for admissions to school."
  },
  {
    "index": 862,
    "source_corpus_id": 256503523,
    "ref_id": "b17",
    "citation_corpus_id": 203836948,
    "start": 2454,
    "end": 2474,
    "title": "INDUCTIVE MATRIX COMPLETION BASED ON GRAPH NEURAL NETWORKS",
    "abstract": "We propose an inductive matrix completion model without using side information. By factorizing the (rating) matrix into the product of low-dimensional latent embeddings of rows (users) and columns (items), a majority of existing matrix completion methods are transductive, since the learned embeddings cannot generalize to unseen rows/columns or to new matrices. To make matrix completion inductive, most previous works use content (side information), such as user's age or movie's genre, to make predictions. However, high-quality content is not always available, and can be hard to extract. Under the extreme setting where not any side information is available other than the matrix to complete, can we still learn an inductive matrix completion model? In this paper, we propose an Inductive Graph-based Matrix Completion (IGMC) model to address this problem. IGMC trains a graph neural network (GNN) based purely on 1-hop subgraphs around (user, item) pairs generated from the rating matrix and maps these subgraphs to their corresponding ratings. It achieves highly competitive performance with state-of-the-art transductive baselines. In addition, IGMC is inductive -it can generalize to users/items unseen during the training (given that their interactions exist), and can even transfer to new tasks. Our transfer learning experiments show that a model trained out of the MovieLens dataset can be directly used to predict Douban movie ratings with surprisingly good performance. Our work demonstrates that: 1) it is possible to train inductive matrix completion models without using side information while achieving similar or better performances than state-of-the-art transductive methods; 2) local graph patterns around a (user, item) pair are effective predictors of the rating this user gives to the item; and 3) Long-range dependencies might not be necessary for modeling recommender systems.",
    "prev": "Introduction\n\nLink prediction is a crucial task in graph machine learning.",
    "curr": "It has various real-world applications, such as recommender systems (Zhang & Chen, 2020), knowledge graph completion (Zhu et al., 2021), and drug interaction prediction (Souri et al., 2022).",
    "next": "Graph Neural Networks have been used in link prediction."
  },
  {
    "index": 863,
    "source_corpus_id": 227209335,
    "ref_id": "b14",
    "citation_corpus_id": 52908831,
    "start": 24092,
    "end": 24116,
    "title": "FFJORD: FREE-FORM CONTINUOUS DYNAMICS FOR SCALABLE REVERSIBLE GENERATIVE MODELS",
    "abstract": "A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling. * Equal contribution. Order determined by coin toss.",
    "prev": "4).",
    "curr": "(Dinh et al., 2016) 3.49 -iResNet  3.45 -Glow (Kingma & Dhariwal, 2018) 3.35 -MintNet (Song et al., 2019b) 3.32 -Residual Flow  3.28 46.37 FFJORD (Grathwohl et al., 2018) 3  Exact likelihood computation Leveraging the connection to neural ODEs, we can compute the density defined by Eq.",
    "next": "(12) via the instantaneous change of variables formula ."
  },
  {
    "index": 867,
    "source_corpus_id": 234357892,
    "ref_id": "b16",
    "citation_corpus_id": 209461008,
    "start": 2979,
    "end": 2983,
    "title": "Published as a conference paper at ICLR 2020 COMPUTATION REALLOCATION FOR OBJECT DETECTION",
    "abstract": "The allocation of computation resources in the backbone is a crucial issue in object detection. However, classification allocation pattern is usually adopted directly to object detector, which is proved to be sub-optimal. In order to reallocate the engaged computation resources in a more efficient way, we present CR-NAS (Computation Reallocation Neural Architecture Search) that can learn computation reallocation strategies across different feature resolution and spatial position diectly on the target detection dataset. A two-level reallocation space is proposed for both stage and spatial reallocation. A novel hierarchical search procedure is adopted to cope with the complex search space. We apply CR-NAS to multiple backbones and achieve consistent improvements. Our CR-ResNet50 and CR-MobileNetV2 outperforms the baseline by 1.9% and 1.7% COCO AP respectively without any additional computation budget. The models discovered by CR-NAS can be equiped to other powerful detection neck/head and be easily transferred to other dataset, e.g. PASCAL VOC, and other vision tasks, e.g. instance segmentation. Our CR-NAS can be used as a plugin to improve the performance of various networks, which is demanding.",
    "prev": "Without considering the prior of faces, the backbone, neck and head design of TinaFace is thus redundant and sub-optimal.",
    "curr": "Since directly taking the backbone of the classification network for object detection is sub-optimal, the recent CR-NAS [17] reallocates the computation across different resolutions.",
    "next": "It is based on the observation that the allocation of computation across different resolutions has a great impact on the Effective Receptive Field (ERF) and affects the detection performance."
  },
  {
    "index": 870,
    "source_corpus_id": 59608630,
    "ref_id": "b18",
    "citation_corpus_id": 3462549,
    "start": 14591,
    "end": 14610,
    "title": "FEW-SHOT AUTOREGRESSIVE DENSITY ESTIMATION: TOWARDS LEARNING TO LEARN DISTRIBUTIONS",
    "abstract": "Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as ImageNet. However, such models require many thousands of gradient-based weight updates and unique image examples for training. Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks. In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation. Our proposed modifications to PixelCNN result in state-of-the art few-shot density estimation on the Omniglot dataset. Furthermore, we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on ImageNet and handwriting on Omniglot without supervision. Finally, we extend the model to natural images and demonstrate few-shot image generation on the Stanford Online Products dataset.",
    "prev": "The concept of 'learning to learn' is not tied to a specific task and thus meta-learning algorithms have been successfully applied to a wide range of challenges like RL (Wang et al., 2016;Finn et al., 2017), program induction (Devlin et al., 2017) few-shot classification (Koch et al., 2015;Vinyals et al., 2016) and scene understanding .",
    "curr": "Some meta learning models generate predictions in an autoregressive fashion by predicting the next target point from the entire prior sequence of consecutive observations (Reed et al., 2018;Mishra et al., 2018).",
    "next": "Algorithms of this kind have delivered state-of-the art results in a range of tasks such as supervised learning to classification."
  },
  {
    "index": 871,
    "source_corpus_id": 263605885,
    "ref_id": "b8",
    "citation_corpus_id": 247315430,
    "start": 7124,
    "end": 7141,
    "title": "Is GPT-3 Text Indistinguishable from Human Text? SCARECROW: A Framework for Scrutinizing Machine Text",
    "abstract": "Modern neural language models can produce remarkably fluent and grammatical text. So much, in fact, that recent work by Clark et al.  (2021)  has reported that conventional crowdsourcing can no longer reliably distinguish between machine-authored (GPT-3) and humanauthored writing. As errors in machine generations become ever subtler and harder to spot, it poses a new challenge to the research community for robust machine text evaluation.We propose a new framework called SCARE-CROW for scrutinizing machine text via crowd annotation. To support the broad range of real machine errors that can be identified by laypeople, the ten error categories of SCARECROWsuch as redundancy , commonsense errors , and incoherence -are identified through several rounds of crowd annotation experiments without a predefined ontology.",
    "prev": "z f 8 G g Y e l r w A N 6 z n 7 j T 0 o 2 d G d K y t P y X S O f u 0 o u a F m y 3 n M w u O E / d c m 5 F / 0 w Y V j p N h L U t T I Z R i M W h c K e o t m l l N R 9 K C Q D X 1 g A s r / a 5 U T L j l A v 2 H t L w J j y + l / w Y X n 6 P 4 M G L f D j o n p 0 s 7 1 s l 7 8 o H s k p g c k x N y R s 5 J n w j y K y D B R t A K f o e r 4 W a 4 t U g N g 2 X N W / J H h O 8 e A K i X u X s = < / l a t e x i t > p ✓,µ (x) / p ✓ (x)e Eµ(x)   Figure 1: The decoding distribution p θ,µ induced by DAEMON scales the input LM distribution p θ with a sequence-level energy function E µ , which leads to a more accurate recovery of the underlying data distribution p d .",
    "curr": "Although pre-trained on large corpora of human texts with scaled up sizes, existing autoregressive language models (LMs) (Radford et al., 2019;Brown et al., 2020;Zhang et al., 2022) are still struggling to produce humanlike texts measured in various aspects, such as repetition, coherence, and consistency (Pillutla et al., 2021;Dou et al., 2022).Existing decoding methods are mainly driven to address two main mis-specifications of an LM's distribution: (i) The long tail of the distribution is unreliable (Holtzman et al., 2020), such that sampling from these low-probability regions often produces low-quality contents that are incoherent.",
    "next": "(ii) The mode of the distribution is degenerated (Welleck et al., 2020), where samples with high probabilities exhibit low diversity with repetitive patterns.As a result, sampling-based decoding methods (Fan et al., 2018;Holtzman et al., 2020;Meister et al., 2022) use various truncation strategies to avoid sampling from the unreliable long tail of the distribution, while recent search-based methods (Li et al., 2022;Su et al., 2022) incorporate additional contrastive objectives to avoid the collapse of degenerated repetitions.Since these two mis-specifications reside at opposing extremes of the probability spectrum, current decoding methods inevitably concentrate on Preprint."
  },
  {
    "index": 874,
    "source_corpus_id": 263608822,
    "ref_id": "b43",
    "citation_corpus_id": 254877499,
    "start": 1940,
    "end": 1961,
    "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions",
    "abstract": "Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, what to retrieve depends on what has already been derived, which in turn may depend on what was previously retrieved. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-ofdistribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning. 1 .",
    "prev": "at retrieval augmentation can sometimes have a negative effect on performance.In this work, we present a thorough analysis on five open-domain question answering benchmarks, characterizing cases when retrieval reduces accuracy.We then propose two methods to mitigate this issue.First, a simple baseline that filters out retrieved passages that do not entail question-answer pairs according to a natural language inference (NLI) model.This is effective in preventing performance reduction, but at a cost of also discarding relevant passages.Thus, we propose a method for automatically generating data to fine-tune the language model to properly leverage retrieved passages, using a mix of relevant and irrelevant contexts at training time.We empirically show that even 1,000 examples suffice to train the model to be robust to irrelevant contexts while maintaining high performance on examples with relevant ones.",
    "curr": "INTRODUCTION\n\nLarge Language Models (LLMs) (Brown et al., 2020;Chowdhery et al., 2022;Touvron et al., 2023) are the foundation on top of which modern language systems are built.However, open-domain question answering (ODQA; Chen et al.",
    "next": "2017) and other knowledge-intensive tasks (Thorne et al., 2018;Petroni et al., 2021) require vast amounts of up-to-date factual knowledge about rare entities that even very large models cannot memorize (Roberts et al., 2020;Dhingra et al., 2022).A dominant approach for combating this issue has been Retrieval Augmented Language Models (RALMs), which incorporate a retrieval mechanism to reduce the need for storing information in the LLM parameters (Guu et al., 2020;Lewis et al., 2020b;Izacard et al., 2022;Rubin & Berant, 2023).Furthermore, RALMs have also been shown to improve ODQA performance in an in-context setting (without any training), simply by prepending retrieved sentences to the input question (Ram et al., 2023).Nevertheless, retrievers are not perfect and past work has shown that noisy retrieval can negatively affect LLM performance (Petroni et al.,"
  },
  {
    "index": 875,
    "source_corpus_id": 261530996,
    "ref_id": "b24",
    "citation_corpus_id": 3338083,
    "start": 3076,
    "end": 3080,
    "title": "Published as a conference paper at ICLR 2018 SYNTHESIZING REALISTIC NEURAL POPULATION ACTIVITY PATTERNS USING GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "The ability to synthesize realistic patterns of neural activity is crucial for studying neural information processing. Here we used the Generative Adversarial Networks (GANs) framework to simulate the concerted activity of a population of neurons. We adapted the Wasserstein-GAN variant to facilitate the generation of unconstrained neural population activity patterns while still benefiting from parameter sharing in the temporal domain. We demonstrate that our proposed GAN, which we termed Spike-GAN, generates spike trains that match accurately the first-and second-order statistics of datasets of tens of neurons and also approximates well their higher-order statistics. We applied Spike-GAN to a real dataset recorded from salamander retina and showed that it performs as well as state-ofthe-art approaches based on the maximum entropy and the dichotomized Gaussian frameworks. Importantly, Spike-GAN does not require to specify a priori the statistics to be matched by the model, and so constitutes a more flexible method than these alternative approaches. Finally, we show how to exploit a trained Spike-GAN to construct 'importance maps' to detect the most relevant statistical structures present in a spike train. Spike-GAN provides a powerful, easy-to-use technique for generating realistic spiking neural activity and for describing the most relevant features of the large-scale neural population recordings studied in modern systems neuroscience.",
    "prev": "That is, (P) minimizes f over x ∈ X and y ∈ Y (the upper-level problem) when y must be one of the minimizers of g(x, ·) over y ∈ Y (the lower-level problem).",
    "curr": "Scalable optimization methods for solving (P) are in high demand to handle increasingly large-scale applications in machine-learning, including meta-learning [50], hyper-parameter optimization [20,3], model selection [37,24], adversarial networks [25,23], game theory [55], and reinforcement learning [34,56].",
    "next": "There is particular interest in developing (stochastic) gradient-descent-based methods due to their simplicity and scalability to large-scale problems [22,13,28,33,12,15,27,54,30,62]."
  },
  {
    "index": 876,
    "source_corpus_id": 257766959,
    "ref_id": "b12",
    "citation_corpus_id": 249209614,
    "start": 3570,
    "end": 3573,
    "title": "CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers",
    "abstract": "Large-scale pretrained transformers have created milestones in text (GPT-3) and text-to-image (DALL-E and CogView) generation.Its application to video generation is still facing many challenges: The potential huge computation cost makes the training from scratch unaffordable; The scarcity and weak relevance of text-video datasets hinder the model understanding complex movement semantics.In this work, we present 9B-parameter transformer CogVideo, trained by inheriting a pretrained text-to-image model, CogView2.We also propose multi-frame-rate hierarchical training strategy to better align text and video clips.As (probably) the first open-source large-scale pretrained text-to-video model, CogVideo outperforms all publicly available models at a large margin in machine and human evaluations.* Equal contribution.Preprint.Under review.",
    "prev": "Despite its potential benefits, text-conditioned video prediction (TVP) is a challenging task because it requires a deep understanding of the initial frames, the natural language instruction, and the grounding between language and images, while predicting based upon all the information above.",
    "curr": "The traditional text-conditioned video generation task [35,36,13,12,26] does not condition on initial frames and thus a model could seemingly perform well if it only generates a few prototypical videos corresponding to the input text.",
    "next": "The TVP task is much more challenging as the initial frames are given and generating prototypical videos is no longer a solution."
  },
  {
    "index": 878,
    "source_corpus_id": 257254919,
    "ref_id": "b3",
    "citation_corpus_id": 21529792,
    "start": 2126,
    "end": 2142,
    "title": "Published as a conference paper at ICLR 2018 LEARNING ROBUST REWARDS WITH ADVERSARIAL INVERSE REINFORCEMENT LEARNING",
    "abstract": "Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually specified reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose AIRL, a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation. We demonstrate that AIRL is able to recover reward functions that are robust to changes in dynamics, enabling us to learn policies even under significant variation in the environment seen during training. Our experiments show that AIRL greatly outperforms prior methods in these transfer settings.",
    "prev": "1\n\nINTRODUCTION\n\nInverse Reinforcement Learning (IRL) techniques have been developed to robustly extract behaviors from expert demonstration and solve the problems of classical Imitation Learning (IL) methods (Ng et al., 1999;Ziebart et al., 2008).",
    "curr": "Among the recent methods for IRL, the Adversarial Imitation Learning (AIL) approach (Ho & Ermon, 2016;Fu et al., 2018;Peng et al., 2021), which casts the optimization over rewards and policies into an adversarial setting, have been proven particularly successful.",
    "next": "These methods, inspired by Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), alternate between learning a discriminator, and improving the agent's policy w.r.t."
  },
  {
    "index": 889,
    "source_corpus_id": 252280353,
    "ref_id": "b15",
    "citation_corpus_id": 220347682,
    "start": 16868,
    "end": 16897,
    "title": "In Search of Lost Domain Generalization",
    "abstract": "The goal of domain generalization algorithms is to predict well on distributions different from those seen during training. While a myriad of domain generalization algorithms exist, inconsistencies in experimental conditions-datasets, architectures, and model selection criteria-render fair and realistic comparisons difficult. In this paper, we are interested in understanding how useful domain generalization algorithms are in realistic settings. As a first step, we realize that model selection is non-trivial for domain generalization tasks. Contrary to prior work, we argue that domain generalization algorithms without a model selection strategy should be regarded as incomplete. Next, we implement DOMAINBED, a testbed for domain generalization including seven multi-domain datasets, nine baseline algorithms, and three model selection criteria. We conduct extensive experiments using DO-MAINBED and find that, when carefully implemented, empirical risk minimization shows state-of-the-art performance across all datasets. Looking forward, we hope that the release of DOMAINBED, along with contributions from fellow researchers, will streamline reproducible and rigorous research in domain generalization. * Alphabetical order, equal contribution.Preprint. Under review.",
    "prev": "Time series OOD algorithms are currently less studied and there are only two recent strong approaches for comparison: GILE (Qian et al., 2021) and AdaRNN (Du et al., 2021).",
    "curr": "5 We further compare with 7 general OOD methods 6 from DomainBed (Gulrajani & Lopez-Paz, 2021): ERM, DANN (Ganin et al., 2016), CORAL (Sun & Saenko, 2016), Mixup (Zhang et al., 2018), GroupDRO (Sagawa et al., 2020), RSC (Huang et al., 2020), and ANDMask (Parascandolo et al., 2021).",
    "next": "More details of these methods are in Sec."
  },
  {
    "index": 891,
    "source_corpus_id": 208202099,
    "ref_id": "b32",
    "citation_corpus_id": 53792719,
    "start": 4767,
    "end": 4787,
    "title": "HIERARCHICAL VISUOMOTOR CONTROL OF HUMANOIDS",
    "abstract": "We aim to build complex humanoid agents that integrate perception, motor control, and memory. In this work, we partly factor this problem into low-level motor control from proprioception and high-level coordination of the low-level skills informed by vision. We develop an architecture capable of surprisingly flexible, task-directed motor control of a relatively high-DoF humanoid body by combining pre-training of low-level motor controllers with a high-level, task-focused controller that switches among low-level sub-policies. The resulting system is able to control a physically-simulated humanoid body to solve tasks that require coupling visual perception from an unstabilized egocentric RGB camera during locomotion in the environment. Supplementary video link 1 * Joint first authors.",
    "prev": "The virtual rodent affords a new opportunity to directly compare principles of artificial control to biological data from real-world rodents, which are more experimentally accessible than humans.",
    "curr": "We draw inspiration from emerging deep reinforcement learning algorithms which now allow artificial agents to perform complex and adaptive movement in physical environments with sensory information that is increasingly similar to that available to animals (Peng et al., 2016;Heess et al., 2017;Merel et al., 2019a;.",
    "next": "Similarly, our virtual rodent exists in a physical world, equipped with a set of actuators that must be coordinated for it to behave effectively."
  },
  {
    "index": 893,
    "source_corpus_id": 263835408,
    "ref_id": "b14",
    "citation_corpus_id": 44134226,
    "start": 9895,
    "end": 9913,
    "title": "Hierarchical Neural Story Generation",
    "abstract": "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.",
    "prev": "Greedy decoding, which simply selects the most probable token under IP θ (x i |x 1:i−1 ), is a special case of sampling when the temperature τ = 0.",
    "curr": "Variants of sampling-based decoding include top-p sampling (Holtzman et al., 2020) and top-k sampling (Fan et al., 2018), which restrict the sampling to the most probable tokens.",
    "next": "FIXED GENERATION CONFIGURATION FOR SAFETY EVALUATION\n\nWe notice that open-source LLMs are usually evaluated for alignment using only default generation methods, and this potentially leaves them vulnerable to misalignment when alternative strategies are used."
  },
  {
    "index": 896,
    "source_corpus_id": 219965999,
    "ref_id": "b33",
    "citation_corpus_id": 211532691,
    "start": 5026,
    "end": 5030,
    "title": "RIDE: REWARDING IMPACT-DRIVEN EXPLORATION FOR PROCEDURALLY-GENERATED ENVIRONMENTS",
    "abstract": "Exploration in sparse reward environments remains one of the key challenges of model-free reinforcement learning. Instead of solely relying on extrinsic rewards provided by the environment, many state-of-the-art methods use intrinsic rewards to encourage exploration. However, we show that existing methods fall short in procedurally-generated environments where an agent is unlikely to visit a state more than once. We propose a novel type of intrinsic reward which encourages the agent to take actions that lead to significant changes in its learned state representation. We evaluate our method on multiple challenging procedurally-generated tasks in MiniGrid, as well as on tasks with high-dimensional observations used in prior work. Our experiments demonstrate that this approach is more sample efficient than existing exploration methods, particularly for procedurally-generated MiniGrid environments. Furthermore, we analyze the learned behavior as well as the intrinsic reward received by our agent. In contrast to previous approaches, our intrinsic reward does not diminish during the course of training and it rewards the agent substantially more for interacting with objects that it can control. * Work done during an internship at Facebook AI Research.",
    "prev": "We evaluate AMIGO on MiniGrid [6], a suite of fast-to-run procedurally-generated environments with a symbolic/discrete (expressed in terms of objects like walls, doors, keys, chests and balls) observation space which isolates the problem of exploration from that of visual perception.",
    "curr": "Furthermore, [34] found that MiniGrid presents a particular challenge for existing state-of-the-art intrinsic motivation approaches.",
    "next": "Here, AMIGO sets a new state-of-the-art on some of the hardest MiniGrid environments [6], being the only method based on intrinsic motivation capable of successfully obtaining extrinsic reward on some of them."
  },
  {
    "index": 902,
    "source_corpus_id": 229212743,
    "ref_id": "b20",
    "citation_corpus_id": 6706414,
    "start": 42302,
    "end": 42327,
    "title": "Published as a conference paper at ICLR 2015 EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES",
    "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples-inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
    "prev": "Following the protocol in (Yu et al., 2019), we evaluate the robustness against five types of image perturbation: cropping and resizing, blurring with Gaussian kernel, JPEG compression, additive Gaussian noise, and random combination of them.We consider two versions of our model: the original version and the immunized version.An immunized model indicates that during training we augment generated images with the corresponding perturbation in random strengths before feeding them to the fingerprint decoder.",
    "curr": "It is worth noting that none of the encoder, decoder, and training data are accessible to the public.Therefore, the robustness against perturbation has to be experimented with the black-box assumption, as protocoled in (Yu et al., 2019).In other words, white-box perturbations such as adversarial image modifications (Goodfellow et al., 2015) and fingerprint overwriting, which requires access to the encoder, decoder, and/or training data, are not applicable in our scenario.",
    "next": "We plot in Figure 5 the comparisons of fingerprint detection accuracy among our original/immunized models and the models of (Yu et al., 2021) w.r.t."
  },
  {
    "index": 904,
    "source_corpus_id": 256615829,
    "ref_id": "b26",
    "citation_corpus_id": 221739314,
    "start": 2570,
    "end": 2586,
    "title": "Contextualized Perturbation for Textual Adversarial Attack",
    "abstract": "Adversarial examples expose the vulnerabilities of natural language processing (NLP) models, and can be used to evaluate and improve their robustness. Existing techniques of generating such examples are typically driven by local heuristic rules that are agnostic to the context, often resulting in unnatural and ungrammatical outputs. This paper presents CLARE, a ContextuaLized AdversaRial Example generation model that produces fluent and grammatical outputs through a mask-then-infill procedure. CLARE builds on a pre-trained masked language model and modifies the inputs in a contextaware manner. We propose three contextualized perturbations, Replace, Insert and Merge, that allow for generating outputs of varied lengths. CLARE can flexibly combine these perturbations and apply them at any position in the inputs, and is thus able to attack the victim model more effectively with fewer edits. Extensive experiments and human evaluation demonstrate that CLARE outperforms the baselines in terms of attack success rate, textual similarity, fluency and grammaticality. Young. 2016. Counter-fitting word vectors to linguistic constraints. In Proc. of NAACL.Daniel Naber et al. 2003. A rule-based style and grammar checker. Citeseer.",
    "prev": "INTRODUCTION\n\nDeep Neural Networks (DNNs) have obtained great progress in the field of natural language processing (NLP) but are vulnerable to adversarial attacks, leading to security and safety concerns, and research on defense algorithms against such attacks is urgently needed.",
    "curr": "Specifically, the most common attack for NLP is word-level attack (Wang et al., 2019b;Garg & Ramakrishnan, 2020;Zang et al., 2020;Li et al., 2021), which is usually implemented by adding, deleting or substituting words within a sentence.",
    "next": "Such an attack often brings catastrophic performance degradation to DNN-based models."
  },
  {
    "index": 907,
    "source_corpus_id": 219965949,
    "ref_id": "b31",
    "citation_corpus_id": 14307651,
    "start": 13721,
    "end": 13725,
    "title": "THE CONCRETE DISTRIBUTION: A CONTINUOUS RELAXATION OF DISCRETE RANDOM VARIABLES",
    "abstract": "The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce CONCRETE random variables-CONtinuous relaxations of disCRETE random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.",
    "prev": "As also mentioned by Tran et al.",
    "curr": "[52], previous work on Gumbel-softmax distributions indicate that the gradient approximation works well when the number of classes is lower than 200 [32,23].",
    "next": "Finally, the question remains if a sequence of additive bijectors of the form of Eq."
  },
  {
    "index": 910,
    "source_corpus_id": 12200521,
    "ref_id": "b14",
    "citation_corpus_id": 252796,
    "start": 13724,
    "end": 13745,
    "title": "Building a Large Annotated Corpus of English: The Penn Treebank",
    "abstract": "There is a growing consensus that significant, rapid progress can be made in both text understanding and spoken language understanding by investigating those phenomena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large corpora. Such corpora are beginning to serve as important research tools for investigators in natural language processing, speech recognition, and integrated spoken language systems, as well as in theoretical linguistics. Annotated corpora promise to be valuable for enterprises as diverse as the automatic construction of statistical models for the grammar of the written and the colloquial spoken language, the development of explicit formal theories of the differing grammars of writing and speech, the investigation of prosodic phenomena in speech, and the evaluation and comparison of the adequacy of parsing models.In this paper, we review our experience with constructing one such large annotated corpus--the Penn Treebank, a corpus 1 consisting of over 4.5 million words of American English. During the first three-year phase of the Penn Treebank Project (1989)(1990)(1991)(1992), this corpus has been annotated for part-of-speech (POS) information. In addition, over half of it has been annotated for skeletal syntactic structure. These materials are available to members of the Linguistic Data Consortium; for details, see Section 5.1.The paper is organized as follows. Section 2 discusses the POS tagging task. After outlining the considerations that informed the design of our POS tagset and presenting the tagset itself, we describe our two-stage tagging process, in which text is first assigned POS tags automatically and then corrected by human annotators. Section 3 briefly presents the results of a comparison between entirely manual and semi-automated tagging, with the latter being shown to be superior on three counts: speed, consistency, and accuracy. In Section 4, we turn to the bracketing task. Just as with the tagging task, we have partially automated the bracketing task: the output of",
    "prev": "Experiments and Discussion\n\nWe evaluate zoneout's performance on the following tasks:\n\n• Classification of hand-written digits on permuted sequential MNIST [Le et al., 2015].",
    "curr": "• Word-level language modeling on the Penn Treebank corpus [Marcus et al., 1993].",
    "next": "• Character-level language modeling on the Penn Treebank corpus\n\nWe first investigate zoneout with a shared zoneout mask on cells and hiddens on the above tasks and compare its performance with other regularizers."
  },
  {
    "index": 912,
    "source_corpus_id": 11480374,
    "ref_id": "b9",
    "citation_corpus_id": 7228830,
    "start": 5988,
    "end": 6007,
    "title": "Adversarial Examples for Evaluating Reading Comprehension Systems",
    "abstract": "Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear.To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75% F1 score to 36%; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7%. We hope our insights will motivate the development of new models that understand language more precisely.",
    "prev": "The ensemble model achieves an exact match (EM) score of 78.8% and F1 score of 85.9%.",
    "curr": "Furthermore, we have tested FusionNet against adversarial SQuAD datasets (Jia & Liang, 2017).",
    "next": "Results show that FusionNet outperforms existing state-of-the-art architectures in both datasets: on AddSent, FusionNet increases the best F1 metric from 46.6% to 51.4%; on AddOneSent, FusionNet boosts the best F1 metric from 56.0% to 60.7%."
  },
  {
    "index": 915,
    "source_corpus_id": 851777,
    "ref_id": "b7",
    "citation_corpus_id": 11383178,
    "start": 20317,
    "end": 20337,
    "title": "IMPORTANCE WEIGHTED AUTOENCODERS",
    "abstract": "The variational autoencoder (VAE; ) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.",
    "prev": "RECENT METHODS IN VARIATIONAL INFERENCE\n\nWe demonstrate Edward's flexibility for experimenting with complex inference algorithms.",
    "curr": "We consider the VAE setup from Figure 2 and  ≤ 85.8 Hierarchical variational model  ≤ 85.4 Importance-weighted auto-encoders (K = 50) (Burda et al., 2016) ≤ 86.3 HVM with IWAE objective (K = 5) ≤ 85.2 Rényi divergence (α = −1) (Li & Turner, 2016) ≤ 140.5 Table 1: Inference methods for a probabilistic decoder on binarized MNIST.",
    "next": "The Edward PPL is a convenient research platform, making it easy to both develop and experiment with many algorithms."
  },
  {
    "index": 916,
    "source_corpus_id": 247158860,
    "ref_id": "b10",
    "citation_corpus_id": 67915085,
    "start": 9066,
    "end": 9086,
    "title": "STOCHASTIC OPTIMIZATION OF SORTING NETWORKS VIA CONTINUOUS RELAXATIONS",
    "abstract": "Sorting input objects is an important step in many machine learning pipelines. However, the sorting operator is non-differentiable with respect to its inputs, which prohibits end-to-end gradient-based optimization. In this work, we propose NeuralSort, a general-purpose continuous relaxation of the output of the sorting operator from permutation matrices to the set of unimodal row-stochastic matrices, where every row sums to one and has a distinct arg max. This relaxation permits straight-through optimization of any computational graph involve a sorting operation. Further, we use this relaxation to enable gradient-based stochastic optimization over the combinatorially large space of permutations by deriving a reparameterized gradient estimator for the Plackett-Luce family of distributions over permutations. We demonstrate the usefulness of our framework on three tasks that require learning semantic orderings of high-dimensional objects, including a fully differentiable, parameterized extension of the k-nearest neighbors algorithm. ;Chierichetti et al., 2018). By learning distributions over unobserved permutations, we can account for the uncertainty in these permutations in a principled manner. However, the challenge with stochastic optimization over discrete distributions lies in gradient estimation with respect to the distribution parameters. Vanilla REINFORCE estimators are impractical for most cases, or necessitate custom control variates for low-variance gradient estimation (Glasserman, 2013).In this regard, we consider the Plackett-Luce (PL) family of distributions over permutations (Plackett, 1975; Luce, 1959). A common modeling choice for ranking models, the PL distribution is parameterized by n scores, with its support defined over the symmetric group consisting of n! permutations. We derive a reparameterizable sampler for stochastic optimization with respect to this distribution, based on Gumbel perturbations to the n (log-)scores. However, the reparameterized sampler requires sorting these perturbed scores, and hence the gradients of a downstream learning objective with respect to the scores are not defined. By using NeuralSort instead, we can approximate the objective and obtain well-defined reparameterized gradient estimates for stochastic optimization.Finally, we apply NeuralSort to tasks that require us to learn semantic orderings of complex, highdimensional input data. First, we consider sorting images of handwritten digits, where the goal is to learn to sort images by their unobserved labels. Our second task extends the first one to quantile regression, where we want to estimate the median (50-th percentile) of a set of handwritten numbers. In addition to identifying the index of the median image in the sequence, we need to learn to map the inferred median digit to its scalar representation. In the third task, we propose an algorithm that learns a basis representation for the k-nearest neighbors (kNN) classifier in an end-to-end procedure. Because the choice of the k nearest neighbors requires a non-differentiable sorting, we use NeuralSort to obtain an approximate, differentiable surrogate. On all tasks, we observe significant empirical improvements due to NeuralSort over the relevant baselines and competing relaxations to permutation matrices.PRELIMINARIESAn n-dimensional permutation z = [z 1 , z 2 , . . . , z n ] T is a list of unique indices {1, 2, . . . , n}. Every permutation z is associated with a permutation matrix P z ∈ {0, 1} n×n with entries given as: P z [i, j] = 1 if j = z i 0 otherwise. multimodal graph matching: Sparse coding meets graph matching. In Advances in Neural Information Processing Systems, 2013.",
    "prev": "Differentiable sorting & ranking.",
    "curr": "Differentiable sorting and ranking algorithms (Adams & Zemel, 2011;Grover et al., 2018;Blondel et al., 2020;Petersen et al., 2021) can be used in training neural networks with sorting and ranking supervision.",
    "next": "Recent approach (Blondel et al., 2020) proposes to construct differentiable sorting and ranking operators as projections onto the permutahedron, i.e., the convex hull of permutations, and using a reduction to isotonic optimization."
  },
  {
    "index": 917,
    "source_corpus_id": 53576131,
    "ref_id": "b3",
    "citation_corpus_id": 38407095,
    "start": 5929,
    "end": 5942,
    "title": "Europarl: A Parallel Corpus for Statistical Machine Translation",
    "abstract": "We collected a corpus of parallel text in 11 languages from the proceedings of the European Parliament, which are published on the web 1 . This corpus has found widespread use in the NLP community. Here, we focus on its acquisition and its application as training data for statistical machine translation (SMT). We trained SMT systems for 110 language pairs, which reveal interesting clues into the challenges ahead.",
    "prev": "• We systematically analyze what information is captured by units in representation across multiple settings by varying network architectures, tasks, and datasets.",
    "curr": "We use VD- CNN (Conneau et al., 2017) for sentiment and topic classification tasks on Yelp Reviews, AG News (Zhang et al., 2015), and DBpedia ontology dataset (Lehmann et al., 2015) and ByteNet (Kalchbrenner et al., 2016) for translation tasks on Europarl (Koehn, 2005) and News Commentary (Tiedemann, 2012) datasets.",
    "next": "• We also analyze how aligned natural language concepts evolve as they get represented in deeper layers."
  },
  {
    "index": 921,
    "source_corpus_id": 219636236,
    "ref_id": "b6",
    "citation_corpus_id": 52889459,
    "start": 27586,
    "end": 27590,
    "title": "LARGE SCALE GAN TRAINING FOR HIGH FIDELITY NATURAL IMAGE SYNTHESIS",
    "abstract": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by truncating the latent space. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128×128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.3 and Fréchet Inception Distance (FID) of 9.6, improving over the previous best IS of 52.52 and FID of 18.65.",
    "prev": "CLUE's latent space optimization mechanism allows it to cope well with high dimensional data.",
    "curr": "Future work can leverage recent advances in scalable BNNs [37] and generative modeling [38] to explore using CLUE for more complex data, such as natural images and natural language.",
    "next": "Broader Impact\n\nAs machine learning models are deployed in high-stakes scenarios, there has been a call for algorithmic transparency into models' behavior."
  },
  {
    "index": 925,
    "source_corpus_id": 249953817,
    "ref_id": "b7",
    "citation_corpus_id": 52967399,
    "start": 2663,
    "end": 2666,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Predicting visual observations for embodied agents is however challenging and computationally demanding: the model needs to capture the complexity and inherent stochasticity of future events while maintaining an inference speed that supports the robot's actions.",
    "curr": "Therefore, recent advances in autoregressive generative models, which leverage Transformers [7] for building neural architectures and learn good representations via self-supervised generative pretraining [8], have not benefited video prediction or robotic applications.",
    "next": "We in particular identify three technical challenges."
  },
  {
    "index": 927,
    "source_corpus_id": 20140417,
    "ref_id": "b0",
    "citation_corpus_id": 11383178,
    "start": 1933,
    "end": 1953,
    "title": "IMPORTANCE WEIGHTED AUTOENCODERS",
    "abstract": "The variational autoencoder (VAE; ) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.",
    "prev": "We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.",
    "curr": "INTRODUCTION\n\nWe build upon AESMC , a method for model learning that itself builds on variational auto-encoders (VAEs) (Kingma & Welling, 2014;Rezende et al., 2014) and importance weighted auto-encoders (IWAEs) (Burda et al., 2016).",
    "next": "AESMC is similarly based on maximizing a lower bound to the log marginal likelihood, but uses SMC (Doucet & Johansen, 2009) as the underlying marginal likelihood estimator instead of importance sampling (IS)."
  },
  {
    "index": 930,
    "source_corpus_id": 238856821,
    "ref_id": "b48",
    "citation_corpus_id": 227230646,
    "start": 2994,
    "end": 3011,
    "title": "Distill and Replay for Continual Language Learning",
    "abstract": "Accumulating knowledge to tackle new tasks without necessarily forgetting the old ones is a hallmark of human-like intelligence. But the current dominant paradigm of machine learning is still to train a model that works well on static datasets. When learning tasks in a stream where data distribution may fluctuate, fitting on new tasks often leads to forgetting on the previous ones. We propose a simple yet effective framework that continually learns natural language understanding tasks with one model. Our framework distills knowledge and replays experience from previous tasks when fitting on a new task, thus named DnR (distill and replay). The framework is based on language models and can be smoothly built with different language model architectures. Experimental results demonstrate that DnR outperfoms previous state-of-the-art models in continually learning tasks of the same type but from different domains, as well as tasks of radically different types. With the distillation method, we further show that it's possible for DnR to incrementally compress the model size while still outperforming most of the baselines. We hope that DnR could promote the empirical application of continual language learning, and contribute to building human-level language intelligence minimally bothered by catastrophic forgetting. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/.",
    "prev": "More recent methods attempt to learn from different types of tasks.",
    "curr": "These include LAMOL (Sun et al., 2019) and its improvements (Chuang et al., 2020;Sun et al., 2020;Kanwatchara et al., 2021).",
    "next": "Despite the effectiveness of these methods in LLL, there are several limitations."
  },
  {
    "index": 931,
    "source_corpus_id": 220042361,
    "ref_id": "b0",
    "citation_corpus_id": 202541346,
    "start": 4131,
    "end": 4152,
    "title": "Published as a conference paper at ICLR 2020 PREDICTION, CONSISTENCY, CURVATURE: REPRESEN- TATION LEARNING FOR LOCALLY-LINEAR CONTROL",
    "abstract": "Many real-world sequential decision-making problems can be formulated as optimal control with high-dimensional observations and unknown dynamics. A promising approach is to embed the high-dimensional observations into a lowerdimensional latent representation space, estimate the latent dynamics model, then utilize this model for control in the latent space. An important open question is how to learn a representation that is amenable to existing control algorithms? In this paper, we focus on learning representations for locally-linear control algorithms, such as iterative LQR (iLQR). By formulating and analyzing the representation learning problem from an optimal control perspective, we establish three underlying principles that the learned representation should comprise: 1) accurate prediction in the observation space, 2) consistency between latent and observation space dynamics, and 3) low curvature in the latent space transitions. These principles naturally correspond to a loss function that consists of three terms: prediction, consistency, and curvature (PCC). Crucially, to make PCC tractable, we derive an amortized variational bound for the PCC loss function. Extensive experiments on benchmark domains demonstrate that the new variational-PCC learning algorithm benefits from significantly more stable and reproducible training, and leads to superior control performance. Further ablation studies give support to the importance of all three PCC components for learning a good latent space for control.Published as a conference paper at ICLR 2020 the process of encoding, transitioning via the latent dynamics, and then decoding, to adhere to the true observation dynamics. The second is consistency: given the ability to encode a observation trajectory sampled from the true environment, we expect the latent dynamics to be consistent with the encoded trajectory. Finally, curvature: in order to learn a latent space that is specifically amenable to LLC algorithms, we expect the (learned) latent dynamics to exhibit low curvature in order to minimize the approximation error of its first-order Taylor expansion employed by LLC algorithms. Our contributions are thus as follows: (1) We propose the Prediction, Consistency, and Curvature (PCC) framework for learning a latent space that is amenable to LLC algorithms and show that the elements of PCC arise systematically from bounding the suboptimality of the solution of the LLC algorithm in the latent space. (2) We design a latent variable model that adheres to the PCC framework and derive a tractable variational bound for training the model.(3)To the best of our knowledge, our proposed curvature loss for the transition dynamics (in the latent space) is novel. We also propose a direct amortization of the Jacobian calculation in the curvature loss to help training with curvature loss more efficiently. (4) Through extensive experimental comparison, we show that the PCC model consistently outperforms E2C (Watter et al., 2015)  and RCE (Banijamali et al., 2018)   on a number of control-from-images tasks, and verify via ablation, the importance of regularizing the model to have consistency and low-curvature. . Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In Advances in Neural Information Processing Systems, pp. 4754-4765, 2018. Roy De Maesschalck, Delphine Jouan-Rimbaud, and Désiré L Massart. The mahalanobis distance. Chemometrics and intelligent laboratory systems, 50(1):1-18, 2000. Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efficient approach to policy search. In . Visual foresight: Model-based deep reinforcement learning for vision-based robotic control. arXiv preprint arXiv:1812.00568, 2018. Bernard Espiau, François Chaumette, and Patrick Rives. A new approach to visual servoing in robotics. , et al. Model-based reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019. 9 Published as a conference paper at ICLR 2020 Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. She. Singularity-avoiding swing-up control for underactuated three-link gymnast robot using virtual coupling between control torques. International Journal of Robust and Nonlinear Control, 25(2):207-221, 2015. Weiwei Li and Emanuel Todorov. Iterative linear quadratic regulator design for nonlinear biological movement systems. In ICINCO (1), pp. 222-229, 2004. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.Kevin P Murphy. Machine learning: a probabilistic perspective. MIT press, 2012.Erik Ordentlich and Marcelo J Weinberger. A distribution dependent refinement of pinsker's inequality.",
    "prev": "To address this issue, a class of algorithms have been developed that are based on learning a lowdimensional latent (embedding) space and a latent model (dynamics), and then using this model to control the system in the latent space.",
    "curr": "This class has been referred to as learning control-lable embedding (LCE) and includes recently developed algorithms, such as E2C [Watter et al., 2015], RCE [Banijamali et al., 2018], SOLAR [Zhang et al., 2019], PCC [Levine et al., 2020], Dreamer [Hafner et al., 2020], and PC3 .",
    "next": "The following two properties are extremely important in designing LCE models and algorithms."
  },
  {
    "index": 932,
    "source_corpus_id": 264590171,
    "ref_id": "b14",
    "citation_corpus_id": 246634193,
    "start": 1799,
    "end": 1818,
    "title": "Published as a conference paper at ICLR 2023 CORRUPTED IMAGE MODELING FOR SELF-SUPERVISED VISUAL PRE-TRAINING",
    "abstract": "We introduce Corrupted Image Modeling (CIM) for self-supervised visual pretraining. CIM uses an auxiliary generator with a small trainable BEiT (Bao et al.,  2021)  to corrupt the input image instead of using artificial [MASK] tokens, where some patches are randomly selected and replaced with plausible alternatives sampled from the BEiT output distribution. Given this corrupted image, an enhancer network learns to either recover all the original image pixels, or predict whether each visual token is replaced by a generator sample or not. The generator and the enhancer are simultaneously trained and synergistically updated. After pre-training, the enhancer can be used as a high-capacity visual encoder for downstream tasks. CIM is a general and flexible visual pre-training framework that is suitable for various network architectures. For the first time, CIM demonstrates that both ViT and CNN can learn rich visual representations using a unified, non-Siamese framework. Experimental results show that our approach achieves compelling results in vision benchmarks, such as ImageNet classification and ADE20K semantic segmentation.",
    "prev": " capture and learn structural information about objects and scenes.The intermediate feature representations obtained from MIM are suitable for fine-tuning on downstream tasks.In this paper, we propose an Image Modeling framework based on random orthogonal projection instead of binary masking as in MIM.Our proposed Random Orthogonal Projection Image Modeling (ROPIM) reduces spatially-wise token information under guaranteed bound on the noise variance and can be considered as masking entire spatial image area under locally varying masking degrees.Since ROPIM uses a random subspace for the projection that realizes the masking step, the readily available complement of the subspace can be used during unmasking to promote recovery of removed information.In this paper, we show that using random orthogonal projection leads to superior performance compared to crop-based masking.We demonstrate state-of-the-art results on several popular benchmarks.",
    "curr": "INTRODUCTION\n\nROPIM MAE (He et al., 2022) CIM (Fang et al., 2023) BEiT (Bao et al., 2022) MoCov3 (Chen et al., 2021b) MFM (Xie et.al., 2023) CAN (Mishra et al., 2022) GPL-MAE (Jiang et al., 2023) GPL-ROPIM Fast methods Best-performing Masked Image Modeling (MIM) (Bao et al., 2022;He et al., 2022;Xie et al., 2022) has achieved promising performance by pretraining backbones that are then fine-tuned on different downstream tasks such as image classification or semantic segmentation.",
    "next": "Most MIM techniques follow the general paradigm of self-prediction, i.e., they randomly mask out some regions in the input data and then learn to recover the missing data.Current MIM methods (Bao et al., 2022;He et al., 2022;Xie et al., 2022) mainly apply masking in the spatial domain by randomly excluding image patches.Since raw image pixels are highly correlated within their spatial neighbourhood, a high masking ratio (60%-75%) leads to high quality features (He et al., 2022;Xie et al., 2022)."
  },
  {
    "index": 936,
    "source_corpus_id": 252967732,
    "ref_id": "b9",
    "citation_corpus_id": 67855286,
    "start": 9462,
    "end": 9481,
    "title": "ANTISYMMETRICRNN: A DYNAMICAL SYSTEM VIEW ON RECURRENT NEURAL NETWORKS",
    "abstract": "Recurrent neural networks have gained widespread use in modeling sequential data. Learning long-term dependencies using these models remains difficult though, due to exploding or vanishing gradients. In this paper, we draw connections between recurrent networks and ordinary differential equations. A special form of recurrent networks called the AntisymmetricRNN is proposed under this theoretical framework, which is able to capture long-term dependencies thanks to the stability property of its underlying differential equation. Existing approaches to improving RNN trainability often incur significant computation overhead. In comparison, AntisymmetricRNN achieves the same goal by design. We showcase the advantage of this new architecture through extensive simulations and experiments. AntisymmetricRNN exhibits much more predictable dynamics. It outperforms regular LSTM models on tasks requiring long-term memory and matches the performance on tasks where short-term dependencies dominate despite being much simpler. advocate going beyond initialization and forcing the weight matrices to be orthogonal throughout the entire learning process. However, some of these approaches come with significant computational overhead and reportedly hinder representation power of these models(Vorontsov et al., 2017). Moreover, orthogonal weight matrices alone do not prevent exploding and vanishing gradients, due to the nonlinear nature of deep neural networks as shown in(Pennington et al., 2017).Here we offer a new perspective on the trainability of RNNs from the dynamical system viewpoint. While exploding gradient is a manifestation of the instability of the underlying dynamical system, vanishing gradient results from a lossy system, properties that have been widely studied in the dynamical system literature(Haber & Ruthotto, 2017;Laurent & von Brecht, 2017). The main contributions of the work are:• We draw connections between RNNs and the ordinary differential equation theory and design new recurrent architectures by discretizing ODEs.",
    "prev": "In this way, the time variable is discretized and the ODE solution is computed by the successive application of an iterated map that operates on the discrete set of points between 0 and T , with a step size > 0.",
    "curr": "Crucially, as already observed for feed-forward and recurrent neural models (Haber & Ruthotto, 2017;Chang et al., 2019), each step of the ODE discretization process can be equated to one layer of a DGN network.",
    "next": "The whole neural architecture contains as many layers as the integration steps in the numerical method (i.e., L = T / ), and each layer = 1, ..., L computes nodes' states x u which approximates x u ( )."
  },
  {
    "index": 937,
    "source_corpus_id": 250144675,
    "ref_id": "b2",
    "citation_corpus_id": 208637407,
    "start": 2711,
    "end": 2713,
    "title": "Published as a conference paper at ICLR 2020 AUGMIX: A SIMPLE DATA PROCESSING METHOD TO IMPROVE ROBUSTNESS AND UNCERTAINTY",
    "abstract": "Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classifiers. We propose AUGMIX, a data processing technique that is simple to implement, adds limited computational overhead, and helps models withstand unforeseen corruptions. AUGMIX significantly improves robustness and uncertainty measures on challenging image classification benchmarks, closing the gap between previous methods and the best possible performance in some cases by more than half.",
    "prev": "Introduction\n\nData augmentation is a crucial component of machine learning tasks as it prevents overfitting caused by a lack of training data and improves task performance without additional inference costs.",
    "curr": "Many data augmentation methods have been proposed across a broad range of research fields, including image recognition [1,2,3,4,5,6,7,8], image processing [9,10], language processing [11,12,13,14], and speech recognition [15,16].",
    "next": "In image recognition, each augmentation algorithm has become an essential component of the modern training recipe through various combinations [17,18,19,20]."
  },
  {
    "index": 941,
    "source_corpus_id": 257102638,
    "ref_id": "b21",
    "citation_corpus_id": 52901777,
    "start": 4009,
    "end": 4031,
    "title": "Bias-Reduced Uncertainty Estimation for Deep Neural Classifiers",
    "abstract": "We consider the problem of uncertainty estimation in the context of (non-Bayesian) deep neural classification. In this context, all known methods are based on extracting uncertainty signals from a trained network optimized to solve the classification problem at hand. We demonstrate that such techniques tend to introduce biased estimates for instances whose predictions are supposed to be highly confident. We argue that this deficiency is an artifact of the dynamics of training with SGD-like optimizers, and it has some properties similar to overfitting. Based on this observation, we develop an uncertainty estimation algorithm that selectively estimates the uncertainty of highly confident points, using earlier snapshots of the trained model, before their estimates are jittered (and way before they are ready for actual classification). We present extensive experiments indicating that the proposed algorithm provides uncertainty estimates that are consistently better than all known methods.",
    "prev": "A subset of ViT models is superior to all other models for all aspects of uncertainty estimation (\"ViT\" in the legend, marked as a red triangle facing upwards); the performance of EfficientNet-V2 and GENet models is worse.",
    "curr": "Among the various metrics proposed for evaluating the performance of uncertainty estimation are: Area Under the Receiver Operating Characteristic (AUROC or AUC), Area Under the Risk-Coverage curve (AURC) (Geifman et al., 2018), selective risk or coverage for a selective accuracy constraint (SAC), Negative Log-likelihood (NLL), Expected Calibration Error (ECE), which is often used for evaluating a model's calibration (see Section 2) and Brier score (Brier, 1950).",
    "next": "All these metrics are well known and are often used for comparing the uncertainty estimation performance of models (Moon et al., 2020;Nado et al., 2021;Maddox et al., 2019;Lakshminarayanan et al., 2017)."
  },
  {
    "index": 943,
    "source_corpus_id": 14911774,
    "ref_id": "b14",
    "citation_corpus_id": 6308361,
    "start": 25501,
    "end": 25526,
    "title": "ReferItGame: Referring to Objects in Photographs of Natural Scenes",
    "abstract": "In this paper we introduce a new game to crowd-source natural language referring expressions. By designing a two player game, we can both collect and verify referring expressions directly within the game. To date, the game has produced a dataset containing 130,525 expressions, referring to 96,654 distinct objects, in 19,894 photographs of natural scenes. This dataset is larger and more varied than previous REG datasets and allows us to study referring expressions in real-world scenes. We provide an in depth analysis of the resulting dataset. Based on our findings, we design a new optimization based model for generating referring expressions and perform experimental evaluations on 3 test sets.",
    "prev": "However, a very interesting effect of supervision is that it improves the interpretability of the code even when agents must communicate about images that do not contain objects in the supervised category set.",
    "curr": "This emerged in a follow-up experiment in which, during training, the sender was again exposed (with equal probability) to the same supervised classification task as above, but now the agents played the referential game on a different dataset of images derived from ReferItGame (Kazemzadeh et al., 2014).",
    "next": "In its general format, the ReferItGame contains annotations of bounding boxes in real images with referring expressions produced by humans when playing the game."
  },
  {
    "index": 944,
    "source_corpus_id": 228063969,
    "ref_id": "b11",
    "citation_corpus_id": 53477919,
    "start": 8724,
    "end": 8741,
    "title": "Optimal Control Via Neural Networks: A Convex Approach",
    "abstract": "Control of complex systems involves both system identification and controller design.Deep neural networks have proven to be successful in many identification tasks, such as classification, prediction, and end-to-end system modeling.However, from the controller design perspective, these networks are difficult to work with because they are typically nonlinear and nonconvex.Therefore many systems are still optimized and controlled based on simple linear models despite their poor identification performance.In this paper we address this problem by explicitly constructing deep neural networks that are convex with respect to their inputs.We show that these input convex networks can be trained to obtain accurate models of complex physical systems.In particular, we design input convex recurrent neural networks to capture temporal behavior of dynamical systems.Then optimal controllers based on these networks can be designed by solving convex optimization problems.Results on both toy models and real-world image denoising and building energy optimization problems demonstrate the modeling accuracy and control efficiency of the proposed approach.",
    "prev": "This is indeed quite different from the setting considered in (Pilanci & Ergen, 2020) and demands a different treatment.",
    "curr": "It could also be useful to mention that there are works in (Amos et al., 2017;Chen et al., 2019) that customize the network architecture for convex inference, but they still require non-convex training.",
    "next": "In recent years, deep learning has been widely deployed in inverse problems to either learn effective priors for iterative algorithms (Bora et al., 2017;Heckel & Hand, 2018), or to directly learn the inversion map using feed-forward networks (Jin et al., 2017;Zhang et al., 2017)."
  },
  {
    "index": 946,
    "source_corpus_id": 259075723,
    "ref_id": "b6",
    "citation_corpus_id": 222378211,
    "start": 4845,
    "end": 4847,
    "title": "R-GAP: RECURSIVE GRADIENT ATTACK ON PRIVACY A PREPRINT",
    "abstract": "Federated learning frameworks have been regarded as a promising approach to break the dilemma between demands on privacy and the promise of learning from large collections of distributed data. Many such frameworks only ask collaborators to share their local update of a common model, i.e. gradients with respect to locally stored data, instead of exposing their raw data to other collaborators. However, recent optimization-based gradient attacks show that raw data can often be accurately recovered from gradients. It has been shown that minimizing the Euclidean distance between true gradients and those calculated from estimated data is often effective in fully recovering private data. However, there is a fundamental lack of theoretical understanding of how and when gradients can lead to unique recovery of original data. Our research fills this gap by providing a closed-form recursive procedure to recover data from gradients in deep neural networks. We demonstrate that gradient attacks consist of recursively solving a sequence of systems of linear equations. Furthermore, our closed-form approach works as well as or even better than optimization-based approaches at a fraction of the computation, we name it Recursive Gradient Attack on Privacy (R-GAP). Additionally, we propose a rank analysis method, which can be used to estimate a network architecture's risk of a gradient attack. Experimental results demonstrate the validity of the closed-form attack and rank analysis, while demonstrating its superior computational properties and lack of susceptibility to local optima vis a vis optimization-based attacks. Source code is available for download from https://github.com/JunyiZhu-AI/R-GAP.",
    "prev": "In FL, a server trains a shared model by applying aggregated gradient updates, received from a large number of clients.",
    "curr": "Gradient leakage attacks A long line of work [5,6,7,8,9,10], primarily on images, has shown that even passive servers can reconstruct client data from gradients, breaking the key privacy promise of FL.",
    "next": "However, these attacks are only applicable to naive FL deployments [11]-in real-life settings with no unrealistic assumptions, they are limited to small batch sizes with no secure aggregation [12]."
  },
  {
    "index": 949,
    "source_corpus_id": 53216818,
    "ref_id": "b21",
    "citation_corpus_id": 997870,
    "start": 21913,
    "end": 21917,
    "title": "DIVIDE-AND-CONQUER REINFORCEMENT LEARNING",
    "abstract": "Standard model-free deep reinforcement learning (RL) algorithms sample a new initial state for each trial, allowing them to optimize policies that can perform well even in highly stochastic environments. However, problems that exhibit considerable initial state variation typically produce high-variance gradient estimates for model-free RL, making direct policy or value function optimization challenging. In this paper, we develop a novel algorithm that instead optimizes an ensemble of policies, each on a different \"slice\" of the initial state space, and gradually unifies them into a single policy that can succeed on the whole state space. This approach, which we term divide-and-conquer RL, is able to solve complex tasks where conventional deep RL methods are ineffective. Our results show that divide-and-conquer RL greatly outperforms conventional policy gradient methods on challenging grasping, manipulation, and locomotion tasks, and exceeds the performance of a variety of prior methods. Videos of policies learned by our algorithm can be viewed here.",
    "prev": "A large number of time steps are not needed to manipulate the object, and a strong signal about progress is readily received.",
    "curr": "However, since the targets can change rapidly, the variance in gradient estimates can be very high for function approximation methods [22].",
    "next": "Trajectory optimization is particularly well suited for such types of problems, since it can efficiently compute near-optimal actions conditioned on the instance, facilitating function approximation."
  },
  {
    "index": 951,
    "source_corpus_id": 238857090,
    "ref_id": "b21",
    "citation_corpus_id": 204960716,
    "start": 37833,
    "end": 37852,
    "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    "abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance. 1",
    "prev": "For direct comparison with existing methods that do not take direct advantage of this assumption, we trained a new Language Processing module that does not make use of templates but makes use of the subtasks sequences annotations ALFRED provides.",
    "curr": "5 Fine-tuning a pre-trained BART (Lewis et al., 2020) model, we directly learned a mapping from a high-level instruction to a sequence of subtasks (e.g.",
    "next": "\"Drop a clean pan on the table\" → \"(PickupObject, Pan), (PutObject, Sink), ...\")."
  },
  {
    "index": 953,
    "source_corpus_id": 219558836,
    "ref_id": "b4",
    "citation_corpus_id": 52967399,
    "start": 2234,
    "end": 2255,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Under review.",
    "curr": "Introduction\n\nPre-trained transformer-based masked language models such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and ALBERT (Lan et al., 2020) have had a dramatic impact on the NLP landscape in the recent year.",
    "next": "The standard recipe of using such models typically involves training a pre-trained model for few epochs on a supervised downstream dataset."
  },
  {
    "index": 961,
    "source_corpus_id": 256389917,
    "ref_id": "b14",
    "citation_corpus_id": 209977508,
    "start": 1969,
    "end": 1990,
    "title": "META-Q-LEARNING",
    "abstract": "This paper introduces Meta-Q-Learning (MQL), a new off-policy algorithm for meta-Reinforcement Learning (meta-RL). MQL builds upon three simple ideas. First, we show that Q-learning is competitive with state-of-the-art meta-RL algorithms if given access to a context variable that is a representation of the past trajectory. Second, a multi-task objective to maximize the average reward across the training tasks is an effective method to meta-train RL policies. Third, past data from the meta-training replay buffer can be recycled to adapt the policy on a new task using off-policy updates. MQL draws upon ideas in propensity estimation to do so and thereby amplifies the amount of available data for adaptation. Experiments on standard continuous-control benchmarks suggest that MQL compares favorably with the state of the art in meta-RL.",
    "prev": "re but also data that evolved in a similar fashion in the past.The time-varying propensity score is quite general: we demonstrate different ways of implementing it and evaluate it on a variety of problems ranging from supervised learning (e.g., image classification problems) where data undergoes a sequence of gradual shifts, to reinforcement learning tasks (e.g., robotic manipulation and continuous control) where data shifts as the policy or the task changes.",
    "curr": "INTRODUCTION\n\nMachine learning models are not expected to perform well when the test data is from a different distribution than the training data.There are many techniques to mitigate the consequent deterioration in performance (Heckman, 1979;Shimodaira, 2000;Huang et al., 2006;Bickel et al., 2007;Sugiyama et al., 2007b;2008;Gretton et al., 2008).These techniques use the propensity score between the train and test data distributions to reweigh the data-and they work well (Agarwal et al., 2011;Wen et al., 2014;Reddi et al., 2015b;Fakoor et al., 2020c;a;Tibshirani et al., 2019) when dealing with a single training and test dataset.But when machine learning models are deployed for real-world problems, they do not just undergo one distribution shift1 (from train to test), but instead suffer many successive distribution shifts (Lu et al., 2019) e.g., search queries to an online retailer or a movie recommendation service evolve as fashion and tastes of the population evolve, etc.Even for problems in healthcare, there are many situations where data drifts gradually, e.g., the different parts of the brain atrophy slowly as the brain ages for both healthy subjects and those with dementia; tracking these changes and distinguishing between them is very useful for staging the disease and deciding treatments (Saito et al., 2022;Wang et al., 2023).Viruses can mutate and drift over time, which can make them resistant to existing treatments (Russell, 2016;Callaway, 2020;Harvey et al., 2021).",
    "next": "To build a method that can account for changes in data,"
  },
  {
    "index": 962,
    "source_corpus_id": 199528271,
    "ref_id": "b9",
    "citation_corpus_id": 52967399,
    "start": 3151,
    "end": 3172,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "For example, when training typical Transformers based neural machine translation models on the De-En IWSLT'14 dataset, removing the warmup stage increases the training loss from 3 to around 10, as shown in Figure 1.",
    "curr": "Similar phenomena are observed in other scenarios like BERT (a bidirectional transformer language model) pre-training (Devlin et al., 2019).",
    "next": "Due to the lack of the theoretical underpinnings, there is neither guarantee that warmup would bring consistent improvements for various machine learning settings nor guidance on how we should In this paper, we conduct both empirical and theoretical analysis of the convergence issue to identify its origin."
  },
  {
    "index": 963,
    "source_corpus_id": 238353966,
    "ref_id": "b0",
    "citation_corpus_id": 49310753,
    "start": 36797,
    "end": 36823,
    "title": "Published as a conference paper at ICLR 2018 MAXIMUM A POSTERIORI POLICY OPTIMISATION",
    "abstract": "We introduce a new algorithm for reinforcement learning called Maximum aposteriori Policy Optimisation (MPO) based on coordinate ascent on a relativeentropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings. , et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.",
    "prev": "Batch normalization and its variant are introduced in deep deterministic policy gradient (DDPG) (Lillicrap et al., 2015) and twin delayed DDPG (Fujimoto et al., 2018) (Bhatt et al., 2020).",
    "curr": "Layer normalization is introduced in the implementation of maximum a posteriori policy optimisation(Abdolmaleki et al., 2018; Hoffman et al.,  2020).",
    "next": "It is also introduced in SAC extensions(Ma et al., 2020; Zhang et al., 2021)\n\nFigure 4 :Figure 5 :\n45Average return and average/standard deviation of estimation bias for DroQ with different dropout rates."
  },
  {
    "index": 967,
    "source_corpus_id": 263608898,
    "ref_id": "b19",
    "citation_corpus_id": 233296292,
    "start": 6518,
    "end": 6536,
    "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings",
    "abstract": "This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using \"entailment\" pairs as positives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2% and 2.2% improvement compared to previous best results. We also show-both theoretically and empirically-that contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available. 1",
    "prev": "(2020); Kumar et al.",
    "curr": "(2022) for CL.In this paper, we use SimCLR (Chen et al., 2020) and SimSiam (Chen & He, 2021) for CIFAR10 dataset and SimCSE (Gao et al., 2021) for BIASBIOS dataset to learn CL representations.",
    "next": "Several works have theoretically studied the success of self-supervised learning (Arora et al., 2019;HaoChen et al., 2021;Lee et al., 2020;Tian et al., 2021;Tosh et al., 2021).Our theoretical analysis of CL loss is partly motivated by Fang et al."
  },
  {
    "index": 968,
    "source_corpus_id": 259076379,
    "ref_id": "b4",
    "citation_corpus_id": 52967399,
    "start": 2249,
    "end": 2256,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Specifically, we provide an efficient GPU inference algorithm for SpQR which yields faster inference than 16-bit baselines at similar accuracy, while enabling memory compression gains of more than 4x.",
    "curr": "Introduction\n\nPretrained large language models (LLMs) improved rapidly from task-specific performance [WSM + 18, DCLT19,RWC + 19], to performing well on general tasks if prompted with instructions [BMR + 20,WBZ + 21,Ope23].",
    "next": "While the improved performance can be attributed to scaling in training data and parameters [KMH + 20,CND + 22] recent trends focused on smaller models trained on more data, that are easier to use at inference time [HBM + 22, BSA + 23, TLI + 23]."
  },
  {
    "index": 970,
    "source_corpus_id": 227161986,
    "ref_id": "b36",
    "citation_corpus_id": 210064473,
    "start": 18945,
    "end": 18963,
    "title": "SPACE: UNSUPERVISED OBJECT-ORIENTED SCENE REPRESENTATION VIA SPATIAL ATTENTION AND DECOMPOSITION",
    "abstract": "The ability to decompose complex multi-object scenes into meaningful abstractions like objects is fundamental to achieve higher-level cognition. Previous approaches for unsupervised object-oriented scene representation learning are either based on spatial-attention or scene-mixture approaches and limited in scalability which is a main obstacle towards modeling real-world scenes. In this paper, we propose a generative latent variable model, called SPACE, that provides a unified probabilistic modeling framework that combines the best of spatial-attention and scene-mixture approaches. SPACE can explicitly provide factorized object representations for foreground objects while also decomposing background segments of complex morphology. Previous models are good at either of these, but not both. SPACE also resolves the scalability problems of previous methods by incorporating parallel spatial-attention and thus is applicable to scenes with a large number of objects without performance degradations. We show through experiments on Atari and 3D-Rooms that SPACE achieves the above properties consistently in comparison to SPAIR, IODINE, and GENESIS. Results of our experiments can be found on our project website: https://sites.google.com/view/space-project-page * Visiting Student at Rutgers University. Authors named inside {} equally contributed. Correspondance to",
    "prev": "An alternative to inferring object keypoints (and corresponding representations) is to learn entire object representations.",
    "curr": "Recent approaches can be broadly categorized into spatial mixture models (Greff et al., 2017;, sequential attention models (Eslami et al., 2016;Kosiorek et al., 2018) or hybrid models that are combinations of both (Lin et al., 2020).",
    "next": "While these methods have shown promising results, they have yet to scale to more complex visual settings."
  },
  {
    "index": 972,
    "source_corpus_id": 208006294,
    "ref_id": "b2",
    "citation_corpus_id": 11212020,
    "start": 2671,
    "end": 2694,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": " and metrics evaluation on both LSTM models and BERT Transformer models on multiple datasets show that our algorithms outperform prior hierarchical explanation algorithms.Our algorithms apply to hierarchical visualization of compositional semantics, extraction of classification rules and improving human trust of models.",
    "curr": "Introduction\n\nRecent advances in deep neural networks have led to impressive results on a range of natural language processing (NLP) tasks, by learning latent, compositional vector representations of text data (Peters et al., 2018;Devlin et al., 2018;Liu et al., 2019b).However, interpretability of the predictions given by these complex, \"black box\" models has always been a limiting factor for use cases that require explanations of the features involved in modeling (e.g., words and phrases) (Guidotti et al., 2018;Ribeiro et al., 2016).Prior efforts on enhancing model interpretability have focused on either constructing models with intrinsically interpretable structures (Bahdanau et al., 2015;Liu et al., 2019a), or developing post-hoc explanation algorithms which can explain model predictions without elucidating the mechanisms by which model works (Mohseni et al., 2018;Guidotti et al., 2018).Among these work, post-hoc explanation has come to the fore as they can operate over a variety of trained models while not affecting predictive performance of models.",
    "next": "Towards post-hoc explanation, a major line of work, additive feature attribution methods (Lundberg and Lee, 2017;Ribeiro et al., 2016;Binder et al., 2016;Shrikumar et al., 2017), explain a model prediction by assigning importance scores to individual input variables.However, these methods may not work for explaining compositional semantics in natural language (e.g., phrases or clauses), as the importance of a phrase often is non-linear combination of the importance of the words in the phrase.Contextual decomposition (CD) (Murdoch et al., 2018) and its hierarchical extension (Singh et al., 2019) go beyond the additive assu"
  },
  {
    "index": 973,
    "source_corpus_id": 251252927,
    "ref_id": "b17",
    "citation_corpus_id": 3144218,
    "start": 2452,
    "end": 2456,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "A variety of applications characterized by this type of graph-structured data include works in the areas of social analysis [5], recommendation systems [56], computer vision [42], study of the properties of chemical compounds [26,48], statistical physics [8,10], and financial forensics [51,57].",
    "curr": "The most popular learning models for relational data use graph convolutions [33], where the idea is to aggregate the attributes of the set of neighbours of a node instead of only utilizing its own attributes.",
    "next": "Despite several empirical studies of various GCN-type models [13,38] that demonstrate that graph convolutions can improve the performance of traditional classification methods, such as a multi-layer perceptron (MLP), there has been limited progress in the theoretical understanding of the benefits of graph convolutions in multi-layer networks in terms of improving node classification tasks."
  },
  {
    "index": 974,
    "source_corpus_id": 3475375,
    "ref_id": "b0",
    "citation_corpus_id": 18828233,
    "start": 6604,
    "end": 6630,
    "title": "TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.",
    "prev": "(2012); Li et al.",
    "curr": "(2015) or the Wasserstein metric Arjovsky and Bottou (2017).",
    "next": "All of these divergence measures yield a minimax objective."
  },
  {
    "index": 976,
    "source_corpus_id": 256194054,
    "ref_id": "b52",
    "citation_corpus_id": 3162051,
    "start": 12877,
    "end": 12897,
    "title": "mixup: BEYOND EMPIRICAL RISK MINIMIZATION",
    "abstract": "Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks. * Alphabetical order.",
    "prev": "In this case, the optimization objective for the student network is a weighted combination of dual losses: a standard supervised cross-entropy loss and a \"distribution-matching\" objective that aims to mimic the teacher's output.",
    "curr": "However, in our case there are no class-labels for the patches generated from a single image, so we solely use the second objective formulated as a Kullback-Leibler (KL) divergence between the student output p s and the teacher's output p t :\nL KL = c ∈ C −p t c log p s c + p t c log p t c(1)\nwhere c are the teachers' classes and the outputs of both student and teacher are temperature τ flattened probabilities, p = softmax(l/τ ), that are generated from logits l.\n\nFor training, we follow (Beyer et al., 2022) in employing a function matching strategy, where the teacher and student models are fed consistently augmented instances, that include heavy augmentations, such as MixUp (Zhang et al., 2018) or CutMix .",
    "next": "However, in contrast to (Beyer et al., 2022), we neither have access to TPUs nor can train 10K epochs on ImageNet-sized datasets."
  },
  {
    "index": 977,
    "source_corpus_id": 249431433,
    "ref_id": "b13",
    "citation_corpus_id": 14124313,
    "start": 2098,
    "end": 2126,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "INTRODUCTION\n\nThe success of deep learning in computer vision is largely driven by Convolutional Neural Networks (CNNs).",
    "curr": "Starting from the milestone work AlexNet (Krizhevsky et al., 2012), CNNs keep pushing the frontier of computer vision (Simonyan & Zisserman, 2015;He et al., 2016;Tan & Le, 2019).",
    "next": "Interestingly, the recently emerged Vision Transformer (ViT) (Dosovitskiy et al., 2020) challenges the leading position of CNNs."
  },
  {
    "index": 978,
    "source_corpus_id": 261823404,
    "ref_id": "b14",
    "citation_corpus_id": 225039882,
    "start": 7252,
    "end": 7279,
    "title": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. 1 arXiv:2010.11929v2 [cs.CV] 3 Jun 2021Published as a conference paper at ICLR 2021 inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.However, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks.RELATED WORKTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global selfattention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.",
    "prev": "RELATED WORK\n\nIn this section, we introduce the recent related improvements in 3D generative models including GAN-based and diffusion-based methods, as well as transformer structure.",
    "curr": "TRANSFORMER\n\nIn recent years, Transformer (Vaswani et al., 2017) has seen rapid progress in many fields including image recognition (Dosovitskiy et al., 2020a;Touvron et al., 2021), object detection (Carion et al., 2020;Zhu et al., 2020), tracking (Cao et al., 2021;2022;2023), segmentation (Zheng et al., 2021;Strudel et al., 2021), and image generation (Van den Oord et al., 2016;Jiang et al., 2021;Mo et al., 2023).Some works (Chen et al., 2020;Child et al., 2019) prove the remarkable of transformer when predicting the pixels autoregressively.Based on the masked token, MaskGIT.",
    "next": "(Chang et al., 2022) achieve promising generation performance.DiT (Peebles & Xie, 2022) adopts the transformer as the backbone of diffusion models of images.Based on the 2D version, Mo et al."
  },
  {
    "index": 981,
    "source_corpus_id": 258887825,
    "ref_id": "b23",
    "citation_corpus_id": 3488815,
    "start": 1233,
    "end": 1237,
    "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
    "abstract": "Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. * Authors ordered alphabetically.",
    "prev": "Under review.",
    "curr": "Introduction\n\nMany noteworthy modern machine learning problems, such as generative adversarial networks (GANs) [13], adversarial training [24], and sharpness-aware minimization (SAM) [10], are instances of minimax problems, formulated as min x max y f (x, y).",
    "next": "First-order methods, such as gradient descent ascent (GDA) [1] and extragradient (EG) [20], are workhorses of minimax optimization in modern machine learning, but they still remain remarkably unreliable."
  },
  {
    "index": 982,
    "source_corpus_id": 263829263,
    "ref_id": "b10",
    "citation_corpus_id": 189928186,
    "start": 2333,
    "end": 2337,
    "title": "Theoretical Limitations of Self-Attention in Neural Sequence Models",
    "abstract": "Transformers are emerging as the new workhorse of NLP, showing great success across tasks. Unlike LSTMs, transformers process input sequences entirely through self-attention. Previous work has suggested that the computational capabilities of self-attention to process hierarchical structures are limited. In this work, we mathematically investigate the computational power of self-attention to model formal languages. Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of selfattention, finding that it cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length. These limitations seem surprising given the practical success of self-attention and the prominent role assigned to hierarchical structure in linguistics, suggesting that natural language can be approximated well with models that are too weak for the formal languages typically assumed in theoretical linguistics.156",
    "prev": "it complexity class TC 0 , i.e., AC 0 -circuits extended by majority gates.We first show a negative result that there is an AC 0 -language that cannot be recognized by an UHAT encoder.On the positive side, we show that UHAT encoders can recognize a rich fragment of AC 0 -languages, namely, all languages definable in first-order logic with arbitrary unary numerical predicates.This logic, includes, for example, all regular languages from AC 0 .We then show that AHAT encoders can recognize all languages of our logic even when we enrich it with counting terms.We apply these results to derive new results on the expressive power of UHAT and AHAT up to permutation of letters (a.k.a.Parikh images).",
    "curr": "Introduction\n\nTransformers have revolutionized natural language processing by facilitating the efficient and effective modeling of intricate contextual relationships within text [19].This remarkable capability has sparked numerous investigations into the potential boundaries of transformers' power [11,22,17,21,12,6,5,7].One natural method for addressing this question is to explore the classes of formal languages that these architectures can recognize.This approach provides an insight into their strengths and limitations.The response to this question naturally relies on the specific features allowed within transformer encoders.These encompass the interplay between encoders and decoders, the kind of functions used for positional encodings and attention mechanisms, and considerations of fixed or unbounded precision, among other factors.",
    "next": "While the capacity of transformers that incorporate both encoders and decoders to recognize languages is well understood today (indeed, such architectures are Turing-complete and can thus recognize any computable language [17]), the expressive power of transformer encoders has not been fully elucidated to date.Unique Hard Attention Transformers (UHAT) are a class of transformer encoders that has been a subject of many recent papers.As was shown by [12]"
  },
  {
    "index": 983,
    "source_corpus_id": 3568073,
    "ref_id": "b23",
    "citation_corpus_id": 6628106,
    "start": 28804,
    "end": 28823,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "The upsampling and downsampling operations in Table 2 correspond to 2 × 2 element replication and average pooling, respectively.",
    "curr": "We train the networks using Adam (Kingma & Ba, 2015) with α = 0.001, β 1 = 0, β 2 = 0.99, and = 10 −8 .",
    "next": "We do not use any learning rate decay or rampdown, but for visualizing generator output at any given point during the training, we use an exponential running average for the weights of the generator with decay 0.999."
  },
  {
    "index": 985,
    "source_corpus_id": 212414722,
    "ref_id": "b15",
    "citation_corpus_id": 210064473,
    "start": 56249,
    "end": 56267,
    "title": "SPACE: UNSUPERVISED OBJECT-ORIENTED SCENE REPRESENTATION VIA SPATIAL ATTENTION AND DECOMPOSITION",
    "abstract": "The ability to decompose complex multi-object scenes into meaningful abstractions like objects is fundamental to achieve higher-level cognition. Previous approaches for unsupervised object-oriented scene representation learning are either based on spatial-attention or scene-mixture approaches and limited in scalability which is a main obstacle towards modeling real-world scenes. In this paper, we propose a generative latent variable model, called SPACE, that provides a unified probabilistic modeling framework that combines the best of spatial-attention and scene-mixture approaches. SPACE can explicitly provide factorized object representations for foreground objects while also decomposing background segments of complex morphology. Previous models are good at either of these, but not both. SPACE also resolves the scalability problems of previous methods by incorporating parallel spatial-attention and thus is applicable to scenes with a large number of objects without performance degradations. We show through experiments on Atari and 3D-Rooms that SPACE achieves the above properties consistently in comparison to SPAIR, IODINE, and GENESIS. Results of our experiments can be found on our project website: https://sites.google.com/view/space-project-page * Visiting Student at Rutgers University. Authors named inside {} equally contributed. Correspondance to",
    "prev": "ng bilinear interpolation.This attention mechanism is motivated by the observation that only part of the image contains information for tracking an object and an inductive bias that objects cannot move a large distance within a short time span (i.e., objects do not teleport).",
    "curr": "Posterior Discovery.The posterior discovery also consists of proposal and rejection phases.The main difference is that we now compute the proposal in spatially-parallel manner by conditioning on the observations x ≤t , i.e., q(z D t |z P t , x ≤t ) = HW h,w=1 q(z D t,h,w |z P t , x ≤t ).Here, the observation x ≤t is encoded into the feature map of dimensionality H×W ×D using a Convolutional LSTM (Xingjian et al., 2015).Then, from each feature we obtain zD t,h,w .Importantly, this is done over all the feature cells h, w in parallel.A similar approach is used in SPAIR (Crawford & Pineau, 2019b), but it infers the object latent representations sequentially and thus is difficult to scale to a large number of objects (Lin et al., 2020).Even if this spatially-parallel proposal plays a key role in making our model scalable, we also observe another challenge due to this high capacity of the discovery module.The problem is that the discovery module tends to dominate the propagation module and thus most of the objects in an image are explained by the discovery module, i.e., objects are rediscovered at every time-step while nothing is propagated.We call this problem propagation collapse.",
    "next": "Why would the model tend to explain an image through discovery while suppressing propagation?First, the model does not care where-either from discovery or propagation-an object is sourced from as long as it can make an accurate reconstruction.Second, the propagation step performs a much harder task than the discovery.For the propagation to properly predict, tracker n needs to learn to find the matching object from an image containing many objects.Although the propagation attention plays an important role in balancing the discovery and pr"
  },
  {
    "index": 988,
    "source_corpus_id": 53831933,
    "ref_id": "b2",
    "citation_corpus_id": 3921978,
    "start": 2285,
    "end": 2305,
    "title": "Emergent Complexity via Multi-Agent Competition",
    "abstract": "Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment. This suggests that a highly capable agent requires a complex environment for training. In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself. We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty. This work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX.",
    "prev": "Yet, even though significant progress has been made thanks to better algorithms, training regimes, and computational infrastructure, the resulting behaviors still tend to exhibit significant idiosyncrasies (e.g.",
    "curr": "Bansal et al., 2018).",
    "next": "One advantage of working with humanoids in this context is that motion capture data is widely available and can serve to help design controllers that produce apparently humanlike movement."
  },
  {
    "index": 990,
    "source_corpus_id": 4862861,
    "ref_id": "b1",
    "citation_corpus_id": 9963298,
    "start": 5135,
    "end": 5162,
    "title": "Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions",
    "abstract": "The context in which language is used provides a strong signal for learning to recover its meaning. In this paper, we show it can be used within a grounded CCG semantic parsing approach that learns a joint model of meaning and context for interpreting and executing natural language instructions, using various types of weak supervision. The joint nature provides crucial benefits by allowing situated cues, such as the set of visible objects, to directly influence learning. It also enables algorithms that learn while executing instructions, for example by trying to replicate human actions. Experiments on a benchmark navigational dataset demonstrate strong performance under differing forms of supervision, including correctly executing 60% more instruction sets relative to the previous state of the art.",
    "prev": "Static datasets such as visual question answering (Antol et al., 2015) provide grounding into images, but no possibility for language learning through interaction.",
    "curr": "Some works utilize a geographical environment such as a maze but still employ static datasets (Artzi & Zettlemoyer, 2013).",
    "next": "It has been argued that virtual embodiment of agents is a viable long-term strategy for artificial intelligence research and the learning of natural language semantics, particularly in the form of games which also contain human players (Kiela et al., 2016)."
  },
  {
    "index": 994,
    "source_corpus_id": 238744039,
    "ref_id": "b32",
    "citation_corpus_id": 44090489,
    "start": 5646,
    "end": 5671,
    "title": "On the Impact of Various Types of Noise on Neural Machine Translation",
    "abstract": "We examine how various types of noise in the parallel training data impact the quality of neural machine translation systems. We create five types of artificial noise and analyze how they degrade performance in neural and statistical machine translation. We find that neural models are generally more harmed by noise than statistical models. For one especially egregious type of noise they learn to just copy the input sentence.",
    "prev": "Although being highly effective in low-resource translation, backtranslation also has issues, as the model is trained on potentially invalid input-output pairs.",
    "curr": "Neural machine translation models being highly sensitive to input noise (Belinkov & Bisk, 2018;Khayrallah & Koehn, 2018), this can severely deteriorate the performance.",
    "next": "Fortunately, many programming languages come with relatively mature tools and technologies for automated test data generation."
  },
  {
    "index": 997,
    "source_corpus_id": 219401642,
    "ref_id": "b7",
    "citation_corpus_id": 202749904,
    "start": 5510,
    "end": 5513,
    "title": "HIGH FIDELITY SPEECH SYNTHESIS WITH ADVERSARIAL NETWORKS",
    "abstract": "Generative adversarial networks have seen rapid development in recent years and have led to remarkable improvements in generative modelling of images. However, their application in the audio domain has received limited attention, and autoregressive models, such as WaveNet, remain the state of the art in generative modelling of audio signals such as human speech. To address this paucity, we introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech. Our architecture is composed of a conditional feed-forward generator producing raw speech audio, and an ensemble of discriminators which operate on random windows of different sizes. The discriminators analyse the audio both in terms of general realism, as well as how well the audio corresponds to the utterance that should be pronounced. To measure the performance of GAN-TTS, we employ both subjective human evaluation (MOS -Mean Opinion Score), as well as novel quantitative metrics (Fréchet DeepSpeech Distance and Kernel DeepSpeech Distance), which we find to be well correlated with MOS. We show that GAN-TTS is capable of generating high-fidelity speech with naturalness comparable to the state-of-the-art models, and unlike autoregressive models, it is highly parallelisable thanks to an efficient feed-forward generator. Listen to GAN-TTS reading this abstract at https://storage.googleapis.com/ deepmind-media/research/abstract.wav.",
    "prev": "It is illustrated in Figure 1.",
    "curr": "The generator is inspired by GAN-TTS [8], a text-to-speech generative adversarial network operating on aligned linguistic features.",
    "next": "We employ the GAN-TTS generator as the decoder in our model, but instead of upsampling pre-computed linguistic features, its input comes from the aligner block."
  },
  {
    "index": 998,
    "source_corpus_id": 259137503,
    "ref_id": "b17",
    "citation_corpus_id": 225039882,
    "start": 1627,
    "end": 1631,
    "title": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. 1 arXiv:2010.11929v2 [cs.CV] 3 Jun 2021Published as a conference paper at ICLR 2021 inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.However, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks.RELATED WORKTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global selfattention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.",
    "prev": "Code is available at https://github.com/NVlabs/FasterViT.",
    "curr": "Introduction\n\nVision Transformers (ViTs) [18] have recently become popular in computer vision and achieved superior performance in various applications such as image classification [38,17,35], object detection [77,21] and semantic segmentation [61,10].",
    "next": "In addition to learning more uniform local and global representations across their architecture when compared to Convolutional Neural Networks (CNNs), ViTs scale properly to large-scale data and model sizes [47,45]."
  },
  {
    "index": 1000,
    "source_corpus_id": 16299141,
    "ref_id": "b2",
    "citation_corpus_id": 11212020,
    "start": 17150,
    "end": 17173,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "Other work has improved language modeling performance by modifying the RNN architecture to better handle increased recurrence depth (Zilly et al., 2016).",
    "curr": "In order to increase capacity and minimize the impact of vanishing gradients, some language and translation mod- WikiText-2  WikiText-103  Train  Valid  Test  Train  Valid  Test  Train  Valid  Test   Articles  ---600  60  60  28,475  60  60  Tokens  929,590 73,761 82, els have also added a soft attention or memory component (Bahdanau et al., 2015;Sukhbaatar et al., 2015;Cheng et al., 2016;Kumar et al., 2016;Xiong et al., 2016;Ahn et al., 2016).",
    "next": "These mechanisms allow for the retrieval and use of relevant previous hidden states."
  },
  {
    "index": 1002,
    "source_corpus_id": 252595881,
    "ref_id": "b3",
    "citation_corpus_id": 46899514,
    "start": 2583,
    "end": 2610,
    "title": "Generative Code Modeling with Graphs",
    "abstract": "Generative models for source code are an interesting structured prediction problem, requiring to reason about both hard syntactic and semantic constraints as well as about natural, likely programs. We present a novel model for this problem that uses a graph to represent the intermediate state of the generated output. The generative procedure interleaves grammar-driven expansion steps with graph augmentation and neural message passing steps. An experimental evaluation shows that our new model can generate semantically meaningful expressions, outperforming a range of strong baselines.Preprint. Work in progress.",
    "prev": "Diffusion models have been used successfully in a variety of settings, outperforming all other methods on image and video (Dhariwal & Nichol, 2021;Ho et al., 2022).",
    "curr": "These successes raise hope for building powerful models for graph generation, a task with diverse applications such as molecule design (Liu et al., 2018), traffic modeling (Yu & Gu, 2019), and code completion (Brockschmidt et al., 2019).",
    "next": "However, generating graphs remains challenging due to their unordered nature and sparsity properties."
  },
  {
    "index": 1005,
    "source_corpus_id": 235446639,
    "ref_id": "b19",
    "citation_corpus_id": 62841605,
    "start": 8329,
    "end": 8355,
    "title": "SPREADING VECTORS FOR SIMILARITY SEARCH",
    "abstract": "Discretizing multi-dimensional data distributions is a fundamental step of modern indexing methods. State-of-the-art techniques learn parameters of quantizers on training data for optimal performance, thus adapting quantizers to the data. In this work, we propose to reverse this paradigm and adapt the data to the quantizer: we train a neural net which last layer forms a fixed parameter-free quantizer, such as pre-defined points of a hyper-sphere. As a proxy objective, we design and train a neural network that favors uniformity in the spherical latent space, while preserving the neighborhood structure after the mapping. We propose a new regularizer derived from the Kozachenko-Leonenko differential entropy estimator to enforce uniformity and combine it with a locality-aware triplet loss. Experiments show that our end-to-end approach outperforms most learned quantization methods, and is competitive with the state of the art on widely adopted benchmarks. Furthermore, we show that training without the quantization step results in almost no difference in accuracy, but yields a generic catalyzer that can be applied with any subsequent quantizer. The code is available online 1 .",
    "prev": "Learning-based algorithms Over the last few years, there has been a growing interest in using machine learning techniques to improve the performance of \"classical\" algorithms.",
    "curr": "This methodology found applications in similarity search (Wang et al., 2016;Sablayrolles et al., 2019;Dong et al., 2020), graph optimization (Khalil et al., 2017;Balcan et al., 2018), data structures (Kraska et al., 2018;Mitzenmacher, 2018), online algorithms (Lykouris & Vassilvitskii, 2018;Purohit et al., 2018), compressed sensing (Mousavi et al., 2015;Baldassarre et al., 2016;Bora et al., 2017) and streaming algorithms (Hsu et al., 2019;Jiang et al., 2019).",
    "next": "The last two papers are closest to our work, as they solve various computational problems over data streams, including distinct elements estimation in Jiang et al."
  },
  {
    "index": 1008,
    "source_corpus_id": 257636846,
    "ref_id": "b24",
    "citation_corpus_id": 229923720,
    "start": 31899,
    "end": 31918,
    "title": "Transformer Feed-Forward Layers Are Key-Value Memories",
    "abstract": "Feed-forward layers constitute two-thirds of a transformer model's parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformerbased language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys' input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model's layers via residual connections to produce the final output distribution.",
    "prev": "Being able to write SDM as an MLP with minor modifications is interesting in light of SDM's connection to Transformer Attention (Bricken & Pehlevan, 2021).",
    "curr": "This link converges with work showing that Transformer MLP layers perform associative memory-like operations that approximate Top-K by showing up to 90% activation sparsity in later layers (Geva et al., 2020;Sukhbaatar et al., 2019;Nelson et al., 2022).",
    "next": "Viewing both Attention and MLPs through the lens of SDM presents their tradeoffs: Attention operates on patterns in the model's current receptive field."
  },
  {
    "index": 1009,
    "source_corpus_id": 235829401,
    "ref_id": "b5",
    "citation_corpus_id": 168169888,
    "start": 14293,
    "end": 14311,
    "title": "Stay on the Path: Instruction Fidelity in Vision-and-Language Navigation",
    "abstract": "Advances in learning and representations have reinvigorated work that connects language to other modalities. A particularly exciting direction is Vision-and-Language Navigation (VLN), in which agents interpret natural language instructions and visual scenes to move through environments and reach goals. Despite recent progress, current research leaves unclear how much of a role language understanding plays in this task, especially because dominant evaluation metrics have focused on goal completion rather than the sequence of actions corresponding to the instructions. Here, we highlight shortcomings of current metrics for the Room-to-Room dataset(Anderson et al., 2018b)and propose a new metric, Coverage weighted by Length Score (CLS). We also show that the existing paths in the dataset are not ideal for evaluating instruction following because they are direct-to-goal shortest paths. We join existing short paths to form more challenging extended paths to create a new data set, Room-for-Room (R4R). Using R4R and CLS, we show that agents that receive rewards for instruction fidelity outperform agents that focus on goal completion.",
    "prev": "Again, CLIP-ViT-B variant leads to dramatically worse performance compared to other visual modules, that we will discuss in Section 5.",
    "curr": "Vision-and-Language Navigation\n\nVision-and-language navigation tests the agent's ability to take action according to human instructions, which recently gains popularity in embodied AI (Anderson et al., 2018b;Chen et al., 2019;Jain et al., 2019;Chen et al., 2019;Qi et al., 2020b;Krantz et al., 2020;Nguyen and Daumé III, 2019;.",
    "next": "Specifically, the agent is put at a location in the environment (Chang et al., 2017) and asked to reach a target by following the language instructions."
  },
  {
    "index": 1010,
    "source_corpus_id": 247518670,
    "ref_id": "b7",
    "citation_corpus_id": 2808403,
    "start": 3736,
    "end": 3761,
    "title": "Deep Learning for Physical Processes: Incorporating Prior Scientific Knowledge",
    "abstract": "We consider the use of Deep Learning methods for modeling complex phenomena like those occurring in natural physical processes. With the large amount of data gathered on these phenomena the data intensive paradigm could begin to challenge more traditional approaches elaborated over the years in fields like maths or physics. However, despite considerable successes in a variety of application domains, the machine learning field is not yet ready to handle the level of complexity required by such problems. Using an example application, namely Sea Surface Temperature Prediction, we show how general background knowledge gained from physics could be used as a guideline for designing efficient Deep Learning models. In order to motivate the approach and to assess its generality we demonstrate a formal link between the solution of a class of differential equations underlying a large family of physical phenomena and the proposed model. Experiments and comparison with series of baselines including a state of the art numerical approach is then provided. * equal contribution .",
    "prev": "First, we can encode a known PDE into a loss function that encourages the model to fulfill the equation (Raissi et al., 2019).",
    "curr": "Another way to go about this is to derive the model structure itself from known laws such as the convection-diffusion equation (de Bézenac et al., 2018).",
    "next": "We will follow the second approach."
  },
  {
    "index": 1011,
    "source_corpus_id": 253264964,
    "ref_id": "b28",
    "citation_corpus_id": 1428702,
    "start": 21021,
    "end": 21040,
    "title": "Learning Word Vectors for Sentiment Analysis",
    "abstract": "Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term-document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.",
    "prev": "COMPARISON WITH BASELINES ON DIFFERENT BENCHMARKS\n\nSettings.",
    "curr": "In this section, we evaluate our MPCFormer framework with different approximations and compare it with baselines on the IMDb dataset and the GLUE benchmark (Maas et al., 2011;Wang et al., 2018).",
    "next": "For all experiments in this section, we use BERT BASE as the base model."
  },
  {
    "index": 1012,
    "source_corpus_id": 21731691,
    "ref_id": "b9",
    "citation_corpus_id": 2134321,
    "start": 5640,
    "end": 5643,
    "title": "Published as a conference paper at ICLR 2016 DEEP COMPRESSION: COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING",
    "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce \"deep compression\", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35× to 49× without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9× to 13×; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35×, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49× from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3× to 4× layerwise speedup and 3× to 7× better energy efficiency.",
    "prev": "In the deep learning literature, low-rank factorization of convolutional filters has been previously used to remove redundancy in trained CNNs [6,17].",
    "curr": "The compression of deep networks has also been studied in [1,10,11], SqueezeNet [14], etc., where the low-rank factorization of filters can be utilized.",
    "next": "MobileNets [13] used depth-wise separable convolutions to obtain significant compression."
  },
  {
    "index": 1013,
    "source_corpus_id": 254823652,
    "ref_id": "b48",
    "citation_corpus_id": 233306870,
    "start": 4363,
    "end": 4380,
    "title": "Gradient Matching for Domain Generalization",
    "abstract": "Machine learning systems typically assume that the distributions of training and test sets match closely. However, a critical requirement of such systems in the real world is their ability to generalize to unseen domains. Here, we propose an inter-domain gradient matching objective that targets domain generalization by maximizing the inner product between gradients from different domains. Since direct optimization of the gradient inner product can be computationally prohibitive -it requires computation of second-order derivatives --we derive a simpler first-order algorithm named Fish that approximates its optimization. We perform experiments on the WILDS benchmark, which captures distribution shift in the real world, as well as the DOMAINBED benchmark that focuses more on syntheticto-real transfer. Our method produces competitive results on both benchmarks, demonstrating its effectiveness across a wide range of domain generalization tasks.",
    "prev": "With increasing attention on this issue, researchers have been probing the generalisation performance of ML models by creating datasets that feature distribution shift tasks (Koh et al., 2021;Gulrajani and Lopez-Paz, 2020;Shah et al., 2020) and proposing algorithms that aim to improve generalisation performance under distribution shift (Ganin et al., 2016;Arjovsky et al., 2019;Sun and Saenko, 2016; Figure 1: Synthetic vs. realistic distribution shift: The distribution shift in synthetic datasets (left, MNIST-CIFAR and CdSprites) are usually extreme and controllable (adjusted via changing the correlation); for realistic datasets (right, WILDS-Camelyon17 and FMoW) distribution shift can be subtle, hard to identify and impossible to control.",
    "curr": "Sagawa et al., 2020;Shi et al., 2022).",
    "next": "In this work, we identify three specific problems with current approaches in distribution shift problems, in computer vision, and develop a suite of experiments to address them."
  },
  {
    "index": 1016,
    "source_corpus_id": 246822636,
    "ref_id": "b23",
    "citation_corpus_id": 8394195,
    "start": 1895,
    "end": 1914,
    "title": "LOSSY IMAGE COMPRESSION WITH COMPRESSIVE AUTOENCODERS",
    "abstract": "We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more flexible than existing codecs. Autoencoders have the potential to address this need, but are difficult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufficient to train deep autoencoders competitive with JPEG 2000 and outperforming recently proposed approaches based on RNNs. Our network is furthermore computationally efficient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.",
    "prev": "With the development of deep learning, learned methods have led to several breakthroughs in this task.",
    "curr": "Currently, the state-of-the-art (SOTA) deep image compression models are built on the auto-encoder framework [Hinton and Salakhutdinov, 2006] with an entropy-constrained bottleneck [Theis et al., 2017, Ballé et al., 2017, Ballé et al., 2018, Mentzer et al., 2018, Minnen et al., 2018a, Lee et al., 2019, Guo et al., 2021].",
    "next": "An entropy model estimates the conditional probability distribution of latents for compression by standard entropy coding algorithms."
  },
  {
    "index": 1017,
    "source_corpus_id": 246652106,
    "ref_id": "b16",
    "citation_corpus_id": 3144218,
    "start": 3286,
    "end": 3308,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "As shown in Figure 1 (b), a noise edge means the connection between two faces of different classes.",
    "curr": "Unlike common graph datasets such as Citeseer, Cora and Pubmed with explicit link relation as edges (Kipf & Welling, 2017), face images do not contain explicit structural information, but only deep features extracted from a trained CNN model.",
    "next": "Therefore, face images are treated as vertices, and the edges between face images are usually constructed based on the kNN (Cover & Hart, 1967) relations when building the graph: Each face serves as a probe to retrieve its k nearest neighbours by deep features (Wang et al., 2019b;Guo et al., 2020;Shen et al., 2021)."
  },
  {
    "index": 1018,
    "source_corpus_id": 252596252,
    "ref_id": "b33",
    "citation_corpus_id": 227209335,
    "start": 1587,
    "end": 1607,
    "title": "SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS",
    "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reversetime SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in diffusion probabilistic modeling and scorebased generative modeling, and allows for new sampling procedures. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, which enables exact likelihood computation, and improved sampling efficiency. In addition, our framework enables conditional generation with an unconditional model, as we demonstrate with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high fidelity generation of 1024ˆ1024 images for the first time from a score-based generative model.",
    "prev": "INTRODUCTION\n\nDiffusion models learn the implicit prior of the underlying data distribution by matching the gradient of the log density (i.e.",
    "curr": "Stein score; ∇ x log p(x)) (Song et al., 2021b).",
    "next": "The prior can be leveraged when solving inverse problems, which aim to recover x from the measurement y, related through the forward measurement operator A and the detector noise n. When we know such forward models, one can incorporate the gradient of the log likelihood (i.e."
  },
  {
    "index": 1019,
    "source_corpus_id": 198986015,
    "ref_id": "b33",
    "citation_corpus_id": 5273326,
    "start": 19465,
    "end": 19469,
    "title": "Published as a conference paper at ICLR 2016 FAST AND ACCURATE DEEP NETWORK LEARNING BY EXPONENTIAL LINEAR UNITS (ELUS)",
    "abstract": "We introduce the \"exponential linear unit\" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PRe-LUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10% classification error for a single crop, single model network.",
    "prev": "The conditional distribution p θ (z c k | z m k ) is parameterised by a multilayer perceptron (MLP) with two hidden layers, 256 units per layer, and ELUs (Clevert et al.",
    "curr": "[34]).",
    "next": "We use the same component VAE featuring a spatial broadcast decoder as MONet to encode and decode z c k , but we replace ReLUs (Glorot et al."
  },
  {
    "index": 1022,
    "source_corpus_id": 239049848,
    "ref_id": "b43",
    "citation_corpus_id": 3626819,
    "start": 67576,
    "end": 67596,
    "title": "Deep contextualized word representations",
    "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
    "prev": "Small weights final layer.Following Andrychowicz et al.",
    "curr": "(2020), we consider using smaller weights for the final layers of the actor and the critic.Specifically, we downscale these weights by a factor of 100 at initialization time.We refer to this strategy as scale down.Gradient clipping.Another stabilizing strategy is gradient clipping (Zhang et al., 2020).To avoid occasional exploding gradients, one simply clips the norm of gradients that are above some threshold.This strategy is popular in NLP (Gehring et al., 2017;Peters et al., 2018) and is sometimes used in RL (Raffin et al., 2019).We consider clipping the gradient when the norm, calculated independently for the actor, critic, and convolutional encoder, is larger than 1 or 10.We refer to these strategies as grad clip 1 and grad clip 10.",
    "next": "Results."
  },
  {
    "index": 1023,
    "source_corpus_id": 263831010,
    "ref_id": "b31",
    "citation_corpus_id": 3144218,
    "start": 24639,
    "end": 24661,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "( 2022) is included in Graphium to reduce the cost of hyper-parameter tuning.More details are given in Appendix E.7.The library supports CPU, GPU and IPU [ §E.8] hardware to accelerate training.Further library optimisations are detailed in Appendix E.9.",
    "curr": "EXPERIMENTS ON BASELINE MODELS\n\nTo demonstrate the capabilities of the Graphium library in a multi-task setting with thousands of labels, a set of standard baselines were run with simple hyperparameter sweeps using 3 popular GNNs, namely GCN (Kipf & Welling, 2017), GIN (Xu et al., 2019), and GINE (Hu et al., 2020a).",
    "next": "A basic hyper-parameter sweep was conducted for each model and multiple random seeds used for initialisation to provide a performance baseline for future experiments to be evaluated against."
  },
  {
    "index": 1024,
    "source_corpus_id": 3609219,
    "ref_id": "b12",
    "citation_corpus_id": 11243593,
    "start": 7843,
    "end": 7864,
    "title": "TRACKING THE WORLD STATE WITH RECURRENT ENTITY NETWORKS",
    "abstract": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped with a dynamic long-term memory which allows it to maintain and update a representation of the state of the world as it receives new data. For language understanding tasks, it can reason on-the-fly as it reads text, not just when it is required to answer a question or respond as is the case for a Memory Network(Sukhbaatar et al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer(Graves et al., 2014;2016)it maintains a fixed size memory and can learn to perform location and content-based read and write operations. However, unlike those models it has a simple parallel architecture in which several memory locations can be updated simultaneously. The EntNet sets a new state-of-the-art on the bAbI tasks, and is the first method to solve all the tasks in the 10k training examples setting. We also demonstrate that it can solve a reasoning task which requires a large number of supporting facts, which other methods are not able to solve, and can generalize past its training horizon. It can also be practically used on large scale datasets such as Children's Book Test, where it obtains competitive performance, reading the story in a single pass.",
    "prev": "AMN does not use strong supervision but can solve tasks that require transitive logic by modeling sentence walks on the fly.",
    "curr": "EntNet constructs dynamic networks based on entities with tied weights for each entity (Henaff et al., 2017).",
    "next": "A key-value update system allows it to update relevant (learned) entities."
  },
  {
    "index": 1026,
    "source_corpus_id": 259095535,
    "ref_id": "b14",
    "citation_corpus_id": 174799399,
    "start": 51070,
    "end": 51089,
    "title": "Learning Deep Transformer Models for Machine Translation",
    "abstract": "Transformer is the state-of-the-art model in recent machine translation evaluations. Two strands of research are promising to improve models of this kind: the first uses wide networks (a.k.a. Transformer-Big) and has been the de facto standard for the development of the Transformer system, and the other uses deeper language representation but faces the difficulty arising from learning deep networks. Here, we continue the line of research on the latter. We claim that a truly deep Transformer model can surpass the Transformer-Big counterpart by 1) proper use of layer normalization and 2) a novel way of passing the combination of previous layers to the next. On WMT'16 English-German, NIST OpenMT'12 Chinese-English and larger WMT'18 Chinese-English tasks, our deep system (30/25-layer encoder) outperforms the shallow Transformer-Big/Base baseline (6-layer encoder) by 0.4∼2.4 BLEU points. As another bonus, the deep model is 1.6X smaller in size and 3X faster in training than Transformer-Big 1 . * Corresponding author. 1  The source code is available at https://github. com/wangqiangneu/dlcl",
    "prev": "• Affine maps W len , W num : R → Rd and W node : R 2 → Rd.",
    "curr": "• A simple stack of five Transformer blocks with eight heads in the self-attention and layer normalization before the MHA and the FF (Wang et al., 2019).",
    "next": "The structure of a Transformer block is summarized in Figure 4."
  },
  {
    "index": 1027,
    "source_corpus_id": 253224188,
    "ref_id": "b0",
    "citation_corpus_id": 182953134,
    "start": 4058,
    "end": 4076,
    "title": "Published as a conference paper at ICLR 2020 DEEP BATCH ACTIVE LEARNING BY DIVERSE, UNCERTAIN GRADIENT LOWER BOUNDS",
    "abstract": "We design a new algorithm for batch active learning with deep neural network models. Our algorithm, Batch Active learning by Diverse Gradient Embeddings (BADGE), samples groups of points that are disparate and high magnitude when represented in a hallucinated gradient space, a strategy designed to incorporate both predictive uncertainty and sample diversity into every selected batch. Crucially, BADGE trades off between uncertainty and diversity without requiring any hand-tuned hyperparameters. While other approaches sometimes succeed for particular batch sizes or architectures, BADGE consistently performs as well or better, making it a useful option for real world active learning problems.Published as a conference paper at ICLR 2020 example, might perform poorly when using a multilayer perceptron. A diversity-based approach might work well when the batch size is very large, but poorly when the batch size is small. Further, what even constitutes a \"large\" or \"small\" batch size is largely a function of the statistical properties of the data in question. These weaknesses pose a major problem for real, practical batch active learning situations, where data are unfamiliar and potentially unstructured. There is no way to know which active learning algorithm is best to use.Moreover, in a real active learning scenario, every change of hyperparameters typically causes the algorithm to label examples not chosen under other hyperparameters, provoking substantial labeling inefficiency. That is, hyperparameter sweeps in active learning can be label expensive. As a result, active learning algorithms need to \"just work\", given fixed hyperparameters, to a greater extent than is typical for supervised learning.Based on these observations, we design an approach which creates diverse batches of examples about which the current model is uncertain. We measure uncertainty as the gradient magnitude with respect to parameters in the final (output) layer, which is computed using the most likely label according to the model. To capture diversity, we collect a batch of examples where these gradients span a diverse set of directions. More specifically, we build up the batch of query points based on these hallucinated gradients using the k-MEANS++ initialization (Arthur and Vassilvitskii, 2007), which simultaneously captures both the magnitude of a candidate gradient and its distance from previously included points in the batch. We name the resulting approach Batch Active learning by Diverse Gradient Embeddings (BADGE).We show that BADGE is robust to architecture choice, batch size, and dataset, generally performing as well as or better than the best baseline across our experiments, which vary all of the aforementioned environmental conditions. We begin by introducing our notation and setting, followed by a description of the BADGE algorithm in Section 3 and experiments in Section 4. We defer our discussion of related work to Section 5.NOTATION AND SETTINGDefine [K] := {1, 2, . . . , K}. Denote by X the instance space and by Y the label space. In this work we consider multiclass classification, so Y = [K]. Denote by D the distribution from which examples are drawn, by D X the unlabeled data distribution, and by D Y|X the conditional distribution over labels given examples. We consider the pool-based active learning setup, where the learner receives an unlabeled dataset U sampled according to D X and can request labels sampled according to D Y|X for any x ∈ U . We use E D to denote expectation under the data distribution D. Given a classifier h : X → Y, which maps examples to labels, and a labeled example (x, y), we denote the 0/1 error of h on (x, y) as 01 (h(x), y) = I(h(x) = y). The performance of a classifier h is measured by its expected 0/1 error, i.e. E D [ 01 (h(x), y)] = Pr (x,y)∼D (h(x) = y). The goal of pool-based active learning is to find a classifier with a small expected 0/1 error using as few label queries as possible. Given a set S of labeled examples (x, y), where each x ∈ S is picked from U , followed by a label query, we use E S as the sample averages over S.In this paper, we consider classifiers h parameterized by underlying neural networks f of fixed architecture, with the weights in the network denoted by θ. We abbreviate the classifier with parameters θ as h θ since the architectures are fixed in any given context, and our classifiers take the form h θ (x) = argmax y∈ [K]  f (x; θ) y , where f (x; θ) ∈ R K is a probability vector of scores assigned to candidate labels, given the example x and parameters θ. We optimize the parameters by minimizing the cross-entropy loss E S [ CE (f (x; θ), y)] over the labeled examples, where CE (p, y) = K i=1 I(y = i) ln 1 /pi = ln 1 /py.",
    "prev": "With the proposed metric, we show that coresets selected by SOTA methods at high pruning rates have much worse data coverage than random pruning, suggesting a linkage between poor data coverage of SOTA methods and poor accuracy at high pruning rates.",
    "curr": "We note that data coverage has also been studied in active learning setting (Ash et al., 2019;Citovsky et al., 2021), but techniques from active learning do not trivially extend to one-shot coreset selection.",
    "next": "We discuss the similarity and differences in Section 5."
  },
  {
    "index": 1030,
    "source_corpus_id": 252595765,
    "ref_id": "b0",
    "citation_corpus_id": 30535508,
    "start": 11529,
    "end": 11550,
    "title": "LEARNING SPARSE NEURAL NETWORKS THROUGH L 0 REGULARIZATION",
    "abstract": "We propose a practical method for L 0 norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of L 0 regularization. However, since the L 0 norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected L 0 norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the hard concrete distribution for the gates, which is obtained by \"stretching\" a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.",
    "prev": "It has also be applied for feature selection (Zhao et al., 2015;Li et al., 2016;Scardapane et al., 2017;Lemhadri et al., 2021).",
    "curr": "While the LASSO is the most widely-used method for relaxing the ℓ 0 sparsity constraint in feature selection, several recent works have proposed new relaxations based on stochastic gates (Srinivas et al., 2017;Louizos et al., 2018;Balın et al., 2019;Trelin & Procházka, 2020;Yamada et al., 2020).",
    "next": "This approach introduces (learnable) Bernoulli random variables for each feature during training, and minimizes the expected loss over realizations of the 0-1 variables (accepting or rejecting features)."
  },
  {
    "index": 1031,
    "source_corpus_id": 58006571,
    "ref_id": "b40",
    "citation_corpus_id": 21946795,
    "start": 4326,
    "end": 4346,
    "title": "ENSEMBLE ADVERSARIAL TRAINING: ATTACKS AND DEFENSES",
    "abstract": "Adversarial examples are perturbed inputs designed to fool machine learning models. Adversarial training injects such examples into training data to increase robustness. To scale this technique to large datasets, perturbations are crafted using fast single-step methods that maximize a linear approximation of the model's loss. We show that this form of adversarial training converges to a degenerate global minimum, wherein small curvature artifacts near the data points obfuscate a linear approximation of the loss. The model thus learns to generate weak perturbations, rather than defend against strong ones. As a result, we find that adversarial training remains vulnerable to black-box attacks, where we transfer perturbations computed on undefended models, as well as to a powerful novel single-step attack that escapes the non-smooth vicinity of the input data via a small random step. We further introduce Ensemble Adversarial Training, a technique that augments training data with perturbations transferred from other models. On ImageNet, Ensemble Adversarial Training yields models with stronger robustness to blackbox attacks. In particular, our most robust model won the first round of the NIPS 2017 competition on Defenses against Adversarial Attacks (Kurakin et al., 2017c). However, subsequent work found that more elaborate black-box attacks could significantly enhance transferability and reduce the accuracy of our models.",
    "prev": "For datasets that are relatively simple and have low intrinsic dimensions (MNIST, Fashion MNIST, etc), we can obtain enough training examples to make sure adversarial training covers most part of the data distribution.",
    "curr": "For high dimensional datasets (CIFAR, ImageNet), adversarial training have been shown difficult (Kurakin et al., 2016;Tramèr et al., 2018) and only limited success was obtained.",
    "next": "A recent attack proposed by  shows that adversarial training can be defeated when the input image is produced by a generative model (for example, a generative adversarial network) rather than selected directly from the test examples."
  },
  {
    "index": 1032,
    "source_corpus_id": 3463636,
    "ref_id": "b5",
    "citation_corpus_id": 6706414,
    "start": 2117,
    "end": 2141,
    "title": "Published as a conference paper at ICLR 2015 EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES",
    "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples-inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
    "prev": "* indicates equal contributions\n\nINTRODUCTION\n\nDeep neural networks (DNNs) have demonstrated their outstanding performance in different domains, ranging from image processing (Krizhevsky et al., 2012;He et al., 2016), text analysis (Collobert & Weston, 2008) to speech recognition .",
    "curr": "Though deep networks have exhibited high performance for these tasks, recently they have been shown to be particularly vulnerable to adversarial perturbations added to the input images (Szegedy et al., 2013;Goodfellow et al., 2015).",
    "next": "These perturbed instances are called adversarial examples, which can lead to undesirable consequences in many practical applications based on DNNs."
  },
  {
    "index": 1033,
    "source_corpus_id": 254198961,
    "ref_id": "b5",
    "citation_corpus_id": 53115163,
    "start": 9077,
    "end": 9096,
    "title": "EXPLORATION BY RANDOM NETWORK DISTILLATION",
    "abstract": "We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.",
    "prev": "This is motivated by applications in robotics and Embodied Intelligence where the state is not directly observable, but can be well-approximated through the combination: s = (x, q), where x denotes stacked RGB images observed by the agent's camera, and q denotes proprioceptive sensory information, e.g., the joint pose of a robot.",
    "curr": "Furthermore, shaped reward functions can be hard to script for real-world applications (Singh et al., 2019) or result in undesirable artifacts or behaviors (Amodei et al., 2016;Burda et al., 2019).",
    "next": "Thus, we desire to learn with simple sparse rewards that accurately capture task completion."
  },
  {
    "index": 1035,
    "source_corpus_id": 67855499,
    "ref_id": "b11",
    "citation_corpus_id": 3529936,
    "start": 2439,
    "end": 2458,
    "title": "Gradient Estimators for Implicit Models",
    "abstract": "Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the Stein gradient estimator, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference, and entropy regularised GANs that provide improved sample diversities.",
    "prev": "Traditional VI methods approximate the true posterior with oversimplified distribution families like factorized Gaussians, which can severely limit the approximation quality and induce pathologies such as over-pruning (Trippe & Turner, 2018).",
    "curr": "These limitations have motivated the recent development of implicit VI methods (Li & Turner, 2018;Shi et al., 2018b), which allow the use of flexible approximate distributions without a tractable density.",
    "next": "However, most of the implicit inference methods require to learn a \"generator network\" that maps a simple distribution to approximate the target posterior."
  },
  {
    "index": 1037,
    "source_corpus_id": 257219472,
    "ref_id": "b10",
    "citation_corpus_id": 52967399,
    "start": 2257,
    "end": 2278,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "When applied to various LLMs, including GPT-3, our approach significantly improves the generated discourse compared to the base LLMs and other techniques in terms of both the overall language and detoxification performance.",
    "curr": "INTRODUCTION\n\nLarge-scale Transformer-based (Vaswani et al., 2017) language models (LMs) have shown tremendous progress and grown in importance across various NLP downstream tasks, often providing stateof-the-art performances over the last few years (Devlin et al., 2019;Yang et al., 2019;Raffel et al., 2020;Peters et al., 2018).",
    "next": "Despite their progress in learning linguistic knowledge, these models have been shown to capture and reproduce toxicity in the ever-larger pretraining datasets."
  },
  {
    "index": 1038,
    "source_corpus_id": 258180544,
    "ref_id": "b9",
    "citation_corpus_id": 49659296,
    "start": 26679,
    "end": 26707,
    "title": "OPTIMISTIC MIRROR DESCENT IN SADDLE-POINT PROBLEMS: GOING THE EXTRA (GRADIENT) MILE",
    "abstract": "A. Owing to their connection with generative adversarial networks (GANs), saddle-point problems have recently attracted considerable interest in machine learning and beyond. By necessity, most theoretical guarantees revolve around convex-concave (or even linear) problems; however, making theoretical inroads towards e cient GAN training depends crucially on moving beyond this classic framework. To make piecemeal progress along these lines, we analyze the behavior of mirror descent (MD) in a class of non-monotone problems whose solutions coincide with those of a naturally associated variational inequality -a property which we call coherence. We rst show that ordinary, \"vanilla\" MD converges under a strict version of this condition, but not otherwise; in particular, it may fail to converge even in bilinear models with a unique solution. We then show that this de ciency is mitigated by optimism: by taking an \"extra-gradient\" step, optimistic mirror descent (OMD) converges in all coherent problems. Our analysis generalizes and extends the results of Daskalakis et al. (   ) for optimistic gradient descent (OGD) in bilinear problems, and makes concrete headway for provable convergence beyond convex-concave games. We also provide stochastic analogues of these results, and we validate our analysis by numerical experiments in a wide array of GAN models (including Gaussian mixture models, and the CelebA and CIFAR-datasets).",
    "prev": "An important remark on the properties of our presented game is due.",
    "curr": "Existing literature tackles settings with (i) (weak-)monotonocity (Mertikopoulos et al., 2019;Diakonikolas et al., 2021), (ii) cocoercivity (Zhu and Marcotte, 1996), (iii) zero-duality gap (Von Neumann, 1928), (iv) unconstrained solution space (Golowich et al., 2020) .",
    "next": "Our game is carefully crafted and -although it has a distinct structure and is nonconvex-nonconcave only due to multilinearity-satisfies none of the latter properties."
  },
  {
    "index": 1040,
    "source_corpus_id": 51559,
    "ref_id": "b27",
    "citation_corpus_id": 1952530,
    "start": 1928,
    "end": 1947,
    "title": "Predicting Polarities of Tweets by Composing Word Embeddings with Long Short-Term Memory",
    "abstract": "In this paper, we introduce Long Short-Term Memory (LSTM) recurrent network for twitter sentiment prediction. With the help of gates and constant error carousels in the memory block structure, the model could handle interactions between words through a flexible compositional function. Experiments on a public noisy labelled data show that our model outperforms several feature-engineering approaches, with the result comparable to the current best data-driven technique. According to the evaluation on a generated negation phrase test set, the proposed architecture doubles the performance of non-neural model based on bag-of-word features. Furthermore, words with special functions (such as negation and transition) are distinguished and the dissimilarities of words with opposite sentiment are magnified. An interesting case study on negation expression processing shows a promising potential of the architecture dealing with complex sentiment phrases.",
    "prev": "Recurrent layers can also be stacked, increasing network depth, representational power and often accuracy.",
    "curr": "RNN applications in the natural language domain range from sentence classification (Wang et al., 2015) to word-and character-level language modeling (Zaremba et al., 2014).",
    "next": "RNNs are also commonly the basic building block for more complex models for tasks such as machine translation (Bahdanau et al., 2015;Luong et al., 2015; or question answering (Kumar et al., 2016;."
  },
  {
    "index": 1042,
    "source_corpus_id": 245668925,
    "ref_id": "b18",
    "citation_corpus_id": 15986631,
    "start": 3620,
    "end": 3638,
    "title": "Deep Reinforcement Learning with a Natural Language Action Space",
    "abstract": "This paper introduces a novel architecture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games. Termed a deep reinforcement relevance network (DRRN), the architecture represents action and state spaces with separate embedding vectors, which are combined with an interaction function to approximate the Q-function in reinforcement learning. We evaluate the DRRN on two popular text games, showing superior performance over other deep Qlearning architectures. Experiments with paraphrased action descriptions show that the model is extracting meaning rather than simply memorizing strings of text.",
    "prev": "On the other hand, since rewards are sparse, the agent only gets a few high-scoring trajectories to learn from, requiring vigorous exploitation in order to get back to the furthest point of the game and make progress thereon.",
    "curr": "Prior approaches to solving these games (He et al., 2016a;Guo et al., 2020) usually employ a single policy and action selection strategy, making it difficult to strike the right balance between exploration and exploitation.",
    "next": "In this paper, we propose eXploit-Then-eXplore (XTX), an algorithm for multi-stage control to explicitly decompose the exploitation and exploration phases within each episode."
  },
  {
    "index": 1046,
    "source_corpus_id": 251066616,
    "ref_id": "b9",
    "citation_corpus_id": 8768364,
    "start": 53746,
    "end": 53765,
    "title": "Density estimation using Real NVP",
    "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.",
    "prev": "B.6 LIMITATIONS AND FUTURE WORK\n\nThe proposed work focuses mainly on a mixture density network (MDN) as the generative channel model, which allows us to exploit some of their useful properties in our formulation.",
    "curr": "Generalizing the proposed few-shot domain adaptation to other types of generative channel models such as conditional GANs, VAEs, and normalizing flows (Dinh et al., 2017) could be an interesting direction.",
    "next": "These generative models can handle more high-dimensional structured inputs."
  },
  {
    "index": 1053,
    "source_corpus_id": 211082795,
    "ref_id": "b21",
    "citation_corpus_id": 604334,
    "start": 1673,
    "end": 1695,
    "title": "Intriguing properties of neural networks",
    "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
    "prev": "∀x .",
    "curr": "x−x p ≤ =⇒F (x)=F (x )(1)\nThis problem carries practical significance, as such networks have been extensively shown to be vulnerable to adversarial examples (Szegedy et al., 2014;Papernot et al., 2016), wherein small-norm perturbations are chosen to cause aribtrary misclassifications.",
    "next": "Numerous solutions have been proposed to address variants of this problem."
  },
  {
    "index": 1054,
    "source_corpus_id": 233306870,
    "ref_id": "b6",
    "citation_corpus_id": 68222714,
    "start": 3462,
    "end": 3475,
    "title": "APPROXIMATING CNNS WITH BAG-OF-LOCAL- FEATURES MODELS WORKS SURPRISINGLY WELL ON IMAGENET",
    "abstract": "Deep Neural Networks (DNNs) excel on many complex perceptual tasks but it has proven notoriously difficult to understand how they reach their decisions. We here introduce a high-performance DNN architecture on ImageNet whose decisions are considerably easier to explain. Our model, a simple variant of the ResNet-50 architecture called BagNet, classifies an image based on the occurrences of small local image features without taking into account their spatial ordering. This strategy is closely related to the bag-of-feature (BoF) models popular before the onset of deep learning and reaches a surprisingly high accuracy on ImageNet (87.6% top-5 for 33 × 33 px features and Alexnet performance for 17 × 17 px features). The constraint on local features makes it straight-forward to analyse how exactly each part of the image influences the classification. Furthermore, the BagNets behave similar to state-of-the art deep neural networks such as VGG-16, ResNet-152 or DenseNet-169 in terms of feature sensitivity, error distribution and interactions between image parts. This suggests that the improvements of DNNs over previous bag-of-feature classifiers in the last few years is mostly achieved by better fine-tuning rather than by qualitatively different decision strategies.",
    "prev": "Consider, for example, a model that is built to distinguish between cows and camels using photos collected in nature under different climates.",
    "curr": "Since CNNs are known to have a bias towards texture (Geirhos et al., 2018, Brendel andBethge, 2019), if we simply try to minimize the average loss across different domains, the classifier is prone to spuriously correlate \"cow\" with grass and \"camels\" with desert, and predict the species using only the background.",
    "next": "Such a classifier can be rendered useless when the animals are placed indoors or in a zoo."
  },
  {
    "index": 1055,
    "source_corpus_id": 252683303,
    "ref_id": "b16",
    "citation_corpus_id": 231632658,
    "start": 3133,
    "end": 3151,
    "title": "What Makes Good In-Context Examples for GPT-3?",
    "abstract": "GPT-3 has attracted lots of attention due to its superior performance across a wide range of NLP tasks, especially with its in-context learning abilities. Despite its success, we found that the empirical results of GPT-3 depend heavily on the choice of in-context examples. In this work, we investigate whether there are more effective strategies for judiciously selecting incontext examples (relative to random sampling) that better leverage GPT-3's in-context learning capabilities. Inspired by the recent success of leveraging a retrieval module to augment neural networks, we propose to retrieve examples that are semantically-similar to a test query sample to formulate its corresponding prompt. Intuitively, the examples selected with such a strategy may serve as more informative inputs to unleash GPT-3's power of text generation. We evaluate the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random selection baseline. Moreover, it is observed that the sentence encoders finetuned on task-related datasets yield even more helpful retrieval results. Notably, significant gains are observed on tasks such as table-totext generation (44.3% on the ToTTo dataset) and open-domain question answering (45.5% on the NQ dataset).",
    "prev": "This work studies example selection in chain-of-thoughts multi-step reasoning.",
    "curr": "Example selection is a central problem in the prompting literature (Liu et al., 2022;Rubin et al., 2022;Su et al., 2022;Lazaridou et al., 2022).",
    "next": "It asks what instances make the best prompts for solving the tasks of interest."
  },
  {
    "index": 1057,
    "source_corpus_id": 248965297,
    "ref_id": "b13",
    "citation_corpus_id": 52967399,
    "start": 2456,
    "end": 2477,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "In ACL, 2021.",
    "curr": "INTRODUCTION\n\nLarge-scale pre-trained language models (PLMs) have achieved great success in promoting state of the art on various natural language understanding and generation tasks (Devlin et al., 2019;Radford et al., 2019;Yang et al., 2019;Brown et al., 2020;.",
    "next": "PLM self-supervision training largely benefits from harvesting local context information in the pre-training corpus."
  },
  {
    "index": 1059,
    "source_corpus_id": 255440465,
    "ref_id": "b14",
    "citation_corpus_id": 225039882,
    "start": 1724,
    "end": 1728,
    "title": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. 1 arXiv:2010.11929v2 [cs.CV] 3 Jun 2021Published as a conference paper at ICLR 2021 inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.However, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks.RELATED WORKTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global selfattention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.",
    "prev": "Introduction\n\nThe transformer architecture [50] has become an important and highly influential model family, due to its simplicity, scalability, and its wide range of applications.",
    "curr": "While originally stemming from the domain of natural language processing (NLP), with the advent of the Vision transformer (ViT) [15], this has become a standard architecture in computer vision, setting various state-of-the-art (SoTA) performances on tasks ranging from representation learning, semantic segmentation, object detection and video understanding [4,5,18,30,31].",
    "next": "However, the original formulation of the transformer includes a quadratic computational complexity with respect to the number of input tokens."
  },
  {
    "index": 1061,
    "source_corpus_id": 226221794,
    "ref_id": "b29",
    "citation_corpus_id": 221878944,
    "start": 9619,
    "end": 9636,
    "title": "How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks",
    "abstract": "We study how neural networks trained by gradient descent extrapolate, i.e., what they learn outside the support of training distribution. Previous works report mixed empirical results when extrapolating with neural networks: while multilayer perceptrons (MLPs) do not extrapolate well in simple tasks, Graph Neural Networks (GNNs), a structured network with MLP modules, have some success in more complex tasks. We provide a theoretical explanation and identify conditions under which MLPs and GNNs extrapolate well. We start by showing ReLU MLPs trained by gradient descent converge quickly to linear functions along any direction from the origin, which suggests ReLU MLPs cannot extrapolate well in most non-linear tasks. On the other hand, ReLU MLPs can provably converge to a linear target function when the training distribution is \"diverse\" enough. These observations lead to a hypothesis: GNNs can extrapolate well in dynamic programming (DP) tasks if we encode appropriate non-linearity in the architecture and input representation. We provide theoretical and empirical support for the hypothesis. Our theory explains previous extrapolation success and suggest their limitations: successful extrapolation relies on incorporating task-specific non-linearity, which often requires domain knowledge or extensive model search.",
    "prev": "Also, we consider a larger family of distribution shifts including covariate shifts.",
    "curr": "The above two categories are not exhaustivee.g., there are some recent works that characterize how some inductive biases favor extrapolation [Xu et al., 2021] and can be better for OOD generalization.",
    "next": "Sample Complexity of Invariant Risk Minimization\n\n\nInvariant Risk Minimization\n\nWe start with some background on IRM [Arjovsky et al., 2019]."
  },
  {
    "index": 1062,
    "source_corpus_id": 259298560,
    "ref_id": "b29",
    "citation_corpus_id": 3488815,
    "start": 12433,
    "end": 12453,
    "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
    "abstract": "Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. * Authors ordered alphabetically.",
    "prev": "We also consider Voneresnet (biological V1 front-end (Dapello et al., 2020)) and ResNet-50 trained as a base network for action recognition (Chen et al., 2021) but include these as separate examples in the Appendix since they use a modified architecture.",
    "curr": "MODELS TRAINED FOR ROBUSTNESS\n\nWe consider two forms of models trained for minimizing a classification loss L ce in the face of input perturbations δ ∈ R h×w×c subject to constraints on the overall magnitude of perturbations in the input space, where x, y, θ are the network input, output, and classifier parameters, respectively:\nL ce (θ, x + δ, y)(1)\nIn adversarially trained networks, projected gradient descent from the output space finds maximal directions of perturbation in the input space limited to length ϵ, and training entails minimizing the effect of these perturbation directions on the network's output (Madry et al., 2018).",
    "next": "In random smoothing (Lecuyer et al., 2018;, a supervised network is trained but in the face of Gaussian noise added to the input space as the base classifier before performing a probabilistic inference."
  },
  {
    "index": 1068,
    "source_corpus_id": 235614244,
    "ref_id": "b4",
    "citation_corpus_id": 11212020,
    "start": 23811,
    "end": 23834,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "ntic segmentation is a saturated task, and most contemporary published works have approximate performances, Hamburger shows considerable improvements over previous state-of-the-art attention modules.",
    "curr": "Method mIoU(%) PSPNet (Zhao et al., 2017) 82.6 DFN * (Yu et al., 2018) 82.7 EncNet  82.9 DANet * (Fu et al., 2019) 82.6 DMNet * (He et al., 2019a) 84.4 APCNet * (He et al., 2019b) 84.2 CFNet * (Zhang et al., 2019b) 84.2 SpyGR *  84.2 SANet * (Zhong et al., 2020) 83.2 OCR * (Yuan et al., 2020) 84.3 HamNet 85.9 Method mIoU(%) PSPNet (Zhao et al., 2017) 47.8 SGR * (Liang et al., 2018) 50.8 EncNet  51.7 DANet * (Fu et al., 2019) 52.6 EMANet * (Li et al., 2019a) 53.1 DMNet * (He et al., 2019a) 54.4 APCNet * (He et al., 2019b) 54.7 CFNet * (Zhang et al., 2019b) 54.0 SpyGR *  52.8 SANet * (Zhong et al., 2020) 53.0 OCR * (Yuan et al., 2020) 54.8 HamNet 55.2 \n\n\nIMAGE GENERATION\n\n\nRELATED WORK\n\nGeneral Survey for Attention The last five years have witnessed a roaring success of attention mechanisms (Bahdanau et al., 2015;Mnih et al., 2014;Xu et al., 2015;Luong et al., 2015) in deep learning.",
    "next": "Roughly speaking, the attention mechanism is a term of adaptively generating the targets' weights to be attended according to the requests."
  },
  {
    "index": 1069,
    "source_corpus_id": 222132819,
    "ref_id": "b12",
    "citation_corpus_id": 202565422,
    "start": 40425,
    "end": 40443,
    "title": "HIERARCHICAL FORESIGHT: SELF-SUPERVISED LEARNING OF LONG-HORIZON TASKS VIA VISUAL SUBGOAL GENERATION",
    "abstract": "Video prediction models combined with planning algorithms have shown promise in enabling robots to learn to perform many vision-based tasks through only selfsupervision, reaching novel goals in cluttered scenes with unseen objects. However, due to the compounding uncertainty in long horizon video prediction and poor scalability of sampling-based planning optimizers, one significant limitation of these approaches is the ability to plan over long horizons to reach distant goals.To that end, we propose a framework for subgoal generation and planning, hierarchical visual foresight (HVF), which generates subgoal images conditioned on a goal image, and uses them for planning. The subgoal images are directly optimized to decompose the task into easy to plan segments, and as a result, we observe that the method naturally identifies semantically meaningful states as subgoals. Across three out of four simulated vision-based manipulation tasks, we find that our method achieves nearly a 200% performance improvement over planning without subgoals and model-free RL approaches. Further, our experiments illustrate that our approach extends to real, cluttered visual scenes. † Work completed at Google Brain Videos and code are available at:",
    "prev": "(c) Role action spaces and role policy structure.",
    "curr": "Figure 2 :\n2Nachum et al., 2018b;a; Levy et al., 2018; Ghosh et al., 2018; Dwiel  et al., 2019;Nair & Finn, 2019;Nasiriany et al., 2019; Dilokthanakul et al., 2019)  or by discovering reusable skills(Daniel et al., 2012; Gregor et al., 2016;Warde-Farley et al., 2018;Shankar & Gupta, 2020;Thomas et al., 2018;Sharma et al., 2020).",
    "next": "In multi-agent settings, many challenges, such as efficient communication(Ossenkopf et al., 2019) and labor division(Wang et al., 2020c) in large systems, necessitate hierarchical learning along the second dimension -over agents(Zhang et al., 2010).Ahilan & Dayan (2019) propose a hierarchy in FeUdal(Dayan & Hinton, 1993;Vezhnevets et al., 2017) style for cooperative multi-agent tasks, where managers and workers are predefined and workers are expected to achieve the goal generated by managers."
  },
  {
    "index": 1074,
    "source_corpus_id": 253080708,
    "ref_id": "b23",
    "citation_corpus_id": 3144218,
    "start": 2175,
    "end": 2199,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "GNNs broadly follow a message-passing framework, meaning that each layer of the GNN aggregates the representations of a node and its neighbors, and transforms these features into a new representation for that node.",
    "curr": "The aggregation function used by the GNN layer is taken to be locally permutationinvariant, since the ordering of the neighbors of a node is arbitrary, and its specific form is a key component of the GNN architecture; varying it gives rise to several common GNN variants (Kipf and Welling, 2017;Veličković et al., 2018;Li et al., 2015;Hamilton et al., 2017;Xu et al., 2019).",
    "next": "The output of a GNN can be used for tasks such as graph classification or node classification."
  },
  {
    "index": 1075,
    "source_corpus_id": 53113561,
    "ref_id": "b17",
    "citation_corpus_id": 1957433,
    "start": 6778,
    "end": 6802,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "BASIC DESIGN OF MACHINE COMPREHENSION MODELS\n\nFor single-turn MC, many top-performing models share a similar architecture, consisting of four major components: (1) question encoding, (2) context encoding, (3) reasoning, and finally (4) answer prediction.",
    "curr": "Initially the word embeddings (e.g., Pennington et al., 2014;Peters et al., 2018) of question tokens Q and context tokens C are taken as input and fed into contextual integration layers, such as LSTMs (Hochreiter & Schmidhuber, 1997) or self attentions (Yu et al., 2018), to encode the question and context.",
    "next": "Multiple integration layers provide contextualized representations of context, and are often inter-weaved with attention, which inject question information."
  },
  {
    "index": 1077,
    "source_corpus_id": 229923128,
    "ref_id": "b18",
    "citation_corpus_id": 8822680,
    "start": 2025,
    "end": 2048,
    "title": "Six Challenges for Neural Machine Translation",
    "abstract": "We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrasebased statistical machine translation.",
    "prev": "INTRODUCTION\n\nWhen translating a word, translation models need to spend a substantial amount of its capacity in disambiguating its sense in the source language and choose a lexeme in the target language which adequately express its meaning (Choi et al., 2017;Tamchyna, 2017).",
    "curr": "However, neural machine translation (NMT) has a severe problem on lexical choice, since it usually has mistranslation errors on low-frequency words (Koehn & Knowles, 2017;Nguyen & Chiang, 2018).",
    "next": "Table 1: All samples that contain the source word \"纽 马 基特\" in raw and distilled training corpora, which are different in target sides (RAW-TGT vs. KD-TGT)."
  },
  {
    "index": 1078,
    "source_corpus_id": 238856778,
    "ref_id": "b30",
    "citation_corpus_id": 1957433,
    "start": 19469,
    "end": 19494,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "(Even without having ever seen a white wolf, a typical English speaker can guess that a white wolf is more likely to resemble an arctic fox than a snorkel.)",
    "curr": "These kinds of relations are often captured by embeddings of class labels (or more detailed class descriptions) (Pennington et al., 2014).",
    "next": "When available, this kind of information about class semantics can be used to construct an improved subspace regularizer by encouraging new class representations to lie close to a convex combination of base classes weighted by their semantic similarity."
  },
  {
    "index": 1081,
    "source_corpus_id": 260351224,
    "ref_id": "b4",
    "citation_corpus_id": 29153681,
    "start": 11096,
    "end": 11121,
    "title": "META-LEARNING WITH DIFFERENTIABLE CLOSED-FORM SOLVERS",
    "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.Most work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent. Nonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently. In this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning. The main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data. This requires back-propagating errors through the solver steps. While normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage. We propose both closed-form and iterative solvers, based on ridge regression and logistic regression components. Our methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.",
    "prev": "Bilevel optimization.",
    "curr": "Bilevel optimization is a powerful tool to study many machine learning applications such as hyperparameter optimization (Franceschi et al., 2018;Shaban et al., 2019), meta-learning (Bertinetto et al., 2018;Franceschi et al., 2018;Rajeswaran et al., 2019;Ji et al., 2020;, neural architecture search (Liu et al., 2018;Zhang et al., 2021a), etc.",
    "next": "Existing approaches are usually approximate implicit differentiation (AID) based (Domke, 2012;Pedregosa, 2016;Gould et al., 2016;Liao et al., 2018;Lorraine et al., 2020), or iterative differentiation (ITD) based (Domke, 2012;Maclaurin et al., 2015;Franceschi et al., 2017;Finn et al., 2017;Shaban et al., 2019;Rajeswaran et al., 2019;Liu et al., 2020)."
  },
  {
    "index": 1083,
    "source_corpus_id": 256663850,
    "ref_id": "b4",
    "citation_corpus_id": 229376913,
    "start": 8174,
    "end": 8191,
    "title": "RealFormer: Transformer Likes Residual Attention",
    "abstract": "Transformer is the backbone of modern NLP models. In this paper, we propose Real-Former, a simple and generic technique to create Residual Attention Layer Transformer networks that significantly outperform the canonical Transformer and its variants (BERT, ETC, etc.) on a wide spectrum of tasks including Masked Language Modeling, GLUE, SQuAD, Neural Machine Translation, WikiHop, HotpotQA, Natural Questions, and OpenKP. We also observe empirically that RealFormer stabilizes training and leads to models with sparser attention. Source code and pre-trained checkpoints for RealFormer can be found at https",
    "prev": "To distinguish it from our RLA, we rename the former as RLA g in the following.",
    "curr": "RealFormer (He et al., 2021) and EA-Transformer (Wang et al., 2021) both added attention scores in the previous layer to the current one, connecting the layers by residual attention.",
    "next": "Bapna et al."
  },
  {
    "index": 1086,
    "source_corpus_id": 212414027,
    "ref_id": "b6",
    "citation_corpus_id": 53464644,
    "start": 24902,
    "end": 24925,
    "title": "LEARNING PROTEIN SEQUENCE EMBEDDINGS USING INFORMATION FROM STRUCTURE",
    "abstract": "Inferring the structural properties of a protein from its amino acid sequence is a challenging yet important problem in biology. Structures are not known for the vast majority of protein sequences, but structure is critical for understanding function. Existing approaches for detecting structural similarity between proteins from sequence are unable to recognize and exploit structural patterns when sequences have diverged too far, limiting our ability to transfer knowledge between structurally related proteins. We newly approach this problem through the lens of representation learning. We introduce a framework that maps any protein sequence to a sequence of vector embeddings -one per amino acid position -that encode structural information. We train bidirectional long short-term memory (LSTM) models on protein sequences with a two-part feedback mechanism that incorporates information from (i) global structural similarity between proteins and (ii) pairwise residue contact maps for individual proteins. To enable learning from structural similarity information, we define a novel similarity measure between arbitrarylength sequences of vector embeddings based on a soft symmetric alignment (SSA) between them. Our method is able to learn useful position-specific embeddings despite lacking direct observations of position-level correspondence between sequences. We show empirically that our multi-task framework outperforms other sequence-based methods and even a top-performing structure-based alignment method when predicting structural similarity, our goal. Finally, we demonstrate that our learned embeddings can be transferred to other protein sequence problems, improving the state-of-the-art in transmembrane domain prediction. 1 1 source code and datasets are available at Published as a conference paper at ICLR 2019 but the problem is challenging, because sequence similarity and structural similarity are only loosely related [1, 2, 3, 4], e.g. similar structural folds can be formed by diverse sequences. As a result, our ability to transfer knowledge between proteins with similar structures is limited.In this work, we address this problem by learning protein sequence embeddings using weak supervision from global structural similarity for the first time. Specifically, we aim to learn a bidirectional LSTM (biLSTM) embedding model, mapping sequences of amino acids to sequences of vector representations, such that residues occurring in similar structural contexts will be close in embedding space. This is difficult, because we have not observed position-level correspondences between sequences, only global sequence similarity. We solve this by defining a whole sequence similarity measure from sequences of vector embeddings. The measure decomposes into an alignment of the sequences and pairwise comparison of the aligned positions in embedding space. For the alignment, we propose a soft symmetric alignment (SSA) mechanism -a symmetrization of the directional alignment commonly used in attention mechanisms. Furthermore, in order to take advantage of information about local structural context within proteins, we extend this framework to include position-level supervision from contacts between residues in the individual protein structures. This multitask framework(Figure 1) allows us to newly leverage both global structural similarity between proteins and residue-residue contacts within proteins for training embedding models.",
    "prev": "In contrast to our work, these methods operate over small molecular graphs and were not applied to large macromolecules, like proteins.",
    "curr": "In parallel, recent work proposes that generative models pre-trained on protein sequences can transfer knowledge to downstream supervised tasks (Bepler & Berger, 2019;Alley et al., 2019;Yang et al., 2019;Rives et al., 2019).",
    "next": "These methods have also been explored for protein design (Wang et al., 2018)."
  },
  {
    "index": 1087,
    "source_corpus_id": 259203325,
    "ref_id": "b3",
    "citation_corpus_id": 199371687,
    "start": 66589,
    "end": 66610,
    "title": "The BEA-2019 Shared Task on Grammatical Error Correction",
    "abstract": "This paper reports on the BEA-",
    "prev": "For our target X set, Jayanthi et al.",
    "curr": "(2020) provide 40k pairs of real mistakes by humans taken from the BEA grammar correction corpus (Bryant et al., 2019), which we use as our ideal target X.\n\n\nC.4.2 GIO\n\nWe first generate embeddings for the train and target sets using MPNet-Base-V2 (Song et al., 2020) on the input side of the data; see Appendix C.2 for an example, we use the same process as in our WMT experiments.",
    "next": "On an AWS p3dn.24xlarge machine with 8 NVIDIA Tesla V100 GPUs, it takes about 4 hours to generate the embeddings for the 15M data."
  },
  {
    "index": 1088,
    "source_corpus_id": 222140947,
    "ref_id": "b17",
    "citation_corpus_id": 4009713,
    "start": 2340,
    "end": 2362,
    "title": "Published as a conference paper at ICLR 2018 UNSUPERVISED REPRESENTATION LEARNING BY PRE- DICTING IMAGE ROTATIONS",
    "abstract": "Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training Con-vNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet.",
    "prev": "Several early methods propose pretext tasks that explore the inherent structures within a single image.",
    "curr": "For example, by identifying spatial arrangement (Doersch et al., 2015), orientation (Gidaris et al., 2018), or chromatic channels , models learn useful representations for downstream tasks.",
    "next": "Recently, another line of works Hjelm et al., 2018;Misra & van der Maaten, 2020;Chen et al., 2020a), e.g."
  },
  {
    "index": 1089,
    "source_corpus_id": 235485093,
    "ref_id": "b14",
    "citation_corpus_id": 212657453,
    "start": 37146,
    "end": 37164,
    "title": "GAUGE EQUIVARIANT MESH CNNS ANISOTROPIC CONVOLUTIONS ON GEOMETRIC GRAPHS",
    "abstract": "A common approach to define convolutions on meshes is to interpret them as a graph and apply graph convolutional networks (GCNs). Such GCNs utilize isotropic kernels and are therefore insensitive to the relative orientation of vertices and thus to the geometry of the mesh as a whole. We propose Gauge Equivariant Mesh CNNs which generalize GCNs to apply anisotropic gauge equivariant kernels. Since the resulting features carry orientation information, we introduce a geometric message passing scheme defined by parallel transporting features over mesh edges. Our experiments validate the significantly improved expressivity of the proposed model over conventional GCNs and other methods. * Equal Contribution",
    "prev": "Another promising application of PDOs is an extension to manifolds.",
    "curr": "Gauge CNNs (Cohen et al., 2019c;Kicanaoglu et al., 2019;Haan et al., 2021;) are a rather general framework for convolutions on manifolds; see Weiler et al.",
    "next": "(2021) for a thorough treatment and literature review."
  },
  {
    "index": 1091,
    "source_corpus_id": 259138847,
    "ref_id": "b8",
    "citation_corpus_id": 247958465,
    "start": 5999,
    "end": 6002,
    "title": "MoEfication: Transformer Feed-forward Layers are Mixtures of Experts",
    "abstract": "Recent work has shown that feed-forward networks (FFNs) in pre-trained Transformers are a key component, storing various linguistic and factual knowledge.However, the computational patterns of FFNs are still unclear.In this work, we study the computational patterns of FFNs and observe that most inputs only activate a tiny ratio of neurons of FFNs.This phenomenon is similar to the sparsity of the human brain, which drives research on functional partitions of the human brain.To verify whether functional partitions also emerge in FFNs, we propose to convert a model into its MoE version with the same parameters, namely MoEfication.Specifically, MoEfication consists of two phases: (1) splitting the parameters of FFNs into multiple functional partitions as experts, and (2) building expert routers to decide which experts will be used for each input.Experimental results show that MoEfication can conditionally use 10% to 30% of FFN parameters while maintaining over 95% original performance for different models on various downstream tasks.Besides, MoEfication brings two advantages: (1) it significantly reduces the FLOPS of inference, i.e., 2x speedup with 25% of FFN parameters, and (2) it provides a fine-grained perspective to study the inner mechanism of FFNs.The source code of this paper can be obtained from https://github.com/thunlp/MoEfication.",
    "prev": "Moreover, unlike classical sparse methods where such a connection is established via explicit sparse regularization [8], the sparsity observed in Transformers is emergent without any explicit design.",
    "curr": "It is worth noting that the observation that Transformers produce sparse activations is previously reported in [9].",
    "next": "Our paper significantly extends upon results in [9] to demonstrate that sparsity emerges prevalently at all layers of Transformers, for both language and vision tasks, on both training and evaluation data, and for some architectures beyond Transformers."
  },
  {
    "index": 1092,
    "source_corpus_id": 236087421,
    "ref_id": "b34",
    "citation_corpus_id": 14124313,
    "start": 5101,
    "end": 5129,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "In the MLP-Mixer , the model obtains the global receptive field through matrix transposition and token-mixing projection such that the long-range dependencies are covered.",
    "curr": "However, this rarely makes full use of the local information, which is very important in CNN-like architecture (Simonyan & Zisserman, 2015;He et al., 2016) because not all pixels need long-range dependencies, and the Therefore, a more ideal way to introduce locality is to directly model the relationship between a feature point and its surrounding feature points at any position, without the need to set a fixed window (and window size) in advance.",
    "next": "To aggregate the features of different spatial positions in the same position and model their relationships, inspired by (Wu et al., 2018;Lin et al., 2019;Wang et al., 2020;Ho et al., 2019), we propose an axial shift strategy for MLP-based architecture, where we spatially shift features in both horizontal and vertical directions."
  },
  {
    "index": 1094,
    "source_corpus_id": 256662465,
    "ref_id": "b37",
    "citation_corpus_id": 220363897,
    "start": 2927,
    "end": 2945,
    "title": "Meta-Learning Symmetries by Reparameterization",
    "abstract": "Many successful deep learning architectures are equivariant to certain transformations in order to conserve parameters and improve generalization: most famously, convolution layers are equivariant to shifts of the input. This approach only works when practitioners know a priori symmetries of the task and can manually construct an architecture with the corresponding equivariances. Our goal is a general approach for learning equivariances from data, without needing prior knowledge of a task's symmetries or custom task-specific architectures. We present a method for learning and encoding equivariances into networks by learning corresponding parameter sharing patterns from data. Our method can provably encode equivarianceinducing parameter sharing for any finite group of symmetry transformations, and we find experimentally that it can automatically learn a variety of equivariances from symmetries in data. We provide our experiment code and pre-trained models at https://github.com/AllanYangZhou/metalearning-symmetries.",
    "prev": "If appropriate equivariance constraints are instead learned, the benefits of equivariance could extend to applications where the data may have unknown or imperfect symmetries.",
    "curr": "Learning approximate equivariance has been recently approached through novel layer operations (Wang et al., 2022;Finzi et al., 2021;Zhou et al., 2020;Yeh et al., 2022;Basu et al., 2021).",
    "next": "Separately, the field of neural architecture search (NAS) aims to optimize full neural network architectures (Zoph & Le, 2017;Real et al., 2017;Elsken et al., 2017;Liu et al., 2018;Lu et al., 2019)."
  },
  {
    "index": 1096,
    "source_corpus_id": 250048824,
    "ref_id": "b6",
    "citation_corpus_id": 240354066,
    "start": 2154,
    "end": 2157,
    "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
    "abstract": "A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of 10000 or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model, and showed that for appropriate choices of the state matrix A, this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning A with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation 60× faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors. 1",
    "prev": "* Equal contribution.",
    "curr": "Introduction\n\nThe Structured State Space model (S4) is a recent deep learning model based on continuous-time dynamical systems that has shown promise on a wide variety of sequence modeling tasks [7].",
    "next": "It is defined as a linear time-invariant (LTI) state space model (SSM), which give it multiple properties [6]: as an SSM, S4 can be simulated as a discrete-time recurrence for efficiency in online or autoregressive settings, and as a LTI model, S4 can be converted into a convolution for parallelizability and computational efficiency at training time."
  },
  {
    "index": 1097,
    "source_corpus_id": 225103395,
    "ref_id": "b35",
    "citation_corpus_id": 2723173,
    "start": 6570,
    "end": 6590,
    "title": "FITNETS: HINTS FOR THIN DEEP NETS",
    "abstract": "While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network.",
    "prev": "Further study of accuracy and error sets have been conducted in (Hacohen & Weinshall, 2020) (error sets over training), and (Hooker et al., 2019) (error after pruning).",
    "curr": "Other work has demonstrated that it is often possible for narrower or shallower neural networks to attain similar accuracy to larger networks when the smaller networks are trained to mimic the larger networks' predictions (Ba & Caruana, 2014;Romero et al., 2015).",
    "next": "We instead seek to study the impact of width and depth on network internal representations and (per-example) outputs, by applying techniques for measuring similarity of neural network hidden representations (Kornblith et al., 2019;Raghu et al., 2017a;Morcos et al., 2018)."
  },
  {
    "index": 1100,
    "source_corpus_id": 36483539,
    "ref_id": "b37",
    "citation_corpus_id": 12713052,
    "start": 2560,
    "end": 2577,
    "title": "NEURAL ARCHITECTURE SEARCH WITH REINFORCEMENT LEARNING",
    "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214. * Work done as a member of the Google Brain Residency program (g.co/brainresidency.) Under review as a conference paper at ICLR 2017 neural network can be typically specified by a variable-length string. It is therefore possible to use a recurrent network -the controller -to generate such string. Training the network specified by the string -the \"child network\" -on the real data will result in an accuracy on a validation set. Using this accuracy as the reward signal, we can compute the policy gradient to update the controller. As a result, in the next iteration, the controller will give higher probabilities to architectures that receive high accuracies. In other words, the controller will learn to improve its search over time.Our experiments show that Neural Architecture Search can design good models from scratch, an achievement considered not possible with other methods. On image recognition with CIFAR-10, Neural Architecture Search can find a novel ConvNet model that is better than most human-invented architectures. Our CIFAR-10 model achieves a 3.65 test set error, while being 1.05x faster than the current best model. On language modeling with Penn Treebank, Neural Architecture Search can design a novel recurrent cell that is also better than previous RNN and LSTM architectures. The cell that our model found achieves a test set perplexity of 62.4 on the Penn Treebank dataset, which is 3.6 perplexity better than the previous state-of-the-art.",
    "prev": "(2017)) is a class of approaches to reduce the size of Deep Neural Networks (DNNs) and accelerate inference.",
    "curr": "Meanwhile, Structure Learning (Zoph & Le (2017), Philipp & Carbonell (2017), Cortes et al.",
    "next": "(2017)) becomes an active research area for DNN structure exploration, which can potentially replace human labor with machine automation for design space exploration."
  },
  {
    "index": 1101,
    "source_corpus_id": 256662499,
    "ref_id": "b55",
    "citation_corpus_id": 13751870,
    "start": 2617,
    "end": 2629,
    "title": "A Call for Clarity in Reporting BLEU Scores",
    "abstract": "The field of machine translation faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to \"the\" BLEU score, BLEU is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for usersupplied reference processing, and provide a new tool, SACREBLEU, 1 to facilitate this. . 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. ArXiv eprints, abs/1609.08144.",
    "prev": "INTRODUCTION\n\nTraining of deep learning models utilizes randomness to improve generalization and training efficiency, thus causing an inherent nondeterminism that hampers the reliability of machine learning evaluation -the consistency of the measurement of evaluation scores across replicated training runs.Gundersen et al.",
    "curr": "(2022) list several sources of nondeterminism, e.g., implementation-level nondeterminism such as random ordering in floating-point accumulation in parallel GPU threads (Pham et al., 2021), algorithmic factors such as variations in meta-parameters and model architecture (Lucic et al., 2018;Henderson et al., 2018;D'Amour et al., 2020), or data-level factors such as variations in pre-processing and evaluation metrics (Post, 2018;Chen et al., 2022) or varying characteristics of data in different splits (Gorman & Bedrick, 2019;Søgaard et al., 2021).Zhuang et al.",
    "next": "(2022) show that implementation-level nondeterminism is partly irreducible, leading to variability in evaluation scores even for training runs on identical data, algorithmic settings and infrastructure.Furthermore, they point out strong effects of certain types of algorithm-level nondeterminism on certain subsets of the data."
  },
  {
    "index": 1102,
    "source_corpus_id": 263333942,
    "ref_id": "b10",
    "citation_corpus_id": 13046179,
    "start": 2171,
    "end": 2197,
    "title": "A BASELINE FOR DETECTING MISCLASSIFIED AND OUT-OF-DISTRIBUTION EXAMPLES IN NEURAL NETWORKS",
    "abstract": "We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.Published as a conference paper at ICLR 2017 one method which outperforms the baseline on some (but not all) tasks. This new method evaluates the quality of a neural network's input reconstruction to determine if an example is abnormal.",
    "prev": "INTRODUCTION\n\nIn deep neural networks, out-of-distribution (OOD) detection distinguishes samples which deviate from the training distribution.Standard OOD detection concerns semantic shifts (Yang et al., 2022;Zhang et al., 2023), where OOD data is defined as test samples from semantic categories unseen during training.Ideally, the neural network should be able to reject such samples as being OOD, while still maintaining strong performance on in-distribution (ID) test samples belonging to seen training categories.",
    "curr": "Methods for detecting OOD samples work by scoring network outputs such as logits or softmax values (Hendrycks & Gimpel, 2017;Hendrycks et al., 2022), post-hoc network adjustment during inference to improve OOD scoring (Sun & Li, 2022;Sun et al., 2021;Djurisic et al., 2023), or by adjusting model training (Wei et al., 2022;Ming et al., 2023;DeVries & Taylor, 2018).These approaches can be used either independently or in conjunction with one another.Typically, post-hoc adjustments together with OOD scoring is the preferred combination since it is highly effective at discerning OOD samples with minimal ID drop and can also be applied directly to already-trained models off-the-shelf.Examples include ReAct (Sun et al., 2021), DICE (Sun & Li, 2022) and more recently, ASH (Djurisic et al., 2023).",
    "next": "On the surface, each method takes different and sometimes even contradictory approaches.ReAct rectifies penultimate activations which exceed a threshold; ASH, on the other hand, prunes penultimate activations that are too low while amplifying remaining activations.While ASH currently achieves state-of-the-art performance,"
  },
  {
    "index": 1103,
    "source_corpus_id": 249712234,
    "ref_id": "b33",
    "citation_corpus_id": 6212000,
    "start": 6056,
    "end": 6076,
    "title": "UNDERSTANDING DEEP LEARNING REQUIRES RE- THINKING GENERALIZATION",
    "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.",
    "prev": "Introduction\n\nA central challenge of machine learning theory is understanding the generalization of overparameterized models.",
    "curr": "While in many real-world settings deep networks achieve low test loss, their high capacity makes theoretical analysis with classical tools difficult, or sometimes impossible (Zhang et al., 2017;Nagarajan and Kolter, 2019b).",
    "next": "Most classical theoretical tools are based on uniform convergence (UC), a property that, when it holds, guarantees that the test loss will be close to the training loss, uniformly over a class of candidate models."
  },
  {
    "index": 1104,
    "source_corpus_id": 67749672,
    "ref_id": "b46",
    "citation_corpus_id": 14687186,
    "start": 3842,
    "end": 3863,
    "title": "Parsing with Compositional Vector Grammars",
    "abstract": "Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness. Instead, we introduce a Compositional Vector Grammar (CVG), which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations. The CVG improves the PCFG of the Stanford Parser by 3.8% to obtain an F1 score of 90.4%. It is fast to train and implemented approximately as an efficient reranker it is about 20% faster than the current Stanford factored parser. The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments.",
    "prev": "One feature shared by many humandesigned representation systems is compositionality: the capacity to represent complex concepts (from objects to procedures to beliefs) by combining simple parts (Fodor & Lepore, 2002).",
    "curr": "While many machine learning approaches make use of human-designed compositional analyses for representation and prediction (Socher et al., 2013;Dong & Lapata, 2016), it is also natural to ask whether (and how) compositionality arises in learning problems where compositional structure has not been built in from the start.",
    "next": "Consider the example in Figure 1, which shows a hypothetical character-based encoding scheme learned for a simple communication task (similar to the one studied by Lazaridou et al., 2016)."
  },
  {
    "index": 1105,
    "source_corpus_id": 85501306,
    "ref_id": "b19",
    "citation_corpus_id": 6628106,
    "start": 20679,
    "end": 20698,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "In each training batch, 16 low-quality (LQ) patches with the size of 48 × 48 are extracted as inputs.",
    "curr": "Our model is trained by ADAM optimizer (Kingma & Ba, 2014) with β 1 = 0.9, β 2 = 0.999, and = 10 −8 .",
    "next": "The initial learning rate is set to 10 −4 and then decreases to half every 2 × 10 5 iterations of back-propagation."
  },
  {
    "index": 1107,
    "source_corpus_id": 29153681,
    "ref_id": "b14",
    "citation_corpus_id": 3431470,
    "start": 8924,
    "end": 8945,
    "title": "FEW-SHOT LEARNING WITH GRAPH NEURAL NET- WORKS",
    "abstract": "We propose to study the problem of few-shot learning with the prism of inference on a partially observed graphical model, constructed from a collection of input images whose label can be either observed or not. By assimilating generic message-passing inference algorithms with their neural-network counterparts, we define a graph neural network architecture that generalizes several of the recently proposed few-shot learning models. Besides providing improved numerical performance, our framework is easily extended to variants of few-shot learning, such as semi-supervised or active learning, demonstrating the ability of graph-based models to operate well on 'relational' tasks.",
    "prev": "Despite its simplicity, this general strategy is particularly effective and it is at the core of several stateof-the-art few-shot classification algorithms (Vinyals et al., 2016;Snell et al., 2017;Sung et al., 2018).",
    "curr": "Interestingly, Garcia & Bruna (2018) interpret learning as information propagation from support (training) to query (test) images and propose a graph neural network that can generalize matching-based approaches.",
    "next": "Since this line of work relies on learning a similarity metric, one distinctive characteristic is that parameter updates only occur within the long time horizon of the outer training loop."
  },
  {
    "index": 1109,
    "source_corpus_id": 232307426,
    "ref_id": "b19",
    "citation_corpus_id": 52877454,
    "start": 1868,
    "end": 1893,
    "title": "DEEP GRAPH INFOMAX",
    "abstract": "We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs-both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.",
    "prev": "Self-supervised learning, a subset of unsupervised learning, learns representations by allowing the data to provide supervision (Devlin et al., 2018).",
    "curr": "Among its mainstream strategies, self-supervised contrastive learning has been successful in visual object recognition (He et al., 2020;Tian et al., 2019;Chen et al., 2020c), speech recognition (Oord et al., 2018;Rivière et al., 2020), language modeling (Kong et al., 2019), graph representation learning (Velickovic et al., 2019) and reinforcement learning (Kipf et al., 2019).",
    "next": "The idea of self-supervised contrastive learning is to learn latent representations such that related instances (e.g., patches from the same image; defined as positive pairs) will have representations within close distance, while unrelated instances (e.g., patches from two different images; defined as negative pairs) will have distant representations (Arora et al., 2019)."
  },
  {
    "index": 1110,
    "source_corpus_id": 244908617,
    "ref_id": "b12",
    "citation_corpus_id": 56657912,
    "start": 25448,
    "end": 25478,
    "title": "BENCHMARKING NEURAL NETWORK ROBUSTNESS TO COMMON CORRUPTIONS AND PERTURBATIONS",
    "abstract": "In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, IMAGENET-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called IMAGENET-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.",
    "prev": "First, we learn pixel classifiers on the clean images using the DDPM, SwAV and MAE representations on the Bedroom-28 and Horse-21 datasets.",
    "curr": "Then, 18 diverse corruption types, adopted from (Hendrycks & Dietterich, 2019), are applied to test images.",
    "next": "Each corruption has five levels of severity."
  },
  {
    "index": 1112,
    "source_corpus_id": 238531695,
    "ref_id": "b23",
    "citation_corpus_id": 52967399,
    "start": 2372,
    "end": 2392,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": " hand, CEL blends each embedding with multiple patches of different scales, providing the self-attention module itself with cross-scale features.On the other hand, LSDA splits the self-attention module into a short-distance one and a long-distance counterpart, which not only reduces the computational burden but also keeps both small-scale and large-scale features in the embeddings.Through the above two designs, we achieve cross-scale attention.Besides, we put forward a dynamic position bias for vision transformers to make the popular relative position bias apply to variable-sized images.Hinging on the cross-scale attention module, we construct a versatile vision architecture, dubbed CrossFormer, which accommodates variable-sized inputs.Extensive experiments show that CrossFormer outperforms the other vision transformers on image classification, object detection, instance segmentation, and semantic segmentation tasks.",
    "curr": "1\n\nINTRODUCTION\n\nIt turns out that transformer (Vaswani et al., 2017;Devlin et al., 2019;Brown et al., 2020) has achieved great success in the field of natural language processing (NLP).Benefitting from its selfattention module, transformer is born with the key ability to build long-distance dependencies.Since long-distance dependencies are also needed by a number of vision tasks (Zhang & Yang, 2021;Chu et al., 2021), a surge of research work (Dosovitskiy et al., 2021;Touvron et al., 2021;Wang et al., 2021) has been conducted to explore various transformer-based vision architectures.",
    "next": "A transformer requires a sequence of embeddings2 (e.g., word embeddings) as input.To adapt this requirement to typical vision tasks, most existing vision transformers (Dosovitskiy et al., 2021;Touvron et al., 2021;Wang et al., 2021;Liu et al., 2021b) produce embeddings by splitting an input image into equal-sized patches.For example, a 224 × 224 image can be split into 56 × 56 patches of size 4 × 4, and these patches are projected through a linear layer to yield an embedding sequence.Inside a"
  },
  {
    "index": 1113,
    "source_corpus_id": 263830433,
    "ref_id": "b15",
    "citation_corpus_id": 233296808,
    "start": 4926,
    "end": 4947,
    "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
    "abstract": "In this work, we explore \"prompt tuning,\" a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient \"prompt ensembling.\" We release code and model checkpoints to reproduce our experiments. 1ReferencesRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,  Danilo Giampiccolo, Bernardo Magnini, and Idan  Szpektor. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognising textual entailment, volume 6, pages 6-4. Venice. . 2019a. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
    "prev": "ext descriptions of the desired tasks that are often significantly easier to obtain compared to full model weights or even a generic linear classifier over the last layer.The versatility and performance of prompting Table 1: Comparison with existing state-of-the-art generalization bounds for test error on different datasets.We report both data-independent and data-dependent bounds (⋆ indicates data-dependent prior and − indicates that the bounds are not available).Note that different works use different architectures and analytic tools so direct comparison can be more nuanced.Nonetheless, our bounds on prompt engineering are significantly tighter than the existing PAC-Bayes bounds in the literature, often within a few percent of the actual test error.",
    "curr": "Dataset\n\nZhou pretrained models have led to the rise of prompt engineering, an emergent paradigm in machine learning where practitioners carefully design the task specification in text or even learn the prompts in a data-driven fashion (Lester et al., 2021).For example, to obtain a two-class image classifier, one would write two sentences that describe the classes (e.g., \"This is a dog\" and \"This is a cat\"), and the two sentences are turned into text embeddings which can be used to classify image embeddings.Despite its empirical success, little is understood of how and why prompting these pretrained models work and, in particular, why the method seems to suffer little from overfitting: manually tuning or even greedily optimizing prompts on a given training set often performs nearly as well on the corresponding test set.",
    "next": "In this paper, we demonstrate that rather simple analysis tools capture this behavior surprisingly well (under some assumptions).In particular, we show that classical PAC-Bayes bounds (McAllester, 1999), when applied to the discrete hypothesis class defined by prompts (and specifically with a prior given by a large language model), are often remarkably tight, even for large domains: for example, we achieve a generalizati"
  },
  {
    "index": 1115,
    "source_corpus_id": 3464416,
    "ref_id": "b12",
    "citation_corpus_id": 6628106,
    "start": 29143,
    "end": 29162,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "Dropout was applied to only fully-connected layers of the network with the probability of retaining the unit being 0.5.",
    "curr": "For all experiments, the softmax classifier is used, and each model is trained by optimizing the objective function using Adam learning rule (Kingma & Ba, 2014).",
    "next": "For each out-ofdistribution dataset, we randomly select 1,000 images for tuning the penalty parameter β, mini-batch size and learning rate."
  },
  {
    "index": 1116,
    "source_corpus_id": 252693131,
    "ref_id": "b22",
    "citation_corpus_id": 209439843,
    "start": 2844,
    "end": 2866,
    "title": "MEASURING COMPOSITIONAL GENERALIZATION: A COMPREHENSIVE METHOD ON REALISTIC DATA",
    "abstract": "State-of-the-art machine learning methods exhibit limited compositional generalization. At the same time, there is a lack of realistic benchmarks that comprehensively measure this ability, which makes it challenging to find and evaluate improvements. We introduce a novel method to systematically construct such benchmarks by maximizing compound divergence while guaranteeing a small atom divergence between train and test sets, and we quantitatively compare this method to other approaches for creating compositional generalization benchmarks. We present a large and realistic natural language question answering dataset that is constructed according to this method, and we use it to analyze the compositional generalization ability of three machine learning architectures. We find that they fail to generalize compositionally and that there is a surprisingly strong negative correlation between compound divergence and accuracy. We also demonstrate how our method can be used to create new compositionality benchmarks on top of the existing SCAN dataset, which confirms these findings. Radev. Improving text-to-SQL evaluation methodology. In ACL, 2018. URL http://aclweb.org/anthology/P18-1033.Jerry A Fodor and Zenon W Pylyshyn. Connectionism and cognitive architecture: A critical analysis. Cognition, 28(1-2):3-71, 1988. URL https://pdfs.semanticscholar.org/d806/ 76034bfabfea59f35698af0f715a555fcf50.pdf.",
    "prev": "This type of compositionality is central to the human ability to generalize from limited data to novel combinations (Lake et al., 2017).",
    "curr": "Recently, several datasets have been proposed to test systematic generalization of machine learning models-SCAN (Lake & Baroni, 2018), PCFG (Hupkes et al., 2020), CFQ (Keysers et al., 2020), and HINT (Li et al., 2021), to name a few.",
    "next": "While conventional neural networks fail dramatically on these datasets, certain inductive biases have been explored to improve systematic generalization."
  },
  {
    "index": 1120,
    "source_corpus_id": 256390109,
    "ref_id": "b26",
    "citation_corpus_id": 247244493,
    "start": 2830,
    "end": 2848,
    "title": "BETTER SUPERVISORY SIGNALS BY OBSERVING LEARNING PATHS",
    "abstract": "Better-supervised models might have better performance. In this paper, we first clarify what makes for good supervision for a classification problem, and then explain two existing label refining methods, label smoothing and knowledge distillation, in terms of our proposed criterion. To further answer why and how better supervision emerges, we observe the learning path, i.e., the trajectory of the model's predictions during training, for each training sample. We find that the model can spontaneously refine \"bad\" labels through a \"zig-zag\" learning path, which occurs on both toy and real datasets. Observing the learning path not only provides a new perspective for understanding knowledge distillation, overfitting, and learning dynamics, but also reveals that the supervisory signal of a teacher network can be very unstable near the best points in training on real tasks. Inspired by this, we propose a new knowledge distillation scheme, Filter-KD, which improves downstream classification performance in various settings.",
    "prev": "Simultaneously, several works have aimed to rigorously formalize why KD can improve the student model performance.",
    "curr": "Some prominent observations from this line of work are that (self-)distillation induces certain favorable optimization biases in the training objective [Phuong andLampert, 2019, Ji andZhu, 2020], lowers variance of the objective [Menon et al., 2021, Dao et al., 2021, Ren et al., 2022, increases regularization towards learning \"simpler\" functions [Mobahi et al., 2020], transfers information from different data views [Allen-Zhu and Li, 2020], and scales per-example gradients based on the teacher's confidence [Furlanello et al., 2018, Tang et al., 2020.",
    "next": "Despite this remarkable progress, there are still many open problems and unexplained phenomena around knowledge distillation; to name a few: -Why do soft labels (sometimes) help?"
  },
  {
    "index": 1121,
    "source_corpus_id": 221818900,
    "ref_id": "b24",
    "citation_corpus_id": 49882757,
    "start": 1897,
    "end": 1915,
    "title": "ClariNet: Parallel Wave Generation in End-to-End Text-to-Speech",
    "abstract": "In this work, we propose an alternative solution for parallel wave generation by WaveNet. In contrast to parallel WaveNet (Oord et al., 2018), we distill a Gaussian inverse autoregressive flow from the autoregressive WaveNet by minimizing a novel regularized KL divergence between their highly-peaked output distributions. Our method computes the KL divergence in closed-form, which simplifies the training algorithm and provides very efficient distillation. In addition, we propose the first text-to-wave neural architecture for speech synthesis, which is fully convolutional and enables fast end-to-end training from scratch. It significantly outperforms the previous pipeline that connects a text-to-spectrogram model to a separately trained WaveNet (Ping et al., 2018). We also successfully distill a parallel waveform synthesizer conditioned on the hidden representation in this end-to-end model. 2 * These authors contributed equally to this work. Correspondence to <weiping.thu@gmail.com>. Our method is named after the musical instrument clarinet, whose sound resembles human voice.2 Audio samples are in https://clarinet-demo.github.io/ arXiv:1807.07281v2 [cs.CL] 30 Jul 2018",
    "prev": "In previous work, likelihood-based models, including autoregressive models (van den Oord et al., 2016;Kalchbrenner et al., 2018; and flow-based models Prenger et al., 2019;, have predominated in audio synthesis because of the simple training objective and superior ability of modeling the fine details of waveform in real data.",
    "curr": "There are other waveform models, which often require auxiliary losses for training, such as flow-based models trained by distillation Ping et al., 2019), variational auto-encoder (VAE) based model , and generative adversarial network (GAN) based models Yamamoto et al., 2020).",
    "next": "Most of previous waveform models focus on audio synthesis with informative local conditioner (e.g., mel spectrogram or aligned linguistic features), with only a few exceptions in unconditional generation ."
  },
  {
    "index": 1124,
    "source_corpus_id": 247996510,
    "ref_id": "b4",
    "citation_corpus_id": 235368289,
    "start": 4765,
    "end": 4786,
    "title": "Measuring and Improving BERT's Mathematical Abilities by Predicting the Order of Reasoning",
    "abstract": "Imagine you are in a supermarket. You have two bananas in your basket and want to buy four apples. How many fruits do you have in total? This seemingly straightforward question can be challenging for data-driven language models, even if trained at scale. However, we would expect such generic language models to possess some mathematical abilities in addition to typical linguistic competence. Towards this goal, we investigate if a commonly used language model, BERT, possesses such mathematical abilities and, if so, to what degree. For that, we fine-tune BERT on a popular dataset for word math problems, AQuA-RAT, and conduct several tests to understand learned representations better. Since we teach models trained on natural language to do formal mathematics, we hypothesize that such models would benefit from training on semi-formal steps that explain how math results are derived. To better accommodate such training, we also propose new pretext tasks for learning mathematical rules. We call them (Neighbor) Reasoning Order Prediction (ROP or NROP). With this new model, we achieve significantly better outcomes than data-driven baselines and even on-par with more tailored models. We also show how to reduce positional bias in such models.",
    "prev": "(2020) showed that SATNet (Wang et al., 2019) could not solve visual Sudoku without using intermediate labels to identify individual Sudoku digit images.",
    "curr": "Similar limitations were observed in language related compounded tasks, including commonsense reasoning (Liu et al., 2022;Wei et al., 2022;Zelikman et al., 2022), math word problems (Piękos et al., 2021;Wei et al., 2022), and programs execution (Nye et al., 2022).",
    "next": "The go-to architectures in this domain are powerful language models, which are trained as sequence-to-sequence models over text."
  },
  {
    "index": 1129,
    "source_corpus_id": 257757426,
    "ref_id": "b20",
    "citation_corpus_id": 238634325,
    "start": 7554,
    "end": 7578,
    "title": "OFFLINE REINFORCEMENT LEARNING WITH IMPLICIT Q-LEARNING",
    "abstract": "Offline reinforcement learning requires reconciling two conflicting aims: learning a policy that improves over the behavior policy that collected the dataset, while at the same time minimizing the deviation from the behavior policy so as to avoid errors due to distributional shift. This trade-off is critical, because most current offline reinforcement learning methods need to query the value of unseen actions during training to improve the policy, and therefore need to either constrain these actions to be in-distribution, or else regularize their values. We propose a new offline RL method that never needs to evaluate actions outside of the dataset, but still enables the learned policy to improve substantially over the best behavior in the data through generalization. The main insight in our work is that, instead of evaluating unseen actions from the latest policy, we can approximate the policy improvement step implicitly by treating the state value function as a random variable, with randomness determined by the action (while still integrating over the dynamics to avoid excessive optimism), and then taking a state conditional upper expectile of this random variable to estimate the value of the best actions in that state. This leverages the generalization capacity of the function approximator to estimate the value of the best available action at a given state without ever directly querying a Q-function with this unseen action. Our algorithm alternates between fitting this upper expectile value function and backing it up into a Q-function, without any explicit policy. Then, we extract the policy via advantage-weighted behavioral cloning, which also avoids querying out-of-sample actions. We dub our method implicit Q-learning (IQL). IQL is easy to implement, computationally efficient, and only requires fitting an additional critic with an asymmetric L2 loss. 1 IQL demonstrates the state-of-the-art performance on D4RL, a standard benchmark for offline reinforcement learning. We also demonstrate that IQL achieves strong performance fine-tuning using online interaction after offline initialization.",
    "prev": "This setting is particularly attractive for applications where there is previous logged experience available but online data collection is expensive (e.g., robotics, healthcare).",
    "curr": "Recently, the field of offline RL has made significant progress, and many offline RL algorithms have been proposed to learn improved policies from diverse and sub-optimal offline demonstrations Fujimoto & Gu, 2021;Kostrikov et al., 2022c;.",
    "next": "Offline RL research typically assumes that the offline dataset is reward-annotated."
  },
  {
    "index": 1130,
    "source_corpus_id": 239769065,
    "ref_id": "b30",
    "citation_corpus_id": 3687922,
    "start": 2541,
    "end": 2544,
    "title": "SEMI-PARAMETRIC TOPOLOGICAL MEMORY FOR NAVIGATION",
    "abstract": "We introduce a new memory architecture for navigation in previously unseen environments, inspired by landmark-based navigation in animals. The proposed semiparametric topological memory (SPTM) consists of a (non-parametric) graph with nodes corresponding to locations in the environment and a (parametric) deep network capable of retrieving nodes from the graph based on observations. The graph stores no metric information, only connectivity of locations corresponding to the nodes. We use SPTM as a planning module in a navigation system. Given only 5 minutes of footage of a previously unseen maze, an SPTM-based navigation agent can build a topological map of the environment and use it to confidently navigate towards goals. The average success rate of the SPTM agent in goal-directed navigation across test environments is higher than the best-performing baseline by a factor of three.",
    "prev": "Some prior methods approach this problem by performing search or optimization over subgoals at test time.",
    "curr": "However, these test-time planning methods either rely on graph search [7,31], which scales poorly with dimensionality [14], or continuous optimization over subgoals [26], which is expensive and can result in model exploitation.",
    "next": "In this paper, we take a different tack and instead use search at training time to automatically generate a curriculum."
  },
  {
    "index": 1131,
    "source_corpus_id": 248810913,
    "ref_id": "b30",
    "citation_corpus_id": 108300988,
    "start": 2602,
    "end": 2624,
    "title": "WHAT DO YOU LEARN FROM CONTEXT? PROBING FOR SENTENCE STRUCTURE IN CONTEXTUALIZED WORD REPRESENTATIONS",
    "abstract": "Contextualized representation models such as ELMo(Peters et al., 2018a)andBERT (Devlin et al., 2018)have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.",
    "prev": "A large number of studies have been devoted towards interpreting DNNs.",
    "curr": "A major line of research work has focused on DNNs in interpreting deep Natural Language Processing (NLP) models and their ability to learn various pre-defined linguistic concepts (Tenney et al., 2019b;Liu et al., 2019a).",
    "next": "More specifically, they rely on pre-defined linguistic concepts such as: parts-of-speech tags and semantic tags, and probe whether the specific linguistic knowledge is learned in various parts of the network."
  },
  {
    "index": 1132,
    "source_corpus_id": 52881014,
    "ref_id": "b25",
    "citation_corpus_id": 10082291,
    "start": 14948,
    "end": 14975,
    "title": "Fast Adaptation in Generative Models with Generative Matching Networks",
    "abstract": "Despite recent advances, the remaining bottlenecks in deep generative models are necessity of extensive training and difficulties with generalization from small number of training examples. We develop a new generative model called Generative Matching Network which is inspired by the recently proposed matching networks for one-shot learning in discriminative tasks. By conditioning on the additional input dataset, our model can instantly learn new concepts that were not available in the training data but conform to a similar generative process. The proposed framework does not explicitly restrict diversity of the conditioning data and also does not require an extensive inference procedure for training or adaptation. Our experiments on the Omniglot dataset demonstrate that Generative Matching Networks significantly improve predictive performance on the fly as more additional data is available and outperform existing state of the art conditional generative models.",
    "prev": "This method has shown promise in robotics (Finn et al., 2017b;Yu et al., 2018).",
    "curr": "In generative modeling, few-shot learning has been addressed from several perspectives, including matching networks (Bartunov and Vetrov, 2017) and variable inference for memory addressing (Bornschein et al., 2017).",
    "next": "Rezende et al."
  },
  {
    "index": 1133,
    "source_corpus_id": 1091965,
    "ref_id": "b22",
    "citation_corpus_id": 14124313,
    "start": 2130,
    "end": 2158,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "(2012) developed AlexNet, an ImageNet-winning CNN with more than 1.1 × 10 9 multiplies.",
    "curr": "In 2014, ImageNetwinning and runner up CNNs increased the number of multiplies to 1.4 × 10 9 (Szegedy et al., 2015) and 1.6 × 10 10 (Simonyan & Zisserman, 2015) respectively.",
    "next": "Despite the powerful representational ability of large scale CNNs, their computational workload prohibits deployment on mobile devices."
  },
  {
    "index": 1134,
    "source_corpus_id": 218901036,
    "ref_id": "b9",
    "citation_corpus_id": 6706414,
    "start": 2472,
    "end": 2476,
    "title": "Published as a conference paper at ICLR 2015 EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES",
    "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples-inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
    "prev": "Motivation and Contributions\n\nThe outputs of deep networks are known to be very sensitive to small perturbations to the input.",
    "curr": "This sensitivity can be exploited to create adversarial examples that undermine robustness by causing trained networks to produce defective results from input changes that are imperceptible to a human [10].",
    "next": "The adversarial scenarios studied in this paper are primarily untargeted white-box attacks on image classification networks."
  },
  {
    "index": 1135,
    "source_corpus_id": 237593074,
    "ref_id": "b10",
    "citation_corpus_id": 225039882,
    "start": 7314,
    "end": 7339,
    "title": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. 1 arXiv:2010.11929v2 [cs.CV] 3 Jun 2021Published as a conference paper at ICLR 2021 inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.However, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks.RELATED WORKTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global selfattention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.",
    "prev": "Architecture We use an encoder-decoder architecture.",
    "curr": "The encoder can be a general image encoder that perceives pixels and encodes them into hidden representations, such as a ConvNet (LeCun et al., 1989;Krizhevsky et al., 2012;He et al., 2016), Transformer (Vaswani et al., 2017;Dosovitskiy et al., 2020), or their combination (Carion et al., 2020).",
    "next": "For generation we use a Transformer decoder, widely used in modern language modeling (Radford et al., 2018;Raffel et al., 2019)."
  },
  {
    "index": 1138,
    "source_corpus_id": 235358707,
    "ref_id": "b14",
    "citation_corpus_id": 4009713,
    "start": 1860,
    "end": 1881,
    "title": "Published as a conference paper at ICLR 2018 UNSUPERVISED REPRESENTATION LEARNING BY PRE- DICTING IMAGE ROTATIONS",
    "abstract": "Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training Con-vNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet.",
    "prev": "The source code is available at https://github.com/tsaishien-chen/IFND.",
    "curr": "INTRODUCTION\n\nSelf-supervised learning of visual representation (Doersch et al., 2015;Pathak et al., 2016;Noroozi & Favaro, 2016;Oord et al., 2018;Gidaris et al., 2018;Chen et al., 2020a) aims to learn a semanticaware embedding space based on the image data without the supervision of human-labeled annotations.",
    "next": "Recently, significant advances have been made by contrastive learning approaches (Hjelm et al., 2019;Tian et al., 2019;Chen et al., 2020b;He et al., 2020;Chen et al., 2020d) to reduce the performance gap with the supervised counterparts."
  },
  {
    "index": 1139,
    "source_corpus_id": 248377648,
    "ref_id": "b11",
    "citation_corpus_id": 28202810,
    "start": 33316,
    "end": 33340,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "In our experiments we use expert policies to collect offline datasets for the pre-training tasks of standing, forward walking, and backward walking respectively.",
    "curr": "To collect these datasets, we pre-train polices with SAC (Haarnoja et al., 2018a) in the state space and collect rollouts of the visual observation.",
    "next": "We test our representation on the downstream task of \"running\"."
  },
  {
    "index": 1140,
    "source_corpus_id": 210932183,
    "ref_id": "b31",
    "citation_corpus_id": 12713052,
    "start": 1608,
    "end": 1625,
    "title": "NEURAL ARCHITECTURE SEARCH WITH REINFORCEMENT LEARNING",
    "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214. * Work done as a member of the Google Brain Residency program (g.co/brainresidency.) Under review as a conference paper at ICLR 2017 neural network can be typically specified by a variable-length string. It is therefore possible to use a recurrent network -the controller -to generate such string. Training the network specified by the string -the \"child network\" -on the real data will result in an accuracy on a validation set. Using this accuracy as the reward signal, we can compute the policy gradient to update the controller. As a result, in the next iteration, the controller will give higher probabilities to architectures that receive high accuracies. In other words, the controller will learn to improve its search over time.Our experiments show that Neural Architecture Search can design good models from scratch, an achievement considered not possible with other methods. On image recognition with CIFAR-10, Neural Architecture Search can find a novel ConvNet model that is better than most human-invented architectures. Our CIFAR-10 model achieves a 3.65 test set error, while being 1.05x faster than the current best model. On language modeling with Penn Treebank, Neural Architecture Search can design a novel recurrent cell that is also better than previous RNN and LSTM architectures. The cell that our model found achieves a test set perplexity of 62.4 on the Penn Treebank dataset, which is 3.6 perplexity better than the previous state-of-the-art.",
    "prev": "To showcase the framework, we compare several state-of-the-art one-shot NAS methods, examine how sensitive they are to their hyperparameters and how they can be improved by tuning their hyperparameters, and compare their performance to that of blackbox optimizers for NAS-Bench-101.",
    "curr": "INTRODUCTION\n\nWhile neural architecture search (NAS) has attracted a lot of attention due to the effectiveness in automatically designing state-of-the-art neural networks (Zoph & Le, 2017;Real et al., 2017;, the focus has recently shifted to making the search process more efficient (Pham et al., 2018;Elsken et al., 2019;Xie et al., 2019;Cai et al., 2019;Casale et al., 2019).",
    "next": "The most crucial concept which led to a reduction in search costs to the order of a single function evaluation is certainly the weight-sharing paradigm: Training only a single large architecture (the one-shot model) subsuming all the possible architectures in the search space (Brock et al., 2018;Pham et al., 2018)."
  },
  {
    "index": 1141,
    "source_corpus_id": 49667227,
    "ref_id": "b66",
    "citation_corpus_id": 22014305,
    "start": 41491,
    "end": 41494,
    "title": "ROUTING NETWORKS: ADAPTIVE SELECTION OF NON-LINEAR FUNCTIONS FOR MULTI-TASK LEARN- ING",
    "abstract": "Multi-task learning (MTL) with neural networks leverages commonalities in tasks to improve performance, but often suffers from task interference which reduces the benefits of transfer. To address this issue we introduce the routing network paradigm, a novel neural network and training algorithm. A routing network is a kind of self-organizing neural network consisting of two components: a router and a set of one or more function blocks. A function block may be any neural network -for example a fully-connected or a convolutional layer. Given an input the router makes a routing decision, choosing a function block to apply and passing the output back to the router recursively, terminating when a fixed recursion depth is reached. In this way the routing network dynamically composes different function blocks for each input. We employ a collaborative multi-agent reinforcement learning (MARL) approach to jointly train the router and function blocks. We evaluate our model against cross-stitch networks and shared-layer baselines on multi-task settings of the MNIST, mini-imagenet, and CIFAR-100 datasets. Our experiments demonstrate a significant improvement in accuracy, with sharper convergence. In addition, routing networks have nearly constant per-task training cost while cross-stitch networks scale linearly with the number of tasks. On CIFAR-100 (20 tasks) we obtain cross-stitch performance levels with an 85% reduction in training time.",
    "prev": "Finding such structure is important for generalization because it shields various parts of the representation or program from changing when one component changes.",
    "curr": "For example, [24,42,67] have begun to examine how to route through an fixed architecture to encapsulate computation, but the hard limitation that prevents fixed architectures from scaling to problems with complexity beyond that which they were trained is their limited number of computation steps.",
    "next": "A parallel line of work runs through the hierarchical reinforcement learning (HRL) literature [9], with recent work [8,27,50,58,84] attempting to learn both lower-level policies as well as a higher-level policy that calls them."
  },
  {
    "index": 1142,
    "source_corpus_id": 219792972,
    "ref_id": "b24",
    "citation_corpus_id": 3568073,
    "start": 7236,
    "end": 7240,
    "title": "PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION",
    "abstract": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at 1024 2 . We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CELEBA dataset.",
    "prev": "MSGAN (aka Miss-GAN) (23) aims to solve the missing mode problem of conditional GANs through a regularization term that maximizes the distance between the generated images with respect to the distance between their corresponding input latent codes.",
    "curr": "Progressive GANs (24) are growing both the generator and discriminator progressively, and approach resembling the layer-wise training of autoencoders.",
    "next": "Method\n\n\nPreliminaries\n\nWe define an N -way, K (tr) -shot supervised classification task, T , as a set D (tr) T composed of i ∈ {1, ."
  },
  {
    "index": 1146,
    "source_corpus_id": 258988005,
    "ref_id": "b11",
    "citation_corpus_id": 237605600,
    "start": 7790,
    "end": 7806,
    "title": "Published as a conference paper at ICLR 2022 SQRT(D) DIMENSION DEPENDENCE OF LANGEVIN MONTE CARLO",
    "abstract": "This article considers the popular MCMC method of unadjusted Langevin Monte Carlo (LMC) and provides a non-asymptotic analysis of its sampling error in 2-Wasserstein distance. The proof is based on a refinement of mean-square analysis in Li et al.(2019), and this refined framework automates the analysis of a large class of sampling algorithms based on discretizations of contractive SDEs. Using this framework, we establish an O √ d / mixing time bound for LMC, without warm start, under the common log-smooth and log-strongly-convex conditions, plus a growth condition on the 3rd-order derivative of the potential of target measures. This bound improves the best previously known O d / result and is optimal (in terms of order) in both dimension d and accuracy tolerance for target measures satisfying the aforementioned assumptions. Our theoretical analysis is further validated by numerical experiments.",
    "prev": "However, we have been particularly motivated by the research on Langevin MCMC which is a class of gradientbased sampling algorithms obtained by discretizing the Langevin diffusion (Parisi, 1981).",
    "curr": "There is a growing body of work on the analysis of Langevin MCMC algorithms of various complexity (overdampled, Metropolis-adjusted, underdamped, higher-order) for sampling from log-concave distributions (Dalalyan, 2017;Durmus & Moulines, 2017;Cheng et al., 2018;Dwivedi et al., 2018;Shen & Lee, 2019;Cao et al., 2020;Mou et al., 2021;Li et al., 2022).",
    "next": "In our experiments, we compare several recent Langevin MCMC methods, which may be of independent interest."
  },
  {
    "index": 1151,
    "source_corpus_id": 227054483,
    "ref_id": "b6",
    "citation_corpus_id": 52967399,
    "start": 2962,
    "end": 2983,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "In other fields of machine learning, the use of large prior datasets to bootstrap acquisition of new capabilities has been studied extensively to good effect.",
    "curr": "For example, language models trained on large, diverse datasets offer representations that drastically improve the efficiency of learning downstream tasks (Devlin et al., 2019).",
    "next": "What would be the analogue of this kind of pre-training in robotics and RL?"
  },
  {
    "index": 1153,
    "source_corpus_id": 235390683,
    "ref_id": "b27",
    "citation_corpus_id": 3292002,
    "start": 2577,
    "end": 2601,
    "title": "GRAPH ATTENTION NETWORKS",
    "abstract": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-theart results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a proteinprotein interaction dataset (wherein test graphs remain unseen during training).",
    "prev": "Due to their prevalence and rich descriptive capacity, pattern mining and discovery on graph data is a prominent research area with powerful implications.",
    "curr": "As the generalization of deep neural networks on graph data, graph neural networks (GNNs) have proved to be powerful in learning representations for graphs and associated entities (nodes, edges, subgraphs), and they have been employed in various applications such as node classification (Kipf & Welling, 2016a;Veličković et al., 2018), node clustering (Pan et al., 2018), recommender systems (Ying et al., 2018) and drug discovery (Duvenaud et al., 2015).",
    "next": "In recent years, the explosive interest in self-supervised learning (SSL) has suggested its great potential in empowering stronger neural networks in an unsupervised manner Kolesnikov et al., 2019;Doersch et al., 2015)."
  },
  {
    "index": 1154,
    "source_corpus_id": 244773230,
    "ref_id": "b17",
    "citation_corpus_id": 204206524,
    "start": 35508,
    "end": 35534,
    "title": "Published as a conference paper at ICLR 2020 DYNAMICAL DISTANCE LEARNING FOR SEMI-SUPERVISED AND UNSUPERVISED SKILL DISCOVERY",
    "abstract": "Reinforcement learning requires manual specification of a reward function to learn a task. While in principle this reward function only needs to specify the task goal, in practice reinforcement learning can be very time-consuming or even infeasible unless the reward function is shaped so as to provide a smooth gradient towards a successful outcome. This shaping is difficult to specify by hand, particularly when the task is learned from raw observations, such as images. In this paper, we study how we can automatically learn dynamical distances: a measure of the expected number of time steps to reach a given goal state from any other state. These dynamical distances can be used to provide well-shaped reward functions for reaching new goals, making it possible to learn complex tasks efficiently. We show that dynamical distances can be used in a semi-supervised regime, where unsupervised interaction with the environment is used to learn the dynamical distances, while a small amount of preference supervision is used to determine the task goal, without any manually engineered reward function or goal examples. We evaluate our method both on a real-world robot and in simulation. We show that our method can learn to turn a valve with a real-world 9-DoF hand, using raw image observations and just ten preference labels, without any other supervision. Videos of the learned skills can be found on the project website: https://sites.google.com/view/dynamical-distance-learning.",
    "prev": "In the future, we would like to extend this work to multi-task settings, employ vision and move towards the context of batch RL, e.g.",
    "curr": "through employing distance learning techniques (Hartikainen et al., 2020) to build on more informative goal spaces.",
    "next": "We measure the performance of the learnt representations to encode progress by running a KNN classification."
  },
  {
    "index": 1158,
    "source_corpus_id": 54445324,
    "ref_id": "b3",
    "citation_corpus_id": 3503217,
    "start": 25596,
    "end": 25621,
    "title": "Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments",
    "abstract": "Ability to continuously learn and adapt from limited experience in nonstationary environments is an important milestone on the path towards general intelligence. In this paper, we cast the problem of continuous adaptation into the learning-to-learn framework. We develop a simple gradient-based meta-learning algorithm suitable for adaptation in dynamically changing and adversarial scenarios. Additionally, we design a new multi-agent competitive environment, RoboSumo, and define iterated adaptation games for testing various aspects of continuous adaptation strategies. We demonstrate that meta-learning enables significantly more efficient adaptation than reactive baselines in the few-shot regime. Our experiments with a population of agents that learn and compete suggest that meta-learners are the fittest.While virtually any changes in an environment could induce some kind of nonstationarity (e.g., changes in the physics or characteristics of the agent), environments with multiple agents are particularly challenging due to complexity of the emergent behavior and are of practical interest with applications ranging from multiplayer games [16] to coordinating self-driving fleets[17]. Multi-agent environments are nonstationary from the perspective of any individual agent since all actors are learning and changing concurrently[7,18]. In this paper, we consider the problem of continuous adaptation to a learning opponent in a competitive multi-agent setting.To this end, we design RoboSumo-a 3D environment with simulated physics that allows pairs of agents to compete against each other. To test continuous adaptation, we introduce iterated adaptation games-a new setting where a trained agent competes against the same opponent for multiple rounds of a repeated game, while both are allowed to update their policies and change their behaviors between the rounds. In such iterated games, from the agent's perspective, the environment changes from round to round, and the agent ought to adapt in order to win the game. Additionally, the competitive component of the environment makes it not only nonstationary but also adversarial, which provides a natural training curriculum and encourages learning robust strategies[7,19,20].We evaluate our meta-learning agents along with a number of baselines on a (single-agent) locomotion task with handcrafted nonstationarity and on iterated adaptation games in RoboSumo. Our results demonstrate that meta-learned adaptation strategies clearly dominate other adaptation methods in the few-shot regime in both single-and multi-agent settings. Finally, we carry out a large-scale experiment where we train a diverse population of agents with different anatomies, policy architectures, and adaptation methods, and make them interact by competing against each other in iterated games. We evaluate the agents based on their TrueSkills[21]in these games, as well as evolve the population as whole for a few generations-the agents that lose disappear, while the winners get duplicated. Our results suggest that the agents with meta-learned adaptation strategies end up being the fittest. Videos that demonstrate adaptation behaviors in different tasks are available at https://goo.gl/tboqaN.",
    "prev": "More relevant to our framework are approaches that parameterize the training process through a recurrent neural network that takes the gradient as input and produces a new set of parameters (Ravi & Larochelle, 2016;Santoro et al., 2016;Andrychowicz et al., 2016;Hochreiter et al., 2001).",
    "curr": "The approach most closely related to us learns an initialization such that the model can adapt to a new task through one or a few gradient updates (Finn et al., 2017;Nichol et al., 2018;Al-Shedivat et al., 2017;Lee & Choi, 2018).",
    "next": "In contrast to our work, these methods focus exclusively on few-shot learning, where the gradient path is trivial as only a single or a handful of training steps are allowed, limiting them to settings where the current task is closely related to previous ones."
  },
  {
    "index": 1160,
    "source_corpus_id": 3290366,
    "ref_id": "b1",
    "citation_corpus_id": 3495200,
    "start": 3105,
    "end": 3108,
    "title": "LEARNING TO REPRESENT PROGRAMS WITH GRAPHS",
    "abstract": "Learning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code's known syntax. For example, long-range dependencies induced by using the same variable or function in distant locations are often not considered. We propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures. In this work, we present how to construct graphs from source code and how to scale Gated Graph Neural Networks training to such large graphs. We evaluate our method on two tasks: VARNAMING, in which a network attempts to predict the name of a variable given its usage, and VARMISUSE, in which the network learns to reason about selecting the correct variable that should be used at a given program location. Our comparison to methods that use less structured program representations shows the advantages of modeling known structure, and suggests that our models learn to infer meaningful names and to solve the VARMISUSE task in many cases. Additionally, our testing showed that VARMISUSE identifies a number of bugs in mature open-source projects.",
    "prev": "Examples include prediction of properties of chemical molecules [9], answering questions about knowledge graphs [25], natural language processing with parsestructured inputs (trees or richer structures like Abstract Meaning Representations) [4], predicting properties of data structures or source code in programming * Work done partially while author was at Microsoft Research.",
    "curr": "languages [2,22], and making predictions from scene graphs [39].",
    "next": "Sequence data can be seen as a special case of a simple chain-structured graph."
  },
  {
    "index": 1161,
    "source_corpus_id": 254926905,
    "ref_id": "b2",
    "citation_corpus_id": 17682909,
    "start": 2924,
    "end": 2926,
    "title": "Spectral Networks and Deep Locally Connected Networks on Graphs",
    "abstract": "Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for lowdimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.",
    "prev": "Can we quantitatively model the effect of applying a finite number of graph convolutions and theoretically predict the \"sweet spot\" for the choice of depth?In this paper, we propose a non-asymptotic analysis framework to study the effects of graph convolutions and oversmoothing using the Contextual Stochastic Block Model (CSBM)[18].",
    "curr": "The CSBM mimics the community structure\n\nIntroduction\n\nGraph Neural Networks (GNNs) are a powerful framework for learning with graph-structured data and have shown great promise in diverse domains such as molecular chemistry, physics, and social network analysis [1,2,3,4,5,6,7].",
    "next": "Most GNN models are built by stacking graph convolutions or message-passing layers [8], where the representation of each node is computed by recursively aggregating and transforming the representations of its neighboring nodes."
  },
  {
    "index": 1162,
    "source_corpus_id": 235613482,
    "ref_id": "b21",
    "citation_corpus_id": 214220671,
    "start": 10830,
    "end": 10848,
    "title": "ABSTRACT DIAGRAMMATIC REASONING WITH MULTIPLEX GRAPH NETWORKS",
    "abstract": "reasoning, particularly in the visual domain, is a complex human ability, but it remains a challenging problem for artificial neural learning systems. In this work we propose MXGNet, a multilayer graph neural network for multi-panel diagrammatic reasoning tasks. MXGNet combines three powerful concepts, namely, object-level representation, graph neural networks and multiplex graphs, for solving visual reasoning tasks. MXGNet first extracts object-level representations for each element in all panels of the diagrams, and then forms a multi-layer multiplex graph capturing multiple relations between objects across different diagram panels. MXGNet summarises the multiple graphs extracted from the diagrams of the task, and uses this summarisation to pick the most probable answer from the given candidates. We have tested MXGNet on two types of diagrammatic reasoning tasks, namely Diagram Syllogisms and Raven Progressive Matrices (RPM). For an Euler Diagram Syllogism task MXGNet achieves state-of-the-art accuracy of 99.8%. For PGM and RAVEN, two comprehensive datasets for RPM reasoning, MXGNet outperforms the state-of-the-art models by a considerable margin.",
    "prev": "(2019) proposed a Logic Embedding Network (LEN) with distracting features, which also uses the auxiliary annotations to boost the performance and designed a teacher model to control the learning trajectories.",
    "curr": "MXGNet (Wang et al., 2020) is a multi-layer graph neural network for multi-panel diagrammatic reasoning tasks.",
    "next": "For better performance, MXGNet also uses auxiliary annotations for model training."
  },
  {
    "index": 1164,
    "source_corpus_id": 247922261,
    "ref_id": "b18",
    "citation_corpus_id": 8348149,
    "start": 2513,
    "end": 2534,
    "title": "Learning Effective and Interpretable Semantic Models using Non-Negative Sparse Embedding",
    "abstract": "In this paper, we introduce an application of matrix factorization to produce corpus-derived, distributional models of semantics that demonstrate cognitive plausibility. We find that word representations learned by Non-Negative Sparse Embedding (NNSE), a variant of matrix factorization, are sparse, effective, and highly interpretable. To the best of our knowledge, this is the first approach which yields semantic representation of words satisfying these three desirable properties. Though extensive experimental evaluations on multiple real-world tasks and datasets, we demonstrate the superiority of semantic models learned by NNSE over other state-of-the-art baselines.",
    "prev": "For this we draw inspiration from overcomplete representations: representations of an input that are nonunique combinations of a number of basis vectors greater than the input's dimensionality (Lewicki & Sejnowski, 2000).",
    "curr": "Mostly studied in the context of the sparse coding literature (Gregor & LeCun, 2010;Goodfellow et al., 2012;Olshausen, 2013), sparse overcomplete representations have been shown to increase stability in the presence of noise (Donoho et al., 2006), have applications in neuroscience (Olshausen & Field, 1996;Lee et al., 2007), and lead to more interpretable representations (Murphy et al., 2012;Fyshe et al., 2015;Faruqui et al., 2015).",
    "next": "However, the choice of basis vectors is generally assumed to be learned using traditional methods such as ICA (Teh et al., 2003) or fitting linear models (Lewicki & Sejnowski, 2000), limiting the expressive power of the encoding function."
  },
  {
    "index": 1165,
    "source_corpus_id": 222133257,
    "ref_id": "b4",
    "citation_corpus_id": 5590763,
    "start": 6162,
    "end": 6165,
    "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
    "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
    "prev": "By unrolling this algorithm, the authors demonstrate superior recovery over ALISTA for a specific setting of M, N and s.\n\nIn a related approach [20] identify undershooting, meaning that reconstructed components are smaller than target components, as a shortcoming of LISTA and propose Gated-LISTA to address these issues.",
    "curr": "The authors introduce gain and overshoot gates to LISTA, which can amplify the reconstruction after each iteration before and after thresholding, yielding an architecture resembling GRU cells [5].",
    "next": "The authors demonstrate better sparse reconstruction than previous LISTA-variants and also show that adding their proposed gates to ALISTA, named AGLISTA, it is possible to improve its performance in the same setting of M, N and s as ALISTA-AT."
  },
  {
    "index": 1166,
    "source_corpus_id": 231698498,
    "ref_id": "b58",
    "citation_corpus_id": 202888885,
    "start": 2681,
    "end": 2700,
    "title": "Published as a conference paper at ICLR 2020 DRAWING EARLY-BIRD TICKETS: TOWARDS MORE EF- FICIENT TRAINING OF DEEP NETWORKS",
    "abstract": "Frankle & Carbin, 2019)shows that there exist winning tickets (small but critical subnetworks) for dense, randomly initialized networks, that can be trained alone to achieve a comparable accuracy to the latter in a similar number of iterations. However, the identification of these winning tickets still requires the costly train-prune-retrain process, limiting their practical benefits. In this paper, we discover for the first time that the winning tickets can be identified at a very early training stage, which we term as Early-Bird (EB) tickets, via lowcost training schemes (e.g., early stopping and low-precision training) at large learning rates. Our finding on the existence of EB tickets is consistent with recently reported observations that the key connectivity patterns of neural networks emerge early. Furthermore, we propose a mask distance metric that can be used to identify EB tickets with a low computational overhead, without needing to know the true winning tickets that emerge after the full training. Finally, we leverage the existence of EB tickets and the proposed mask distance to develop efficient training methods, which are achieved by first identifying EB tickets via low-cost schemes, and then continuing to train merely the EB tickets towards the target accuracy. Experiments based on various deep networks and datasets validate: 1) the existence of EB tickets and the effectiveness of mask distance in efficiently identifying them; and 2) that the proposed efficient training via EB tickets can achieve up to 5.8× ∼ 10.7× energy savings while maintaining comparable or even better accuracy as compared to the most competitive state-ofthe-art training methods, demonstrating a promising and easily adopted method for tackling the often cost-prohibitive deep network training.",
    "prev": "INTRODUCTION\n\nThe record-breaking performance of modern deep neural networks (DNNs) comes at a prohibitive training cost due to the required massive training data and parameters, limiting the development of the highly demanded DNN-powered intelligent solutions for numerous applications Wu et al., 2018).",
    "curr": "As an illustration, training ResNet-50 involves 10 18 FLOPs (floating-point operations) and can take 14 days on one state-of-the-art (SOTA) GPU (You et al., 2020b).",
    "next": "Meanwhile, the large DNN training costs have raised increasing financial and environmental concerns."
  },
  {
    "index": 1168,
    "source_corpus_id": 259298217,
    "ref_id": "b42",
    "citation_corpus_id": 246652474,
    "start": 34913,
    "end": 34929,
    "title": "GRAPH-RELATIONAL DOMAIN ADAPTATION",
    "abstract": "Existing domain adaptation methods tend to treat every domain equally and align them all perfectly. Such uniform alignment ignores topological structures among different domains; therefore it may be beneficial for nearby domains, but not necessarily for distant domains. In this work, we relax such uniform alignment by using a domain graph to encode domain adjacency, e.g., a graph of states in the US with each state as a domain and each edge indicating adjacency, thereby allowing domains to align flexibly based on the graph structure. We generalize the existing adversarial learning framework with a novel graph discriminator using encodingconditioned graph embeddings. Theoretical analysis shows that at equilibrium, our method recovers classic domain adaptation when the graph is a clique, and achieves non-trivial alignment for other types of graphs. Empirical results show that our approach successfully generalizes uniform alignment, naturally incorporates domain information represented by graphs, and improves upon existing domain adaptation methods on both synthetic and real-world datasets 1 . * Work conducted during internship at AWS AI Labs. 1 Code will soon be available at https://github.com/Wang-ML-Lab/GRDA arXiv:2202.03628v2 [cs.LG] 21 Apr 2023Published as a conference paper at ICLR 2022One naïve DA method for such graph-relational domains is to perform DA for each pair of neighboring domains separately. Unfortunately, due to the strict alignment between each domain pair, this method will still lead to uniform alignment so long as the graph is connected. To generalize DA to the graphrelational domains, we argue that an ideal method should (1) only enforce uniform alignment when the domain graph is a clique (i.e., every two domains are adjacent), and (2) more importantly, relax uniform alignment to adapt more flexibly across domains according to any non-clique domain graph, thereby naturally incorporating information on the domain adjacency. In this paper, we generalize adversarial DA methods and replace the traditional binary (or multi-class) discriminator with a novel graph discriminator: instead of distinguishing among different domains, our graph discriminator takes as input the encodings of data to reconstruct the domain graph. We show that our method enjoys the following theoretical guarantees: it recovers classic DA when the the domain graph is a clique, and realizes intuitive alignments for other types of graphs such as chains and stars (seeFig. 4). We summarize our contributions as follows:• We propose to use a graph to characterize domain relations and develop graph-relational domain adaptation (GRDA) as the first general adversarial DA method to adapt across domains living on a graph. • We provide theoretical analysis showing that at equilibrium, our method can retain the capability of uniform alignment when the domain graph is a clique, and achieve non-trivial alignment for other types of graphs. • Empirical results on both synthetic and real-world datasets demonstrate the superiority of our method over the state-of-the-art DA methods. Eric Granger. Unsupervised multi-target domain adaptation through knowledge distillation. In . Sentry: Selective entropy optimization via committee consistency for unsupervised domain adaptation. In Bridging theory and algorithm for domain adaptation. arXiv preprint arXiv:1904.05801, 2019.",
    "prev": "CLASSIFICATION\n\nDatasets.",
    "curr": "DG-15 (Xu et al., 2022) is a synthetic binary classification dataset with 15 groups.",
    "next": "Each group contains 100 data points."
  },
  {
    "index": 1170,
    "source_corpus_id": 231719359,
    "ref_id": "b29",
    "citation_corpus_id": 52900371,
    "start": 3378,
    "end": 3391,
    "title": "FLUCTUATION-DISSIPATION RELATIONS FOR STOCHASTIC GRADIENT DESCENT",
    "abstract": "The notion of the stationary equilibrium ensemble has played a central role in statistical mechanics. In machine learning as well, training serves as generalized equilibration that drives the probability distribution of model parameters toward stationarity. Here, we derive stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in the stochastic gradient descent algorithm. These relations hold exactly for any stationary state and can in particular be used to adaptively set training schedule. We can further use the relations to efficiently extract information pertaining to a loss-function landscape such as the magnitudes of its Hessian and anharmonicity. Our claims are empirically verified.",
    "prev": " this SDE analogy assumes that each minibatch is randomly sampled from the full dataset, which implies that some examples will be sampled multiple times in one epoch.",
    "curr": "Furthermore, the most common SDE analogy holds only for vanishing learning rates (Yaida, 2019) and therefore misses the generalization benefits of finite learning rates which we identify in this work.",
    "next": "An important exception is Li et al."
  },
  {
    "index": 1172,
    "source_corpus_id": 257496038,
    "ref_id": "b11",
    "citation_corpus_id": 208547755,
    "start": 3566,
    "end": 3586,
    "title": "Published as a conference paper at ICLR 2020 DREAM TO CONTROL: LEARNING BEHAVIORS BY LATENT IMAGINATION",
    "abstract": "Learned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.",
    "prev": "Predictions of the reward r t , discount factor γ t , and next latent state z t+1 are computed based on h t using MLPs.",
    "curr": "o t− z t− ô t− a t− h t− r t− · · · · · ·\n1 arXiv:2303.07109v1 [cs.LG] 13 Mar 2023\n\nPublished as a conference paper at ICLR 2023 A promising model-based concept is learning in imagination (Ha & Schmidhuber, 2018;Kaiser et al., 2020;Hafner et al., 2020;Hafner et al., 2021): instead of learning behaviors from the collected experience directly, a generative model of the environment dynamics is learned in a (self-)supervised manner.",
    "next": "Such a so-called world model can create new trajectories by iteratively predicting the next state and reward."
  },
  {
    "index": 1176,
    "source_corpus_id": 52917627,
    "ref_id": "b6",
    "citation_corpus_id": 5763832,
    "start": 6756,
    "end": 6777,
    "title": "A Differentiable Physics Engine for Deep Learning in Robotics",
    "abstract": "An important field in robotics is the optimization of controllers. Currently, robots are often treated as a black box in this optimization process, which is the reason why derivative-free optimization methods such as evolutionary algorithms or reinforcement learning are omnipresent. When gradient-based methods are used, models are kept small or rely on finite difference approximations for the Jacobian. This method quickly grows expensive with increasing numbers of parameters, such as found in deep learning. We propose the implementation of a modern physics engine, which can differentiate control parameters. This engine is implemented for both CPU and GPU. Firstly, this paper shows how such an engine speeds up the optimization process, even for small problems. Furthermore, it explains why this is an alternative approach to deep Q-learning, for using deep learning in robotics. Finally, we argue that this is a big step for deep learning in robotics, as it opens up new possibilities to optimize robots, both in hardware and software.",
    "prev": "RELATED WORK\n\nDifferentiable physics simulators.",
    "curr": "Researchers have developed many differentiable physics simulators (Ehrhardt et al., 2017;Degrave et al., 2016).",
    "next": "In particular, Battaglia et al."
  },
  {
    "index": 1178,
    "source_corpus_id": 239616181,
    "ref_id": "b31",
    "citation_corpus_id": 210064473,
    "start": 5294,
    "end": 5312,
    "title": "SPACE: UNSUPERVISED OBJECT-ORIENTED SCENE REPRESENTATION VIA SPATIAL ATTENTION AND DECOMPOSITION",
    "abstract": "The ability to decompose complex multi-object scenes into meaningful abstractions like objects is fundamental to achieve higher-level cognition. Previous approaches for unsupervised object-oriented scene representation learning are either based on spatial-attention or scene-mixture approaches and limited in scalability which is a main obstacle towards modeling real-world scenes. In this paper, we propose a generative latent variable model, called SPACE, that provides a unified probabilistic modeling framework that combines the best of spatial-attention and scene-mixture approaches. SPACE can explicitly provide factorized object representations for foreground objects while also decomposing background segments of complex morphology. Previous models are good at either of these, but not both. SPACE also resolves the scalability problems of previous methods by incorporating parallel spatial-attention and thus is applicable to scenes with a large number of objects without performance degradations. We show through experiments on Atari and 3D-Rooms that SPACE achieves the above properties consistently in comparison to SPAIR, IODINE, and GENESIS. Results of our experiments can be found on our project website: https://sites.google.com/view/space-project-page * Visiting Student at Rutgers University. Authors named inside {} equally contributed. Correspondance to",
    "prev": "(4) While achieving better performance, the proposed model is much simpler than previous approaches.",
    "curr": "PRELIMINARIES\n\n\nOBJECT-CENTRIC REPRESENTATION LEARNING WITH PIXEL-MIXTURE DECODER\n\nA common framework for learning object-centric representations is via a form of auto-encoders (Locatello et al., 2020;Lin et al., 2020b).",
    "next": "In this framework, an encoder takes an input image to return a set of object representation vectors or slot vectors {s 1 , ."
  },
  {
    "index": 1179,
    "source_corpus_id": 252668432,
    "ref_id": "b11",
    "citation_corpus_id": 229297705,
    "start": 5683,
    "end": 5699,
    "title": "TOWARDS RESOLVING THE IMPLICIT BIAS OF GRADIENT DESCENT FOR MATRIX FACTORIZATION: GREEDY LOW-RANK LEARNING",
    "abstract": "Matrix factorization is a simple and natural test-bed to investigate the implicit regularization of gradient descent. Gunasekar et al. (2018)  conjectured that Gradient Flow with infinitesimal initialization converges to the solution that minimizes the nuclear norm, but a series of recent papers argued that the language of norm minimization is not sufficient to give a full characterization for the implicit regularization. In this work, we provide theoretical and empirical evidence that for depth-2 matrix factorization, gradient flow with infinitesimal initialization is mathematically equivalent to a simple heuristic rank minimization algorithm, Greedy Low-Rank Learning, under some reasonable assumptions. This generalizes the rank minimization view from previous works to a much broader setting and enables us to construct counter-examples to refute the conjecture from (Gunasekar et al.,  2018). We also extend the results to the case where depth ≥ 3, and we show that the benefit of being deeper is that the above convergence has a much weaker dependence over initialization magnitude so that this rank minimization is more likely to take effect for initialization with practical scale. * Alphabet ordering.",
    "prev": "In contrast to the so-called kernel regime where the implicit bias is described by the Neural Tangent Kernel (Jacot et al., 2018), there are several active regimes (also called rich or feature-learning regimes), whose implicit bias often feature a form sparsity that is absent from the kernel regime.",
    "curr": "Such active regimes have been observed for example in DNNs with small initialization (Chizat & Bach, 2018;Rotskoff & Vanden-Eijnden, 2018;Li et al., 2020;Jacot et al., 2022a), with L 2regularization (Savarese et al., 2019;Ongie et al., 2020;Jacot et al., 2022b) or when trained on exponentially decaying losses (Gunasekar et al., 2018a;b;Soudry et al., 2018;Du et al., 2018;Ji & Telgarsky, 2018;Chizat & Bach, 2020;Ji & Telgarsky, 2020).",
    "next": "In the latter two cases, the implicit bias is described by the representation cost:\nR(f ) = min W:f W =f W 2\nwhere f is a function that can be represented by the network and the minimization is over all parameters W that result in a network function f W equal to f , the parameters W form a vector and W is the L 2 -norm."
  },
  {
    "index": 1180,
    "source_corpus_id": 53332150,
    "ref_id": "b20",
    "citation_corpus_id": 6628106,
    "start": 2376,
    "end": 2395,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "In comparison, the optimization routines used to train deep nets have arguable changed only little.",
    "curr": "Comparably simple first-order methods like SGD (Robbins & Monro, 1951), its momentum variants (MOMENTUM) (Polyak, 1964;Nesterov, 1983) and ADAM (Kingma & Ba, 2015) remain standards (Goodfellow et al., 2016;Karpathy, 2017).",
    "next": "The low practical relevance of more advanced optimization methods is not for lack of research, though."
  },
  {
    "index": 1186,
    "source_corpus_id": 232013402,
    "ref_id": "b21",
    "citation_corpus_id": 3144218,
    "start": 2403,
    "end": 2425,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "INTRODUCTION\n\nGraph neural networks (GNNs) (Zhou et al., 2018;Wu et al., 2019), which work with graph structured data, have recently attracted considerable attention, as they can learn expressive representations for various graph-related tasks such as node classification, link prediction, and graph classification.",
    "curr": "While the majority of the existing works on GNNs focus on the message passing strategies for neighborhood aggregation (Kipf & Welling, 2017;Hamilton et al., 2017), which aims to encode the nodes in a graph accurately, graph pooling Ying et al., 2018) that maps the set of nodes into a compact representation is crucial in capturing a meaningful structure of an entire graph.",
    "next": "As a simplest approach for graph pooling, we can average or sum all node features in the given graph (Atwood & Towsley, 2016;Xu et al., 2019) (Figure 1 (B))."
  },
  {
    "index": 1187,
    "source_corpus_id": 264288929,
    "ref_id": "b2",
    "citation_corpus_id": 53115163,
    "start": 5411,
    "end": 5430,
    "title": "EXPLORATION BY RANDOM NETWORK DISTILLATION",
    "abstract": "We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.",
    "prev": "23b]'s K 6 /7 regret through a computationally efficient algorithm.Still, there remain significant gaps between the current upper bounds and the √ K lower bound.In this work, we push the frontiers both on the information theoretical limits and the achievable bounds under computational constraints: 1) we present the first (computationally inefficient) algorithm that provably obtains O( √ K) regret, showing that this is the minimax K dependence (Section 3); 2) we obtain O(K 3 /4 ) regret with a polynomial-time algorithm (Section 4).Below, we briefly describe the elements in our approaches.",
    "curr": "Inefficient\n\n√ K algorithm.We convert the linear MDP problem to a linear bandit problem by mapping each policy to a single dH-dimensional feature vector, where d is the ambient dimension of the linear MDP and H K 3 /4 in this paper.Beyond theoretical advancement, exploration in policy optimization has also showcased its potential in addressing real-world challenges, as evidenced by empirical studies [Burda et al., 2018, Pan et al., 2019].",
    "next": "Preliminaries\n\nNo-Regret Learning in MDPs.An (episodic) MDP is specified by a tuple M = (S, A, P ) where S is the state space (possibly infinite), A is the action space (assumed to be finite with size A = |A|), P : S × A → ∆(S) is the transition kernal.The state space is assumed to be layered, i.e., S = S 1 ∪ S 2 ∪ • • • ∪ S H where S h ∩ S h ′ = ∅ for any 1 ≤ h < h ′ ≤ H, and transition is only possible from one layer to the next, that is, P (s ′ | s, a) = 0 only when s ∈ S h and s ′ ∈ S h+1 .Without loss of generality, we assume S 1 = {s 1 }."
  },
  {
    "index": 1189,
    "source_corpus_id": 264436566,
    "ref_id": "b51",
    "citation_corpus_id": 222140788,
    "start": 27408,
    "end": 27412,
    "title": "Published as a conference paper at ICLR 2021 DENOISING DIFFUSION IMPLICIT MODELS",
    "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples 10× to 50× faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error.",
    "prev": "Dataset.We collect meshes in 4 categories (T-shirt, Top, Skirts, Trousers) from the Cloth3D dataset [4] and regroup them into two new categories: upper garments (including T-shirt and Top) and lower garments (including Skirts and Trousers).For both MeshDiffusion (using the DMTet representation) G-MeshDiffusion (using the G-SHELL representation), we run inverse rendering on meshes with known environment lightmaps and known materials using RGB, binary mask, and depth supervision.We generally follow the same settings of [35] for G-MeshDiffusion.For GET3D, we follow the same training setting as [11] and render multiview RGB images for training.",
    "curr": "Evaluation metrics.For each model, we sample a set of meshes, with the size of the test sets, using 100 steps of DDIM [52] and apply standard Laplacian smoothing to these meshes.Similar to [11,35], we evaluate point cloud metrics between the point clouds sampled from generated meshes and those from ground truth meshes.To compensate the lack of perceptual measure in the point cloud metrics, we also evaluate the generated results with multiview FID (MV-FID) [35,63], which is computed by an average of FID (Fréchet Inception Distance) scores of 20 views (rendered with fixed light sources and a diffuse-only mesh material).During rendering, we do not re-orient the face normals towards the camera so that the difference between watertight and open surfaces can be taken into account.",
    "next": "Qualitative and quantitative results.The quantitative results are given in Table 4.We observe that G-MeshDiffusion generally achieves better performance than the watertight mesh generation methods (MeshDiffusion and GET3D), but more importantly, G-MeshDiffusion can better capture the single-sided nature of non-watertight meshes as it"
  },
  {
    "index": 1191,
    "source_corpus_id": 231986303,
    "ref_id": "b23",
    "citation_corpus_id": 3509777,
    "start": 5768,
    "end": 5771,
    "title": "The power of deeper networks for expressing natural functions",
    "abstract": "It is well-known that neural networks are universal approximators, but that deeper networks tend to be much more efficient than shallow ones. We shed light on this by proving that the total number of neurons m required to approximate natural classes of multivariate polynomials of n variables grows only linearly with n for deep neural networks, but grows exponentially when merely a single hidden layer is allowed. We also provide evidence that when the number of hidden layers is increased from 1 to k, the neuron requirement grows exponentially not with n but with n 1/k , suggesting that the minimum number of layers required for computational tractability grows only logarithmically with n.",
    "prev": "For example, it has been shown that it is possible to set the weights of a deep ReLU network such that the number of linear regions computed by the network grows exponentially in the depth [7,9,20,26,27].",
    "curr": "Other works consider the degree of polynomials approximable by networks of different depths [19,24] and the topological invariants of networks [5].",
    "next": "While such work has sometimes been used to explain the utility of different neural network architectures (especially deeper ones), a second strand of prior work has shown that a significant gap can exist between the functions expressible by a given architecture and those which may be learned in practice."
  },
  {
    "index": 1192,
    "source_corpus_id": 259937490,
    "ref_id": "b9",
    "citation_corpus_id": 244130146,
    "start": 3443,
    "end": 3447,
    "title": "Published as a conference paper at ICLR 2022 SOLVING INVERSE PROBLEMS IN MEDICAL IMAGING WITH SCORE-BASED GENERATIVE MODELS",
    "abstract": "Reconstructing medical images from partial measurements is an important inverse problem in Computed Tomography (CT) and Magnetic Resonance Imaging (MRI). Existing solutions based on machine learning typically train a model to directly map measurements to medical images, leveraging a training dataset of paired images and measurements. These measurements are typically synthesized from images using a fixed physical model of the measurement process, which hinders the generalization capability of models to unknown measurement processes. To address this issue, we propose a fully unsupervised technique for inverse problem solving, leveraging the recently introduced score-based generative models. Specifically, we first train a score-based generative model on medical images to capture their prior distribution. Given measurements and a physical model of the measurement process at test time, we introduce a sampling method to reconstruct an image consistent with both the prior and the observed measurements. Our method does not assume a fixed measurement process during training, and can thus be flexibly adapted to different measurement processes at test time. Empirically, we observe comparable or better performance to supervised learning techniques in several medical imaging tasks in CT and MRI, while demonstrating significantly better generalization to unknown measurement processes.",
    "prev": "notes some forward measurement operator (can be linear or nonlinear) and η ∈ R m is additive noise.Usually, we are interested in the case when m < n, which follows many real-world scenarios.When m < n, the problem is ill-posed and some kind of regularizer (or prior) is necessary to obtain a meaningful solution.",
    "curr": "In the literature, the traditional approach of using hand-crafted priors (e.g.sparsity) is slowly being replaced by rich, learned priors such as deep generative models.Recently, there has been a lot of interests in using diffusion models as structural priors due to their state-of-the-art performance in image generation [7][8][9].Compared to GANs, diffusion models are generally easier and more stable to train as they do not rely on an adversarial training scheme, making them a generative prior that is more readily accessible [7].The most common approach for using diffusion models as priors is to resort to posterior sampling, which has been extensively explored in the literature [10][11][12][13][14][15][16][17].However, despite their remarkable success, these techniques exhibit several limitations.The primary challenge is that the majority of existing works that employ diffusion models as priors train these models directly in the pixel space, which requires substantial computational resources and a large volume of training data.In real-world applications such as computed tomography (CT) and magnetic resonance imaging (MRI) reconstruction, where images are inherently either 3D or even 4D [18], training diffusion models directly in the pixel space is often infeasible.",
    "next": "CT Reconstruction\n\nLatent diffusion models (LDMs), which embed data in order to operate in a lower-dimensional space, present a potential solution to this challenge, along with considerable improvements in computational efficiency [19,20].They also can provide a great amount of flexibility, as they can enable one to transfer and generalize these models to different domains by fine-tuning on small amo"
  },
  {
    "index": 1193,
    "source_corpus_id": 252873172,
    "ref_id": "b8",
    "citation_corpus_id": 238744187,
    "start": 2727,
    "end": 2744,
    "title": "OPEN-VOCABULARY OBJECT DETECTION VIA VISION AND LANGUAGE KNOWLEDGE DISTILLATION",
    "abstract": "We aim at advancing open-vocabulary object detection, which detects objects described by arbitrary text inputs. The fundamental challenge is the availability of training data. It is costly to further scale up the number of classes contained in existing object detection datasets. To overcome this challenge, we propose ViLD, a training method via Vision and Language knowledge Distillation. Our method distills the knowledge from a pretrained open-vocabulary image classification model (teacher) into a two-stage detector (student). Specifically, we use the teacher model to encode category texts and image regions of object proposals. Then we train a student detector, whose region embeddings of detected boxes are aligned with the text and image embeddings inferred by the teacher. We benchmark on LVIS by holding out all rare categories as novel categories that are not seen during training. ViLD obtains 16.1 mask AP r with a ResNet-50 backbone, even outperforming the supervised counterpart by 3.8. When trained with a stronger teacher model ALIGN, ViLD achieves 26.3 AP r . The model can directly transfer to other datasets without finetuning, achieving 72.2 AP 50 on PASCAL VOC, 36.6 AP on COCO and 11.8 AP on Objects365. On COCO, ViLD outperforms the previous state-of-theart (Zareian et al., 2021) by 4.8 on novel AP and 11.4 on overall AP. Code and demo are open-sourced at https://github.com/tensorflow/tpu/ tree/master/models/official/detection/projects/vild.",
    "prev": "Unlike image classification, object detection is more challenging and has a larger gap than the pre-training tasks.",
    "curr": "Several methods, such as RegionCLIP (Zhong et al., 2022), ViLD (Gu et al., 2021), and Detic , have been suggested to transfer knowledge from pre-trained CLIP  model to some modules of object detectors.",
    "next": "However, real images and annotations are still required for some key modules of the object detectors, such as RPNs or RoI heads."
  },
  {
    "index": 1195,
    "source_corpus_id": 258557287,
    "ref_id": "b22",
    "citation_corpus_id": 227209335,
    "start": 11292,
    "end": 11311,
    "title": "SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS",
    "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reversetime SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in diffusion probabilistic modeling and scorebased generative modeling, and allows for new sampling procedures. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, which enables exact likelihood computation, and improved sampling efficiency. In addition, our framework enables conditional generation with an unconditional model, as we demonstrate with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high fidelity generation of 1024ˆ1024 images for the first time from a score-based generative model.",
    "prev": "BACKGROUND\n\nIn this section, we first review diffusion models in Section 3.1 and we discuss how they are used for solving inverse problems in Section 3.2.",
    "curr": "DENOISING DIFFUSION MODELS\n\nDiffusion models (Sohl-Dickstein et al., 2015;Ho et al., 2020;Song et al., 2021b) consist of two processes: a forward process that gradually adds noise to input images and a reverse process that learns to generate images by iterative denoising.",
    "next": "Formally the forward process can be expressed by the variance preserving stochastic differential equation (VP-SDE) (Song et al., 2021b) dx \" 1 2 βptqxdt`aβptqdw for t P r0, T s where βptq :\" β min`p β max´βmin q t T rescales the time variable, and dw is the standard Wiener process."
  },
  {
    "index": 1196,
    "source_corpus_id": 3463660,
    "ref_id": "b6",
    "citation_corpus_id": 5590763,
    "start": 23522,
    "end": 23540,
    "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
    "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
    "prev": "We then employ a meta controller called interpreter in an iterative manner.",
    "curr": "For the ith interpretation step, the interpreter computes the word attention as:\nτ˚$ ' ' ' ' ' & ' ' ' ' ' % Word attention: o i l 9 exp \" S cos pp i´1 , w l q ‰ Attended context: w i \" ÿ l o i l w l Attended word: s i \" ÿ l o i l w l Interpreter state: p i \" GRUpp i´1 , w i q(6)\nwhere S cos is cosine similarity and GRU is the gated recurrent unit (Cho et al., 2014).",
    "next": "Here we use τ˚to represent an approximation of τ via soft word attention."
  },
  {
    "index": 1197,
    "source_corpus_id": 211296452,
    "ref_id": "b20",
    "citation_corpus_id": 6018348,
    "start": 2313,
    "end": 2331,
    "title": "Distant Supervision for Relation Extraction with an Incomplete Knowledge Base",
    "abstract": "Distant supervision, heuristically labeling a corpus using a knowledge base, has emerged as a popular choice for training relation extractors. In this paper, we show that a significant number of \"negative\" examples generated by the labeling process are false negatives because the knowledge base is incomplete. Therefore the heuristic for generating negative examples has a serious flaw. Building on a state-of-the-art distantly-supervised extraction algorithm, we proposed an algorithm that learns from only positive and unlabeled labels at the pair-of-entity level. Experimental results demonstrate its advantage over existing algorithms.",
    "prev": "\", one can identify the entity Grateful Dead and the path of relations LeadSinger, BirthDate to efficiently extract the answer-provided that this information is present in the KB.",
    "curr": "Unfortunately, KBs are often incomplete (Min et al., 2013).",
    "next": "While relation extraction methods can be used to populate KBs, this process is inherently error-prone, expensive and slow."
  },
  {
    "index": 1201,
    "source_corpus_id": 53216170,
    "ref_id": "b19",
    "citation_corpus_id": 7228830,
    "start": 2122,
    "end": 2141,
    "title": "Adversarial Examples for Evaluating Reading Comprehension Systems",
    "abstract": "Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear.To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75% F1 score to 36%; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7%. We hope our insights will motivate the development of new models that understand language more precisely.",
    "prev": "However, while standard encoders (e.g.",
    "curr": "bidirectional LSTMs) theoretically have the ability to handle arbitrary long-distance relationships, in practice they often fail to correctly handle long texts and are easily distracted by simple noise (Jia & Liang, 2017).",
    "next": "In this work, we focus on an improvement of sequence encoders that is compatible with a wide range of decoder choices."
  },
  {
    "index": 1203,
    "source_corpus_id": 174802369,
    "ref_id": "b48",
    "citation_corpus_id": 3693512,
    "start": 8117,
    "end": 8136,
    "title": "LIFELONG LEARNING WITH DYNAMICALLY EXPAND- ABLE NETWORKS",
    "abstract": "We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets in lifelong learning scenarios on multiple public datasets, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch model with substantially fewer number of parameters.",
    "prev": "Progressive networks Schwarz et al., 2018) was one of the earliest works in this direction and was successfully applied to reinforcement learning problems; the base architecture was duplicated and lateral connections added in response to new tasks.",
    "curr": "Dynamically Expandable Network (DEN) (Yoon et al., 2018) also expands its network by selecting drifting units and retraining them on new tasks.",
    "next": "In contrast to our method, these approaches require the architecture grow with each new task."
  },
  {
    "index": 1204,
    "source_corpus_id": 260704206,
    "ref_id": "b30",
    "citation_corpus_id": 259262608,
    "start": 4302,
    "end": 4319,
    "title": "Understanding In-Context Learning via Supportive Pretraining Data",
    "abstract": "In-context learning (ICL) improves language models' performance on a variety of NLP tasks by simply demonstrating a handful of examples at inference time. It is not well understood why ICL ability emerges, as the model has never been specifically trained on such demonstrations. Unlike prior work that explores implicit mechanisms behind ICL, we study ICL via investigating the pretraining data. Specifically, we first adapt an iterative, gradient-based approach to find a small subset of pretraining data that supports ICL. We observe that a continued pretraining on this small subset significantly improves the model's ICL ability, by up to 18%. We then compare the supportive subset constrastively with random subsets of pretraining data and discover: (1) The supportive pretraining data to ICL do not have a higher domain relevance to downstream tasks. (2) The supportive pretraining data have a higher mass of rarely occurring, long-tail tokens. (3) The supportive pretraining data are challenging examples where the information gain from long-range context is below average, indicating learning to incorporate difficult long-range context encourages ICL. Our work takes a first step towards understanding ICL via analyzing instance-level pretraining data. Our insights have a potential to enhance the ICL ability of language models by actively guiding the construction of pretraining data in the future.ReferencesEkin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2022. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661.Tiago A Almeida, José María G Hidalgo, and Akebo Yamakami. 2011. Contributions to the study of sms spam filtering: new collection and results. In Proceedings of the 11th ACM symposium on Document engineering, pages 259-262. . 2022. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers. arXiv preprint arXiv:2212.10559.",
    "prev": "These new capabilities enable better alignment of the model with various data-use regulations, e.g., the fair use doctrine in the United States (Henderson et al., 2023) and the GDPR in the European Union , as detailed in §2.",
    "curr": "This is in contrast to parametric models, where removing high-risk data is infeasible after training (Bourtoule et al., 2020; and data attribution at scale is difficult (Zhang et al., 2021;Han et al., 2023).",
    "next": "We introduce SILO, a new nonparametric language model that follows our proposal ( §4)."
  },
  {
    "index": 1205,
    "source_corpus_id": 252715596,
    "ref_id": "b28",
    "citation_corpus_id": 28202810,
    "start": 10589,
    "end": 10612,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "The objective of the reinforcement learning problem then changes to constrain the accumulated cost under a given safety threshold d ∈ R:\nπ * = arg max π E t γ t r t s. t. E t c t < d.(1)\nNext, we review RL algorithms for POMDPs and CMDPs, laying the foundations for the safe RL algorithm we propose in Section 5 dedicated to CPOMDPs with unknown environment dynamics.",
    "curr": "Stochastic latent actor-critic\n\nSoft actor-critic [SAC; Haarnoja et al., 2018a] is an RL approach based on the maximum entropy framework.",
    "next": "Besides the traditional reinforcement learning objective of maximizing the reward, it also aims to maximize the entropy of the policy."
  },
  {
    "index": 1207,
    "source_corpus_id": 4737664,
    "ref_id": "b22",
    "citation_corpus_id": 14911774,
    "start": 6190,
    "end": 6213,
    "title": "MULTI-AGENT COOPERATION AND THE EMERGENCE OF (NATURAL) LANGUAGE",
    "abstract": "The current mainstream approach to train natural language systems is to expose them to large amounts of text. This passive learning is problematic if we are interested in developing interactive machines, such as conversational agents. We propose a framework for language learning that relies on multi-agent communication. We study this learning in the context of referential games. In these games, a sender and a receiver see a pair of images. The sender is told one of them is the target and is allowed to send a message from a fixed, arbitary vocabulary to the receiver. The receiver must rely on this message to identify the target. Thus, the agents develop their own language interactively out of the need to communicate. We show that two networks with simple configurations are able to learn to coordinate in the referential game. We further explore how to make changes to the game environment to cause the \"word meanings\" induced in the game to better reflect intuitive semantic properties of the images. In addition, we present a simple strategy for grounding the agents' code into natural language. Both of these are necessary steps towards developing machines that are able to communicate with humans productively. * Work done while at Facebook AI Research.The central problem of our program, then, is the following: How do we design environments that foster the development of a language that is portable to new situations and to new communication partners (in particular humans)?We start from the most basic challenge of using a language in order to refer to things in the context of a two-agent game. We focus on two questions. First, whether tabula rasa agents succeed in communication. Second, what features of the environment lead to the development of codes resembling human language.We assess this latter question in two ways. First, we consider whether the agents associate general conceptual properties, such as broad object categories (as opposed to low-level visual properties), to the symbols they learn to use. Second, we examine whether the agents' \"word usage\" is partially interpretable by humans in an online experiment.Other researchers have proposed communication-based environments for the development of coordination-capable AI. Work in multi-agent systems has focused on the design of pre-programmed communication systems to solve specific tasks (e.g., robot soccer, Stone & Veloso 1998). Most related to our work,Sukhbaatar et al. (2016) andFoerster et al. (2016)show that neural networks can evolve communication in the context of games without a pre-coded protocol. We pursue the same question, but further ask how we can change our environment to make the emergent language more interpretable.Others (e.g., the SHRLDU program of Winograd 1971 or the game inWang et al. 2016)propose building a communicating AI by putting humans in the loop from the very beginning. This approach has benefits but faces serious scalability issues, as active human intervention is required at each step. An attractive component of our game-based paradigm is that humans may be added as players, but do not need to be there all the time.",
    "prev": "REFERENTIAL GAMES AS MULTI-AGENT CO-OPERATIVE REINFORCEMENT LEARNING\n\nThe referential game is implemented as an instance of multi-agent co-operative reinforcement learning, in which two agents take discrete actions in their environment in order to maximize a shared reward.",
    "curr": "GAME AND TERMINOLOGY\n\nThe referential game is a variant of the Lewis signaling game (Lewis, 1969), which has been extensively used in linguistic and cognitive studies in the context of language evolution (e.g., Briscoe, 2002;Cangelosi & Parisi, 2002;Steels & Loetzsch, 2012;Spike et al., 2016;Lazaridou et al., 2017).",
    "next": "Figure 1 provides a schematic description of our setup."
  },
  {
    "index": 1210,
    "source_corpus_id": 209475822,
    "ref_id": "b5",
    "citation_corpus_id": 7586460,
    "start": 2422,
    "end": 2442,
    "title": "Lifelong Learning for Sentiment Classification",
    "abstract": "This paper proposes a novel lifelong learning (LL) approach to sentiment classification. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we first discuss LL in general and then LL for sentiment classification in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochastic gradient descent. Our experimental results show that the proposed method outperforms baseline methods significantly, which demonstrates that lifelong learning is a promising research direction.",
    "prev": "In this paper, we focus on lifelong language learning, where a machine achieves lifelong learning on a stream of natural language processing (NLP) tasks.",
    "curr": "To the best of our knowledge, lifelong language learning has been studied in only a few instances; for sentiment analysis (Chen et al., 2015b;Xia et al., 2017), conversational agents (Lee, 2017), word representation learning (Xu et al., 2018), sentence representation learning (Liu et al., 2019), text classification, and question answering (d'Autume et al., 2019).",
    "next": "However, in all previous work, the tasks in the stream are essentially the same task but in different domains."
  },
  {
    "index": 1212,
    "source_corpus_id": 238419044,
    "ref_id": "b29",
    "citation_corpus_id": 6212000,
    "start": 1093,
    "end": 1113,
    "title": "UNDERSTANDING DEEP LEARNING REQUIRES RE- THINKING GENERALIZATION",
    "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.",
    "prev": "They also point to a new and simple regularization scheme which we show performs comparably to the current state of the art.",
    "curr": "INTRODUCTION\n\nThe observation that high capacity deep neural networks trained with mini-batched stochastic gradient descent, referred to SGD in this paper, tend to generalize well (Zhang et al., 2017) contradicts the classical wisdom in statistical learning theory (e.g., Vapnik (1998) ) and has stimulated intense research interest in understanding the generalization behaviour of modern neural networks.",
    "next": "In this direction, generalization bounds for over-parameterized neural networks are obtained (Allen-Zhu et al., 2019;Bartlett et al., 2017;Neyshabur et al., 2015;2018a;b;Arora et al., 2018; 2019) and a curious \"double descent\" phenomenon is observed and analyzed (Belkin et al., 2019;Nakkiran et al., 2019;Yang et al., 2020)."
  },
  {
    "index": 1214,
    "source_corpus_id": 3323727,
    "ref_id": "b17",
    "citation_corpus_id": 38407095,
    "start": 29641,
    "end": 29654,
    "title": "Europarl: A Parallel Corpus for Statistical Machine Translation",
    "abstract": "We collected a corpus of parallel text in 11 languages from the proceedings of the European Parliament, which are published on the web 1 . This corpus has found widespread use in the NLP community. Here, we focus on its acquisition and its application as training data for statistical machine translation (SMT). We trained SMT systems for 110 language pairs, which reveal interesting clues into the challenges ahead.",
    "prev": "LARGER DATASETS\n\nWMT13 Experiments.",
    "curr": "We run a similar LSTM architecture as above for the WMT13 dataset (Koehn, 2005) (1.7M sentences train, 190K sentences test) and we provide additional experiments for quantized distillation technique, see Table 6.",
    "next": "We note that, on this large dataset, PM quantization does not perform well, even with bucketing."
  },
  {
    "index": 1215,
    "source_corpus_id": 257280442,
    "ref_id": "b11",
    "citation_corpus_id": 225039882,
    "start": 9259,
    "end": 9285,
    "title": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. 1 arXiv:2010.11929v2 [cs.CV] 3 Jun 2021Published as a conference paper at ICLR 2021 inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.However, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks.RELATED WORKTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global selfattention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.",
    "prev": "ConvNets & ViTs ConvNets have dominated the vision community since the deep learning era (Simonyan & Zisserman, 2015;He et al., 2016).",
    "curr": "Recently, ViTs (Dosovitskiy et al., 2020) introduce purely attention-based transformers (Vaswani et al., 2017) to the vision community and have set new SOTA performances on various vision tasks.",
    "next": "A common and plausible conjecture is that these gratifying achievements are credited to the self-attention mechanism."
  },
  {
    "index": 1216,
    "source_corpus_id": 219721263,
    "ref_id": "b10",
    "citation_corpus_id": 213085920,
    "start": 2983,
    "end": 2987,
    "title": "Published as a conference paper at ICLR 2020 STRATEGIES FOR PRE-TRAINING GRAPH NEURAL NETWORKS",
    "abstract": "Many applications of machine learning require a model to make accurate predictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that naïve strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4% absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction. * Equal contribution. Project website, data and code: Attribute Masking Supervised Attribute Prediction Structural Similarity Prediction Structure prediction Context Prediction (b) Categorization of our pre-training methods Graph space Node space Graph embeddings Node embeddings Linear classifier Figure 1: (a.i) When only node-level pre-training is used, nodes of different shapes (semantically different nodes) can be well separated, however, node embeddings are not composable, and thus resulting graph embeddings (denoted by their classes, + and −) that are created by pooling node-level embeddings are not separable. (a.ii) With graph-level pre-training only, graph embeddings are well separated, however the embeddings of individual nodes do not necessarily capture their domainspecific semantics. (a.iii) High-quality node embeddings are such that nodes of different types are well separated, while at the same time, the embedding space is also composable. This allows for accurate and robust representations of entire graphs and enables robust transfer of pre-trained models to a variety of downstream tasks. (b) Categorization of pre-training methods for GNNs. Crucially, our methods, i.e., Context Prediction, Attribute Masking, and graph-level supervised pre-training (Supervised Attribute Prediction) enable both node-level and graph-level pre-training.matter of increasing the number of labeled pre-training datasets that are from the same domain as the downstream task. Instead, it requires substantial domain expertise to carefully select examples and target labels that are correlated with the downstream task of interest. Otherwise, the transfer of knowledge from related pre-training tasks to a new downstream task can harm generalization, which is known as negative transfer(Rosenstein et al., 2005)and significantly limits the applicability and reliability of pre-trained models.Present work. Here, we focus on pre-training as an approach to transfer learning in Graph Neural Networks (GNNs)(Kipf & Welling, 2017;Hamilton et al., 2017a;Ying et al., 2018b;Xu et al., 2019;2018)for graph-level property prediction. Our work presents two key contributions.(1) We conduct the first systematic large-scale investigation of strategies for pre-training GNNs. For that, we build two large new pre-training datasets, which we share with the community: a chemistry dataset with 2 million graphs and a biology dataset with 395K graphs. We also show that large domain-specific datasets are crucial to investigate pre-training and that existing downstream benchmark datasets are too small to evaluate models in a statistically reliable way.(2) We develop an effective pretraining strategy for GNNs and demonstrate its effectiveness and its ability for out-of-distribution generalization on hard transfer-learning problems.",
    "prev": "[9]).",
    "curr": "To learn graph representations, GNN-based frameworks make use of three generic modules, which provide i) feature aggregation, ii) graph pooling (i.e., readout), and iii) classification [10].",
    "next": "The feature aggregator provides a vector representation for each node of the graph, referred to as a node embedding."
  },
  {
    "index": 1219,
    "source_corpus_id": 238354201,
    "ref_id": "b9",
    "citation_corpus_id": 225039882,
    "start": 1701,
    "end": 1726,
    "title": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. 1 arXiv:2010.11929v2 [cs.CV] 3 Jun 2021Published as a conference paper at ICLR 2021 inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.However, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks.RELATED WORKTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global selfattention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.",
    "prev": "Our source code is open-source and available at: https://github.com/apple/ml-cvnets.",
    "curr": "INTRODUCTION\n\nSelf-attention-based models, especially vision transformers (ViTs; Figure 1a; Dosovitskiy et al., 2021), are an alternative to convolutional neural networks (CNNs) to learn visual representations.",
    "next": "Briefly, ViT divides an image into a sequence of non-overlapping patches and then learns interpatch representations using multi-headed self-attention in transformers (Vaswani et al., 2017)."
  },
  {
    "index": 1220,
    "source_corpus_id": 263608332,
    "ref_id": "b59",
    "citation_corpus_id": 211132598,
    "start": 9017,
    "end": 9036,
    "title": "Published as a conference paper at ICLR 2020 FEDERATED LEARNING WITH MATCHED AVERAGING",
    "abstract": "Federated learning allows edge devices to collaboratively learn a shared model while keeping the training data on device, decoupling the ability to do model training from the need to store the data in the cloud. We propose the Federated matched averaging (FedMA) algorithm designed for federated learning of modern neural network architectures e.g. convolutional neural networks (CNNs) and LSTMs. FedMA constructs the shared global model in a layer-wise manner by matching and averaging hidden elements (i.e. channels for convolution layers; hidden states for LSTM; neurons for fully connected layers) with similar feature extraction signatures. Our experiments indicate that FedMA not only outperforms popular state-of-the-art federated learning algorithms on deep CNN and LSTM architectures trained on real world datasets, but also reduces the overall communication burden.",
    "prev": " self-attention in the Transformer encoder/decoder blocks (Fedus et al., 2022;Chowdhery et al., 2022).The use of MoE in LLMs has been demonstrated to effectively scale the model sizes up further without costing proportional increase in the computation complexity, as the computation in MoE is effectively sparse.The majority of works in this group are designed for the joint training of \"experts\" and the gating/aggregation module.In our problem setting, the experts are pre-trained on their respective domains and serve as a starting point for the Fusion of Experts.",
    "curr": "Federated/collaborative Learning.Federated/collaborative learning allows various clients/agents to jointly learn using their own (mostly private) local data (Kairouz et al., 2021;Wang et al., 2021).During the federated learning, participating clients conduct in-situ learning and computing before their locally learned models are communicated to a central server for model aggregation or fusion (McMahan et al., 2017).et al., 2017;Wang et al., 2020b;Li & Wang, 2019;Lin et al., 2020).Our work can be seen as a special case of federated learning where clients train their own models locally and share it with the central server for training the FoE model to aggregate them.",
    "next": "Combining pre-trained models.One common way a practitioner can interact with a pre-trained model is via an API.Such models typically vary in performance and cost.FrugalML (Chen et al., 2020) aims to maximize the usage of the cheapest API on \"easy\" inputs, while only querying the more expensive ones when needed.This work has also been extended to LLMs (Chen et al., 2023b)."
  },
  {
    "index": 1223,
    "source_corpus_id": 53802740,
    "ref_id": "b8",
    "citation_corpus_id": 4994434,
    "start": 2085,
    "end": 2111,
    "title": "Towards a Neural Statistician",
    "abstract": "An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes.",
    "prev": "These applications have given rise to vast interest in few-shot learning (Fei-Fei et al., 2006;Lake et al., 2011), which emphasizes data efficiency via information sharing across related tasks.",
    "curr": "Despite recent advances, notably in meta-learning based approaches (Ravi and Larochelle, 2017;Vinyals et al., 2016;Edwards and Storkey, 2017;Finn et al., 2017;, there remains a lack of general purpose methods for flexible, data-efficient learning.",
    "next": "Due to the ubiquity of recent work, a unifying view is needed to understand and improve these methods."
  },
  {
    "index": 1224,
    "source_corpus_id": 246431014,
    "ref_id": "b39",
    "citation_corpus_id": 786898,
    "start": 8060,
    "end": 8080,
    "title": "PARTICULAR OBJECT RETRIEVAL WITH INTEGRAL MAX-POOLING OF CNN ACTIVATIONS",
    "abstract": "Recently, image representation built upon Convolutional Neural Network (CNN)   has been shown to provide effective descriptors for image search, outperforming pre-CNN features as short-vector representations. Yet such models are not compatible with geometry-aware re-ranking methods and still outperformed, on some particular object retrieval benchmarks, by traditional image search systems relying on precise descriptor matching, geometric re-ranking, or query expansion. This work revisits both retrieval stages, namely initial search and re-ranking, by employing the same primitive information derived from the CNN. We build compact feature vectors that encode several image regions without the need to feed multiple inputs to the network. Furthermore, we extend integral images to handle max-pooling on convolutional layer activations, allowing us to efficiently localize matching objects. The resulting bounding box is finally used for image reranking. As a result, this paper significantly improves existing CNN-based recognition pipeline: We report for the first time results competing with traditional methods on the challenging Oxford5k and Paris6k datasets. * Research partially conducted while G. Tolias and H. Jégou were at Inria. We would like to thank Florent Perronnin for his valuable feedback.",
    "prev": "image-level feature vectors, are commonly produced by averaging all local features, e.g.",
    "curr": "via global average or max pooling (Babenko & Lempitsky, 2015;Tolias et al., 2016;Gordo et al., 2016).",
    "next": "A global contrastive loss for training."
  },
  {
    "index": 1225,
    "source_corpus_id": 236635379,
    "ref_id": "b12",
    "citation_corpus_id": 1957433,
    "start": 9589,
    "end": 9613,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "As hardware grew more powerful, neural nets led to breakthroughs in image understanding (Krizhevsky et al., 2012;Zeiler & Fergus, 2014;Szegedy et al., 2015) and interest intensified: autoregressive models that could process and complete samples of handwriting were developed (Graves, 2013), and new convolutional network designs led to good results in structured output spaces like semantic segmentation (Farabet et al., 2012;Long et al., 2015;Ronneberger et al., 2015), pose estimation (Toshev & Szegedy, 2014), detection (Sermanet et al., 2014), captioning (You et al., 2016), and optical flow (Fischer et al., 2015).",
    "curr": "At the same time, natural language applications research has made extensive progressive in capturing the structured nature of language, typically via autoregressive models (Collobert et al., 2011;Sutskever et al., 2014;Vaswani et al., 2017;Radford et al., 2019;Brown et al., 2020) or context prediction (Mikolov et al., 2013;Pennington et al., 2014;Devlin et al., 2019).",
    "next": "Similar to our work, several groups have proposed to solve tasks in multiple domains (e.g."
  },
  {
    "index": 1226,
    "source_corpus_id": 238215172,
    "ref_id": "b71",
    "citation_corpus_id": 221376381,
    "start": 7085,
    "end": 7104,
    "title": "EXTREME MEMORIZATION VIA SCALE OF INITIALIZA- TION",
    "abstract": "We construct an experimental setup in which changing the scale of initialization strongly impacts the implicit regularization induced by SGD, interpolating from good generalization performance to completely memorizing the training set while making little progress on the test set. Moreover, we find that the extent and manner in which generalization ability is affected depends on the activation and loss function used, with sin activation demonstrating extreme memorization. In the case of the homogeneous ReLU activation, we show that this behavior can be attributed to the loss function. Our empirical investigation reveals that increasing the scale of initialization correlates with misalignment of representations and gradients across examples in the same class. This insight allows us to devise an alignment measure over gradients and representations which can capture this phenomenon. We demonstrate that our alignment measure correlates with generalization of deep models trained on image classification tasks.",
    "prev": "We explicitly focus on generalization effects of SGD in this work.",
    "curr": "Other possible sources of generalization for neural networks have been proposed that do not lean on stochastic sampling, for example generalization results that only require overparametrization (Neyshabur et al., 2018;Advani et al., 2020), large width (Golubeva et al., 2021), and well-behaved initialization schemes (Wu et al., 2017;Mehta et al., 2020).",
    "next": "We will not discuss these here."
  },
  {
    "index": 1227,
    "source_corpus_id": 3526391,
    "ref_id": "b3",
    "citation_corpus_id": 5590763,
    "start": 3087,
    "end": 3104,
    "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
    "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
    "prev": "2015.",
    "curr": "INTRODUCTION\n\nModern neural networks are known to generalize well when the training and testing data are sampled from the same distribution (Krizhevsky et al., 2012;Simonyan & Zisserman, 2015;He et al., 2016;Cho et al., 2014;Zhang et al., 2017).",
    "next": "However, when deploying neural networks in real-world applications, there is often very little control over the testing data distribution."
  },
  {
    "index": 1229,
    "source_corpus_id": 264172668,
    "ref_id": "b46",
    "citation_corpus_id": 3626819,
    "start": 2006,
    "end": 2027,
    "title": "Deep contextualized word representations",
    "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
    "prev": "We release code and data at\n\nINTRODUCTION\n\nDriven by the advancements of Large Language Models (LLMs) (Brown et al., 2020;Chowdhery et al., 2022;OpenAI, 2023;Touvron et al., 2023a), a transformative wave has reshaped the landscape in multiple areas of Artificial Intelligence, elevating performance across diverse tasks.",
    "curr": "From a parametric perspective, the objective of pre-training is to encode substantial amounts of knowledge into model parameters through language modeling on extensive corpora (Peters et al., 2018;Radford et al.",
    "next": ";Devlin et al., 2019;Delétang et al., 2023)."
  },
  {
    "index": 1230,
    "source_corpus_id": 258418258,
    "ref_id": "b5",
    "citation_corpus_id": 13807351,
    "start": 3365,
    "end": 3388,
    "title": "ENTROPY-SGD: BIASING GRADIENT DESCENT INTO WIDE VALLEYS",
    "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.",
    "prev": "The training loss landscape is complex and nonconvex with many local minima of different generalization abilities.",
    "curr": "Many studies have investigated the relationship between the loss surface's geometry and generalization performance (Hochreiter & Schmidhuber, 1994;McAllester, 1999;Keskar et al., 2017;Neyshabur et al., 2017;Jiang et al., 2020), and found that flatter minima generalize better than sharper minima (Dziugaite & Roy, 2017;Petzka et al., 2021;Chaudhari et al., 2017;Keskar et al., 2017;Jiang et al., 2020).",
    "next": "Sharpness-aware minimization (SAM) (Foret et al., 2021) is the current state-of-the-art to seek flat minima by solving a min-max optimization problem."
  },
  {
    "index": 1231,
    "source_corpus_id": 86840468,
    "ref_id": "b38",
    "citation_corpus_id": 26873455,
    "start": 2472,
    "end": 2492,
    "title": "Adversarial Training for Unsupervised Bilingual Lexicon Induction",
    "abstract": "Word embeddings are well known to capture linguistic regularities of the language on which they are trained. Researchers also observe that these regularities can transfer across languages. However, previous endeavors to connect separate monolingual word embeddings typically require cross-lingual signals as supervision, either in the form of parallel corpus or seed lexicon. In this work, we show that such cross-lingual connection can actually be established without any form of supervision. We achieve this end by formulating the problem as a natural adversarial game, and investigating techniques that are crucial to successful training. We carry out evaluation on the unsupervised bilingual lexicon induction task. Even though this task appears intrinsically cross-lingual, we are able to demonstrate encouraging performance without any cross-lingual clues.",
    "prev": "We present convincing results in a few visual domains, such as no-glasses to glasses, adding facial hair based on a reference image, etc.",
    "curr": "INTRODUCTION\n\nIn the problem of unsupervised domain translation, the algorithm receives two sets of samples, one from each domain, and learns a function that maps between a sample in one domain to the analogous sample in the other domain Yi et al., 2017;Benaim & Wolf, 2017;Liu & Tuzel, 2016;Liu et al., 2017;Choi et al., 2017;Conneau et al., 2017;Zhang et al., 2017a;b;Lample et al., 2018).",
    "next": "The term unsupervised means, in this context, that the two sets are unpaired."
  },
  {
    "index": 1235,
    "source_corpus_id": 250089240,
    "ref_id": "b27",
    "citation_corpus_id": 213085920,
    "start": 2415,
    "end": 2419,
    "title": "Published as a conference paper at ICLR 2020 STRATEGIES FOR PRE-TRAINING GRAPH NEURAL NETWORKS",
    "abstract": "Many applications of machine learning require a model to make accurate predictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that naïve strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4% absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction. * Equal contribution. Project website, data and code: Attribute Masking Supervised Attribute Prediction Structural Similarity Prediction Structure prediction Context Prediction (b) Categorization of our pre-training methods Graph space Node space Graph embeddings Node embeddings Linear classifier Figure 1: (a.i) When only node-level pre-training is used, nodes of different shapes (semantically different nodes) can be well separated, however, node embeddings are not composable, and thus resulting graph embeddings (denoted by their classes, + and −) that are created by pooling node-level embeddings are not separable. (a.ii) With graph-level pre-training only, graph embeddings are well separated, however the embeddings of individual nodes do not necessarily capture their domainspecific semantics. (a.iii) High-quality node embeddings are such that nodes of different types are well separated, while at the same time, the embedding space is also composable. This allows for accurate and robust representations of entire graphs and enables robust transfer of pre-trained models to a variety of downstream tasks. (b) Categorization of pre-training methods for GNNs. Crucially, our methods, i.e., Context Prediction, Attribute Masking, and graph-level supervised pre-training (Supervised Attribute Prediction) enable both node-level and graph-level pre-training.matter of increasing the number of labeled pre-training datasets that are from the same domain as the downstream task. Instead, it requires substantial domain expertise to carefully select examples and target labels that are correlated with the downstream task of interest. Otherwise, the transfer of knowledge from related pre-training tasks to a new downstream task can harm generalization, which is known as negative transfer(Rosenstein et al., 2005)and significantly limits the applicability and reliability of pre-trained models.Present work. Here, we focus on pre-training as an approach to transfer learning in Graph Neural Networks (GNNs)(Kipf & Welling, 2017;Hamilton et al., 2017a;Ying et al., 2018b;Xu et al., 2019;2018)for graph-level property prediction. Our work presents two key contributions.(1) We conduct the first systematic large-scale investigation of strategies for pre-training GNNs. For that, we build two large new pre-training datasets, which we share with the community: a chemistry dataset with 2 million graphs and a biology dataset with 395K graphs. We also show that large domain-specific datasets are crucial to investigate pre-training and that existing downstream benchmark datasets are too small to evaluate models in a statistically reliable way.(2) We develop an effective pretraining strategy for GNNs and demonstrate its effectiveness and its ability for out-of-distribution generalization on hard transfer-learning problems.",
    "prev": "However, the problem remains challenging due to the limited number of labeled molecules as it is in general expensive and time-consuming to label molecules, which usually requires expensive physics simulations or wet-lab experiments.",
    "curr": "As a result, recently, there has been growing interest in developing pretraining or self-supervised learning methods for learning molecular representations by leveraging the huge amount of unlabeled molecule data [28,35,63,75].",
    "next": "These methods have shown superior performance on many tasks, especially when the number of labeled molecules is insufficient."
  },
  {
    "index": 1236,
    "source_corpus_id": 263620365,
    "ref_id": "b64",
    "citation_corpus_id": 248811614,
    "start": 4051,
    "end": 4069,
    "title": "FREEMATCH: SELF-ADAPTIVE THRESHOLDING FOR SEMI-SUPERVISED LEARNING",
    "abstract": "Semi-supervised Learning (SSL) has witnessed great success owing to the impressive performances brought by various methods based on pseudo labeling and consistency regularization. However, we argue that existing methods might fail to utilize the unlabeled data more effectively since they either use a pre-defined / fixed threshold or an ad-hoc threshold adjusting scheme, resulting in inferior performance and slow convergence. We first analyze a motivating example to obtain intuitions on the relationship between the desirable threshold and model's learning status. Based on the analysis, we hence propose FreeMatch to adjust the confidence threshold in a self-adaptive manner according to the model's learning status. We further introduce a self-adaptive class fairness regularization penalty to encourage the model for diverse predictions during the early training stage. Extensive experiments indicate the superiority of FreeMatch especially when the labeled data are extremely rare. FreeMatch achieves 5.78%, 13.59%, and 1.28% error rate reduction over the latest state-of-the-art method FlexMatch on CIFAR-10 with 1 label per class, STL-10 with 4 labels per class, and Im-ageNet with 100 labels per class, respectively. Moreover, FreeMatch can also boost the performance of imbalanced SSL. The codes can be found at https: //github.com/microsoft/Semi-supervised-learning.",
    "prev": "n a supervised learning (SL) manner that requires manually labeling data, which is limited in quantity and labor-exhaustive.To extend SL with massive unlabeled data, semi-supervised learning (SSL) exploits the information of unlabeled data with limited labeled data (Tarvainen & Valpola, 2017;Sohn et al., 2020) in the self-training paradigm of pseudo-labeling (Lee et al., 2013), i.e., training models with unlabeled data and pseudo labels assigned by models' predictions.",
    "curr": "As a widely used technique, the main problem of SSL is how to generate accurate pseudo labels without or with tolerable effects of confirmation bias (Arazo et al., 2020), i.e., overfitting to incorrect pseudo labels from teacher models.There were three main strands of research, aiming at obtaining high-quality pseudo labels and a high sampling rate while being capable of various tasks and scenarios.Firstly, mainstream methods utilize threshold-based pseudo labeling (Sohn et al., 2020;Zhang et al., 2021;Kim et al., 2022;Wang et al., 2022b) with ad-hoc or complex hand-crafted strategies to select high-quality pseudo labels.However, these algorithms are predefined and task-specific, i.e., they are designed for classification tasks but cannot handle more challenging regression tasks.",
    "next": "The second strand introduces pre-trained teacher models (Zhou & Li, 2010;Xie et al., 2020b) to generate high-quality pseudo labels, which require extra computational cost (e.g., double training times (Pham et al., 2021)) or suffer from confirmation bias (Yalniz et al., 2019).The third line explores consistency regulaizations (Xie et al., 2020a;Sohn et al., 2020;Li et al., 2021) to prevent confirmation bias of inaccurate pseudo labels, e.g., optimizing the consistency loss with weak-strong augmentation, which only work for specific modalities with prior augmentations.Therefore, none of the previous SSL methods achieved three goals simultaneously."
  },
  {
    "index": 1241,
    "source_corpus_id": 222291282,
    "ref_id": "b5",
    "citation_corpus_id": 1463401,
    "start": 2327,
    "end": 2346,
    "title": "HIERARCHICAL MULTISCALE RECURRENT NEURAL NETWORKS",
    "abstract": "Learning both hierarchical and temporal representation has been among the longstanding challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural network, that can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that the proposed model can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence generation. * Yoshua Bengio is CIFAR Senior Fellow.",
    "prev": "The majority of these models are trained to minimize an expected loss using gradient-based optimization, so the problem of gradient estimation for discrete latent variable models has received considerable attention over recent years.",
    "curr": "Existing estimation techniques can be broadly categorized into two groups, based on whether they require one loss evaluation (Glynn, 1990;Williams, 1992;Bengio et al., 2013;Mnih & Gregor, 2014;Chung et al., 2017;Jang et al., 2017;Grathwohl et al., 2018) or multiple loss evaluations (Gu et al., 2016;Mnih & Rezende, 2016;Tucker et al., 2017) per estimate.",
    "next": "These estimators reduce variance by introducing bias or increasing the computational cost with the overall goal being to reduce the total mean squared error."
  },
  {
    "index": 1242,
    "source_corpus_id": 251320513,
    "ref_id": "b2",
    "citation_corpus_id": 221995507,
    "start": 1484,
    "end": 1487,
    "title": "UNCERTAINTY SETS FOR IMAGE CLASSIFIERS USING CONFORMAL PREDICTION",
    "abstract": "Convolutional image classifiers can achieve high predictive accuracy, but quantifying their uncertainty remains an unresolved challenge, hindering their deployment in consequential settings. Existing uncertainty quantification techniques, such as Platt scaling, attempt to calibrate the network's probability estimates, but they do not have formal guarantees. We present an algorithm that modifies any classifier to output a predictive set containing the true label with a user-specified probability, such as 90%. The algorithm is simple and fast like Platt scaling, but provides a formal finite-sample coverage guarantee for every model and dataset. Our method modifies an existing conformal prediction algorithm to give more stable predictive sets by regularizing the small scores of unlikely classes after Platt scaling. In experiments on both Imagenet and Imagenet-V2 with ResNet-152 and other classifiers, our scheme outperforms existing approaches, achieving coverage with sets that are often factors of 5 to 10 smaller than a stand-alone Platt scaling baseline. * equal contribution † Project website here. . Accurate uncertainties for deep learning using calibrated regression. 2018. arXiv:1807.00263.Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems, pp. 6402-6413, 2017.Jing Lei and Larry Wasserman. Distribution-free prediction bands for non-parametric regression.",
    "prev": "Our job is to design a procedure that takes the output of f and post-processes it into quantities with desirable statistical guarantees.",
    "curr": "Split conformal prediction [1,2], which we will henceforth refer to simply as \"conformal prediction\", has been useful in areas such as computer vision [3] and natural language processing [4] to provide such a guarantee.",
    "next": "By measuring the model's performance on a calibration dataset (X i , Y i ) n i=1 of feature-response pairs, conformal prediction post-processes the model to construct prediction sets that bound the miscoverage,\nP Y n+1 / ∈ C(X n+1 ) ≤ α,(1)\nwhere (X n+1 , Y n+1 ) is a new test point, α is a user-specified error rate (e.g., 10%), and C is a function of the model and calibration data that outputs a prediction set."
  },
  {
    "index": 1243,
    "source_corpus_id": 257280401,
    "ref_id": "b26",
    "citation_corpus_id": 204824219,
    "start": 6347,
    "end": 6364,
    "title": "Published as a conference paper at ICLR 2020 WEAKLY SUPERVISED DISENTANGLEMENT WITH GUARANTEES",
    "abstract": "Learning disentangled representations that correspond to factors of variation in real-world data is critical to interpretable and human-controllable machine learning. Recently, concerns about the viability of learning disentangled representations in a purely unsupervised manner has spurred a shift toward the incorporation of weak supervision. However, there is currently no formalism that identifies when and how weak supervision will guarantee disentanglement. To address this issue, we provide a theoretical framework to assist in analyzing the disentanglement guarantees (or lack thereof) conferred by weak supervision when coupled with learning algorithms based on distribution matching. We empirically verify the guarantees and limitations of several weak supervision methods (restricted labeling, match-pairing, and rank-pairing), demonstrating the predictive power and usefulness of our theoretical framework.1. We formalize weakly-supervised learning as distribution matching in an extended space. * Work done during an internship at Google Brain. arXiv:1910.09772v2 [cs.LG] 10 Apr 2020Published as a conference paper at ICLR 2020 2. We propose a set of definitions for disentanglement that can handle correlated factors and are inspired by many existing definitions in the literature(Higgins et al., 2018;Suter et al.,  2018; Ridgeway & Mozer, 2018).3. Using these definitions, we provide a conceptually useful and theoretically rigorous calculus of disentanglement.4. We apply our theoretical framework of disentanglement to analyze three notable classes of weak supervision methods (restricted labeling, match pairing, and rank pairing). We show that although certain weak supervision methods (e.g., style-labeling in style-content disentanglement) do not guarantee disentanglement, our calculus can determine whether disentanglement is guaranteed when multiple sources of weak supervision are combined.5. Finally, we perform extensive experiments to systematically and empirically verify our predicted guarantees. 1FROM UNSUPERVISED TO WEAKLY SUPERVISED DISTRIBUTION MATCHINGOur goal in disentangled representation learning is to identify a latent-variable generative model whose latent variables correspond to ground truth factors of variation in the data. To identify the role that weak supervision plays in providing guarantees on disentanglement, we first formalize the model families we are considering, the forms of weak supervision, and finally the metrics we will use to evaluate and prove components of disentanglement.We consider data-generating processes where S ∈ R n are the factors of variation, with distribution p * (s), and X ∈ R m is the observed data point which is a deterministic function of S, i.e., X = g * (S). Many existing algorithms in unsupervised learning of disentangled representations aim to learn a latent-variable model with prior p(z) and generator g, where g(Z) d = g * (S). However, simply matching the marginal distribution over data is not enough: the learned latent variables Z and the true generating factors S could still be entangled with each other(Locatello et al., 2019).To address the failures of unsupervised learning of disentangled representations, we leverage weak supervision, where information about the data-generating process is conveyed through additional observations. By performing distribution matching on an augmented space (instead of just on the observation X), we can provide guarantees on learned representations.",
    "prev": "(2021) use deep latent variable models for learning to transform independent exogenous factors into endogenous causes that correspond to causally related concepts in the data.",
    "curr": "In the weakly-supervised setting, recent work has focused on using observations of \"real\"-counterfactuals instead of a variable's parents to obtain disentangled representations (Hosoya, 2019;Bouchacourt et al., 2018;Shu et al., 2020;Locatello et al., 2020b).",
    "next": "Besserve et al."
  },
  {
    "index": 1244,
    "source_corpus_id": 2263947,
    "ref_id": "b31",
    "citation_corpus_id": 2187805,
    "start": 6382,
    "end": 6402,
    "title": "A NOTE ON THE EVALUATION OF GENERATIVE MODELS",
    "abstract": "Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria-average log-likelihood, Parzen window estimates, and visual fidelity of samples-are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.",
    "prev": "Unfortunately, density estimates in such high dimensions are known to be very unreliable both in theory (Wasserman, 2006, Ch.",
    "curr": "6) and in practice (Theis et al., 2016).",
    "next": "We can instead ask humans to evaluate the generated images (Denton et al., 2015;Salimans et al., 2016), but while evaluators should be able to distinguish cases where the samples are over-dispersed (support of the model is too large), it may be more difficult to find under-dispersed samples (too concentrated at the modes), or imbalances in the proportions of different shapes, since the samples themselves will be plausible images."
  },
  {
    "index": 1249,
    "source_corpus_id": 250144478,
    "ref_id": "b32",
    "citation_corpus_id": 231632854,
    "start": 9873,
    "end": 9892,
    "title": "IN DEFENSE OF PSEUDO-LABELING: AN UNCERTAINTY-AWARE PSEUDO-LABEL SELEC- TION FRAMEWORK FOR SEMI-SUPERVISED LEARNING",
    "abstract": "The recent research in semi-supervised learning (SSL) is mostly dominated by consistency regularization based methods which achieve strong performance. However, they heavily rely on domain-specific data augmentations, which are not easy to generate for all data modalities. Pseudo-labeling (PL) is a general SSL approach that does not have this constraint but performs relatively poorly in its original formulation. We argue that PL underperforms due to the erroneous high confidence predictions from poorly calibrated models; these predictions generate many incorrect pseudo-labels, leading to noisy training. We propose an uncertainty-aware pseudo-label selection (UPS) framework which improves pseudo labeling accuracy by drastically reducing the amount of noise encountered in the training process. Furthermore, UPS generalizes the pseudo-labeling process, allowing for the creation of negative pseudo-labels; these negative pseudo-labels can be used for multi-label classification as well as negative learning to improve the single-label classification. We achieve strong performance when compared to recent SSL methods on the CIFAR-10 and CIFAR-100 datasets. Also, we demonstrate the versatility of our method on the video dataset UCF-101 and the multi-label dataset Pascal VOC.",
    "prev": "Prevailing SSL methods (Sohn et al., 2020;Berthelot et al., 2019b;a) share a similar strategy: training a model with the labeled data and generating pseudo-labels for unlabeled data based on the model predictions.",
    "curr": "Pseudo-labeling methods (Lee, 2013;Xie et al., 2020;Rizve et al., 2021) predict pseudo-labels for unlabeled data and add them to the training data for re-training.",
    "next": "Consistency-regularization methods (Sajjadi et al., 2016;Laine & Aila, 2016;Berthelot et al., 2019b) apply a random perturbation to an unlabeled image and then use the prediction as the pseudo-label of the same image under a different augmentation."
  },
  {
    "index": 1251,
    "source_corpus_id": 232307359,
    "ref_id": "b44",
    "citation_corpus_id": 3725815,
    "start": 2931,
    "end": 2949,
    "title": "Self-Attention with Relative Position Representations",
    "abstract": "Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al.(2017)achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graphlabeled inputs.",
    "prev": "1).",
    "curr": "The source code representation naturally lends itself to models from natural language processing (NLP), e.g., long short-term memory networks (Hochreiter & Schmidhuber, 1997) (LSTM) or Transformers (Vaswani et al., 2017;Radford et al., 2019;Shaw et al., 2018).",
    "next": "On the other hand, models leveraging the structure representations are typically based on graph neural networks (GNNs) (Kipf & Welling, 2017;Xu et al., 2019;Veličković et al., 2018;You et al., 2019;Hamilton et al., 2017;Li et al., 2015;Klicpera et al., 2020)."
  },
  {
    "index": 1252,
    "source_corpus_id": 256274566,
    "ref_id": "b13",
    "citation_corpus_id": 213987212,
    "start": 11192,
    "end": 11211,
    "title": "Published as a conference paper at ICLR 2020 BRIDGING MODE CONNECTIVITY IN LOSS LANDSCAPES AND ADVERSARIAL ROBUSTNESS",
    "abstract": "Mode connectivity provides novel geometric insights on analyzing loss landscapes and enables building high-accuracy pathways between well-trained neural networks. In this work, we propose to employ mode connectivity in loss landscapes to study the adversarial robustness of deep neural networks, and provide novel methods for improving this robustness. Our experiments cover various types of adversarial attacks applied to different network architectures and datasets. When network models are tampered with backdoor or error-injection attacks, our results demonstrate that the path connection learned using limited amount of bonafide data can effectively mitigate adversarial effects while maintaining the original accuracy on clean data. Therefore, mode connectivity provides users with the power to repair backdoored or error-injected models. We also use mode connectivity to investigate the loss landscapes of regular and robust models against evasion attacks. Experiments show that there exists a barrier in adversarial robustness loss on the path connecting regular and adversarially-trained models. A high correlation is observed between the adversarial robustness loss and the largest eigenvalue of the input Hessian matrix, for which theoretical justifications are provided. Our results suggest that mode connectivity offers a holistic tool and practical means for evaluating and improving adversarial robustness 1 .",
    "prev": "Trigger recovery aims to reverse engineer the trigger pattern (Wang et al., 2019;Guo et al., 2019;Liu et al., 2019;Sun et al., 2020;Liu et al., 2022;Xiang et al., 2022;Hu et al., 2022).",
    "curr": "Backdoor model detection aims to determine if a model is affected by triggers (Chen et al., 2019;Kolouri et al., 2020;Wang et al., 2020;Guo et al., 2021;Shen et al., 2021;Xu et al., 2021), It is worth noting that the detected models still need mitigation methods to remove the trigger (Liu et al., 2018a;Zhao et al., 2020a;Wu & Wang, 2021;Li et al., 2021b;Zeng et al., 2022;Guan et al., 2022).",
    "next": "Backdoor mitigation can also be achieved by robust learning strategies (Borgnia et al., 2021;Huang et al., 2022;Dolatabadi et al., 2022)."
  },
  {
    "index": 1256,
    "source_corpus_id": 54462139,
    "ref_id": "b3",
    "citation_corpus_id": 53221030,
    "start": 2235,
    "end": 2251,
    "title": "JANOSSY POOLING: LEARNING DEEP PERMUTATION- INVARIANT FUNCTIONS FOR VARIABLE-SIZE INPUTS",
    "abstract": "We consider a simple and overarching representation for permutation-invariant functions of sequences (or set functions). Our approach, which we call Janossy pooling, expresses a permutation-invariant function as the average of a permutation-sensitive function applied to all reorderings of the input sequence. This allows us to leverage the rich and mature literature on permutation-sensitive functions to construct novel and flexible permutation-invariant functions. If carried out naively, Janossy pooling can be computationally prohibitive. To allow computational tractability, we consider three kinds of approximations: canonical orderings of sequences, functions with k-order interactions, and stochastic optimization algorithms with random permutations. Our framework unifies a variety of existing work in the literature, and suggests possible modeling and algorithmic extensions. We explore a few in our experiments, which demonstrate improved performance over current state-of-the-art methods.",
    "prev": "The typical approach is to compose elementwise operations with permutation-invariant reduction operations, such as summing (Zaheer et al., 2017) or taking the maximum (Qi et al., 2017) over the whole set.",
    "curr": "Since the reduction operator compresses a set of any size down to a single descriptor, this can be a significant bottleneck in what information about the set can be represented efficiently (Qi et al., 2017;Le & Duan, 2018;Anonymous, 2019).",
    "next": "We take an alternative approach based on an idea explored in Vinyals et al."
  },
  {
    "index": 1258,
    "source_corpus_id": 249062882,
    "ref_id": "b56",
    "citation_corpus_id": 3432876,
    "start": 5189,
    "end": 5211,
    "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
    "abstract": "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.",
    "prev": "In order to explore how barriers in the loss surface expose a model's generalization strategy, we will consider a variety of text classification tasks.",
    "curr": "We focus on Natural Language Inference (NLI; Williams et al., 2018;Consortium et al., 1996), as well as paraphrase and grammatical acceptability 2 IDENTIFYING GENERALIZATION STRATEGIES Finetuning on standard GLUE  datasets often leads to models that perform similarly on in-domain (ID) test sets (Sellam et al., 2021).",
    "next": "In this paper, to evaluate the functional differences between these models, we will measure generalization to OOD domains."
  },
  {
    "index": 1259,
    "source_corpus_id": 257834100,
    "ref_id": "b50",
    "citation_corpus_id": 57761150,
    "start": 4582,
    "end": 4602,
    "title": "REVEALING INTERPRETABLE OBJECT REPRESENTA- TIONS FROM HUMAN BEHAVIOR",
    "abstract": "To study how mental object representations are related to behavior, we estimated sparse, non-negative representations of objects using human behavioral judgments on images representative of 1,854 object categories. These representations predicted a latent similarity structure between objects, which captured most of the explainable variance in human behavioral judgments. Individual dimensions in the low-dimensional embedding were found to be highly reproducible and interpretable as conveying degrees of taxonomic membership, functionality, and perceptual attributes. We further demonstrated the predictive power of the embeddings for explaining other forms of human behavior, including categorization, typicality judgments, and feature ratings, suggesting that the dimensions reflect human conceptual representations of objects beyond the specific task.",
    "prev": "Our embedding space, induced under a novel conditional framework, not only enables the accurate prediction of face similarity, but also Psychological embeddings.",
    "curr": "Multidimensional scaling (MDS) is often used to learn psychological embeddings from human similarity judgments (Zheng et al., 2019;Roads & Love, 2021;Dima et al., 2022;Josephs et al., 2021).",
    "next": "As MDS approaches cannot embed images outside of the training set, researchers have used pretrained models as feature extractors (Sanders & Nosofsky, 2020;Peterson et al., 2018;Attarian et al., 2020), which can introduce unwanted implicit biases (Krishnakumar et al., 2021;Steed & Caliskan, 2021)."
  },
  {
    "index": 1261,
    "source_corpus_id": 229348988,
    "ref_id": "b36",
    "citation_corpus_id": 7147309,
    "start": 2309,
    "end": 2331,
    "title": "SEQUENCE LEVEL TRAINING WITH RECURRENT NEURAL NETWORKS",
    "abstract": "Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster.",
    "prev": "For instance, we may want to avoid toxic content; prevent certain demographic biases; or steer generations towards a certain topic or style.",
    "curr": "Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016;Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b;Tambwekar et al., 2019) to improve certain a priori desirable features.",
    "next": "However, such an optimization process is not infallible; Liu et al."
  },
  {
    "index": 1263,
    "source_corpus_id": 211010532,
    "ref_id": "b36",
    "citation_corpus_id": 3347806,
    "start": 2681,
    "end": 2701,
    "title": "Under review as a conference paper NEURAL LANGUAGE MODELING BY JOINTLY LEARNING SYNTAX AND LEXICON",
    "abstract": "We propose a neural language model capable of unsupervised syntactic structure induction. The model leverages the structure information to form better semantic representations and better language modeling. Standard recurrent neural networks are limited by their structure and fail to efficiently use syntactic information. On the other hand, tree-structured recursive networks usually require additional structural supervision at the cost of human expert annotation. In this paper, We propose a novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure to learn a better language model. In our model, the gradient can be directly back-propagated from the language model loss into the neural parsing network. Experiments show that the proposed model can discover the underlying syntactic structure and achieve state-of-the-art performance on word/character-level language model tasks.",
    "prev": "INTRODUCTION\n\nGrammar induction, which is closely related to unsupervised parsing and latent tree learning, allows one to associate syntactic trees, i.e., constituency and dependency trees, with sentences.",
    "curr": "As grammar induction essentially assumes no supervision from gold-standard syntactic trees, the existing approaches for this task mainly rely on unsupervised objectives, such as language modeling (Shen et al., 2018b;Kim et al., 2019a;b) and cloze-style word prediction (Drozdov et al., 2019) to train their task-oriented models.",
    "next": "On the other hand, there is a trend in the natural language processing (NLP) community of leveraging pre-trained language models (LMs), e.g., ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), as a means of acquiring contextualized word representations."
  },
  {
    "index": 1266,
    "source_corpus_id": 17611960,
    "ref_id": "b25",
    "citation_corpus_id": 14124313,
    "start": 1836,
    "end": 1863,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "1: Shattered gradients in a PL-function.",
    "curr": "& Fergus, 2014;Simonyan & Zisserman, 2015;Szegedy et al., 2015;He et al., 2015).",
    "next": "Even in convex settings, convergence for nonsmooth functions is lower-bounded by 1/ √ N (Bubeck, 2015)."
  },
  {
    "index": 1267,
    "source_corpus_id": 264128411,
    "ref_id": "b26",
    "citation_corpus_id": 14091946,
    "start": 5558,
    "end": 5581,
    "title": "Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies",
    "abstract": "The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture's grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.",
    "prev": "COMPA BENCHMARK\n\nBackground.CompA-order evaluates an ALMs' capability to understand the order of occurrence between multiple acoustic events.CompA-attribute evaluates attribute-binding for acoustic events.The task is to choose (or retrieve) the right caption given the audio and vice-versa.More examples are in Table 4.",
    "curr": "(Levesque et al., 2012), has been earlier used for a variety of language-related tasks (Rudinger et al., 2018;Sakaguchi et al., 2021;Zhao et al., 2018).We are particularly inspired by the Winoground dataset proposed by Thrush et al.",
    "next": "(2022) built for evaluating visio-linguistic compositional reasoning.Each instance in each benchmark has two (or three) audio-caption pairs, where each audio has the same acoustic events but with a different composition (i.e., for example, different order of occurrence of the events in the audio), and each caption has the same words but with minute differences.The task of an ALM is to match the right caption with the right audio and vice-versa."
  },
  {
    "index": 1269,
    "source_corpus_id": 67855286,
    "ref_id": "b29",
    "citation_corpus_id": 3005102,
    "start": 7183,
    "end": 7210,
    "title": "A RECURRENT NEURAL NETWORK WITHOUT CHAOS",
    "abstract": "We introduce an exceptionally simple gated recurrent neural network (RNN) that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.",
    "prev": "Connections between dynamical systems and RNNs have not been well explored.",
    "curr": "Laurent & von Brecht (2017) study the behavior of dynamical systems induced by recurrent networks, and show that LSTMs and GRUs exhibit chaotic dynamics in the absence of input data.",
    "next": "They propose a simplified gated RNN named the chaos free network (CFN), that has non-chaotic dynamics and achieves comparable performance to LSTMs and GRUs on language modeling."
  },
  {
    "index": 1271,
    "source_corpus_id": 261582259,
    "ref_id": "b16",
    "citation_corpus_id": 3568073,
    "start": 20621,
    "end": 20642,
    "title": "PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION",
    "abstract": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at 1024 2 . We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CELEBA dataset.",
    "prev": "We use CelebA-HQ and ImageNet in our experiments.",
    "curr": "CelebA-HQ (Karras et al., 2018) is a high-quality subset of CelebA (Liu et al., 2015) which consists of 30,000 images of faces from human celebrities.",
    "next": "ImageNet (Deng et al., 2009) contains 1,281,167 images spanning 1000 different classes and is a widely-used dataset for generation and vision tasks."
  },
  {
    "index": 1272,
    "source_corpus_id": 201657791,
    "ref_id": "b4",
    "citation_corpus_id": 62823235,
    "start": 21356,
    "end": 21359,
    "title": "Universal Stagewise Learning for Non-Convex Problems with Convergence on Averaged Solutions",
    "abstract": "Although stochastic gradient descent (SGD) method and its variants (e.g., stochastic momentum methods, ADAGRAD) are the choice of algorithms for solving nonconvex problems (especially deep learning), there still remain big gaps between the theory and the practice with many questions unresolved. For example, there is still a lack of theories of convergence for SGD and its variants that use stagewise step size and return an averaged solution in practice. In addition, theoretical insights of why adaptive step size of ADAGRAD could improve non-adaptive step size of SGD is still missing for non-convex optimization. This paper aims to address these questions and fill the gap between theory and practice. We propose a universal stagewise optimization framework for a broad family of non-smooth non-convex (namely weakly convex) problems with the following key features: (i) at each stage any suitable stochastic convex optimization algorithms (e.g., SGD or ADAGRAD) that return an averaged solution can be employed for minimizing a regularized convex problem; (ii) the step size is decreased in a stagewise manner; (iii) an averaged solution is returned as the final solution that is selected from all stagewise averaged solutions with sampling probabilities increasing as the stage number. Our theoretical results of stagewise ADAGRAD exhibit its adaptive convergence, therefore shed insights on its faster convergence for problems with sparse stochastic gradients than stagewise SGD. To the best of our knowledge, these new results are the first of their kind for addressing the unresolved issues of existing theories mentioned earlier.",
    "prev": "We aim to achieve an adaptive convergence by using PPD-AdaGrad.",
    "curr": "The analysis of PPD-AdaGrad is inspired by the analysis of AdaGrad for non-convex minimization problems [5].",
    "next": "The key difference is that we have to carefully deal with the primal-dual updates for the non-convex min-max problem."
  },
  {
    "index": 1273,
    "source_corpus_id": 53717167,
    "ref_id": "b22",
    "citation_corpus_id": 3488815,
    "start": 1908,
    "end": 1911,
    "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
    "abstract": "Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. * Authors ordered alphabetically.",
    "prev": "The code is available at\n\nIntroduction\n\nDespite their impressive performance on many supervised learning tasks, deep neural networks (DNNs) are often highly susceptible to adversarial perturbations imperceptible to the human eye [16,37].",
    "curr": "These \"adversarial attacks\" have received enormous attention in the machine learning literature over recent years [7,8,16,22,23,27,31,32,39].",
    "next": "Adversarial attack studies have mainly focused on developing effective attack and defense schemes."
  },
  {
    "index": 1274,
    "source_corpus_id": 220055784,
    "ref_id": "b38",
    "citation_corpus_id": 212718244,
    "start": 2783,
    "end": 2786,
    "title": "Published as a conference paper at ICLR 2020 TRAINING INDIVIDUALLY FAIR ML MODELS WITH SENSITIVE SUBSPACE ROBUSTNESS",
    "abstract": "We consider training machine learning models that are fair in the sense that their performance is invariant under certain sensitive perturbations to the inputs. For example, the performance of a resume screening system should be invariant under changes to the gender and/or ethnicity of the applicant. We formalize this notion of algorithmic fairness as a variant of individual fairness and develop a distributionally robust optimization approach to enforce it during training. We also demonstrate the effectiveness of the approach on two ML tasks that are susceptible to gender and racial biases.",
    "prev": "Despite its benefits, individual fairness was dismissed as impractical because there is no consensus on which users are similar for many ML tasks.",
    "curr": "Fortunately, there is a flurry of recent work that addresses this issue [15,36,39,29].",
    "next": "In this paper, we assume there is a similarity metric for the ML task at hand and consider the follow-up task of enforcing individual fairness."
  },
  {
    "index": 1275,
    "source_corpus_id": 260519400,
    "ref_id": "b38",
    "citation_corpus_id": 53116133,
    "start": 3315,
    "end": 3318,
    "title": "POINCARÉ GLOVE: HYPERBOLIC WORD EMBEDDINGS",
    "abstract": "Words are not created equal. In fact, they form an aristocratic graph with a latent hierarchical structure that the next generation of unsupervised learned word embeddings should reveal. In this paper, justified by the notion of delta-hyperbolicity or tree-likeliness of a space, we propose to embed words in a Cartesian product of hyperbolic spaces which we theoretically connect to the Gaussian word embeddings and their Fisher geometry. This connection allows us to introduce a novel principled hypernymy score for word embeddings. Moreover, we adapt the well-known Glove algorithm to learn unsupervised word embeddings in this type of Riemannian manifolds. We further explain how to solve the analogy task using the Riemannian parallel transport that generalizes vector arithmetics to this new type of geometry. Empirically, based on extensive experiments, we prove that our embeddings, trained unsupervised, are the first to simultaneously outperform strong and popular baselines on the tasks of similarity, analogy and hypernymy detection. In particular, for word hypernymy, we obtain new state-of-the-art on fully unsupervised WBLESS classification accuracy. * All authors contributed equally.",
    "prev": "Compared to a flat Euclidean geometry, a hyperbolic space shows a higher embedding accuracy under fewer dimensions in such cases.",
    "curr": "Because a wide variety of real-world data encompasses some type of latent hierarchical structures [14,28,22,16], it has been empirically proven that a hyperbolic space is able to capture such intrinsic features thorough representation learning [16,8,29,39,19,2,11].",
    "next": "Motivated by such expressive characteristics, various machine learning methods, including support vector machines [7] and neural networks [9,12,25,27,6] have derived the analogous benefits from the introduction of a hyperbolic space, with an aim to improve the performance on advanced tasks beyond just representing data."
  },
  {
    "index": 1276,
    "source_corpus_id": 264590778,
    "ref_id": "b13",
    "citation_corpus_id": 221878771,
    "start": 2185,
    "end": 2206,
    "title": "REALTOXICITYPROMPTS: Evaluating Neural Toxic Degeneration in Language Models",
    "abstract": "Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RE-ALTOXICITYPROMPTS, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widelyused toxicity classifier. Using REALTOXICI-TYPROMPTS, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data-or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning \"bad\" words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2;Radford et al., 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.",
    "prev": "INTRODUCTION\n\n\"We want AI agents that can discover like we can, not which contain what we have discovered.\"",
    "curr": "--Prof.Richard Sutton, The Bitter Lesson, 2019\n\nBy virtue of their ability to \"predict the next token(s)\", contemporary pre-trained language models (LMs) have shown remarkable proficiency in memorizing extensive corpora, thereby enabling the generation of text indistinguishable from human-produced content (Brown et al., 2020).However, successful memorization of human knowledge does not assure a model's propensity to perform as per societal expectations.Recent research has exposed behavioral anomalies in these LMs (Weidinger et al., 2022), which include the generation of harmful content (Gehman et al., 2020;Bommasani et al., 2021), the reinforcement of bias (Venkit et al., 2022;Liu et al., 2022), and the dissemination of disinformation (Tamkin et al., 2021;Lin et al., 2022).This process of enhancing desirable societal behaviors and inhibiting undesirable ones is commonly referred to as \"social alignment\" (Gabriel, 2020;Taylor et al., 2016).",
    "next": "Supervised Fine-Tuning (SFT) presents a straightforward method for achieving alignment by training LMs using socially aligned data (Figure 1 [a]).However, this method often yields models susceptible to adversarial attacks, like \"jailbreaking prompting\" (Subhash, 2023;Xu et al., 2021), due to limited exposure to misaligned data during training (Amodei et al., 2016).To address this, a more advanced technique, \"reward modeling\" has been proposed (Leike et al., 2018;Christiano et al., 2017).This involves training a reward model as a surrogate for human judgment to guide the optimization of the LM (e.g., OpenAI's RLHF, Figure 1 [b]).However, it is crucial"
  },
  {
    "index": 1277,
    "source_corpus_id": 240288910,
    "ref_id": "b15",
    "citation_corpus_id": 227209335,
    "start": 2190,
    "end": 2208,
    "title": "SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS",
    "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reversetime SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in diffusion probabilistic modeling and scorebased generative modeling, and allows for new sampling procedures. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, which enables exact likelihood computation, and improved sampling efficiency. In addition, our framework enables conditional generation with an unconditional model, as we demonstrate with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high fidelity generation of 1024ˆ1024 images for the first time from a score-based generative model.",
    "prev": "Introduction\n\nModern unsupervised representation learning techniques can generate images of our world with intricate detail (e.g.",
    "curr": "Karras et al., 2019;Song et al., 2020;Razavi et al., 2019), and yet, the latent representations from which these images are generated remain entangled and challenging to interpret (Schölkopf et al., 2021;Locatello et al., 2019).",
    "next": "At the same time, the success of pre-trained of transformers (Devlin et al., 2018;Brown et al., 2020) shows that advances in our ability to disentangle the underlying generative factors can lead to dramatic improvements in the sample complexity of downstream supervised tasks (Bengio & LeCun, 2007;Bengio et al., 2013)."
  },
  {
    "index": 1282,
    "source_corpus_id": 8257350,
    "ref_id": "b22",
    "citation_corpus_id": 4167933,
    "start": 4015,
    "end": 4032,
    "title": "CONVOLUTIONAL NEURAL NETWORKS WITH LOW- RANK REGULARIZATION",
    "abstract": "Large CNNs have delivered impressive performance in various computer vision applications. But the storage and computation requirements make it problematic for deploying these models on mobile devices. Recently, tensor decompositions have been used for speeding up CNNs. In this paper, we further develop the tensor decomposition technique. We propose a new algorithm for computing the low-rank tensor decomposition for removing the redundancy in the convolution kernels. The algorithm finds the exact global optimizer of the decomposition and is more effective than iterative methods. Based on the decomposition, we further propose a new method for training low-rank constrained CNNs from scratch. Interestingly, while achieving a significant speedup, sometimes the lowrank constrained CNNs delivers significantly better performance than their nonconstrained counterparts. On the CIFAR-10 dataset, the proposed low-rank NIN model achieves 91.31% accuracy (without data augmentation), which also improves upon state-of-the-art result. We evaluated the proposed method on CIFAR-10 and ILSVRC12 datasets for a variety of modern CNNs, including AlexNet, NIN, VGG and GoogleNet with success. For example, the forward time of VGG-16 is reduced by half while the performance is still comparable. Empirical success suggests that low-rank tensor decompositions can be a very useful tool for speeding up large CNNs.",
    "prev": "Thus much more costly computing resources are in demand for RNN based models.",
    "curr": "To alleviate the above problems, several techniques can be employed, i.e., low rank approximation (Sainath et al., 2013;Jaderberg et al., 2014;Lebedev et al., 2014;Tai et al., 2016), sparsity (Liu et al., 2015;Han et al., 2015;, and quantization.",
    "next": "All of them are build on the redundancy of current networks and can be combined."
  },
  {
    "index": 1283,
    "source_corpus_id": 3708505,
    "ref_id": "b1",
    "citation_corpus_id": 13757156,
    "start": 13413,
    "end": 13430,
    "title": "Gaussian Process Behaviour in Wide Deep Neural Networks",
    "abstract": "Whilst deep neural networks have shown great empirical success, there is still much work to be done to understand their theoretical properties. In this paper, we study the relationship between random, wide, fully connected, feedforward networks with more than one hidden layer and Gaussian processes with a recursive kernel definition. We show that, under broad conditions, as we make the architecture increasingly wide, the implied random function converges in distribution to a Gaussian process, formalising and extending existing results byNeal (1996)to deep networks. To evaluate convergence rates empirically, we use maximum mean discrepancy. We then compare finite Bayesian deep networks from the literature to Gaussian processes in terms of the key predictive quantities of interest, finding that in some cases the agreement can be very close. We discuss the desirability of Gaussian process behaviour and review non-Gaussian alternative models from the literature. 1",
    "prev": "In Appendix C we provide an alternative derivation, in terms of marginalization over intermediate layers, which does not depend on the order of limits, in the case of a Gaussian prior on the weights.",
    "curr": "A concurrent work (Anonymous, 2018) further derives the convergence rate towards a GP if all layers are taken to infinite width simultaneously.",
    "next": "Suppose that z l−1 j is a GP, identical and independent for every j (and hence x l j (x) are independent and identically distributed)."
  },
  {
    "index": 1286,
    "source_corpus_id": 211132391,
    "ref_id": "b3",
    "citation_corpus_id": 2410333,
    "start": 19207,
    "end": 19229,
    "title": "DECISION-BASED ADVERSARIAL ATTACKS: RELIABLE ATTACKS AGAINST BLACK-BOX MACHINE LEARNING MODELS",
    "abstract": "Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient-or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox (https://github.com/bethgelab/foolbox).Gradient-basedModel M",
    "prev": "ADVERSARIAL ATTACKS AS WORST CASE ANALYSIS\n\nWe first evaluate the ability of conditional generative models to detect standard attacks, and then try to detect attacks designed to fool the detector (likelihood function).",
    "curr": "We evalulate both the gradient based Carlini-Wagner L 2 attack (CW-L 2 ) (Carlini & Wagner, 2017b) and the gradient free boundary attack (Brendel et al., 2018).",
    "next": "Results are shown in table 2 on the left."
  },
  {
    "index": 1289,
    "source_corpus_id": 174797767,
    "ref_id": "b51",
    "citation_corpus_id": 1793573,
    "start": 10366,
    "end": 10370,
    "title": "Neuronal Synchrony in Complex-Valued Deep Networks",
    "abstract": "Deep learning has recently led to great successes in tasks such as image recognition (e.gKrizhevsky et al., 2012). However, deep networks are still outmatched by the power and versatility of the brain, perhaps in part due to the richer neuronal computations available to cortical circuits. The challenge is to identify which neuronal mechanisms are relevant, and to find suitable abstractions to model them. Here, we show how aspects of spike timing, long hypothesized to play a crucial role in cortical information processing, could be incorporated into deep networks to build richer, versatile representations.We introduce a neural network formulation based on complex-valued neuronal units that is not only biologically meaningful but also amenable to a variety of deep learning frameworks. Here, units are attributed both a firing rate and a phase, the latter indicating properties of spike timing. We show how this formulation qualitatively captures several aspects thought to be related to neuronal synchrony, including gating of information processing and dynamic binding of distributed object representations. Focusing on the latter, we demonstrate the potential of the approach in several simple experiments. Thus, neuronal synchrony could be a flexible mechanism that fulfills multiple functional roles in deep networks.",
    "prev": "Synthetic visual tasks There is a long history of using synthetic visual recognition challenges for systematically evaluating computer vision algorithms [50,51].",
    "curr": "For instance, there are several studies that have increased visual classification difficulty by adding clutter to images [52][53][54] and/or parameterically controlling intra-class image variability [12,48,55,56].",
    "next": "Similar to the current study, synthetic object categorization challenges have also been used to study how feedback mechanisms can help recognition under various degraded conditions [38,45]."
  },
  {
    "index": 1293,
    "source_corpus_id": 252992725,
    "ref_id": "b13",
    "citation_corpus_id": 237347130,
    "start": 32925,
    "end": 32945,
    "title": "Published as a conference paper at ICLR 2022 TRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION",
    "abstract": "Since the introduction of the transformer model byVaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark. 1 1 Code & models: https://github.com/ofirpress/attention_with_linear_biases 2Figure 7in the appendix plots training speed, in words per second, against L.",
    "prev": "Using a combination of scratchpad (a.k.a.",
    "curr": "\"chain-of-thought\") (Nye et al., 2021, Wei et al., 2022 and recency bias (Press et al., 2022), we demonstrate that Transformers can be guided towards learning recurrent (depth-T ) solutions, which generalize outof-distribution and to longer sequence lengths (Figure 7, yellow curves).",
    "next": "Details are deferred to Section B.2.3."
  },
  {
    "index": 1296,
    "source_corpus_id": 52895832,
    "ref_id": "b10",
    "citation_corpus_id": 65455367,
    "start": 2149,
    "end": 2169,
    "title": "ON THE CONVERGENCE OF ADAM AND BEYOND",
    "abstract": "Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSPROP, ADAM, ADADELTA, NADAM are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where ADAM does not converge to the optimal solution, and describe the precise problems with the previous analysis of ADAM algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with \"long-term memory\" of past gradients, and propose new variants of the ADAM algorithm which not only fix the convergence issues but often also lead to improved empirical performance.",
    "prev": "INTRODUCTION\n\nFirst-order optimization algorithms with adaptive learning rate play an important role in deep learning due to their efficiency in solving large-scale optimization problems.",
    "curr": "Denote g t ∈ R n as the gradient of loss function f with respect to its parameters θ ∈ R n at timestep t, then the general updating rule of these algorithms can be written as follows (Reddi et al., 2018):\nθ t+1 = θ t − α t √ v t m t .",
    "next": "(1)\nIn the above equation, m t φ(g 1 , ."
  },
  {
    "index": 1297,
    "source_corpus_id": 251564473,
    "ref_id": "b29",
    "citation_corpus_id": 215238853,
    "start": 3593,
    "end": 3610,
    "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices",
    "abstract": "Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resourcelimited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning. Basically, MobileBERT is a thin version of BERT LARGE , while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we first train a specially designed teacher model, an invertedbottleneck incorporated BERT LARGE model. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is 4.3× smaller and 5.5× faster than BERT BASE while achieving competitive results on well-known benchmarks. On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERT BASE ), and 62 ms latency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERT BASE ).",
    "prev": "This ignores potentially useful internal representations which can also be levered for knowledge transfer.",
    "curr": "Various extensions have been proposed in the literature along these lines (see, e.g., [Sun et al., 2020, Aguilar et al., 2020, Li et al., 2019, Sun et al., 2019 and references therein).",
    "next": "However, despite their success, most use the teacher model in a black-box manner, and do not fully utilize the domain understanding it contains [Cho andHariharan, 2019, Stanton et al., 2021]."
  },
  {
    "index": 1298,
    "source_corpus_id": 259313791,
    "ref_id": "b13",
    "citation_corpus_id": 213987212,
    "start": 11192,
    "end": 11211,
    "title": "Published as a conference paper at ICLR 2020 BRIDGING MODE CONNECTIVITY IN LOSS LANDSCAPES AND ADVERSARIAL ROBUSTNESS",
    "abstract": "Mode connectivity provides novel geometric insights on analyzing loss landscapes and enables building high-accuracy pathways between well-trained neural networks. In this work, we propose to employ mode connectivity in loss landscapes to study the adversarial robustness of deep neural networks, and provide novel methods for improving this robustness. Our experiments cover various types of adversarial attacks applied to different network architectures and datasets. When network models are tampered with backdoor or error-injection attacks, our results demonstrate that the path connection learned using limited amount of bonafide data can effectively mitigate adversarial effects while maintaining the original accuracy on clean data. Therefore, mode connectivity provides users with the power to repair backdoored or error-injected models. We also use mode connectivity to investigate the loss landscapes of regular and robust models against evasion attacks. Experiments show that there exists a barrier in adversarial robustness loss on the path connecting regular and adversarially-trained models. A high correlation is observed between the adversarial robustness loss and the largest eigenvalue of the input Hessian matrix, for which theoretical justifications are provided. Our results suggest that mode connectivity offers a holistic tool and practical means for evaluating and improving adversarial robustness 1 .",
    "prev": "Trigger recovery aims to reverse engineer the trigger pattern (Wang et al., 2019;Guo et al., 2019;Liu et al., 2019;Sun et al., 2020;Liu et al., 2022;Xiang et al., 2022;Hu et al., 2022).",
    "curr": "Backdoor model detection aims to determine if a model is affected by triggers (Chen et al., 2019;Kolouri et al., 2020;Wang et al., 2020;Guo et al., 2021;Shen et al., 2021;Xu et al., 2021), It is worth noting that the detected models still need mitigation methods to remove the trigger (Liu et al., 2018a;Zhao et al., 2020a;Wu & Wang, 2021;Li et al., 2021b;Zeng et al., 2022;Guan et al., 2022).",
    "next": "Backdoor mitigation can also be achieved by robust learning strategies (Borgnia et al., 2021;Huang et al., 2022;Dolatabadi et al., 2022)."
  },
  {
    "index": 1299,
    "source_corpus_id": 220127956,
    "ref_id": "b28",
    "citation_corpus_id": 14124313,
    "start": 23613,
    "end": 23617,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "In particular, we desire to understand, if the insights gained from experiments on smaller networks can be generalized to more complex networks and datasets.",
    "curr": "Understanding Influence Functions in Deep Architectures\n\nSetup: In this section, we evaluate the accuracy of influence estimates using MNIST [21] and CIFAR-10 [22] datasets across different network architectures including small CNN [3], LeNet [27], ResNet-18, ResNet-50 [28], VGG-13 and VGG-14 [29] 4 .",
    "next": "To compute influence estimates, we choose two test points for each architecture: a) the test-point with the highest loss, and b) the test-point at the 50 th percentile of the losses of all test points."
  },
  {
    "index": 1300,
    "source_corpus_id": 235422251,
    "ref_id": "b29",
    "citation_corpus_id": 10048734,
    "start": 3487,
    "end": 3505,
    "title": "A Linear Programming Formulation for Global Inference in Natural Language Tasks",
    "abstract": "Given a collection of discrete random variables representing outcomes of learned local predictors in natural language, e.g., named entities and relations, we seek an optimal global assignment to the variables in the presence of general (non-sequential) constraints. Examples of these constraints include the type of arguments a relation can take, and the mutual activity of different relations, etc. We develop a linear programming formulation for this problem and evaluate it in the context of simultaneously learning named entities and relations. Our approach allows us to efficiently incorporate domain and task specific constraints at decision time, resulting in significant improvements in the accuracy and the \"human-like\" quality of the inferences.",
    "prev": "INTRODUCTION\n\nStructured prediction is a field of machine learning where outputs are expected to obey some predefined discrete structure.",
    "curr": "Instances of structured prediction with various output structures occur in many applications, including computer vision (e.g., scene graph generation (Johnson et al., 2015) with graph-structured output), natural language processing (e.g., linguistic parsing (Niculae et al., 2018) with tree-structured output, relation extraction (Roth & Yih, 2004) with tuple-structured output) or modeling the spatial structure of physical entities and processes (Jiang, 2020).",
    "next": "A key difficulty faced by all models is to tractably model interdependencies between different parts of the output."
  },
  {
    "index": 1301,
    "source_corpus_id": 108300988,
    "ref_id": "b32",
    "citation_corpus_id": 1957433,
    "start": 1415,
    "end": 1439,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.",
    "curr": "INTRODUCTION 1\n\nPretrained word embeddings (Mikolov et al., 2013;Pennington et al., 2014) are a staple tool for NLP.",
    "next": "These models provide continuous representations for word types, typically learned from cooccurrence statistics on unlabeled data, and improve generalization of downstream models across many domains."
  },
  {
    "index": 1306,
    "source_corpus_id": 209862298,
    "ref_id": "b7",
    "citation_corpus_id": 52967399,
    "start": 1760,
    "end": 1780,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "We hope our study can inspire the design of novel optimization techniques, and help discover and analyze the behaviors of DNNs.",
    "curr": "INTRODUCTION\n\nIn the past few decades, deep neural networks (DNNs) have achieved remarkable success in various difficult tasks of machine learning (Krizhevsky et al., 2012;Graves et al., 2013;He et al., 2016;Silver et al., 2017;Devlin et al., 2019).",
    "next": "Albeit the great progress DNNs have made, there are still many problems which have not been thoroughly studied, such as the expressivity and optimization of DNNs."
  },
  {
    "index": 1307,
    "source_corpus_id": 237581512,
    "ref_id": "b15",
    "citation_corpus_id": 3292002,
    "start": 35221,
    "end": 35246,
    "title": "GRAPH ATTENTION NETWORKS",
    "abstract": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-theart results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a proteinprotein interaction dataset (wherein test graphs remain unseen during training).",
    "prev": "In GCN, AGGREGATE is implemented as weighted average with respect to the inverse of square root of node degrees:\nh k i = σ W k j∈N (i)∪{i} α ij h k−1 j ,(6)\nwhere W k is a learnable matrix, α ij = 1/ |N (i)| · |N (j)|, and σ is activation function.",
    "curr": "Graph Attention Networks (GAT) (Veličković et al., 2018).",
    "next": "In GAT, AGGREGATE is implemented as multi-head self-attention:\nh k i = S s=1 σ W k,s j∈N (i)∪{i} SOFTMAX α k,s ij h k−1 j ,(7)\nwhere α k,s ij = LeakyReLU w k,s W k,s h k i W k,s h k j is (unnormalized) attention weight, w is a learnable vector, denotes concatenate operation, and S is the number of attention heads."
  },
  {
    "index": 1308,
    "source_corpus_id": 245650881,
    "ref_id": "b7",
    "citation_corpus_id": 28202810,
    "start": 34895,
    "end": 34918,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "What we propose is a transfer learning mechanism that can be combined with any existing DRL methods.",
    "curr": "For environments with discrete action spaces (CartPole, Acrobot), we use the DQN algorithm (Mnih et al., 2015), while for environments with continuous action spaces (Cheetah-Run, HalfCheetah, Hopper, Walker2d, 3DBall), we use the SAC algorithm (Haarnoja et al., 2018).",
    "next": "To ensure a fair comparison, we use the same base DRL learner with the same hyperparameter settings for all tested methods, as detailed in Appendix E.1.3."
  },
  {
    "index": 1310,
    "source_corpus_id": 252780502,
    "ref_id": "b29",
    "citation_corpus_id": 31934728,
    "start": 2932,
    "end": 2954,
    "title": "A System for Identifying and Exploring Text Repetition in Large Historical Document Corpora",
    "abstract": "We present a software for retrieving and exploring duplicated text passages in low quality OCR historical text corpora. The system combines NCBI BLAST, a software created for comparing and aligning biological sequences, with the Solr search and indexing engine, providing a web interface to easily query and browse the clusters of duplicated texts. We demonstrate the system on a corpus of scanned and OCR-recognized Finnish newspapers and journals from years 1771 to 1910.",
    "prev": "[2022] conclude: \"Further work is yet needed to better understand the role of test set leakage in the performance of LMs.\"",
    "curr": "Patterns of duplication are also themselves of interest, for studying the dissemination of reproduced content such as literature or news [Cordell, 2015, Smith et al., 2015, Vesanto et al., 2017.",
    "next": "In contrast to the literature on semantic textual similarity, where deep neural architectures predominate -e.g."
  },
  {
    "index": 1312,
    "source_corpus_id": 53327717,
    "ref_id": "b25",
    "citation_corpus_id": 14124313,
    "start": 2784,
    "end": 2811,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "However, there is no analogous result of practical interest for the non-convex optimization problem of a deep neural network.",
    "curr": "An illustration of this issue is the diversity of learning rate schedules used to train deep convolutional networks with SGD: Simonyan & Zisserman (2015) and He et al.",
    "next": "(2016) adapt the learning rate according to the validation performance, while Szegedy et al."
  },
  {
    "index": 1315,
    "source_corpus_id": 249191952,
    "ref_id": "b83",
    "citation_corpus_id": 236477395,
    "start": 2139,
    "end": 2142,
    "title": "Unsupervised Energy-based Adversarial Domain Adaptation for Cross-domain Text Classification",
    "abstract": "Transferring knowledge from a label-rich domain (source domain) to a label-scarce domain (target domain) for pervasive cross-domain Text Classification (TC) is a non-trivial task. To overcome this issue, we propose EADA, a novel unsupervised energy-based adversarial domain adaptation framework. First, a deep pre-trained language model (e.g. RoBERTa) is leveraged as a shared feature extractor that maps the text sequences from both source and target domains to a feature space. Since the source features maintain good feature discriminability because of the full supervised training, we design a method that encourages target features towards the source ones via adversarial learning. An autoencoder is designed as an energy function that focuses on reconstructing source feature embeddings, while the feature extractor aims to generate source-like target feature embeddings to deceive the autoencoder. In this manner, the target feature embeddings become domain-invariant and inherit great discriminability. Extensive experiments on multidomain sentiment classification (Amazon review dataset) and Yes/No question-answering classification (BoolQ and MARCO dataset) are conducted. The experimental results validate that EADA largely alleviates the domain discrepancy while maintaining excellent discriminability and achieves state-of-the-art cross-domain TC performance.",
    "prev": "Extensive experiments demonstrate BETA outperforms existing methods on all DABP benchmarks, and is even comparable with the standard domain adaptation methods that use the source-domain data.",
    "curr": "Introduction\n\nUnsupervised domain adaptation (UDA) [19,47,59,85] aims to transfer knowledge from a labeled source domain to an unlabeled target domain and has wide applications [66,26,84,81].",
    "next": "However, UDA methods require to access the source-domain data, thus raising concerns about data privacy and portability issues."
  },
  {
    "index": 1318,
    "source_corpus_id": 249712407,
    "ref_id": "b2",
    "citation_corpus_id": 239050418,
    "start": 2555,
    "end": 2558,
    "title": "A FINE-GRAINED ANALYSIS ON DISTRIBUTION SHIFT",
    "abstract": "Robustness to distribution shifts is critical for deploying machine learning models in the real world. Despite this necessity, there has been little work in defining the underlying mechanisms that cause these shifts and evaluating the robustness of algorithms across multiple, different distribution shifts. To this end, we introduce a framework that enables fine-grained analysis of various distribution shifts. We provide a holistic analysis of current state-of-the-art methods by evaluating 19 distinct methods grouped into five categories across both synthetic and real-world datasets. Overall, we train more than 85K models. Our experimental framework can be easily extended to include new methods, shifts, and datasets. We find, unlike previous work (Gulrajani & Lopez-Paz, 2021), that progress has been made over a standard ERM baseline; in particular, pretraining and augmentations (learned or heuristic) offer large gains in many cases. However, the best methods are not consistent over different datasets and shifts. . Model-agnostic meta-learning for fast adaptation of deep networks. In . Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. In Achieving robustness in the wild via adversarial mixing with disentangled representations. In",
    "prev": "Given data from multiple domains that share a common optimal predictor, the domain generalization (DG) task [1,2] encapsulates this challenge by evaluating accuracy on an unseen domain.",
    "curr": "Recent empirical studies of DG algorithms [3,4] have characterized different kinds of distribution shifts across domains.",
    "next": "Using MNIST as an example, a diversity shift is when domains are created either by adding new values of a spurious attribute like rotation (e.g., Rotated-MNIST dataset [5,6]) whereas a correlation shift is when domains exhibit different values of correlation between the class label and a spurious attribute like color (e.g., Colored-MNIST [7])."
  },
  {
    "index": 1322,
    "source_corpus_id": 252735281,
    "ref_id": "b4",
    "citation_corpus_id": 221802286,
    "start": 6876,
    "end": 6898,
    "title": "PRUNING NEURAL NETWORKS AT INITIALIZATION: WHY ARE WE MISSING THE MARK?",
    "abstract": "Recent work has explored the possibility of pruning neural networks at initialization. We assess proposals for doing so: SNIP (Lee et al., 2019), GraSP (Wang  et al., 2020), SynFlow (Tanaka et al., 2020, and magnitude pruning. Although these methods surpass the trivial baseline of random pruning, they remain below the accuracy of magnitude pruning after training, and we endeavor to understand why. We show that, unlike pruning after training, accuracy is the same or higher when randomly shuffling which weights these methods prune within each layer or sampling new initial values. As such, the per-weight pruning decisions made by these methods can be replaced by a per-layer choice of the fraction of weights to prune. This property undermines the claimed justifications for these methods and suggests broader challenges with the underlying pruning heuristics, the desire to prune at initialization, or both.Early pruning methods. We study the following methods for pruning early in training. We assess the fidelity of our reimplimentations of these methods in the appendices specified below.Random. This method issues each weight a random score s ∼ Uniform(0, 1) and removes weights with the lowest scores. Empirically, it prunes each layer to approximately sparsity s. Random pruning is a naive method for early pruning whose performance any new proposal should surpass.",
    "prev": "Second, how does SGD starting from the masked rewind point extract and use this information?",
    "curr": "The mask indeed provides actionable information beyond that stored in the network weights at the rewind pointusing a random mask or even pruning the smallest magnitude weights at this point leads to higher error after training (Frankle et al., 2021).",
    "next": "Third, why are we forced to prune only a small fraction of weights at each iteration?"
  },
  {
    "index": 1323,
    "source_corpus_id": 246863713,
    "ref_id": "b44",
    "citation_corpus_id": 227209335,
    "start": 2318,
    "end": 2338,
    "title": "SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS",
    "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reversetime SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in diffusion probabilistic modeling and scorebased generative modeling, and allows for new sampling procedures. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, which enables exact likelihood computation, and improved sampling efficiency. In addition, our framework enables conditional generation with an unconditional model, as we demonstrate with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high fidelity generation of 1024ˆ1024 images for the first time from a score-based generative model.",
    "prev": "INTRODUCTION\n\nDenoising diffusion probabilistic models (DDPMs) (Sohl-Dickstein et al., 2015;Ho et al., 2020) have recently outperformed alternative approaches to model the distribution of natural images both in the realism of individual samples and their diversity .",
    "curr": "These advantages of diffusion models are successfully exploited in applications, such as colorization (Song et al., 2021b), inpainting (Song et al., 2021b), super-resolution (Saharia et al., 2021;Li et al., 2022), and semantic editing (Meng et al., 2021), where DDPM often achieve more impressive results compared to GANs.",
    "next": "One crucial feature of diffusion models is the existence of a deterministic invertible mapping from the data distribution to the limiting distribution of the diffusion process, commonly being a standard normal distribution."
  },
  {
    "index": 1324,
    "source_corpus_id": 248811614,
    "ref_id": "b19",
    "citation_corpus_id": 13123084,
    "start": 2790,
    "end": 2810,
    "title": "Published as a conference paper at ICLR 2017 TEMPORAL ENSEMBLING FOR SEMI-SUPERVISED LEARNING",
    "abstract": "In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce self-ensembling, where we form a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the (non-augmented) classification error rate from 18.44% to 7.05% in SVHN with 500 labels and from 18.63% to 16.55% in CIFAR-10 with 4000 labels, and further to 5.12% and 12.16% by enabling the standard augmentations. We additionally obtain a clear improvement in CIFAR-100 classification accuracy by using random images from the Tiny Images dataset as unlabeled extra inputs during training. Finally, we demonstrate good tolerance to incorrect labels.",
    "prev": "To alleviate such reliance, semi-supervised learning (SSL) (Zhu, 2005;Zhu & Goldberg, 2009;Sohn et al., 2020;Rosenberg et al., 2005;Gong et al., 2016;Kervadec et al., 2019;Dai et al., 2017) is developed to improve the model's generalization performance by exploiting a large volume of unlabeled data.",
    "curr": "Pseudo labeling (Lee et al., 2013;Xie et al., 2020b;McLachlan, 1975;Rizve et al., 2020) and consistency regularization (Bachman et al., 2014;Samuli & Timo, 2017;Sajjadi et al., 2016) are two popular paradigms designed for modern SSL.",
    "next": "Recently, their combinations have shown promising results (Xie et al., 2020a;Sohn et al., 2020;Pham et al., 2021;Xu et al., 2021;Zhang et al., 2021)."
  },
  {
    "index": 1330,
    "source_corpus_id": 239049743,
    "ref_id": "b10",
    "citation_corpus_id": 1463401,
    "start": 3599,
    "end": 3618,
    "title": "HIERARCHICAL MULTISCALE RECURRENT NEURAL NETWORKS",
    "abstract": "Learning both hierarchical and temporal representation has been among the longstanding challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural network, that can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that the proposed model can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence generation. * Yoshua Bengio is CIFAR Senior Fellow.",
    "prev": "planning in model-based reinforcement learning) as the same future point in time can be reached with shorter and more accurate state rollouts (Pertsch et al., 2020;Zakharov et al., 2021).",
    "curr": "These benefits have recently prompted a number of proposed models that either perform transitions with temporal jumps of arbitrary length (Koutnik et al., 2014;Saxena et al., 2021), or aim to identify significant events (or key-frames) in sequential data and model the transitions between these events (Schmidhuber, 1991;Chung et al., 2017;Neitz et al., 2018;Jayaraman et al., 2018;Shang et al., 2019;Kipf et al., 2019;Kim et al., 2019;Pertsch et al., 2020;Zakharov et al., 2021).",
    "next": "The large variety of different event criteria defined in these studies demonstrates the lack of a widely established definition of this concept in this literature."
  },
  {
    "index": 1333,
    "source_corpus_id": 257532608,
    "ref_id": "b16",
    "citation_corpus_id": 247011290,
    "start": 15573,
    "end": 15593,
    "title": "Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution",
    "abstract": "When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer-the \"head\"). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR → STL, CIFAR10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2% higher accuracy ID but 7% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head-this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1% better ID, 10% better OOD than full fine-tuning). . Smart: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization.",
    "prev": "• Finetuning (FT): Trains a single model on all the tasks in a sequence, one task at a time.",
    "curr": "• Linear-Probing-Finetuning (LP-FT): LP-FT (Kumar et al., 2021) is the same as FT except that before each task training, we first learn a task-specific classifier Θ for the task via linear probing and then train both the feature extractor Φ and the classifier Θ on the task to reduce the feature drift.",
    "next": "• Multitask (MT): Trains the model on the data from both the current and previous tasks using the multitask training objective (equation 6 in Appendix)."
  },
  {
    "index": 1334,
    "source_corpus_id": 263835075,
    "ref_id": "b6",
    "citation_corpus_id": 221376730,
    "start": 84536,
    "end": 84555,
    "title": "Efficient Reinforcement Learning in Factored MDPs with Application to Constrained RL",
    "abstract": "Reinforcement learning (RL) in episodic factored Markov decision processes (FMDPs) is studied. We propose an algorithm called FMDP-BF, which leverages the factorization structure of the FMDP. The algorithm's regret is shown to be exponentially smaller than optimal algorithms in non-factored MDPs, and improves on the best previous result for FMDPs (Osband and Van Roy, 2014) by a factored of H|S i |, where |S i | is the cardinality of the factored state subspace and H is the planning horizon. To show the optimality of our bounds, we also provide a lower bound for FMDP, which indicates that our algorithm is near-optimal w.r.t timestep T , horizon H and factored state-action subspace cardinality. Finally, as an application, we study a new formulation of constrained RL, known as RL with knapsack constraints (RLwK), and provides the first sample-efficient algorithm based on FMDP-BF. arXiv:2008.13319v2 [cs.LG] 15 Sep 2020 1. For continuous budget and cost, we need to construct -net, in which case m equals to 1 .",
    "prev": "With the latter formalization of the FMDP induced by a causal graph, we now have all the components to introduce our learning problem in the next section.",
    "curr": "Reinforcement learning with partial causal graph priors\n\nIn the previous section, we show how the prior knowledge of a causal graph over the MDP variables can be exploited to obtain an FMDP representation of the problem, which is well-known to allow for more efficient reinforcement learning thanks to the factorization of the transition model and reward function [Osband and Van Roy, 2014, Xu and Tewari, 2020, Tian et al., 2020, Chen et al., 2020, Talebi et al., 2021, Rosenberg and Mansour, 2021.",
    "next": "However, in several applications is unreasonable to assume prior knowledge of the full causal graph, and causal identification is costly in general [Gillispie andPerlman, 2001, Shah andPeters, 2020]."
  },
  {
    "index": 1336,
    "source_corpus_id": 263831485,
    "ref_id": "b9",
    "citation_corpus_id": 52967399,
    "start": 2780,
    "end": 2801,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Average\n\nFigure 1: Zero-shot and fine-tuned MRR (higher is better) of ULTRA pre-trained on three graphs (FB15k-237, WN18RR, CoDEx-Medium).On average, zero-shot performance is better than best reported baselines trained on specific graphs (0.395 vs 0.344).More results in Figure 4 and Table 1.",
    "curr": "INTRODUCTION\n\nModern machine learning applications increasingly rely on the pre-training and fine-tuning paradigm.In this paradigm, a backbone model often trained on large datasets in a self-supervised fashion is commonly known as a foundation model (FM) (Bommasani et al., 2021).After pretraining, FMs can be fine-tuned on smaller downstream tasks.In order to transfer to a broad set of downstream tasks, FMs leverage certain invariances pertaining to a domain of interest, e.g., large language models like BERT (Devlin et al., 2019), GPT-4 (OpenAI, 2023), Llama-2 (Touvron et al., Preprint 2023) operate on a fixed vocabulary of tokens; vision models operate on raw pixels (He et al., 2016;Radford et al., 2021) or image patches (Dosovitskiy et al., 2021); chemistry models (Ying et al., 2021;Zheng et al., 2023) learn a vocabulary of atoms from the periodic table.",
    "next": "Representation learning on knowledge graphs (KGs), however, has not yet witnessed the benefits of transfer learning despite a wide range of downstream applications such as precision medicine (Chandak et al., 2023), materials science (Venugopal et al., 2022;Statt et al., 2023), virtual assistants (Ilyas et al., 2022), or product graphs in e-commerce (Dong, 2018).The key problem is that different KGs typically have different entity and relation vocabularies.Classic transductive KG embedding models (Ali et al., 2021) learn entity and relation embeddings tailored for each specific vocabulary and cannot generalize even to new nodes within the same graph.More rec"
  },
  {
    "index": 1338,
    "source_corpus_id": 221878944,
    "ref_id": "b34",
    "citation_corpus_id": 208547770,
    "start": 3802,
    "end": 3828,
    "title": "DEEP LEARNING FOR SYMBOLIC MATHEMATICS",
    "abstract": "Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica. * Equal contribution.",
    "prev": "multilayer perceptrons (MLPs), fail to extrapolate well when learning simple polynomial functions [Haley andSoloway, 1992, Barnard andWessels, 1992].",
    "curr": "However, recent works show Graph Neural Networks (GNNs) [Scarselli et al., 2009], a class of structured networks with MLP building blocks, can be successful in more challenging tasks, such as predicting the time evolution of physical systems [Battaglia et al., 2016], solving mathematical equations [Lample and Charton, 2020], and learning graph algorithms [Velickovic et al., 2020].",
    "next": "In these tasks, GNNs can generalize to graphs larger than those in the training set."
  },
  {
    "index": 1339,
    "source_corpus_id": 67915085,
    "ref_id": "b5",
    "citation_corpus_id": 3384895,
    "start": 4383,
    "end": 4405,
    "title": "Published as a conference paper at ICLR 2018 SMOOTH LOSS FUNCTIONS FOR DEEP TOP-K CLASSIFICATION",
    "abstract": "The top-k error is a common measure of performance in machine learning and computer vision. In practice, top-k classification is typically performed with deep neural networks trained with the cross-entropy loss. Theoretical results indeed suggest that cross-entropy is an optimal learning objective for such a task in the limit of infinite data. In the context of limited and noisy data however, the use of a loss function that is specifically designed for top-k classification can bring significant improvements. Our empirical evidence suggests that the loss function must be smooth and have non-sparse gradients in order to work well with deep neural networks. Consequently, we introduce a family of smoothed loss functions that are suited to top-k optimization via deep learning. The widely used cross-entropy is a special case of our family. Evaluating our smooth loss functions is computationally challenging: a naïve algorithm would require O( n k ) operations, where n is the number of classes. Thanks to a connection to polynomial algebra and a divideand-conquer approach, we provide an algorithm with a time complexity of O(kn). Furthermore, we present a novel approximation to obtain fast and stable algorithms on GPUs with single floating point precision. We compare the performance of the cross-entropy loss and our margin-based losses in various regimes of noise and data size, for the predominant use case of k = 5. Our investigation reveals that our loss is more robust to noise and overfitting than cross-entropy.",
    "prev": "In Advances in Neural Information Processing Systems, 2013.",
    "curr": "INTRODUCTION\n\nLearning to automatically sort objects is useful in many machine learning applications, such as topk multi-class classification (Berrada et al., 2018), ranking documents for information retrieval (Liu et al., 2009), and multi-object target tracking in computer vision (Bar-Shalom & Li, 1995).",
    "next": "Such algorithms typically require learning informative representations of complex, high-dimensional data, such as images, before sorting and subsequent downstream processing."
  },
  {
    "index": 1345,
    "source_corpus_id": 263831492,
    "ref_id": "b6",
    "citation_corpus_id": 52967399,
    "start": 37137,
    "end": 37158,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Large Language Models.Large Language Models belong to a class of artificial intelligence models used in natural language processing tasks.These models are designed to process and generate both human language and structured inputs, such as code in various programming languages.",
    "curr": "Examples of large language models include Generative Pre-trained Transformer models like GPT-3 (Brown et al., 2020) or GPT-4 (OpenAI, 2023), Bidirectional Encoder Representations from Transformers, BERT (Devlin et al., 2019), and others.LLMs are getting increasingly popular as an AIassistance for code generation tasks, like PaLM (Chowdhery et al., 2022), GitHub Copilot (Chen et al., 2021;GitHub, 2021), etc.",
    "next": "LLMs are usually trained in two steps.The main phase is training, where these models are exposed to very large corpora of data, usually collected over the internet.The architecture of LLMs is based on transformers and has a very large number of parameters.Therefore, it can capture relations between different parts of the input text to produce coherent outputs.For some applications, we need to perform fine-tuning to expose the model to application-specific inputs.During inference, when a user provides inputs and a prompt that contains instructions to an LLM, it generates the output with respect to these instructions."
  },
  {
    "index": 1347,
    "source_corpus_id": 248377629,
    "ref_id": "b5",
    "citation_corpus_id": 56475856,
    "start": 6494,
    "end": 6517,
    "title": "LEARNING TO ADAPT IN DYNAMIC, REAL-WORLD ENVIRONMENTS THROUGH META-REINFORCEMENT LEARNING",
    "abstract": "Although reinforcement learning methods can achieve impressive results in simulation, the real world presents two major challenges: generating samples is exceedingly expensive, and unexpected perturbations or unseen situations cause proficient but specialized policies to fail at test time. Given that it is impractical to train separate policies to accommodate all situations the agent may see in the real world, this work proposes to learn how to quickly and effectively adapt online to new tasks. To enable sample-efficient learning, we consider learning online adaptation in the context of model-based reinforcement learning. Our approach uses meta-learning to train a dynamics model prior such that, when combined with recent data, this prior can be rapidly adapted to the local context. Our experiments demonstrate online adaptation for continuous control tasks on both simulated and real-world agents. We first show simulated agents adapting their behavior online to novel terrains, crippled body parts, and highly-dynamic environments. We also illustrate the importance of incorporating online adaptation into autonomous agents that operate in the real world by applying our method to a real dynamic legged millirobot. We demonstrate the agent's learned ability to quickly adapt online to a missing leg, adjust to novel terrains and slopes, account for miscalibration or errors in pose estimation, and compensate for pulling payloads. 2 1 * Equal Contribution 2 Videos available at: https://sites.",
    "prev": "RELATED WORK\n\nMeta-Reinforcement Learning.",
    "curr": "Meta-RL approaches (Duan et al., 2016;Wang et al., 2017;Finn et al., 2017;Yu et al., 2018;Rothfuss et al., 2019;Vuorio et al., 2018;Nagabandi et al., 2019;Clavera et al., 2019;Rakelly et al., 2019;Vuorio et al., 2019;Yang et al., 2019;Zintgraf et al., 2019;Humplik et al., 2019;Zintgraf et al., 2020;Liu et al., 2021) hold the promise of allowing learning agents to quickly adapt to novel tasks by learning to learn from a distribution of tasks.",
    "next": "Despite the recent advances in the field, most existing meta-RL algorithms are limited to short-horizon and dense-reward tasks."
  },
  {
    "index": 1350,
    "source_corpus_id": 222133972,
    "ref_id": "b6",
    "citation_corpus_id": 203838325,
    "start": 8027,
    "end": 8048,
    "title": "Ridge Regression: Structure, Cross-Validation, and Sketching",
    "abstract": "We study the following three fundamental problems about ridge regression: (1) what is the structure of the estimator? (2) how to correctly use cross-validation to choose the regularization parameter? and (3) how to accelerate computation without losing too much accuracy? We consider the three problems in a unified large-data linear model. We give a precise representation of ridge regression as a covariance matrix-dependent linear combination of the true parameter and the noise. We study the bias of K-fold cross-validation for choosing the regularization parameter, and propose a simple bias-correction. We analyze the accuracy of primal and dual sketching for ridge regression, showing they are surprisingly accurate. Our results are illustrated by simulations and by analyzing empirical data.",
    "prev": "More generally, our proposed analysis sheds light on the effect of entry-wise nonlinear transformations on the eigenspectra of data/feature matrices.",
    "curr": "Thus, looking forward (and perhaps more importantly, given the use of nonlinear transformations in designing modern neural network models as well as the recent interest in applying RMT to neural network analyses (Li & Nguyen, 2018;Seddik et al., 2018;Jacot et al., 2019;Liu & Dobriban, 2019)), we expect that our analysis opens the door to improved analysis of computationally efficient methods for large dimensional machine learning and neural network models more generally.",
    "next": "System model and preliminaries\n\nBasic setup."
  },
  {
    "index": 1352,
    "source_corpus_id": 258865613,
    "ref_id": "b25",
    "citation_corpus_id": 5378837,
    "start": 2060,
    "end": 2086,
    "title": "Observed versus latent features for knowledge base and text inference",
    "abstract": "In this paper we show the surprising effectiveness of a simple observed features model in comparison to latent feature models on two benchmark knowledge base completion datasets, FB15K and WN18. We also compare latent and observed feature models on a more challenging dataset derived from FB15K, and additionally coupled with textual mentions from a web-scale corpus. We show that the observed features model is most effective at capturing the information present for entity pairs with textual relations, and a combination of the two combines the strengths of both model types.",
    "prev": "of queries investigated previously and precisely identifying the gap between its formulation and its goal, as well as providing complexity analysis for the currently investigated queries.Moreover, we develop a new dataset containing ten new types of queries with features that have never been considered and therefore can provide a thorough investigation of complex queries.Finally, we propose a new neural-symbolic method, Fuzzy Inference with Truth value (FIT), where we equip the neural link predictors with fuzzy logic theory to support end-to-end learning using complex queries with provable reasoning capability.Empirical results show that our method outperforms previous methods significantly in the new dataset and also surpasses previous methods in the existing dataset at the same time.",
    "curr": "Introduction\n\nKnowledge graph (KG) is a mighty knowledge base that encodes relational knowledge into a graph representation.However, due to the fact that modern knowledge graphs are often autogenerated [Toutanova and Chen, 2015] or constructed by crowd-sourcing [Vrandečić and Krötzsch, 2014], they are considered noisy and incomplete, which is also known as the Open World Assumption (OWA) [Libkin and Sirangelo, 2009].Complex query answering (CQA) on knowledge graphs is a practical task that can support many applications [Ren et al., 2023a, Wang et al., 2022].The CQA task requires answering the existential first order logic formula, which is one family of the logical formula involving logical operators, conjunction (∧), disjunction (∨), and negation (¬), as well as the existential quantifier ∃.In particular, as CQA is based on KGs that are with OWA, it should perform reasoning, which utilizes available knowledge to produce the prediction of the missing one where traditional traversal methods are doomed to fail [Ren et al., 2020].",
    "next": "To tackle this challenge, the query embedding method has been proposed [Hamilton et al., 2018], which aims to represent a set of entities by a low dimensional embedding.In addition to"
  },
  {
    "index": 1354,
    "source_corpus_id": 245334837,
    "ref_id": "b16",
    "citation_corpus_id": 53776053,
    "start": 2157,
    "end": 2160,
    "title": "LEARNING ACTIONABLE REPRESENTATIONS WITH GOAL-CONDITIONED POLICIES",
    "abstract": "Representation learning is a central challenge across a range of machine learning areas. In reinforcement learning, effective and functional representations have the potential to tremendously accelerate learning progress and solve more challenging problems. Most prior work on representation learning has focused on generative approaches, learning representations that capture all underlying factors of variation in the observation space in a more disentangled or well-ordered manner. In this paper, we instead aim to learn functionally salient representations: representations that are not necessarily complete in terms of capturing all factors of variation in the observation space, but rather aim to capture those factors of variation that are important for decision making -that are \"actionable.\" These representations are aware of the dynamics of the environment, and capture only the elements of the observation that are necessary for decision making rather than all factors of variation, without explicit reconstruction of the observation. We show how these representations can be useful to improve exploration for sparse reward problems, to enable long horizon hierarchical reinforcement learning, and as a state representation for learning policies for downstream tasks. We evaluate our method on a number of simulated environments, and compare it to prior methods for representation learning, exploration, and hierarchical reinforcement learning. *",
    "prev": "This typically uses a simple insight: suboptimal experience for one task may be optimal for another task.",
    "curr": "By conditioning on some piece of information, such as a goal, reward function parameterization, or reward value, such experience can be used for simple behavior cloning [5,6,8,10,17,23,26,29,32,34,38].",
    "next": "We refer to this set of approaches as RL VIA SU-PERVISED LEARNING (RVS)."
  },
  {
    "index": 1355,
    "source_corpus_id": 251647974,
    "ref_id": "b26",
    "citation_corpus_id": 3507990,
    "start": 27514,
    "end": 27532,
    "title": "Published as a conference paper at ICLR 2018 META-LEARNING FOR SEMI-SUPERVISED FEW-SHOT CLASSIFICATION",
    "abstract": "In few-shot classification, we are interested in learning algorithms that train a classifier from only a handful of labeled examples. Recent progress in few-shot classification has featured meta-learning, in which a parameterized model for a learning algorithm is defined and trained on episodes representing different classification problems, each with a small labeled training set and its corresponding test set. In this work, we advance this few-shot classification paradigm towards a scenario where unlabeled examples are also available within each episode. We consider two situations: one where all unlabeled examples are assumed to belong to the same set of classes as the labeled examples of the episode, as well as the more challenging situation where examples from other distractor classes are also provided. To address this paradigm, we propose novel extensions of Prototypical Networks (Snell et al., 2017) that are augmented with the ability to use unlabeled examples when producing prototypes. These models are trained in an end-to-end way on episodes, to learn to leverage the unlabeled examples successfully. We evaluate these methods on versions of the Omniglot and miniImageNet benchmarks, adapted to this new framework augmented with unlabeled examples. We also propose a new split of ImageNet, consisting of a large set of classes, with a hierarchical structure. Our experiments confirm that our Prototypical Networks can learn to improve their predictions due to unlabeled examples, much like a semi-supervised algorithm would. * Equal contribution. 1 See the following blog post for an overview:",
    "prev": "Experimental setting.",
    "curr": "We evaluate performance on two standard benchmarks: mini-ImageNet (Vinyals et al., 2016) and tiered-ImageNet (Ren et al., 2018).",
    "next": "We compare against MatchNet (Vinyals et al., 2016), Baseline++ (Chen et al., 2018), MetaOptNet (Lee et al., 2019), and Meta-Baseline (Chen et al., 2020b), who only assume the input is a vector."
  },
  {
    "index": 1356,
    "source_corpus_id": 263620587,
    "ref_id": "b39",
    "citation_corpus_id": 3488815,
    "start": 3143,
    "end": 3163,
    "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
    "abstract": "Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. * Authors ordered alphabetically.",
    "prev": "(2017); Gower et al.",
    "curr": "(2020).While finite-sum minimization problems are well-studied, recent applications in areas such as generative adversarial networks (Goodfellow et al., 2014), robust machine learning (Madry et al., 2018), and multi-agent reinforcement learning (Zhang et al., 2021) require solving more general equilibrium problems.Such tasks are neatly unified under the umbrella of monotone inclusion (MI), which has a rich history within optimization theory and operations research (Facchinei & Pang, 2003).",
    "next": "An important special case of (MI) is the variational inequality (VI) problem defined as below, where G is the subdifferential of the indicator function of a closed convex set C ⊆ R d : find u * ∈ C such that ⟨F (u * ), u − u * ⟩ ≥ 0, ∀u ∈ C.\n\n(VI)\n\nA more specialized template that results in interesting examples of VI and monotone inclusion problems is min-max optimization min x max y f (x) − g(y) + Φ(x, y), where Φ is convex-concave with Lipschitz gradients and f, g are proper convex lower semicontinuous (l.s.c."
  },
  {
    "index": 1358,
    "source_corpus_id": 263830786,
    "ref_id": "b23",
    "citation_corpus_id": 2134321,
    "start": 3152,
    "end": 3170,
    "title": "Published as a conference paper at ICLR 2016 DEEP COMPRESSION: COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING",
    "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce \"deep compression\", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35× to 49× without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9× to 13×; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35×, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49× from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3× to 4× layerwise speedup and 3× to 7× better energy efficiency.",
    "prev": "xtremely performant on a wide range of natural language tasks, but they require enormous amounts of compute to train (OpenAI, 2023;Anthropic, 2023).As such, there is growing interest in building strong moderate-sized models, such as LLaMA (Touvron et al., 2023a;b), MPT (MosaicML, 2023), and Falcon (Almazrouei et al., 2023), that allow for efficient inference and fine-tuning.These LLMs are available in varied sizes suited for different use cases, but training each individual model from scratch-even the smallest billion-parameter models-requires substantial computational resources that are cost-prohibitive for most organizations.In this work, we seek to address the following question:\n\nCan we produce a smaller, general-purpose, and competitive LLM by leveraging existing pre-trained LLMs, while using much less compute than training one from scratch?",
    "curr": "We explore structured pruning as a means to achieve this goal.Pruning is commonly viewed as a solution for compressing task-specific models (Han et al., 2016;Li et al., 2016;Lagunas et al., 2021;Xia et al., 2022;Kurtic et al., 2023), removing redundant parameters and accelerating inference without sacrificing task performance.However, for general-purpose LLMs, pruning inevitably results in performance degradation compared to original models (Frantar & Alistarh, 2023;Sun et al., 2023;Ma et al., 2023), especially when without significant compute invested post-pruning.In this Preprint.",
    "next": "work, we use pruning as an effective approach for developing smaller yet competitive LLMs that require only a fraction of the training compute compared to training them from scratch."
  },
  {
    "index": 1359,
    "source_corpus_id": 2331610,
    "ref_id": "b9",
    "citation_corpus_id": 5834589,
    "start": 2993,
    "end": 3013,
    "title": "ON LARGE-BATCH TRAINING FOR DEEP LEARNING: GENERALIZATION GAP AND SHARP MINIMA",
    "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say 32-512 data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions-and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap. Published as a conference paper at ICLR 2017 These methods minimize the objective function f by iteratively taking steps of the form: J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. . Weak sharp minima and penalty functions in mathematical programming. PhD thesis, University of Cambridge, 1988.Michael P Friedlander and Mark Schmidt. Hybrid deterministic-stochastic methods for data fitting.",
    "prev": "The overhead of communicating weight updates and the long tail of the machine and network latency distributions slow down execution and produce thorny engineering challenges.",
    "curr": "For the synchronous algorithm, there are rapidly diminishing returns from increasing the effective batch size (LeCun et al., 2012;Keskar et al., 2017).",
    "next": "For the asynchronous algorithm, gradient interference from inconsistent weights can cause updates to thrash and even, in some cases, result in worse final accuracy or completely stall learning progress."
  },
  {
    "index": 1360,
    "source_corpus_id": 58004637,
    "ref_id": "b10",
    "citation_corpus_id": 2863491,
    "start": 4116,
    "end": 4134,
    "title": "On Using Very Large Target Vocabulary for Neural Machine Translation",
    "abstract": "Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrasebased statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method based on importance sampling that allows us to use a very large target vocabulary without increasing training complexity. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to match, and in some cases outperform, the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use an ensemble of a few models with very large target vocabularies, we achieve performance comparable to the state of the art (measured by BLEU) on both the English→German and English→French translation tasks of WMT'14.",
    "prev": "Still, these algorithms suffer from high learning variance and poor sample complexity.",
    "curr": "INTRODUCTION\n\nEnd-to-end supervised learning with deep neural networks (DNNs) has taken the stage in the past few years, achieving state-of-the-art performance in multiple domains including computer vision (Szegedy et al., 2017), natural language processing (Sutskever et al., 2014;Jean et al., 2015), and speech recognition (Xiong et al., 2016).",
    "next": "Many of the tasks addressed by DNNs can be naturally decomposed to a series of functions."
  },
  {
    "index": 1362,
    "source_corpus_id": 246823327,
    "ref_id": "b7",
    "citation_corpus_id": 207852415,
    "start": 2669,
    "end": 2694,
    "title": "Published as a conference paper at ICLR 2020 ON THE RELATIONSHIP BETWEEN SELF-ATTENTION AND CONVOLUTIONAL LAYERS",
    "abstract": "Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies,Ramachandran et al. (2019)showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: do learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Specifically, we prove that a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer. Our numerical experiments then show that self-attention layers attend to pixel-grid patterns similarly to CNN layers, corroborating our analysis. Our code is publicly available 1 .Published as a conference paper at ICLR 2020Contributions. In this work, we put forth theoretical and empirical evidence that self-attention layers can (and do) learn to behave similar to convolutional layers: I. From a theoretical perspective, we provide a constructive proof showing that self-attention layers can express any convolutional layers. Specifically, we show that a single multi-head self-attention layer using relative positional encoding can be re-parametrized to express any convolutional layer.II. Our experiments show that the first few layers of attention-only architectures (Ramachandran et al., 2019) do learn to attend on grid-like pattern around each query pixel, similar to our theoretical construction.Strikingly, this behavior is confirmed both for our quadratic encoding, but also for relative encoding that is learned. Our results seem to suggest that localized convolution is the right inductive bias for the first few layers of an image classifying network. We provide an interactive website 2 to explore how self-attention exploits localized position-based attention in lower layers and contentbased attention in deeper layers. For reproducibility purposes, our code is publicly available.",
    "prev": "From the perspective of convolutional neural networks (CNNs), MSAs are a transformation of all feature map points with large-sized and data-specific kernels.",
    "curr": "Therefore, MSAs are at least as expressive as convolutional layers (Convs) (Cordonnier et al., 2020), although this does not guarantee that MSAs will behave like Convs.",
    "next": "Is the weak inductive bias of MSA, such as modeling long-range dependencies, beneficial for the predictive performance?"
  },
  {
    "index": 1363,
    "source_corpus_id": 248496446,
    "ref_id": "b10",
    "citation_corpus_id": 232417873,
    "start": 10241,
    "end": 10257,
    "title": "GROUNDING PHYSICAL CONCEPTS OF OBJECTS AND EVENTS THROUGH DYNAMIC VISUAL REASONING",
    "abstract": "We study the problem of dynamic visual reasoning on raw videos. This is a challenging problem; currently, state-of-the-art models often require dense supervision on physical object properties and events from simulation, which are impractical to obtain in real life. In this paper, we present the Dynamic Concept Learner (DCL), a unified framework that grounds physical objects and events from dynamic scenes and language. DCL first adopts a trajectory extractor to track each object over time and to represent it as a latent, object-centric feature vector. Building upon this object-centric representation, DCL learns to approximate the dynamic interaction among objects using graph networks. DCL further incorporates a semantic parser to parse question into semantic programs and, finally, a program executor to run the program to answer the question, levering the learned dynamics model. After training, DCL can detect and associate objects across the frames, ground visual properties and physical events, understand the causal relationship between events, make future and counterfactual predictions, and leverage these extracted presentations for answering queries. DCL achieves state-of-the-art performance on CLEVRER, a challenging causal video reasoning dataset, even without using ground-truth attributes and collision labels from simulations for training. We further test DCL on a newly proposed video-retrieval and event localization dataset derived from CLEVRER, showing its strong generalization capacity.",
    "prev": "Our work is also related to answering questions about visual content.",
    "curr": "Various benchmarks have been proposed to handle the tasks of cross-modal learning (Lei et al., 2018;Chen et al., 2019;Li et al., 2020;Wu et al., 2021;Hong et al., 2022).",
    "next": "However, they mainly focus on understanding human actions and activities rather than learning physical events and properties, which is essential for robot planning and control."
  },
  {
    "index": 1367,
    "source_corpus_id": 231639158,
    "ref_id": "b17",
    "citation_corpus_id": 204207649,
    "start": 23260,
    "end": 23280,
    "title": "META-LEARNING ACQUISITION FUNCTIONS FOR TRANSFER LEARNING IN BAYESIAN OPTIMIZATION",
    "abstract": "Transferring knowledge across tasks to improve data-efficiency is one of the open key challenges in the field of global black-box optimization. Readily available algorithms are typically designed to be universal optimizers and, therefore, often suboptimal for specific tasks. We propose a novel transfer learning method to obtain customized optimizers within the well-established framework of Bayesian optimization, allowing our algorithm to utilize the proven generalization capabilities of Gaussian processes. Using reinforcement learning to meta-train an acquisition function (AF) on a set of related tasks, the proposed method learns to extract implicit structural information and to exploit it for improved data-efficiency. We present experiments on a simulation-to-real transfer task as well as on several synthetic functions and on two hyperparameter search problems. The results show that our algorithm (1) automatically identifies structural properties of objective functions from available source tasks or simulations, (2) performs favourably in settings with both scarse and abundant source data, and (3) falls back to the performance level of general AFs if no particular structure is present.",
    "prev": "MetaBO Using the acquisition function instead of the surrogate model for transfer learning is another option.",
    "curr": "MetaBO is the state-of-the-art transfer acquisition function and was presented last year at ICLR (Volpp et al., 2020).",
    "next": "We use the implementation provided by the authors."
  },
  {
    "index": 1370,
    "source_corpus_id": 53841789,
    "ref_id": "b18",
    "citation_corpus_id": 3463260,
    "start": 1386,
    "end": 1407,
    "title": "Published as a conference paper at ICLR 2018 DISTRIBUTED PRIORITIZED EXPERIENCE REPLAY",
    "abstract": "We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible. The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network. The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors. Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time.",
    "prev": "We demonstrate the efficacy of our agent to learn, in an unsupervised manner, to reach a diverse set of goals on three domains -Atari, the DeepMind Control Suite and DeepMind Lab.",
    "curr": "INTRODUCTION\n\nCurrently, the best performing methods on many reinforcement learning benchmark problems combine model-free reinforcement learning methods with policies represented using deep neural networks (Horgan et al., 2018;Espeholt et al., 2018).",
    "next": "Despite reaching or surpassing human-level performance on many challenging tasks, deep model-free reinforcement learning methods that learn purely from the reward signal learn in a way that differs greatly from the manner in which humans learn."
  },
  {
    "index": 1373,
    "source_corpus_id": 259841489,
    "ref_id": "b15",
    "citation_corpus_id": 6628106,
    "start": 35108,
    "end": 35129,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "We conducted all experiments on a single NVIDIA GeForce RTX 3090 GPU, except for the image completion tasks presented in Section 5.2; we used 8 TPUv3 cores supported by TPU Research Cloud 2 for the 2D image completion task.",
    "curr": "For optimization, we used Adam (Kingma and Ba, 2015) optimizer with a cosine learning rate schedule.",
    "next": "Unless specified, we selected the base learning rate from a grid of {5 × 10 −4.50 , 5 × 10 −4.25 , 5 × 10 −4.00 , 5 × 10 −3.75 , 5 × 10 −3.50 } based on validation task log-likelihood."
  },
  {
    "index": 1374,
    "source_corpus_id": 259064088,
    "ref_id": "b49",
    "citation_corpus_id": 238744324,
    "start": 2794,
    "end": 2797,
    "title": "ON COVARIATE SHIFT OF LATENT CONFOUNDERS IN IMITATION AND REINFORCEMENT LEARNING",
    "abstract": "We consider the problem of using expert data with unobserved confounders for imitation and reinforcement learning. We begin by defining the problem of learning from confounded expert data in a contextual MDP setup. We analyze the limitations of learning from such data with and without external reward, and propose an adjustment of standard imitation learning algorithms to fit this setup. We then discuss the problem of distribution shift between the expert data and the online environment when the data is only partially observable. We prove possibility and impossibility results for imitation learning under arbitrary distribution shift of the missing covariates. When additional external reward is provided, we propose a sampling procedure that addresses the unknown shift and prove convergence to an optimal solution. Finally, we validate our claims empirically on challenging assistive healthcare and recommender system simulation tasks.",
    "prev": "The problem of hidden confounding, where outcome and decisions are both dependent on an unobserved factor, is widely overlooked in many of the concurrent offline RL methods.",
    "curr": "Nevertheless, it may induce significant errors, even for the simplest of bandit problems, and is especially aggravated in the sequential setting [7,66,79].",
    "next": "Hidden confounding exists in numerous applications."
  },
  {
    "index": 1375,
    "source_corpus_id": 3534906,
    "ref_id": "b19",
    "citation_corpus_id": 1671874,
    "start": 6656,
    "end": 6670,
    "title": "WORDNET: A LEXICAL DATABASE FOR ENGLISH",
    "abstract": "PROJECT GOALSWordNet is a lexical database for English organized in accordance with current psycholinguistic theories. Lexicalized concepts are organized by semantic relations (synonymy, antonymy, hyponymy, meronymy, etc.) for nouns, verbs, and adjectives.",
    "prev": "Note that C includes both the atomic and composite classes.",
    "curr": "In this paper's empirical section, we generate composite classes by imposing an existing lexical hierarchy on the class labels (Miller, 1995).",
    "next": "Partial labels For an example i, we use partial label to describe any elementỹ i ⊂ {1, ..., k} such that y i ⊃ y i ."
  },
  {
    "index": 1378,
    "source_corpus_id": 3484654,
    "ref_id": "b39",
    "citation_corpus_id": 15085443,
    "start": 23363,
    "end": 23387,
    "title": "Revisiting natural gradient for deep networks",
    "abstract": "We evaluate natural gradient, an algorithm originally proposed in Amari (1997), for learning deep models. The contributions of this paper are as follows. We show the connection between natural gradient and three other recently proposed methods: Hessian-Free (Martens, 2010), Krylov Subspace Descent (Vinyals and Povey, 2012) and TONGA(Le Roux et al., 2008). We empirically evaluate the robustness of natural gradient to the ordering of the training set compared to stochastic gradient descent and show how unlabeled data can be used to improve generalization error. Another contribution is to extend natural gradient to incorporate second error information alongside the manifold information. Lastly we benchmark this new algorithm as well as natural gradient, where both are implemented using a truncated Newton approach for inverting the metric matrix instead of using a diagonal approximation of it.",
    "prev": "A neural network with an appropriate choice of loss function is a probabilistic model and therefore defines a Fisher information matrix.",
    "curr": "Furthermore, the Fisher information matrix can be seen to define a convex quadratic approximation to the objective function of a probabilistic neural model (Pascanu & Bengio, 2014;Martens, 2014).",
    "next": "Importantly for our use case, the Fisher information matrix is positive definite by definition as well as non-diagonal."
  },
  {
    "index": 1379,
    "source_corpus_id": 235353039,
    "ref_id": "b40",
    "citation_corpus_id": 209501080,
    "start": 3525,
    "end": 3546,
    "title": "Published as a conference paper at ICLR 2020 LOCALITY AND COMPOSITIONALITY IN ZERO-SHOT LEARNING",
    "abstract": "In this work we study locality and compositionality in the context of learning representations for Zero Shot Learning (ZSL). In order to well-isolate the importance of these properties in learned representations, we impose the additional constraint that, differently from most recent work in ZSL, no pre-training on different datasets (e.g. ImageNet) is performed. The results of our experiments show how locality, in terms of small parts of the input, and compositionality, i.e. how well can the learned representations be expressed as a function of a smaller vocabulary, are both deeply related to generalization and motivate the focus on more local-aware models in future research directions for representation learning.",
    "prev": "Outside of RL, representation learning can help with ZSG, e.g.",
    "curr": "using unsupervised learning to obtain a representation that readily transfers to unseen classes in vision tasks (Bucher et al., 2017;Sylvain et al., 2020;.",
    "next": "In RL, unsupervised representation learning in the form of auxiliary objectives can be used to provide a richer learning signal over learning from reward alone, which helps the agent avoid overfitting on task-specific information (Raileanu and Fergus, 2021)."
  },
  {
    "index": 1383,
    "source_corpus_id": 239769246,
    "ref_id": "b29",
    "citation_corpus_id": 3708505,
    "start": 3779,
    "end": 3796,
    "title": "DEEP NEURAL NETWORKS AS GAUSSIAN PROCESSES",
    "abstract": "A deep fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP) in the limit of infinite network width. This correspondence enables exact Bayesian inference for neural networks on regression tasks by means of straightforward matrix computations. For single hiddenlayer networks, the covariance function of this GP has long been known. Recently, kernel functions for multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified the correspondence between using these kernels as the covariance function for a GP and performing fully Bayesian prediction with a deep neural network. In this work, we derive this correspondence and develop a computationally efficient pipeline to compute the covariance functions. We then use the resulting GP to perform Bayesian inference for deep neural networks on MNIST and CIFAR-10. We find that the GP-based predictions are competitive and can outperform neural networks trained with stochastic gradient descent. We observe that the trained neural network accuracy approaches that of the corresponding GP-based computation with increasing layer width, and that the GP uncertainty is strongly correlated with prediction error. We connect our observations to the recent development of signal propagation in random neural networks. * Both authors contributed equally to this work. † Work done as a member of the Google Brain Residency program (g.co/brainresidency). 1 Throughout this paper, we assume the conditions on the parameter distributions and nonlinearities are such that the Central Limit Theorem will hold; for instance, that the weight variance is scaled inversely proportional to the layer width.",
    "prev": "Neal showed that the outputs of a randomly initialized one-hidden layer neural network (with appropriate scaling of the variance of the initialization distribution) converges to a GP over functions in the limit of an infinite number of hidden units.",
    "curr": "Follow-up work extended this correspondence with analytical expressions for the kernel covariance for shallow NNs by Williams [1997], and more recently for deep fully-connected NNs [Lee et al., 2018, de G. Matthews et al., 2018, convolutional NNs with many channels , Garriga-Alonso et al., 2019, and more general architectures [Yang, 2019].",
    "next": "The correspondence enables exact Bayesian inference in the associated GP model for infinitewidth NNs on regression tasks and has led to some recent breakthroughs in our understanding of overparameterized NNs [Jacot et al., 2018, Arora et al., 2019, Belkin et al., 2018, Daniely et al., 2016, Yang and Salman, 2019, Bietti and Mairal, 2019."
  },
  {
    "index": 1384,
    "source_corpus_id": 52979229,
    "ref_id": "b31",
    "citation_corpus_id": 14124313,
    "start": 1621,
    "end": 1648,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "* These authors contributed equally.",
    "curr": "Introduction\n\nState-of-the-art vision and image-based tasks such as image classification (Krizhevsky et al., 2012;Simonyan & Zisserman, 2015;He et al., 2016), object detection (Ren et al., 2017; and segmentation (Long et al., 2015) are all built upon deep convolutional neural networks (CNNs).",
    "next": "While CNN architectures have evolved to become more efficient, the general trend has been to use larger models with greater memory and compute requirements to achieve higher accuracy."
  },
  {
    "index": 1386,
    "source_corpus_id": 228376209,
    "ref_id": "b15",
    "citation_corpus_id": 8768364,
    "start": 3094,
    "end": 3113,
    "title": "Density estimation using Real NVP",
    "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.",
    "prev": "Birkäuser, NY, 55(58-63):94, 2015.",
    "curr": "INTRODUCTION\n\nNormalizing flows (Dinh et al., 2014;Rezende & Mohamed, 2015) have recently gathered much interest within the machine learning community, ever since its recent breakthrough in modelling high dimensional image data (Dinh et al., 2017;Kingma & Dhariwal, 2018).",
    "next": "They are characterized by an invertible mapping that can reshape the distribution of its input data into a simpler or more complex one."
  },
  {
    "index": 1387,
    "source_corpus_id": 246430821,
    "ref_id": "b34",
    "citation_corpus_id": 5776935,
    "start": 16746,
    "end": 16765,
    "title": "PDE-NET: LEARNING PDES FROM DATA",
    "abstract": "Partial differential equations (PDEs) play a prominent role in many disciplines such as applied mathematics, physics, chemistry, material science, computer science, etc. PDEs are commonly derived based on physical laws or empirical observations. However, the governing equations for many complex systems in modern applications are still not fully known. With the rapid development of sensors, computational power, and data storage in the past decade, huge quantities of data can be easily collected and efficiently stored. Such vast quantity of data offers new opportunities for data-driven discovery of hidden physical laws. Inspired by the latest development of neural network designs in deep learning, we propose a new feed-forward deep network, called PDE-Net, to fulfill two objectives at the same time: to accurately predict dynamics of complex systems and to uncover the underlying hidden PDE models. The basic idea of the proposed PDE-Net is to learn differential operators by learning convolution kernels (filters), and apply neural networks or other machine learning methods to approximate the unknown nonlinear responses. Comparing with existing approaches, which either assume the form of the nonlinear response is known or fix certain finite difference approximations of differential operators, our approach has the most flexibility by learning both differential operators and the nonlinear responses. A special feature of the proposed PDE-Net is that all filters are properly constrained, which enables us to easily identify the governing PDE models while still maintaining the expressive and predictive power of the network. These constrains are carefully designed by fully exploiting the relation between the orders of differential operators and the orders of sum rules of filters (an important concept originated from wavelet theory). We also discuss relations of the PDE-Net with some existing networks in computer vision such as Network-In-Network (NIN) and Residual Neural Network (ResNet). Numerical experiments show that the PDE-Net has the potential to uncover the hidden PDE of the observed dynamics, and predict the dynamical behavior for a relatively long time, even in a noisy environment. * Equal contribution.",
    "prev": "Mathematically, the Π-block seeks to approximate the function F via polynomial combination of solution u and its spatial derivatives, given by\nF( U (k) ) = Nc c=1 f c · N l l=1 K (c,l) U (k+1)(2)\nwhere N c and N l denote the numbers of channels and parallel Conv layers respectively; denotes the Conv operation; (c, l) indicates the Conv filter K of l-th layer and c-th channel; f c is the weight in 1 × 1 Conv layer while the bias is omitted for simplicity.",
    "curr": "This multiplicative representation of Π-block promotes the network expressiveness for nonlinear function compared with the additive representation (i.e., weighted sum of multiple output) commonly seen, such as in PDE-Net (Long et al., 2018).",
    "next": "We demonstrate in Appendix Section A that Π-block is an universal polynomial approximator to the nonlinear function F. In addition, this network architecture features the capability to encode physics knowledge, such as the existing term in the PDE or the known initial and boundary conditions (I/BCs)."
  },
  {
    "index": 1389,
    "source_corpus_id": 231592776,
    "ref_id": "b49",
    "citation_corpus_id": 3280568,
    "start": 2093,
    "end": 2113,
    "title": "Workshop track -ICLR 2018 LEARNING INVARIANCES FOR POLICY GENERALIZATION",
    "abstract": "While recent progress has spawned very powerful machine learning systems, those agents remain extremely specialized and fail to transfer the knowledge they gain to similar yet unseen tasks. In this paper, we study a simple reinforcement learning problem and focus on learning policies that encode the proper invariances for generalization to different settings. We evaluate three potential methods for policy generalization: data augmentation, meta-learning and adversarial training. We find our data augmentation method to be effective, and study the potential of metalearning and adversarial learning as alternative task-agnostic approaches.",
    "prev": "We show the agent's trajectories using faded blocks.",
    "curr": "Current reinforcement learning (RL) approaches often learn policies that do not generalize to environments different than those the agent was trained on, even when these environments are semantically equivalent (Tachet des Combes et al., 2018;Song et al., 2019;Cobbe et al., 2019).",
    "next": "For example, consider a jumping task where an agent, learning from pixels, needs to jump over an obstacle ( Figure 1)."
  },
  {
    "index": 1390,
    "source_corpus_id": 3329316,
    "ref_id": "b3",
    "citation_corpus_id": 3199842,
    "start": 4306,
    "end": 4323,
    "title": "VARIATIONAL AUTO-ENCODED DEEP GAUSSIAN PRO- CESSES",
    "abstract": "We develop a scalable deep non-parametric generative model by augmenting deep Gaussian processes with a recognition model. Inference is performed in a novel scalable variational framework where the variational posterior distributions are reparametrized through a multilayer perceptron. The key aspect of this reformulation is that it prevents the proliferation of variational parameters which otherwise grow linearly in proportion to the sample size. We derive a new formulation of the variational lower bound that allows us to distribute most of the computation in a way that enables to handle datasets of the size of mainstream deep learning tasks. We show the efficacy of the method on a variety of challenges including deep unsupervised learning and deep Bayesian optimization.",
    "prev": "A potential solution is to construct a variational autoencoder (VAE) that learns the parameters of an inference network (recognition model or encoder) jointly with those of the generative model (decoder) (Kingma & Welling, 2014;Rezende et al., 2014).",
    "curr": "However, most existing VAEs rely on Gaussian latent variables, with the neural networks (NNs) acting as nonlinear transforms between adjacent layers (Sonderby et al., 2016;Dai et al., 2016;Ishaan et al., 2017).",
    "next": "A primary reason is that there is a simple reparameterization trick for Gaussian latent variables that allows efficiently computing the noisy gradients of the evidence lower bound (ELBO) with respect to the NN parameters."
  },
  {
    "index": 1392,
    "source_corpus_id": 258823075,
    "ref_id": "b69",
    "citation_corpus_id": 227209335,
    "start": 2675,
    "end": 2678,
    "title": "SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS",
    "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reversetime SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in diffusion probabilistic modeling and scorebased generative modeling, and allows for new sampling procedures. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, which enables exact likelihood computation, and improved sampling efficiency. In addition, our framework enables conditional generation with an unconditional model, as we demonstrate with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high fidelity generation of 1024ˆ1024 images for the first time from a score-based generative model.",
    "prev": "e.For the implementations we approximate the gradient of the sliced MMD by using only a finite number P of slices.We show that the resulting error has complexity O( d/P ), where d is the data dimension.These results enable us to train generative models by approximating MMD gradient flows by neural networks even for image applications.We demonstrate the efficiency of our model by image generation on MNIST, FashionMNIST and CIFAR10.",
    "curr": "Introduction\n\nWith the rise of generative models, the field of gradient flows in measure spaces received increasing attention.Based on classical Markov chain Monte Carlo methods, Welling and Teh [80] proposed to apply the Langevin dynamics for inferring samples from a known probability density function.This corresponds to simulating a Wasserstein gradient flow with respect to the Kullback-Leibler divergence, see [38].Closely related to this approach are current state-of-the-art image generation methods like score-based models [68,69] or diffusion models [36,70], which significantly outperform classical generative models like GANs [27] or VAEs [41].A general aim of such algorithms [5,36,81] is to establish a path between input and target distribution, where \"unseen\" data points are established via the randomness of the input distribution.",
    "next": "For approximating gradient flows with respect to other functionals than the KL divergence, the authors of [1,2,15,23,55] proposed the use of suitable forward and backward discretizations.To   reduce the computational effort of evaluating distance measures on high-dimensional probability distributions, the sliced Wasserstein metric was introduced in [65].The main idea of the sliced Wasserstein distance is to compare one-dimensional projections of the corresponding probability distributions instead of the distributions themselves.This approach can be generalized to more general probability metrics [44] and was applied in the context of Wasserstein gradient flows in [13,52]."
  },
  {
    "index": 1393,
    "source_corpus_id": 256390009,
    "ref_id": "b3",
    "citation_corpus_id": 52889459,
    "start": 25351,
    "end": 25371,
    "title": "LARGE SCALE GAN TRAINING FOR HIGH FIDELITY NATURAL IMAGE SYNTHESIS",
    "abstract": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by truncating the latent space. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128×128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.3 and Fréchet Inception Distance (FID) of 9.6, improving over the previous best IS of 52.52 and FID of 18.65.",
    "prev": "Since there has been lack of studies that explored the minority-focused generation in the unconditional setting as ours (see related work in Section 1 for details), we consider three generic frameworks that are widely adopted in literature.",
    "curr": "The first two baselines are GAN-based frameworks, BigGAN (Brock et al., 2019) and StyleGAN .",
    "next": "The third baseline, which is our main interest for comparison, is a diffusion-based generative model, DDPM (Ho et al., 2020) with the standard sampler (3)."
  },
  {
    "index": 1394,
    "source_corpus_id": 235436185,
    "ref_id": "b27",
    "citation_corpus_id": 14307651,
    "start": 8208,
    "end": 8214,
    "title": "THE CONCRETE DISTRIBUTION: A CONTINUOUS RELAXATION OF DISCRETE RANDOM VARIABLES",
    "abstract": "The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce CONCRETE random variables-CONtinuous relaxations of disCRETE random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.",
    "prev": "Because the latent visual tokens are discrete, the model training is non-differentiable.",
    "curr": "Gumbel-softmax relaxation [JGP17,MMT17] is employed to train the model parameters.",
    "next": "Moreover, a uniform prior is put on q φ during dVAE training."
  },
  {
    "index": 1396,
    "source_corpus_id": 257636556,
    "ref_id": "b16",
    "citation_corpus_id": 231979542,
    "start": 7024,
    "end": 7027,
    "title": "Published as a conference paper at ICLR 2021 EFFECTIVE AND EFFICIENT VOTE ATTACK ON CAP- SULE NETWORKS",
    "abstract": "Standard Convolutional Neural Networks (CNNs) can be easily fooled by images with small quasi-imperceptible artificial perturbations. As alternatives to CNNs, the recently proposed Capsule Networks (CapsNets) are shown to be more robust to white-box attack than CNNs under popular attack protocols. Besides, the class-conditional reconstruction part of CapsNets is also used to detect adversarial examples. In this work, we investigate the adversarial robustness of CapsNets, especially how the inner workings of CapsNets change when the output capsules are attacked. The first observation is that adversarial examples misled CapsNets by manipulating the votes from primary capsules. Another observation is the high computational cost, when we directly apply multi-step attack methods designed for CNNs to attack CapsNets, due to the computationally expensive routing mechanism. Motivated by these two observations, we propose a novel vote attack where we attack votes of CapsNets directly. Our vote attack is not only effective but also efficient by circumventing the routing process. Furthermore, we integrate our vote attack into the detection-aware attack paradigm, which can successfully bypass the class-conditional reconstruction based detection method. Extensive experiments demonstrate the superior attack performance of our vote attack on CapsNets.",
    "prev": "The previous works of attack on semantic segmentation models have been focused on the adversarial attack [64,14,25,2,19].",
    "curr": "The works [51,17,61] have demonstrated that various deep neural networks (DNNs) can be misled by adversarial examples with small imperceptible perturbations.",
    "next": "The works [14,64] extended adversarial examples to semantic segmentation."
  },
  {
    "index": 1397,
    "source_corpus_id": 226227077,
    "ref_id": "b22",
    "citation_corpus_id": 209531600,
    "start": 6348,
    "end": 6368,
    "title": "PAC CONFIDENCE SETS FOR DEEP NEURAL NET- WORKS VIA CALIBRATED PREDICTION",
    "abstract": "We propose an algorithm combining calibrated prediction and generalization bounds from learning theory to construct confidence sets for deep neural networks with PAC guarantees-i.e., the confidence set for a given input contains the true label with high probability. We demonstrate how our approach can be used to construct PAC confidence sets on ResNet for ImageNet, a visual object tracking model, and a dynamics model for the half-cheetah reinforcement learning problem. 1 1 Our code is available at https://github.com/sangdon/PAC-confidence-set. . Safe model-based reinforcement learning with stability guarantees. In Advances in neural information processing systems, pp. 908-918, 2017.Marko Bohanec and Vladislav Rajkovic. Knowledge acquisition and explanation for multi-attribute decision making.",
    "prev": "However, traditional approaches do not provide theoretical guarantees on the predicted confidences.",
    "curr": "There has been work on predicting confidence sets (i.e., predict a set of labels instead of a single label) with theoretical guarantees (Park et al., 2020a), but this approach does not provide the confidence of the most likely prediction, as is often desired.",
    "next": "There has also been work providing guarantees on the overall calibration error (Kumar et al., 2019), but this approach does not provide per-prediction guarantees."
  },
  {
    "index": 1400,
    "source_corpus_id": 252683312,
    "ref_id": "b5",
    "citation_corpus_id": 207808916,
    "start": 6795,
    "end": 6818,
    "title": "DEEP DOUBLE DESCENT: WHERE BIGGER MODELS AND MORE DATA HURT",
    "abstract": "We show that a variety of modern deep learning tasks exhibit a \"double-descent\" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.",
    "prev": "It is well known in statistics that generalization error has a \"U\" shape against model capacity, which is usually attributed to the bias-variance trade-off.",
    "curr": "Although this common wisdom was challenged by the observation of double descent (Nakkiran et al., 2021), the \"U\" curve can be recovered from a double descent simply by changing the x-axis from the number of model parameters N to the 2-norm of model parameters w ≡ ||w|| 2 (Ng and Ma, 2022).",
    "next": "Although the LU mechanism may remind readers of related phenomena (Schoenholz et al., 2016;Yang and Schoenholz, 2017;Nakkiran et al., 2021), their setups are not exactly the same as ours."
  },
  {
    "index": 1402,
    "source_corpus_id": 243756979,
    "ref_id": "b42",
    "citation_corpus_id": 212859361,
    "start": 2753,
    "end": 2772,
    "title": "DROPEDGE: TOWARDS DEEP GRAPH CONVOLU- TIONAL NETWORKS ON NODE CLASSIFICATION",
    "abstract": "Over-fitting and over-smoothing are two main obstacles of developing deep Graph Convolutional Networks (GCNs) for node classification. In particular, over-fitting weakens the generalization ability on small dataset, while over-smoothing impedes model training by isolating output representations from the input features with the increase in network depth. This paper proposes DropEdge, a novel and flexible technique to alleviate both issues. At its core, DropEdge randomly removes a certain number of edges from the input graph at each training epoch, acting like a data augmenter and also a message passing reducer. Furthermore, we theoretically demonstrate that DropEdge either reduces the convergence speed of over-smoothing or relieves the information loss caused by it. More importantly, our DropEdge is a general skill that can be equipped with many other backbone models (e.g. GCN, ResGCN, GraphSAGE, and JKNet) for enhanced performance. Extensive experiments on several benchmarks verify that DropEdge consistently improves the performance on a variety of both shallow and deep GCNs. The effect of DropEdge on preventing over-smoothing is empirically visualized and validated as well. Codes are released on https://github.com/DropEdge/DropEdge.",
    "prev": "INTRODUCTION\n\nGraphs provide a powerful abstraction for complex datasets that arise in a variety of applications such as social networks, transportation networks, and biological sciences (Hamilton et al., 2017;Derrow-Pinion et al., 2021;Zitnik & Leskovec, 2017;Chanussot et al., 2021).",
    "curr": "Despite recent advances in graph neural networks (GNNs), when trained with supervised data alone, these networks can easily overfit and may fail to generalize (Rong et al., 2019).",
    "next": "Thus, finding ways to form simplified representations of graph-structured data without labels is an important yet unsolved challenge."
  },
  {
    "index": 1403,
    "source_corpus_id": 3508638,
    "ref_id": "b0",
    "citation_corpus_id": 18828233,
    "start": 3103,
    "end": 3106,
    "title": "TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.",
    "prev": "For instance, GANs aim to achieve an equilibrium between a generator and a discriminator; while VAEs are devoted to maximizing a lower bound of the data log-likelihood.",
    "curr": "A rich array of theoretical analyses and model extensions have been developed independently for GANs [1,2,50,41] and VAEs [5,10,25,17], respectively.",
    "next": "A few works attempt to combine the two objectives in a single model for improved inference and sample generation [32,35,52,54]."
  },
  {
    "index": 1406,
    "source_corpus_id": 252846202,
    "ref_id": "b7",
    "citation_corpus_id": 212945787,
    "start": 2811,
    "end": 2830,
    "title": "RESIDUAL ENERGY-BASED MODELS FOR TEXT GENERATION",
    "abstract": "Text generation is ubiquitous in many NLP tasks, from summarization, to dialogue and machine translation. The dominant parametric approach is based on locally normalized models which predict one word at a time. While these work remarkably well, they are plagued by exposure bias due to the greedy nature of the generation process. In this work, we investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, we first work in the residual of a pretrained locally normalized language model and second we train using noise contrastive estimation. Furthermore, since the EBM works at the sequence level, we can leverage pretrained bi-directional contextual representations, such as BERT and RoBERTa. Our experiments on two large language modeling datasets show that residual EBMs yield lower perplexity compared to locally normalized baselines. Moreover, generation via importance sampling is very efficient and of higher quality than the baseline models according to human evaluation.Published as a conference paper at ICLR 2020 perturbations of the ground truth would be efficient but hardly useful for generation purposes, when at test time the model needs to generate from scratch.",
    "prev": "In practice, given different data types, we can parameterize the energy function with different neural networks as needed, such as multi-layer perceptrons (MLPs), convolutional neural networks (CNNs) (LeCun et al., 1998), and graph neural networks (GNNs) (Gori et al., 2005;Scarselli et al., 2008).",
    "curr": "Recently, EBMs have been drawing increasing attention and are demonstrated to be effective in various domains, including images (Ngiam et al., 2011;Xie et al., 2016;Du & Mordatch, 2019), videos (Xie et al., 2017), texts (Deng et al., 2020), 3D objects (Xie et al., 2018), molecules (Liu et al., 2021;Hataya et al., 2021), and proteins (Du et al., 2020b).",
    "next": "Nonetheless, learning (a.k.a., training) EBMs is known to be challenging since we cannot compute the exact likelihood due to the intractable normalization constant."
  },
  {
    "index": 1408,
    "source_corpus_id": 4564356,
    "ref_id": "b0",
    "citation_corpus_id": 11212020,
    "start": 5756,
    "end": 5779,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "RNN is broadly used given its capability in capturing long-range dependency through recurrent computation.",
    "curr": "It has been applied to various NLP tasks, e.g., question answering (Wang et al., 2017), neural machine translation (Bahdanau et al., 2015), sentiment analysis , natural language inference (Liu et al., 2016), etc.",
    "next": "However, training the basic RNN encounters the gradient dispersion problem, and is difficult to parallelize."
  },
  {
    "index": 1411,
    "source_corpus_id": 244478674,
    "ref_id": "b66",
    "citation_corpus_id": 59523594,
    "start": 2091,
    "end": 2110,
    "title": "Multi-Task Deep Neural Networks for Natural Language Understanding",
    "abstract": "In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains. MT-DNN extends the model proposed inLiu et al. (2015)by incorporating a pre-trained bidirectional transformer language model, known as BERT(Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.2% (1.8% absolute improvement). We also demonstrate using the SNLI and Sc-iTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. Our code and pre-trained models will be made publicly available.",
    "prev": "While self-supervised pre-training has been shown to be highly effective at exploiting large amounts of unlabeled data without relying on human annotation, there is still much to explore regarding transfer learning in a multi-task co-training setup.",
    "curr": "Prior seminal works like T5  and MT-DNN (Liu et al., 2019a) have demonstrated a degree of promise in the paradigm of multi-task co-training (Caruana, 1997).",
    "next": "However, the challenge of catastrophic forgetting remains."
  },
  {
    "index": 1413,
    "source_corpus_id": 221112385,
    "ref_id": "b15",
    "citation_corpus_id": 56657912,
    "start": 4989,
    "end": 4992,
    "title": "BENCHMARKING NEURAL NETWORK ROBUSTNESS TO COMMON CORRUPTIONS AND PERTURBATIONS",
    "abstract": "In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, IMAGENET-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called IMAGENET-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.",
    "prev": "In this paper, we experiment with three types of augmentations: rotation, color jittering, and texture randomization, as visualized in Figure 1.",
    "curr": "We evaluate our approach across a variety of diverse tasks including large-scale classification [5], fine-grained classification [34,33], few-shot classification [23], and classification on corrupted data [2,16].",
    "next": "Our representation shows consistent performance gains with increasing number of augmentations."
  },
  {
    "index": 1414,
    "source_corpus_id": 67855984,
    "ref_id": "b27",
    "citation_corpus_id": 3162051,
    "start": 12911,
    "end": 12931,
    "title": "mixup: BEYOND EMPIRICAL RISK MINIMIZATION",
    "abstract": "Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks. * Alphabetical order.",
    "prev": "In addition, no dropout (Srivastava et al., 2014) is applied to any baseline according to the best practices in (Ioffe & Szegedy, 2015).",
    "curr": "For Tiny ImageNet and ImageNet-2012, the baseline models are slightly different: we follow the settings from (Zhang et al., 2018), and the details are described in the corresponding paragraphs.",
    "next": "CIFAR-10 and CIFAR-100."
  },
  {
    "index": 1415,
    "source_corpus_id": 10635893,
    "ref_id": "b32",
    "citation_corpus_id": 8888540,
    "start": 4796,
    "end": 4815,
    "title": "Feature Noising for Log-linear Structured Prediction",
    "abstract": "NLP models have many and sparse features, and regularization is key for balancing model overfitting versus underfitting. A recently repopularized form of regularization is to generate fake training data by repeatedly adding noise to real data. We reinterpret this noising as an explicit regularizer, and approximate it with a second-order formula that can be used during training without actually generating fake data. We show how to apply this method to structured prediction using multinomial logistic regression and linear-chain CRFs. We tackle the key challenge of developing a dynamic program to compute the gradient of the regularizer efficiently. The regularizer is a sum over inputs, so we can estimate it more accurately via a semi-supervised or transductive extension. Applied to text classification and NER, our method provides a >1% absolute performance gain over use of standard L 2 regularization.",
    "prev": "These methods, however, all consider weights and hidden units instead of the input data, and are motivated by the vanishing and exploding gradient problem.",
    "curr": "Feature noising has been demonstrated to be effective for structured prediction tasks, and has been interpreted as an explicit regularizer (Wang et al., 2013).",
    "next": "Additionally, Wager et al."
  },
  {
    "index": 1416,
    "source_corpus_id": 250144560,
    "ref_id": "b11",
    "citation_corpus_id": 52967399,
    "start": 3204,
    "end": 3225,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Transfer learning plays a central role in industrial computer vision and natural language processing pipelines, where models learn generic features that are useful across many tasks.",
    "curr": "For example, feature extractors pre-trained on the ImageNet dataset can enhance object detectors (Ren et al., 2015), and large transformer models trained on vast text corpora develop conceptual understandings which can be readily fine-tuned for question answering or language inference (Devlin et al., 2019).",
    "next": "One might wonder if deep neural networks for tabular data, which are typically shallow and whose hierarchical feature extraction is unexplored, can also build representations that are transferable beyond their pre-training tasks."
  },
  {
    "index": 1418,
    "source_corpus_id": 252596302,
    "ref_id": "b2",
    "citation_corpus_id": 11212020,
    "start": 4687,
    "end": 4710,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "Most graph models (Dauparas et al., 2022;Ingraham et al., 2019;Jing et al., 2020;Hsu et al., 2022;Hsu et al., 2022) adopt the autoregressive decoding scheme to generate amino acids, dramatically slowing down the inference process.",
    "curr": "Interestingly, few studies have attempted to improve the model efficiency, perhaps because the efficiency gain requires sacrificing some accuracy (Bahdanau et al., 2015;Vaswani et al., 2017;Ghazvininejad et al., 2019;Geng et al., 2021;Wang et al., 2019;Gu et al., 2018), while the latter is more important than efficiency in protein design.",
    "next": "To address this dilemma, AlphaDesign  proposes a parallel self-correcting module to speed up inference while almost maintaining the recovery."
  },
  {
    "index": 1419,
    "source_corpus_id": 195218755,
    "ref_id": "b23",
    "citation_corpus_id": 3693334,
    "start": 4685,
    "end": 4688,
    "title": "Visualizing the Loss Landscape of Neural Nets",
    "abstract": "Neural network training relies on our ability to find \"good\" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and wellchosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple \"filter normalization\" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.",
    "prev": "On the one hand, we are inspired by the success of AlphaGo [35,36], where a policy network is used to generate proposals for the Monte-Carlo tree search.",
    "curr": "On the other hand, we are inspired by the recent research into understanding deep neural networks [30,24,37].",
    "next": "Deep neural networks, frequently observed in practices, is much less likely to get stuck in sub-optimal points."
  },
  {
    "index": 1420,
    "source_corpus_id": 244488409,
    "ref_id": "b11",
    "citation_corpus_id": 219965949,
    "start": 29195,
    "end": 29212,
    "title": "IDF++: Analyzing and Improving Integer Discrete Flows for Lossless Compression",
    "abstract": "In this paper we analyse and improve integer discrete flows for lossless compression. Integer discrete flows are a recently proposed class of models that learn invertible transformations for integer-valued random variables. Due to its discrete nature, they can be combined in a straightforward manner with entropy coding schemes for lossless compression without the need for bits-back coding. We discuss the potential difference in flexibility between invertible flows for discrete random variables and flows for continuous random variables and show that (integer) discrete flows are more flexible than previously claimed. We furthermore investigate the influence of quantization operators on optimization and gradient bias in integer discrete flows. Finally, we introduce modifications to the architecture to improve the performance of this model class for lossless compression.",
    "prev": "Results are shown in Table 4.",
    "curr": "First, compared to 4 baselines (i.e., IDF, IDF++ (van den Berg et al., 2020), Glow (Kingma & Dhariwal, 2018), and RealNVP (Dinh et al., 2016), PC+IDF achieved the best bpd on ImageNet32 and ImageNet64.",
    "next": "Next, PC+IDF improved over its base model IDF by 0.04, 0.16, and 0.19 bpd on three datasets, respectively."
  },
  {
    "index": 1421,
    "source_corpus_id": 252846354,
    "ref_id": "b41",
    "citation_corpus_id": 3566136,
    "start": 3673,
    "end": 3701,
    "title": "Published as a conference paper at ICLR 2018 RELATIONAL NEURAL EXPECTATION MAXIMIZATION: UNSUPERVISED DISCOVERY OF OBJECTS AND THEIR INTERACTIONS",
    "abstract": "Common-sense physical reasoning is an essential ingredient for any intelligent agent operating in the real-world. For example, it can be used to simulate the environment, or to infer the state of parts of the world that are currently unobserved. In order to match real-world conditions this causal knowledge must be learned without access to supervised data. To address this problem we present a novel method that learns to discover objects and model their physical interactions from raw visual images in a purely unsupervised fashion. It incorporates prior knowledge about the compositional nature of human perception to factor interactions between object-pairs and learn efficiently. On videos of bouncing balls we show the superior modelling capabilities of our method compared to other unsupervised neural approaches that do not incorporate such prior knowledge. We demonstrate its ability to handle occlusion and show that it can extrapolate learned knowledge to scenes with different numbers of objects. * Work performed while at IDSIA.",
    "prev": "This frequently results in poorly learned object dynamics, producing unrealistic future predictions over longer horizons (Oprea et al., 2020).",
    "curr": "Another perspective to dynamics learning is through object-centric dynamics models (Kosiorek et al., 2018;van Steenkiste et al., 2018;Kossen et al., 2019).",
    "next": "This class of methods first represents a scene as a set of object-centric features (a.k.a."
  },
  {
    "index": 1432,
    "source_corpus_id": 247595263,
    "ref_id": "b17",
    "citation_corpus_id": 560565,
    "start": 12284,
    "end": 12302,
    "title": "Solving General Arithmetic Word Problems",
    "abstract": "This paper presents a novel approach to automatically solving arithmetic word problems. This is the first algorithmic approach that can handle arithmetic problems with multiple steps and operations, without depending on additional annotations or predefined templates. We develop a theory for expression trees that can be used to represent and evaluate the target arithmetic expressions; we use it to uniquely decompose the target arithmetic problem to multiple classification problems; we then compose an expression tree, combining these with world knowledge through a constrained inference framework. Our classifiers gain from the use of quantity schemas that supports better extraction of features. Experimental results show that our method outperforms existing systems, achieving state of the art performance on benchmark datasets of arithmetic word problems.",
    "prev": "3\n\n• Arithmetic reasoning.",
    "curr": "For these tasks, we used the Math Word Problem Repository (Koncel-Kedziorski et al., 2016), including AddSub (Hosseini et al., 2014, MultiArith (Roy & Roth, 2015), and ASDiv (Miao et al., 2020).",
    "next": "We also included AQUA- RAT (Ling et al., 2017), a recently published benchmark of grade-school-math problems (GSM8K; Cobbe et al., 2021), and a challenge dataset over math word problems (SVAMP; Patel et al., 2021)."
  },
  {
    "index": 1435,
    "source_corpus_id": 264406064,
    "ref_id": "b46",
    "citation_corpus_id": 990233,
    "start": 2454,
    "end": 2475,
    "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    "abstract": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.",
    "prev": "character, word, and sentence levels, respectively.Besides, we use a fidelity filter to ensure that PromptAttack maintains the original semantic meanings of the adversarial examples.Further, we enhance the attack power of PromptAttack by ensembling adversarial examples at different perturbation levels.Comprehensive empirical results using Llama2 and GPT-3.5 validate that PromptAttack consistently yields a much higher attack success rate compared to AdvGLUE and AdvGLUE++.Interesting findings include that a simple emoji can easily mislead GPT-3.5 to make wrong predictions.Our project page is available at PromptAttack.",
    "curr": "Introduction\n\nLarge language models (LLMs) that are pre-trained on massive text corpora can be foundation models (Bommasani et al., 2021) to power various downstream applications.In particular, LLMs (Garg et al., 2022;Liu et al., 2023a;Wei et al., 2022) can yield superior performance in various natural language processing (NLP) downstream tasks, such as sentiment analysis (Socher et al., 2013) and logical reasoning (Miao et al., 2023;Liu et al., 2023a).However, in some critical areas such as medicine (Singhal et al., 2023) and industrial control (Song et al., 2023), LLM's reliability is of equal importance.This paper studies one key aspect of LLM's reliability-adversarial robustness.",
    "next": "Existing research evaluates adversarial robustness of LLMs on the GLUE dataset (Wang et al., 2018), in which an LLM is required to solve a classification task according to a prompt containing both a task description and an original sample (as shown in Figure 2).In particular, Zhu et al."
  },
  {
    "index": 1438,
    "source_corpus_id": 67856605,
    "ref_id": "b8",
    "citation_corpus_id": 3568073,
    "start": 3239,
    "end": 3260,
    "title": "PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION",
    "abstract": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at 1024 2 . We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CELEBA dataset.",
    "prev": "The generator is trained adversarially by optimizing a minimax objective together with a discriminator.",
    "curr": "In practice, GANs have been shown to be very successful in a range of applications including generating photorealistic images (Karras et al., 2018).",
    "next": "Other than generating samples, many downstream tasks require a good generative model, such as image inpainting (Pathak et al., 2016;Yeh et al., 2017)."
  },
  {
    "index": 1439,
    "source_corpus_id": 253265114,
    "ref_id": "b4",
    "citation_corpus_id": 243865348,
    "start": 3134,
    "end": 3155,
    "title": "Shortcutted Commonsense: Data Spuriousness in Deep Learning of Commonsense Reasoning",
    "abstract": "Question: What is something someone driving a car needs even to begin?",
    "prev": "Still, given LMs' black-box nature, it is unclear whether this knowledge is being used properly (Doshi-Velez & Kim, 2017;Lipton, 2018).",
    "curr": "Previous studies have shown that LMs often learn spurious correlations from artifacts in downstream training data, thus limiting their generalizability (Branco et al., 2021;Geirhos et al., 2020;D'Amour et al., 2020).",
    "next": "With this in mind, a number of prior works aim to make LMs' reasoning processes more explicit by generating free-text rationales, which use LMs' internal knowledge to describe a reasoning process in natural language (Narang et al., 2020;Wei et al., 2022b;Marasović et al., 2022;Zelikman et al., 2022)."
  },
  {
    "index": 1440,
    "source_corpus_id": 248377334,
    "ref_id": "b7",
    "citation_corpus_id": 233394003,
    "start": 3076,
    "end": 3095,
    "title": "Published as a conference paper at ICLR 2021 DC3: A LEARNING METHOD FOR OPTIMIZATION WITH HARD CONSTRAINTS",
    "abstract": "Large optimization problems with hard constraints arise in many settings, yet classical solvers are often prohibitively slow, motivating the use of deep networks as cheap \"approximate solvers.\" Unfortunately, naive deep learning approaches typically cannot enforce the hard constraints of such problems, leading to infeasible solutions. In this work, we present Deep Constraint Completion and Correction (DC3), an algorithm to address this challenge. Specifically, this method enforces feasibility via a differentiable procedure, which implicitly completes partial solutions to satisfy equality constraints and unrolls gradient-based corrections to satisfy inequality constraints. We demonstrate the effectiveness of DC3 in both synthetic optimization tasks and the real-world setting of AC optimal power flow, where hard constraints encode the physics of the electrical grid. In both cases, DC3 achieves near-optimal objective values while preserving feasibility. * These authors contributed equally.Published as a conference paper at ICLR 2021 • AC optimal power flow. We show how the general DC3 framework can be used to optimize power flows on the electrical grid. This difficult non-convex optimization task must be solved at scale and is especially critical for renewable energy adoption. Our results greatly improve upon the performance of general-purpose deep learning methods on this task.RELATED WORKOur approach is situated within the broader literature on fast optimization methods, and draws inspiration from literature on implicit layers and on incorporating constraints into neural networks. We briefly describe each of these areas and their relationship to the present work.Fast optimization methods. Many classical optimization methods have been proposed to improve the practical efficiency of solving optimization problems. These include general techniques such as constraint and variable elimination (i.e., the removal of non-active constraints or redundant variables, respectively), as well as problem-specific techniques (e.g., KKT factorization techniques in the case of convex quadratic programs)(Nocedal & Wright, 2006). Our present work builds upon aspects of this literature, applying concepts from variable elimination to reduce the number of degrees of freedom associated with the optimization problems we wish to solve.In addition to the classical optimization literature, there has been a large body of literature in deep learning that has sought to approximate or speed up optimization models. As described in reviews on topics such as combinatorial optimization (Bengio et al., 2020) and optimal power flow (Hasan et al., 2020), ML methods to speed up optimization models have thus far taken two main approaches. The first class of approaches, akin to work on surrogate modeling(Koziel & Leifsson, 2013), has involved training machine learning models to map directly from optimization inputs to full solutions. However, such approaches have often struggled to produce solutions that are both feasible and (near-)optimal. The second class of approaches has instead focused on employing machine learning approaches alongside or in the loop of optimization models, e.g., to learn warm-start points (see, e.g., Baker (2019) and Dong et al.(2020)) or to enable constraint elimination techniques by predicting active constraints (see, e.g., Misra et al.(2018)). We view our work as part of the former set of approaches, but drawing important inspiration from the latter: that employing structural knowledge about the optimization model is paramount to achieving both feasibility and optimality.Constraints in neural networks. While deep learning is often thought of as wholly unconstrained, in reality, it is quite common to incorporate (simple) constraints within deep learning procedures. For instance, softmax layers encode simplex constraints, sigmoids instantiate upper and lower bounds, ReLUs encode projections onto the positive orthant, and convolutional layers enforce translational equivariance (an idea taken further in general group-equivariant networks (Cohen & Welling, 2016)). Recent work has also focused on embedding specialized kinds of constraints into neural networks, such as conservation of energy (see, e.g., Greydanus et al.(2019)and Beucler et al.(2019)), and homogeneous linear inequality constraints(Frerix et al., 2020). However, while these represent common \"special cases,\" there has to date been little work on building more general hard constraints into deep learning models.",
    "prev": "PINNs are far less efficient than classical methods for solving most PDEs, but are promising tools for high-dimensional or parametric PDEs (Xue et al., 2020) and data assimilation problems.",
    "curr": "The training of PINNs also serves as a model problem for the general challenge of imposing physical constraints on neural networks, an area of fervent and increasing interest (Wang et al., 2021b;Li et al., 2021;Donti et al., 2021).",
    "next": "Training pathologies in PINNs."
  },
  {
    "index": 1442,
    "source_corpus_id": 257219875,
    "ref_id": "b5",
    "citation_corpus_id": 1996665,
    "start": 1948,
    "end": 1966,
    "title": "Published as a conference paper at ICLR 2015 SEMANTIC IMAGE SEGMENTATION WITH DEEP CON- VOLUTIONAL NETS AND FULLY CONNECTED CRFS",
    "abstract": "Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called \"semantic image segmentation\"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our \"DeepLab\" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.",
    "prev": "Extensive experiments on cross-domain image classification and semantic segmentation validate the superiority of DUC.",
    "curr": "INTRODUCTION\n\nDespite the superb performances of deep neural networks (DNNs) on various tasks (Krizhevsky et al., 2012;Chen et al., 2015), their training typically requires massive annotations, which poses formidable cost for practical applications.",
    "next": "Moreover, they commonly assume training and testing data follow the same distribution, making the model brittle to distribution shifts (Ben- David et al., 2010)."
  },
  {
    "index": 1443,
    "source_corpus_id": 263310960,
    "ref_id": "b14",
    "citation_corpus_id": 257280243,
    "start": 7755,
    "end": 7759,
    "title": "PARETO INVARIANT RISK MINIMIZATION: TOWARDS MITIGATING THE OPTIMIZATION DILEMMA IN OUT- OF-DISTRIBUTION GENERALIZATION",
    "abstract": "Recently, there has been a growing surge of interest in enabling machine learning systems to generalize well to Out-of-Distribution (OOD) data. Most efforts are devoted to advancing optimization objectives that regularize models to capture the underlying invariance; however, there often are compromises in the optimization process of these OOD objectives: i) Many OOD objectives have to be relaxed as penalty terms of Empirical Risk Minimization (ERM) for the ease of optimization, while the relaxed forms can weaken the robustness of the original objective; ii) The penalty terms also require careful tuning of the penalty weights due to the intrinsic conflicts between ERM and OOD objectives. Consequently, these compromises could easily lead to suboptimal performance of either the ERM or OOD objective. To address these issues, we introduce a multi-objective optimization (MOO) perspective to understand the OOD optimization process, and propose a new optimization scheme called PAreto Invariant Risk Minimization (PAIR). PAIR improves the robustness of OOD objectives by cooperatively optimizing with other OOD objectives, thereby bridging the gaps caused by the relaxations. Then PAIR approaches a Pareto optimal solution that trades off the ERM and OOD objectives properly. Extensive experiments on challenging benchmarks, WILDS, show that PAIR alleviates the compromises and yields top OOD performances. 1 * Work done during an internship at Tencent AI Lab. 1 Code is available at https://github.com/LFhase/PAIR.Published as a conference paper at ICLR 2023To address these issues, we propose a new optimization scheme for OOD generalization, called PAreto Invariant Risk Minimization (PAIR), which includes a new optimizer (PAIR-o) and a new model selection criteria (PAIR-s). Owing to the MOO formulation, PAIR-o allows for cooperative optimization with other OOD objectives to improve the robustness of practical OOD objectives. Despite the huge gaps between IRMv1 and IRM, we show that incorporating VREx (Krueger et al.,  2021)  into IRMv1 provably recovers the causal invariance (Arjovsky et al., 2019) for some group of problem instances (Sec. 3.2). When given robust OOD objectives, PAIR-o finds a descent path with adaptive penalty weights, which leads to a Pareto optimal solution that trades off ERM and OOD performance properly (Sec. 4). In addition, the MOO analysis also motivates PAIR-s, which facilitates the OOD model selection by considering the trade-offs between ERM and OOD objectives.We conducted extensive experiments on challenging OOD benchmarks. Empirical results show that PAIR-o successfully alleviates the objective conflicts and empowers IRMv1 to achieve high perfor-",
    "prev": "Recall that there has been a significant effort in OOD community to learn invariant features and discard spurious features [5].",
    "curr": "However, these approaches have not shown satisfactory performance when applied to real-world datasets [27], which may be due to the fact that invariant learning requires numerous domains [51], strong regularization [67], and faces additional difficulties induced by non-linearity [51], overparameterization [35], and optimization challenges [15].",
    "next": "In contrast, our findings offer a new perspective that spurious features diversification actually improves OOD performance, which can be easily implemented as shown in ensemble-based models and has achieved remarkable empirical success."
  },
  {
    "index": 1445,
    "source_corpus_id": 6775391,
    "ref_id": "b0",
    "citation_corpus_id": 2684987,
    "start": 3483,
    "end": 3521,
    "title": "DENSITY MODELING OF IMAGES USING A GENERALIZED NORMALIZATION TRANSFORMATION",
    "abstract": "We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. The data are linearly transformed, and each component is then normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and a constant. We optimize the parameters of the full transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. The optimized transformation substantially Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than alternative methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We demonstrate the use of the model as a prior probability density that can be used to remove additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized using the same Gaussianization objective, thus offering an unsupervised method of optimizing a deep network architecture.",
    "prev": "Different compression applications, such as data storage or transmission over limited-capacity channels, demand different rate-distortion trade-offs.",
    "curr": "(Ballé, Laparra, and Simoncelli, 2016).",
    "next": "A vector of image intensities x ∈ R N is mapped to a latent code space via a parametric analysis transform, y = g a (x; φ)."
  },
  {
    "index": 1447,
    "source_corpus_id": 229371407,
    "ref_id": "b24",
    "citation_corpus_id": 211252650,
    "start": 10150,
    "end": 10171,
    "title": "Published as a conference paper at ICLR 2020 LEARNING TO REPRESENT PROGRAMS WITH PROPERTY SIGNATURES",
    "abstract": "We introduce the notion of property signatures, a representation for programs and program specifications meant for consumption by machine learning algorithms. Given a function with input type τ in and output type τ out , a property is a function of type: (τ in , τ out ) → Bool that (informally) describes some simple property of the function under consideration. For instance, if τ in and τ out are both lists of the same type, one property might ask 'is the input list the same length as the output list?'. If we have a list of such properties, we can evaluate them all for our function to get a list of outputs that we will call the property signature. Crucially, we can 'guess' the property signature for a function given only a set of input/output pairs meant to specify that function. We discuss several potential applications of property signatures and show experimentally that they can be used to improve over a baseline synthesizer so that it emits twice as many programs in less than one-tenth of the time.",
    "prev": "Program representation Prior work has studied neural representation of programs.",
    "curr": "Odena & Sutton (2019) propose property signatures to represent input-output examples, and use property signatures to guide an enumerative search.",
    "next": "Graph neural networks have also been used to encode the syntax of programs Dinella et al., 2019) for bug fixing, variable naming, and synthesis."
  },
  {
    "index": 1448,
    "source_corpus_id": 224803601,
    "ref_id": "b27",
    "citation_corpus_id": 9027681,
    "start": 4672,
    "end": 4695,
    "title": "Compositional Semantic Parsing on Semi-Structured Tables",
    "abstract": "Two important aspects of semantic parsing for question answering are the breadth of the knowledge source and the depth of logical compositionality. While existing work trades off one aspect for another, this paper simultaneously makes progress on both fronts through a new task: answering complex questions on semi-structured tables using question-answer pairs as supervision. The central challenge arises from two compounding factors: the broader domain results in an open-ended set of relations, and the deeper compositionality results in a combinatorial explosion in the space of logical forms. We propose a logical-form driven parsing algorithm guided by strong typing constraints and show that it obtains significant improvements over natural baselines. For evaluation, we created a new dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available.",
    "prev": "OTT-QA is distinguished from the existing QA datasets in two aspects.",
    "curr": "Existing table-based QA datasets (Pasupat & Liang, 2015;Yu et al., 2018; operates in the closed setting without requiring any retrieval, whereas most existing open QA datasets (Joshi et al., 2017;Yang et al., 2018) require only text retrieval, not table retrieval.",
    "next": "One dataset, Natural Questions (Kwiatkowski et al., 2019) includes some tabular information in its corpus, but the tables are nearly always of a restricted type (infobox tables with only a single row)."
  },
  {
    "index": 1449,
    "source_corpus_id": 210063976,
    "ref_id": "b18",
    "citation_corpus_id": 52167799,
    "start": 2848,
    "end": 2869,
    "title": "Training Millions of Personalized Dialogue Agents",
    "abstract": "Current dialogue systems are not very engaging for users, especially when trained end-toend without relying on proactive reengaging scripted strategies.Zhang et al. (2018)showed that the engagement level of end-to-end dialogue models increases when conditioning them on text personas providing some personalized back-story to the model. However, the dataset used in(Zhang et al., 2018)is synthetic and of limited size as it contains around 1k different personas. In this paper we introduce a new dataset providing 5 million personas and 700 million persona-based dialogues. Our experiments show that, at this scale, training using personas still improves the performance of end-to-end systems. In addition, we show that other tasks benefit from the wide coverage of our dataset by fine-tuning our model on the data from(Zhang et al., 2018)and achieving state-of-the-art results.",
    "prev": "Two classes of fine-tuned architecture are typically built on top: Bi-encoders and Cross-encoders.",
    "curr": "Cross-encoders (Wolf et al., 2019;Vig & Ramea, 2019), which perform full (cross) self-attention over a given input and label candidate, tend to attain much higher accuracies than their counterparts, Bi-encoders (Mazaré et al., 2018;, which perform self-attention over the input and candidate label separately and combine them at the end for a final representation.",
    "next": "As the representations are separate, Bi-encoders are able to cache the encoded candidates, and reuse these representations for each input resulting in fast prediction times."
  },
  {
    "index": 1450,
    "source_corpus_id": 262084051,
    "ref_id": "b17",
    "citation_corpus_id": 252683303,
    "start": 3409,
    "end": 3413,
    "title": "COMPLEXITY-BASED PROMPTING FOR MULTI-STEP REASONING",
    "abstract": "We study the task of prompting large-scale language models to perform multistep reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexitybased prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on multistep reasoning tasks over strong baselines. We further extend our complexitybased criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority of generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3 and Codex, our approach substantially improves multi-step reasoning accuracy and achieves new state-of-the-art (SOTA) performance on three math benchmarks (GSM8K, MultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and Penguins), with an average +5.3 and up to +18 accuracy improvements. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift.",
    "prev": "e language models (LLMs) which emerge as the favored approach for various applications and demonstrate multi-dimensional abilities, including instruction following [6,49,59], coding assistance [7,32,39,45], and mathematical problem-solving [13,26,38,69].Among various tasks, solving mathematical problems is more challenging as they often require highly complex and symbolic multi-step reasoning capabilities.Although some close-sourced models, e.g., GPT-3.5-Turbo[46], GPT-4 [48] and PaLM-2 [62], have demonstrated promising performance on some mathematical problem-solving benchmarks, it is still a mystery how these models are trained and what data these models use.Therefore, how to equip open-source LLMs (e.g., LLaMA [61,62]) with good mathematical problem-solving skills remains an open challenge.",
    "curr": "To tackle this challenge, two popular lines of research to improve the mathematical problem-solving abilities of LLMs are: prompt-based methods and finetuning-based methods.Prompt-based methods [18,18,66,66,67,74] aim to activate the potential capacities of LLMs by choosing suitable prompting inputs without modifying the model parameters.Finetuning-based methods update the open-source LLMs (e.g., LLaMA) under the guidance of some other powerful closed-source LLMs (e.g., , GPT-4 [48]).While prompt-based methods are model-dependent and sensitive to many factors, finetuning-based methods, despite being simple and model-agnostic, heavily rely on effective training data on downstream mathematical questions.Our work aims to improve finetuning-based methods with a novel method to bootstrap available mathematical questions in the training set.Specifically, we propose to bootstrap the questions in both forward and backward reasoning directions.For the forward direction, we have the original and LLM-rephrased questions.",
    "next": "For the backward direction, we have the self-verification question [68] and FOBAR question [28]."
  },
  {
    "index": 1451,
    "source_corpus_id": 249209899,
    "ref_id": "b18",
    "citation_corpus_id": 218502350,
    "start": 3292,
    "end": 3313,
    "title": "Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?",
    "abstract": "Algorithmic approaches to interpreting machine learning models have proliferated in recent years. We carry out human subject tests that are the first of their kind to isolate the effect of algorithmic explanations on a key aspect of model interpretability, simulatability, while avoiding important confounding experimental factors. A model is simulatable when a person can predict its behavior on new inputs. Through two kinds of simulation tests involving text and tabular data, we evaluate five explanations methods: (1) LIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a Composite approach that combines explanations from each method. Clear evidence of method effectiveness is found in very few cases: LIME improves simulatability in tabular classification, and our Prototype method is effective in counterfactual simulation tests. We also collect subjective ratings of explanations, but we do not find that ratings are predictive of how helpful explanations are. Our results provide the first reliable and comprehensive estimates of how explanations influence simulatability across a variety of explanation methods and data domains. We show that (1) we need to be careful about the metrics we use to evaluate explanation methods, and (2) there is significant room for improvement in current methods. 1",
    "prev": "What makes ProtoPNets appealing is that, despite performing comparably to more opaque predictors, they explain their own predictions in terms of relevant part-prototypes and of examples that these are sourced from.",
    "curr": "These explanations are -by design -more faithful than those extracted by post-hoc approaches (Dombrowski et al., 2019;Teso, 2019;Lakkaraju & Bastani, 2020;Sixt et al., 2020) and can effectively help stakeholders to simulate and anticipate the model's reasoning (Hase & Bansal, 2020).",
    "next": "Despite all these advantages, ProtoPNets are prone -like regular neural networks -to picking up confounders from the training data (e.g., class-correlated watermarks), thus suffering from compromised generalization and out-of-distribution performance (Lapuschkin et al., 2019;Geirhos et al., 2020)."
  },
  {
    "index": 1452,
    "source_corpus_id": 260378901,
    "ref_id": "b30",
    "citation_corpus_id": 11758569,
    "start": 1735,
    "end": 1738,
    "title": "UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks -demonstrating their applicability as general image representations.arXiv:1511.06434v2 [cs.LG] 7 Jan 2016Under review as a conference paper at ICLR 2016• We show that the generators have interesting vector arithmetic properties allowing for easy manipulation of many semantic qualities of generated samples.",
    "prev": "Introduction\n\nGenerative image modeling has a long history [14,48,9] in computer vision, and it has received major developments in multiple directions in the deep learning era [12].",
    "curr": "We have seen explosive development in generative adversarial learning [44,13,31,2,19,8], though many GAN models remain hard to train.",
    "next": "VAE models [22] are easier to train, but the resulting image quality is often blurry."
  },
  {
    "index": 1453,
    "source_corpus_id": 238407710,
    "ref_id": "b6",
    "citation_corpus_id": 13757156,
    "start": 6535,
    "end": 6563,
    "title": "Gaussian Process Behaviour in Wide Deep Neural Networks",
    "abstract": "Whilst deep neural networks have shown great empirical success, there is still much work to be done to understand their theoretical properties. In this paper, we study the relationship between random, wide, fully connected, feedforward networks with more than one hidden layer and Gaussian processes with a recursive kernel definition. We show that, under broad conditions, as we make the architecture increasingly wide, the implied random function converges in distribution to a Gaussian process, formalising and extending existing results byNeal (1996)to deep networks. To evaluate convergence rates empirically, we use maximum mean discrepancy. We then compare finite Bayesian deep networks from the literature to Gaussian processes in terms of the key predictive quantities of interest, finding that in some cases the agreement can be very close. We discuss the desirability of Gaussian process behaviour and review non-Gaussian alternative models from the literature. 1",
    "prev": "The research on infinitely-wide networks can be traced back to the seminal work of Neal (1996), which showed that single hidden layer networks with random weights at initialization (without training) are Gaussian Processes (GPs) in the infinite width limit.",
    "curr": "Later, the connection between GPs and multi-layer infinitely-wide networks with Gaussian initialization (Lee et al., 2018;de G. Matthews et al., 2018) and orthogonal weights (Huang et al., 2021) was reported.",
    "next": "Recent trends in Neural Tangent Kernel (NTK) have led to a proliferation of studies on the optimization and generalization of infinitely (ultra)-wide networks."
  },
  {
    "index": 1454,
    "source_corpus_id": 244345628,
    "ref_id": "b70",
    "citation_corpus_id": 231807280,
    "start": 9263,
    "end": 9267,
    "title": "UNDERSTANDING THE ROLE OF IMPORTANCE WEIGHT- ING FOR DEEP LEARNING",
    "abstract": "The recent paper byByrd & Lipton (2019), based on empirical observations, raises a major concern on the impact of importance weighting for the over-parameterized deep learning models. They observe that as long as the model can separate the training data, the impact of importance weighting diminishes as the training proceeds. Nevertheless, there lacks a rigorous characterization of this phenomenon. In this paper, we provide formal characterizations and theoretical justifications on the role of importance weighting with respect to the implicit bias of gradient descent and margin-based learning theory. We reveal both the optimization dynamics and generalization performance under deep learning models. Our work not only explains the various novel phenomenons observed for importance weighting in deep learning, but also extends to the studies where the weights are being optimized as part of the model, which applies to a number of topics under active research.",
    "prev": "Importance sampling methods have a long history in the MCMC machine learning literature where the sampling is conditioned on the predicted importance of samples [46,44].",
    "curr": "In deep learning, importance sampling methods have been studied theoretically for linearly-separable data [8] and recently in more generality [70].",
    "next": "Furthermore, there exist many importance sampling heuristics in deep learning training including different forms of hard sample mining [62,71,11], weighting based on a focal loss [40], re-weighting for imbalance, [15,26,18] and gradient based scoring [35]."
  },
  {
    "index": 1455,
    "source_corpus_id": 208547770,
    "ref_id": "b0",
    "citation_corpus_id": 14298291,
    "start": 1636,
    "end": 1659,
    "title": "Learning Continuous Semantic Representations of Symbolic Expressions",
    "abstract": "Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of algebraic and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.",
    "prev": "However, the success of neural networks in symbolic computation is still extremely limited: combining symbolic reasoning with continuous representations is now one of the challenges of machine learning.",
    "curr": "Only a few studies investigated the capacity of neural network to deal with mathematical objects, and apart from a small number of exceptions Loos et al., 2017;Allamanis et al., 2017;Arabshahi et al., 2018b), the majority of these works focus on arithmetic tasks like integer addition and multiplication (Zaremba & Sutskever, 2014;Trask et al., 2018).",
    "next": "On these tasks, neural approaches tend to perform poorly, and require the introduction of components biased towards the task at hand Trask et al., 2018)."
  },
  {
    "index": 1456,
    "source_corpus_id": 257205760,
    "ref_id": "b1",
    "citation_corpus_id": 3482308,
    "start": 1872,
    "end": 1892,
    "title": "Understanding Deep Neural Networks with Rectified Linear Units",
    "abstract": "In this paper we investigate the family of functions representable by deep neural networks (DNN) with rectified linear units (ReLU). We give the first-ever polynomial time (in the size of data) algorithm to train to global optimality a ReLU DNN with one hidden layer, assuming the input dimension and number of nodes of the network as fixed constants.We also improve on the known lower bounds on size (from exponential to super exponential) for approximating a ReLU deep net function by a shallower ReLU net. Our gap theorems hold for smoothly parametrized families of \"hard\" functions, contrary to countable, discrete families known in the literature. An example consequence of our gap theorems is the following: for every natural number k there exists a function representable by a ReLU DNN with k 2 hidden layers and total size k 3 , such that any ReLU DNN with at most k hidden layers will require at least 1 2 k k+1 − 1 total nodes.Finally, we construct a family of R n → R piecewise linear functions for n ≥ 2 (also smoothly parameterized), whose number of affine pieces scales exponentially with the dimension n at any fixed size and depth. To the best of our knowledge, such a construction with exponential dependence on n has not been achieved by previous families of \"hard\" functions in the neural nets literature. This construction utilizes the theory of zonotopes from polyhedral theory.",
    "prev": "However, in order to gain a complete understanding of what is going on in modern neural networks, we would also like to answer the following question: what is the precise set of functions we can compute exactly with neural networks of a certain depth?",
    "curr": "For instance, insights about exact representability have recently boosted our understanding of the computational complexity to train neural networks in terms of both, algorithms (Arora et al., 2018;Khalife & Basu, 2022) and hardness results (Goel et al., 2021;Froese et al., 2022;Bertschinger et al., 2022).",
    "next": "Arguably, the most prominent activation function nowadays is the rectified linear unit (ReLU) (Glorot et al., 2011;Goodfellow et al., 2016)."
  },
  {
    "index": 1457,
    "source_corpus_id": 204922497,
    "ref_id": "b18",
    "citation_corpus_id": 6628106,
    "start": 13790,
    "end": 13809,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "We use a K = 256 dimensional bottleneck and a diagonal Gaussian for p(z|x).",
    "curr": "The networks were trained using Ten-sorFlow for 200 epochs using the Adam optimizer (Kingma & Ba, 2015) with a learning rate of 0.0001.",
    "next": "Full hyperparameter details can be found in Appendix A."
  },
  {
    "index": 1458,
    "source_corpus_id": 189898036,
    "ref_id": "b26",
    "citation_corpus_id": 52920337,
    "start": 6920,
    "end": 6944,
    "title": "Gradient descent aligns the layers of deep linear networks",
    "abstract": "This paper establishes risk convergence and asymptotic weight matrix alignment -a form of implicit regularization -of gradient flow and gradient descent when applied to deep linear networks on linearly separable data. In more detail, for gradient flow applied to strictly decreasing loss functions (with similar results for gradient descent with particular decreasing step sizes): (i) the risk converges to 0; (ii) the normalized i th weight matrix asymptotically equals its rank-1 approximation uiv i ; (iii) these rank-1 matrices are aligned across layers, meaning |v i+1 ui| → 1. In the case of the logistic loss (binary cross entropy), more can be said: the linear function induced by the network -the product of its weight matrices -converges to the same direction as the maximum margin solution. This last property was identified in prior work, but only under assumptions on gradient descent which here are implied by the alignment phenomenon.",
    "prev": "It is shown in [Wei et al., 2018] that the regularization path does converge to the max-margin direction for (fully) homogeneous neural networks with cross-entropy or logistic loss.",
    "curr": "For gradient flow, similar results on the convergent direction are known for linear fully-connected networks [Ji and Telgarsky, 2019].",
    "next": "For gradient descent on linear fully-connected and convolutional networks, [Gunasekar et al., 2018b] formulates a constrained optimization problem and proves that gradient descent converges to the direction of a KKT point of this problem or even the max-margin direction, under various assumptions including the convergence of loss and gradient directions."
  },
  {
    "index": 1460,
    "source_corpus_id": 49882757,
    "ref_id": "b2",
    "citation_corpus_id": 11212020,
    "start": 5790,
    "end": 5813,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "Deep Voice 1 & 2 retain the traditional TTS pipeline, which has separate grapheme-to-phoneme, phoneme duration, frequency, and waveform synthesis models.",
    "curr": "In contrast, Tacotron, Deep Voice 3, and Char2Wav employ the attention based sequence-to-sequence models (Bahdanau et al., 2015), yielding more compact architectures.",
    "next": "In the literature, these models are usually referred to as \"endto-end\" speech synthesis."
  },
  {
    "index": 1461,
    "source_corpus_id": 231592453,
    "ref_id": "b3",
    "citation_corpus_id": 204838340,
    "start": 7801,
    "end": 7821,
    "title": "CONTRASTIVE REPRESENTATION DISTILLATION",
    "abstract": "Often we wish to transfer representational knowledge from one neural network to another.Examples include distilling a large network into a smaller one, transferring knowledge from one sensory modality to a second, or ensembling a collection of models into a single estimator.Knowledge distillation, the standard approach to these problems, minimizes the KL divergence between the probabilistic outputs of a teacher and student network.We demonstrate that this objective ignores important structural knowledge of the teacher network.This motivates an alternative objective by which we train a student to capture significantly more information in the teacher's representation of the data.We formulate this objective as contrastive learning.Experiments demonstrate that our resulting new objective outperforms knowledge distillation and other cutting-edge distillers on a variety of knowledge transfer tasks, including single model compression, ensemble distillation, and cross-modal transfer.Our method sets a new state-of-the-art in many transfer tasks, and sometimes even outperforms the teacher network when combined with knowledge distillation.",
    "prev": "Similarly, works in (Ahn et al., 2019;Yim et al., 2017;Koratana et al., 2019;Huang & Wang, 2017) have utilized different learning objectives including consistency on feature maps, consistency on probability mass function, and maximizing the mutual information.",
    "curr": "CRD (Tian et al., 2019a), which is derived from CMC (Tian et al., 2019b), optimizes the student network by a similar objective to Oord et al.",
    "next": "(2018) using a derived lower bound on mutual information."
  },
  {
    "index": 1462,
    "source_corpus_id": 204893960,
    "ref_id": "b4",
    "citation_corpus_id": 3515208,
    "start": 7393,
    "end": 7421,
    "title": "STOCHASTIC GRADIENT DESCENT PERFORMS VARIATIONAL INFERENCE, CONVERGES TO LIMIT CYCLES FOR DEEP NETWORKS",
    "abstract": "Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive. We prove that SGD minimizes an average potential over the posterior distribution of weights along with an entropic regularization term. This potential is however not the original loss function in general. So SGD does perform variational inference, but for a different loss than the one used to compute the gradients. Even more surprisingly, SGD does not even converge in the classical sense: we show that the most likely trajectories of SGD for deep networks do not behave like Brownian motion around critical points. Instead, they resemble closed loops with deterministic components. We prove that such \"out-of-equilibrium\" behavior is a consequence of highly non-isotropic gradient noise in SGD; the covariance matrix of mini-batch gradients for deep networks has a rank as small as 1% of its dimension. We provide extensive empirical validation of these claims, proven in the appendix.",
    "prev": "The empirically motivated measures from prior literature that we consider are based on sharpness measure (Keskar et al., 2016); Fisher-Rao measure (Liang et al., 2017); distance of trained weights from initialization (Nagarajan and Kolter, 2019b) and path norm (Neyshabur et al., 2015a).",
    "curr": "Finally, we consider some optimization based measures based on the speed of the optimization algorithm as motivated by the work of (Hardt et al., 2015) and (Wilson et al., 2017a), and the magnitude of the gradient noise as motivated by the work of (Chaudhari and Soatto, 2018) and (Smith and Le, 2017).",
    "next": "A few papers have explored a large scale study of generalization in deep networks."
  },
  {
    "index": 1464,
    "source_corpus_id": 227247851,
    "ref_id": "b6",
    "citation_corpus_id": 52967399,
    "start": 1485,
    "end": 1506,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "We show the effectiveness of this method to retain improvements in out-of-distribution settings even if no particular bias is targeted by the biased model.",
    "curr": "INTRODUCTION\n\nThe natural language processing community has made tremendous progress in using pre-trained language models to improve predictive accuracy (Devlin et al., 2019;Raffel et al., 2019).",
    "next": "Models have now surpassed human performance on language understanding benchmarks such as Super-GLUE ."
  },
  {
    "index": 1466,
    "source_corpus_id": 222208678,
    "ref_id": "b9",
    "citation_corpus_id": 208857409,
    "start": 8119,
    "end": 8142,
    "title": "YOUR CLASSIFIER IS SECRETLY AN ENERGY BASED MODEL AND YOU SHOULD TREAT IT LIKE ONE",
    "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model.",
    "prev": "Similar to previous authors (Welling & Teh, 2011;Chen et al., 2014), we forgo the Metropolis-Hastings and momentum updates for increased efficiency.",
    "curr": "Although the fixed cutoff T results in a biased sampler, previous works have demonstrated the feasibility of training generative models on images with truncated Langevin MCMC methods (Nijkamp et al., 2020;Du & Mordatch, 2019;Grathwohl et al., 2019).",
    "next": "The model density from Equation 1 approaches the data distribution P D while training, leading to an increased ability in distinguishing between synthesized sets Y − from real sets Y + ."
  },
  {
    "index": 1467,
    "source_corpus_id": 251223792,
    "ref_id": "b21",
    "citation_corpus_id": 964287,
    "start": 1741,
    "end": 1752,
    "title": "ROUGE: A Package for Automatic Evaluation of Summaries",
    "abstract": "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluatio ns. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST.",
    "prev": "is automatic evaluation.",
    "curr": "Traditionally, automatic metrics that rely on discrete token-level matching such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002) have been utilized to check whether system outputs are of high quality across four dimensions (Kryscinski et al., 2019;Yuan et al., 2021): coherence, factuality, fluency, and informativeness.",
    "next": "These metrics do not correlate well with human judgments on all four dimensions of text quality (Fabbri et al., 2021)."
  },
  {
    "index": 1469,
    "source_corpus_id": 252917984,
    "ref_id": "b23",
    "citation_corpus_id": 174797767,
    "start": 3826,
    "end": 3844,
    "title": "Disentangling neural mechanisms for perceptual grouping",
    "abstract": "Forming perceptual groups and individuating objects in visual scenes is an essential step towards visual intelligence. This ability is thought to arise in the brain from computations implemented by bottom-up, horizontal, and top-down connections between neurons. However, the relative contributions of these connections to perceptual grouping are poorly understood. We address this question by systematically evaluating neural network architectures featuring combinations of these connections on two synthetic visual tasks, which stress low-level \"gestalt\" vs. high-level object cues for perceptual grouping. We show that increasing the difficulty of either task strains learning for networks that rely solely on bottom-up processing. Horizontal connections resolve this limitation on tasks with gestalt cues by supporting incremental spatial propagation of activities, whereas top-down connections rescue learning on tasks featuring object cues by propagating coarse predictions about the position of the target object. Our findings disassociate the computational roles of bottom-up, horizontal and top-down connectivity, and demonstrate how a model featuring all of these interactions can more flexibly learn to form perceptual groups.Extant theory suggests that there are two distinct types of feedback strategies: A low-level strategy of grouping visual features with neighboring features according to Gestalt laws including similarity, good continuation, etc.[13][14][15][16][17][18][19]. In contrast, an object-based strategy is mediated by expectations † These authors contributed equally to this work.",
    "prev": "The exact implementation of the S4 model can be viewed as a (depthwise) global convolutional model with an involved computation global convolution kernel.",
    "curr": "Thanks to the global receptive field of the convolution kernel, S4 is able to handle tasks that require LRD, such as Pathfinder [Tay et al., 2020b], where classic local CNNs fail [Linsley et al., 2018, Kim et al., 2019.",
    "next": "Also, the use of Fast Fourier Transform (FFT) and techniques from numerical linear algebra make the computational complexity of S4 tractable compared to the quadratic complexity of attention."
  },
  {
    "index": 1470,
    "source_corpus_id": 239050360,
    "ref_id": "b4",
    "citation_corpus_id": 247084450,
    "start": 5834,
    "end": 5852,
    "title": "AUTO-SCALING VISION TRANSFORMERS WITHOUT TRAINING",
    "abstract": "This work targets automated designing and scaling of Vision Transformers (ViTs). The motivation comes from two pain spots: 1) the lack of efficient and principled methods for designing and scaling ViTs; 2) the tremendous computational cost of training ViT that is much heavier than its convolution counterpart. To tackle these issues, we propose As-ViT, an auto-scaling framework for ViTs without training, which automatically discovers and scales up ViTs in an efficient and principled manner. Specifically, we first design a \"seed\" ViT topology by leveraging a trainingfree search process. This extremely fast search is fulfilled by a comprehensive study of ViT's network complexity, yielding a strong Kendall-tau correlation with ground-truth accuracies. Second, starting from the \"seed\" topology, we automate the scaling rule for ViTs by growing widths/depths to different ViT layers. This results in a series of architectures with different numbers of parameters in a single run. Finally, based on the observation that ViTs can tolerate coarse tokenization in early training stages, we propose a progressive tokenization strategy to train ViTs faster and cheaper. As a unified framework, As-ViT achieves strong performance on classification (83.5% top1 on ImageNet-1k) and detection (52.7% mAP on COCO) without any manual crafting nor scaling of ViT architectures: the end-toend model design and scaling process costs only 12 hours on one V100 GPU. Our code is available at https://github.com/VITA-Group/AsViT.Published as a conference paper at ICLR 2022We point out that the above gaps are inherently connected by the core architecture problem: how to design and scale-up ViTs? Different from the convolutional layer that directly digests raw pixels, ViTs embed coarse-level local patches as input tokens. Shall we divide an image into non-overlapping tokens of smaller size, or larger but overlapped tokens? The former could embed more visual details in each token but ignores spatial coherency, while the latter sacrifices the local details but may benefit more spatial correlations among tokens. A further question is on ViT's depth/width trade-off: shall we prefer a wider and shallower ViT, or a narrower but deeper one? A similar dilemma also persists for ViT training: reducing the number of tokens would effectively speed up the ViT training, but meanwhile might sacrifice the training performance if sticking to coarse tokens from end to end.In this work, we aim to reform the discovery of novel ViT architectures. Our framework, called As-ViT (Auto-scaling ViT), allows for extremely fast, efficient, and principled ViT design and scaling. In short, As-ViT first finds a promising \"seed\" topology for ViT of small depths and widths, then progressively \"grow\" it into different sizes (number of parameters) to meet different needs. Specifically, our \"seed\" ViT topology is discovered from a search space relaxed from recent manual ViT designs. To compare different topologies, we automate this process by a training-free architecture search approach and the measurement of ViT's complexity, which are extremely fast and efficient. This training-free search is supported by our comprehensive study of various network complexity metrics, where we find the expected length distortion has the best trade-off between time costs and Kendall-tau correlations. Our \"seed\" ViT topology is then progressively scaled up from a small network to a large one, generating a series of ViT variants in a single run. Each step, the increases of depth and width are automatically and efficiently balanced by comparing network complexities. Furthermore, to address the data-hungry and heavy computation costs of ViTs, we make our ViT tokens elastic, and propose a progressive re-tokenization method for efficient ViT training. We summarize our contributions as below:",
    "prev": "Finally, our ablation experiments highlight the impact of MEND's key components, showing that variants of MEND are likely to scale to models with hundreds of billions of parameters.",
    "curr": "INTRODUCTION\n\nIncreasingly large models have improved performance on a variety of modern computer vision (Huang et al., 2017;Chen et al., 2022) and especially natural language processing (Vaswani et al., 2017;Brown et al., 2020) problems.",
    "next": "However, a key challenge in deploying and maintaining such models is issuing patches to adjust model behavior after deployment (Sinitsin et al., 2020)."
  },
  {
    "index": 1471,
    "source_corpus_id": 263608308,
    "ref_id": "b51",
    "citation_corpus_id": 3144218,
    "start": 2519,
    "end": 2541,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "o add relevant edges while omitting less beneficial ones.For the first time, our theoretical analysis explores how PR-MPNNs enhance expressive power, and we identify precise conditions under which they outperform purely randomized approaches.Empirically, we demonstrate that our approach effectively mitigates issues like over-squashing and under-reaching.In addition, on established realworld datasets, our method exhibits competitive or superior predictive performance compared to traditional MPNN models and recent graph transformer architectures.",
    "curr": "INTRODUCTION\n\nGraph-structured data is prevalent across various application domains, including fields like chemoand bioinformatics (Barabasi & Oltvai, 2004;Jumper et al., 2021;Reiser et al., 2022), combinatorial optimization (Cappart et al., 2023), and social-network analysis (Easley et al., 2012), highlighting the need for machine learning techniques designed explicitly for graphs.In recent years, message-passing graph neural networks (MPNNs) (Kipf & Welling, 2017;Gilmer et al., 2017;Scarselli et al., 2008b;Veličković et al., 2018) have become the dominant approach in this area, showing promising performance in tasks such as predicting molecular properties (Klicpera et al., 2020;Jumper et al., 2021) or enhancing combinatorial solvers (Cappart et al., 2023).",
    "next": "However, MPNNs have a limitation due to their local aggregation mechanism.They focus on encoding local structures, severely limiting their expressive power (Morris et al., 2019;Xu et al., 2019;Morris et al., 2021).In addition, MPNNs struggle to capture global or long-range information, possibly leading Figure 1: Overview of the probabilistically rewired MPNN framework.PR-MPNNs use an upstream model to learn priors θ for candidate edges, parameterizing a probability mass function conditioned on exactly-k constraints.Subsequently, we sample multiple k-edge adjacency matrices (here: k = 1) from this distribution, aggregate these matrices (here: subtraction), and use the resulting adjacency matr"
  },
  {
    "index": 1473,
    "source_corpus_id": 252545164,
    "ref_id": "b19",
    "citation_corpus_id": 219965819,
    "start": 3474,
    "end": 3497,
    "title": "LIPSCHITZ RECURRENT NEURAL NETWORKS",
    "abstract": "Differential equations are a natural choice for modeling recurrent neural networks because they can be viewed as dynamical systems with a driving input. In this work, we propose a recurrent unit that describes the hidden state's evolution with two parts: a well-understood linear component plus a Lipschitz nonlinearity. This particular functional form simplifies stability analysis, which enables us to provide an asymptotic stability guarantee. Further, we demonstrate that Lipschitz recurrent units are more robust with respect to perturbations. We evaluate our approach on a range of benchmark tasks, and we show it outperforms existing recurrent units.",
    "prev": "Their complexity, however, is bottlenecked by their differential equation numerical solver that limits their scalability to longer-term sequences.",
    "curr": "How can we take advantage of LTC's generalization and causality capabilities and scale them to competitively learn long-range sequences without gradient issues, compared to advanced recurrent neural networks (RNNs) (Erichson et al., 2021;Gu et al., 2020a;Rusch and Mishra, 2021), convolutional neural networks (CNNs) (Cheng et al., 2022;Lea et al., 2016;Romero et al., 2021b), and attention-based models (Vaswani et al., 2017)?",
    "next": "In this work, we set out to leverage the elegant formulation of structural state-space models (S4) (Gu et al., 2022a) to obtain linear liquid network instances that possess the approximation capabilities of both S4 and LTCs."
  },
  {
    "index": 1475,
    "source_corpus_id": 239049483,
    "ref_id": "b34",
    "citation_corpus_id": 604334,
    "start": 7887,
    "end": 7909,
    "title": "Intriguing properties of neural networks",
    "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
    "prev": "1: (a) an efficient but incomplete data generator (b) using a solver to obtain the labels for random samples.",
    "curr": "ADVERSARIAL ROBUSTNESS\n\nAdversarial robustness refers to the robustness of a machine learning model to a small perturbation of the input instance (Szegedy et al., 2014).",
    "next": "We define an adversarial attack in Eq."
  },
  {
    "index": 1479,
    "source_corpus_id": 4722462,
    "ref_id": "b0",
    "citation_corpus_id": 11212020,
    "start": 1941,
    "end": 1964,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "INTRODUCTION\n\nBuilding intelligent machines that can converse with humans is a longstanding challenge in artificial intelligence.",
    "curr": "Remarkable successes have been achieved in natural language processing (NLP) via the use of supervised learning approaches on large-scale datasets (Bahdanau et al., 2015;Wu et al., 2016;Gehring et al., 2017;.",
    "next": "Machine translation is no exception: most translation systems are trained to derive statistical patterns from huge parallel corpora."
  },
  {
    "index": 1480,
    "source_corpus_id": 49881601,
    "ref_id": "b15",
    "citation_corpus_id": 5959482,
    "start": 2572,
    "end": 2593,
    "title": "Efficient Estimation of Word Representations in Vector Space",
    "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities. arXiv:1301.3781v3 [cs.CL] 7 Sep 2013 1 The test set is available at www.fit.vutbr.cz/˜imikolov/rnnlm/word-test.v1.txt 2",
    "prev": "Many problems can be cast in this form: In a natural language processing setting, x represents a context (e.g.",
    "curr": "a bag of words), y represents a candidate word, and the target similarity measures the likelihood to observe y in context x [Mikolov et al., 2013, Pennington et al., 2014, Levy and Goldberg, 2014.",
    "next": "In recommender systems, x represents a user query (the user id and any available contextual information), y represents a candidate item to recommend, and the target similarity is a measure of relevance of item y to query x, e.g."
  },
  {
    "index": 1482,
    "source_corpus_id": 246652474,
    "ref_id": "b1",
    "citation_corpus_id": 46890017,
    "start": 7968,
    "end": 7987,
    "title": "Domain Adaptation with Adversarial Training and Graph Embeddings",
    "abstract": "The success of deep neural networks (DNNs) is heavily dependent on the availability of labeled data. However, obtaining labeled data is a big challenge in many real-world problems. In such scenarios, a DNN model can leverage labeled and unlabeled data from a related domain, but it has to deal with the shift in data distributions between the source and the target domains. In this paper, we study the problem of classifying social media posts during a crisis event (e.g., Earthquake). For that, we use labeled and unlabeled data from past similar events (e.g., Flood) and unlabeled data for the current event. We propose a novel model that performs adversarial learning based domain adaptation to deal with distribution drifts and graph based semi-supervised learning to leverage unlabeled data within a single unified deep learning framework. Our experiments with two real-world crisis datasets collected from Twitter demonstrate significant improvements over several baselines.",
    "prev": "Usually they focus on adaptation between two domains where data points themselves are graphs.",
    "curr": "For example, (Pilancı & Vural, 2019;Pilanci & Vural, 2020) use frequency analysis to align the data graphs between the source domain and the target domains, and (Alam et al., 2018;Ding et al., 2018) perform label propagation on the data graph.",
    "next": "In contrast, GRDA considers a setting completely different from the above references."
  },
  {
    "index": 1483,
    "source_corpus_id": 259075246,
    "ref_id": "b17",
    "citation_corpus_id": 247315559,
    "start": 2212,
    "end": 2215,
    "title": "UniXcoder: Unified Cross-Modal Pre-training for Code Representation",
    "abstract": "Pre-trained models for programming languages have recently demonstrated great success on code intelligence. To support both code-related understanding and generation tasks, recent works attempt to pre-train unified encoder-decoder models. However, such encoder-decoder framework is sub-optimal for auto-regressive tasks, especially code completion that requires a decoder-only manner for efficient inference. In this paper, we present UniXcoder, a unified cross-modal pre-trained model for programming language. The model utilizes mask attention matrices with prefix adapters to control the behavior of the model and leverages cross-modal contents like AST and code comment to enhance code representation. To encode AST that is represented as a tree in parallel, we propose a one-to-one mapping method to transform AST in a sequence structure that retains all structural information from the tree. Furthermore, we propose to utilize multi-modal contents to learn representation of code fragment with contrastive learning, and then align representations among programming languages using a cross-modal generation task. We evaluate UniXcoder on five code-related tasks over nine datasets. To further evaluate the performance of code fragment representation, we also construct a dataset for a new task, called zero-shot code-to-code search. Results show that our model achieves state-of-the-art performance on most tasks and analysis reveals that comment and AST can both enhance UniXcoder.",
    "prev": "Introduction\n\nLarge language models (LLMs) [8,10,39,28] have been instrumental in paving new avenues for innovative applications across diverse domains, with programming being a notably attractive and promising domain [9,41,4,43].In particular, the rise and application of code auto-completion systems like GitHub's Copilot 1 , driven by OpenAI's Codex [9], have the potential to substantially changed the manner in which we interact with code.These changes facilitate coding for beginners and improve efficiency of the coding process for experienced developers.",
    "curr": "A variety of code auto-completion models [9,18,15,27,23,2] have emerged in recent years, each boasting unique capabilities and performance characteristics.This emergence of models emphasizes the increasing importance of AI in the realm of programming, leading to a more diversified and competitive landscape.However, current evaluation datasets and benchmarks [26,34,3] predominantly focus on completion tasks within the scope of a single file.This focus fails to reflect the complexity and intricacies of real-world programming scenarios, where developers frequently work on multi-file projects, often navigating through and understanding code spanning several repositories.",
    "next": "Recognizing the need for a more comprehensive evaluation, we introduce RepoBench, a new benchmark for evaluating the effectiveness of repository-level code auto-completion systems.Specifically, RepoBench offers three distinct evaluation sub-tasks, each emphasizing a unique aspect of a fully functioning code auto-completion system: (1) The Retrieval Task (RepoBench-R),"
  },
  {
    "index": 1484,
    "source_corpus_id": 247996981,
    "ref_id": "b71",
    "citation_corpus_id": 202888885,
    "start": 7970,
    "end": 7973,
    "title": "Published as a conference paper at ICLR 2020 DRAWING EARLY-BIRD TICKETS: TOWARDS MORE EF- FICIENT TRAINING OF DEEP NETWORKS",
    "abstract": "Frankle & Carbin, 2019)shows that there exist winning tickets (small but critical subnetworks) for dense, randomly initialized networks, that can be trained alone to achieve a comparable accuracy to the latter in a similar number of iterations. However, the identification of these winning tickets still requires the costly train-prune-retrain process, limiting their practical benefits. In this paper, we discover for the first time that the winning tickets can be identified at a very early training stage, which we term as Early-Bird (EB) tickets, via lowcost training schemes (e.g., early stopping and low-precision training) at large learning rates. Our finding on the existence of EB tickets is consistent with recently reported observations that the key connectivity patterns of neural networks emerge early. Furthermore, we propose a mask distance metric that can be used to identify EB tickets with a low computational overhead, without needing to know the true winning tickets that emerge after the full training. Finally, we leverage the existence of EB tickets and the proposed mask distance to develop efficient training methods, which are achieved by first identifying EB tickets via low-cost schemes, and then continuing to train merely the EB tickets towards the target accuracy. Experiments based on various deep networks and datasets validate: 1) the existence of EB tickets and the effectiveness of mask distance in efficiently identifying them; and 2) that the proposed efficient training via EB tickets can achieve up to 5.8× ∼ 10.7× energy savings while maintaining comparable or even better accuracy as compared to the most competitive state-ofthe-art training methods, demonstrating a promising and easily adopted method for tackling the often cost-prohibitive deep network training.",
    "prev": "Even though these methods surpass the trivial baseline of random network pruning at initialization, [19] suggests that accuracy of these methods on standard classification benchmarks remains below the dense network obtained after training.",
    "curr": "Pruning after training introduced by the Lottery Ticket Hypothesis [16], adapted by various works [3,5,6,12,17,18,20,47,49,72,75], prune the neural network weights based on the magnitude after the network is trained.",
    "next": "Typically the dense network is reset to the initial weights after pruning and retrained from scratch."
  },
  {
    "index": 1485,
    "source_corpus_id": 222272074,
    "ref_id": "b21",
    "citation_corpus_id": 3626819,
    "start": 2468,
    "end": 2488,
    "title": "Deep contextualized word representations",
    "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
    "prev": "Open code for reproducing and extending our results is provided.",
    "curr": "* equal contribution arXiv:2010.04290v1 [cs.LG]\n\nINTRODUCTION AND MOTIVATION\n\nDeep Learning revolutionized Machine Learning by improving the accuracy by dozens of percents for fundamental tasks in Natural Language Processing (NLP) through learning representations of a natural language via a deep neural network (Mikolov et al., 2013;Radford et al., 2018;Le and Mikolov, 2014;Peters et al., 2018;Radford et al., 2019).",
    "next": "Lately, it was shown that there is no need to train those networks from scratch each time we receive a new task/data, but to fine-tune a full pre-trained model on the specific task (Dai and Le, 2015;Radford et al., 2018;Devlin et al., 2019)."
  },
  {
    "index": 1487,
    "source_corpus_id": 264439160,
    "ref_id": "b9",
    "citation_corpus_id": 247084324,
    "start": 2098,
    "end": 2121,
    "title": "Overcoming a Theoretical Limitation of Self-Attention",
    "abstract": "Although transformers are remarkably effective for many tasks, there are some surprisingly easy-looking regular languages that they struggle with. Hahn shows that for languages where acceptance depends on a single input symbol, a transformer's classification decisions become less and less confident (that is, with crossentropy approaching 1 bit per string) as input strings get longer and longer. We examine this limitation using two languages: PAR-ITY, the language of bit strings with an odd number of 1s, and FIRST, the language of bit strings starting with a 1. We demonstrate three ways of overcoming the limitation suggested by Hahn's lemma. First, we settle an open question by constructing a transformer that recognizes PARITY with perfect accuracy, and similarly for FIRST. Second, we use layer normalization to bring the cross-entropy of both models arbitrarily close to zero. Third, when transformers need to focus on a single position, as for FIRST, we find that they can fail to generalize to longer strings; we offer a simple remedy to this problem that also improves length generalization in machine translation.",
    "prev": "(2023)does not correctly predict Transformers' out-of-distribution behavior, but our conjecture does.Overall, our work provides a novel perspective on the mechanisms of compositional generalization and the algorithmic capabilities of Transformers.",
    "curr": "Introduction\n\nLarge language models (LLMs) have shown impressive abilities in natural language generation, reading comprehension, code-synthesis, instruction-following, commonsense reasoning, and many other tasks (Brown et al., 2020;Chen et al., 2021;Chowdhery et al., 2022;Lewkowycz et al., 2022b;Gunasekar et al., 2023;Touvron et al., 2023).However, when evaluated in controlled studies, Transformers often struggle with out-of-distribution generalization (Nogueira et al., 2021;Ontañón et al., 2022;Dziri et al., 2023;Wu et al., 2023;Saparov et al., 2023).It is thus not clear how to reconcile Transformers' seemingly-impressive performance in some settings with their fragility in others.",
    "next": "In this work, we aim to understand when standard decoder-only Transformers can generalize systematically beyond their training distribution.We adopt the approach of recent studies and focus on length generalization on algorithmic tasks as a measure of how well language models can learn to reason (Nogueira et al., 2021;Kim et al., 2021;Anil et al., 2022;Lee et al., 2023;Dziri et al., 2023;Welleck et al., 2022;Liu et al., 2023).Length generalization evaluates the model on problems that Figure 1: a A selection of tasks studied in this paper partitioned by whether they can be s"
  },
  {
    "index": 1489,
    "source_corpus_id": 3515219,
    "ref_id": "b16",
    "citation_corpus_id": 8822680,
    "start": 2635,
    "end": 2658,
    "title": "Six Challenges for Neural Machine Translation",
    "abstract": "We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrasebased statistical machine translation.",
    "prev": "Thanks to this, NMT has been reported to significantly improve over SMT both in automatic metrics and human evaluation (Wu et al., 2016).",
    "curr": "Nevertheless, for the same reasons described above, NMT requires a large parallel corpus to be effective, and is known to fail when the training data is not big enough (Koehn & Knowles, 2017).",
    "next": "Unfortunately, the lack of large parallel corpora is a practical problem for the vast majority of language pairs, including low-resource languages (e.g."
  },
  {
    "index": 1490,
    "source_corpus_id": 227343966,
    "ref_id": "b29",
    "citation_corpus_id": 85498775,
    "start": 2157,
    "end": 2180,
    "title": "Competence-based Curriculum Learning for Neural Machine Translation",
    "abstract": "Current state-of-the-art NMT systems use large neural networks that are not only slow to train, but also often require many heuristics and optimization tricks, such as specialized learning rate schedules and large batch sizes. This is undesirable as it requires extensive hyperparameter tuning. In this paper, we propose a curriculum learning framework for NMT that reduces training time, reduces the need for specialized heuristics or large batch sizes, and results in overall better performance. Our framework consists of a principled way of deciding which training samples are shown to the model at different times during training, based on the estimated difficulty of a sample and the current competence of the model. Filtering training samples in this manner prevents the model from getting stuck in bad local optima, making it converge faster and reach a better solution than the common approach of uniformly sampling training examples. Furthermore, the proposed method can be easily applied to existing NMT models by simply modifying their input data pipelines. We show that our framework can help improve the training time and the performance of both recurrent neural network models and Transformers, achieving up to a 70% decrease in training time, while at the same time obtaining accuracy improvements of up to 2.2 BLEU.",
    "prev": "introduction\n\nInspired by the importance of properly ordering information when teaching humans (Avrahami et al., 1997), curriculum learning (CL) proposes training models by presenting easier examples earlier during training (Elman, 1993;Sanger, 1994;Bengio et al., 2009).",
    "curr": "Previous empirical studies have shown instances where curriculum learning can improve convergence speed and/or generalization in domains such as natural language processing (Cirik et al., 2016;Platanios et al., 2019), computer vision (Pentina et al., 2015;Sarafianos et al., 2017;Guo et al., 2018;, and neural evolutionary computing (Zaremba & Sutskever, 2014).",
    "next": "In contrast to curriculum learning, anti-curriculum learning selects the most difficult examples first and gradually exposes the model to easier ones."
  },
  {
    "index": 1492,
    "source_corpus_id": 252992876,
    "ref_id": "b10",
    "citation_corpus_id": 231592390,
    "start": 3465,
    "end": 3485,
    "title": "Published as a conference paper at ICLR 2021 UNLEARNABLE EXAMPLES: MAKING PERSONAL DATA UNEXPLOITABLE",
    "abstract": "The volume of \"free\" data on the internet has been key to the current success of deep learning. However, it also raises privacy concerns about the unauthorized exploitation of personal data for training commercial models. It is thus crucial to develop methods to prevent unauthorized data exploitation. This paper raises the question: can data be made unlearnable for deep learning models? We present a type of error-minimizing noise that can indeed make training examples unlearnable. Error-minimizing noise is intentionally generated to reduce the error of one or more of the training example(s) close to zero, which can trick the model into believing there is \"nothing\" to learn from these example(s). The noise is restricted to be imperceptible to human eyes, and thus does not affect normal data utility. We empirically verify the effectiveness of error-minimizing noise in both samplewise and class-wise forms. We also demonstrate its flexibility under extensive experimental settings and practicability in a case study of face recognition. Our work establishes an important first step towards making personal data unexploitable to deep learning models.The development of unlearnable examples should take full advantage of the unique characteristics, and more importantly, the weaknesses of DNNs. One well-studied characteristic of DNNs is that they tend to capture more of the high-frequency components of the data . Surprisingly, † Correspondence to: Xingjun Ma Learned-Miller. Labeled faces in the wild: A database forstudying face recognition in unconstrained environments. 2008.",
    "prev": "However, they also have the potential risk of privacy leakage.",
    "curr": "Thus, growing efforts (Huang et al., 2020;Fowl et al., 2021) have been made on protecting data from unauthorized usage by making the data samples unlearnable (Huang et al., 2020;Fowl et al., 2021;He et al., 2022).",
    "next": "In these methods, they generate the unlearnable examples by injecting imperceptible \"shortcut\" perturbation."
  },
  {
    "index": 1493,
    "source_corpus_id": 238582773,
    "ref_id": "b30",
    "citation_corpus_id": 1671874,
    "start": 29554,
    "end": 29568,
    "title": "WORDNET: A LEXICAL DATABASE FOR ENGLISH",
    "abstract": "PROJECT GOALSWordNet is a lexical database for English organized in accordance with current psycholinguistic theories. Lexicalized concepts are organized by semantic relations (synonymy, antonymy, hyponymy, meronymy, etc.) for nouns, verbs, and adjectives.",
    "prev": "Web-crawled data.",
    "curr": "We use the user tags and machine tags of YFCC15M to form a tag list, and use WordNet (Miller, 1995) to find synonyms for each tag in the tag list to form a synonym tag list.",
    "next": "The tag list and synonym tag list form a query list."
  },
  {
    "index": 1494,
    "source_corpus_id": 257985547,
    "ref_id": "b9",
    "citation_corpus_id": 233169183,
    "start": 8597,
    "end": 8616,
    "title": "PLASTICINELAB: A SOFT-BODY MANIPULATION BENCHMARK WITH DIFFERENTIABLE PHYSICS",
    "abstract": "Simulated virtual environments serve as one of the main driving forces behind developing and evaluating skill learning algorithms. However, existing environments typically only simulate rigid body physics. Additionally, the simulation process usually does not provide gradients that might be useful for planning and control optimizations. We introduce a new differentiable physics benchmark called PasticineLab, which includes a diverse collection of soft body manipulation tasks. In each task, the agent uses manipulators to deform the plasticine into a desired configuration. The underlying physics engine supports differentiable elastic and plastic deformation using the DiffTaichi system, posing many underexplored challenges to robotic agents. We evaluate several existing reinforcement learning (RL) methods and gradient-based methods on this benchmark. Experimental results suggest that 1) RL-based approaches struggle to solve most of the tasks efficiently; 2) gradient-based approaches, by optimizing open-loop control sequences with the built-in differentiable physics engine, can rapidly find a solution within tens of iterations, but still fall short on multi-stage tasks that require long-term planning. We expect that PlasticineLab will encourage the development of novel algorithms that combine differentiable physics and RL for more complex physics-based skill learning tasks. PlasticineLab is publicly available 1 . * This work was done during an internship at the MIT-IBM Watson AI Lab.",
    "prev": "Differentiable Physics.",
    "curr": "Differentiable physics gains traction recently with the emerging differentiable physics simulators (DPS) (Hu et al., 2019;2020;Huang et al., 2021;Qiao et al., 2021).",
    "next": "It has brought success in many domains and provides a distinctly new approach to control tasks."
  },
  {
    "index": 1495,
    "source_corpus_id": 247570285,
    "ref_id": "b5",
    "citation_corpus_id": 1066490,
    "start": 2307,
    "end": 2330,
    "title": "Demographic Dialectal Variation in Social Media: A Case Study of African-American English",
    "abstract": "Though dialectal language is increasingly abundant on social media, few resources exist for developing NLP tools to handle such language. We conduct a case study of dialectal language in online conversational text by investigating African-American English (AAE) on Twitter. We propose a distantly supervised model to identify AAE-like language from demographics associated with geo-located messages, and we verify that this language follows well-known AAE linguistic phenomena. In addition, we analyze the quality of existing language identification and dependency parsing tools on AAE-like text, demonstrating that they perform poorly on such text compared to text associated with white speakers. We also provide an ensemble classifier for language identification which eliminates this disparity and release a new corpus of tweets containing AAE-like language.Data and software resources are available at:, it calculates a unigram language model that sums to one across the vocabulary. This hints at a more complete modeling approach ( §2.3). 6  To build the vocabulary, we select all words used by at least 20 different users, resulting in 191,873 unique words; other words are mapped to an out-of-vocabulary symbol.",
    "prev": "1\n\nINTRODUCTION\n\nIt is well acknowledged that modern neural network based machine learning models tend to underperform when they are evaluated on data distributions that differ from the one they were trained on.",
    "curr": "For example, machine learning model performance has been observed to degrade under train-test mismatch in topics (Gururangan et al., 2020), demographics (Blodgett et al., 2016;Amodei et al., 2016;Hovy & Søgaard, 2015;Grother et al., 2019), geographic regions , and even data collection processes (Beery et al., 2018;Zech et al., 2018;Michel & Neubig, 2018).",
    "next": "In particular, these models often perform poorly when evaluated on subpopulations, domains that are present but underrepresented in their training data , and they can latch on to spurious correlations (McCoy et al., 2019)."
  },
  {
    "index": 1497,
    "source_corpus_id": 258332176,
    "ref_id": "b20",
    "citation_corpus_id": 211842237,
    "start": 8400,
    "end": 8404,
    "title": "Published as a conference paper at ICLR 2020 DIRECTIONAL MESSAGE PASSING FOR MOLECULAR GRAPHS",
    "abstract": "Graph neural networks have recently achieved great successes in predicting quantum mechanical properties of molecules. These models represent a molecule as a graph using only the distance between atoms (nodes). They do not, however, consider the spatial direction from one atom to another, despite directional information playing a central role in empirical potentials for molecules, e.g. in angular potentials. To alleviate this limitation we propose directional message passing, in which we embed the messages passed between atoms instead of the atoms themselves. Each message is associated with a direction in coordinate space. These directional message embeddings are rotationally equivariant since the associated directions rotate with the molecule. We propose a message passing scheme analogous to belief propagation, which uses the directional information by transforming messages based on the angle between them. Additionally, we use spherical Bessel functions and spherical harmonics to construct theoretically well-founded, orthogonal representations that achieve better performance than the currently prevalent Gaussian radial basis representations while using fewer than 1 /4 of the parameters. We leverage these innovations to construct the directional message passing neural network (DimeNet). DimeNet outperforms previous GNNs on average by 76 % on MD17 and by 31 % on QM9. Our implementation is available online. 1 1 https://www.daml.in.tum.de/dimenet arXiv:2003.03123v2 [cs.LG] 5 Apr 2022Published as a conference paper at ICLR 2020 embeddings are equivariant with respect to the above transformations since the directions move with the molecule. Hence, they preserve the relative directional information between neighboring atoms. We propose to let message embeddings interact based on the distance between atoms and the angle between directions. Both distances and angles are invariant to translation, rotation, and inversion of the molecule, as required. Additionally, we show that the distance and angle can be jointly represented in a principled and effective manner by using spherical Bessel functions and spherical harmonics. We leverage these innovations to construct the directional message passing neural network (DimeNet). DimeNet can learn both molecular properties and atomic forces. It is twice continuously differentiable and solely based on the atom types and coordinates, which are essential properties for performing molecular dynamics simulations. DimeNet outperforms previous GNNs on average by 76 % on MD17 and by 31 % on QM9. Our paper's main contributions are:",
    "prev": "Recent advances of this category [4,16,3] introduced higher-order spherical harmonic tensors and significantly improved the performance.",
    "curr": "Another major type of EGNNs adopted equivariance or invariance by scalarizing vectorial features, which was first introduced by SchNet [43] and later developed to DimeNet [21], SphereNet [37] and GemNet [20], with increasingly comprehensive geometric information encoded with spherical harmonics.",
    "next": "Moreover, PaiNN [41] and TorchMD-NET [45] focused on iteratively updating scalar and vector features, and recently proposed ViSNet [49] further extended the idea of PaiNN by including the runtime geometric computation (RGC) and the corresponding vector-scalar interactive message passing (ViS-MP) to form a complete representation of molecule structures with lower computational cost."
  },
  {
    "index": 1498,
    "source_corpus_id": 84591,
    "ref_id": "b1",
    "citation_corpus_id": 11212020,
    "start": 1754,
    "end": 1757,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "After training these models on a massive database of image-label pairs like ImageNet [26], the network easily adapts to a variety of similar visual tasks, achieving impressive results on image classification [5,25,32] or localization [9,21] tasks.",
    "curr": "In other perceptual domains such as natural language processing or speech recognition, deep networks have proven highly effective as well [2,12,28,30].",
    "next": "However, all of these recent results rely on a supervisory signal from large-scale databases of hand-labeled data, ignoring much of the useful information present in the structure of the data itself."
  },
  {
    "index": 1499,
    "source_corpus_id": 226278023,
    "ref_id": "b11",
    "citation_corpus_id": 108296442,
    "start": 5119,
    "end": 5137,
    "title": "THE NEURO-SYMBOLIC CONCEPT LEARNER: INTERPRETING SCENES, WORDS, AND SENTENCES FROM NATURAL SUPERVISION",
    "abstract": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "prev": "We test D3DP-Nets in few-shot concept learning, visual question answering (VQA) and scene generation.",
    "curr": "We train concept classifiers for object shapes, object colors/materials, and spatial relationships on our inferred disentangled feature spaces, and show they outperform current stateof-the-art (Mao et al., 2019;Hu et al., 2016), which use 2D representations.",
    "next": "We show that a VQA modular network that incorporates our concept classifiers shows improved generalization over the state-of-the-art (Mao et al., 2019) with dramatically fewer examples."
  },
  {
    "index": 1500,
    "source_corpus_id": 236134152,
    "ref_id": "b47",
    "citation_corpus_id": 216562627,
    "start": 3287,
    "end": 3308,
    "title": "Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels",
    "abstract": "We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to regularize the value function. Existing model-free approaches, such as Soft Actor-Critic (SAC)[20], are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based[21,31,22]methods and recently proposed contrastive learning[42]. Our approach can be combined with any model-free reinforcement learning algorithm, requiring only minor modifications. An implementation can be found at https://sites.google.com/view/data-regularized-q.IntroductionSample-efficient deep reinforcement learning (RL) algorithms capable of directly training from image pixels would open up many real-world applications in control and robotics. However, simultaneously training a convolutional encoder alongside a policy network is challenging when given limited environment interaction, strong correlation between samples and a typically sparse reward signal. Naive attempts to use a large capacity encoder result in severe over-fitting (seeFigure 1a) and smaller encoders produce impoverished representations that limit task performance.Limited supervision is a common problem across AI and a number of approaches are adopted: (i) pretraining with self-supervised learning (SSL), followed by standard supervised learning; (ii) supervised learning with an additional auxiliary loss and (iii) supervised learning with data augmentation. SSL approaches are highly effective in the large data regime, e.g. in domains such as vision [7, 23] and NLP[12,13]where large (unlabeled) datasets are readily available. However, in RL this is not the case: an off-policy RL agent is trained on a replay buffer that grows as the agent interacts with the environment. But in a sample efficient regime the buffer may only hold 10 4 -10 5 transitions from a few hundred trajectories, limiting the effectiveness of SSL methods.A wide range of auxiliary loss functions have been proposed to augment supervised objectives, e.g. weight regularization, noise injection [25], or some form of online SSL objective. In RL, * Equal contribution. Author ordering determined by coin flip. Both authors are corresponding.Preprint. Under review.",
    "prev": "Over the last three years, the RL community has made significant headway on this problem, improving sample-efficiency significantly.",
    "curr": "The key insight to solving visual control is the learning of better low-dimensional representations, either through autoencoders [Yarats et al., 2019, Finn et al., 2015, variational inference [Hafner et al., 2018, contrastive learning [Srinivas et al., 2020, Yarats et al., 2021a, self-prediction [Schwarzer et al., 2020b], or data augmentations [Yarats et al., 2021b.",
    "next": "However, current state-of-the-art model-free methods are still limited in three ways."
  },
  {
    "index": 1502,
    "source_corpus_id": 239049858,
    "ref_id": "b0",
    "citation_corpus_id": 3508234,
    "start": 3595,
    "end": 3616,
    "title": "Online Learning Rate Adaptation with Hypergradient Descent",
    "abstract": "We introduce a general method for improving the convergence rate of gradientbased optimizers that is easy to implement and works well in practice. We analyze the effectiveness of the method by applying it to stochastic gradient descent, stochastic gradient descent with Nesterov momentum, and Adam, showing that it improves upon these commonly used algorithms on a range of optimization problems; in particular the kinds of objective functions that arise frequently in deep neural network training. Our method works by dynamically updating the learning rate during optimization using the gradient with respect to the learning rate of the update rule itself. Computing this \"hypergradient\" needs little additional computation, requires only one extra copy of the original gradient to be stored in memory, and relies upon nothing more than what is provided by reverse-mode automatic differentiation.",
    "prev": "We demonstrate our algorithm handles a range of hyperparameter initialisations and datasets, improving test loss after a single training episode ('one pass').",
    "curr": "Relaxing differentiation-through-optimisation (Domke, 2012) and hypergradient descent's (Baydin et al., 2018) exactness allows us to improve computational and memory efficiency.",
    "next": "Our scalable one-pass method improves performance from arbitrary hyperparameter initialisations, and could be augmented with a further search over those initialisations if desired."
  },
  {
    "index": 1506,
    "source_corpus_id": 49299039,
    "ref_id": "b9",
    "citation_corpus_id": 3568073,
    "start": 3397,
    "end": 3418,
    "title": "PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION",
    "abstract": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at 1024 2 . We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CELEBA dataset.",
    "prev": "(Yeh et al., 2017) showed how a latent model trained with GAN can be used to perform inpainting of tightly-cropped 64 × 64 face images.",
    "curr": "Below, we show that such models trained with GANs cannot generalize to higher resolution (eventhough GAN-based systems are now able to obtain high-quality samples at high resolutions (Karras et al., 2018)).",
    "next": "We argue that it is the limited dimensionality of the latent space in GANs and other existing latent models that precludes them from spanning the space of high-resolution natural images."
  },
  {
    "index": 1507,
    "source_corpus_id": 58006691,
    "ref_id": "b28",
    "citation_corpus_id": 10565222,
    "start": 1854,
    "end": 1871,
    "title": "A Network-based End-to-End Trainable Task-oriented Dialogue System",
    "abstract": "Teaching machines to accomplish tasks by conversing naturally with humans is challenging. Currently, developing taskoriented dialogue systems requires creating multiple components and typically this involves either a large amount of handcrafting, or acquiring costly labelled datasets to solve a statistical learning problem for each component. In this work we introduce a neural network-based text-in, textout end-to-end trainable goal-oriented dialogue system along with a new way of collecting dialogue data based on a novel pipe-lined Wizard-of-Oz framework. This approach allows us to develop dialogue systems easily and without making too many assumptions about the task at hand. The results show that the model can converse with human subjects naturally whilst helping them to accomplish tasks in a restaurant search domain.",
    "prev": "INTRODUCTION\n\nTask-oriented dialogue systems aim to achieve specific user goals such as restaurant reservation or navigation inquiry within a limited dialogue turns via natural language.",
    "curr": "Traditional pipeline solutions are composed of natural language understanding, dialogue management and natural language generation (Young et al., 2013;Wen et al., 2017), where each module is designed separately and expensively.",
    "next": "In order to reduce human effort and scale up between domains, end-to-end dialogue systems, which input plain text and directly output system responses, have shown promising results based on recurrent neural networks (Zhao et al., 2017;Lei et al., 2018) and memory networks (Sukhbaatar et al., 2015)."
  },
  {
    "index": 1510,
    "source_corpus_id": 260154786,
    "ref_id": "b17",
    "citation_corpus_id": 3075448,
    "start": 17117,
    "end": 17121,
    "title": "HIGH-DIMENSIONAL CONTINUOUS CONTROL USING GENERALIZED ADVANTAGE ESTIMATION",
    "abstract": "Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(λ). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.Published as a conference paper at ICLR 2016 λ ∈ [0, 1], the generalized advantage estimator (GAE). Related methods have been proposed in the context of online actor-critic methods(Kimura & Kobayashi, 1998;Wawrzyński, 2009). We provide a more general analysis, which is applicable in both the online and batch settings, and discuss an interpretation of our method as an instance of reward shaping(Ng et al., 1999), where the approximate value function is used to shape the reward.We present experimental results on a number of highly challenging 3D locomotion tasks, where we show that our approach can learn complex gaits using high-dimensional, general purpose neural network function approximators for both the policy and the value function, each with over 10 4 parameters. The policies perform torque-level control of simulated 3D robots with up to 33 state dimensions and 10 actuators.The contributions of this paper are summarized as follows:1. We provide justification and intuition for an effective variance reduction scheme for policy gradients, which we call generalized advantage estimation (GAE). While the formula has been proposed in prior work(Kimura & Kobayashi, 1998;Wawrzyński, 2009), our analysis is novel and enables GAE to be applied with a more general set of algorithms, including the batch trust-region algorithm we use for our experiments. 2. We propose the use of a trust region optimization method for the value function, which we find is a robust and efficient way to train neural network value functions with thousands of parameters. 3. By combining(1)and(2)above, we obtain an algorithm that empirically is effective at learning neural network policies for challenging control tasks. The results extend the state of the art in using reinforcement learning for high-dimensional continuous control. Videos are available at https://sites.google.com/site/gaepapersupp.",
    "prev": "We explain later in the experiments how to choose a baseline.",
    "curr": "One can perform a Monte Carlo estimate [17] or generalized advantage function (GAE) [18] to estimate returns based on the marginal gain.",
    "next": "To encourage exploration, similar to standard PG, we can employ a soft policy update based on entropy penalization, resulting in diverse trajectory samples."
  },
  {
    "index": 1512,
    "source_corpus_id": 254591453,
    "ref_id": "b12",
    "citation_corpus_id": 225039882,
    "start": 6643,
    "end": 6669,
    "title": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. 1 arXiv:2010.11929v2 [cs.CV] 3 Jun 2021Published as a conference paper at ICLR 2021 inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.However, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks.RELATED WORKTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global selfattention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.",
    "prev": "translation invariance, scale-invariance, and feature locality (Xu et al., 2021b) than ConvNets and can better capture long-range relationships between image pixels.",
    "curr": "In the original ViT architecture (Dosovitskiy et al., 2021;Touvron et al., 2021a), images are split into patches and are transformed into tokens that are passed through the encoder of a transformer (Vaswani et al., 2017).",
    "next": "Based on this framework, LeViT (Graham et al., 2021) achieves a significant performance improvement over ViT by combining convolutional and transformer encoder layers."
  },
  {
    "index": 1514,
    "source_corpus_id": 238407985,
    "ref_id": "b0",
    "citation_corpus_id": 52967399,
    "start": 2103,
    "end": 2106,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "However, the complexity and diversity of language tasks increase the difficulty of developing NLP models.",
    "curr": "Thankfully, NLP is being revolutionized by large-scale pre-trained language models such as BERT [1] and GPT-2 [2], which can be adapted to a variety of downstream NLP tasks with less training data and resources.",
    "next": "Users can directly download such models and transfer them to their tasks, such as text classification [3] and sequence tagging [4]."
  },
  {
    "index": 1515,
    "source_corpus_id": 252735209,
    "ref_id": "b20",
    "citation_corpus_id": 236635379,
    "start": 23986,
    "end": 24007,
    "title": "PERCEIVER IO: A GENERAL ARCHITECTURE FOR STRUCTURED INPUTS & OUTPUTS",
    "abstract": "A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain & task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence.Is the development of problem-specific models for each new set of inputs and outputs unavoidable? Life would be drastically simpler if a single neural network architecture could handle a wide variety of both input modalities and output tasks. In this work, we propose such an architecture, with the ultimate goal of building a network that can easily integrate and transform arbitrary information for arbitrary tasks. Our starting point is the Perceiver(Jaegle et al., 2021), an architecture which has demonstrated a remarkable ability to handle data from many modalities with no changes to the network architecture. The Perceiver uses attention to map inputs of a wide range of modalities to a fixed-size latent space that is further processed by a deep, fully attentional network. This process decouples the bulk of the network's processing from the size and modality-specific details of the input, allowing it to scale to large and multimodal data.But the Perceiver can only handle simple output spaces like classification. Much of the complexity of real-world tasks comes from the variety, size, and structure of their outputs, and in this regard i arXiv:2107.14795v3 [cs.LG] Gong. VATT: Transformers for multimodal self-supervised learning from raw video, audio and text.. A naturalistic open source movie for optical flow evaluation. . CANINE: pre-training an efficient tokenization-free encoder for language representation. Transactions of the Association for Computational Linguistics, 10:73-91, 2022.Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. . Sim2real transfer learning for 3D human pose estimation: motion to the rescue. . Learning to estimate hidden motions with global motion aggregation. . Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.Iasonas Kokkinos. Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory.",
    "prev": "F t consists of L slots {l 0 , l 1 , ..l L−1 }, each of dimension d l so that F t ∈ R L×d l .",
    "curr": "The messages in M ′ t compete with each other to write into each KS's state slot via a cross-attention mechanism.The query, in this case, is a linear projection of the F t , i.e., Q = F t W q , whereas the keys and values are linear projections of the messages M ′ t .KS state is updated as:\nF t ← softmax Q(M ′ t W e ) T √ de M ′ t W v\nAfter this, self-attention is applied to the KS using a transformer encoder tower constituting a Perceiver-IO architecture (Jaegle et al., 2022).",
    "next": "Step 3: Reading from the Knowledge Source.The KS makes the updated state available to the agents should they deem to use it.We again utilize cross attention to perform the reading operation."
  },
  {
    "index": 1516,
    "source_corpus_id": 238353834,
    "ref_id": "b42",
    "citation_corpus_id": 222208678,
    "start": 10051,
    "end": 10071,
    "title": "SET PREDICTION WITHOUT IMPOSING STRUCTURE AS CONDITIONAL DENSITY ESTIMATION",
    "abstract": "Set prediction is about learning to predict a collection of unordered variables with unknown interrelations. Training such models with set losses imposes the structure of a metric space over sets. We focus on stochastic and underdefined cases, where an incorrectly chosen loss function leads to implausible predictions. Example tasks include conditional point-cloud reconstruction and predicting future states of molecules. In this paper, we propose an alternative to training via set losses by viewing learning as conditional density estimation. Our learning framework fits deep energy-based models and approximates the intractable likelihood with gradient-guided sampling. Furthermore, we propose a stochastically augmented prediction algorithm that enables multiple predictions, reflecting the possible variations in the target set. We empirically demonstrate on a variety of datasets the capability to learn multi-modal densities and produce multiple plausible predictions. Our approach is competitive with previous set prediction models on standard benchmarks. More importantly, it extends the family of addressable tasks beyond those that have unambiguous predictions.",
    "prev": "Independent sampling Along with MLP based generation, the most popular method for set and graph creation is to draw n points i.i.d.",
    "curr": "from a low dimensional normal distribution, and to concatenate the latent vector to each sample (Köhler et al., 2020;Yang et al., 2019b;Stelzner et al., 2020;Satorras et al., 2021;Zhang et al., 2021a;Liu et al., 2021).",
    "next": "The main advantage of independent sampling is that it does not constrain the number of points that can be generated."
  },
  {
    "index": 1517,
    "source_corpus_id": 211069110,
    "ref_id": "b33",
    "citation_corpus_id": 11591887,
    "start": 2045,
    "end": 2064,
    "title": "OFFLINE BILINGUAL WORD VECTORS, ORTHOGONAL TRANSFORMATIONS AND THE INVERTED SOFTMAX",
    "abstract": "Usually bilingual word vectors are trained \"online\".Mikolov et al. (2013a)showed they can also be found \"offline\"; whereby two pre-trained embeddings are aligned with a linear transformation, using dictionaries compiled from expert knowledge. In this work, we prove that the linear transformation between two spaces should be orthogonal. This transformation can be obtained using the singular value decomposition. We introduce a novel \"inverted softmax\" for identifying translation pairs, with which we improve the precision @1 of Mikolov's original mapping from 34% to 43%, when translating a test set composed of both common and rare English words into Italian. Orthogonal transformations are more robust to noise, enabling us to learn the transformation without expert bilingual signal by constructing a \"pseudo-dictionary\" from the identical character strings which appear in both languages, achieving 40% precision on the same test set. Finally, we extend our method to retrieve the true translations of English sentences from a corpus of 200k Italian sentences with a precision @1 of 68%.",
    "prev": "Embedding alignment was originally studied for word vectors with the goal of enabling cross-lingual transfer, where the embeddings for two languages are in alignment if word translations, e.g.",
    "curr": "cat and Katze, have similar representations (Mikolov et al., 2013a;Smith et al., 2017).",
    "next": "Recently, large pretrained models have largely subsumed word vectors based on their accuracy on downstream tasks, partly due to the fact that their word representations are context-dependent, allowing them to more richly capture the meaning of a word (Peters et al., 2018;Howard & Ruder, 2018;Radford et al., 2018;Devlin et al., 2018)."
  },
  {
    "index": 1518,
    "source_corpus_id": 264439306,
    "ref_id": "b25",
    "citation_corpus_id": 3144218,
    "start": 47282,
    "end": 47304,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "., p}, where r is the perturbation rate and |E| is the number of edges.Detailed choices of p for each dataset in attacking group/individual fairness are summarized in Table 4.",
    "curr": "Training the victim model.We use a fixed list of random seed ([0, 1, 2, 42, 100]) to train each victim model 5 times and report the mean and standard deviation.Regarding the victim models in group fairness attacks, we train a 2-layer GCN (Kipf & Welling, 2017) for 400 epochs and a 2-layer FairGNN (Dai & Wang, 2021) for 2000 epochs to evaluate the efficacy of fairness attacks.The hidden dimension, learning rate, weight decay and dropout rate of GCN and FairGNN are set to 128, 1e − 3, 1e − 5 and 0.5, respectively.The regularization parameters in FairGNN, namely α and β, are set to 100 and 1 for all datasets, respectively.Regarding the victim models in individual fairness attacks, we train a 2-layer GCN (Kipf & Welling, 2017) and 2-layer InFoRM-GNN (Kang et al., 2020;Dong et al., 2021) for 400 epochs.The hidden dimension, learning rate, weight decay and dropout rate of GCN and InFoRM-GNN are set to 128, 1e − 3, 1e − 5 and 0.5, respectively.The regularization parameter in InFoRM-GNN is set to 0.1 for all datasets.",
    "next": "E ADDITIONAL EXPERIMENTAL RESULTS: ATTACKING STATISTICAL PARITY ON GRAPH NEURAL NETWORKS\n\nA -FATE with FairGNN as the victim model.Here, we study how robust FairGNN is in fairness attacks against statistical parity with linear GCN a"
  },
  {
    "index": 1519,
    "source_corpus_id": 235266159,
    "ref_id": "b69",
    "citation_corpus_id": 231632202,
    "start": 1959,
    "end": 1962,
    "title": "Published as a conference paper at ICLR 2021 FREE LUNCH FOR FEW-SHOT LEARNING: DISTRIBUTION CALIBRATION",
    "abstract": "Learning from a limited number of samples is challenging since the learned model can easily become overfitted based on the biased distribution formed by only a few training examples. In this paper, we calibrate the distribution of these fewsample classes by transferring statistics from the classes with sufficient examples. Then an adequate number of examples can be sampled from the calibrated distribution to expand the inputs to the classifier. We assume every dimension in the feature representation follows a Gaussian distribution so that the mean and the variance of the distribution can borrow from that of similar classes whose statistics are better estimated with an adequate number of samples. Our method can be built on top of off-the-shelf pretrained feature extractors and classification models without extra parameters. We show that a simple logistic regression classifier trained using the features sampled from our calibrated distribution can outperform the state-of-the-art accuracy on three datasets (5% improvement on miniImageNet compared to the next best). The visualization of these generated features demonstrates that our calibrated distribution is an accurate estimation.",
    "prev": "Experiments demonstrate that the proposed method is superior to baselines and robust to a broad range of label noise types.",
    "curr": "Introduction\n\nLearning with noisy labels is one of the most challenging problems in weakly-supervised learning, since noisy labels are ubiquitous in the real world [40,74,45,1,70].",
    "next": "For instance, both crowdsourcing and web crawling yield large numbers of noisy labels everyday [15]."
  },
  {
    "index": 1521,
    "source_corpus_id": 216562627,
    "ref_id": "b25",
    "citation_corpus_id": 14717992,
    "start": 3963,
    "end": 3967,
    "title": "REINFORCEMENT LEARNING WITH UNSUPERVISED AUXILIARY TASKS",
    "abstract": "Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-theart on Atari, averaging 880% expert human performance, and a challenging suite of first-person, three-dimensional Labyrinth tasks leading to a mean speedup in learning of 10× and averaging 87% expert human performance on Labyrinth.Natural and artificial agents live in a stream of sensorimotor data. At each time step t, the agent receives observations o t and executes actions a t . These actions influence the future course of the sensorimotor stream. In this paper we develop agents that learn to predict and control this stream, by solving a host of reinforcement learning problems, each focusing on a distinct feature of the sensorimotor stream. Our hypothesis is that an agent that can flexibly control its future experiences will also be able to achieve any goal with which it is presented, such as maximising its future rewards.The classic reinforcement learning paradigm focuses on the maximisation of extrinsic reward. However, in many interesting domains, extrinsic rewards are only rarely observed. This raises questions of what and how to learn in their absence. Even if extrinsic rewards are frequent, the sensorimotor stream contains an abundance of other possible learning targets. Traditionally, unsupervised learning attempts to reconstruct these targets, such as the pixels in the current or subsequent frame. It is typically used to accelerate the acquisition of a useful representation. In contrast, our learning objective is to predict and control features of the sensorimotor stream, by treating them as pseudorewards for reinforcement learning. Intuitively, this set of tasks is more closely matched with the agent's long-term goals, potentially leading to more useful representations.Consider a baby that learns to maximise the cumulative amount of red that it observes. To correctly predict the optimal value, the baby must understand how to increase \"redness\" by various means, including manipulation (bringing a red object closer to the eyes); locomotion (moving in front of a red object); and communication (crying until the parents bring a red object). These behaviours are likely to recur for many other goals that the baby may subsequently encounter. No understanding of these behaviours is required to simply reconstruct the redness of current or subsequent images.Our architecture uses reinforcement learning to approximate both the optimal policy and optimal value function for many different pseudo-rewards. It also makes other auxiliary predictions that serve to focus the agent on important aspects of the task. These include the long-term goal of predicting cumulative extrinsic reward as well as short-term predictions of extrinsic reward. To learn more efficiently, our agents use an experience replay mechanism to provide additional updates * Joint first authors. Ordered alphabetically by first name.",
    "prev": "Furthermore, there is a clear performance improvement relative to (a), particularly for the more challenging Walker Walk task.",
    "curr": "reconstruction objectives [26,49] or alternate tasks are often used.",
    "next": "However, these objectives are unrelated to the task at hand, thus have no guarantee of inducing an appropriate representation for the policy network."
  },
  {
    "index": 1522,
    "source_corpus_id": 257353706,
    "ref_id": "b6",
    "citation_corpus_id": 10986188,
    "start": 5622,
    "end": 5642,
    "title": "Class-Based n-gram Models of Natural Language",
    "abstract": "We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics.",
    "prev": "Unsupervised pretraining has achieved tremendous success in image recognition (Caron et al., 2019), objective detection (Dai et al., 2021), natural language processing (Devlin et al., 2018;Radford et al., 2018;Song et al., 2019) and speech recognition (Schneider et al., 2019;Baevski et al., 2020).",
    "curr": "Two most widely-used pretraining approaches are (1) feature-based approaches (Brown et al., 1992;Mikolov et al., 2013;Melamud et al., 2016;Peter et al., 2018), which pretrains a model to extract representations and directly uses the pretrained representations as inputs for the downstream tasks; (2) fine-tuning based approaches, (see, e.g., Devlin et al., 2018), which fine-tunes all the model parameters in the neighborhood of pretrained representations based on downstream tasks.",
    "next": "Erhan et al."
  },
  {
    "index": 1526,
    "source_corpus_id": 250243971,
    "ref_id": "b19",
    "citation_corpus_id": 11816014,
    "start": 2256,
    "end": 2280,
    "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
    "abstract": "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com.",
    "prev": "Moreover, the evaluation of compressing an already compact model shows our method can further reduce 9% to 30% parameters with an insignificant impact on task accuracy.",
    "curr": "INTRODUCTION\n\nLanguage models built with transformers (Devlin et al., 2018) have attained extensive success in natural language tasks such as language modeling (Radford et al., 2018), text classification (Wang et al., 2018), question answering (Rajpurkar et al., 2016), and summarization (Liu, 2019).",
    "next": "The success is achieved by fine-tuning a big transformer model pre-trained with a large corpus."
  },
  {
    "index": 1527,
    "source_corpus_id": 264426355,
    "ref_id": "b5",
    "citation_corpus_id": 248986755,
    "start": 4284,
    "end": 4306,
    "title": "Instruction Induction: From Few Examples to Natural Language Task Descriptions",
    "abstract": "Large language models are able to perform a task by conditioning on a few input-output demonstrations -a paradigm known as incontext learning. We show that language models can explicitly infer an underlying task from a few demonstrations by prompting them to generate a natural language instruction that fits the examples. To explore this ability, we introduce the instruction induction challenge, compile a dataset consisting of 24 tasks, and define a novel evaluation metric based on executing the generated instruction. We discover that, to a large extent, the ability to generate instructions does indeed emerge when using a model that is both large enough and aligned to follow instructions; InstructGPT achieves 65.7% of human performance in our execution-based metric, while the original GPT-3 model reaches only 9.8% of human performance. This surprising result suggests that instruction induction might be a viable learning paradigm in and of itself, where instead of fitting a set of latent continuous parameters to the data, one searches for the best description in the natural language hypothesis space. 1",
    "prev": "hodologies exhibit significant drawbacks.Many prompting methods are ad hoc because of their human-in-the-loop development paradigm.In such a process, given a target task, we first draft an initial prompt.Then, we refine the prompt using techniques such as chain-of-thought, few-shot demonstrations, and coding-style problem descriptions (Wei et al., 2022c,a;Gao et al., 2023) based on the model's performance on the target task.We note that in practice, a hand-crafted prompt optimized for one task rarely translates to satisfactory performance in another task (Zhang et al., 2023).Therefore, each task becomes a new expedition, with its own set of trials, errors, and validations.Such an ad hoc human-in-the-loop development procedure introduces extensive human labor requirements, which significantly hinder the applicability of LLMs in real-world applications.",
    "curr": "Existing works develop algorithms to automatically generate prompts instead of relying on ad hoc human optimization (Shin et al., 2020;Honovich et al., 2022;Zhou et al., 2022).However, these methods often lack feedback loops, such that the refinement procedure essentially performs a random search.For example, in each refinement iteration, Zhou et al.",
    "next": "(2022) simply rephrases the prompt into multiple candidates, and then select the candidate that yields the best performance as the refined prompt.Note that such a procedure fails to learn from past successes and failures, such that refined prompt does not enrich the original prompt with additional context."
  },
  {
    "index": 1528,
    "source_corpus_id": 259341801,
    "ref_id": "b10",
    "citation_corpus_id": 252715594,
    "start": 3168,
    "end": 3172,
    "title": "PHENAKI: VARIABLE LENGTH VIDEO GENERATION FROM OPEN DOMAIN TEXTUAL DESCRIPTIONS",
    "abstract": "We present Phenaki, a model capable of realistic video synthesis, given a sequence of textual prompts. Generating videos from text is particularly challenging due to the computational cost, limited quantities of high quality text-video data and variable length of videos. To address these issues, we introduce a new model for learning video representation which compresses the video to a small representation of discrete tokens. This tokenizer uses causal attention in time, which allows it to work with variable-length videos. To generate video tokens from text we are using a bidirectional masked transformer conditioned on pre-computed text tokens. The generated video tokens are subsequently de-tokenized to create the actual video. To address data issues, we demonstrate how joint training on a large corpus of image-text pairs as well as a smaller number of video-text examples can result in generalization beyond what is available in the video datasets. Compared to the previous video generation methods, Phenaki can generate arbitrary long videos conditioned on a sequence of prompts (i.e. time variable text or a story) in open domain. To the best of our knowledge, this is the first time a paper studies generating videos from time variable prompts. In addition, compared to the perframe baselines, the proposed video encoder-decoder computes fewer tokens per video but results in better spatio-temporal consistency. ‡ Equal contribution.",
    "prev": "Additional samples are provided Appendices C and D.\n\n1 Introduction 1.1 Generative models are training on synthetic data from generative models Due to rapid advances in generative artificial intelligence (AI), synthetic data of all kinds is rapidly proliferating.",
    "curr": "Publicly available generative models have not only revolutionized the image, audio, and text domains [2][3][4][5][6][7][8][9], but they are also starting to impact the creation of videos, 3D models, graphs, tables, software, and even websites [10][11][12][13][14][15].",
    "next": "Companies like Google, Microsoft, and Shutterstock are incorporating generative models into their consumer services, and the output from these services and popular generative models like Stable Diffusion [2] (for images) and ChatGPT [16] (for text) tend to end up on the Internet."
  },
  {
    "index": 1531,
    "source_corpus_id": 263909329,
    "ref_id": "b51",
    "citation_corpus_id": 252815378,
    "start": 9646,
    "end": 9668,
    "title": "Fine-tuned Language Models are Continual Learners",
    "abstract": "Recent work on large language models relies on the intuition that most natural language processing tasks can be described via natural language instructions and that models trained on these instructions show strong zero-shot performance on several standard datasets. However, these models even though impressive still perform poorly on a wide range of tasks outside of their respective training and evaluation sets. To address this limitation, we argue that a model should be able to keep extending its knowledge and abilities, without forgetting previous skills. In spite of the limited success of Continual Learning we show that Fine-tuned Language Models can be continual learners. We empirically investigate the reason for this success and conclude that Continual Learning emerges from self-supervision pre-training. Our resulting model Continual-T0 (CT0) is able to learn 8 new diverse language generation tasks, while still maintaining good performance on previous tasks, spanning in total 70 datasets. Finally, we show that CT0 is able to combine instructions in ways it was never trained for, demonstrating some level of instruction compositionality.",
    "prev": "ation methods for neural networks, primarily focusing on regulating the norm of network parameters (Glorot & Bengio, 2010;He et al., 2015).Theoretical works try to study these methods through dynamical isometry (Saxe et al., 2013) or mean field theory (Poole et al., 2016).Orthogonal initialization, which supports layer-wise dynamical isometry in fully-connected layers, has been extended to CNNs via Delta orthogonal initialization (Xiao et al., 2018).However, there has been limited research on initialization methods specifically for Transformers.Most of these works focus on theoretical approaches to train Transformers without skip connections or normalization layers (Noci et al., 2022;He et al., 2023).Mimetic initialization (Trockman & Kolter, 2023) seeks to initialize attention based on the principles of pre-trained Transformers.",
    "curr": "Continual pre-training.Recent research explores adapting pre-trained networks for new or improved datasets.While some target datasets from different domains (Scialom et al., 2022;Ke et al., 2022;Gupta et al., 2023;Qin et al., 2022), others focus on datasets that evolve over time (Han et al., 2020;Jang et al., 2021;Loureiro et al., 2022).Model expansion is similar to continual pre-training, with the distinction being a change in the model size rather than the data distribution.",
    "next": "PRELIMINARIES\n\nModel expansion aims to initialize a large model with the weights from its smaller pre-trained counterparts.Concretely, suppose we have pre-trained weights θ S in a source network f S (•; θ trained\n\n\nS\n\n), our goal is to design a mapping θ expanded An example for model expansion is to use a pre-trained ResNet-18 (He et al., 2016) or BERT-Small (f S ) to facilitate the training of ResNet-50 or BERT-Base (f T ), respectively.Instead of training the larger models from scratch, the idea is to initialize them with the weights of their smaller pre-trained counterparts, i.e., ResNet-18 or BERT-Small, respectively."
  },
  {
    "index": 1532,
    "source_corpus_id": 264487385,
    "ref_id": "b48",
    "citation_corpus_id": 219531522,
    "start": 1858,
    "end": 1875,
    "title": "FASTSPEECH 2: FAST AND HIGH-QUALITY END-TO- END TEXT TO SPEECH",
    "abstract": "Non-autoregressive text to speech (TTS) models such as FastSpeech(Ren et al., 2019)can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has several disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the issues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x training speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and Fast-Speech 2 can even surpass autoregressive models. Audio samples are available at https://speechresearch.github.io/fastspeech2/. * Authors contribute equally to this work. † Corresponding author arXiv:2006.04558v8 [eess.AS] 8 Aug 2022 2019). They usually suffer from slow inference speed and robustness (word skipping and repeating) issues(Ren et al., 2019;Chen et al., 2020). In recent years, non-autoregressive TTS models(Ren et al., 2019;Łańcucki, 2020;Lim et al., 2020;Miao et al., 2020;are designed to address these issues, which generate mel-spectrograms with extremely fast speed and avoid robustness issues, while achieving comparable voice quality with previous autoregressive models.Among those non-autoregressive TTS methods, FastSpeech (Ren et al., 2019) is one of the most successful models. FastSpeech designs two ways to alleviate the one-to-many mapping problem: 1) Reducing data variance in the target side by using the generated mel-spectrogram from an autoregressive teacher model as the training target (i.e., knowledge distillation). 2) Introducing the duration information (extracted from the attention map of the teacher model) to expand the text sequence to match the length of the mel-spectrogram sequence. While these designs in FastSpeech ease the learning of the one-to-many mapping problem (see Section 2.1) in TTS, they also bring several disadvantages: 1) The two-stage teacher-student training pipeline makes the training process complicated.2) The target mel-spectrograms generated from the teacher model have some information loss 1 compared with the ground-truth ones, since the quality of the audio synthesized from the generated mel-spectrograms is usually worse than that from the ground-truth ones.3) The duration extracted from the attention map of teacher model is not accurate enough.",
    "prev": "odel can be adapted to different downstream tasks with strong performance.Specifically, we pre-trained a generative model, named SpeechFlow, on 60k hours of untranscribed speech with Flow Matching and masked conditions.Experiment results show the pre-trained generative model can be fine-tuned with task-specific data to match or surpass existing expert models on speech enhancement, separation, and synthesis.Our work suggested a foundational model for generation tasks in speech can be built with generative pre-training.",
    "curr": "INTRODUCTION\n\nDiscriminative models have long been the mainstream in speech applications since the deep learning era.These models are applied to different types of tasks such as speech recognition (Graves et al., 2006), enhancement, and separation (Luo & Mesgarani, 2019).Interestingly, even for applications that can be naturally formulated as generative modeling problems, such as text-to-speech (TTS), we see most popular models remained discriminative (Shen et al., 2018;Ren et al., 2021).Consequentially, pre-trained foundation models (Baevski et al., 2020;Hsu et al., 2021) that served as the upstream of speech applications focused more on learning useful representation for discriminative tasks rather than modeling the data distribution p(speech).In this paper, we seek to answer whether generative models can serve as foundation models for speech applications or not.",
    "next": "Unlike discriminative models, generative models enable sampling of the data distribution.For example, generative TTS models (Habib et al., 2019) allow different emotions to be sampled given a fixed text as discriminative models produce a fixed output.Up to the present, generative models in speech are usually designed for a given purpose via task-specific conditioning or distribution mapping.Perhaps the most well-known examples of task-specific conditional generative models are neural vocoders (Kong et al., 2020;Chen et al., 2020).These models learn to map simple priors (e.g., normal distribution) to wavefo"
  },
  {
    "index": 1533,
    "source_corpus_id": 219980606,
    "ref_id": "b24",
    "citation_corpus_id": 6628106,
    "start": 18830,
    "end": 18834,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "We experiment with three different families of warping functions: nonparametric (NP) [34], CPA-based (CPAb) [50], and our TSP-based functions (TSPb).",
    "curr": "We minimize the mean squared error over 200 epochs of ADAM [25] with learning rate η = 0.01.",
    "next": "We explore two training strategies to obtain hard segmentations at the end of training: (i) 160 epochs with linear interpolation followed by 40 epochs of integer interpolation (160, 40), and (ii) 200 epochs with integer interpolation only (0, 200)."
  },
  {
    "index": 1534,
    "source_corpus_id": 240354190,
    "ref_id": "b12",
    "citation_corpus_id": 6212000,
    "start": 5265,
    "end": 5285,
    "title": "UNDERSTANDING DEEP LEARNING REQUIRES RE- THINKING GENERALIZATION",
    "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.",
    "prev": "Kernel machines enjoy firmer theoretical footing than deep neural networks, which allows one to accurately study their training and generalization (Rasmussen & Williams, 2006;Schölkopf & Smola, 2002).",
    "curr": "Moreover, they share many of the phenomena that overparameterized neural networks exhibit, such as interpolating the training data (Zhang et al., 2017;Liang & Rakhlin, 2018;Belkin et al., 2018).",
    "next": "However, the exact equivalence between neural networks and kernel machines breaks for finite width networks."
  },
  {
    "index": 1538,
    "source_corpus_id": 252716013,
    "ref_id": "b41",
    "citation_corpus_id": 165163607,
    "start": 25909,
    "end": 25935,
    "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions",
    "abstract": "In this paper we study yes/no questions that are naturally occurring -meaning that they are generated in unprompted and unconstrained settings. We build a reading comprehension dataset, BoolQ, of such questions, and show that they are unexpectedly challenging. They often query for complex, non-factoid information, and require difficult entailment-like inference to solve. We also explore the effectiveness of a range of transfer learning baselines. We find that transferring from entailment data is more effective than transferring from paraphrase or extractive QA data, and that it, surprisingly, continues to be very beneficial even when starting from massive pre-trained language models such as BERT. Our best method trains BERT on MultiNLI and then re-trains it on our train set. It achieves 80.4% accuracy compared to 90% accuracy of human annotators (and 62% majority-baseline), leaving a significant gap for future work.",
    "prev": "We aim to validate whether AMA provides consistent lift across diverse tasks (Section 5.1), works across model families (Section 5.2), and reliably aggregates the predictions across prompts (Section 5.3).",
    "curr": "Experimental details We use a diverse set of tasks: SuperGLUE [Wang et al., 2019], NLI [Mostafazadeh et al., 2017, Nie et al., 2020, classification [Zhang et al., 2015, Socher et al., 2013, He and McAuley, 2016, and QA tasks [Kasai et al., 2022, Kwiatkowski et al., 2019, Berant et al., 2013, Dua et al., 2019.",
    "next": "For all tasks, we compare to published results of the OpenAI few-shot-prompted GPT3-175B parameter model using the numbers reported in  and, for classification tasks, Zhao et al."
  },
  {
    "index": 1539,
    "source_corpus_id": 3861760,
    "ref_id": "b16",
    "citation_corpus_id": 252796,
    "start": 23601,
    "end": 23622,
    "title": "Building a Large Annotated Corpus of English: The Penn Treebank",
    "abstract": "There is a growing consensus that significant, rapid progress can be made in both text understanding and spoken language understanding by investigating those phenomena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large corpora. Such corpora are beginning to serve as important research tools for investigators in natural language processing, speech recognition, and integrated spoken language systems, as well as in theoretical linguistics. Annotated corpora promise to be valuable for enterprises as diverse as the automatic construction of statistical models for the grammar of the written and the colloquial spoken language, the development of explicit formal theories of the differing grammars of writing and speech, the investigation of prosodic phenomena in speech, and the evaluation and comparison of the adequacy of parsing models.In this paper, we review our experience with constructing one such large annotated corpus--the Penn Treebank, a corpus 1 consisting of over 4.5 million words of American English. During the first three-year phase of the Penn Treebank Project (1989)(1990)(1991)(1992), this corpus has been annotated for part-of-speech (POS) information. In addition, over half of it has been annotated for skeletal syntactic structure. These materials are available to members of the Linguistic Data Consortium; for details, see Section 5.1.The paper is organized as follows. Section 2 discusses the POS tagging task. After outlining the considerations that informed the design of our POS tagset and presenting the tagset itself, we describe our two-stage tagging process, in which text is first assigned POS tags automatically and then corrected by human annotators. Section 3 briefly presents the results of a comparison between entirely manual and semi-automated tagging, with the latter being shown to be superior on three counts: speed, consistency, and accuracy. In Section 4, we turn to the bracketing task. Just as with the tagging task, we have partially automated the bracketing task: the output of",
    "prev": "However, flipout is applicable to a wider variety of architectures, including convolutional nets and RNNs.",
    "curr": "REGULARIZATION FOR LANGUAGE MODELING\n\nWe evaluated the regularization effect of flipout on the character-level and word-level language modeling tasks with the Penn Treebank corpus (PTB) (Marcus et al., 1993).",
    "next": "We compared flipout to several other methods for regularizing RNNs: naïve dropout (Zaremba et al., 2014), variational dropout (Gal & Ghahramani, 2016), recurrent dropout (Semeniuta et al., 2016), zoneout (Krueger et al., 2016), and DropConnect (Merity et al., 2017)."
  },
  {
    "index": 1541,
    "source_corpus_id": 212945787,
    "ref_id": "b28",
    "citation_corpus_id": 7147309,
    "start": 2536,
    "end": 2558,
    "title": "SEQUENCE LEVEL TRAINING WITH RECURRENT NEURAL NETWORKS",
    "abstract": "Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster.",
    "prev": "First, the designer of the model needs to specify the order in which tokens are generated.",
    "curr": "Second, at training time the model is conditioned on ground truth context while at test time it is conditioned on its own generations, a discrepancy referred to as exposure bias (Ranzato et al., 2016).",
    "next": "Finally, while heuristics like beam search somewhat help rescore at the sequence level, generation generally lacks long-range coherency because it is produced by the greedy selection of one token at a time without lookahead."
  },
  {
    "index": 1542,
    "source_corpus_id": 252734952,
    "ref_id": "b42",
    "citation_corpus_id": 12718048,
    "start": 2540,
    "end": 2561,
    "title": "A Syntactic Neural Model for General-Purpose Code Generation",
    "abstract": "We consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like Python. Existing datadriven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language. Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge. Experiments find this an effective way to scale up to generation of complex programs from natural language descriptions, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.",
    "prev": "NL→code models facilitate programming for both professional and inexperienced programmers, by allowing programmers to write code by only expressing their higher-level intent.",
    "curr": "Many existing code generation models either learn directly from input-output pairs provided as training data (Allamanis et al., 2015;Yin and Neubig, 2017;Iyer et al., 2018;Brockschmidt et al., 2019;Xu et al., 2020;Alon et al., 2020;, or learn the mapping between input and output implicitly from naturally occurring corpora of intertwined natural language and code (Austin et al., 2021;Nijkamp et al., 2022).",
    "next": "Nevertheless, all these works assume that all libraries and function calls were seen in the training data; and that at test time, the trained model will need to generate only seen libraries and function calls."
  },
  {
    "index": 1543,
    "source_corpus_id": 258866127,
    "ref_id": "b7",
    "citation_corpus_id": 247292203,
    "start": 5957,
    "end": 5960,
    "title": "Deep Reinforcement Learning for Entity Alignment",
    "abstract": "Embedding-based methods have attracted increasing attention in recent entity alignment (EA) studies. Although great promise they can offer, there are still several limitations. The most notable is that they identify the aligned entities based on cosine similarity, ignoring the semantics underlying the embeddings themselves. Furthermore, these methods are shortsighted, heuristically selecting the closest entity as the target and allowing multiple entities to match the same candidate. To address these limitations, we model entity alignment as a sequential decision-making task, in which an agent sequentially decides whether two entities are matched or mismatched based on their representation vectors. The proposed reinforcement learning (RL)-based entity alignment framework can be flexibly adapted to most embedding-based EA methods. The experimental results demonstrate that it consistently advances the performance of several state-of-the-art methods, with a maximum improvement of 31.1% on Hits@1.",
    "prev": "To formally prove this idea, we dissect the ELBO in Equation (4) as follows:\nE p θ (y|x) log p(x, y) p θ (y|x) = E p θ (y|x) log p θ (x|y) − D KL (p θ (y|x) ∥ p(y))(7)\nThe complete derivation can be found in Appendix A.",
    "curr": "Therefore, we have:\nlog p(x) = E p θ (y|x) log p θ (x|y) reconstruction term − D KL (p θ (y|x) ∥ p(y)) distribution matching term + D KL (p θ (y|x) ∥ p(y|x))\nprediction matching term (8) The first term aims to reconstruct the original embedding x based on y generated from x, which has not been studied by the existing EEA works.",
    "next": "The second term imposes the distribution y conditioned on x to match the prior distribution of y, which has been investigated by the GAN-based EEA methods [9][10][11]."
  },
  {
    "index": 1545,
    "source_corpus_id": 250089125,
    "ref_id": "b11",
    "citation_corpus_id": 52967399,
    "start": 1290,
    "end": 1310,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Introduction\n\nModeling long range dependencies on sequential data is a crucial step towards closing the gap with human-level performance on many tasks.",
    "curr": "Attention based models like Transformer [Vaswani et al., 2017] have proven to be a strong choice of backbone architecture for a considerable number of tasks across modalities and scale [Devlin et al., 2019, Brown et al., 2020, Dosovitskiy et al., 2021.",
    "next": "Vanilla Multi-Head-Attention famously incurs Ω(L 2 ) penalty in modeling a sequence of length L. This is prohibitive at best for tasks where the model is required to capture long range dependencies from various parts of the input."
  },
  {
    "index": 1546,
    "source_corpus_id": 248965495,
    "ref_id": "b7",
    "citation_corpus_id": 239049858,
    "start": 12989,
    "end": 13009,
    "title": "SCALABLE ONE-PASS OPTIMISATION OF HIGH-DIMENSIONAL WEIGHT-UPDATE HYPERPARAMETERS BY IMPLICIT DIFFERENTIATION",
    "abstract": "Machine learning training methods depend plentifully and intricately on hyperparameters, motivating automated strategies for their optimisation. Many existing algorithms restart training for each new hyperparameter choice, at considerable computational cost. Some hypergradient-based one-pass methods exist, but these either cannot be applied to arbitrary optimiser hyperparameters (such as learning rates and momenta) or take several times longer to train than their base models. We extend these existing methods to develop an approximate hypergradient-based hyperparameter optimiser which is applicable to any continuous hyperparameter appearing in a differentiable model weight update, yet requires only one training episode, with no restarts. We also provide a motivating argument for convergence to the true hypergradient, and perform tractable gradient-based optimisation of independent learning rates for each model parameter. Our method performs competitively from varied random hyperparameter initialisations on several UCI datasets and Fashion-MNIST (using a one-layer MLP), Penn Treebank (using an LSTM) and CIFAR-10 (using a ResNet-18), in time only 2-3x greater than vanilla training. : A novel banditbased approach to hyperparameter optimization.",
    "prev": "This computation can be done exactly if |A Ψadapt | is small, which is the case considered in this paper (as will be discussed in Section 3.4).",
    "curr": "Otherwise, an approximation to the inverse Hessian (e.g., Neumann approximation (Lorraine et al., 2020;Clarke et al., 2022)) could be used, which reduces both the memory and computational complexities to O(|A Ψ |).",
    "next": "Combining Equations (4) and (5), we have a recipe for computing the hypergradient d L V /d ψ meta exactly for a single task, as summarized in Algorithm 1."
  },
  {
    "index": 1550,
    "source_corpus_id": 220968856,
    "ref_id": "b22",
    "citation_corpus_id": 6981893,
    "start": 7168,
    "end": 7171,
    "title": "LEARNING VISUAL PREDICTIVE MODELS OF PHYSICS FOR PLAYING BILLIARDS",
    "abstract": "The ability to plan and execute goal specific actions in varied and unseen environments settings is a central requirement of intelligent agents. In this paper, we explore how an agent can be equipped with an internal model of the dynamics of the external world, and how it can use this model to plan novel actions by running multiple internal simulations (\"visual imagination\"). Our models directly process raw visual input, and use a novel object-centric prediction formulation based on visual glimpses centered on objects (fixations) to enforce translational invariance of the learned physical laws. The agent trains itself through random interaction with a collection of different environments, and the resulting model can then be used to plan goal-directed actions in novel environments that were previously never encountered by the agent. We demonstrate that our agent can accurately plan actions for playing a simulated billiards game, which requires pushing a ball into a target position or into collision with another ball. * equal contribution 1 arXiv:1511.07404v3 [cs.CV]",
    "prev": "Such models date back to \"NeuroAnimator\" [27] for simulating articulated objects.",
    "curr": "Several methods in recent years have leveraged deep networks to build data-driven models of intuitive physics [8,12,20,23,62].",
    "next": "However, these methods either require access to the underlying ground-truth state-space or do not scale to long-range due to absence of interaction reasoning."
  },
  {
    "index": 1554,
    "source_corpus_id": 53046534,
    "ref_id": "b28",
    "citation_corpus_id": 2187805,
    "start": 7943,
    "end": 7963,
    "title": "A NOTE ON THE EVALUATION OF GENERATIVE MODELS",
    "abstract": "Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria-average log-likelihood, Parzen window estimates, and visual fidelity of samples-are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.",
    "prev": "To extract probabilities from the model density, we need to integrate over some region Ω: P (Ω) = Ω p(x; θ)dx.",
    "curr": "Adding noise to the data during model optimization can mock this integration step, encouraging the density model to output something nearer to probabilities (Theis et al., 2016):\nlog p(x n + δ; θ)p(δ) dδ ≥ E δ [log p(x n + δ; θ)] ≈ log p(x n +δ; θ)\nwhereδ is a sample from p(δ).",
    "next": "The resulting objective is a lower-bound, making it a suitable optimization target."
  },
  {
    "index": 1555,
    "source_corpus_id": 260438511,
    "ref_id": "b33",
    "citation_corpus_id": 222140788,
    "start": 11041,
    "end": 11045,
    "title": "Published as a conference paper at ICLR 2021 DENOISING DIFFUSION IMPLICIT MODELS",
    "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples 10× to 50× faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error.",
    "prev": "[30] study the capability of generating unsafe images and hateful memes of various text-to-image models.The authors then propose a new classifier that outperforms existing built-in safety checkers of these models.",
    "curr": "Diffusion-based Inversion.Image manipulation with generative networks often requires inversion [31,32], the process of finding a latent representation that corresponds to a given image.For diffusion models, Dhariwal & Nichol [33] demonstrate that the DDIM [34] sampling process can be inverted in a closed-form manner, extracting a latent noise map that will produce a given real image.More recent works [35][36][37][38] try to invert a user-provided concept to a new pseudo-word in the model's vocabulary.The most relevant approach for our work is Textual Inversion [36] which learns to capture the user-provided concept by representing it through new \"words\" in the embedding space of a frozen text-to-image model without changing the model weights.In particular, the authors designate a placeholder string, c * , to represent the new concept the user wishes to learn.They replace the vector associated with the tokenized string with a learned embedding v * , in essence \"injecting\" the concept into the model vocabulary.The technique is referred to as Textual Inversion and consists of finding an approximate solution to the following optimization problem:\nv * = arg min v E z∼E(x),c * ,ϵ∼N (0,1),t ∥ϵ − ϵ θ (z t , c * , t)∥ 2 2 .",
    "next": "Preliminaries\n\nBasic setu"
  },
  {
    "index": 1558,
    "source_corpus_id": 33985488,
    "ref_id": "b42",
    "citation_corpus_id": 6628106,
    "start": 9479,
    "end": 9483,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "The policy is represented by a feedforward neural network with a single hyperbolic tangent hidden layer and a softmax output layer.",
    "curr": "Parameters are updated using Adam [43].",
    "next": "The UHPG is scaled by the inverse of the cumulative likelihood-ratio across goals in G (or a large constant if the denominator is close to zero) to avoid large discrepancies between step sizes."
  },
  {
    "index": 1560,
    "source_corpus_id": 252668297,
    "ref_id": "b22",
    "citation_corpus_id": 227209335,
    "start": 1932,
    "end": 1952,
    "title": "SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS",
    "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reversetime SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in diffusion probabilistic modeling and scorebased generative modeling, and allows for new sampling procedures. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, which enables exact likelihood computation, and improved sampling efficiency. In addition, our framework enables conditional generation with an unconditional model, as we demonstrate with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high fidelity generation of 1024ˆ1024 images for the first time from a score-based generative model.",
    "prev": "arXiv:2202.09671v3 [stat.ML] 30 Sep 2022\n\nIntroduction\n\nGenerating photo-realistic images with probabilistic models is a challenging and important task in machine learning and computer vision, with many potential applications in data augmentation, image editing, style transfer, etc.",
    "curr": "Recently, a new class of image generative models based on diffusion processes [Sohl-Dickstein et al., 2015] has achieved remarkable results on various commonly used image generation benchmarks [Song and Ermon, 2019, Ho et al., 2020, Song et al., 2021b, surpassing many existing deep generative models, such as autoregressive models [van den Oord et al., 2016], variational auto-encoders (VAEs) [Kingma and Welling, 2013, Rezende et al., 2014, van den Oord et al., 2017, Razavi et al., 2019, and generative adversarial networks (GANs) [Goodfellow et al., 2014, Radford et al., 2015, Miyato et al., 2018, Brock et al., 2019, Karras et al., 2019, 2020b.",
    "next": "This new modeling class, which includes both score-based and diffusion-based generative models, uses noise injection to gradually corrupt the data distribution into a simple noise distribution that can be easily sampled from, and then uses a denoising network to reverse the noise injection to generate photo-realistic images."
  },
  {
    "index": 1563,
    "source_corpus_id": 208637449,
    "ref_id": "b30",
    "citation_corpus_id": 3536221,
    "start": 9501,
    "end": 9523,
    "title": "MODEL-ENSEMBLE TRUST-REGION POLICY OPTI- MIZATION",
    "abstract": "Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning. However, they tend to suffer from high sample complexity which hinders their use in real-world domains. Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and, to date, it has succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and we show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning than backpropagation through time. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks 1 2 .Published as a conference paper at ICLR 2018 assumption in this approach, henceforth termed vanilla model-based RL, is that with enough data, the learned model will be accurate enough, such that a policy optimized on it will also perform well in the real environment.Although vanilla model-based RL can work well on low-dimensional tasks with relatively simple dynamics, we find that on more challenging continuous control tasks, performance was highly unstable. The reason is that the policy optimization tends to exploit regions where insufficient data is available to train the model, leading to catastrophic failures. Previous work has pointed out this issue as model biasWhile this issue can be regarded as a form of overfitting, we emphasize that standard countermeasures from the supervised learning literature, such as regularization or cross validation, are not sufficient here -supervised learning can guarantee generalization to states from the same distribution as the data, but the policy optimization stage steers the optimization exactly towards areas where data is scarce and the model is inaccurate. This problem is severely aggravated when expressive models such as deep neural networks are employed.",
    "prev": "Most famously, the Dyna algorithm (Sutton, 1990) proposes using real experience to learn a model and then using the model to train a model-free policy.",
    "curr": "A number of more recent works have explored how to incorporate this idea into deep architectures (Kalweit & Boedecker, 2017;Feinberg et al., 2018;Buckman et al., 2018;Serban et al., 2018;Kurutach et al., 2018;Kaiser et al., 2019), with an emphasis on dealing with the errors that are introduced by approximate models.",
    "next": "In these approaches, the policy or value function is typically trained using on-policy rollouts from the model without using additional planning."
  },
  {
    "index": 1564,
    "source_corpus_id": 259836930,
    "ref_id": "b14",
    "citation_corpus_id": 213085920,
    "start": 2700,
    "end": 2717,
    "title": "Published as a conference paper at ICLR 2020 STRATEGIES FOR PRE-TRAINING GRAPH NEURAL NETWORKS",
    "abstract": "Many applications of machine learning require a model to make accurate predictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that naïve strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4% absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction. * Equal contribution. Project website, data and code: Attribute Masking Supervised Attribute Prediction Structural Similarity Prediction Structure prediction Context Prediction (b) Categorization of our pre-training methods Graph space Node space Graph embeddings Node embeddings Linear classifier Figure 1: (a.i) When only node-level pre-training is used, nodes of different shapes (semantically different nodes) can be well separated, however, node embeddings are not composable, and thus resulting graph embeddings (denoted by their classes, + and −) that are created by pooling node-level embeddings are not separable. (a.ii) With graph-level pre-training only, graph embeddings are well separated, however the embeddings of individual nodes do not necessarily capture their domainspecific semantics. (a.iii) High-quality node embeddings are such that nodes of different types are well separated, while at the same time, the embedding space is also composable. This allows for accurate and robust representations of entire graphs and enables robust transfer of pre-trained models to a variety of downstream tasks. (b) Categorization of pre-training methods for GNNs. Crucially, our methods, i.e., Context Prediction, Attribute Masking, and graph-level supervised pre-training (Supervised Attribute Prediction) enable both node-level and graph-level pre-training.matter of increasing the number of labeled pre-training datasets that are from the same domain as the downstream task. Instead, it requires substantial domain expertise to carefully select examples and target labels that are correlated with the downstream task of interest. Otherwise, the transfer of knowledge from related pre-training tasks to a new downstream task can harm generalization, which is known as negative transfer(Rosenstein et al., 2005)and significantly limits the applicability and reliability of pre-trained models.Present work. Here, we focus on pre-training as an approach to transfer learning in Graph Neural Networks (GNNs)(Kipf & Welling, 2017;Hamilton et al., 2017a;Ying et al., 2018b;Xu et al., 2019;2018)for graph-level property prediction. Our work presents two key contributions.(1) We conduct the first systematic large-scale investigation of strategies for pre-training GNNs. For that, we build two large new pre-training datasets, which we share with the community: a chemistry dataset with 2 million graphs and a biology dataset with 395K graphs. We also show that large domain-specific datasets are crucial to investigate pre-training and that existing downstream benchmark datasets are too small to evaluate models in a statistically reliable way.(2) We develop an effective pretraining strategy for GNNs and demonstrate its effectiveness and its ability for out-of-distribution generalization on hard transfer-learning problems.",
    "prev": "gns and integrates visually dissimilar 2D and 3D modalities of the same molecule at fine-grained atomic level, painting a more comprehensive depiction of each molecule.Extensive experiments show that MOLEBLEND achieves state-of-the-art performance across major 2D/3D molecular benchmarks.We further provide theoretical insights from the perspective of mutual-information maximization, demonstrating that our method unifies contrastive, generative (cross-modality prediction) and mask-then-predict (single-modality prediction) objectives into one single cohesive framework.",
    "curr": "INTRODUCTION\n\nSelf-supervised learning has been successfully applied to molecular representation learning (Xia et al., 2023;Chithrananda et al., 2020), where meaningful representations are extracted from a large amount of unlabeled molecules.The learned representation can then be finetuned to support diverse downstream molecular tasks.Early works design learning objectives based on a single modality (2D topological graphs (Hu et al., 2020;Rong et al., 2020;You et al., 2020), or 3D spatial structures (Zaidi et al., 2022;Liu et al., 2022a;Zhou et al., 2023)).Recently, multimodal molecular pretraining that exploits both 2D and 3D modalities in a single framework (Liu et al., 2022b;Stärk et al., 2022;Liu et al., 2023;Luo et al., 2022;Zhu et al., 2022) has emerged as an alternative solution.",
    "next": "Multimodal pretraining aims to align representations from different modalities.Most existing methods naturally adopt two models (Figure 1(a)) to encode 2D and 3D information separately (Liu et al., 2022b;Stärk et al., 2022;Liu et al., 2023).Contrastive learning is typically employed to attract representations of 2D graphs with their corresponding 3D conformations of the same molecule, and repulse those from different molecules.Another school of study is generative methods that bridge 2D and 3D modalities via mutual prediction (Figure 1(a-b)), such as taking 2D graphs as input to predict 3D information, and vice versa (Liu et al., 2022b;"
  },
  {
    "index": 1565,
    "source_corpus_id": 209475155,
    "ref_id": "b16",
    "citation_corpus_id": 6628106,
    "start": 20153,
    "end": 20172,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "the state dynamics were learned using predicted control trajectories only, and the state transitions are not made available to the learner (this is the same approach used in Amos et al., 2018).",
    "curr": "The experiments were implemented in Pytorch 1.2.0 using the built-in Adam optimizer (Kingma & Ba, 2014) for 1000 steps using default parameters.",
    "next": "The MPC optimization problems were solved for the 'expert' and 'learner' using OSQP (Stellato et al., 2017) with settings (eps_abs=1E-10, eps_rel=1E-10, eps_rim_inf=1E-10, eps_dual_inf=1E-10)."
  },
  {
    "index": 1572,
    "source_corpus_id": 53116049,
    "ref_id": "b23",
    "citation_corpus_id": 3144218,
    "start": 3786,
    "end": 3808,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "The prior knowledge about the semantic/functional structure of the world helps to improve the navigation.",
    "curr": "We propose to use Graph Convolutional Networks (GCNs) (Kipf & Welling, 2017) to incorporate the prior knowledge into a Deep Reinforcement Learning framework.",
    "next": "The knowledge of the agent is encoded in a graph."
  },
  {
    "index": 1574,
    "source_corpus_id": 209439835,
    "ref_id": "b10",
    "citation_corpus_id": 3144218,
    "start": 3501,
    "end": 3523,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "These architectures effectively combine node features and graph topology to build distributed node representations.",
    "curr": "GNNs can be used to solve node classification (Kipf & Welling, 2017) and link prediction  tasks, or they can be applied to downstream graph classification (Bacciu et al., 2018).",
    "next": "In literature, such models are usually evaluated on chemical and social domains (Xu et al., 2019)."
  },
  {
    "index": 1575,
    "source_corpus_id": 252846328,
    "ref_id": "b28",
    "citation_corpus_id": 204960716,
    "start": 11385,
    "end": 11404,
    "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    "abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance. 1",
    "prev": "In a parallel development, the area of natural language processing was dominated by recurrent neural networks (RNNs) before self-attention-based neural networks (Vaswani et al., 2017) revolutionized this field.",
    "curr": "Based on their technique, various auto-encoding models (Devlin et al., 2018;Sanh et al., 2019;Lan et al., 2019) for tasks like sentence classification, sequence-to-sequence models Lewis et al., 2020) for translation or summarizing, and auto-regressive models (Radford et al., 2019;Brown et al., 2020) for natural language generation tasks showed the strength of the self-attention mechanism.",
    "next": "In the light of these successes, transformer-based models have also been devised for tabular data classification (Arik & Pfister, 2019;Somepalli et al., 2021;Kossen et al., 2021) and learning joint representations of tabular and textual data (Yin et al., 2020)."
  },
  {
    "index": 1577,
    "source_corpus_id": 222130583,
    "ref_id": "b12",
    "citation_corpus_id": 3144218,
    "start": 3718,
    "end": 3739,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "They are constructed by stacking (graph) neural network layers which essentially propagate and transform node features over the given graph topology.",
    "curr": "Different types of layers have been proposed and used in practice, including graph convolutional layers (GCN) (Bruna et al., 2014;Kipf & Welling, 2017), graph attention layers (GAT) (Velickovic et al., 2018) and many others (Hamilton et al., 2017;Wijesinghe & Wang, 2019;Zeng et al., 2020;Abu-El-Haija et al., 2019).",
    "next": "However, most of the existing GNN architectures have two fundamental weaknesses which restrict their learning ability on general graph-structured data."
  },
  {
    "index": 1581,
    "source_corpus_id": 210848920,
    "ref_id": "b17",
    "citation_corpus_id": 196198494,
    "start": 2722,
    "end": 2739,
    "title": "SUMBT: Slot-Utterance Matching for Universal and Scalable Belief Tracking",
    "abstract": "In goal-oriented dialog systems, belief trackers estimate the probability distribution of slotvalues at every dialog turn. Previous neural approaches have modeled domain-and slot-dependent belief trackers, and have difficulty in adding new slot-values, resulting in lack of flexibility of domain ontology configurations. In this paper, we propose a new approach to universal and scalable belief tracker, called slot-utterance matching belief tracker (SUMBT). The model learns the relations between domain-slot-types and slotvalues appearing in utterances through attention mechanisms based on contextual semantic vectors. Furthermore, the model predicts slot-value labels in a non-parametric way. From our experiments on two dialog corpora, WOZ 2.0 and MultiWOZ, the proposed model showed performance improvement in comparison with slot-dependent methods and achieved the state-of-the-art joint accuracy.",
    "prev": "Existing DST models can be categorized into two types: fixed-and open-vocabulary.",
    "curr": "Fixed vocabulary models assume known slot ontology and generate a score for each candidate of (slot,value) Lee et al., 2019).",
    "next": "Recent approaches propose open-vocabulary models that can generate the candidates, especially for slots such as entity names and time, from the dialogue history (Lei et al., 2018;Wu et al., 2019)."
  },
  {
    "index": 1582,
    "source_corpus_id": 254854614,
    "ref_id": "b20",
    "citation_corpus_id": 203902511,
    "start": 12260,
    "end": 12277,
    "title": "Published as a conference paper at ICLR 2020 IS A GOOD REPRESENTATION SUFFICIENT FOR SAM- PLE EFFICIENT REINFORCEMENT LEARNING?",
    "abstract": "Modern deep learning methods provide effective means to learn good representations. However, is a good representation itself sufficient for sample efficient reinforcement learning? This question has largely been studied only with respect to (worst-case) approximation error, in the more classical approximate dynamic programming literature. With regards to the statistical viewpoint, this question is largely unexplored, and the extant body of literature mainly focuses on conditions which permit sample efficient reinforcement learning with little understanding of what are necessary conditions for efficient reinforcement learning. This work shows that, from the statistical viewpoint, the situation is far subtler than suggested by the more traditional approximation viewpoint, where the requirements on the representation that suffice for sample efficient RL are even more stringent. Our main results provide sharp thresholds for reinforcement learning methods, showing that there are hard limitations on what constitutes good function approximation (in terms of the dimensionality of the representation), where we focus on natural representational conditions relevant to value-based, model-based, and policy-based learning. These lower bounds highlight that having a good (valuebased, model-based, or policy-based) representation in and of itself is insufficient for efficient reinforcement learning, unless the quality of this approximation passes certain hard thresholds. Furthermore, our lower bounds also imply exponential separations on the sample complexity between 1) value-based learning with perfect representation and value-based learning with a good-but-not-perfect representation, 2) value-based learning and policy-based learning, 3) policy-based learning and supervised learning and 4) reinforcement learning and imitation learning. bounds for design matrices with applications to combinatorial geometry and locally correctable codes. In Proceedings of the forty-third annual ACM symposium on Theory of computing, pp. 519-528. ACM, 2011.Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming, volume 5. Athena Scientific Belmont, MA, 1996.Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. arXiv preprint arXiv:1905.00360, 2019.Lijie Chen and Ruosong Wang. Classical algorithms from quantum and arthur-merlin communication protocols.ford. Provably efficient RL with rich observations via latent state decoding. arXiv preprint arXiv:1901.09018, 2019a.Simon S Du, Yuping Luo, Ruosong Wang, and Hanrui Zhang. Provably efficient Q-learning with function approximation via distribution shift error checking oracle. arXiv preprint arXiv:1906.06321, 2019b.Amir-massoud Farahmand. Regularization in reinforcement learning. 2011.Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision processes. arXiv preprint arXiv:1901.11275, 2019. Schapire. Contextual decision processes with low bellman rank are PAC-learnable. In",
    "prev": "Conversely, a sufficient condition for Assumption 1 to be satisfied with B Q = 2 is that {r h (·, ·), P h (s |·, ·)} ⊆ {f ∈ H : f H ≤ 1} for all h ∈ [H] and s ∈ S .",
    "curr": "Moreover, only assuming Q\n* h ∈ H, Q * h ≤ B Q H for all h ∈ [H]\nis not enough in order to obtain sample size guarantees which are polynomial in H and d (Du et al., 2020).",
    "next": "The main quantity that characterizes the complexity of the RKHS function class in the kernelized setting is the maximum information gain (Srinivas et al., 2010) \nΓ k (T, λ) := sup D⊆S×A,|D|≤T 1 2 ln |I + λ −1 K D,D |,(14)\nwhere K D,D denotes the Gram matrix, | · | denotes the determinant, λ > 0 is a regularization parameter, and the index k indicates the kernel."
  },
  {
    "index": 1584,
    "source_corpus_id": 67856290,
    "ref_id": "b21",
    "citation_corpus_id": 207468,
    "start": 3206,
    "end": 3223,
    "title": "A Hierarchical Neural Autoencoder for Paragraphs and Documents",
    "abstract": "Natural language generation of coherent long texts like paragraphs or longer documents is a challenging problem for recurrent networks models. In this paper, we explore an important step toward this generation task: training an LSTM (Longshort term memory) auto-encoder to preserve and reconstruct multi-sentence paragraphs. We introduce an LSTM model that hierarchically builds an embedding for a paragraph from embeddings for sentences and words, then decodes this embedding to reconstruct the original paragraph. We evaluate the reconstructed paragraph using standard metrics like ROUGE and Entity Grid, showing that neural models are able to encode texts in a way that preserve syntactic, semantic, and discourse coherence. While only a first step toward generating coherent text units from neural models, our work has the potential to significantly impact natural language generation and summarization 1 .",
    "prev": "In both cases, agent behavior is highly coordinated and non-deterministic, and the space of all multi-agent trajectories is naively exponentially large.",
    "curr": "When modeling such sequential data, it is often beneficial to design hierarchical models that can capture long-term coordination using intermediate variables or representations (Li et al., 2015;Zheng et al., 2016).",
    "next": "An attractive use-case for these intermediate variables is to capture interesting highlevel behavioral semantics in an interpretable and manipulable way."
  },
  {
    "index": 1585,
    "source_corpus_id": 247446906,
    "ref_id": "b7",
    "citation_corpus_id": 11336213,
    "start": 14780,
    "end": 14798,
    "title": "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches",
    "abstract": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder-Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.",
    "prev": "We use Equation 1 for directed correlation graphs, and Equation 2 for undirected distance graph.",
    "curr": "Next, to model the temporal dependency in EEGs, we employ Gated Recurrent Units (GRUs) (Cho et al., 2014), a variant of RNN with a gating mechanism.",
    "next": "Specifically, the matrix multiplications in GRUs are replaced with diffusion convolutions (or ChebNet spectral graph convolutions for undirected distance-based graph) (Li et al., 2018), allowing spatiotemporal modeling of EEG signals (referred to as \"DCGRU\"):\nX :,m G f θ = Φ K−1 k=0 θ k Λ k Φ X :,m = K−1 k=0 θ k L k X :,m = K−1r (t) = σ Θ r G [X (t) , H (t−1) ] + b r u (t) = σ Θ u G [X (t) , H (t−1) ] + b u(3)C (t) = tanh Θ C G [X (t) , (r (t) H (t−1) )]+b C H (t) = u (t) H (t−1) +(1−u (t) ) C (t) (4)\nHere, X (t) , H (t) denote the input and output of DCGRU at time step t respectively, σ denotes Sigmoid function, represents the Hadamard product, r (t) , u (t) , C (t) denote reset gate, update gate and candidate at time step t respectively, G denotes the diffusion convolution (or ChebNet spectral graph convolution), Θ r , b r , Θ u , b u , Θ C and b C are the weights and biases for the corresponding convolutional filters."
  },
  {
    "index": 1589,
    "source_corpus_id": 264802494,
    "ref_id": "b35",
    "citation_corpus_id": 230433941,
    "start": 9391,
    "end": 9408,
    "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
    "abstract": "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were \"virtual tokens\". We apply prefix-tuning to GPT-2 for table-totext generation and to BART for summarization. We show that by modifying only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. 2020. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. Amodei. 2020. Language models are few-shot learners.",
    "prev": "roficiency in handling sub-optimal data.The Trajectory Transformer (Janner et al., 2021) trains on sequences of discretized states, actions, and rewards, indicating a more direct solution.Our work focuses on utilizing the cross-domain knowledge, i.e., language pre-training, as privileged information to enhance DT-based methods, which thus is orthogonal to these works.",
    "curr": "Large Language Models (LLMs) have been the most pronounced application of the Transformer architecture in recent years (Radford et al., 2018;2019;Brown et al., 2020;OpenAI, 2023;Devlin et al., 2018;Touvron et al., 2023a;b).Pre-trained on massive amounts of corpus, LLMs have shown surprising few-shot and even zero-shot ability in language tasks, such as GPT series (Radford et al., 2018;2019;Brown et al., 2020;OpenAI, 2023).To personalize LLMs for different downstream user applications with computational efficiency, researchers commonly utilize parameter-efficient finetuning techniques (Hu et al., 2021;Zhang et al., 2023a;Li & Liang, 2021;Lester et al., 2021;Liu et al., 2022;Wang et al., 2023a) to finetune LLMs.In this work, we use the GPT-2 architecture (Radford et al., 2019) as the backbone due to its affordability and use LoRA (Hu et al., 2021) for downstream finetuning.",
    "next": "LMs for decision making.The great success of LMs in language tasks also motivates researchers to explore the potential of LMs for decision making problems (Ahn et al., 2022;Huang et al., 2022;Driess et al., 2023;Wu et al., 2023).One line of works (Ahn et al., 2022;Huang et al., 2022;Driess et al., 2023;Wu et al., 2023) utilizes LMs for high-level task decomposition and task planning, while their low-level execution policy is learned or designed separately.Another line of works (Li et al., 2022;Reed et al., 2022;Lin et al., 2023;Brohan et al., 2023;Tang et al., 2023;Wang et al., 2023b) exploits the representation and generalization power of pre-trained LMs.Li et al."
  },
  {
    "index": 1592,
    "source_corpus_id": 62823235,
    "ref_id": "b20",
    "citation_corpus_id": 6628106,
    "start": 8616,
    "end": 8620,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "However, none of these studies provide results for algorithms that return an averaged solution.",
    "curr": "Although adaptive variants of SGD, e.g., ADAGRAD [12], ADAM [21,28], were widely used for training deep neural networks, there are few studies on theoretical analysis of these algorithms for non-convex problems.",
    "next": "Several recent studies attempted to analyze ADAGRAD for non-convex problems [32,24,4,39]."
  },
  {
    "index": 1593,
    "source_corpus_id": 235293845,
    "ref_id": "b29",
    "citation_corpus_id": 2955580,
    "start": 2338,
    "end": 2354,
    "title": "A Persona-Based Neural Conversation Model",
    "abstract": "We present persona-based models for handling the issue of speaker consistency in neural response generation. A speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style. A dyadic speakeraddressee model captures properties of interactions between two interlocutors. Our models yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models, with similar gains in speaker consistency as measured by human judges.",
    "prev": "1\n\nINTRODUCTION\n\nNegotiation is ubiquitous in human interaction, from e-commerce to the multi-billion dollar sales of companies.",
    "curr": "Learning how to negotiate effectively involves deep pragmatic understanding and planning the dialogue strategically (Thompson; Bazerman et al., 2000b;Pruitt, 2013  Modern dialogue systems for collaborative tasks such as restaurant or flight reservations have made considerable progress by modeling the dialogue history and structure explicitly using the semantic content, like slot-value pairs (Larionov et al., 2018;Young, 2006), or implicitly with encoder-decoder architectures (Sordoni et al., 2015;Li et al., 2016).",
    "next": "In such tasks, users communicate explicit intentions, enabling systems to map the utterances into specific intent slots (Li et al., 2020)."
  },
  {
    "index": 1594,
    "source_corpus_id": 248964978,
    "ref_id": "b21",
    "citation_corpus_id": 990233,
    "start": 12649,
    "end": 12670,
    "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    "abstract": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.",
    "prev": "It is worth noting that multiple repetitions bring little additional time consumption compared to the inference of GPT, we thus simply set it to 100 for all tasks.",
    "curr": "Evaluation Protocol\n\nWe evaluate the proposed method on nine widely-used text-classification datasets including SST-2 (Socher et al., 2013), SST-5 (Socher et al., 2013), Subj (Pang and Lee, 2004), MR (Pang and Lee, 2005), AP (Zhang et al., 2015), DBPedia (Zhang et al., 2015), AGNews (Zhang et al., 2015), RTE (Dagan et al., 2005), and TREC (Voorhees and Tice, 2000).",
    "next": "SST-2, SST-5, MR and AP are sentiment classification tasks."
  },
  {
    "index": 1595,
    "source_corpus_id": 227338206,
    "ref_id": "b7",
    "citation_corpus_id": 208857409,
    "start": 2439,
    "end": 2462,
    "title": "YOUR CLASSIFIER IS SECRETLY AN ENERGY BASED MODEL AND YOU SHOULD TREAT IT LIKE ONE",
    "abstract": "We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model.",
    "prev": "Particularly, they have been extensively used in the context of restricted Boltzmann machines (Smolensky, 1986;Hinton, 2002), deep belief networks , Markov random fields (Carreira-Perpinan & Hinton, 2005;Hinton & Salakhutdinov, 2006), and recently also with deep neural networks (?",
    "curr": "Song & Ermon, 2019;Du & Mordatch, 2019;Grathwohl et al., 2019;Nijkamp et al., 2019).",
    "next": "Fitting an unnormalized density model to a dataset is challenging due to the missing normalization constant of the distribution."
  },
  {
    "index": 1596,
    "source_corpus_id": 202750112,
    "ref_id": "b6",
    "citation_corpus_id": 2768038,
    "start": 3027,
    "end": 3029,
    "title": "Under review as conference paper at ICLR 2015 EMBEDDING ENTITIES AND RELATIONS FOR LEARN- ING AND INFERENCE IN KNOWLEDGE BASES",
    "abstract": "We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013)  and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2% vs. 54.7% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as BornInCitypa, bq^CityInCountrypb, cq ùñ N ationalitypa, cq. We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics, and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-ofthe-art confidence-based rule mining approach in mining horn rules that involve compositional reasoning.",
    "prev": "Here, we choose knowledge graph-related tasks to explore due to its representativeness.",
    "curr": "In knowledge base completion (KBC) tasks, embedding-based models [6,7,8,9,10,11] can easily obtain a very competitive score by fitting data using various neural network techniques, but lacking an explicit modeling to construct explanations by directly exploiting graph structure prevents it from being interpretable, a critical property of reasoning, since Euclidean embedding space will not produce a clearly stated and human-readable representation.",
    "next": "One is for the AthletePlaysForTeam task and the other for the OrganizationHiredPerson task."
  },
  {
    "index": 1598,
    "source_corpus_id": 235614202,
    "ref_id": "b31",
    "citation_corpus_id": 6628106,
    "start": 36409,
    "end": 36428,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "For training, we carry out the random cropping of the given patches to the size of 64 × 64, and the data augmentation was done by flipping the cropped patches horizontally and vertically.",
    "curr": "For optimization, we used Adam (Kingma & Ba, 2015) optimizer for the three generators and RMSProp (Tieleman & Hinton, 2012) optimizer for the two critics.",
    "next": "The initial learning rates were set to 0.0004 and 0.0005 for Adam and RMSProp, respectively."
  },
  {
    "index": 1600,
    "source_corpus_id": 251279905,
    "ref_id": "b39",
    "citation_corpus_id": 49666783,
    "start": 3421,
    "end": 3438,
    "title": "ALGORITHMIC FRAMEWORK FOR MODEL-BASED DEEP REINFORCEMENT LEARNING WITH THEORETI- CAL GUARANTEES",
    "abstract": "Model-based reinforcement learning (RL) is considered to be a promising approach to reduce the sample complexity that hinders model-free RL. However, the theoretical understanding of such methods has been rather limited. This paper introduces a novel algorithmic framework for designing and analyzing model-based RL algorithms with theoretical guarantees. We design a meta-algorithm with a theoretical guarantee of monotone improvement to a local maximum of the expected reward. The meta-algorithm iteratively builds a lower bound of the expected reward based on the estimated dynamical model and sample trajectories, and then maximizes the lower bound jointly over the policy and the model. The framework extends the optimism-in-face-of-uncertainty principle to non-linear dynamical models in a way that requires no explicit uncertainty quantification. Instantiating our framework with simplification gives a variant of model-based RL algorithms Stochastic Lower Bounds Optimization (SLBO). Experiments demonstrate that SLBO achieves stateof-the-art performance when only one million or fewer samples are permitted on a range of continuous control benchmark tasks. 1",
    "prev": "Most of these remarkable advances rely on scalable and often decentralized algorithms for computing Nash equilibria (Nash, 1951)-a standard game-theoretic notion of rationality-in two-player zero-sum games.",
    "curr": "Nevertheless, while single-agent RL has enjoyed rapid theoretical progress over the last few years (e.g., see (Jin et al., 2018;Agarwal et al., 2020;Li et al., 2021;Luo et al., 2019;Sidford et al., 2018), and references therein), a comprehensive understanding of the multi-agent landscape still remains elusive.",
    "next": "Indeed, provable guarantees for efficiently computing Nash equilibria have been thus far limited to either fully competitive settings, such as two-player zero-sum games (Daskalakis et al., 2020;Wei et al., 2021;Sayin et al., 2021;Cen et al., 2021;Sayin et al., 2020;Condon, 1993), or environments in which agents are endeavoring to coordinate towards a common global objective (Claus and Boutilier, 1998;Wang and Sandholm, 2002;Leonardos et al., 2021;Ding et al., 2022;Zhang et al., 2021b;Chen et al., 2022;Maheshwari et al., 2022;Fox et al., 2022)."
  },
  {
    "index": 1603,
    "source_corpus_id": 256194369,
    "ref_id": "b8",
    "citation_corpus_id": 52967399,
    "start": 3614,
    "end": 3635,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Therefore, an ideal model editor should provide continuous and promptly fixing of newly emerged mistakes in an effective and efficient manner.",
    "curr": "INTRODUCTION\n\nTransformer-based models, particularly large Pretrained Language Models (PLMs) (Devlin et al., 2019;Brown et al., 2020) have become the backbone model of modern Natural Language Processing (NLP) and have enabled promising results in various downstream tasks (Lv et al., 2019;Budzianowski & Vulic, 2019;Ramnath et al., 2020).",
    "next": "However, PLMs still produce undesirable outputs occasionally (Zhao et al., 2019;Basta et al., 2021)."
  },
  {
    "index": 1605,
    "source_corpus_id": 13253311,
    "ref_id": "b9",
    "citation_corpus_id": 6628106,
    "start": 9927,
    "end": 9948,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": ", b (L) }, which allows us to train it in an end-to-end fashion using back-propagation.",
    "curr": "We adopt a Lasso-like objective [Tibshirani, 1996],\nL(D) = 1 N |D| i=1 ψ(x i ) − y i 2 + λ L l=1 W (l) 1 ,(7)\nthat is, a linear combination of L 2 loss and L 1 regularization, and apply a stochastic gradient descent algorithm with mini-batches and Adam [Kingma and Ba, 2015] for calculating the updates:\nθ t+1 = θ t + Adam ∂L(D (t) ) ∂θ , α ,(8)\nwhere D (t) denotes the current mini-batch and α is the stepsize parameter.",
    "next": "The choice of Adam is not critical and standard stochastic gradient descent also works."
  },
  {
    "index": 1607,
    "source_corpus_id": 170079235,
    "ref_id": "b33",
    "citation_corpus_id": 12713052,
    "start": 6923,
    "end": 6927,
    "title": "NEURAL ARCHITECTURE SEARCH WITH REINFORCEMENT LEARNING",
    "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214. * Work done as a member of the Google Brain Residency program (g.co/brainresidency.) Under review as a conference paper at ICLR 2017 neural network can be typically specified by a variable-length string. It is therefore possible to use a recurrent network -the controller -to generate such string. Training the network specified by the string -the \"child network\" -on the real data will result in an accuracy on a validation set. Using this accuracy as the reward signal, we can compute the policy gradient to update the controller. As a result, in the next iteration, the controller will give higher probabilities to architectures that receive high accuracies. In other words, the controller will learn to improve its search over time.Our experiments show that Neural Architecture Search can design good models from scratch, an achievement considered not possible with other methods. On image recognition with CIFAR-10, Neural Architecture Search can find a novel ConvNet model that is better than most human-invented architectures. Our CIFAR-10 model achieves a 3.65 test set error, while being 1.05x faster than the current best model. On language modeling with Penn Treebank, Neural Architecture Search can design a novel recurrent cell that is also better than previous RNN and LSTM architectures. The cell that our model found achieves a test set perplexity of 62.4 on the Penn Treebank dataset, which is 3.6 perplexity better than the previous state-of-the-art.",
    "prev": "However, these video CNN architectures were the results of careful manual designs by human experts.",
    "curr": "Neural Architecture Search (NAS), the concept of automatically finding better CNN architectures based on data, is becoming increasingly popular [34,35,14].",
    "next": "Rather than relying on human expert knowledge to design a CNN model, neural architecture search allows the machines to generate better performing CNN models optimized for the data."
  },
  {
    "index": 1610,
    "source_corpus_id": 244129839,
    "ref_id": "b16",
    "citation_corpus_id": 3544583,
    "start": 32053,
    "end": 32075,
    "title": "SCALABLE PRIVATE LEARNING WITH PATE",
    "abstract": "The rapid adoption of machine learning has increased concerns about the privacy implications of machine learning models trained on sensitive data, such as medical records or other personal information. To address those concerns, one promising approach is Private Aggregation of Teacher Ensembles, or PATE, which transfers to a \"student\" model the knowledge of an ensemble of \"teacher\" models, with intuitive privacy provided by training teachers on disjoint data and strong privacy guaranteed by noisy aggregation of teachers' answers. However, PATE has so far been evaluated only on simple classification tasks like MNIST, leaving unclear its utility when applied to larger-scale learning tasks and real-world datasets. In this work, we show how PATE can scale to learning tasks with large numbers of output classes and uncurated, imbalanced training data with errors. For this, we introduce new noisy aggregation mechanisms for teacher ensembles that are more selective and add less noise, and prove their tighter differential-privacy guarantees. Our new mechanisms build on two insights: the chance of teacher consensus is increased by using more concentrated noise and, lacking consensus, no answer need be given to a student. The consensus answers used are more likely to be correct, offer better intuitive privacy, and incur lower-differential privacy cost. Our evaluation shows our mechanisms improve on the original PATE on all measures, and scale to larger tasks with both high utility and very strong privacy (ε < 1.0).",
    "prev": "The problem of model sensitivity, particularly to variability in the training set, can lead to an increase generalization error (Elisseeff et al., 2003) as well as to leaking training set information (Dwork, 2006;Yeom et al., 2018).",
    "curr": "Thus, stability-enhancing learning rules have received significant attention in order to bolster desirable properties, such as privacy (Liu et al., 2020;Papernot et al., 2018;Wang et al., 2016).",
    "next": "One such approach is model ensembling, which has been used as a variance reduction method since the advent of statistical learning (Zhou et al., 2002;Valentini et al., 2004;Opitz and Maclin, 1999;Tumer and Ghosh, 1996;Dvornik et al., 2019;Hasan et al., 2020;Freund and Schapire, 1997;Sagi and Rokach, 2018;Polikar, 2012;Che et al., 2011;Perrone and Cooper, 1992;Hansen and Salamon, 1990)."
  },
  {
    "index": 1611,
    "source_corpus_id": 253581623,
    "ref_id": "b22",
    "citation_corpus_id": 247363675,
    "start": 3670,
    "end": 3690,
    "title": "SO(2)-EQUIVARIANT REINFORCEMENT LEARNING",
    "abstract": "Equivariant neural networks enforce symmetry within the structure of their convolutional layers, resulting in a substantial improvement in sample efficiency when learning an equivariant or invariant function. Such models are applicable to robotic manipulation learning which can often be formulated as a rotationally symmetric problem. This paper studies equivariant model architectures in the context of Q-learning and actor-critic reinforcement learning. We identify equivariant and invariant characteristics of the optimal Q-function and the optimal policy and propose equivariant DQN and SAC algorithms that leverage this structure. We present experiments that demonstrate that our equivariant versions of DQN and SAC can be significantly more sample efficient than competing algorithms on an important class of robotic manipulation problems.",
    "prev": "Supplementary video and code are available at https://pointw.github.io/extrinsic_page/.",
    "curr": "INTRODUCTION\n\nRecently, equivariant learning has shown great success in various machine learning domains like trajectory prediction , robotics (Simeonov et al., 2022), and reinforcement learning (Wang et al., 2022c).",
    "next": "Equivariant networks (Cohen & Welling, 2016; can improve generalization and sample efficiency during learning by encoding task symmetries directly into the model structure."
  },
  {
    "index": 1613,
    "source_corpus_id": 235358966,
    "ref_id": "b8",
    "citation_corpus_id": 211252411,
    "start": 20621,
    "end": 20638,
    "title": "Few-Shot Learning via Learning the Representation, Provably",
    "abstract": "This paper studies few-shot learning via representation learning, where one uses T source tasks with n 1 data per task to learn a representation in order to reduce the sample complexity of a target task for which there is only n 2 (≪ n 1 ) data. Specifically, we focus on the setting where there exists a good common representation between source and target, and our goal is to understand how much of a sample size reduction is possible. First, we study the setting where this common representation is low-dimensional and provide a fast rate of O C(Φ) n1T + k n2 ; here, Φ is the representation function class, C (Φ) is its complexity measure, and k is the dimension of the representation. When specialized to linear representation functions, this rate becomes O dk n1T + k n2 where d(≫ k) is the ambient input dimension, which is a substantial improvement over the rate without using representation learning, i.e. over the rate of O d n2 . Second, we consider the setting where the common representation may be high-dimensional but is capacity-constrained (say in norm); here, we again demonstrate the advantage of representation learning in both high-dimensional linear regression and neural network learning. Our results demonstrate representation learning can fully utilize all n 1 T samples from source tasks. * Institute for Advanced Study.",
    "prev": "Therefore, MLTI reduces the generalization error, which we also empirically validate in the experiments.",
    "curr": "METRIC-BASED META-LEARNING WITH MLTI\n\nIn the metric-based meta-learning, we consider the ProtoNet with linear representation in the binary classification, which has been commonly considered in other theoretical analysis of meta-learning, see, e.g., (Du et al., 2020;Tripuraneni et al., 2020).",
    "next": "Specifically, we assume f P N θ (x) = θ x and d(·, ·) represents the squared Euclidean distance, then the loss of ProtoNet can be simplified as\narg min θ |I| i=1 N k=1 log p(y i,k = r|x i,k ) = arg min θ |I| i=1 N k=1 1 1 + exp( (x i,k − (c1 + c2)/2, θ ) ,(9)\nwhere c1 and c2 are defined as the prototypes of class 1 and 2, respectively."
  },
  {
    "index": 1615,
    "source_corpus_id": 259841016,
    "ref_id": "b3",
    "citation_corpus_id": 8768364,
    "start": 3286,
    "end": 3304,
    "title": "Density estimation using Real NVP",
    "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.",
    "prev": "For example, the conjugacy condition has been removed by the black-box VI methods which allow a broad class of models via Monte carlo gradient estimators (Nott et al., 2012;Paisley et al., 2012;Ranganath et al., 2014;Rezende et al., 2014;Kingma & Welling, 2014).",
    "curr": "On the other hand, more flexible variational families have been proposed that either explicitly incorporate more complicated structures among the parameters (Jaakkola & Jordan, 1998;Saul & Jordan, 1996;Giordano et al., 2015;Tran et al., 2015) or borrow ideas from invertible transformation of probability distributions (Rezende & Mohamed, 2015;Dinh et al., 2017;Kingma et al., 2016;Papamakarios et al., 2019).",
    "next": "All these methods require tractable densities for the variational distributions."
  },
  {
    "index": 1617,
    "source_corpus_id": 247778706,
    "ref_id": "b58",
    "citation_corpus_id": 12713052,
    "start": 2622,
    "end": 2638,
    "title": "NEURAL ARCHITECTURE SEARCH WITH REINFORCEMENT LEARNING",
    "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214. * Work done as a member of the Google Brain Residency program (g.co/brainresidency.) Under review as a conference paper at ICLR 2017 neural network can be typically specified by a variable-length string. It is therefore possible to use a recurrent network -the controller -to generate such string. Training the network specified by the string -the \"child network\" -on the real data will result in an accuracy on a validation set. Using this accuracy as the reward signal, we can compute the policy gradient to update the controller. As a result, in the next iteration, the controller will give higher probabilities to architectures that receive high accuracies. In other words, the controller will learn to improve its search over time.Our experiments show that Neural Architecture Search can design good models from scratch, an achievement considered not possible with other methods. On image recognition with CIFAR-10, Neural Architecture Search can find a novel ConvNet model that is better than most human-invented architectures. Our CIFAR-10 model achieves a 3.65 test set error, while being 1.05x faster than the current best model. On language modeling with Penn Treebank, Neural Architecture Search can design a novel recurrent cell that is also better than previous RNN and LSTM architectures. The cell that our model found achieves a test set perplexity of 62.4 on the Penn Treebank dataset, which is 3.6 perplexity better than the previous state-of-the-art.",
    "prev": "Our code is available at https://github.com/skhu101/GM-NAS.",
    "curr": "INTRODUCTION\n\nIn recent years, there has been a surge of interest in Neural Architecture Search (NAS) (Stanley & Miikkulainen, 2002;Zoph & Le, 2017;Pham et al., 2018;Real et al., 2019; for its ability to identify high-performing architectures in a series of machine learning tasks.",
    "next": "Pioneering works in this field require training and evaluating thousands of architectures from scratch, which consume huge amounts of computational resources (Miikkulainen et al., 2019;Zoph & Le, 2017;."
  },
  {
    "index": 1619,
    "source_corpus_id": 259088961,
    "ref_id": "b36",
    "citation_corpus_id": 57373913,
    "start": 1946,
    "end": 1968,
    "title": "OPPORTUNISTIC LEARNING: BUDGETED COST-SENSITIVE LEARNING FROM DATA STREAMS",
    "abstract": "In many real-world learning scenarios, features are only acquirable at a cost constrained under a budget. In this paper, we propose a novel approach for costsensitive feature acquisition at the prediction-time. The suggested method acquires features incrementally based on a context-aware feature-value function. We formulate the problem in the reinforcement learning paradigm, and introduce a reward function based on the utility of each feature. Specifically, MC dropout sampling is used to measure expected variations of the model uncertainty which is used as a feature-value function. Furthermore, we suggest sharing representations between the class predictor and value function estimator networks. The suggested approach is completely online and is readily applicable to stream learning setups. The solution is evaluated on three different datasets including the well-known MNIST dataset as a benchmark as well as two cost-sensitive datasets: Yahoo Learning to Rank and a dataset in the medical domain for diabetes classification. According to the results, the proposed method is able to efficiently acquire features and make accurate predictions.A version of the source code and the health dataset preproccessing code for this paper is available at: https",
    "prev": "* Equal contribution.",
    "curr": "1 Prior works have also referred to the problem as as sequential information maximization (\n\nINTRODUCTION\n\nMany machine learning applications rely on high-dimensional datasets with significant data acquisition costs.For example, medical diagnosis can depend on a range of demographic features, lab tests and physical examinations, and collecting each piece of information takes time and money (Kachuee et al., 2018;Erion et al., 2021;He et al., 2022).To improve interpretability and reduce data acquisition costs, a natural approach is to adaptively query features given the current information, so that each prediction relies on only a small number of features.This approach is referred to as dynamic feature selection (DFS), 1 and it is a promising paradigm considered by several works in recent years (Kachuee et al., 2018;Janisch et al., 2019;Chattopadhyay et al., 2022;2023;Covert et al., 2023).",
    "next": "Among the existing methods that address this problem, two main approaches have emerged.One idea is to formulate DFS as a Markov decision process (MDP) and use reinforcement learning (RL) (Dulac-Arnold et al., 2011;Mnih et al., 2014;Kachuee et al., 2018;Janisch et al., 2019).This approach has the capacity to discover the optimal policy, but it faces training difficulties that are common in RL (Henderson et al., 2018).Alternatively, another line of work focuses on greedy approaches, where features are selected"
  },
  {
    "index": 1620,
    "source_corpus_id": 220404588,
    "ref_id": "b27",
    "citation_corpus_id": 8696462,
    "start": 5866,
    "end": 5869,
    "title": "SEMI-SUPERVISED KNOWLEDGE TRANSFER FOR DEEP LEARNING FROM PRIVATE TRAINING DATA",
    "abstract": "Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information.To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data: Private Aggregation of Teacher Ensembles (PATE). The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as \"teachers\" for a \"student\" model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student's privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student's training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings. Compared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning.Recent attacks exploiting this implicit memorization in machine learning have demonstrated that private, sensitive training data can be recovered from models. Such attacks can proceed directly, by analyzing internal model parameters, but also indirectly, by repeatedly querying opaque models to gather data for the attack's analysis. For example,Fredrikson et al. (2015)used hill-climbing on the output probabilities of a computer-vision classifier to reveal individual faces from the training data.",
    "prev": "In general, this subroutines solves the following problem: given any k < p, find a linear projectionV k (t) ∈ R p×k such that the reconstruction error g t −V k (t)V k (t) g t is small.",
    "curr": "To implement this subroutine, we follow a long line of work that studies private data analysis with access to an auxiliary public dataset S h drawn from the same distribution P, for which we don't need to provide formal privacy guarantee [2,3,5,16,28].",
    "next": "In our case, we computeV k (t) which is given the top k eigenspaces of the gradients evaluated on S h ."
  },
  {
    "index": 1623,
    "source_corpus_id": 264490556,
    "ref_id": "b20",
    "citation_corpus_id": 1428702,
    "start": 4397,
    "end": 4416,
    "title": "Learning Word Vectors for Sentiment Analysis",
    "abstract": "Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term-document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.",
    "prev": "We empirically evaluate our DP framework on commonly studied tasks (in non-privacy literature).Following the influential work of Ziegler et al.",
    "curr": "( 2020), we evaluate two main scenarios: (i) alignment via RL without human in the loop for a positive review generation task on the IMDb dataset (Maas et al., 2011), and (ii) alignment via RL from human feedback (RLHF) for a summarization task on the Reddit TL;DR dataset (Völske et al., 2017).Our experimental results indicate that privately aligning LLMs is possible, offering competitive utility while ensuring strong privacy protections.As a representative example, on the IMDb dataset, the average reward obtained by our DP GPT2-Large model for generating positive reviews is 3.37 with ϵ = 4, whereas the best performing non-private model achieves an average reward of 3.47.",
    "next": "Our experiments also show that increasing the model size typically leads to more favorable privacyreward trade-offs, hence, we anticipate that as pre-trained LLMs get better, alignment with DP should become easier."
  },
  {
    "index": 1626,
    "source_corpus_id": 3495200,
    "ref_id": "b19",
    "citation_corpus_id": 1957433,
    "start": 4669,
    "end": 4694,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "Additionally, for VARMISUSE, learning variable usage semantics (e.g., \"a filename is needed here\") is required.",
    "curr": "This \"fill the blank element\" task is related to methods for learning distributed representations of natural language words, such as Word2Vec (Mikolov et al., 2013) and GLoVe (Pennington et al., 2014).",
    "next": "However, we can learn from a much richer structure such as data flow information."
  },
  {
    "index": 1627,
    "source_corpus_id": 498451,
    "ref_id": "b13",
    "citation_corpus_id": 6628106,
    "start": 2447,
    "end": 2465,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "Standard line-search algorithms cannot be used on the primal objective function in this setting, as (i) there may not exist a step-size guaranteeing a monotonic decrease because of the use of sub-gradients, and (ii) even in the smooth case, each function evaluation requires a forward pass over the entire data set without any update, making the approach computationally unfeasible.",
    "curr": "Choosing the learning rate thus remains an open issue, with the state-of-the-art algorithms suggesting adaptive learning rates (Duchi et al., 2011;Zeiler, 2012;Kingma & Ba, 2015).",
    "next": "In addition, techniques such as batch normalization (Ioffe & Szegedy, 2015) and dropout (Srivastava et al., 2014) have been introduced to respectively reduce the sensitivity to the learning rate and to prevent from overfitting."
  },
  {
    "index": 1628,
    "source_corpus_id": 209318411,
    "ref_id": "b6",
    "citation_corpus_id": 11383178,
    "start": 5362,
    "end": 5382,
    "title": "IMPORTANCE WEIGHTED AUTOENCODERS",
    "abstract": "The variational autoencoder (VAE; ) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.",
    "prev": "However, the log transformation introduces some challenges: Monte Carlo estimation techniques such as importance sampling do not straightforwardly give unbiased estimates of this quantity.",
    "curr": "Nevertheless, there has been significant work to construct estimators of the log marginal likelihood in which it is possible to explicitly trade off between bias against computational cost (Burda et al., 2016;Bamler et al., 2017;Nowozin, 2018).",
    "next": "Unfortunately, while there are asymptotic regimes where the bias of these estimators approaches zero, it is always possible to optimize the parameters to increase this bias to infinity."
  },
  {
    "index": 1629,
    "source_corpus_id": 248085789,
    "ref_id": "b20",
    "citation_corpus_id": 14337532,
    "start": 31640,
    "end": 31667,
    "title": "SGDR: STOCHASTIC GRADIENT DESCENT WITH WARM RESTARTS",
    "abstract": "Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradientbased optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at",
    "prev": "Training We use Stochastic Gradient Descent (SGD) as the optimizer in training.",
    "curr": "We use η = 0.032 as the initial learning rate and decay the learning with a cosine annealing (Loshchilov & Hutter, 2017), which is also used in SimSiam.",
    "next": "By default, we train R = 100 rounds with local  Cross-silo FL vs Cross-device FL This paper primarily focuses on cross-silo FL where clients are stateful with high availability."
  },
  {
    "index": 1631,
    "source_corpus_id": 52895739,
    "ref_id": "b25",
    "citation_corpus_id": 8257350,
    "start": 42620,
    "end": 42624,
    "title": "ALTERNATING MULTI-BIT QUANTIZATION FOR RECURRENT NEURAL NETWORKS",
    "abstract": "Recurrent neural networks have achieved excellent performance in many applications. However, on portable devices with limited resources, the models are often too large to deploy. For applications on the server with large scale concurrent requests, the latency during inference can also be very critical for costly computing resources. In this work, we address these problems by quantizing the network, both weights and activations, into multiple binary codes {−1, +1}. We formulate the quantization as an optimization problem. Under the key observation that once the quantization coefficients are fixed the binary codes can be derived efficiently by binary search tree, alternating minimization is then applied. We test the quantization for two well-known RNNs, i.e., long short term memory (LSTM) and gated recurrent unit (GRU), on the language models. Compared with the full-precision counter part, by 2-bit quantization we can achieve ∼16× memory saving and ∼6× real inference acceleration on CPUs, with only a reasonable loss in the accuracy. By 3-bit quantization, we can achieve almost no loss in the accuracy or even surpass the original model, with ∼10.5× memory saving and ∼3× real inference acceleration. Both results beat the exiting quantization works with large margins. We extend our alternating quantization to image classification tasks. In both RNNs and feedforward neural networks, the method also achieves excellent performance. rank matrix factorization for deep neural network training with high-dimensional output targets. In ICASSP, pp. 6655-6659. IEEE, 2013.Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.",
    "prev": "Training and inference on quantized nets can be made more efficient by also quantizing the activation [12,18,23], and such networks have achieved impressive performance on large-scale tasks such as ImageNet classification [18,24].",
    "curr": "In the NLP land, quantized language models have been successfully trained using alternating multi-bit quantization [22].",
    "next": "Theories Li et al."
  },
  {
    "index": 1632,
    "source_corpus_id": 56177829,
    "ref_id": "b15",
    "citation_corpus_id": 5176587,
    "start": 2607,
    "end": 2630,
    "title": "Published as a conference paper at ICLR 2018 NOISY NETWORKS FOR EXPLORATION",
    "abstract": "We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent's policy can be used to aid efficient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights. NoisyNet is straightforward to implement and adds little computational overhead. We find that replacing the conventional exploration heuristics for A3C, DQN and Dueling agents (entropy reward and -greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance. * Equal contribution. , et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015. . Deep exploration via randomized value functions. arXiv preprint arXiv:1703.07608, 2017.Georg Ostrovski, Marc G Bellemare, Aaron van den Oord, and Remi Munos. Count-based exploration with neural density models. arXiv preprint arXiv:1703.01310, 2017.Pierre-Yves Oudeyer and Frederic Kaplan. What is intrinsic motivation? A typology of computational approaches. Frontiers in neurorobotics, 1, 2007.",
    "prev": "Examples include upper confidence bound (UCB) (Auer et al., 2002) and Thompson sampling (TS) (Thompson, 1933).",
    "curr": "In recent years, these have been extended to practical exploration algorithms for large state-spaces and shown to improve performance (Osband et al., 2016a;O'Donoghue et al., 2018;Fortunato et al., 2018).",
    "next": "However, these methods assume that the observation noise distribution is independent of the evaluation point, while in practice heteroscedastic observation noise is omnipresent in RL."
  },
  {
    "index": 1633,
    "source_corpus_id": 246634839,
    "ref_id": "b21",
    "citation_corpus_id": 53216818,
    "start": 17956,
    "end": 17976,
    "title": "Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control",
    "abstract": "We propose a \"plan online and learn offline\" framework for the setting where an agent, with an internal model, needs to continually act and learn in the world. Our work builds on the synergistic relationship between local model-based control, global value function learning, and exploration. We study how local trajectory optimization can cope with approximation errors in the value function, and can stabilize and accelerate value function learning. Conversely, we also study how approximate value functions can help reduce the planning horizon and allow for better policies beyond local solutions. Finally, we also demonstrate how trajectory optimization can be used to perform temporally coordinated exploration in conjunction with estimating uncertainty in value function approximation. This exploration is critical for fast and stable learning of the value function. Combining these components enable solutions to complex control tasks, like humanoid locomotion and dexterous in-hand manipulation, in the equivalent of a few minutes of experience in the real world.",
    "prev": "EXPERIMENTS\n\nTo demonstrate the versatility of BAM, we apply it in a variety of scenarios.",
    "curr": "As BAM is a learning paradigm, it can be implemented as a module in a larger framework allowing it to be easily used in settings such as control/reinforcement learning and domain adaptation (Thompson, 1933;Osband et al., 2018;Lowrey et al., 2018;Yoon et al., 2018).",
    "next": "BAM requires the ability to construct the posterior, p(θ t |D <t , W t ), and evaluate the log marginal likelihood, log p(D t |D <t , W t )."
  },
  {
    "index": 1634,
    "source_corpus_id": 261076491,
    "ref_id": "b8",
    "citation_corpus_id": 252222320,
    "start": 8995,
    "end": 8998,
    "title": "PALI: A JOINTLY-SCALED MULTILINGUAL LANGUAGE-IMAGE MODEL",
    "abstract": "Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI (Pathways Language and Image model), a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pre-trained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pre-training tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design. . MaXM: Towards multilingual visual question answering. arXiv preprint arXiv:2209.05401, 2022b. . Microsoft COCO Captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. . Winner team Mia at TextVQA challenge 2021: Vision-and-language representation learning with pre-trained sequence-to-sequence model. arXiv preprint arXiv:2106.15332, 2021. , et al. Learning transferable visual models from natural language supervision. . Scaling vision with sparse mixture of experts. , et al. Scaling up models and data with t5x and seqio. arXiv preprint arXiv:2203.17189, 2022. Sun. Objects365: A large-scale, high-quality dataset for object detection. In . Learning robust global representations by penalizing local predictive power.",
    "prev": "UC2 [67] expands English multimodal data using machine translation and introduces specific pretraining objectives to enhance fine-grained alignment between image regions and multilingual tokens.",
    "curr": "In the era of LLMs, PaLI [9] develops a 17B multilingual language-image model based on 10B image-text pairs spanning 100 languages.",
    "next": "Differing from these studies, which try to simultaneously achieve multilingual and multimodel alignment, we focus on effectively leveraging pretrained multilingual LLMs in multimodal learning across various languages."
  },
  {
    "index": 1635,
    "source_corpus_id": 257219618,
    "ref_id": "b27",
    "citation_corpus_id": 211258652,
    "start": 2676,
    "end": 2694,
    "title": "Training Question Answering Models From Synthetic Data",
    "abstract": "Question and answer generation is a data augmentation method that aims to improve question answering (QA) models given the limited amount of human labeled data. However, a considerable gap remains between synthetic and human-generated question-answer pairs. This work aims to narrow this gap by taking advantage of large language models and explores several factors such as model size, quality of pretrained models, scale of data synthesized, and algorithmic choices. On the SQUAD1.1 question answering task, we achieve higher accuracy using solely synthetic questions and answers than when using the SQUAD1.1 training set questions alone. Removing access to real Wikipedia data, we synthesize questions and answers from a synthetic corpus generated by an 8.3 billion parameter GPT-2 model. With no access to human supervision and only access to other models, we are able to train state of the art question answering networks on entirely model-generated data that achieve 88.4 Exact Match (EM) and 93.9 F1 score on the SQUAD1.1 dev set. We further apply our methodology to SQUAD2.0 and show a 2.8 absolute gain on EM score compared to prior work using synthetic data.",
    "prev": "Training gaussian mixture models at scale via coresets.",
    "curr": "INTRODUCTION\n\nOwing to the superior generative capacity of large-scale pre-trained language models (PLMs), there has been an emerging trend of using these powerful models (e.g., GPT) to generate training data for downstream tasks (Anaby-Tavor et al., 2020;Puri et al., 2020;Kumar et al., 2020;Lee et al., 2021, inter alia).",
    "next": "Among them, a new line of generation-based zero-shot learning using the unfinetuned PLM pushes the envelope further (Schick & Schütze, 2021;Ye et al., 2022a;Meng et al., 2022), featuring total annotation-free training for downstream tasks."
  },
  {
    "index": 1636,
    "source_corpus_id": 202565422,
    "ref_id": "b21",
    "citation_corpus_id": 52077536,
    "start": 10044,
    "end": 10067,
    "title": "TIME-AGNOSTIC PREDICTION: PREDICTING PREDICTABLE VIDEO FRAMES",
    "abstract": "Prediction is arguably one of the most basic functions of an intelligent system. In general, the problem of predicting events in the future or between two waypoints is exceedingly difficult. However, most phenomena naturally pass through relatively predictable bottlenecks-while we cannot predict the precise trajectory of a robot arm between being at rest and holding an object up, we can be certain that it must have picked the object up. To exploit this, we decouple visual prediction from a rigid notion of time. While conventional approaches predict frames at regularly spaced temporal intervals, our time-agnostic predictors (TAP) are not tied to specific times so that they may instead discover predictable \"bottleneck\" frames no matter when they occur. We evaluate our approach for future and intermediate frame prediction across three robotic manipulation tasks. Our predictions are not only of higher visual quality, but also correspond to coherent semantic subgoals in temporally extended tasks.Recall the bottle-tilting uncertainty profile.Fig 1 depictsuncertainty profiles for several other prediction settings, including both forward/future prediction (given a start frame) and intermediate prediction (given start and end frames). Our time-agnostic reframing of the prediction problem targets the minima of these profiles, where prediction is intuitively easiest. We refer to these minima states as \"bottlenecks.\"At this point, one might ask: are these \"easy\" bottlenecks actually useful to predict? Intuitively, bottlenecks naturally correspond to reliable subgoals-an agent hoping to solve the maze inFig 1 (e)would do well to target its bottlenecks as subgoals. In our experiments, we evaluate the usefulness of our predictions as subgoals in simulated robotic manipulation tasks. 1Figure 1: (a) Over time as the bottle is tilted, the uncertainty first rises and then falls as the bottle is held steady after tilting. (b)-(e) Similar uncertainty profiles corresponding to various scenarios-a ball rolling down the side of a bowl, a car driving on a highway with an exit 100m away, an iron pellet tossed in the direction of a magnet, and intermediate frame prediction in a maze traversal given start and end states. The red asterisks along the x-axis correspond to the asterisks in the maze-these \"bottleneck\" states must occur in any successful traversal.Our main contributions are: (i) we reframe the video prediction problem to be time-agnostic, (ii) we propose a novel technical approach to solve this problem, (iii) we show that our approach effectively identifies \"bottleneck states\" across several tasks, and (iv) we show that these bottlenecks correspond to subgoals that aid in planning towards complex end goals.",
    "prev": "Furthermore, our hierarchical planning framework is agnostic to state space, and could directly operate in one of the above latent spaces.",
    "curr": "A number of recent works have explored reaching novel goals using only self-supervision (Finn & Levine, 2016;Eysenbach et al., 2019;Kurutach et al., 2018;Wang et al., 2019;Jayaraman et al., 2019;Nair et al., 2018a).",
    "next": "In particular, time-agnostic prediction (TAP) (Jayaraman et al., 2019) aims to identify bottlenecks in long-horizon visual tasks, while other prior works (Nair et al., 2018a;Finn & Levine, 2016) reach novel goals using model-free or model-based RL."
  },
  {
    "index": 1645,
    "source_corpus_id": 52086172,
    "ref_id": "b1",
    "citation_corpus_id": 748227,
    "start": 1282,
    "end": 1303,
    "title": "Generating Sentences from a Continuous Space",
    "abstract": "The standard recurrent neural network language model (rnnlm) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an rnn-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.",
    "prev": "INTRODUCTION\n\nDeep generative models trained via Stochastic Gradient Variational Bayes (SGVB) (Kingma & Welling, 2014a; efficiently couple the expressiveness of deep neural networks with the robustness to uncertainty of probabilistic latent variables.",
    "curr": "This combination has lead to their success in tasks ranging from image generation (Gregor et al., 2015;Rezende et al., 2016) to semi-supervised learning Maaløe et al., 2016) to language modeling (Bowman et al., 2016).",
    "next": "Various extensions to SGVB have been proposed (Burda et al., 2016;Maaløe et al., 2016;Salimans et al., 2015), but one conspicuous absence is an extension to Bayesian nonparametric processes."
  },
  {
    "index": 1647,
    "source_corpus_id": 245144606,
    "ref_id": "b18",
    "citation_corpus_id": 12122362,
    "start": 2142,
    "end": 2162,
    "title": "Published as a conference paper at ICLR 2017 LEARNING TO REMEMBER RARE EVENTS",
    "abstract": "Despite recent advances, memory-augmented deep neural networks are still limited when it comes to life-long and one-shot learning, especially in remembering rare events. We present a large-scale life-long memory module for use in deep learning. The module exploits fast nearest-neighbor algorithms for efficiency and thus scales to large memory sizes. Except for the nearest-neighbor query, the module is fully differentiable and trained end-to-end with no extra supervision. It operates in a life-long manner, i.e., without the need to reset it during training. Our memory module can be easily added to any part of a supervised neural network. To show its versatility we add it to a number of networks, from simple convolutional ones tested on image classification to deep sequence-to-sequence and recurrent-convolutional models. In all cases, the enhanced network gains the ability to remember and do life-long one-shot learning. Our module remembers training examples shown many thousands of steps in the past and it can successfully generalize from them. We set new state-of-the-art for one-shot learning on the Omniglot dataset and demonstrate, for the first time, life-long one-shot learning in recurrent neural networks on a large-scale machine translation task. * First two authors contributed equally. † Work done as a member of the Google Brain Residency program (g.co/brainresidency). ‡ Work done during internship at Google Brain.",
    "prev": "INTRODUCTION\n\nFew-shot learning with an external memory is known to learn new concepts quickly with only a few samples, especially when embedded in a meta-learning setting (Santoro et al., 2016).",
    "curr": "A common tactic is to store short-term memory (Munkhdalai & Yu, 2017;Munkhdalai et al., 2018;Kaiser et al., 2017) as obtained from the support set of the current task, and to empty it at the end of a task.",
    "next": "Another tactic is to let the memory store long-term knowledge distilled from all the training tasks (Zhen et al., 2020a), which provides a conceptual context to learn a new task."
  },
  {
    "index": 1648,
    "source_corpus_id": 259129342,
    "ref_id": "b6",
    "citation_corpus_id": 52967399,
    "start": 3691,
    "end": 3711,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "the scenario in Figure 1, where in the training corpus there are a large number of correlations, such as the word vaccine correlating with an increased number of disease cases.",
    "curr": "If we take the position that the success of LLMs (Radford et al., 2019;Devlin et al., 2019;Ouyang et al., 2022;Zhang et al., 2022;OpenAI, 2023, inter alia) lies in capturing a vast set of statistical correlations among terms (Bender et al., 2021), then the crucial yet missing step is how to process such correlations and infer causal relationships, for which a fundamental building block is this CORR2CAUSE inference skill.",
    "next": "To this end, we collect the first dataset, CORR2CAUSE, to test the pure causal reasoning abilities of large language models."
  },
  {
    "index": 1649,
    "source_corpus_id": 52901998,
    "ref_id": "b7",
    "citation_corpus_id": 1528564,
    "start": 6433,
    "end": 6449,
    "title": "MUPROP: UNBIASED BACKPROPAGATION FOR STOCHASTIC NEURAL NETWORKS",
    "abstract": "Deep neural networks are powerful parametric models that can be trained efficiently using the backpropagation algorithm. Stochastic neural networks combine the power of large parametric functions with that of graphical models, which makes it possible to learn very complex distributions. However, as backpropagation is not directly applicable to stochastic networks that include discrete sampling operations within their computational graph, training such networks remains difficult. We present MuProp, an unbiased gradient estimator for stochastic networks, designed to make this task easier. MuProp improves on the likelihood-ratio estimator by reducing its variance using a control variate based on the first-order Taylor expansion of a mean-field network. Crucially, unlike prior attempts at using backpropagation for training stochastic networks, the resulting estimator is unbiased and well behaved. Our experiments on structured output prediction and discrete latent variable modeling demonstrate that MuProp yields consistently good performance across a range of difficult tasks.",
    "prev": "SFNNs are difficult to optimize on large datasets (Tang & Salakhutdinov, 2013) due to the binary stochastic variables.",
    "curr": "Although there has been significant effort in improving training efficiency (Rezende et al., 2014;Gu et al., 2016), success has been partial.",
    "next": "In contrast, CVAEs (Sohn et al., 2015) assume Gaussian stochastic variables, which are easier to optimize on large datasets using the re-parameterization trick."
  },
  {
    "index": 1650,
    "source_corpus_id": 235313715,
    "ref_id": "b31",
    "citation_corpus_id": 3652072,
    "start": 11182,
    "end": 11185,
    "title": "LEARNING PARAMETRIC CLOSED-LOOP POLICIES FOR MARKOV POTENTIAL GAMES",
    "abstract": "Multiagent systems where the agents interact among themselves and with an stochastic environment can be formalized as stochastic games. We study a subclass of these games, named Markov potential games (MPGs), that appear often in economic and engineering applications when the agents share some common resource. We consider MPGs with continuous state-action variables, coupled constraints and nonconvex rewards. Previous analysis followed a variational approach that is only valid for very simple cases (convex rewards, invertible dynamics, and no coupled constraints); or considered deterministic dynamics and provided openloop (OL) analysis, studying strategies that consist in predefined action sequences, which are not optimal for stochastic environments. We present a closed-loop (CL) analysis for MPGs and consider parametric policies that depend on the current state and where agents adapt to stochastic transitions. We provide easily verifiable, sufficient and necessary conditions for a stochastic game to be an MPG, even for complex parametric functions (e.g., deep neural networks); and show that a closed-loop Nash equilibrium (NE) can be found (or at least approximated) by solving a related optimal control problem (OCP). This is useful since solving an OCP-which is a single-objective problem-is usually much simpler than solving the original set of coupled OCPs that form the game-which is a multiobjective control problem. This is a considerable improvement over the previously standard approach for the CL analysis of MPGs, which gives no approximate solution if no NE belongs to the chosen parametric family, and which is practical only for simple parametric forms. We illustrate the theoretical contributions with an example by applying our approach to a noncooperative communications engineering game. We then solve the game with a deep reinforcement learning algorithm that learns policies that closely approximates an exact variational NE of the game.",
    "prev": "Other works on MPGs.",
    "curr": "There are only a few papers in the recent literature that define and analyze MARL settings under the term Markov Potential Games using slight different definitions (see [18,33]).",
    "next": "These papers mainly focus on state-based potential MDPs (i.e., MDPs in which every state is a potential game) and require rather restrictive additional conditions, such as equality or monotonicity of the state-based potential functions, to address the computational challenge of finding Nash policies."
  },
  {
    "index": 1651,
    "source_corpus_id": 237532682,
    "ref_id": "b24",
    "citation_corpus_id": 220265858,
    "start": 2913,
    "end": 2916,
    "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
    "abstract": "Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficienctly be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.Preprint. Under review.",
    "prev": "For many of these tasks the scaling behavior of neural networks is highly predictable; model fit or test loss can be described precisely as a function of its number of parameters [18,21,16,17,31].",
    "curr": "Neural machine translation (NMT) has long enjoyed the benefits of scaling [19,4,25], but studies investigating the scaling behavior of NMT models are missing.",
    "next": "We present the first large-scale systematic study of scaling laws for encoder-decoder Transformer models applied to NMT [36]."
  },
  {
    "index": 1655,
    "source_corpus_id": 108367114,
    "ref_id": "b4",
    "citation_corpus_id": 21671720,
    "start": 3313,
    "end": 3333,
    "title": "Published as a conference paper at ICLR 2018 LEVERAGING GRAMMAR AND REINFORCEMENT LEARNING FOR NEURAL PROGRAM SYNTHESIS",
    "abstract": "Program synthesis is the task of automatically generating a program consistent with a specification. Recent years have seen proposal of a number of neural approaches for program synthesis, many of which adopt a sequence generation paradigm similar to neural machine translation, in which sequence-to-sequence models are trained to maximize the likelihood of known reference programs. While achieving impressive results, this strategy has two key limitations. First, it ignores Program Aliasing: the fact that many different programs may satisfy a given specification (especially with incomplete specifications such as a few input-output examples). By maximizing the likelihood of only a single reference program, it penalizes many semantically correct programs, which can adversely affect the synthesizer performance. Second, this strategy overlooks the fact that programs have a strict syntax that can be efficiently checked. To address the first limitation, we perform reinforcement learning on top of a supervised model with an objective that explicitly maximizes the likelihood of generating semantically correct programs. For addressing the second limitation, we introduce a training procedure that directly maximizes the probability of generating syntactically correct programs that fulfill the specification. We show that our contributions lead to improved accuracy of the models, especially in cases where the training data is limited.",
    "prev": "In this paper, we consider two problem settings.",
    "curr": "The first is the Karel domain and the recently proposed Karel synthesis model (Bunel et al., 2018).",
    "next": "We identify many distributions of input examples and DSL programs for which the Karel synthesis model performs poorly."
  },
  {
    "index": 1656,
    "source_corpus_id": 222140630,
    "ref_id": "b30",
    "citation_corpus_id": 6954272,
    "start": 2373,
    "end": 2392,
    "title": "MULTI-TASK SEQUENCE TO SEQUENCE LEARNING",
    "abstract": "Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three multi-task learning (MTL) settings for sequence to sequence models: (a) the oneto-many setting -where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting -useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting -where multiple encoders and decoders are shared, which is the case with unsupervised objectives and translation. Our results show that training on a small amount of parsing and image caption data can improve the translation quality between English and German by up to 1.5 BLEU points over strong single-task baselines on the WMT benchmarks. Furthermore, we have established a new state-of-the-art result in constituent parsing with 93.0 F 1 . Lastly, we reveal interesting properties of the two unsupervised learning objectives, autoencoder and skip-thought, in the MTL context: autoencoder helps less in terms of perplexities but more on BLEU scores compared to skip-thought. * Minh-Thang Luong is also a student at Stanford University.",
    "prev": "Deep multi-task learning (Deep MTL) has shown a similar ability to adapt knowledge across tasks whose observed variables are embedded in a shared space.",
    "curr": "Examples include vision, where the input for all tasks (photograph, drawing, or otherwise) is pixels arranged in a 2D plane (Zhang et al., 2014;Misra et al., 2016;Rebuffi et al., 2017); natural language (Collobert & Weston, 2008;Luong et al., 2016;Hashimoto et al., 2017), speech processing (Seltzer & Droppo, 2013;Huang et al., 2015), and genomics (Alipanahi et al., 2015), which exploit the 1D structure of text, waveforms, and nucleotide sequences; and video game-playing (Jaderberg et al., 2017;Teh et al., 2017), where interactions are organized across space and time.",
    "next": "Yet, many real-world prediction tasks have no such spatial organization; their input and output variables are simply labeled values, e.g., the height of a tree, the cost of a haircut, or the score on a standardized test."
  },
  {
    "index": 1660,
    "source_corpus_id": 244117789,
    "ref_id": "b6",
    "citation_corpus_id": 238259607,
    "start": 8515,
    "end": 8538,
    "title": "Published as a conference paper at ICLR 2022 GENERALIZED KERNEL THINNING",
    "abstract": "The kernel thinning (KT) algorithm of Dwivedi and Mackey (2021) compresses a probability distribution more effectively than independent sampling by targeting a reproducing kernel Hilbert space (RKHS) and leveraging a less smooth squareroot kernel. Here we provide four improvements. First, we show that KT applied directly to the target RKHS yields tighter, dimension-free guarantees for any kernel, any distribution, and any fixed function in the RKHS. Second, we show that, for analytic kernels like Gaussian, inverse multiquadric, and sinc, target KT admits maximum mean discrepancy (MMD) guarantees comparable to or better than those of square-root KT without making explicit use of a square-root kernel. Third, we prove that KT with a fractional power kernel yields better-than-Monte-Carlo MMD guarantees for non-smooth kernels, like Laplace and Matérn, that do not have square-roots. Fourth, we establish that KT applied to a sum of the target and power kernels (a procedure we call KT+) simultaneously inherits the improved MMD guarantees of power KT and the tighter individual function guarantees of target KT. In our experiments with target KT and KT+, we witness significant improvements in integration error even in 100 dimensions and when compressing challenging differential equation posteriors.",
    "prev": "We capture such bounds abstractly using the following definition of a sub-Gaussian thinning Hence the integration error of ALG is dominated by the sub-Gaussian parameter ν(n).",
    "curr": "Example 1 (KT-SPLIT) Given a kernel k and n input points S in , the KT-SPLIT(δ) algorithm 1 of Dwivedi & Mackey (2022;2021, Alg.",
    "next": "1a) takes Θ(n 2 ) kernel evaluations to output a coreset of size n out with better-than-i.i.d."
  },
  {
    "index": 1662,
    "source_corpus_id": 263608162,
    "ref_id": "b13",
    "citation_corpus_id": 233296808,
    "start": 1503,
    "end": 1523,
    "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
    "abstract": "In this work, we explore \"prompt tuning,\" a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient \"prompt ensembling.\" We release code and model checkpoints to reproduce our experiments. 1ReferencesRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,  Danilo Giampiccolo, Bernardo Magnini, and Idan  Szpektor. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognising textual entailment, volume 6, pages 6-4. Venice. . 2019a. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
    "prev": "F770398E17A9CE9F66E\nPrompt tuning in natural language processing (NLP) has become an increasingly popular method for adapting large language models to specific tasks.However, the transferability of these prompts, especially continuous prompts, between different models remains a challenge.In this work, we propose a zero-shot continuous prompt transfer method, where source prompts are encoded into relative space and the corresponding target prompts are searched for transferring to target models.Experimental results confirm the effectiveness of our method, showing that \"task semantics\" in continuous prompts can be generalized across various language models.Moreover, we find that combining \"task semantics\" from multiple source models can further enhance the generalizability of transfer.",
    "curr": "1\n\nIntroduction\n\nRecently in natural language processing (NLP), there has been a paradigm shift from full language model finetuning to the optimization of a small subset of prompt tokens (Shin et al., 2020;Lester et al., 2021;Li and Liang, 2021;Zhong et al., 2021).As language models have dramatically increased in size and may contain billions of parameters (Brown et al., 2020), the strategy of freezing language models while optimizing the learnable prompt parameters becomes the most affordable and efficient alternative for downstream tasks.This technique, referred to as prompt tuning, has gained substantial recognition for its efficacy across a range of language models (Shin et al., 2020;Lester et al., 2021;Li and Liang, 2021;Zhong et al., 2021).",
    "next": "Various prompt tuning methods have been explored, which can be generally categorized into discrete and continuous cases.Discrete prompt tuning, such as AutoPrompt (Shin et al., 2020), primarily focuses on the selection and optimization of a predetermined set of tokens within a language model's vocabulary.By contrast, continuous prompt tuning (Zhong et al., 2021) allows the modification of continuous prompt embeddings by gradient descent.The latter often offers bet"
  },
  {
    "index": 1664,
    "source_corpus_id": 261100593,
    "ref_id": "b14",
    "citation_corpus_id": 257279766,
    "start": 3617,
    "end": 3634,
    "title": "TDR-CL: TARGETED DOUBLY ROBUST COLLABORA- TIVE LEARNING FOR DEBIASED RECOMMENDATIONS",
    "abstract": "Bias is a common problem inherent in recommender systems, which is entangled with users' preferences and poses a great challenge to unbiased learning. For debiasing tasks, the doubly robust (DR) method and its variants show superior performance due to the double robustness property, that is, DR is unbiased when either imputed errors or learned propensities are accurate. However, our theoretical analysis reveals that DR usually has a large variance. Meanwhile, DR would suffer unexpectedly large bias and poor generalization caused by inaccurate imputed errors and learned propensities, which usually occur in practice. In this paper, we propose a principled approach that can effectively reduce the bias and variance simultaneously for existing DR approaches when the error imputation model is misspecified. In addition, we further propose a novel semi-parametric collaborative learning approach that decomposes imputed errors into parametric and nonparametric parts and updates them collaboratively, resulting in more accurate predictions. Both theoretical analysis and experiments demonstrate the superiority of the proposed methods compared with existing debiasing methods.Published as a conference paper at ICLR 2023 agnostic framework and can be assembled into any DR method by updating its error imputation model, resulting in more accurate predictions.To further reduce the bias and variance during the training process, we propose a novel uniformdata-free TDR-based collaborative learning (TDR-CL) approach that decomposes imputed errors into a parametric imputation model part and a nonparametric error part, where the latter adaptively rectifies the residual bias of the former. By updating the two parts collaboratively, TDR-CL achieves a more accurate and robust prediction. Both theoretical analysis and experiments demonstrate the superiority of TDR and TDR-CL compared with existing methods.PRELIMINARIESMany debiasing tasks in RS can be formulated using the widely adopted potential outcome framework(Neyman, 1990;Rubin, 1974). Denote U = {u}, I = {i} and D = U × I as the sets of users, items and user-item pairs, respectively. Let x u,i , r u,i , and o u,i be the feature, feedback, and exposure status of user-item pair (u, i), where o u,i = 1 or 0 represents whether the item i is exposed to user u or not. Define r u,i (1) as the potential outcome if o u,i had been set to 1, which is observed only when o u,i = 1. In RS, we are often interested in answering the causal question: \"if we recommend products to users, what would be the feedback?\". This question can be formulated as to learn the quantity E (r u,i (1)|x u,i ), i.e., it requires to predict r u,i (1) using feature x u,i , where E denotes the expectation with respect to the target distribution P. Many classical tasks in RS can be defined as estimating this quantity, such as rating prediction(Schnabel et al., 2016)and post-click conversion rate prediction (Guo et al., 2021). More examples can be found inWu et al. (2022b).Let f θ (x u,i ) be a model used to predict r u,i (1) with parameter θ. Ideally, if all r u,i (1) for (u, i) ∈ D were observed, θ can be trained directly by optimizing the following ideal loss",
    "prev": "However, the DR methods still have many limitations.",
    "curr": "Theoretical analysis shows that inverse probability scoring (IPS) and DR methods may have infinite bias, variance, and generalization error bounds, in the presence of extremely small propensity scores (Schnabel et al., 2016;Wang et al., 2019;Guo et al., 2021;Li et al., 2023b).",
    "next": "In addition, due to the fact that users are more inclined to evaluate the preferred items, the problem of data missing not at random (MNAR) often occurs in RS."
  },
  {
    "index": 1665,
    "source_corpus_id": 228083457,
    "ref_id": "b2",
    "citation_corpus_id": 52889459,
    "start": 2085,
    "end": 2105,
    "title": "LARGE SCALE GAN TRAINING FOR HIGH FIDELITY NATURAL IMAGE SYNTHESIS",
    "abstract": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by truncating the latent space. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128×128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.3 and Fréchet Inception Distance (FID) of 9.6, improving over the previous best IS of 52.52 and FID of 18.65.",
    "prev": "INTRODUCTION\n\nSince their introduction by Goodfellow et al.",
    "curr": "(2014), generative adversarial networks (GANs) have seen remarkable progress, with current models capable of generating samples of very high quality (Brock et al., 2018;Karras et al., 2019a;2019b).",
    "next": "In recent years, particular effort has been invested in constructing controllable models, which allow manipulating attributes of the generated images."
  },
  {
    "index": 1668,
    "source_corpus_id": 253265269,
    "ref_id": "b25",
    "citation_corpus_id": 244116787,
    "start": 27226,
    "end": 27229,
    "title": "Published as a conference paper at ICLR 2022 SALIENT IMAGENET: HOW TO DISCOVER SPURIOUS FEATURES IN DEEP LEARNING?",
    "abstract": "a) class: band aid, spurious feature: fingers, -41.54% (b) class: space bar, spurious feature: keys, -46.15% (c) class: plate, spurious feature: food, -32.31% (d) class: butterfly, spurious feature: flowers, -21.54% (e) class: potter's wheel, spurious feature: vase, -21.54%",
    "prev": "However, most of these datasets are built to assess the model performance when going away from training data distribution and, thus, provide almost no understanding about the nature of the in-distribution errors.",
    "curr": "Currently, the only ImageNet extensions that help analyzing the in-distribution model errors are the multiclass relabelling or saliency of the validation set [24,4,30,26].",
    "next": "However, this relabelling only explains one type of model error that is caused by the co-occurrences of other objects in the scene."
  },
  {
    "index": 1669,
    "source_corpus_id": 59553475,
    "ref_id": "b1",
    "citation_corpus_id": 13352766,
    "start": 6567,
    "end": 6587,
    "title": "N2N Learning: Network to Network Compression via Policy Gradient Reinforcement Learning",
    "abstract": "While bigger and deeper neural network architectures continue to advance the state-of-the-art for many computer vision tasks, real-world adoption of these networks is impeded by hardware and speed constraints. Conventional model compression methods attempt to address this problem by modifying the architecture manually or using pre-defined heuristics. Since the space of all reduced architectures is very large, modifying the architecture of a deep neural network in this way is a difficult task. In this paper, we tackle this issue by introducing a principled method for learning reduced network architectures in a data-driven way using reinforcement learning. Our approach takes a larger 'teacher' network as input and outputs a compressed 'student' network derived from the 'teacher' network. In the first stage of our method, a recurrent policy network aggressively removes layers from the large 'teacher' model. In the second stage, another recurrent policy network carefully reduces the size of each remaining layer. The resulting network is then evaluated to obtain a reward -a score based on the accuracy and compression of the network. Our approach uses this reward signal with policy gradients to train the policies to find a locally optimal student network. Our experiments show that we can achieve compression rates of more than 10× for models such as ResNet-34 while maintaining similar performance to the input 'teacher' network. We also present a valuable transfer learning result which shows that policies which are pre-trained on smaller 'teacher' networks can be used to rapidly speed up training on larger 'teacher' networks.",
    "prev": "Neural Architecture Search (NAS): NAS has recently been an active research topic (Zoph & Le, 2016;Zoph et al., 2017;Real et al., 2018;Pham et al., 2018;Liu et al., 2017a;b;Luo et al., 2018).",
    "curr": "Some existing works in NAS are focused on searching for architectures that not only can achieve high performance but also respect some resource or computation constraints (Ashok et al., 2018;Tan et al., 2018;Dong et al., 2018;Hsu et al., 2018;Elsken et al., 2018a).",
    "next": "NAO (Luo et al., 2018) and our work share the idea of mapping network architectures into a latent continuous embedding space."
  },
  {
    "index": 1671,
    "source_corpus_id": 263830421,
    "ref_id": "b47",
    "citation_corpus_id": 259138847,
    "start": 2287,
    "end": 2290,
    "title": "The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers",
    "abstract": "This paper studies the curious phenomenon for machine learning models with Transformer architectures that their activation maps are sparse. By activation map we refer to the intermediate output of the multi-layer perceptrons (MLPs) after a ReLU activation function, and by \"sparse\" we mean that on average very few entries (e.g., 3.0% for T5-Base and 6.3% for ViT-B16) are nonzero for each input to MLP. Moreover, larger Transformers with more layers and wider MLP hidden dimensions are sparser as measured by the percentage of nonzero entries. Through extensive experiments we demonstrate that the emergence of sparsity is a prevalent phenomenon that occurs for both natural language processing and vision tasks, on both training and evaluation data, for Transformers of various configurations, at layers of all depth levels, as well as for other architectures including MLP-mixers and 2-layer MLPs. We show that sparsity also emerges using training datasets with random labels, or with random inputs, or with infinite amount of data, demonstrating that sparsity is not a result of a specific family of datasets. We discuss how sparsity immediately implies a way to significantly reduce the FLOP count and improve efficiency for Transformers. Moreover, we demonstrate perhaps surprisingly that enforcing an even sparser activation via Top-k thresholding with a small value of k brings a collection of desired but missing properties for Transformers, namely less sensitivity to noisy training data, more robustness to input corruptions, and better calibration for their prediction confidence. * Equal contribution arXiv:2210.06313v2 [cs.LG] 9 Jun 2023",
    "prev": "Introduction\n\nThe widespread excitement surrounding Large Language Models (LLMs) has sparked significant interest in leveraging AI across diverse domains [5,9,6].However, realizing the potential of LLMs is challenged by their significant computational and memory requirements during inference [60,40,3].To enhance the inference efficiency 1 , various techniques have been explored, including quantization [12,50], speculative decoding [41], pruning [53,71], and weight sparsification [20,15].Among these techniques, achieving activation sparsity offers a compelling advantage by providing a favorable balance between accuracy and speedup, especially on modern hardware like GPUs [51].",
    "curr": "Notably, employing the Rectified Linear Unit (ReLU) activation function [22] in neural networks is recognized for inducing sparse activations and has been adopted in various prior works [27,44,48,69].To reaffirm this property, we employ the OPT model [80], utilizing ReLU, and measure the sparsity of activations in the Feed Forward Network (FFN) between the fully connected layers.As illustrated in Fig.",
    "next": "1a, all layers exhibit sparsity exceeding 90%.On average, across all layers, this activation sparsity results in substantial weight transfer (I/O) savings between the GPU and CPU, impacting 95% of the rows of the down projection layer's weights (Fig."
  },
  {
    "index": 1672,
    "source_corpus_id": 244488490,
    "ref_id": "b66",
    "citation_corpus_id": 189898655,
    "start": 7842,
    "end": 7860,
    "title": "PSEUDO-LIDAR++: ACCURATE DEPTH FOR 3D OBJECT DETECTION IN AUTONOMOUS DRIVING",
    "abstract": "Detecting objects such as cars and pedestrians in 3D plays an indispensable role in autonomous driving.Existing approaches largely rely on expensive LiDAR sensors for accurate depth information.While recently pseudo-LiDAR has been introduced as a promising alternative, at a much lower cost based solely on stereo images, there is still a notable performance gap.In this paper we provide substantial advances to the pseudo-LiDAR framework through improvements in stereo depth estimation.Concretely, we adapt the stereo network architecture and loss function to be more aligned with accurate depth estimation of faraway objects -currently the primary weakness of pseudo-LiDAR.Further, we explore the idea to leverage cheaper but extremely sparse LiDAR sensors, which alone provide insufficient information for 3D detection, to de-bias our depth estimation.We propose a depthpropagation algorithm, guided by the initial depth estimates, to diffuse these few exact measurements across the entire depth map.We show on the KITTI object detection benchmark that our combined approach yields substantial improvements in depth estimation and stereo-based 3D object detection -outperforming the previous state-of-the-art detection accuracy for faraway objects by 40%.Our code is available at https://github.com/mileyan/Pseudo_Lidar_V2.",
    "prev": "(2021) propose point densification networks under the supervision of dense ground truth points.",
    "curr": "Though some papers internally conduct point denoising (Yu et al., 2018a;Yifan et al., 2019), most of these works mainly assume that there are no strong outliers among the input points, such as flying-point cloud (You et al., 2019).",
    "next": "Figure 2: Two-stage architecture for point cloud reconstruction."
  },
  {
    "index": 1673,
    "source_corpus_id": 207847719,
    "ref_id": "b1",
    "citation_corpus_id": 49430686,
    "start": 3089,
    "end": 3107,
    "title": "Graph-to-Sequence Learning using Gated Graph Neural Networks",
    "abstract": "Many NLP applications can be framed as a graph-to-sequence learning problem. Previous work proposing neural architectures on this setting obtained promising results compared to grammar-based approaches but still rely on linearisation heuristics and/or standard recurrent networks to achieve the best performance. In this work, we propose a new model that encodes the full structural information contained in the graph. Our architecture couples the recently proposed Gated Graph Neural Networks with an input transformation that allows nodes and edges to have their own hidden representations, while tackling the parameter explosion problem present in previous work. Experimental results show that our model outperforms strong baselines in generation from AMR graphs and syntax-based neural machine translation.",
    "prev": "Hence, such methods are not directly applicable for tasks such as link prediction which require relation embedding vectors.",
    "curr": "Initial attempts at learning representations for relations in graphs (Monti et al., 2018;Beck et al., 2018) have shown some performance gains on tasks like node classification and neural machine translation.",
    "next": "There has been extensive research on embedding Knowledge Graphs (KG) Wang et al., 2017) where representations of both nodes and relations are jointly learned."
  },
  {
    "index": 1674,
    "source_corpus_id": 254070084,
    "ref_id": "b10",
    "citation_corpus_id": 5834589,
    "start": 1894,
    "end": 1915,
    "title": "ON LARGE-BATCH TRAINING FOR DEEP LEARNING: GENERALIZATION GAP AND SHARP MINIMA",
    "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say 32-512 data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions-and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap. Published as a conference paper at ICLR 2017 These methods minimize the objective function f by iteratively taking steps of the form: J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. . Weak sharp minima and penalty functions in mathematical programming. PhD thesis, University of Cambridge, 1988.Michael P Friedlander and Mark Schmidt. Hybrid deterministic-stochastic methods for data fitting.",
    "prev": "† skaur@princeton.edu, Princeton University.",
    "curr": "‡\n\nIntroduction\n\nWhile small-batch SGD has frequently been observed to outperform large-batch SGD [Geiping et al., 2022, Keskar et al., 2017, Masters and Luschi, 2018, Smith et al., 2021, Wu et al., 2020, Jastrzebski et al., 2018, Wu et al., 2018, Wen et al., 2020, Mori and Ueda, 2020, the upstream cause for this generalization gap is a contested topic, approached from a variety of analytical perspectives [Goyal et al., 2017, Wu et al., 2020, Geiping et al., 2022.",
    "next": "Initial work in this field has generally focused on the learning rate to batch-size ratio [Keskar et al., 2017, Masters and Luschi, 2018, Goyal et al., 2017, Mandt et al., 2017, He et al., 2019, Li et al., 2019 or on recreating stochastic noise via mini-batching [Wu et al., 2020, Jastrzebski et al., 2018, Zhu et al., 2019, Mori and Ueda, 2020, Cheng et al., 2020, Simsekli et al., 2019, Xie et al., 2021, whereas recent works have pivoted focus on understanding how mini-batch SGD may implicitly regularize certain quantities that improve generalization [Geiping et al., 2022, Barrett and Dherin, 2020, Smith et al., 2021, Jastrzebski et al., 2020."
  },
  {
    "index": 1675,
    "source_corpus_id": 249395201,
    "ref_id": "b13",
    "citation_corpus_id": 3398677,
    "start": 2767,
    "end": 2786,
    "title": "Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step",
    "abstract": "Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players' parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step. * Equal contribution.",
    "prev": "GANs in practice, however, are known to suffer from a variety of issues ranging from non-convergence and training instability to mode collapse Mescheder et al., 2018).",
    "curr": "As a result, a wide array of analyses and modifications has been proposed for GANs, including improving the network architectures (Karras et al., 2019;Sauer et al., 2021;Zhang et al., 2019), gaining theoretical understanding of GAN training Heusel et al., 2017;Mescheder et al., 2017;, changing the objective functions Bellemare et al., 2017;Deshpande et al., 2018;Li et al., 2017a;Nowozin et al., 2016;Zheng and Zhou, 2021;Yang et al., 2021), regularizing the weights and/or gradients Fedus et al., 2018;Mescheder et al., 2018;Miyato et al., 2018a;Roth et al., 2017;Salimans et al., 2016), utilizing side information (Wang et al., 2018;Zhang et al., 2017;2020b), adding a mapping from the data to latent representation (Donahue et al., 2016;Dumoulin et al., 2016;Li et al., 2017b), and applying differentiable data augmentation (Karras et al., 2020a;Zhang et al., 2020a;Zhao et al., 2020).",
    "next": "A simple technique to stabilize GAN training is to inject instance noise, i.e., to add noise to the discriminator input, which can widen the support of both the generator and discriminator distributions and prevent the discriminator from overfitting Sønderby et al., 2017)."
  },
  {
    "index": 1676,
    "source_corpus_id": 234358843,
    "ref_id": "b6",
    "citation_corpus_id": 53388625,
    "start": 3166,
    "end": 3189,
    "title": "THE LOTTERY TICKET HYPOTHESIS: FINDING SPARSE, TRAINABLE NEURAL NETWORKS",
    "abstract": "Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard technique for pruning weights naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the lottery ticket hypothesis: dense, randomly-initialized feed-forward networks contain subnetworks (winning tickets) that-when trained in isolation-arrive at comparable test accuracy in a comparable number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Furthermore, the winning tickets we find above that size learn faster than the original network and exhibit higher test accuracy. arXiv:1803.03635v4 [cs.LG] 27 Nov 2018 1. Randomly initialize a neural network f (x; θ 0 ) (where θ 0 ∼ D θ ). 2. Train the network for j iterations, reaching parameters θ j . 3. Prune s% of the parameters, creating a mask m where P m = (100 − s)%. 4. To extract the winning ticket, reset the remaining parameters to their values in θ 0 , creating the untrained network f (x; m θ 0 ).If dense networks contain winning tickets and pruning reveals them, then the network f (x; m θ 0 )when trained for j iterations-will reach similar accuracy to f (x; θ j ) at least as quickly, and m will be too sparse for a randomly-reinitialized or randomly-sparsified network to do the same.Results. We identify winning tickets in a fully-connected architecture for MNIST and convolutional architectures for CIFAR10 across several optimization strategies (SGD, momentum, and Adam) with techniques like dropout, weight decay, and batchnorm. In deeper networks, our pruning-based strategy for finding winning tickets is sensitive to the learning rate: it requires warmup to find winning tickets at higher learning rates. The winning tickets we find are 10-20% (or less) of the size of the Anonymous. Gradient descent provably optimizes over-parameterized neural networks. Dally. Dsd: Regularizing deep neural networks with dense-sparse-dense training flow. Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size. arXiv preprint arXiv:1602.07360, 2016.",
    "prev": "One common method is the global MP criteria (see, e.g., Morcos et al.",
    "curr": "(2019)), * Work done at KAIST 1 i.e., simultaneously training and pruning arXiv:2010.07611v2 [cs.LG]\n\nINTRODUCTION\n\nNeural network pruning is an art of removing \"unimportant weights\" from a model, with an intention to meet practical constraints (Han et al., 2015), mitigate overfitting (Hanson & Pratt, 1988), enhance interpretability (Mozer & Smolensky, 1988), or deepen our understanding on neural network training (Frankle & Carbin, 2019).",
    "next": "Yet, the importance of weight is still a vaguely defined notion, and thus a wide range of pruning algorithms based on various importance scores has been proposed."
  },
  {
    "index": 1677,
    "source_corpus_id": 20285896,
    "ref_id": "b0",
    "citation_corpus_id": 18828233,
    "start": 5003,
    "end": 5005,
    "title": "TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.",
    "prev": "Negative Logarithm Alternative It is worth mentioning that in the above formulation we adopt − log(D r (x)) as an alternative of log(1−D r (x)) for the generator's loss [12].",
    "curr": "The rationale is that, when the discriminator perfectly distinguishes fake samples, the log(1−D r (x)) loss function of the generator may suffer from the gradient vanishing problem [12,1].",
    "next": "The motivation of using the − log(D r (x)) is that: though giving a difference gradient scale, it always preserves the same gradient direction as log(1−D r (x))."
  },
  {
    "index": 1678,
    "source_corpus_id": 221971208,
    "ref_id": "b33",
    "citation_corpus_id": 11754890,
    "start": 8747,
    "end": 8751,
    "title": "Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval",
    "abstract": "Methods of deep neural networks (DNNs)have recently demonstrated superior performance on a number of natural language processing tasks. However, in most previous work, the models are learned based on either unsupervised objectives, which does not directly optimize the desired task, or singletask supervised objectives, which often suffer from insufficient training data. We develop a multi-task DNN for learning representations across multiple tasks, not only leveraging large amounts of cross-task data, but also benefiting from a regularization effect that leads to more general representations to help tasks in new domains. Our multi-task DNN approach combines tasks of multiple-domain classification (for query classification) and information retrieval (ranking for web search), and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation.",
    "prev": "A prominent approach uses explicit parameter sharing; for instance, by means of common low-level layers leading to different heads.",
    "curr": "Among others, this has been successfully applied to vision [72], language [34], and reinforcement learning [18] tasks.",
    "next": "In addition, a variety of ways to combine taskspecific representations have arisen, such as cross-stitch networks [41], or lateral connections [53]."
  },
  {
    "index": 1680,
    "source_corpus_id": 423406,
    "ref_id": "b26",
    "citation_corpus_id": 3116311,
    "start": 2566,
    "end": 2586,
    "title": "Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions",
    "abstract": "We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model's ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.",
    "prev": "In contrast to sequential models, these models' architectures are organized according to each sentence's syntactic structure, that is, the hierarchical organization of words into nested phrases that characterizes human intuitions about how words combine to form grammatical sentences.",
    "curr": "Prior work on tree-structured models has assumed that trees are either provided together with the input sentences (Clark et al., 2008;Grefenstette & Sadrzadeh, 2011;Socher et al., 2011;2013;Tai et al., 2015) or that they are predicted based on explicit treebank annotations jointly with the downstream task (Bowman et al., 2016;Dyer et al., 2016).",
    "next": "The last approach for constructing sentence representations uses convolutional neural networks to produce the representation in a bottom up manner, either with syntactic information (Ma et al., 2015) or without (Kim, 2014;Kalchbrenner et al., 2014)."
  },
  {
    "index": 1682,
    "source_corpus_id": 5071138,
    "ref_id": "b4",
    "citation_corpus_id": 26501419,
    "start": 3485,
    "end": 3505,
    "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
    "abstract": "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K questionanswer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a featurebased classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that Trivi-aQA is a challenging testbed that is worth significant future study. 1",
    "prev": "It is therefore critical, that the dataset benchmarks established for the RC task keep progressing in complexity to reflect the challenges that arise in true language understanding, thereby enabling the development of models and techniques to solve these challenges.",
    "curr": "For RC in particular, there has been significant progress over the recent years with several benchmark datasets, the most popular of which are the SQuAD dataset (Rajpurkar et al., 2016a), TriviaQA (Joshi et al., 2017), MS MARCO (Nguyen et al., 2016), MovieQA (Tapaswi et al., 2016) and clozestyle datasets (Mostafazadeh et al., 2016;Onishi et al., 2016;Hermann et al., 2015).",
    "next": "However, these benchmarks, owing to both the nature of the passages and the QA pairs to evaluate the RC task, have 2 primary limitations in studying language understanding: (i) Other than MovieQA, which is a small dataset of 15K QA pairs, all other largescale RC datasets deal only with factual descriptive passages and not narratives (involving events with causality linkages that require reasoning and background knowledge) which is the case with a lot of real-world content such as story books, movies, news reports, etc."
  },
  {
    "index": 1683,
    "source_corpus_id": 264820183,
    "ref_id": "b23",
    "citation_corpus_id": 222140788,
    "start": 2599,
    "end": 2618,
    "title": "Published as a conference paper at ICLR 2021 DENOISING DIFFUSION IMPLICIT MODELS",
    "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples 10× to 50× faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error.",
    "prev": "ying lengths of shot-level videos.Specifically, we propose a random-mask video diffusion model to automatically generate transitions based on textual descriptions.By providing the images of different scenes as inputs, combined with textbased control, our model generates transition videos that ensure coherence and visual quality.Furthermore, the model can be readily extended to various tasks such as image-to-video animation and auto-regressive video prediction.To conduct a comprehensive evaluation of this new generative task, we propose three assessing criteria for smooth and creative transition: temporal consistency, semantic similarity, and video-text semantic alignment.Extensive experiments validate the effectiveness of our approach over existing methods for generative transition and prediction, enabling the creation of story-level long videos.Project page: https://vchitect.github.io/SEINE-project/.",
    "curr": "INTRODUCTION\n\nCurrently, with the success of diffusion model-based (Ho et al., 2020;Song et al., 2021a;b) textto-image generation models (Ramesh et al., 2021;2022;Saharia et al., 2022;Balaji et al., 2022;Rombach et al., 2022), a series of video generation works (Singer et al., 2023;Ho et al., 2022;Blattmann et al., 2023;Zhou et al., 2022;He et al., 2022;Wang et al., 2023b) have emerged and demonstrated impressive results.However, current video generation methods typically only yield \"shot-level\" video generation, which only consists of around a few seconds and depicts a single scene.Such shot-level short videos fall short of the demands for cinematic and film productions.",
    "next": "In cinematic or industrial-level video productions, \"story-level\" long video is typically characterized by the creation of distinct shots with different scenes.These individual shots of various lengths are interconnected through techniques like transitions and editing, providing a way for longer video and more intricate visual storytelling.The process of combining scenes or shots in film and video editing, known as tr"
  },
  {
    "index": 1685,
    "source_corpus_id": 221836662,
    "ref_id": "b13",
    "citation_corpus_id": 52920808,
    "start": 1891,
    "end": 1894,
    "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks",
    "abstract": "One of the mystery in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an m hidden node shallow neural network with ReLU activation and n training data, we show as long as m is large enough and the data is non-degenerate, randomly initialized gradient descent converges a globally optimal solution with a linear convergence rate for the quadratic loss function.Our analysis is based on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods. * Equal Contribution.",
    "prev": "Through the NTK, theoretical tools from kernel methods were introduced to the study of deep overparametrized neural networks.",
    "curr": "Theoretical results were thereby established regarding the convergence [1,13,14], generalization [4,9], and loss landscape [20] of overparametrized neural networks in the NTK regime.",
    "next": "While NTK has proved to be a powerful theoretical tool, a recent work [17] posed an important question whether the NTK is significantly different from our repertoire of standard kernels."
  },
  {
    "index": 1687,
    "source_corpus_id": 211027382,
    "ref_id": "b13",
    "citation_corpus_id": 6628106,
    "start": 23573,
    "end": 23592,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "GNN message-passing layers from different time steps have independent weights.",
    "curr": "We train our model using Adam (Kingma & Ba, 2014) with default parameters (α = 0.9, β = 0.999) and set the learning rate to be 10 −3 initially but linearly decrease it to 10 −4 until the 30th epoch.",
    "next": "We use a weight decay of 10 −4 for regularization and stop the training once the loss on validation set starts to increase (which usually happens around 30 epochs)."
  },
  {
    "index": 1688,
    "source_corpus_id": 3482308,
    "ref_id": "b31",
    "citation_corpus_id": 4071727,
    "start": 3687,
    "end": 3691,
    "title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",
    "abstract": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.",
    "prev": "While there is less of evidence now that pre-training actually helps, several other solutions have since been put forth to address the issue of efficiently training DNNs.",
    "curr": "These include heuristics such as dropouts [37], but also considering alternate deep architectures such as convolutional neural networks [32], deep belief networks [15], and deep Boltzmann machines [30].",
    "next": "In addition, deep architectures based on new non-saturating activation functions have been suggested to be more effectively trainable -the most successful and widely popular of these is the rectified linear unit (ReLU) activation, i.e., σ(x) = max{0, x}, which is the focus of study in this paper."
  },
  {
    "index": 1691,
    "source_corpus_id": 251648935,
    "ref_id": "b21",
    "citation_corpus_id": 1428702,
    "start": 2565,
    "end": 2583,
    "title": "Learning Word Vectors for Sentiment Analysis",
    "abstract": "Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term-document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.",
    "prev": "This is problematic precisely because many black box models, such as deep neural networks, perform well by creating complex structures and combining features in their latent layers.",
    "curr": "To address this, recent methods have been proposed to learn the interaction between features (Sundararajan et al., 2020a;Maas et al., 2011).",
    "next": "Their definition of interaction assumes features affect each other symmetrically; however, in many real-world applications, feature interactions may be asymmetrical."
  },
  {
    "index": 1693,
    "source_corpus_id": 253080775,
    "ref_id": "b35",
    "citation_corpus_id": 53296520,
    "start": 7985,
    "end": 8006,
    "title": "COMMONSENSEQA: A Question Answering Challenge Targeting Commonsense Knowledge",
    "abstract": "When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present COMMONSENSEQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from CON-CEPTNET (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018)  and obtains 56% accuracy, well below human performance, which is 89%.",
    "prev": "On the other hand, ELI5 (Fan et al., 2019) (Jhamtani & Clark, 2020), and EntailmentBank (Dalvi et al., 2021) focus on explanation and reasoning much like WIKIWHY, albeit with significant differences (Table 1).",
    "curr": "CoS-E's explanations for CommonsenseQA (Talmor et al., 2019) mark an important first step, but the commonsense explanations have limited depth, often requiring a single hop of reasoning.",
    "next": "eQASC and EntailmentBank feature richer explanations with more complex structure, tightly focusing on grade school level science facts."
  },
  {
    "index": 1695,
    "source_corpus_id": 246863450,
    "ref_id": "b10",
    "citation_corpus_id": 53867751,
    "start": 5796,
    "end": 5814,
    "title": "EMERGENT COORDINATION THROUGH COMPETITION",
    "abstract": "We study the emergence of cooperative behaviors in reinforcement learning agents by introducing a challenging competitive multi-agent soccer environment with continuous simulated physics. We demonstrate that decentralized, populationbased training with co-play can lead to a progression in agents' behaviors: from random, to simple ball chasing, and finally showing evidence of cooperation. Our study highlights several of the challenges encountered in large scale multi-agent training in continuous control. In particular, we demonstrate that the automatic optimization of simple shaping rewards, not themselves conducive to co-operative behavior, can lead to long-horizon team behavior. We further apply an evaluation scheme, grounded by game theoretic principals, that can assess agent performance in the absence of pre-defined evaluation tasks or human baselines. * Equal contribution. 1  The environment is released at https://git.io/dm_soccer. Published as a conference paper at ICLR 2019RETRACE-SVG0Throughout our experiments we use Stochastic Value Gradients (SVG0) (Heess et al., 2015b) as our reinforcement learning algorithm for continuous control. This is an actor-critic policy gradient algorithm, which in our setting is used to estimate gradients ∂ ∂θ J i (π θ ; π \\i ) of the objective (1) for each game. Averaging these gradients over games will effectively optimize the PBT objective J(π θ ). Policies are additionally regularized with an entropy loss H(π) i.e. we maximizeĴ(π θ ) := J(π θ ) + αH(π θ ) using the Adam optimizer(Kingma & Ba, 2014)to apply gradient updates where α represents a multiplicative entropy cost factor. A derivation of SVG0 is provided in Appendix A.SVG utilizes a differentiable Q-critic. Our critic is learned using experience replay, minimizing a k-step TD-error with off-policy retrace corrections(Munos et al., 2016), using a separate target",
    "prev": "Most interestingly, we show that NeuPL enables transfer learning across policies, discovering exploiters to strong opponents that would have been inaccessible to comparable baselines (Section 2.2).",
    "curr": "Finally, we show the appeal of NeuPL in the challenge domain of MuJoCo Football (Liu et al., 2019) where players must continuously refine their movement skills in order to coordinate as a team.",
    "next": "In this highly transitive game, NeuPL naturally represents a short sequence of best-responses without the need for a carefully chosen truncation criteria (Section 2.4)."
  },
  {
    "index": 1696,
    "source_corpus_id": 247593929,
    "ref_id": "b2",
    "citation_corpus_id": 222133374,
    "start": 5629,
    "end": 5648,
    "title": "HETEROFL: COMPUTATION AND COMMUNICATION EFFICIENT FEDERATED LEARNING FOR HETEROGE- NEOUS CLIENTS",
    "abstract": "Federated Learning (FL) is a method of training machine learning models on private data distributed over a large number of possibly heterogeneous clients such as mobile phones and IoT devices. In this work, we propose a new federated learning framework named HeteroFL to address heterogeneous clients equipped with very different computation and communication capabilities. Our solution can enable the training of heterogeneous local models with varying computation complexities and still produce a single global inference model. For the first time, our method challenges the underlying assumption of existing work that local models have to share the same architecture as the global model. We demonstrate several strategies to enhance FL training and conduct extensive empirical evaluations, including five computation complexity levels of three model architecture on three datasets. We show that adaptively distributing subnetworks according to clients' capabilities is both computation and communication efficient. Our code is available here.arXiv:2010.01264v3 [cs.LG] 14 Dec 2021Published as a conference paper at ICLR 2021 capabilities. However, to stably aggregate heterogeneous local models to a single global model under various heterogeneous settings is not apparent. Addressing these issues is thus a key component of our work. Our main contributions of this work are three-fold. Tal Ben-Nun and Torsten Hoefler. Demystifying parallel and distributed deep learning: An in-depth concurrency analysis. : Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015. . Improving federated learning personalization via model agnostic meta learning. arXiv preprint arXiv:",
    "prev": "For example, in a learning task for face recognition, clients may use different types of devices (e.g., computers, tablets or smartphones) to participate in learning.",
    "curr": "To accommodate different hardware, one can turn to more resource-flexible architectures trained by distillation from ensemble (Lin et al., 2020), partial model averaging (Diao et al., 2021), or directly combining predictions (Shi et al., 2021).",
    "next": "Specifically, (Diao et al., 2021) is the first heterogeneous-width solution (HeteroFL) allowing insitu model-size switching."
  },
  {
    "index": 1699,
    "source_corpus_id": 239050236,
    "ref_id": "b16",
    "citation_corpus_id": 231812484,
    "start": 5383,
    "end": 5401,
    "title": "Published as a conference paper at ICLR 2021 DDPNOPT: DIFFERENTIAL DYNAMIC PROGRAMMING NEURAL OPTIMIZER",
    "abstract": "Interpretation of Deep Neural Networks (DNNs) training as an optimal control problem with nonlinear dynamical systems has received considerable attention recently, yet the algorithmic development remains relatively limited. In this work, we make an attempt along this line by reformulating the training procedure from the trajectory optimization perspective. We first show that most widely-used algorithms for training DNNs can be linked to the Differential Dynamic Programming (DDP), a celebrated second-order method rooted in the Approximate Dynamic Programming. In this vein, we propose a new class of optimizer, DDP Neural Optimizer (DDP-NOpt), for training feedforward and convolution networks. DDPNOpt features layer-wise feedback policies which improve convergence and reduce sensitivity to hyper-parameter over existing methods. It outperforms other optimal-control inspired training methods in both convergence and complexity, and is competitive against state-of-the-art first and second order methods. We also observe DDPNOpt has surprising benefit in preventing gradient vanishing. Our work opens up new avenues for principled algorithmic design built upon the optimal control theory.",
    "prev": "While the former is characterized by two coupled partial differential equations (PDEs) (Léonard, 2013), the latter integrates over a notably complex SDE that resembles neither its diffusion nor reversed process (Song et al., 2021).",
    "curr": "Nevertheless, inspired by the recent advance on understanding deep learning through the optimal control perspective (Li & Hao, 2018;Liu et al., 2021a;b), we show that Forward-Backward SDEs -a mathematical methodology appearing in stochastic optimal control for solving nonlinear PDEs (Han et al., 2018) -paves an elegant way to connect the two objectives.",
    "next": "The implication of our findings is nontrivial: It yields an exact log-likelihood expression of SB that precisely generalizes the one of SGM (Song et al., 2021) to fully nonlinear diffusion, thereby providing novel theoretical connections between the two model classes."
  },
  {
    "index": 1700,
    "source_corpus_id": 238354231,
    "ref_id": "b30",
    "citation_corpus_id": 56657912,
    "start": 19776,
    "end": 19780,
    "title": "BENCHMARKING NEURAL NETWORK ROBUSTNESS TO COMMON CORRUPTIONS AND PERTURBATIONS",
    "abstract": "In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, IMAGENET-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called IMAGENET-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.",
    "prev": "We consider input perturbations that are common in the literature: (a) white noise; (b) salt and pepper; and (c) adversarial perturbations (see Section F).",
    "curr": "We evaluate the average performance of NFM with different model architectures on CIFAR-10 [38], CIFAR-100 [38], ImageNet [13], and CIFAR-10c [31].",
    "next": "We use a pre-activated residual network (ResNet) with depth 18 [ [36]; and noisy mixup [76]."
  },
  {
    "index": 1701,
    "source_corpus_id": 226278174,
    "ref_id": "b27",
    "citation_corpus_id": 211126567,
    "start": 4863,
    "end": 4880,
    "title": "Published as a conference paper at ICLR 2020 QUERY2BOX: REASONING OVER KNOWLEDGE GRAPHS IN VECTOR SPACE USING BOX EMBEDDINGS",
    "abstract": "Answering complex logical queries on large-scale incomplete knowledge graphs (KGs) is a fundamental yet challenging task. Recently, a promising approach to this problem has been to embed KG entities as well as the query into a vector space such that entities that answer the query are embedded close to the query. However, prior work models queries as single points in the vector space, which is problematic because a complex query represents a potentially large set of its answer entities, but it is unclear how such a set can be represented as a single point. Furthermore, prior work can only handle queries that use conjunctions (∧) and existential quantifiers (∃). Handling queries with logical disjunctions (∨) remains an open problem. Here we propose QUERY2BOX, an embedding-based framework for reasoning over arbitrary queries with ∧, ∨, and ∃ operators in massive and incomplete KGs. Our main insight is that queries can be embedded as boxes (i.e., hyper-rectangles), where a set of points inside the box corresponds to a set of answer entities of the query. We show that conjunctions can be naturally represented as intersections of boxes and also prove a negative result that handling disjunctions would require embedding with dimension proportional to the number of KG entities. However, we show that by transforming queries into a Disjunctive Normal Form, QUERY2BOX is capable of handling arbitrary logical queries with ∧, ∨, ∃ in a scalable manner. We demonstrate the effectiveness of QUERY2BOX on three large KGs and show that QUERY2BOX achieves up to 25% relative improvement over the state of the art. * Equal contributions. Project website with data and code: http://snap.stanford.edu/ query2box",
    "prev": "We then propose two approaches for identifying the most likely values for the variable nodes in a query -either by continuous or by combinatorial optimisation.",
    "curr": "Recent work on embedding logical queries on KGs (Hamilton et al., 2018;Daza & Cochez, 2020;Ren et al., 2020) has suggested that in order to go beyond link prediction, more elaborate architectures, and a large and diverse dataset with millions of queries is required.",
    "next": "In this work, we show that this is not the case, and demonstrate that it is possible to use an efficient neural link predictor trained for 1-hop query answering, to generalise to up to 8 complex query structures."
  },
  {
    "index": 1702,
    "source_corpus_id": 249240397,
    "ref_id": "b48",
    "citation_corpus_id": 11758569,
    "start": 2369,
    "end": 2391,
    "title": "UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks -demonstrating their applicability as general image representations.arXiv:1511.06434v2 [cs.LG] 7 Jan 2016Under review as a conference paper at ICLR 2016• We show that the generators have interesting vector arithmetic properties allowing for easy manipulation of many semantic qualities of generated samples.",
    "prev": "Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) constitute the state of the art (SOTA) for the task of image synthesis.",
    "curr": "However, despite the remarkable progress in this domain through improvements to the image generator's architecture (Radford et al., 2016;Karras et al., 2018;2020b;Brock et al., 2019), their inner workings remain to a large extent unexplored.",
    "next": "Developing a better understanding of the way in which high-level concepts are represented and composed to form synthetic images is important for a number of downstream tasks such as generative model interpretability (Shen et al., 2020a;Bau et al., 2019;Yang et al., 2021) and image editing (Härkönen et al., 2020;Shen et al., 2020c;Tzelepis et al., 2021;Bau et al., 2020)."
  },
  {
    "index": 1703,
    "source_corpus_id": 52895409,
    "ref_id": "b22",
    "citation_corpus_id": 3568073,
    "start": 32598,
    "end": 32619,
    "title": "PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION",
    "abstract": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at 1024 2 . We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CELEBA dataset.",
    "prev": "), CelebA (Liu et al.",
    "curr": "(2015)), and CelebAHQ (Karras et al., 2018) datasets.",
    "next": "We compare our approach to recent stabilization techniques: WGAN-GP , instance noise , spectral normalization (SN) , and gradient penalty (GP) , as well as the original GAN  on CIFAR-10."
  },
  {
    "index": 1705,
    "source_corpus_id": 10756563,
    "ref_id": "b11",
    "citation_corpus_id": 6628106,
    "start": 14193,
    "end": 14211,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "No noticeable improvement was observed.",
    "curr": "Throughout the experiments, the adaptive learning rate method Adam by Kingma & Ba (2016) is used as the optimization algorithm.",
    "next": "The architecture of f consists of four convolutional layers with 64, 128, 256, 128 filters respectively, each followed by max pooling and ReLU non-linearity."
  },
  {
    "index": 1706,
    "source_corpus_id": 260551862,
    "ref_id": "b33",
    "citation_corpus_id": 604334,
    "start": 1504,
    "end": 1526,
    "title": "Intriguing properties of neural networks",
    "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
    "prev": "We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.",
    "curr": "Code is available at\n\nINTRODUCTION\n\nAdversarial examples are examples that are created by making small perturbations to the input designed to significantly increase the loss incurred by a machine learning model (Szegedy et al., 2014;Goodfellow et al., 2015).",
    "next": "Several models, including state of the art convolutional neural networks, lack the ability to classify adversarial examples correctly, sometimes even when the adversarial perturbation is constrained to be so small that a human observer cannot perceive it."
  },
  {
    "index": 1708,
    "source_corpus_id": 262217283,
    "ref_id": "b31",
    "citation_corpus_id": 251442769,
    "start": 4896,
    "end": 4899,
    "title": "Published as a conference paper at ICLR 2023 SIMPLIFIED STATE SPACE LAYERS FOR SEQUENCE MODELING",
    "abstract": "Models using structured state space sequence (S4) layers have achieved state-ofthe-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages 87.4% on the long range arena benchmark, and 98.5% on the most difficult Path-X task. Re. Efficiently modeling long sequences with structured state",
    "prev": "In biological neural networks, the emergence of diverse neural timescales occurs via a variety of interacting mechanisms.The timescales of individual neurons' activity are determined by cellular and synaptic processes, such as the membrane or synaptic time constants that vary across brain areas and neuron types [19,20].These single-neuron timescales characterize the timescale of neural activity in the absence of recurrent interactions.However, neurons are embedded within rich network structures that also shape neural dynamics introducing network-mediated timescales.The strength of recurrent connections [21,22,23], as well as their topology [24,25,7,26] give rise to network-mediated timescales that can be much longer than single-neuron timescales.",
    "curr": "Heterogeneous and tunable single-neuron timescales have been proposed as a mechanism to adapt the timescales of RNN dynamics to task requirements and improve their performance [27,28,29,30,31,32].In these studies, the time constants of individual neurons are trained together with network connectivity.For tasks with long temporal dependencies, the distribution of trained single-neuron timescales becomes heterogeneous according to the task's memory requirements [27].Explicit training of single-neuron timescales improves network performance in benchmark RNN tasks in rate [28,29] and spiking [30,31,27] networks and leads to greater robustness [27] and adaptability to novel stimuli [32].While these studies propose the adaptability of single-neuron timescales as a potential mechanism for solving time-dependent tasks, the exact contribution of single-neuron and network-mediated timescales in optimally solving the tasks is unknown.",
    "next": "Here, we study how single-neuron and network-mediated timescales shape the dynamics and performance of RNNs trained on long-memory tasks.We show that the extent to which longer single-neuron or network-mediated timescales are necessary for solving such tas"
  },
  {
    "index": 1710,
    "source_corpus_id": 5834589,
    "ref_id": "b12",
    "citation_corpus_id": 6628106,
    "start": 6486,
    "end": 6505,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "The vector of weights is denoted by x and is subscripted by k to denote an iteration.",
    "curr": "We use the term small-batch (SB) method to denote SGD, or one of its variants like ADAM (Kingma & Ba, 2015) and ADAGRAD (Duchi et al., 2011), with the proviso that the gradient approximation is based on a small mini-batch.",
    "next": "In our setup, the batch B k is randomly sampled and its size is kept fixed for every iteration."
  },
  {
    "index": 1712,
    "source_corpus_id": 257365136,
    "ref_id": "b24",
    "citation_corpus_id": 40100965,
    "start": 1609,
    "end": 1630,
    "title": "Universal Language Model Fine-tuning for Text Classification",
    "abstract": "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100× more data. We opensource our pretrained models and code 1 .",
    "prev": "1 * Work done during an internship at MIT-IBM Watson AI Lab.",
    "curr": "INTRODUCTION\n\nFinetuning pretrained language models (PLMs) has led to significant improvements across various downstream NLP tasks Howard & Ruder, 2018;Raffel et al., 2020).",
    "next": "However, the conventional paradigm of full task-specific finetuning (FT) is difficult to scale to multiple tasks, given that modern PLMs can have hundreds of millions (or even billions) of parameters."
  },
  {
    "index": 1715,
    "source_corpus_id": 174801410,
    "ref_id": "b19",
    "citation_corpus_id": 53221030,
    "start": 1967,
    "end": 1988,
    "title": "JANOSSY POOLING: LEARNING DEEP PERMUTATION- INVARIANT FUNCTIONS FOR VARIABLE-SIZE INPUTS",
    "abstract": "We consider a simple and overarching representation for permutation-invariant functions of sequences (or set functions). Our approach, which we call Janossy pooling, expresses a permutation-invariant function as the average of a permutation-sensitive function applied to all reorderings of the input sequence. This allows us to leverage the rich and mature literature on permutation-sensitive functions to construct novel and flexible permutation-invariant functions. If carried out naively, Janossy pooling can be computationally prohibitive. To allow computational tractability, we consider three kinds of approximations: canonical orderings of sequences, functions with k-order interactions, and stochastic optimization algorithms with random permutations. Our framework unifies a variety of existing work in the literature, and suggests possible modeling and algorithmic extensions. We explore a few in our experiments, which demonstrate improved performance over current state-of-the-art methods.",
    "prev": "While there has been recent progress on learning such functions (Zaheer et al., 2017;Qi et al., 2017), they compress a set of any size down to a single feature vector in one step.",
    "curr": "This can be a significant bottleneck in what these functions can represent efficiently, particularly when relations between elements of the set need to be modeled (Murphy et al., 2019;Zhang et al., 2019b).",
    "next": "Decoder: This turns the latent space back into a set."
  },
  {
    "index": 1718,
    "source_corpus_id": 199577786,
    "ref_id": "b14",
    "citation_corpus_id": 1957433,
    "start": 6784,
    "end": 6809,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "2 An RL-based Generator-Evaluator Architecture Given a passage X p \" tx p 1 , x p 2 , ..., x p N u and a target answer X a \" tx a 1 , x a 2 , ..., x a L u, the task of natural question generation is to generate the best natural language questionŶ \" ty 1 , y 2 , ..., y T u which maximizes the conditional likelihoodŶ \" argmax Y P pY |X p , X a q.",
    "curr": "We use 300-dim GloVe (Pennington et al., 2014) embeddings and 1024-dim BERT (Devlin et al., 2018) embeddings to embed each word in the passage and the answer.",
    "next": "Let us denote g p i and g a j as the GloVe embeddings of passage word x p i and answer word x a j , respectively."
  },
  {
    "index": 1720,
    "source_corpus_id": 235313572,
    "ref_id": "b30",
    "citation_corpus_id": 5834589,
    "start": 5164,
    "end": 5185,
    "title": "ON LARGE-BATCH TRAINING FOR DEEP LEARNING: GENERALIZATION GAP AND SHARP MINIMA",
    "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say 32-512 data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions-and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap. Published as a conference paper at ICLR 2017 These methods minimize the objective function f by iteratively taking steps of the form: J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. . Weak sharp minima and penalty functions in mathematical programming. PhD thesis, University of Cambridge, 1988.Michael P Friedlander and Mark Schmidt. Hybrid deterministic-stochastic methods for data fitting.",
    "prev": "The first-order optimizers (e.g., SGD and Adam (Kingma & Ba, 2015)) only seek the model parameters that minimize the training error.",
    "curr": "They dismiss the higher-order information such as flatness that correlates with generalization (Keskar et al., 2017;Kleinberg et al., 2018;Jastrzębski et al., 2019;Smith & Le, 2018;Chaudhari et al., 2017).",
    "next": "The above study and reasoning lead us to the recently proposed sharpness-aware minimizer (SAM) (Foret et al., 2021) that explicitly smooths the loss geometry during model training."
  },
  {
    "index": 1721,
    "source_corpus_id": 251647798,
    "ref_id": "b45",
    "citation_corpus_id": 12713052,
    "start": 2868,
    "end": 2885,
    "title": "NEURAL ARCHITECTURE SEARCH WITH REINFORCEMENT LEARNING",
    "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214. * Work done as a member of the Google Brain Residency program (g.co/brainresidency.) Under review as a conference paper at ICLR 2017 neural network can be typically specified by a variable-length string. It is therefore possible to use a recurrent network -the controller -to generate such string. Training the network specified by the string -the \"child network\" -on the real data will result in an accuracy on a validation set. Using this accuracy as the reward signal, we can compute the policy gradient to update the controller. As a result, in the next iteration, the controller will give higher probabilities to architectures that receive high accuracies. In other words, the controller will learn to improve its search over time.Our experiments show that Neural Architecture Search can design good models from scratch, an achievement considered not possible with other methods. On image recognition with CIFAR-10, Neural Architecture Search can find a novel ConvNet model that is better than most human-invented architectures. Our CIFAR-10 model achieves a 3.65 test set error, while being 1.05x faster than the current best model. On language modeling with Penn Treebank, Neural Architecture Search can design a novel recurrent cell that is also better than previous RNN and LSTM architectures. The cell that our model found achieves a test set perplexity of 62.4 on the Penn Treebank dataset, which is 3.6 perplexity better than the previous state-of-the-art.",
    "prev": "The most common way of constructing a deep CNN is to stack a number of convolutional layers as well as other basic layers organized with the predefined feature connection topology.",
    "curr": "Along with great advances in CNN architecture design by manual engineering (Krizhevsky et al., 2012;He et al., 2016;Howard et al., 2017) and automatic searching (Zoph & Le, 2017;Pham et al., 2018;Howard et al., 2019), lots of prevailing classification backbones have been presented.",
    "next": "Recent works Hu et al., 2018b;Chen et al., 2020) show that incorporating attention mechanisms into convolutional blocks can further push the performance boundaries of modern CNNs, and thus it has attracted great research interest in the deep learning community."
  },
  {
    "index": 1722,
    "source_corpus_id": 52889459,
    "ref_id": "b10",
    "citation_corpus_id": 3398677,
    "start": 4569,
    "end": 4588,
    "title": "Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step",
    "abstract": "Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players' parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step. * Equal contribution.",
    "prev": "Without auxiliary stabilization techniques, this training procedure is notoriously brittle, requiring finely-tuned hyperparameters and architectural choices to work at all.",
    "curr": "Much recent research has accordingly focused on modifications to the vanilla GAN procedure to impart stability, drawing on a growing body of empirical and theoretical insights (Nowozin et al., 2016;Sønderby et al., 2017;Fedus et al., 2018).",
    "next": "One line of work is focused on changing the objective function Mao et al., 2016;Lim & Ye, 2017;Bellemare et al., 2017;Salimans et al., 2018) to encourage convergence."
  },
  {
    "index": 1723,
    "source_corpus_id": 238419164,
    "ref_id": "b4",
    "citation_corpus_id": 52255687,
    "start": 3282,
    "end": 3307,
    "title": "Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science",
    "abstract": "In this paper, we propose data statements as a design solution and professional practice for natural language processing technologists, in both research and development. Through the adoption and widespread use of data statements, the field can begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. We present a form that data statements can take and explore the implications of adopting them as part of regular practice. We argue that data statements will help alleviate issues related to exclusion and bias in language technology, lead to better precision in claims about how natural language processing research can generalize and thus better engineering results, protect companies from public embarrassment, and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.",
    "prev": "Specifically for machine learning, recent reproducibility initiatives (Pineau et al., 2021) nicely summarize how differences in used data, miss-or under-specification of training and evaluation metrics, along with frequent over-claims of conclusions beyond gathered empirical evidence impose persisting obstacles in our current literature.",
    "curr": "Similar conclusions have been reached in related works focused on specifics of reinforcement learning (Li & Talwalkar, 2019), neural architecture search (Lindauer & Hutter, 2020), human-centered machine learning model cards (Mitchell et al., 2019), or general dataset sheet specifications (Bender & Friedman, 2018;Gebru et al., 2018), which all make valuable propositions to overcome existing gaps through the creation of standardized best-practice (check-)lists.",
    "next": "It should thus come as no surprise that the emerging work in continual learning is no stranger to the above challenges."
  },
  {
    "index": 1725,
    "source_corpus_id": 221136343,
    "ref_id": "b2",
    "citation_corpus_id": 3495200,
    "start": 2036,
    "end": 2039,
    "title": "LEARNING TO REPRESENT PROGRAMS WITH GRAPHS",
    "abstract": "Learning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code's known syntax. For example, long-range dependencies induced by using the same variable or function in distant locations are often not considered. We propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures. In this work, we present how to construct graphs from source code and how to scale Gated Graph Neural Networks training to such large graphs. We evaluate our method on two tasks: VARNAMING, in which a network attempts to predict the name of a variable given its usage, and VARMISUSE, in which the network learns to reason about selecting the correct variable that should be used at a given program location. Our comparison to methods that use less structured program representations shows the advantages of modeling known structure, and suggests that our models learn to infer meaningful names and to solve the VARMISUSE task in many cases. Additionally, our testing showed that VARMISUSE identifies a number of bugs in mature open-source projects.",
    "prev": "Introduction\n\nGraph neural networks (GNNs) have received substantial attention in recent years due to their ability to model irregularly structured data.",
    "curr": "As a result, they are extensively used for applications as diverse as molecular interactions [10,44], social networks [15], recommendation systems [38] or program understanding [3].",
    "next": "Recent advancements have centered around building more sophisticated models [30,15,40], including new types of layers [24,39,45] and better aggregation functions [8]."
  },
  {
    "index": 1728,
    "source_corpus_id": 252438904,
    "ref_id": "b52",
    "citation_corpus_id": 248476097,
    "start": 7038,
    "end": 7044,
    "title": "FAST SAMPLING OF DIFFUSION MODELS WITH EXPO- NENTIAL INTEGRATOR",
    "abstract": "The past few years have witnessed the great success of Diffusion models (DMs) in generating high-fidelity samples in generative modeling tasks. A major limitation of the DM is its notoriously slow sampling procedure which normally requires hundreds to thousands of time discretization steps of the learned diffusion process to reach the desired accuracy. Our goal is to develop a fast sampling method for DMs with fewer steps while retaining high sample quality. To this end, we systematically analyze the sampling procedure in DMs and identify key factors that affect the sample quality, among which the method of discretization is most crucial. By carefully examining the learned diffusion process, we propose Diffusion Exponential Integrator Sampler (DEIS). It is based on the Exponential Integrator designed for discretizing ordinary differential equations (ODEs) and leverages a semilinear structure of the learned diffusion process to reduce the discretization error. The proposed method can be applied to any DMs and can generate highfidelity samples in as few as 10 steps. Moreover, by directly using pre-trained DMs, we achieve state-of-art sampling performance when the number of score function evaluation (NFE) is limited, e.g., 4.17 FID with 10 NFEs, 2.86 FID with only 20 NFEs on CIFAR10. Project page and code: https://qsh-zh.github.io/deis.",
    "prev": "The bounds require that the score estimate is L ∞ -accurate (i.e., uniformly accurate), as opposed to L 2 -accurate (see, e.g., [De +21]).",
    "curr": "This is particularly problematic because the score matching objective is an L 2 loss (see Section 2 for details), and there are empirical studies suggesting that in practice, the score estimate is not in fact L ∞ -accurate (e.g., [ZC23]).",
    "next": "Intuitively, this is because we cannot expect that the score estimate we obtain in practice will be accurate in regions of space where the true density is very low, simply because we do not expect to see many (or indeed, any) samples from such regions."
  },
  {
    "index": 1730,
    "source_corpus_id": 49672236,
    "ref_id": "b10",
    "citation_corpus_id": 3488815,
    "start": 2732,
    "end": 2735,
    "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
    "abstract": "Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. * Authors ordered alphabetically.",
    "prev": "The problem of finding an adversarial example can be posed as solving an optimization problem-within a small neighbourhood around the original example, find a point to optimize the cost function measuring the \"successfulness\" of an attack.",
    "curr": "Solving this objective function with gradient-based optimizer leads to state-of-the-art attacks [9,3,10,4,11].",
    "next": "Most current attacks [3,9,4,12] consider the \"white-box\" setting, where the machine learning model is fully exposed to the attacker."
  },
  {
    "index": 1731,
    "source_corpus_id": 247595275,
    "ref_id": "b52",
    "citation_corpus_id": 216562627,
    "start": 7606,
    "end": 7626,
    "title": "Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels",
    "abstract": "We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to regularize the value function. Existing model-free approaches, such as Soft Actor-Critic (SAC)[20], are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based[21,31,22]methods and recently proposed contrastive learning[42]. Our approach can be combined with any model-free reinforcement learning algorithm, requiring only minor modifications. An implementation can be found at https://sites.google.com/view/data-regularized-q.IntroductionSample-efficient deep reinforcement learning (RL) algorithms capable of directly training from image pixels would open up many real-world applications in control and robotics. However, simultaneously training a convolutional encoder alongside a policy network is challenging when given limited environment interaction, strong correlation between samples and a typically sparse reward signal. Naive attempts to use a large capacity encoder result in severe over-fitting (seeFigure 1a) and smaller encoders produce impoverished representations that limit task performance.Limited supervision is a common problem across AI and a number of approaches are adopted: (i) pretraining with self-supervised learning (SSL), followed by standard supervised learning; (ii) supervised learning with an additional auxiliary loss and (iii) supervised learning with data augmentation. SSL approaches are highly effective in the large data regime, e.g. in domains such as vision [7, 23] and NLP[12,13]where large (unlabeled) datasets are readily available. However, in RL this is not the case: an off-policy RL agent is trained on a replay buffer that grows as the agent interacts with the environment. But in a sample efficient regime the buffer may only hold 10 4 -10 5 transitions from a few hundred trajectories, limiting the effectiveness of SSL methods.A wide range of auxiliary loss functions have been proposed to augment supervised objectives, e.g. weight regularization, noise injection [25], or some form of online SSL objective. In RL, * Equal contribution. Author ordering determined by coin flip. Both authors are corresponding.Preprint. Under review.",
    "prev": "Data augmentation for RL.",
    "curr": "In the context of RL, data augmentation has been widely investigated for improving data-efficiency Yarats et al., 2021), or RL generalization (Cobbe et al., 2019;Lee et al., 2019).",
    "next": "For example, RAD  demonstrated that data augmentation, such as random crop, can improve both data-efficiency and generalization of RL algorithms."
  },
  {
    "index": 1732,
    "source_corpus_id": 215786510,
    "ref_id": "b0",
    "citation_corpus_id": 49411844,
    "start": 2142,
    "end": 2145,
    "title": "DARTS: Differentiable Architecture Search",
    "abstract": "This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques.",
    "prev": "Introduction\n\nNeural architecture search has become an important tool for automating machine learning (ML) [43,9] but can require hundreds of thousands of GPU-hours to train and contribute hundreds of thousands of kilograms to CO 2 emissions [46].",
    "curr": "Recently, approaches based on weight-sharing have achieved state-of-the-art performance on canonical image classification and language modeling problems while drastically reducing the computational cost of NAS to just that of training a single sharedweights network [40,29,32].",
    "next": "Crucially, methods such as DARTS [32], GDAS [17], and many others [40,57,51,39,48,31,27,9,37,3,49] combine weight-sharing with a continuous relaxation of the discrete search space to allow cheap gradient updates to architecture hyperparameters, enabling the use of popular optimizers such as stochastic gradient descent (SGD) or Adam [23]."
  },
  {
    "index": 1734,
    "source_corpus_id": 263831292,
    "ref_id": "b11",
    "citation_corpus_id": 210064473,
    "start": 2687,
    "end": 2704,
    "title": "SPACE: UNSUPERVISED OBJECT-ORIENTED SCENE REPRESENTATION VIA SPATIAL ATTENTION AND DECOMPOSITION",
    "abstract": "The ability to decompose complex multi-object scenes into meaningful abstractions like objects is fundamental to achieve higher-level cognition. Previous approaches for unsupervised object-oriented scene representation learning are either based on spatial-attention or scene-mixture approaches and limited in scalability which is a main obstacle towards modeling real-world scenes. In this paper, we propose a generative latent variable model, called SPACE, that provides a unified probabilistic modeling framework that combines the best of spatial-attention and scene-mixture approaches. SPACE can explicitly provide factorized object representations for foreground objects while also decomposing background segments of complex morphology. Previous models are good at either of these, but not both. SPACE also resolves the scalability problems of previous methods by incorporating parallel spatial-attention and thus is applicable to scenes with a large number of objects without performance degradations. We show through experiments on Atari and 3D-Rooms that SPACE achieves the above properties consistently in comparison to SPAIR, IODINE, and GENESIS. Results of our experiments can be found on our project website: https://sites.google.com/view/space-project-page * Visiting Student at Rutgers University. Authors named inside {} equally contributed. Correspondance to",
    "prev": "INTRODUCTION\n\nDespite tremendous advances in machine learning, a large gap still exists between humans and machines in terms of learning efficiency and generalization (Tenenbaum et al., 2011;Behrens et al., 2018;Schölkopf et al., 2021).A key reason for this is thought to be that machines lack the ability to generalize compositionally, which humans heavily rely on (Fodor and Pylyshyn, 1988;Lake et al., 2017;Battaglia et al., 2018;Goyal and Bengio, 2022;Greff et al., 2020).Namely, humans are able to recompose previously learned knowledge to generalize to never-before-seen situations.",
    "curr": "Significant work has thus gone into the problem of learning representations that can generalize compositionally.One prominent effort is object-centric representation learning (Burgess et al., 2019;Greff et al., 2019;Locatello et al., 2020a;Lin et al., 2020;Singh et al., 2022;Elsayed et al., 2022;Seitzer et al., 2023), which aims to represent each object in an image via a distinct subset of the image's latent code.Due to this modular structure, object-centric representations are widely conjectured to enable compositional generalization (Battaglia et al., 2018;Kipf et al., 2020;Greff et al., 2020;Locatello et al., 2020a).Yet, it remains unclear when this conjecture will actually be true.This is because a theoretical understanding of compositional generalization for unsupervised objectcentric representations is lacking, and empirical methods are frequently not scrutinized for their ability to generalize compositionally.Consequently, it is uncertain to what extent recent advancements in object-centric learning promote compositional generalization and what obstacles still need to be overcome.",
    "next": "In this work, we take a step towards addressing this gap by investigating theoretically when compositional generalization is possible in object-centric"
  },
  {
    "index": 1735,
    "source_corpus_id": 246823979,
    "ref_id": "b17",
    "citation_corpus_id": 196831582,
    "start": 5152,
    "end": 5174,
    "title": "Published as a conference paper at ICLR 2020 ON THE \"STEERABILITY\" OF GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "An open secret in contemporary machine learning is that many models work beautifully on standard benchmarks but fail to generalize outside the lab. This has been attributed to biased training data, which provide poor coverage over real world events. Generative models are no exception, but recent advances in generative adversarial networks (GANs) suggest otherwise -these models can now synthesize strikingly realistic and diverse images. Is generative modeling of photos a solved problem? We show that although current GANs can fit standard datasets very well, they still fall short of being comprehensive models of the visual manifold. In particular, we study their ability to fit simple transformations such as camera movements and color changes. We find that the models reflect the biases of the datasets on which they are trained (e.g., centered objects), but that they also exhibit some capacity for generalization: by \"steering\" in latent space, we can shift the distribution while still creating realistic images. We hypothesize that the degree of distributional shift is related to the breadth of the training data distribution. Thus, we conduct experiments to quantify the limits of GAN transformations and introduce techniques to mitigate the problem. Code is released on our project page:",
    "prev": "In DisCo, changing a factor is implemented by traversing one discovered direction in the latent space.",
    "curr": "For discovering the factors, DisCo adopts a typical network module, Navigator, to provides candidate traversal directions in the latent space (Voynov & Babenko, 2020;Jahanian et al., 2020;Shen et al., 2020).",
    "next": "For disentangled representation learning, to model the various image variations, we propose a novel ∆-Contrastor to build a Variation Space where we apply the contrastive loss."
  },
  {
    "index": 1736,
    "source_corpus_id": 238531782,
    "ref_id": "b22",
    "citation_corpus_id": 52909341,
    "start": 2510,
    "end": 2532,
    "title": "NEAR-OPTIMAL REPRESENTATION LEARNING FOR HIERARCHICAL REINFORCEMENT LEARNING",
    "abstract": "We study the problem of representation learning in goal-conditioned hierarchical reinforcement learning. In such hierarchical structures, a higher-level controller solves tasks by iteratively communicating goals which a lower-level policy is trained to reach. Accordingly, the choice of representation -the mapping of observation space to goal space -is crucial. To study this problem, we develop a notion of sub-optimality of a representation, defined in terms of expected reward of the optimal hierarchical policy using this representation. We derive expressions which bound the sub-optimality and show how these expressions can be translated to representation learning objectives which may be optimized in practice. Results on a number of difficult continuous-control tasks show that our approach to representation learning yields qualitatively better representations as well as quantitatively better hierarchical policies, compared to existing methods. 1",
    "prev": "Early GCHRL studies used a hand-designed subgoal space, such as positions of robots (Nachum et al., 2018;Levy et al., 2019) or objects in images (Kulkarni et al., 2016a).",
    "curr": "To alleviate the dependency on domain-specific knowledge, recent works investigate learning subgoal representations along with hierarchical policies (Nachum et al., 2019a;Dilokthanakul et al., 2019;Li et al., 2021), which have shown promise in a more general setting.",
    "next": "Although the exploration ability of HRL is boosted via decomposing tasks and reusing low-level policies, existing GCHRL methods struggle in long-horizon tasks with sparse external rewards."
  },
  {
    "index": 1739,
    "source_corpus_id": 18114929,
    "ref_id": "b21",
    "citation_corpus_id": 5959482,
    "start": 2209,
    "end": 2212,
    "title": "Efficient Estimation of Word Representations in Vector Space",
    "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities. arXiv:1301.3781v3 [cs.CL] 7 Sep 2013 1 The test set is available at www.fit.vutbr.cz/˜imikolov/rnnlm/word-test.v1.txt 2",
    "prev": "INTRODUCTION\n\nDistributed representations are embeddings of words in a real vector space, achieved via an appropriate function that models the interaction between neighboring words in sentences (e.g.",
    "curr": ": neural networks [6,23,13], log-bilinear models [25,22], co-occurrence statistics [28,19]).",
    "next": "Such an approach has been strikingly successful in capturing the syntactic and semantic similarity between words (and pairs of words), via simple linear algebraic relations between their corresponding vector representations."
  },
  {
    "index": 1742,
    "source_corpus_id": 259936734,
    "ref_id": "b3",
    "citation_corpus_id": 222067132,
    "start": 2938,
    "end": 2940,
    "title": "Published as a conference paper at ICLR 2021 RETHINKING ATTENTION WITH PERFORMERS",
    "abstract": "We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attentionkernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can also be used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers. * Equal contribution. Correspondence to {kchoro,lcolwell}@google.com. Code for Transformer models on protein data can be found in github.com/google-research/ google-research/tree/master/protein_lm and Performer code can be found in github.com/ google-research/google-research/tree/master/performer. Google AI Blog: https:// ai.googleblog.com/2020/10/rethinking-attention-with-performers.html Published as a conference paper at ICLR 2021 layers(Child et al., 2019). Unfortunately, there is a lack of rigorous guarantees for the representation power produced by such methods, and sometimes the validity of sparsity patterns can only be verified empirically through trial and error by constructing special GPU operations (e.g. either writing C++ CUDA kernels (Child et al., 2019) or using TVMs(Beltagy et al., 2020)). Other techniques which aim to reduce Transformers' space complexity include reversible residual layers allowing one-time activation storage in training(Kitaev et al., 2020)and shared attention weights(Xiao et al., 2019). These constraints may impede application to long-sequence problems, where approximations of the attention mechanism are not sufficient. Approximations based on truncated back-propagation  are also unable to capture long-distance correlations since the gradients are only propagated inside a localized window. Other methods propose biased estimation of regular attention but only in the non-causal setting and with large mean squared error .In response, we introduce the first Transformer architectures, Performers, capable of provably accurate and practical estimation of regular (softmax) full-rank attention, but of only linear space and time complexity and not relying on any priors such as sparsity or low-rankness. Performers use the Fast Attention Via positive Orthogonal Random features (FAVOR+) mechanism, leveraging new methods for approximating softmax and Gaussian kernels, which we propose. We believe these methods are of independent interest, contributing to the theory of scalable kernel methods. Consequently, Performers are the first linear architectures fully compatible (via small amounts of fine-tuning) with regular Transformers, providing strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and lower variance of the approximation.FAVOR+ can be also applied to efficiently model other kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, that are beyond the reach of regular Transformers, and find for them optimal attention-kernels. FAVOR+ can also be applied beyond the Transformer scope as a more scalable replacement for regular attention, which itself has a wide variety of uses in computer vision(Fu et al., 2019), reinforcement learning (Zambaldi et al., 2019), training with softmax cross entropy loss, and even combinatorial optimization (Vinyals et al., 2015).",
    "prev": "Emerging use cases such as long document querying and story writing have demonstrated a need for models with such long context.",
    "curr": "To reduce the computational requirement of attention on such long context, there have been numerous methods proposed to approximate attention [2,3,4,8,9,14,19,20].",
    "next": "Though these methods have seen some use cases, as far as we know, most large-scale training runs still use standard attention."
  },
  {
    "index": 1743,
    "source_corpus_id": 247292764,
    "ref_id": "b19",
    "citation_corpus_id": 221785486,
    "start": 2746,
    "end": 2764,
    "title": "Published as a conference paper at ICLR 2021 LEARNING FROM PROTEIN STRUCTURE WITH GEOMETRIC VECTOR PERCEPTRONS",
    "abstract": "Learning on 3D structures of large biomolecules is emerging as a distinct area in machine learning, but there has yet to emerge a unifying network architecture that simultaneously leverages the geometric and relational aspects of the problem domain. To address this gap, we introduce geometric vector perceptrons, which extend standard dense layers to operate on collections of Euclidean vectors. Graph neural networks equipped with such layers are able to perform both geometric and relational reasoning on efficient representations of macromolecules. We demonstrate our approach on two important problems in learning from protein structure: model quality assessment and computational protein design. Our approach improves over existing classes of architectures on both problems, including state-ofthe-art convolutional neural networks and graph neural networks. We release our code at https://github.com/drorlab/gvp.",
    "prev": "Despite its effectiveness in various applications, a more intrinsic and informative representation for molecules is the 3D geometry, also known as conformation, where atoms are represented as their Cartesian coordinates.",
    "curr": "The 3D structures determine the biological and physical properties of molecules and hence play a key role in many applications such as computational drug and material design (Thomas et al., 2018;Gebauer et al., 2021;Jing et al., 2021;Batzner et al., 2021).",
    "next": "Unfortunately, how to predict stable molecular conformation remains a challenging problem."
  },
  {
    "index": 1744,
    "source_corpus_id": 203736530,
    "ref_id": "b21",
    "citation_corpus_id": 3708505,
    "start": 9156,
    "end": 9173,
    "title": "DEEP NEURAL NETWORKS AS GAUSSIAN PROCESSES",
    "abstract": "A deep fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP) in the limit of infinite network width. This correspondence enables exact Bayesian inference for neural networks on regression tasks by means of straightforward matrix computations. For single hiddenlayer networks, the covariance function of this GP has long been known. Recently, kernel functions for multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified the correspondence between using these kernels as the covariance function for a GP and performing fully Bayesian prediction with a deep neural network. In this work, we derive this correspondence and develop a computationally efficient pipeline to compute the covariance functions. We then use the resulting GP to perform Bayesian inference for deep neural networks on MNIST and CIFAR-10. We find that the GP-based predictions are competitive and can outperform neural networks trained with stochastic gradient descent. We observe that the trained neural network accuracy approaches that of the corresponding GP-based computation with increasing layer width, and that the GP uncertainty is strongly correlated with prediction error. We connect our observations to the recent development of signal propagation in random neural networks. * Both authors contributed equally to this work. † Work done as a member of the Google Brain Residency program (g.co/brainresidency). 1 Throughout this paper, we assume the conditions on the parameter distributions and nonlinearities are such that the Central Limit Theorem will hold; for instance, that the weight variance is scaled inversely proportional to the layer width.",
    "prev": "The focus of this paper, neural tangent kernel is induced from a neural network architecture.",
    "curr": "The connection between infinitely wide neural networks and kernel methods is not new (Neal, 1996;Williams, 1997;Roux & Bengio, 2007;Hazan & Jaakkola, 2015;Lee et al., 2018;Matthews et al., 2018;Novak et al., 2019;Garriga-Alonso et al., 2019;Cho & Saul, 2009;Daniely et al., 2016;Daniely, 2017).",
    "next": "However, these kernels correspond to neural network where only the last layer is trained."
  },
  {
    "index": 1745,
    "source_corpus_id": 51979536,
    "ref_id": "b26",
    "citation_corpus_id": 5037032,
    "start": 3358,
    "end": 3361,
    "title": "ZERO-SHOT VISUAL IMITATION",
    "abstract": "The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both what and how to imitate.We pursue an alternative paradigm wherein an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss.In our framework, the role of the expert is only to communicate the goals (i.e., what to imitate) during inference.The learned policy is then employed to mimic the expert (i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task.Our method is \"zero-shot\" in the sense that the agent never has access to expert actions during training or for the task demonstration at inference.We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen office environments with a TurtleBot.Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance.Videos, models, and more details are available at https://pathak22.github.io/zeroshot-imitation/. * Denotes equal contribution.",
    "prev": "There are plenty of other examples, from playing Minecraft to visiting your local zoo, where no extrinsic rewards are required.",
    "curr": "Indeed, there is evidence that pre-training an agent on a given environment using only intrinsic rewards allows it to learn much faster when fine-tuned to a novel task in a novel environment [27,28].",
    "next": "Yet, so far, there has been no systematic study of learning with only intrinsic rewards."
  },
  {
    "index": 1747,
    "source_corpus_id": 225040049,
    "ref_id": "b23",
    "citation_corpus_id": 28202810,
    "start": 2452,
    "end": 2456,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "This algorithm has a disarmingly simple approach to the optimization problem at hand: given a parametrization of the policy, it updates the parameters in the direction of steepest ascent of the associated integrated value function.",
    "curr": "Impressive progress has been made recently in the understanding of the convergence and optimization properties of this class of algorithms in the tabular setting [2,8,11], in particular leveraging the natural tradeoff between exploration and exploitation offered for entropy-regularized rewards by softmax policies [24,30].",
    "next": "However, this simple algorithm alone is not sufficient to explain the multitude of recent breakthroughs in this field: in application domains such as Starcraft, robotics or movement planning, the space of possible states and actions are exceedingly large -or even continuous -and can therefore not be represented efficiently by tabular policies [23]."
  },
  {
    "index": 1748,
    "source_corpus_id": 11663659,
    "ref_id": "b3",
    "citation_corpus_id": 10872458,
    "start": 7592,
    "end": 7619,
    "title": "Reweighted Wake-Sleep",
    "abstract": "Training deep directed graphical models with many hidden variables and performing inference remains a major challenge. Helmholtz machines and deep belief networks are such models, and the wake-sleep algorithm has been proposed to train them. The wake-sleep algorithm relies on training not just the directed generative model but also a conditional generative model (the inference network) that runs backward from visible to latent, estimating the posterior distribution of latent given visible. We propose a novel interpretation of the wake-sleep algorithm which suggests that better estimators of the gradient can be obtained by sampling latent variables multiple times from the inference network. This view is based on importance sampling as an estimator of the likelihood, with the approximate inference network as a proposal distribution. This interpretation is confirmed experimentally, showing that better likelihood can be achieved with this reweighted wake-sleep procedure, which also provides a natural way to estimate the likelihood itself. Based on this interpretation, we propose that a sigmoid belief network is not sufficiently powerful for the layers of the inference network, in order to recover a good estimator of the posterior distribution of latent variables. Our experiments show that using a more powerful layer model, such as NADE, yields substantially better generative models.1",
    "prev": "Ladder variational autoencoders (Sønderby et al., 2016) increase the power of the architecture of both approximating posterior and prior.",
    "curr": "Neural adaptive importance sampling (Du et al., 2015) and reweighted wake-sleep (Bornschein & Bengio, 2015) use sophisticated approximations to the gradient of the log-likelihood that do not admit direct backpropagation.",
    "next": "Structured variational autoencoders use conjugate priors to construct powerful approximating posterior distributions (Johnson et al., 2016)."
  },
  {
    "index": 1750,
    "source_corpus_id": 259108646,
    "ref_id": "b1",
    "citation_corpus_id": 235436185,
    "start": 2817,
    "end": 2820,
    "title": "BEIT: BERT Pre-Training of Image Transformers",
    "abstract": "We introduce a self-supervised vision representation model BEIT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT [DCLT19] developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e., image patches (such as 16×16 pixels), and visual tokens (i.e., discrete tokens). We first \"tokenize\" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEIT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods.",
    "prev": "Code shall be released.",
    "curr": "Introduction\n\nImage recognition and image generation are both fundamental tasks in the field of computer vision [2,26,59,41,17,31,56,18].",
    "next": "Recognition tasks aim to perceive and understand the visual world, while generation tasks aim to create new visual data for various applications."
  },
  {
    "index": 1751,
    "source_corpus_id": 202749904,
    "ref_id": "b36",
    "citation_corpus_id": 49882757,
    "start": 2961,
    "end": 2979,
    "title": "ClariNet: Parallel Wave Generation in End-to-End Text-to-Speech",
    "abstract": "In this work, we propose an alternative solution for parallel wave generation by WaveNet. In contrast to parallel WaveNet (Oord et al., 2018), we distill a Gaussian inverse autoregressive flow from the autoregressive WaveNet by minimizing a novel regularized KL divergence between their highly-peaked output distributions. Our method computes the KL divergence in closed-form, which simplifies the training algorithm and provides very efficient distillation. In addition, we propose the first text-to-wave neural architecture for speech synthesis, which is fully convolutional and enables fast end-to-end training from scratch. It significantly outperforms the previous pipeline that connects a text-to-spectrogram model to a separately trained WaveNet (Ping et al., 2018). We also successfully distill a parallel waveform synthesizer conditioned on the hidden representation in this end-to-end model. 2 * These authors contributed equally to this work. Correspondence to <weiping.thu@gmail.com>. Our method is named after the musical instrument clarinet, whose sound resembles human voice.2 Audio samples are in https://clarinet-demo.github.io/ arXiv:1807.07281v2 [cs.CL] 30 Jul 2018",
    "prev": "A lot of recent research on neural models for TTS has focused on improving parallelism by predicting multiple time steps in parallel, e.g.",
    "curr": "using flow-based models (van den Oord et al., 2018;Ping et al., 2019;Prenger et al., 2019;Kim et al., 2019).",
    "next": "Such highly parallelisable models are more suitable to run efficiently on modern hardware."
  },
  {
    "index": 1753,
    "source_corpus_id": 203836132,
    "ref_id": "b40",
    "citation_corpus_id": 3566136,
    "start": 4940,
    "end": 4969,
    "title": "Published as a conference paper at ICLR 2018 RELATIONAL NEURAL EXPECTATION MAXIMIZATION: UNSUPERVISED DISCOVERY OF OBJECTS AND THEIR INTERACTIONS",
    "abstract": "Common-sense physical reasoning is an essential ingredient for any intelligent agent operating in the real-world. For example, it can be used to simulate the environment, or to infer the state of parts of the world that are currently unobserved. In order to match real-world conditions this causal knowledge must be learned without access to supervised data. To address this problem we present a novel method that learns to discover objects and model their physical interactions from raw visual images in a purely unsupervised fashion. It incorporates prior knowledge about the compositional nature of human perception to factor interactions between object-pairs and learn efficiently. On videos of bouncing balls we show the superior modelling capabilities of our method compared to other unsupervised neural approaches that do not incorporate such prior knowledge. We demonstrate its ability to handle occlusion and show that it can extrapolate learned knowledge to scenes with different numbers of objects. * Work performed while at IDSIA.",
    "prev": "In situations where interactions between objects are prevalent, understanding and explicitly modeling these interactions in an object-centric state-space is valuable for obtaining good predictive models (Watters et al., 2017).",
    "curr": "Existing works in this area, such as SQAIR (Kosiorek et al., 2018), DDPAE (Hsieh et al., 2018), R-NEM (Van Steenkiste et al., 2018), and COBRA  have explored these concepts, but have not demonstrated realistic long term video predictions on par with supervised approaches to modeling physics.",
    "next": "To push the limits of unsupervised learning of physical interactions, we propose STOVE, a structured, object-aware video model."
  },
  {
    "index": 1757,
    "source_corpus_id": 246634117,
    "ref_id": "b32",
    "citation_corpus_id": 211132756,
    "start": 7648,
    "end": 7667,
    "title": "Published as a conference paper at ICLR 2020 SCALABLE AND ORDER-ROBUST CONTINUAL LEARN- ING WITH ADDITIVE PARAMETER DECOMPOSITION",
    "abstract": "While recent continual learning methods largely alleviate the catastrophic problem on toy-sized datasets, some issues remain to be tackled to apply them to real-world problem domains. First, a continual learning model should effectively handle catastrophic forgetting and be efficient to train even with a large number of tasks. Secondly, it needs to tackle the problem of order-sensitivity, where the performance of the tasks largely varies based on the order of the task arrival sequence, as it may cause serious problems where fairness plays a critical role (e.g. medical diagnosis). To tackle these practical challenges, we propose a novel continual learning method that is scalable as well as order-robust, which instead of learning a completely shared set of weights, represents the parameters for each task as a sum of task-shared and sparse task-adaptive parameters. With our Additive Parameter Decomposition (APD), the task-adaptive parameters for earlier tasks remain mostly unaffected, where we update them only to reflect the changes made to the task-shared parameters. This decomposition of parameters effectively prevents catastrophic forgetting and order-sensitivity, while being computation-and memory-efficient. Further, we can achieve even better scalability with APD using hierarchical knowledge consolidation, which clusters the task-adaptive parameters to obtain hierarchically shared parameters. We validate our network with APD, APD-Net, on multiple benchmark datasets against state-of-the-art continual learning methods, which it largely outperforms in accuracy, scalability, and order-robustness.",
    "prev": "In order to find the optimal structure for each of the sequential tasks, Reinforced Continual Learning (RCL) (Xu & Zhu, 2018) leverages reinforcement learning and (Li et al., 2019) adapts architecture search.",
    "curr": "APD (Yoon et al., 2020) adds additional task-specific parameters for each task and selectively learns the task-shared parameters.",
    "next": "Regularization-based methods."
  },
  {
    "index": 1758,
    "source_corpus_id": 3485693,
    "ref_id": "b10",
    "citation_corpus_id": 17786716,
    "start": 8697,
    "end": 8703,
    "title": "Identity Matters in Deep Learning",
    "abstract": "An emerging design principle in deep learning is that each layer of a deep artificial neural network should be able to easily express the identity transformation. This idea not only motivated various normalization techniques, such as batch normalization, but was also key to the immense success of residual networks.In this work, we put the principle of identity parameterization on a more solid theoretical footing alongside further empirical progress. We first give a strikingly simple proof that arbitrarily deep linear residual networks have no spurious local optima. The same result for linear feed-forward networks in their standard parameterization is substantially more delicate. Second, we show that residual networks with ReLu activations have universal finite-sample expressivity in the sense that the network can represent any function of its sample provided that the model has more parameters than the sample size.Directly inspired by our theory, we experiment with a radically simple residual architecture consisting of only residual convolutional layers and ReLu activations, but no batch normalization, dropout, or max pool. Our model improves significantly on previous all-convolutional networks on the CIFAR10, CIFAR100, and ImageNet classification benchmarks. * Google Brain. m@mrtz.org † Princeton University. tengyu@cs.princeton.edu. Work performed at Google.",
    "prev": "Kawaguchi [Kaw16] shows that the landscape of deep neural nets does not have bad local minima but has degenerate saddle points.",
    "curr": "Hardt and Ma [HM17] show that re-parametrization using identity connection as in residual networks [HZRS16a] can remove the degenerate saddle points in the optimization landscape of deep linear residual networks.",
    "next": "Soudry and Carmon [SC16] showed that an over-parameterized neural network does not have bad differentiable local minimum."
  },
  {
    "index": 1760,
    "source_corpus_id": 260203143,
    "ref_id": "b36",
    "citation_corpus_id": 604334,
    "start": 2167,
    "end": 2189,
    "title": "Intriguing properties of neural networks",
    "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
    "prev": "dversarial image.The generation of benign-appearing adversarial images leverages a novel embedding-space-based methodology, operating with no access to the LLM model.Instead, the attacks require access only to the vision encoder and utilize one of our four embedding space targeting strategies.By not requiring access to the LLM, the attacks lower the entry barrier for attackers, particularly when vision encoders such as CLIP are embedded in closed-source LLMs.The attacks achieve a high success rate across different VLMs, highlighting the risk of cross-modality alignment vulnerabilities, and the need for new alignment approaches for multi-modal models.Content warning: We provide illustrative adversarial attack examples to reveal the generative models' vulnerabilities, aiming to aid the development of robust models to adversarial attacks.",
    "curr": "INTRODUCTION\n\nAdversarial attacks on Large Language Models (LLMs) (Zou et al., 2023), aiming at manipulating model outputs through input perturbations (Szegedy et al., 2014;Goodfellow et al., 2014) have garnered significant research interest in AI safety (Kaur et al., 2022;Carlini et al., 2021).These adversarial textual inputs and prompt injections (Liu et al., 2023b;Perez & Ribeiro, 2022) exhibit high transferability, enabling them to bypass the safety guards of different LLMs (Wei et al., 2023).However, text-based attacks can be easily spotted by humans or automated filters, leading to security patches and, consequently, diminishing their effectiveness as a persistent threat (Greshake et al., 2023;Markov et al., 2023).",
    "next": "With the integration of additional modalities into multi-modal language models (OpenAI, 2023;Bubeck et al., 2023;Liu et al., 2023a;Zhu et al., 2023), a newfound vulnerability to adversarial attacks via these augmented modalities has emerged (Goh et al., 2021;Noever & Noever, 2021).Concurrent with our work, multiple researchers have begun investigating vulnerabilities arising from the vision component in Vision-Language Models (VLMs).Thei"
  },
  {
    "index": 1761,
    "source_corpus_id": 257280083,
    "ref_id": "b0",
    "citation_corpus_id": 212657478,
    "start": 5703,
    "end": 5721,
    "title": "META-LEARNING CURIOSITY ALGORITHMS",
    "abstract": "We hypothesize that curiosity is a mechanism found by evolution that encourages meaningful exploration early in an agent's life in order to expose it to experiences that enable it to obtain high rewards over the course of its lifetime. We formulate the problem of generating curious behavior as one of meta-learning: an outer loop will search over a space of curiosity mechanisms that dynamically adapt the agent's reward signal, and an inner loop will perform standard reinforcement learning using the adapted reward signal. However, current meta-RL methods based on transferring neural network weights have only generalized between very similar tasks. To broaden the generalization, we instead propose to meta-learn algorithms: pieces of code similar to those designed by humans in ML papers. Our rich language of programs combines neural networks with other building blocks such as buffers, nearest-neighbor modules and custom loss functions. We demonstrate the effectiveness of the approach empirically, finding two novel curiosity algorithms that perform on par or better than human-designed published curiosity algorithms in domains as disparate as grid navigation with image inputs, acrobot, lunar lander, ant and hopper. * Equal contribution.",
    "prev": "INTRODUCTION\n\nLearning to learn (Thrun & Pratt, 1998), also known as meta-learning, aims to learn general knowledge about how to solve unseen, yet relevant tasks from prior experiences solving diverse tasks.",
    "curr": "In recent years, the concept of meta-learning has found various applications, e.g., few-shot classification , reinforcement learning (Duan et al., 2017;Houthooft et al., 2018;Alet et al., 2020), hyperparameter optimization (Franceschi et al., 2018), and so on.",
    "next": "Among them, few-shot classification is arguably the most popular one, whose goal is to learn some knowledge to classify test samples of unseen classes during (meta-)training with few labeled samples."
  },
  {
    "index": 1763,
    "source_corpus_id": 208548538,
    "ref_id": "b8",
    "citation_corpus_id": 6706414,
    "start": 1713,
    "end": 1738,
    "title": "Published as a conference paper at ICLR 2015 EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES",
    "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples-inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
    "prev": "Vertically, the model trained on small neural networks achieves similar performance on large neural networks.",
    "curr": "Code for all experiments is available at\n\nINTRODUCTION\n\nDespite their outstanding performances on various tasks, neural networks are found to be vulnerable to adversarial examples (Goodfellow et al., 2015;Szegedy et al., 2013).",
    "next": "The brittleness of neural networks can have costly consequences in areas such as autonomous driving, finance and healthcare."
  },
  {
    "index": 1764,
    "source_corpus_id": 59310641,
    "ref_id": "b36",
    "citation_corpus_id": 1998416,
    "start": 2092,
    "end": 2111,
    "title": "Effective Approaches to Attention-based Neural Machine Translation",
    "abstract": "An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker. 1",
    "prev": "RNNs integrate context information by updating a hidden state at every time-step, CNNs summarize a fixed size context through multiple layers, while as self-attention directly summarizes all context.",
    "curr": "Attention assigns context elements attention weights which define a weighted sum over context representations Sukhbaatar et al., 2015;Chorowski et al., 2015;Luong et al., 2015).",
    "next": "Source-target attention summarizes information from another sequence such as in machine translation while as self-attention operates over the current sequence."
  },
  {
    "index": 1765,
    "source_corpus_id": 252762544,
    "ref_id": "b3",
    "citation_corpus_id": 233033916,
    "start": 3203,
    "end": 3224,
    "title": "What Will it Take to Fix Benchmarking in Natural Language Understanding?",
    "abstract": "Evaluation for many natural language understanding (NLU) tasks is broken: Unreliable and biased systems score so highly on standard benchmarks that there is little room for researchers who develop better systems to demonstrate their improvements. The recent trend to abandon IID benchmarks in favor of adversarially-constructed, out-of-distribution test sets ensures that current models will perform poorly, but ultimately only obscures the abilities that we want our benchmarks to measure. In this position paper, we lay out four criteria that we argue NLU benchmarks should meet. We argue most current benchmarks fail at these criteria, and that adversarial data collection does not meaningfully address the causes of these failures. Instead, restoring a healthy evaluation ecosystem will require significant progress in the design of benchmark datasets, the reliability with which they are annotated, their size, and the ways they handle social bias.",
    "prev": "(Wallace et al., 2022) A major concern is that adversarial data collection proliferates idiosyncratic examples that do well in fooling models but eliminate coverage of necessary yet easier test cases.",
    "curr": "This can, in turn, reduce dataset diversity and limit external validity (Bowman & Dahl, 2021).",
    "next": "A growing line of theoretical and empirical research on static benchmarks has improved our understanding of the strengths and limitations of this setting."
  },
  {
    "index": 1768,
    "source_corpus_id": 219558527,
    "ref_id": "b42",
    "citation_corpus_id": 202539496,
    "start": 1428,
    "end": 1431,
    "title": "Entity, Relation, and Event Extraction with Contextualized Span Representations",
    "abstract": "We examine the capabilities of a unified, multitask framework for three information extraction tasks: named entity recognition, relation extraction, and event extraction. Our framework (called DYGIE++) accomplishes all tasks by enumerating, refining, and scoring text spans designed to capture local (withinsentence) and global (cross-sentence) context. Our framework achieves state-of-theart results across all tasks, on four datasets from a variety of domains. We perform experiments comparing different techniques to construct span representations. Contextualized embeddings like BERT perform well at capturing relationships among entities in the same or adjacent sentences, while dynamic span graph updates model long-range crosssentence relationships. For instance, propagating span representations via predicted coreference links can enable the model to disambiguate challenging entity mentions. Our code is publicly available at https://github. com/dwadden/dygiepp and can be easily adapted for new tasks or datasets.",
    "prev": "Generally, we observe a decrease in their relative impact when modifying the fine-tuning process based on our findings.",
    "curr": "Introduction\n\nFine-tuning self-supervised pre-trained models has significantly boosted state-of-the-art performance on Natural Language Processing (NLP) tasks [13,28,43,51,56].",
    "next": "One of the most effective models for this process is BERT [7]."
  },
  {
    "index": 1770,
    "source_corpus_id": 259847777,
    "ref_id": "b4",
    "citation_corpus_id": 52889459,
    "start": 4588,
    "end": 4608,
    "title": "LARGE SCALE GAN TRAINING FOR HIGH FIDELITY NATURAL IMAGE SYNTHESIS",
    "abstract": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by truncating the latent space. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128×128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.3 and Fréchet Inception Distance (FID) of 9.6, improving over the previous best IS of 52.52 and FID of 18.65.",
    "prev": "INTRODUCTION\n\nA generative adversarial network (GAN) (Goodfellow et al., 2014) is a popular approach for generative modeling.",
    "curr": "GANs have achieved remarkable performance in various domains such as image (Brock et al., 2019;Karras et al., 2019;, audio (Kumar et al., 2019;Kong et al., 2020), and video (Tulyakov et al., 2018;Hao et al., 2021).",
    "next": "The aim of GAN is to learn a target probability measure via a neural network, called a generator."
  },
  {
    "index": 1773,
    "source_corpus_id": 237491529,
    "ref_id": "b14",
    "citation_corpus_id": 16326763,
    "start": 4626,
    "end": 4649,
    "title": "CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING",
    "abstract": "We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies \"end-to-end\": directly from raw pixel inputs. * These authors contributed equally.",
    "prev": "Besides, the modeling of all continuous parameter dimensions all the time introduces redundancy in computation and policy learning, and may also have the scalability issue when the hybrid action space becomes high-dimensional.",
    "curr": "Algorithm Scalability Stationarity Dependence Latent PADDPG HPPO PDQN HHQN HyAR (Ours) Latent Space\n\nINTRODUCTION\n\nDeep Reinforcement learning (DRL) has recently shown a great success in a variety of decisionmaking problems that involve controls with either discrete actions, such as Go (Silver et al., 2016) and Atari (Mnih et al., 2015), or continuous actions, such as robot control (Schulman et al., 2015;Lillicrap et al., 2015).",
    "next": "However, in contrast to these two kinds of homogeneous action space, many real-world scenarios requires more complex controls with discrete-continuous hybrid action space, e.g., Robot Soccer (Masson et al., 2016) and Games (Xiong et al., 2018)."
  },
  {
    "index": 1779,
    "source_corpus_id": 108297336,
    "ref_id": "b17",
    "citation_corpus_id": 13123084,
    "start": 2460,
    "end": 2481,
    "title": "Published as a conference paper at ICLR 2017 TEMPORAL ENSEMBLING FOR SEMI-SUPERVISED LEARNING",
    "abstract": "In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce self-ensembling, where we form a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the (non-augmented) classification error rate from 18.44% to 7.05% in SVHN with 500 labels and from 18.63% to 16.55% in CIFAR-10 with 4000 labels, and further to 5.12% and 12.16% by enabling the standard augmentations. We additionally obtain a clear improvement in CIFAR-100 classification accuracy by using random images from the Tiny Images dataset as unlabeled extra inputs during training. Finally, we demonstrate good tolerance to incorrect labels.",
    "prev": "Semi-supervised learning is particularly valuable in applications such as medical imaging, where labeled data may be scarce and expensive (Oliver et al., 2018).",
    "curr": "Currently the best semi-supervised results are obtained by consistency-enforcing approaches (Bachman et al., 2014;Laine and Aila, 2017;Tarvainen and Valpola, 2017;Miyato et al., 2017;Park et al., 2017).",
    "next": "These methods use unlabeled data to stabilize their predictions under input or weight perturbations."
  },
  {
    "index": 1784,
    "source_corpus_id": 257404922,
    "ref_id": "b72",
    "citation_corpus_id": 237372712,
    "start": 18004,
    "end": 18024,
    "title": "MINIF2F: A CROSS-SYSTEM BENCHMARK FOR FORMAL OLYMPIAD-LEVEL MATHEMATICS",
    "abstract": "We present miniF2F, a dataset of formal Olympiad-level mathematics problems statements intended to provide a unified cross-system benchmark for neural theorem proving. The miniF2F benchmark currently targets Metamath, Lean, Isabelle (partially) and HOL Light (partially) and consists of 488 problem statements drawn from the AIME, AMC, and the International Mathematical Olympiad (IMO), as well as material from high-school and undergraduate mathematics courses. We report baseline results using GPT-f (Polu & Sutskever, 2020), a neural theorem prover based on GPT-3 (Brown et al., 2020)  and provide an analysis of its performance. We intend for miniF2F to be a community-driven effort and hope that our benchmark will help spur advances in neural theorem proving.",
    "prev": "Through ablations, we study the effectiveness of Magnushammer and the contribution of its training components.",
    "curr": "More experimental results and details can be found in Appendix E.\n\n\nExperimental details\n\nBenchmarks In our experiments, we use two benchmarks: PISA (Jiang et al., 2021) and MiniF2F (Zheng et al., 2022).",
    "next": "PISA contains problems randomly selected from the Archive of Formal Proofs; we use the same 1000 problems as  for our evaluations."
  },
  {
    "index": 1786,
    "source_corpus_id": 247292326,
    "ref_id": "b3",
    "citation_corpus_id": 54443381,
    "start": 12166,
    "end": 12189,
    "title": "EFFICIENT LIFELONG LEARNING WITH A-GEM",
    "abstract": "In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC  and other regularizationbased methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency. 1",
    "prev": "Memory-based methods (Parisi et al., 2019) have achieved excellent performances in accommodating new knowledge while retaining previously learned experience.",
    "curr": "Such memory-based methods, such as gradient episodic memory (GEM) (Lopez-Paz & Ranzato, 2017) and averaged gradient episodic memory (A-GEM) (Chaudhry et al., 2018), store a subset of data from previous tasks and replay the memorized data when training on the current task.",
    "next": "For instance, A-GEM treats the losses on the episodic memories of previous tasks as inequality constraints in optimizing the objectives of current tasks and changes the model updates from the plain gradient g to g − wg ref , where g ref is the gradient computed from the loss on the memorized data and w is a non-negative weight."
  },
  {
    "index": 1788,
    "source_corpus_id": 245329426,
    "ref_id": "b53",
    "citation_corpus_id": 212877887,
    "start": 4132,
    "end": 4149,
    "title": "THE INGREDIENTS OF REAL-WORLD ROBOTIC REINFORCEMENT LEARNING",
    "abstract": "The success of reinforcement learning for real world robotics has been, in many cases limited to instrumented laboratory scenarios, often requiring arduous human effort and oversight to enable continuous learning. In this work, we discuss the elements that are needed for a robotic learning system that can continually and autonomously improve with data collected in the real world. We propose a particular instantiation of such a system, using dexterous manipulation as our case study. Subsequently, we investigate a number of challenges that come up when learning without instrumentation. In such settings, learning must be feasible without manually designed resets, using only on-board perception, and without hand-engineered reward functions. We propose simple and scalable solutions to these challenges, and then demonstrate the efficacy of our proposed system on a set of dexterous robotic manipulation tasks, providing an in-depth analysis of the challenges associated with this learning paradigm. We demonstrate that our complete system can learn without any human intervention, acquiring a variety of vision-based skills with a real-world three-fingered hand. Results and videos can be found at https://sites.google.com/view/realworld-rl/.",
    "prev": "6.1).",
    "curr": "Moreover, while prior work has examined settings such as RL without resets (Eysenbach et al., 2017;Zhu et al., 2020;, ecological RL (Co-Reyes et al., 2020), or RL amidst non-stationarity (Xie et al., 2020) in isolated scenarios, these settings are not well-represented in existing benchmarks.",
    "next": "As a result, there is not a consistent formal framework for evaluating autonomy in reinforcement learning and there is limited work in this direction compared to the vast literature on reinforcement learning."
  },
  {
    "index": 1790,
    "source_corpus_id": 219721074,
    "ref_id": "b1",
    "citation_corpus_id": 8768364,
    "start": 4130,
    "end": 4133,
    "title": "Density estimation using Real NVP",
    "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.",
    "prev": "• We experiment with encoding distributions of increasing flexibility on various tasks including sets, language and graphs, and show that a simple mixture model is sufficient for modeling discrete, categorical distribution accurately.",
    "curr": "Introduction\n\nNormalizing Flows have been recently popular for tasks like image modeling [2,3,5,13] and speech generation [11,32] by providing efficient parallel sampling and exact density evaluation.",
    "next": "The concept normalizing flows rely on is the rule of change of variables, a continuous transformation naturally working on continuous data."
  },
  {
    "index": 1791,
    "source_corpus_id": 235613568,
    "ref_id": "b1",
    "citation_corpus_id": 195798643,
    "start": 3182,
    "end": 3199,
    "title": "On the Convergence of FedAvg on Non-IID Data",
    "abstract": "Federated learning enables a large amount of edge computing devices to learn a centralized model while keeping all local data on edge devices. As a leading algorithm in this setting, Federated Averaging (FedAvg) runs Stochastic Gradient Descent (SGD) in parallel on a small subset of the total devices and averages the sequences only once in a while. Despite its simplicity, it lacks theoretical guarantees in the federated setting. In this paper, we analyze the convergence of FedAvg on non-iid data. We investigate the effect of different sampling and averaging schemes, which are crucial especially when data are unbalanced. We prove a concise convergence rate of O( 1 T ) for FedAvg with proper sampling and averaging schemes in convex problems, where T is the total number of steps. Our results show that heterogeneity of data slows down the convergence, which is intrinsic in the federated setting. Low device participation rate can be achieved without severely harming the optimization process in federated learning. We show that there is a trade-off between communication efficiency and convergence rate. We analyze the necessity of learning rate decay by taking a linear regression as an example. Our work serves as a guideline for algorithm design in applications of federated learning, where heterogeneity and unbalance of data are the common case. * Equal contribution.",
    "prev": "from their aggregated data, FEDAVG has been shown convergent to the ideal model trained in a centralized way using the aggregated data (Zinkevich et al., 2010;McMahan et al., 2017;Zhou & Cong, 2017).",
    "curr": "Its performance, however, can degrade drastically if such an assumption does not hold in practice (Karimireddy et al., 2020;Li et al., 2020b;Zhao et al., 2018): FEDAVG simply drifts away from the ideal model.",
    "next": "Moreover, by only taking weight average, FEDAVG does not fully utilize the information among clients (e.g., variances), and may have negative effects on over-parameterized models like neural networks due to their permutation-invariant property in the weight space (Wang et al., 2020;Yurochkin et al., 2019)."
  },
  {
    "index": 1792,
    "source_corpus_id": 210843644,
    "ref_id": "b13",
    "citation_corpus_id": 8393918,
    "start": 2249,
    "end": 2266,
    "title": "Published as a conference paper at ICLR 2016 GATED GRAPH SEQUENCE NEURAL NETWORKS",
    "abstract": "Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks(Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be described as abstract data structures.Published as a conference paper at ICLR 2016 to learn features on the graph that encode the partial output sequence that has already been produced (e.g., the path so far if outputting a path) and that still needs to be produced (e.g., the remaining path). We will show how the GNN framework can be adapted to these settings, leading to a novel graph-based neural network model that we call Gated Graph Sequence Neural Networks (GGS-NNs).We illustrate aspects of this general model in experiments on bAbI tasks  and graph algorithm learning tasks that illustrate the capabilities of the model. We then present an application to the verification of computer programs. When attempting to prove properties such as memory safety (i.e., that there are no null pointer dereferences in a program), a core problem is to find mathematical descriptions of the data structures used in a program. FollowingBrockschmidt et al. (2015), we have phrased this as a machine learning problem where we will learn to map from a set of input graphs, representing the state of memory, to a logical description of the data structures that have been instantiated. Whereas Brockschmidt et al.(2015)relied on a large amount of hand-engineering of features, we show that the system can be replaced with a GGS-NN at no cost in accuracy.",
    "prev": "Experimental results show the proposed Geom-GCN achieved state-of-the-art performance on a wide range of open datasets of graphs.",
    "curr": "INTRODUCTION\n\nMessage-passing neural networks (MPNNs), such as GNN (Scarselli et al., 2008), ChebNet (Defferrard et al., 2016), GG-NN (Li et al., 2016), GCN (Kipf & Welling, 2017), are powerful for learning on graphs with various applications ranging from brain networks to online social network (Gilmer et al., 2017;Wang et al., 2019).",
    "next": "In a layer of MPNNs, each node sends its feature representation, a \"message\", to the nodes in its neighborhood; and then updates its feature representation by aggregating all \"messages\" received from the neighborhood."
  },
  {
    "index": 1793,
    "source_corpus_id": 227126808,
    "ref_id": "b34",
    "citation_corpus_id": 3162051,
    "start": 34841,
    "end": 34850,
    "title": "mixup: BEYOND EMPIRICAL RISK MINIMIZATION",
    "abstract": "Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks. * Alphabetical order.",
    "prev": "Here we give some interpretation to the quantitative guarantees of Theorem 4.1:\n\n• The number of pixels d only needs to depend logarithmically on the number of public/private images and polynomially in the sparsity k pub , k priv , which will be some small positive integer (e.g.",
    "curr": "k pub + k priv = 4 or 8 in [HSC + 20], k pub + k priv = 4 or 6 in [HSLA20] and k pub + k priv = 2 in the implementation of MixUp in [ZCDLP18]), so the regime in which Theorem 4.1 applies is quite realistic.",
    "next": "• Note that we can achieve recovery even when m = o(n k priv priv )."
  },
  {
    "index": 1794,
    "source_corpus_id": 49667762,
    "ref_id": "b17",
    "citation_corpus_id": 2009318,
    "start": 4768,
    "end": 4794,
    "title": "NEURAL GPUS LEARN ALGORITHMS",
    "abstract": "Learning an algorithm from examples is a fundamental problem that has been widely studied. It has been addressed using neural networks too, in particular by Neural Turing Machines (NTMs). These are fully differentiable computers that use backpropagation to learn their own programming. Despite their appeal NTMs have a weakness that is caused by their sequential nature: they are not parallel and are are hard to train due to their large depth when unfolded. We present a neural network architecture to address this problem: the Neural GPU. It is based on a type of convolutional gated recurrent unit and, like the NTM, is computationally universal. Unlike the NTM, the Neural GPU is highly parallel which makes it easier to train and efficient to run. An essential property of algorithms is their ability to handle inputs of arbitrary size. We show that the Neural GPU can be trained on short instances of an algorithmic task and successfully generalize to long instances. We verified it on a number of tasks including long addition and long multiplication of numbers represented in binary. We train the Neural GPU on numbers with up-to 20 bits and observe no errors whatsoever while testing it, even on much longer numbers. To achieve these results we introduce a technique for training deep recurrent networks: parameter sharing relaxation. We also found a small amount of dropout and gradient noise to have a large positive effect on learning and generalization.",
    "prev": "h t i represents the representation for input symbol 1 ≤ i ≤ m at recurrent time-step t. With dynamic halting, T is dynamically determined for each position (Section 2.2).",
    "curr": "bias may be crucial for several algorithmic and language understanding tasks of varying complexity: in contrast to models such as the Neural Turing Machine (Graves et al., 2014), the Neural GPU (Kaiser & Sutskever, 2016) or Stack RNNs , the Transformer does not generalize well to input lengths not encountered during training.",
    "next": "In this paper, we introduce the Universal Transformer (UT), a parallel-in-time recurrent self-attentive sequence model which can be cast as a generalization of the Transformer model, yielding increased theoretical capabilities and improved results on a wide range of challenging sequence-to-sequence tasks."
  },
  {
    "index": 1795,
    "source_corpus_id": 259308864,
    "ref_id": "b13",
    "citation_corpus_id": 182952848,
    "start": 2261,
    "end": 2283,
    "title": "GLTR: Statistical Detection and Visualization of Generated Text",
    "abstract": "The rapid improvement of language models has raised the specter of abuse of text generation systems. This progress motivates the development of simple methods for detecting generated text that can be used by and explained to non-experts. We develop GLTR, a tool to support humans in detecting whether a text was generated by a model. GLTR applies a suite of baseline statistical methods that can detect generation artifacts across common sampling schemes. In a human-subjects study, we show that the annotation scheme provided by GLTR improves the human detection-rate of fake text from 54% to 72% without any prior training. GLTR is open-source and publicly deployed, and has already been widely used to detect generated outputs.",
    "prev": "There are two main camps of existing attempts to address these challenges.",
    "curr": "One camp, inspired by Turing [1950], aims at generically distinguishing machine-generated text from that of the humans [Gehrmann et al., 2019, Mitchell et al., 2023, Hovy, 2016, Zellers et al., 2019, OpenAI, 2023b.",
    "next": "These works primarily leverage hand-crafted or learned \"statistical patterns\" of generated text, thus their performance is not robust to distribution changes (e.g., by prompting / conditioning), prone to biases [Liang et al., 2023], and vulnerable to adversarial attacks."
  },
  {
    "index": 1798,
    "source_corpus_id": 220249831,
    "ref_id": "b40",
    "citation_corpus_id": 1428702,
    "start": 32626,
    "end": 32645,
    "title": "Learning Word Vectors for Sentiment Analysis",
    "abstract": "Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term-document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.",
    "prev": "C Additional Results\n\nC.1 Simulating heteroskedastic noise on IMDB-review.",
    "curr": "IMDB-review dataset has a total of 50,000 (25,000 positive and 25,000 negative reviews) movie reviews for binary sentiment classification [Maas et al., 2011].",
    "next": "To simulate heteroskedastic noise for this binary classification problem, we project 5% of the labels of negative reviews to positive, and 40% in the reverse direction."
  },
  {
    "index": 1800,
    "source_corpus_id": 7167114,
    "ref_id": "b18",
    "citation_corpus_id": 6628106,
    "start": 13790,
    "end": 13809,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "We use a K = 256 dimensional bottleneck and a diagonal Gaussian for p(z|x).",
    "curr": "The networks were trained using Ten-sorFlow for 200 epochs using the Adam optimizer (Kingma & Ba, 2015) with a learning rate of 0.0001.",
    "next": "Full hyperparameter details can be found in Appendix A."
  },
  {
    "index": 1808,
    "source_corpus_id": 220364453,
    "ref_id": "b0",
    "citation_corpus_id": 15534684,
    "start": 8897,
    "end": 8916,
    "title": "VARIATIONAL LOSSY AUTOENCODER",
    "abstract": "Representation learning seeks to expose certain aspects of observed data in a learned representation that's amenable to downstream tasks like classification. For instance, a good representation for 2D images might be one that describes only global structure and discards information about detailed texture. In this paper, we present a simple but principled method to learn such global representations by combining Variational Autoencoder (VAE) with neural autoregressive models such as RNN, MADE and PixelRNN/CNN. Our proposed VAE model allows us to have control over what the global latent code can learn and by designing the architecture accordingly, we can force the global latent code to discard irrelevant information such as texture in 2D images, and hence the VAE only \"autoencodes\" data in a lossy fashion. In addition, by leveraging autoregressive models as both prior distribution p(z) and decoding distribution p(x|z), we can greatly improve generative modeling performance of VAEs, achieving new state-of-the-art results on MNIST, OMNIGLOT and Caltech-101 Silhouettes density estimation tasks as well as competitive results on CIFAR10.",
    "prev": "Despite similarities with autoencoder approaches, the absence of an explicit encoding network offers several advantages.",
    "curr": "This simplicity ensures that latent codes must be utilised, unlike VAEs with powerful decoders (Chen et al., 2017).",
    "next": "Similar to GONs, Normalizing Flow methods are also capable of encoding and decoding with a single set of weights."
  },
  {
    "index": 1812,
    "source_corpus_id": 2906360,
    "ref_id": "b14",
    "citation_corpus_id": 6715185,
    "start": 1751,
    "end": 1776,
    "title": "Published as a conference paper at ICLR 2016 NEURAL PROGRAMMER: INDUCING LATENT PROGRAMS WITH GRADIENT DESCENT",
    "abstract": "Deep neural networks have achieved impressive supervised classification performance in many tasks including image recognition, speech recognition, and sequence to sequence learning. However, this success has not been translated to applications like question answering that may involve complex arithmetic and logic reasoning. A major limitation of these models is in their inability to learn even simple arithmetic and logic operations. For example, it has been shown that neural networks fail to learn to add two binary numbers reliably. In this work, we propose Neural Programmer, a neural network augmented with a small set of basic arithmetic and logic operations that can be trained end-to-end using backpropagation. Neural Programmer can call these augmented operations over several steps, thereby inducing compositional programs that are more complex than the built-in operations. The model learns from a weak supervision signal which is the result of execution of the correct program, hence it does not require expensive annotation of the correct program itself. The decisions of what operations to call, and what data segments to apply to are inferred by Neural Programmer. Such decisions, during training, are done in a differentiable fashion so that the entire network can be trained jointly by gradient descent. We find that training the model is difficult, but it can be greatly improved by adding random noise to the gradient. On a fairly complex synthetic table-comprehension dataset, traditional recurrent networks and attentional models perform poorly while Neural Programmer typically obtains nearly perfect accuracy. * Work done during an internship at Google.",
    "prev": "INTRODUCTION\n\nA dream of artificial intelligence is to build systems that can write computer programs.",
    "curr": "Recently, there has been much interest in program-like neural network models (Graves et al., 2014;Kurach et al., 2015;Joulin & Mikolov, 2015;Grefenstette et al., 2015;Sukhbaatar et al., 2015;Neelakantan et al., 2016;Kaiser & Sutskever, 2016;Reed & de Freitas, 2016;Zaremba et al., 2016;Graves et al., 2016), but none of these can write programs; that is, they do not generate human-readable source code.",
    "next": "Only very recently, Riedel et al."
  },
  {
    "index": 1813,
    "source_corpus_id": 52898972,
    "ref_id": "b9",
    "citation_corpus_id": 17707860,
    "start": 2865,
    "end": 2882,
    "title": "Published as a conference paper at ICLR 2017 DELVING INTO TRANSFERABLE ADVERSARIAL EX- AMPLES AND BLACK-BOX ATTACKS",
    "abstract": "An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack Clarifai.com,",
    "prev": "However, recent works have revealed that deep learning models are often vulnerable to adversarial examples Papernot et al., 2016), which are maliciously designed to deceive the target model by generating carefully crafted adversarial perturbations on original clean inputs.",
    "curr": "Moreover, adversarial examples can transfer across models to mislead other models with a high probability Liu et al., 2017).",
    "next": "How to effectively defense against adversarial attacks is crucial for security-critical computer vision systems, such as autonomous driving."
  },
  {
    "index": 1814,
    "source_corpus_id": 251320393,
    "ref_id": "b41",
    "citation_corpus_id": 211678094,
    "start": 3896,
    "end": 3915,
    "title": "ADAPTIVE FEDERATED OPTIMIZATION",
    "abstract": "Federated learning is a distributed machine learning paradigm in which a large number of clients coordinate with a central server to learn a model without sharing their own training data. Standard federated optimization methods such as Federated Averaging (FEDAVG) are often difficult to tune and exhibit unfavorable convergence behavior. In non-federated settings, adaptive optimization methods have had notable success in combating such issues. In this work, we propose federated versions of adaptive optimizers, including ADAGRAD, ADAM, and YOGI, and analyze their convergence in the presence of heterogeneous data for general nonconvex settings. Our results highlight the interplay between client heterogeneity and communication efficiency. We also perform extensive experiments on these methods and show that the use of adaptive optimizers can significantly improve the performance of federated learning. * Authors contributed equally to this work arXiv:2003.00295v5 [cs.LG] 8 Sep 2021Published as a conference paper at ICLR 2021 adaptivity by using adaptive optimizers as client or server optimizers. Building upon this, we develop novel adaptive optimization techniques for FL by using per-coordinate methods as server optimizers. By focusing on adaptive server optimization, we enable use of adaptive learning rates without increase in client storage or communication costs, and ensure compatibility with cross-device FL.Main contributions In light of the above, we highlight the main contributions of the paper.• We study a general framework for federated optimization using server and client optimizers. This framework generalizes many existing federated optimization methods, including FEDAVG. • We use this framework to design novel, cross-device compatible, adaptive federated optimization methods, and provide convergence analysis in general nonconvex settings. To the best of our knowledge, these are the first methods for FL using adaptive server optimization. We show an important interplay between the number of local steps and the heterogeneity among clients. • We introduce comprehensive and reproducible empirical benchmarks for comparing federated optimization methods. These benchmarks consist of seven diverse and representative FL tasks involving both image and text data, with varying amounts of heterogeneity and numbers of clients. • We demonstrate strong empirical performance of our adaptive optimizers throughout, improving upon commonly used baselines. Our results show that our methods can be easier to tune, and highlight their utility in cross-device settings.",
    "prev": "This work considers three popular datasets: CIFAR-10 and FEMNIST for image classification and, SpeechCommands for audio classification.",
    "curr": "INTRODUCTION\n\nDespite it being a relatively new subfield of machine learning (ML), Federated Learning (FL) (McMahan et al., 2017;Reddi et al., 2021;Horvath et al., 2021) has become an indispensable tool to enable privacy-preserving collaboratively learning, as well as to deliver personalised models tailored to the end-user's local data and context (Arivazhagan et al., 2019;Hilmkil et al., 2021;Cheng et al., 2021).",
    "next": "For example: next-word prediction (Hard et al., 2018), physical activity detection (Doherty et al., 2017), keyword spotting (Hard et al., 2020), among others."
  },
  {
    "index": 1815,
    "source_corpus_id": 258079422,
    "ref_id": "b8",
    "citation_corpus_id": 52967399,
    "start": 2548,
    "end": 2569,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "This is important in the real world, where the data shifts constantly and new domains, events or topics keep emerging(Ke et al., 2022b)and the LM needs to be updated to serve the users better.",
    "curr": "INTRODUCTION\n\nPre-trained language models (LMs) like BERT (Devlin et al., 2019) and RoBERTa  have significantly advanced NLP.",
    "next": "Recently, LMs have also been used by many continual learning (CL) systems to learn a sequence of end-tasks incrementally (Ke et al., 2021a;Sun et al., 2020;, which we call continual end-task learning."
  },
  {
    "index": 1816,
    "source_corpus_id": 255440621,
    "ref_id": "b0",
    "citation_corpus_id": 52967399,
    "start": 2162,
    "end": 2182,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "We show experimentally that these methods consistently and significantly outperform investigated parameter-efficient fine-tuning strategies across different backbone models and different tasks in natural language processing 1 .",
    "curr": "Introduction\n\nLarge pretrained models have achieved the state-of-the-art performances across a wide variety of downstream natural language processing tasks through fine-tuning on task-specific labeled data [Devlin et al., 2019, Yang et al., 2019, Sun et al., 2019, Clark et al., 2019, Lewis et al., 2020a, Bao et al., 2020, He et al., 2020, Raffel et al., 2020, Ziems et al., 2022.",
    "next": "However, fine-tuning all the parameters and storing them separately for different tasks is expensive in terms of computation and storage overhead (e.g., 355M parameters for RoBERTa  and 175B parameters for GPT-3 [Brown et al., 2020])."
  },
  {
    "index": 1818,
    "source_corpus_id": 244527108,
    "ref_id": "b29",
    "citation_corpus_id": 13298214,
    "start": 22862,
    "end": 22884,
    "title": "LEARNING TO NAVIGATE IN COMPLEX ENVIRONMENTS",
    "abstract": "Learning to navigate in complex environments with dynamic elements is an important milestone in developing AI agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks leveraging multimodal sensory inputs. In particular we consider jointly learning the goal-driven reinforcement learning problem with auxiliary depth prediction and loop closure classification tasks. This approach can learn to navigate from raw sensory input in complicated 3D mazes, approaching human-level performance even under conditions where the goal location changes frequently. We provide detailed analysis of the agent behaviour 1 , its ability to localise, and its network activity dynamics, showing that the agent implicitly learns key navigation abilities.",
    "prev": "The first class of methods studies the usage of data augmentation for broadening the data distribution for training more robust feature representation (Laskin et al., 2020;Kostrikov et al., 2020;Schwarzer et al., 2021;Yarats et al., 2021).",
    "curr": "The second class explores the role of auxiliary tasks in learning representations, such as weakly-supervised classification and location recognition, for dealing with sparse and delayed supervision (Lee et al., 2020b;Mirowski et al., 2017;Oord et al., 2018).",
    "next": "The third class of methods, specifically tailored to model-based RL models, leverages generative modelling of environment dynamics, enabling joint learning of the representations and the dynamics model (Ha and Schmidhuber, 2018;Buesing et al., 2018;Hafner et al., 2019;Lee et al., 2020a;Schrittwieser et al., 2020;Hafner et al., 2020b)."
  },
  {
    "index": 1819,
    "source_corpus_id": 231985599,
    "ref_id": "b17",
    "citation_corpus_id": 28202810,
    "start": 5363,
    "end": 5386,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "We conduct experiments on Atari games (Bellemare et al., 2013) and DeepMind Control suite (Tassa et al., 2018) in low data regime.",
    "curr": "The experiment results show that our auxiliary task combined with Rainbow (Hessel et al., 2017) for discrete control tasks or SAC (Haarnoja et al., 2018) for continuous control tasks achieves superior performance over other state-of-the-art baselines for this regime.",
    "next": "Our method can be further combined with existing unsupervised contrastive learning methods to achieve even better performance."
  },
  {
    "index": 1820,
    "source_corpus_id": 254926918,
    "ref_id": "b3",
    "citation_corpus_id": 234357520,
    "start": 6542,
    "end": 6563,
    "title": "VICREG: VARIANCE-INVARIANCE-COVARIANCE RE- GULARIZATION FOR SELF-SUPERVISED LEARNING",
    "abstract": "Recent self-supervised methods for image representation learning maximize the agreement between embedding vectors produced by encoders fed with different views of the same image. The main challenge is to prevent a collapse in which the encoders produce constant or non-informative vectors. We introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with two regularizations terms applied to both embeddings separately: (1) a term that maintains the variance of each embedding dimension above a threshold, (2) a term that decorrelates each pair of variables. Unlike most other approaches to the same problem, VICReg does not require techniques such as: weight sharing between the branches, batch normalization, feature-wise normalization, output quantization, stop gradient, memory banks, etc., and achieves results on par with the state of the art on several downstream tasks. In addition, we show that our variance regularization term stabilizes the training of other methods and leads to performance improvements.",
    "prev": "Embeddings of positive pairs are brought together while keeping the embeddings of the batch overall distant from one another.",
    "curr": "There are three main categories of approaches for doing so: contrastive learning methods (Chen et al., 2020;He et al., 2020) explicitly push away the embeddings of a batch of inputs from one another; distillation-based methods (Grill et al., 2020;Chen and He, 2021) use an asymmetric embedding architecture, allowing the model to discard the \"push away\" part; entropy maximization methods (Bardes et al., 2022;Ermolov and Sebe, 2020) maintain a high entropy in the embedding space.",
    "next": "Image manipulations as data-augmentations."
  },
  {
    "index": 1821,
    "source_corpus_id": 229923865,
    "ref_id": "b38",
    "citation_corpus_id": 53216818,
    "start": 7813,
    "end": 7834,
    "title": "Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control",
    "abstract": "We propose a \"plan online and learn offline\" framework for the setting where an agent, with an internal model, needs to continually act and learn in the world. Our work builds on the synergistic relationship between local model-based control, global value function learning, and exploration. We study how local trajectory optimization can cope with approximation errors in the value function, and can stabilize and accelerate value function learning. Conversely, we also study how approximate value functions can help reduce the planning horizon and allow for better policies beyond local solutions. Finally, we also demonstrate how trajectory optimization can be used to perform temporally coordinated exploration in conjunction with estimating uncertainty in value function approximation. This exploration is critical for fast and stable learning of the value function. Combining these components enable solutions to complex control tasks, like humanoid locomotion and dexterous in-hand manipulation, in the equivalent of a few minutes of experience in the real world.",
    "prev": "RELATED WORK\n\nOffline and Model-based RL: A number of prior works have studied the problem of learning behaviors from existing offline datasets.",
    "curr": "While recent progress has been made in applying model-free RL techniques to this problem of offline or batch RL (Fujimoto et al., 2019;Wu et al., 2019;Kumar et al., 2019;Nair et al., 2020b), one approach that has shown promise is offline model-based RL (Lowrey et al., 2019;Kidambi et al., 2020;Yu et al., 2020;Argenson & Dulac-Arnold, 2020), where the agent learns a predictive model of the world from data.",
    "next": "Such model-based methods have seen success both in the offline and online RL settings, and have a rich history of being effective for planning (Deisenroth & Rasmussen, 2011;Watter et al., 2015;McAllister & Rasmussen, 2016;Chua et al., 2018;Amos et al., 2018;Hafner et al., 2019b;Nagabandi et al., 2018;Kahn et al., 2020;Dong et al., 2020) or policy optimization (Sutton, 1991;Weber et al., 2017;Ha & Schmidhuber, 2018;Janner et al., 2019;Wang & Ba, 2019;Hafner et al., 2019a)."
  },
  {
    "index": 1822,
    "source_corpus_id": 260350986,
    "ref_id": "b1",
    "citation_corpus_id": 173188048,
    "start": 4872,
    "end": 4892,
    "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms",
    "abstract": "We introduce a large-scale dataset of math word problems and an interpretable neural math problem solver that learns to map problems to operation programs. Due to annotation challenges, current datasets in this domain have been either relatively small in scale or did not offer precise operational annotations over diverse problem types. We introduce a new representation language to model precise operation programs corresponding to each math problem that aim to improve both the performance and the interpretability of the learned models. Using this representation language, our new dataset, MathQA, significantly enhances the AQuA dataset with fully-specified operational programs. We additionally introduce a neural sequence-to-program model enhanced with automatic problem categorization. Our experiments show improvements over competitive baselines in our MathQA as well as the AQuA datasets. The results are still significantly lower than human performance indicating that the dataset poses new challenges for future research. Our dataset is available at: https: //math-qa.github.io/math-QA/.",
    "prev": "Namely, given multiple solutions to a question, it uses confidence scores as weights to vote among the answers, which provides a soft way to focus on more accurate solutions.",
    "curr": "We evaluate SelfCheck on three math tasks, namely GSM8K (Cobbe et al., 2021), MathQA (Amini et al., 2019), and MATH (Hendrycks et al., 2021).",
    "next": "For all datasets, we find that using SelfCheck achieves a significant increase in final answer accuracies compared with simple majority voting and other baselines."
  },
  {
    "index": 1823,
    "source_corpus_id": 263605855,
    "ref_id": "b35",
    "citation_corpus_id": 3104920,
    "start": 14891,
    "end": 14895,
    "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
    "abstract": "We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsumption hierarchy over constituents and their denotations, based on a large corpus of 30K images and 150K descriptive captions.",
    "prev": "We fine-tune a LoRA module on the pre-trained SEED-LLaMA with the template as below,\n\nUSER: <Instruction> ASSISTANT: <Answer>\n\nOnly the content of <Answer> is accounted for loss.The overall instruction tuning phase takes 16 hours for SEED-LLaMA-8B and 27 hours for SEED-LLaMA-14B with 32 A100-80G GPUs.",
    "curr": "Image → Text Text → Image Image → Text Text → Image\nR@1 R@5 R@10 R@1 R@5 R@10 R@m R@1 R@5 R@10 R@1 R@5 R@10 R@m BLIP-2 [11] 81.9  4 Experiment\n\n\nSEED Tokenizer\n\nEvaluation of Causal Embeddings.We evaluate the performance of Causal Q-Former on the imagetext retrieval using COCO [35] and Flickr30K [36].The performance is measured by Recall@K (R@K).Note that we adopt the dual-stream paradigm for inference and remove the image-textmatching (ITM) re-rank module in BLIP-2 for a fair comparison.As shown in Tab.",
    "next": "1, our Causal Q-former achieves better results than BLIP-2 in terms of an aggregated metric Recall@mean.It demonstrates that the output query embeddings with causal dependency do not drop performance than the output embeddings with bi-directional attention in BLIP-2."
  },
  {
    "index": 1827,
    "source_corpus_id": 254198988,
    "ref_id": "b24",
    "citation_corpus_id": 3162051,
    "start": 3142,
    "end": 3162,
    "title": "mixup: BEYOND EMPIRICAL RISK MINIMIZATION",
    "abstract": "Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks. * Alphabetical order.",
    "prev": "In this phase, the model parameter gradually moves away from the correct solution until it is sufficient apart and approaches a location depending on the noise realization.",
    "curr": "INTRODUCTION\n\nMixup has empirically shown its effectiveness in improving the generalization and robustness of deep classification models (Zhang et al., 2018;Guo et al., 2019a;b;Thulasidasan et al., 2019;Zhang et al., 2022b).",
    "next": "Unlike the vanilla empirical risk minimization (ERM), in which networks are trained using the original training set, Mixup trains the networks with synthetic examples."
  },
  {
    "index": 1828,
    "source_corpus_id": 44981793,
    "ref_id": "b7",
    "citation_corpus_id": 2428314,
    "start": 4782,
    "end": 4784,
    "title": "Published as a conference paper at ICLR 2017 CATEGORICAL REPARAMETERIZATION WITH GUMBEL-SOFTMAX",
    "abstract": "Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification. * Work done during an internship at Google Brain.",
    "prev": "On a practical level, however, our training algorithm is significantly different and we show results on much larger networks then their work.",
    "curr": "Another close line of work is using the reparameterization trick for discrete variables using continuous approximations such as the Gumbal-softmax [15,8].",
    "next": "However since we are looking at the activation distribution, we can approximate using the simpler Gaussian distribution as seen in Fig 1."
  },
  {
    "index": 1829,
    "source_corpus_id": 253581364,
    "ref_id": "b4",
    "citation_corpus_id": 202749904,
    "start": 1952,
    "end": 1976,
    "title": "HIGH FIDELITY SPEECH SYNTHESIS WITH ADVERSARIAL NETWORKS",
    "abstract": "Generative adversarial networks have seen rapid development in recent years and have led to remarkable improvements in generative modelling of images. However, their application in the audio domain has received limited attention, and autoregressive models, such as WaveNet, remain the state of the art in generative modelling of audio signals such as human speech. To address this paucity, we introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech. Our architecture is composed of a conditional feed-forward generator producing raw speech audio, and an ensemble of discriminators which operate on random windows of different sizes. The discriminators analyse the audio both in terms of general realism, as well as how well the audio corresponds to the utterance that should be pronounced. To measure the performance of GAN-TTS, we employ both subjective human evaluation (MOS -Mean Opinion Score), as well as novel quantitative metrics (Fréchet DeepSpeech Distance and Kernel DeepSpeech Distance), which we find to be well correlated with MOS. We show that GAN-TTS is capable of generating high-fidelity speech with naturalness comparable to the state-of-the-art models, and unlike autoregressive models, it is highly parallelisable thanks to an efficient feed-forward generator. Listen to GAN-TTS reading this abstract at https://storage.googleapis.com/ deepmind-media/research/abstract.wav.",
    "prev": "However, this usually suffers from poor synthesis quality due to the training/inference mismatch of the acoustic model and vocoder.",
    "curr": "End-toend training methods has been recently proposed to tackle such issues (Bińkowski et al., 2020;Weiss et al., 2021;Donahue et al., 2021;.",
    "next": "Despite the high quality, however, the training process of end-to-end models is often costly, as the waveform synthesis part needs to be trained again when training each different model."
  },
  {
    "index": 1834,
    "source_corpus_id": 260378993,
    "ref_id": "b5",
    "citation_corpus_id": 51880415,
    "start": 14628,
    "end": 14642,
    "title": "How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures",
    "abstract": "With recent advances in network architectures for Neural Machine Translation (NMT) recurrent models have effectively been replaced by either convolutional or self-attentional approaches, such as in the Transformer. While the main innovation of the Transformer architecture is its use of self-attentional layers, there are several other aspects, such as attention with multiple heads and the use of many attention layers, that distinguish the model from previous baselines. In this work we take a fine-grained look at the different architectures for NMT. We introduce an Architecture Definition Language (ADL) allowing for a flexible combination of common building blocks. Making use of this language, we show in experiments that one can bring recurrent and convolutional models very close to the Transformer performance by borrowing concepts from the Transformer architecture, but not using self-attention. Additionally, we find that self-attention is much more important for the encoder side than for the decoder side, where it can be replaced by a RNN or CNN without a loss in performance in most settings. Surprisingly, even a model without any target side self-attention performs well.",
    "prev": "Normalization In Transformers, MoE layers are typically used to replace the feedforward layer in each encoder block.",
    "curr": "Thus, when using pre-normalization as most modern Transformer architectures (Domhan, 2018;Xiong et al., 2020;Riquelme et al., 2021;Fedus et al., 2022), the inputs to the MoE layer are \"layer normalized\".",
    "next": "This causes stability issues when scaling the model dimension d, since the softmax approaches a one-hot vector as d → ∞ (see Appendix E)."
  },
  {
    "index": 1838,
    "source_corpus_id": 252693138,
    "ref_id": "b3",
    "citation_corpus_id": 11336213,
    "start": 1830,
    "end": 1848,
    "title": "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches",
    "abstract": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder-Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.",
    "prev": "We demonstrate perplexity improvements with this new model on the Penn Treebank language modeling benchmark.",
    "curr": "INTRODUCTION\n\nStandard recurrent neural networks (RNNs), including simple RNNs (Elman, 1990), GRUs (Cho et al., 2014), and LSTMs (Hochreiter & Schmidhuber, 1997), rely on a fixed, finite number of neurons to remember information across timesteps.",
    "next": "When implemented with finite precision, they are theoretically just very large finite automata, restricting the class of formal languages they recognize to regular languages (Kleene, 1951)."
  },
  {
    "index": 1839,
    "source_corpus_id": 229153786,
    "ref_id": "b20",
    "citation_corpus_id": 14091946,
    "start": 4509,
    "end": 4530,
    "title": "Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies",
    "abstract": "The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture's grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.",
    "prev": "However, long-range dependencies have usually been studied with respect to a particular linguistic function (e.g.",
    "curr": "subject-verb number agreement (Linzen et al., 2016;Gulordava et al., 2018;Lakretz et al., 2019), and there has been less attention on the broader question of how sensitivity to prior context -broadly construed -is functionally organized within these recurrent networks.",
    "next": "Therefore, drawing on prior work in the neuroscience literature, here we demonstrate a model-free approach to mapping processing timescale in recurrent neural networks."
  },
  {
    "index": 1840,
    "source_corpus_id": 248118520,
    "ref_id": "b4",
    "citation_corpus_id": 11383178,
    "start": 1906,
    "end": 1908,
    "title": "IMPORTANCE WEIGHTED AUTOENCODERS",
    "abstract": "The variational autoencoder (VAE; ) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.",
    "prev": "-learning surrogate models for sequential decision making.",
    "curr": "arXiv preprint arXiv:\n\nINTRODUCTION\n\nNeural processes (NPs) have been in the spotlight as they stochastically complete unseen target points considering a given context dataset without huge inference computation (Garnelo et al., 2018a;b;Kim et al., 2019).",
    "next": "NPs leverage neural networks to derive an identifier suitable for a novel task using context representation, which contains information about given context data points."
  },
  {
    "index": 1841,
    "source_corpus_id": 247696299,
    "ref_id": "b7",
    "citation_corpus_id": 11336213,
    "start": 27125,
    "end": 27143,
    "title": "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches",
    "abstract": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder-Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.",
    "prev": "PERIMENTS\n\nThis section evaluates our approach on a diverse set of dynamical systems from the robotics domain in simulations and real systems.We show that HiP-RSSM outperforms contemporary recurrent state-space models (RSSMs) and recurrent neural networks (RNNs) by a significant margin under changing dynamics scenarios.Further, we show that HiP-RSSM outperforms these models even under situations with partial observability/ missing values.We also baseline our HiP-RSSM with contemporary multi-task models and improve performance, particularly in modelling non-Markovian dynamics and under partial observability.Finally, the visualizations of the Gaussian latent task variables in HiP-RSSM demonstrates that they learn meaningful representations of the causal factors of variations in dynamics in an unsupervised fashion.",
    "curr": "We consider the following baselines:\n\n• RNNs -We compare our method to two widely used recurrent neural network architectures, LSTMs (Hochreiter & Schmidhuber, 1997) and GRUs (Cho et al., 2014).",
    "next": "• RSSMs -Among several RSSMs from the literature, we chose RKN (Becker et al., 2019) as these have shown excellent performance for dynamics learning (Shaj et al., 2020) and relies on exact inference as in our case."
  },
  {
    "index": 1843,
    "source_corpus_id": 36549760,
    "ref_id": "b6",
    "citation_corpus_id": 6104263,
    "start": 8585,
    "end": 8607,
    "title": "Adversarially Learned Inference",
    "abstract": "We introduce the adversarially learned inference (ALI) model, which jointly learns a generation network and an inference network using an adversarial process. The generation network maps samples from stochastic latent variables to the data space while the inference network maps training examples in data space to the space of latent variables. An adversarial game is cast between these two networks and a discriminative network that is trained to distinguish between joint latent/data-space samples from the generative network and joint samples from the inference network. We illustrate the ability of the model to learn mutually coherent inference and generation networks through the inspections of model samples and reconstructions and confirm the usefulness of the learned representations by obtaining a performance competitive with other recent approaches on the semi-supervised SVHN task.",
    "prev": "GANs suffer from the \"modecollapse\" problem, where the generator assigns mass to a small subset of the support of the population distribution-that is, it may generate realistic samples, but there are many more realistic samples that it cannot generate.",
    "curr": "This is particularly problematic if we want to use GANs to manipulate data rather than generate new data; even GAN variants that include some kind of inference machinery (e.g., Donahue et al., 2016;Dumoulin et al., 2016;Perarnau et al., 2016) to determine what z best matches some x tend to produce reconstructions that are reminiscent of the input but do not preserve its identity.",
    "next": "On the other hand, VAEs (especially those with simple likelihoods π) often exhibit a tradeoff between sharp reconstructions and sensible-looking samples (see Figure 2)."
  },
  {
    "index": 1845,
    "source_corpus_id": 239015813,
    "ref_id": "b2",
    "citation_corpus_id": 22191393,
    "start": 6387,
    "end": 6405,
    "title": "Published as a conference paper at ICLR 2018 FASTGCN: FAST LEARNING WITH GRAPH CONVOLU- TIONAL NETWORKS VIA IMPORTANCE SAMPLING",
    "abstract": "The graph convolutional networks (GCN) recently proposed by Kipf and Welling are an effective graph model for semi-supervised learning. This model, however, was originally designed to be learned with the presence of both training and test data. Moreover, the recursive neighborhood expansion across layers poses time and memory challenges for training with large, dense graphs. To relax the requirement of simultaneous availability of test data, we interpret graph convolutions as integral transforms of embedding functions under probability measures. Such an interpretation allows for the use of Monte Carlo approaches to consistently estimate the integrals, which in turn leads to a batched training scheme as we propose in this work-FastGCN. Enhanced with importance sampling, FastGCN not only is efficient for training but also generalizes well for inference. We show a comprehensive set of experiments to demonstrate its effectiveness compared with GCN and related models. In particular, training is orders of magnitude more efficient while predictions remain comparably accurate. * These two authors contribute equally.",
    "prev": "Concurrently, Graph-MLP also tries to bypass GNN neighbor fetching (Hu et al., 2021) by training an MLP with a neighbor contrastive loss, but it only considers transductive but not the more practical inductive setting.",
    "curr": "Some sampling works focus on speed up GNN training (Zou et al., 2019;Chen et al., 2018), which are complementary to our goal on inference acceleration.",
    "next": "GNN distillation."
  },
  {
    "index": 1848,
    "source_corpus_id": 263830401,
    "ref_id": "b23",
    "citation_corpus_id": 212956516,
    "start": 6890,
    "end": 6894,
    "title": "Published as a conference paper at ICLR 2021 DEEP SYMBOLIC REGRESSION: RECOVERING MATHEMATICAL EXPRESSIONS FROM DATA VIA RISK-SEEKING POLICY GRADIENTS",
    "abstract": "Discovering the underlying mathematical expressions describing a dataset is a core challenge for artificial intelligence. This is the problem of symbolic regression. Despite recent advances in training neural networks to solve complex tasks, deep learning approaches to symbolic regression are underexplored. We propose a framework that leverages deep learning for symbolic regression via a simple idea: use a large model to search the space of small models. Specifically, we use a recurrent neural network to emit a distribution over tractable mathematical expressions and employ a novel risk-seeking policy gradient to train the network to generate better-fitting expressions. Our algorithm outperforms several baseline methods (including Eureqa, the gold standard for symbolic regression) in its ability to exactly recover symbolic expressions on a series of benchmark problems, both with and without added noise. More broadly, our contributions include a framework that can be applied to optimize hierarchical, variable-length objects under a blackbox performance metric, with the ability to incorporate constraints in situ, and a risk-seeking policy gradient formulation that optimizes for best-case performance instead of expected performance.",
    "prev": "In this work, we aim to infer interpretable dynamical equations when no domain knowledge is available.",
    "curr": "Approaches to symbolic regression.Symbolic regression aims to find a concise mathematical function that accurately models the data.While it is possible to generate complex analytical expressions that fit the observations, an unnecessarily lengthy function is often impractical.Symbolic regression seeks a balance between fidelity to the data and simplicity of form.Therefore, predictions are typically also evaluated in terms of some \"complexity\" metric.Because the symbolic output makes it difficult to formulate differentiable losses, SR has traditionally benefitted comparably little from advances in autodifferentiation and gradient-based optimization frameworks.The dominant approach has thus been based on evolutionary algorithms such as genetic programming (GP) [15][16][17][18][19][20], optionally guided by neural networks [21][22][23] and recently also employing reinforcement learning [24] -see [10,25] for reviews.Most of these approaches require a separate optimization for each new observed system, severely limiting scalability.",
    "next": "Transformers for symbolic regression.With the advent of transformer models [26], efficient learning of sequence-to-sequence tasks for a broad variety of modalities became feasible.Paired with large-scale pre-training on synthetic data, transformers have been used for symbolic tasks such as integration [27],\n\nformal logic [28], and theorem proving [29].Few recent works applied them to functional SR [30][31][32][33] obtaining comparable results to GP methods, with a key advantage: after one-time pre-training, inference is often orders of magnitude faster since no training is needed for previously unseen systems.Landajuela et al."
  },
  {
    "index": 1849,
    "source_corpus_id": 227746856,
    "ref_id": "b43",
    "citation_corpus_id": 604334,
    "start": 2543,
    "end": 2547,
    "title": "Intriguing properties of neural networks",
    "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
    "prev": "However, in this paper we find that while aux-inputs often help in-distribution error, they can hurt OOD error.Conversely, aux-output methods such as pre-training, transfer learning, and multi-task learning may improve OOD performance by changing the inductive bias of the model through auxiliary supervi- * Equal contribution.",
    "curr": "Introduction\n\nWhen models are tested on distributions that are different from the training distribution, they typically suffer large drops in performance [44,2,22,17,3].",
    "next": "For example, in remote sensing, central problems include predicting poverty, crop type, and land cover from satellite imagery for downstream humanitarian, policy, and environmental applications [52,21,49,40]."
  },
  {
    "index": 1852,
    "source_corpus_id": 252545361,
    "ref_id": "b28",
    "citation_corpus_id": 6628106,
    "start": 34692,
    "end": 34698,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "For validation and testing we use 80 and 300 images respectively taken from 20 random classes that are not used for training.",
    "curr": "We use mean-squared-error loss and Adam [KB14] optimizer with β 1 = 0.9, β 2 = 0.999.",
    "next": "For moderate training set sizes up to 3000 images we find that heuristically adjusting the initial learning rate with the help of an automated learning rate annealing performs well."
  },
  {
    "index": 1856,
    "source_corpus_id": 246634950,
    "ref_id": "b29",
    "citation_corpus_id": 3548196,
    "start": 2650,
    "end": 2669,
    "title": "Published as a conference paper at ICLR 2018 SENSITIVITY AND GENERALIZATION IN NEURAL NETWORKS: AN EMPIRICAL STUDY",
    "abstract": "In practice it is often found that large over-parameterized neural networks generalize better than their smaller counterparts, an observation that appears to conflict with classical notions of function complexity, which typically favor smaller models. In this work, we investigate this tension between complexity and generalization through an extensive empirical exploration of two natural metrics of complexity related to sensitivity to input perturbations. Our experiments survey thousands of models with various fully-connected architectures, optimizers, and other hyper-parameters, as well as four different image classification datasets.We find that trained neural networks are more robust to input perturbations in the vicinity of the training data manifold, as measured by the norm of the input-output Jacobian of the network, and that it correlates well with generalization. We further establish that factors associated with poor generalization -such as full-batch training or using random labels -correspond to lower robustness, while factors associated with good generalization -such as data augmentation and ReLU non-linearities -give rise to more robust functions. Finally, we demonstrate how the input-output Jacobian norm can be predictive of generalization at the level of individual test points. * Work done as a member of the Google Brain Residency program (g.co/brainresidency)",
    "prev": "INTRODUCTION\n\nMost recent breakthroughs in deep learning are fairly achieved with the increased complexity of over-parameterized networks (Brown et al., 2020;Raffel et al., 2020;Dosovitskiy et al., 2021;Fedus et al., 2021. arXiv:2101Jumper et al., 2021;Berner et al., 2019).",
    "curr": "It is well-known that large models train better (Neyshabur et al., 2019;Novak et al., 2018;Allen-Zhu et al., 2019), generalize better (Hendrycks & Dietterich, 2019;Xie & Yuille, 2020;Zhao et al., 2018), and transfer better (Chen et al., 2020b;a;2021b).",
    "next": "However, the upsurge of large models exacerbates the gap between research and practice since many real-life applications demand compact and efficient networks."
  },
  {
    "index": 1861,
    "source_corpus_id": 252780443,
    "ref_id": "b81",
    "citation_corpus_id": 1957433,
    "start": 10170,
    "end": 10194,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "Background: Pre-trained Language Models\n\nIn this section, we discuss background surrounding pretrained language models, pretraining objectives and other unified pretraining proposals.",
    "curr": "Pre-trained Language Models\n\nLearning pre-trained representations for language is a far-reaching pillar of modern NLP research, dating back to (Mikolov et al., 2013;Pennington et al., 2014;Neumann et al., 2018;Dai & Le, 2015;Howard & Ruder, 2018).",
    "next": "The first pre-trained Transformer, GPT, was proposed by (Radford et al., 2019) and was trained as a causal language model."
  },
  {
    "index": 1862,
    "source_corpus_id": 265038030,
    "ref_id": "b14",
    "citation_corpus_id": 3366315,
    "start": 2403,
    "end": 2423,
    "title": "Published as a conference paper at ICLR 2018 SPECTRAL NORMALIZATION FOR GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. The code with Chainer (Tokui et al., 2015), generated images and pretrained models are available at https://github.com/pfnet-research/sngan_ projection.Published as a conference paper at ICLR 2018• Lipschitz constant is the only hyper-parameter to be tuned, and the algorithm does not require intensive tuning of the only hyper-parameter for satisfactory performance. • Implementation is simple and the additional computational cost is small.",
    "prev": "The original GAN is, however, highly unstable and often suffers from mode collapse.",
    "curr": "Much of recent researches has focused on improving the stability of GANs (Radford et al., 2015;Heusel et al., 2017;Miyato et al., 2018;Karras et al., 2018).",
    "next": "On the theoretical aspect, Nagarajan & Kolter (2017) proved that gradient based training of the original GAN is locally stable."
  },
  {
    "index": 1863,
    "source_corpus_id": 253238010,
    "ref_id": "b12",
    "citation_corpus_id": 222090060,
    "start": 13015,
    "end": 13042,
    "title": "Published as a conference paper at ICLR 2021 INTERPRETING GRAPH NEURAL NETWORKS FOR NLP WITH DIFFERENTIABLE EDGE MASKING",
    "abstract": "Graph neural networks (GNNs) have become a popular approach to integrating structural inductive biases into NLP models. However, there has been little work on interpreting them, and specifically on understanding which parts of the graphs (e.g. syntactic trees or co-reference structures) contribute to a prediction. In this work, we introduce a post-hoc method for interpreting the predictions of GNNs which identifies unnecessary edges. Given a trained GNN model, we learn a simple classifier that, for every edge in every layer, predicts if that edge can be dropped. We demonstrate that such a classifier can be trained in a fully differentiable fashion, employing stochastic gates and encouraging sparsity through the expected L 0 norm. We use our technique as an attribution method to analyse GNN models for two tasks -question answering and semantic role labelling -providing insights into the information flow in these models. We show that we can drop a large proportion of edges without deteriorating the performance of the model, while we can analyse the remaining edges for interpreting model predictions.",
    "prev": "the input features or intermediate embeddings of each point.",
    "curr": "Some methods to interpret graph neural networks can be applied to geometric data (Ying et al., 2019;Luo et al., 2020;Schlichtkrull et al., 2021;Yuan et al., 2021 Luo et al., 2020).",
    "next": "So, some works focusing on improving the attention mechanism for better interpretability (Bai et al., 2021;Miao et al., 2022), some propose to identify representative prototypes during training Chen et al., 2019a), and some methods (Taghanaki et al., 2020;Sun et al., 2022) adopt the information bottleneck principle (Tishby et al., 2000)."
  },
  {
    "index": 1864,
    "source_corpus_id": 256503575,
    "ref_id": "b25",
    "citation_corpus_id": 5037032,
    "start": 2287,
    "end": 2308,
    "title": "ZERO-SHOT VISUAL IMITATION",
    "abstract": "The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both what and how to imitate.We pursue an alternative paradigm wherein an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss.In our framework, the role of the expert is only to communicate the goals (i.e., what to imitate) during inference.The learned policy is then employed to mimic the expert (i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task.Our method is \"zero-shot\" in the sense that the agent never has access to expert actions during training or for the task demonstration at inference.We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen office environments with a TurtleBot.Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance.Videos, models, and more details are available at https://pathak22.github.io/zeroshot-imitation/. * Denotes equal contribution.",
    "prev": "Beyond hand-crafting reward functions that are non-trivial, imitation learning (IL) offers a data-driven way to learn behaviors and recover informative rewards from expert demonstrations without access to any explicit reward (Arora & Doshi, 2021;Hussein et al., 2017).",
    "curr": "Recently, visual imitation learning (VIL) (Pathak et al., 2018;Rafailov et al., 2021) has attracted increasing attention, which aims to learn from high-dimensional visual demonstrations like image sequences or videos.",
    "next": "Compared with previous IL works tackling low-dimensional inputs, i.e."
  },
  {
    "index": 1866,
    "source_corpus_id": 255340832,
    "ref_id": "b45",
    "citation_corpus_id": 3257353,
    "start": 5264,
    "end": 5282,
    "title": "That's So Annoying!!!: A Lexical and Frame-Semantic Embedding Based Data Augmentation Approach to Automatic Categorization of Annoying Behaviors using #petpeeve Tweets *",
    "abstract": "We propose a novel data augmentation approach to enhance computational behavioral analysis using social media text. In particular, we collect a Twitter corpus of the descriptions of annoying behaviors using the #petpeeve hashtags. In the qualitative analysis, we study the language use in these tweets, with a special focus on the fine-grained categories and the geographic variation of the language. In quantitative analysis, we show that lexical and syntactic features are useful for automatic categorization of annoying behaviors, and frame-semantic features further boost the performance; that leveraging large lexical embeddings to create additional training instances significantly improves the lexical model; and incorporating frame-semantic embedding achieves the best overall performance.",
    "prev": "However, the label of the augmented pair will still be \"Entailment\".",
    "curr": "Karimi et al., 2021;Fadaee et al., 2017;Sennrich et al., 2015;Wang & Yang, 2015;Andreas, 2020;Kobayashi, 2018).",
    "next": "These augmentation methods are largely tailored to a particular modality in isolation."
  },
  {
    "index": 1867,
    "source_corpus_id": 211068995,
    "ref_id": "b21",
    "citation_corpus_id": 49881601,
    "start": 2786,
    "end": 2809,
    "title": "Efficient Training on Very Large Corpora via Gramian Estimation",
    "abstract": "We study the problem of learning similarity functions over very large corpora using neural network embedding models. These models are typically trained using SGD with sampling of random observed and unobserved pairs, with a number of samples that grows quadratically with the corpus size, making it expensive to scale to very large corpora. We propose new efficient methods to train these models without having to sample unobserved pairs. Inspired by matrix factorization, our approach relies on adding a global quadratic penalty to all pairs of examples and expressing this term as the matrix-inner-product of two generalized Gramians. We show that the gradient of this term can be efficiently computed by maintaining estimates of the Gramians, and develop variance reduction schemes to improve the quality of the estimates. We conduct large-scale experiments that show a significant improvement in training time and generalization quality compared to traditional sampling methods. * Google Research. arXiv:1807.07187v1 [stat.ML] 18 Jul 2018 1 In many applications, it is desirable for the two embedding functions u, v to share certain parameters, e.g. embeddings of categorical features common to left and right items; hence, we use the same θ for both.2 This also includes cosine similarity models when the embedding functions u, v are normalized. 3 One advantage of an inner-product model is that it allows for efficient retrieval: given a query item x, the problem of retrieving items y with high similarity to x is a maximum inner product search problem (MIPS), which can be approximated efficiently [Shrivastava and Li, 2014, Neyshabur andSrebro, 2015].",
    "prev": "Many real-world applications besides query-document retrieval can be cast into this form.",
    "curr": "For example, in recommendation systems, q represents a user query and d represents a candidate item to recommend (Krichene et al., 2019).",
    "next": "In extreme multi-label classification, q represents a web-page document and d represents the categories or hashtags of interests (Jain et al., 2019;."
  },
  {
    "index": 1869,
    "source_corpus_id": 86393936,
    "ref_id": "b6",
    "citation_corpus_id": 3300406,
    "start": 4712,
    "end": 4734,
    "title": "EIGENOPTION DISCOVERY THROUGH THE DEEP SUCCESSOR REPRESENTATION",
    "abstract": "Options in reinforcement learning allow agents to hierarchically decompose a task into subtasks, having the potential to speed up learning and planning. However, autonomously learning effective sets of options is still a major challenge in the field. In this paper we focus on the recently introduced idea of using representation learning methods to guide the option discovery process. Specifically, we look at eigenoptions, options obtained from representations that encode diffusive information flow in the environment. We extend the existing algorithms for eigenoption discovery to settings with stochastic transitions and in which handcrafted features are not available. We propose an algorithm that discovers eigenoptions while learning non-linear state representations from raw pixels. It exploits recent successes in the deep reinforcement learning literature and the equivalence between proto-value functions and the successor representation. We use traditional tabular domains to provide intuition about our approach and Atari 2600 games to demonstrate its potential.",
    "prev": "PVFs have also been proposed by neuroscientists as a model for the emergence of grid cells in the entorhinal cortex (Stachenfeld et al., 2017).",
    "curr": "The use of PVFs for discovering subgoals in reinforcement learning has been investigated in (Machado et al., 2017) and combined with function approximation in (Machado et al., 2018), though using a less rigorous approach to eigenfunction approximation than SpIN.",
    "next": "A qualitative comparison of the two approaches is given in the supplementary material in Sec."
  },
  {
    "index": 1870,
    "source_corpus_id": 252872842,
    "ref_id": "b36",
    "citation_corpus_id": 20827927,
    "start": 3021,
    "end": 3041,
    "title": "Variational Inference of Disentangled Latent Concepts from Unlabeled Observations",
    "abstract": "Disentangled representations, where the higher level data generative factors are reflected in disjoint latent dimensions, offer several benefits such as ease of deriving invariant representations, transferability to other tasks, interpretability, etc. We consider the problem of unsupervised learning of disentangled representations from large pool of unlabeled observations, and propose a variational inference based approach to infer disentangled latent factors. We introduce a regularizer on the expectation of the approximate posterior over observed data that encourages the disentanglement. We evaluate the proposed approach using several quantitative metrics and empirically observe significant gains over existing methods in terms of both disentanglement and data likelihood (reconstruction quality).",
    "prev": "Variational autoencoders (VAEs) achieve this balance by maximizing a lower bound on the data log-likelihood, which consists of a reconstruction term and a Kullback-Leibler (KL) divergence term between the posterior and prior distributions (Kingma & Welling, 2013;.",
    "curr": "The development of new GAE meta-priors (Bengio et al., 2013) generalizes assumptions about the data distribution in the latent space and have enabled the learning of representations that are hierarchically organized (Sønderby et al., 2016;Vahdat & Kautz, 2020), semi-supervised (Kingma et al., 2014), clustered (Makhzani et al., 2015;Lechat et al., 2021), or disentangled (Kumar et al., 2018;Horan et al., 2021).",
    "next": "Meta-priors are typically enforced with an additive term in the GAE loss function that minimizes a statistical distance, e.g., KL divergence, between the posterior and a specific prior (Tschannen et al., 2018)."
  },
  {
    "index": 1871,
    "source_corpus_id": 247222625,
    "ref_id": "b30",
    "citation_corpus_id": 29842525,
    "start": 3190,
    "end": 3216,
    "title": "Published as a conference paper at ICLR 2017 AUTOENCODING VARIATIONAL INFERENCE FOR TOPIC MODELS",
    "abstract": "Topic models are one of the most popular methods for learning representations of text, but a major challenge is that any change to the topic model requires mathematically deriving a new inference algorithm. A promising approach to address this problem is autoencoding variational Bayes (AEVB), but it has proven difficult to apply to topic models in practice. We present what is to our knowledge the first effective AEVB based inference method for latent Dirichlet allocation (LDA), which we call Autoencoded Variational Inference For Topic Model (AVITM). This model tackles the problems caused for AEVB by the Dirichlet prior and by component collapsing. We find that AVITM matches traditional methods in accuracy with much better inference time. Indeed, because of the inference network, we find that it is unnecessary to pay the computational cost of running variational optimization on test data. Because AVITM is black box, it is readily applied to new topic models. As a dramatic illustration of this, we present a new topic model called ProdLDA, that replaces the mixture model in LDA with a product of experts. By changing only one line of code from LDA, we find that ProdLDA yields much more interpretable topics, even if LDA is trained via collapsed Gibbs sampling.",
    "prev": "Bayesian inference of a BPTM is usually based on Gibbs sampling or variational inference (VI), which can be less scalable for big corpora and need to be customized accordingly.",
    "curr": "With the recent development in auto-encoding VI, originated from variational autoencoders (VAEs) (Kingma & Welling, 2014;Rezende et al., 2014), deep neural networks have been successfully used to develop neural topic models (NTMs) (Miao et al., 2016;Srivastava & Sutton, 2017;Burkhardt & Kramer, 2019;Zhang et al., 2018;Dieng et al., 2020;Zhao et al., 2021).",
    "next": "The key advantage of NTMs is that approximate posterior inference can be carried out easily via a forward pass of the encoder network, without the need for expensive iterative inference scheme per test observation as in both Gibbs sampling and conventional VI."
  },
  {
    "index": 1873,
    "source_corpus_id": 226281876,
    "ref_id": "b19",
    "citation_corpus_id": 208547755,
    "start": 2101,
    "end": 2104,
    "title": "Published as a conference paper at ICLR 2020 DREAM TO CONTROL: LEARNING BEHAVIORS BY LATENT IMAGINATION",
    "abstract": "Learned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.",
    "prev": "These results indicate where and how to utilize planning in reinforcement learning settings, and highlight a number of open questions for future MBRL research.",
    "curr": "Model-based reinforcement learning (MBRL) has seen much interest in recent years, with advances yielding impressive gains over model-free methods in data efficiency [10,12,20,67], zero-and few-shot learning [13,31,53], and strategic thinking [3,55,56,57,51].",
    "next": "These methods combine planning and learning in a variety of ways, with planning specifically referring to the process of using a learned or given model of the world to construct imagined future trajectories or plans."
  },
  {
    "index": 1874,
    "source_corpus_id": 220920191,
    "ref_id": "b35",
    "citation_corpus_id": 3626819,
    "start": 2303,
    "end": 2324,
    "title": "Deep contextualized word representations",
    "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
    "prev": "We will release the source codes as soon as possible.",
    "curr": "INTRODUCTION\n\nPre-trained models (PTMs) like ELMo (Peters et al., 2018), GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) have shown remarkable success of effectively transferring knowledge learned from large-scale unlabeled data to downstream NLP tasks, such as text classification (Socher et al., 2013) and natural language inference (Bowman et al., 2015;, with limited or no training data.",
    "next": "To extend such pretraining-finetuning paradigm to multiple languages, some endeavors such as multilingual BERT (Devlin et al., 2019) and XLM (Conneau & Lample, 2019) have been made for learning cross-lingual representation."
  },
  {
    "index": 1875,
    "source_corpus_id": 246634506,
    "ref_id": "b37",
    "citation_corpus_id": 3292002,
    "start": 28464,
    "end": 28489,
    "title": "GRAPH ATTENTION NETWORKS",
    "abstract": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-theart results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a proteinprotein interaction dataset (wherein test graphs remain unseen during training).",
    "prev": "3(c), we compare the test accuracy averaged on eight graphs when using different GNNs e.g.",
    "curr": "GCN, SGC (Wu et al., 2019) and GAT (Velickovic et al., 2018), for data generation (See Appendix G for more results).",
    "next": "The results verify that our approach achieves consistently superior performance in different cases."
  },
  {
    "index": 1876,
    "source_corpus_id": 263671952,
    "ref_id": "b15",
    "citation_corpus_id": 40100965,
    "start": 2275,
    "end": 2299,
    "title": "Universal Language Model Fine-tuning for Text Classification",
    "abstract": "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100× more data. We opensource our pretrained models and code 1 .",
    "prev": "y targeted outcomes, akin to goal-conditioned policies in reinforcement learning.We show that the pre-trained OC-GFN model can allow for a direct extraction of a policy capable of sampling from any new reward functions in downstream tasks.Nonetheless, adapting OC-GFN on a downstream task-specific reward involves an intractable marginalization over possible outcomes.We propose a novel way to approximate this marginalization by learning an amortized predictor enabling efficient fine-tuning.Extensive experimental results validate the efficacy of our approach, demonstrating the effectiveness of pre-training the OC-GFN, and its ability to swiftly adapt to downstream tasks and discover modes more efficiently.This work may serve as a foundation for further exploration of pre-training strategies in the context of GFlowNets.",
    "curr": "Introduction\n\nUnsupervised learning on large stores of data on the internet has resulted in significant advances in a variety of domains (Devlin et al., 2018, Henaff, 2020, Howard and Ruder, 2018, Radford et al., 2019).Pre-training with unsupervised objectives, such as next-token prediction in auto-regressive language models (Radford et al., 2019), on large-scale unlabelled data enables the development of models that can be effectively fine-tuned for novel tasks using few samples (Brown et al., 2020).Unsupervised learning at scale allows models to learn good representations, which enables data-efficient adaptation to novel tasks, and is central to the recent development towards larger models.",
    "next": "On the other hand, in the context of amortized inference, Generative Flow Networks (GFlowNets; Bengio et al., 2021) enable learning generative models for sampling from high-dimensional distributions over discrete compositional objects.Inspired by reinforcement learning (RL), GFlowNets learn a stochastic policy to sequentially generate compositional objects with a probability proportional to a given reward, instead of reward maximization.Therefore, GFlowNets have found success in applicat"
  },
  {
    "index": 1878,
    "source_corpus_id": 233287919,
    "ref_id": "b5",
    "citation_corpus_id": 1998416,
    "start": 4092,
    "end": 4112,
    "title": "Effective Approaches to Attention-based Neural Machine Translation",
    "abstract": "An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker. 1",
    "prev": "Reinforce (Sutton et al., 2000), PPO (Schulman et al., 2017)) are applied to update the permutation-generating encoder.",
    "curr": "then, the left-to-right autoregressive order has been essential for application domains such as image captioning (Vinyals et al., 2015b;Xu et al., 2015), machine translation (Luong et al., 2015;Bahdanau et al., 2015) and distant fields like image synthesis (van den Oord et al., 2016).",
    "next": "However, interest in non left-to-right autoregressive orders is resurfacing (Welleck et al., 2019b;Stern et al., 2019), and evidence Gū et al., 2018;Alvarez-Melis & Jaakkola, 2017) suggests adaptive orders may produce more accurate autoregressive models."
  },
  {
    "index": 1879,
    "source_corpus_id": 257232422,
    "ref_id": "b16",
    "citation_corpus_id": 3144218,
    "start": 15698,
    "end": 15719,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": ", N T , the nonlinear activation function ρ : R → R after each layer, and parameters ζ.",
    "curr": "For graph convolutional networks, we adapt the approach by Kipf & Welling (2017) to data z 0 e ∈ R N 0 on edges e: The t th layer is given by\n(5) z t +1 e = ρ W t +1 1 z t e + W t +1 2 ẽ∈N (e) z tẽ + b t +1 where W t +1 1 ∈ R N t +1 ×N t and W t +1 2 ∈ R N t +1\n×N t are small matrices with learnable parameters and b t +1 ∈ R N t +1 is the bias, all stored in the parameters ζ.",
    "next": "We define the neighborhood of an edge in a triangle mesh as N (e) := {ẽ ∈ E | e andẽ share a vertex}."
  },
  {
    "index": 1880,
    "source_corpus_id": 252668917,
    "ref_id": "b0",
    "citation_corpus_id": 202776931,
    "start": 29650,
    "end": 29671,
    "title": "JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation",
    "abstract": "Interactive programming with interleaved code snippet cells and natural language markdown is recently gaining popularity in the form of Jupyter notebooks, which accelerate prototyping and collaboration. To study code generation conditioned on a long context history, we present JuICe, a corpus of 1.5 million examples with a curated test set of 3.7K instances based on online programming assignments. Compared with existing contextual code generation datasets, JuICe provides refined human-curated data, open-domain code, and an order of magnitude more training data. Using JuICe, we train models for two tasks:(1) generation of the API call sequence in a code cell, and (2) full code cell generation, both conditioned on the NL-Code history up to a particular code cell. Experiments using current baseline code generation models show that both context and distant supervision aid in generation, and that the dataset is challenging for current systems.",
    "prev": "In most of those cases, only directly relevant input information is given to the model.",
    "curr": "In contrast, a few previous works instantiate benchmarks that measure the ability to generate programs given surrounding program context beyond the target program, such as variables and other methods (Iyer et al., 2018) or alternating \"cells\" of preceding code and text blocks (Agashe et al., 2019), while the primary focus is to generate the target program itself.",
    "next": "We propose a new benchmark that requires a progressive generation of subprograms through multi-turn prompts."
  },
  {
    "index": 1882,
    "source_corpus_id": 264426077,
    "ref_id": "b44",
    "citation_corpus_id": 228376209,
    "start": 12136,
    "end": 12140,
    "title": "CONVEX POTENTIAL FLOWS: UNIVERSAL PROBABILITY DISTRIBUTIONS WITH OPTIMAL TRANSPORT AND CONVEX OPTIMIZATION",
    "abstract": "Flow-based models are powerful tools for designing probabilistic models with tractable density. This paper introduces Convex Potential Flows (CP-Flow), a natural and efficient parameterization of invertible models inspired by the optimal transport (OT) theory. CP-Flows are the gradient map of a strongly convex neural potential function. The convexity implies invertibility and allows us to resort to convex optimization to solve the convex conjugate for efficient inversion. To enable maximum likelihood training, we derive a new gradient estimator of the log-determinant of the Jacobian, which involves solving an inverse-Hessian vector product using the conjugate gradient method. The gradient estimator has constantmemory cost, and can be made effectively unbiased by reducing the error tolerance level of the convex optimization routine. Theoretically, we prove that CP-Flows are universal density approximators and are optimal in the OT sense. Our empirical results show that CP-Flow performs competitively on standard benchmarks of density estimation and variational inference. arXiv:2012.05942v1 [cs.LG] 10 Dec 2020 for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018. . Ot-flow: Fast and accurate continuous normalizing flows via optimal transport. arXiv preprint arXiv:2006.00104, 2020. George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for density estimation. Ludger Rüschendorf and Svetlozar T Rachev. A characterization of random variables with minimum l2-distance. Journal of multivariate analysis, 32(1):48-54, 1990. Filippo Santambrogio. Optimal transport for applied mathematicians. Birkäuser, NY, 55(58-63):94, 2015.",
    "prev": "The simple proof of this result follows by combining properties of input convex neural networks from Amos et al.",
    "curr": "[4] and the characterization of proximal operators from Gribonval and Nikolova [43] (see Appendix C.1).The C 2 condition for the nonlinearity 3 g is imposed to ensure differentiability of the ICNN ψ θ and the LPN f θ , which will become useful shortly.Although this rules out popular choices like Rectifying Linear Units (ReLUs), there exist a wide range of eligible options satisfying all the constraints.Following [45], we adopt the softplus function g(x) = 1 β log(1 + exp (βx)), a β-smooth approximation of ReLU.Importantly, LPN can be highly expressive (representing any continous proximal operator) under reasonable settings, given the universality of ICNN [45].",
    "next": "Networks that are defined as gradients of ICNN have been explored in inverse problems in a related work.Cohen et al."
  },
  {
    "index": 1883,
    "source_corpus_id": 52911374,
    "ref_id": "b13",
    "citation_corpus_id": 14717992,
    "start": 20978,
    "end": 21001,
    "title": "REINFORCEMENT LEARNING WITH UNSUPERVISED AUXILIARY TASKS",
    "abstract": "Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-theart on Atari, averaging 880% expert human performance, and a challenging suite of first-person, three-dimensional Labyrinth tasks leading to a mean speedup in learning of 10× and averaging 87% expert human performance on Labyrinth.Natural and artificial agents live in a stream of sensorimotor data. At each time step t, the agent receives observations o t and executes actions a t . These actions influence the future course of the sensorimotor stream. In this paper we develop agents that learn to predict and control this stream, by solving a host of reinforcement learning problems, each focusing on a distinct feature of the sensorimotor stream. Our hypothesis is that an agent that can flexibly control its future experiences will also be able to achieve any goal with which it is presented, such as maximising its future rewards.The classic reinforcement learning paradigm focuses on the maximisation of extrinsic reward. However, in many interesting domains, extrinsic rewards are only rarely observed. This raises questions of what and how to learn in their absence. Even if extrinsic rewards are frequent, the sensorimotor stream contains an abundance of other possible learning targets. Traditionally, unsupervised learning attempts to reconstruct these targets, such as the pixels in the current or subsequent frame. It is typically used to accelerate the acquisition of a useful representation. In contrast, our learning objective is to predict and control features of the sensorimotor stream, by treating them as pseudorewards for reinforcement learning. Intuitively, this set of tasks is more closely matched with the agent's long-term goals, potentially leading to more useful representations.Consider a baby that learns to maximise the cumulative amount of red that it observes. To correctly predict the optimal value, the baby must understand how to increase \"redness\" by various means, including manipulation (bringing a red object closer to the eyes); locomotion (moving in front of a red object); and communication (crying until the parents bring a red object). These behaviours are likely to recur for many other goals that the baby may subsequently encounter. No understanding of these behaviours is required to simply reconstruct the redness of current or subsequent images.Our architecture uses reinforcement learning to approximate both the optimal policy and optimal value function for many different pseudo-rewards. It also makes other auxiliary predictions that serve to focus the agent on important aspects of the task. These include the long-term goal of predicting cumulative extrinsic reward as well as short-term predictions of extrinsic reward. To learn more efficiently, our agents use an experience replay mechanism to provide additional updates * Joint first authors. Ordered alphabetically by first name.",
    "prev": "This naturally emerging curriculum expedites learning in the AGILE-A3C when compared to the A3C-trained policy that only receives signal upon reaching a perfect goal state.",
    "curr": "We did observe, however, that the A3C algorithm could be improved significantly by applying the auxiliary task of reward prediction (RP; Jaderberg et al., 2016), which was applied to language learning tasks by Hermann et al.",
    "next": "(2017) (see the A3C and A3C-RP curves in Figure 4)."
  },
  {
    "index": 1884,
    "source_corpus_id": 238419650,
    "ref_id": "b30",
    "citation_corpus_id": 11758569,
    "start": 2773,
    "end": 2795,
    "title": "UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks -demonstrating their applicability as general image representations.arXiv:1511.06434v2 [cs.LG] 7 Jan 2016Under review as a conference paper at ICLR 2016• We show that the generators have interesting vector arithmetic properties allowing for easy manipulation of many semantic qualities of generated samples.",
    "prev": "INTRODUCTION\n\nSince the discovery of Generative Adversarial Networks (GANs, Goodfellow et al.",
    "curr": "(2014)), there has been a surge in generative modeling (Radford et al., 2016;Brock et al., 2019;Karras et al., 2019).",
    "next": "In the past few years, Optimal Transport (OT, Villani (2008)) theory has been pivotal in addressing important issues of generative models."
  },
  {
    "index": 1885,
    "source_corpus_id": 9128667,
    "ref_id": "b23",
    "citation_corpus_id": 71638,
    "start": 2410,
    "end": 2430,
    "title": "Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning",
    "abstract": "While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning -leveraging unlabeled examples to learn about the structure of a domain -remains a difficult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predictive neural network (\"PredNet\") architecture that is inspired by the concept of \"predictive coding\" from the neuroscience literature. These networks learn to predict future frames in a video sequence, with each layer in the network making local predictions and only forwarding deviations from those predictions to subsequent network layers. We show that these networks are able to robustly learn to predict the movement of synthetic (rendered) objects, and that in doing so, the networks learn internal representations that are useful for decoding latent object parameters (e.g. pose) that support object recognition with fewer training views. We also show that these networks can scale to complex natural image streams (car-mounted camera videos), capturing key aspects of both egocentric movement and the movement of objects in the visual scene, and generalizing across video datasets. These results suggest that prediction represents a powerful framework for unsupervised learning, allowing for implicit learning of object and scene structure.",
    "prev": "INTRODUCTION\n\nUnderstanding the interaction dynamics of objects and predicting what happens next is one of the key capabilities of humans which we heavily rely on to make decisions in everyday life (Bubic et al., 2010).",
    "curr": "A model that can accurately predict future observations of complex sensory modalities such as vision must internally represent the complex dynamics of real-world objects and people, and therefore is more likely to acquire a representation that can be used for a variety of visual perception tasks, such as object tracking and action recognition (Srivastava et al., 2015;Lotter et al., 2017;Denton & Birodkar, 2017).",
    "next": "Furthermore, such models can be inherently useful themselves, for example, to allow an autonomous agent or robot to decide how to interact with the world to bring about a desired outcome (Oh et al., 2015;."
  },
  {
    "index": 1887,
    "source_corpus_id": 223953610,
    "ref_id": "b15",
    "citation_corpus_id": 84591,
    "start": 6923,
    "end": 6945,
    "title": "Adversarial Feature Learning",
    "abstract": "The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing generators learn to \"linearize semantics\" in the latent space of such models. Intuitively, such latent spaces may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.",
    "prev": "There has been a wide range of research on self-supervised learning of visual representations in which properties of the images themselves act as supervision.",
    "curr": "The objectives for these methods cover a variety of tasks such as solving jigsaw puzzles (Noroozi & Favaro, 2016), colorizing grayscale images , learning to count (Noroozi et al., 2017), predicting context (Doersch et al., 2015), inpainting (Pathak et al., 2016), adversarial training (Donahue et al., 2017) and predicting image rotations (Gidaris et al., 2018).",
    "next": "This type of representation learning is not limited to learning from single frames."
  },
  {
    "index": 1888,
    "source_corpus_id": 3508167,
    "ref_id": "b9",
    "citation_corpus_id": 12639289,
    "start": 17885,
    "end": 17889,
    "title": "Recurrent Continuous Translation Models",
    "abstract": "We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.",
    "prev": "We use a simplified version of this neural attention mechanism in SliceNet, as introduced above.",
    "curr": "Convolutional architectures have been used to obtain good results in word-level neural machine translation starting from [10] and later in [14].",
    "next": "These early models used a standard RNN on top of the convolution to generate the output."
  },
  {
    "index": 1889,
    "source_corpus_id": 108306873,
    "ref_id": "b20",
    "citation_corpus_id": 14992224,
    "start": 5950,
    "end": 5968,
    "title": "Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data",
    "abstract": "We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions by means of variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction. * Justin Bayer is also affiliated with sensed.io UG (haftungsbeschränkt),",
    "prev": "Indeed, the idea of leveraging domain knowledge and imposing model driven constraints on the latent space, has been studied in different domains.",
    "curr": "Much of this work is focused on learning representations for predicting and controlling physical systems where various assumptions about the underlying dynamic model are made (Watter et al., 2015;Iten et al., 2018;Fraccaro et al., 2017;Karl et al., 2017).",
    "next": "Bezenac et al."
  },
  {
    "index": 1891,
    "source_corpus_id": 253117027,
    "ref_id": "b26",
    "citation_corpus_id": 235313916,
    "start": 5490,
    "end": 5510,
    "title": "Provably Secure Generative Linguistic Steganography",
    "abstract": "Generative linguistic steganography mainly utilized language models and applied steganographic sampling (stegosampling) to generate high-security steganographic text (stegotext). However, previous methods generally lead to statistical differences between the conditional probability distributions of stegotext and natural text, which brings about security risks. In this paper, to further ensure security, we present a novel provably secure generative linguistic steganographic method ADG, which recursively embeds secret information by Adaptive Dynamic Grouping of tokens according to their probability given by an offthe-shelf language model. We not only prove the security of ADG mathematically, but also conduct extensive experiments on three public corpora to further verify its imperceptibility. The experimental results reveal that the proposed method is able to generate stegotext with nearly perfect security.",
    "prev": " approximation procedures to construct couplings between one factorable distribution and one autoregressively specified distribution, both having arbitrarily large supports, while still retaining marginalization guarantees.We show that, because ciphertext can be made to look uniformly random, and any distribution of covertext can be specified autoregressively, iMEC can be leveraged to perform steganography with arbitrary covertext distributions and plaintext messages.To the best of our knowledge, this represents the first instance of a steganography algorithm with perfect security guarantees that is applicable to arbitrary distributions of covertext.",
    "curr": "In our experiments, we evaluate iMEC using GPT-2 (Radford et al., 2019) (a language model), WaveRNN (Kalchbrenner et al., 2018) (an audio model), and Image Transfomer (an image model) as covertext distributions.We compare against arithmetic coding (Ziegler et al., 2019), Meteor (Kaptchuk et al., 2021), and adaptive dynamic grouping (ADG) (Zhang et al., 2021), other recent methods for performing information theoretic-steganography with deep generative models.To examine empirical security, we estimate the KL divergence between the stegotext and the covertext for each method.For iMEC, we find that the divergence is on the order of numerical precision, in agreement with our theoretical guarantees.In contrast, arithmetic coding, Meteor, and ADG yield KL divergences many orders of magnitude larger, reflecting their weaker security guarantees.To examine encoding efficiency, we measure the number of bits transmitted per step.We find that iMEC generally yields superior efficiency results to those of arithmetic coding, Meteor, and ADG, despite its stricter constraints.",
    "next": "We would summarize our theoretical results as showing that minimum entropy coupling-based approaches are the most information efficient perfect security approaches and our empirical results as showing that minimum entropy coupling-based approaches can be more information efficient t"
  },
  {
    "index": 1892,
    "source_corpus_id": 259286823,
    "ref_id": "b57",
    "citation_corpus_id": 173991122,
    "start": 2101,
    "end": 2104,
    "title": "Published as a conference paper at ICLR 2020 CONTINUAL LEARNING WITH HYPERNETWORKS",
    "abstract": "Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, we present a novel approach based on task-conditioned hypernetworks, i.e., networks that generate the weights of a target model based on task identity. Continual learning (CL) is less difficult for this class of models thanks to a simple key feature: instead of recalling the input-output relations of all previously seen data, task-conditioned hypernetworks only require rehearsing task-specific weight realizations, which can be maintained in memory using a simple regularizer. Besides achieving state-ofthe-art performance on standard CL benchmarks, additional experiments on long task sequences reveal that task-conditioned hypernetworks display a very large capacity to retain previous memories. Notably, such long memory lifetimes are achieved in a compressive regime, when the number of trainable hypernetwork weights is comparable or smaller than target network size. We provide insight into the structure of low-dimensional task embedding spaces (the input space of the hypernetwork) and show that task-conditioned hypernetworks demonstrate transfer learning. Finally, forward information transfer is further supported by empirical results on a challenging CL benchmark based on the CIFAR-10/100 image datasets.10 Published as a conference paper at ICLR 2020 Andrew K Lampinen and James L McClelland. Embedded meta-learning: Toward more flexible deep-learning models. arXiv preprint arXiv:1905.09950, 2019.Moshe Leshno and Shimon Schocken. Multilayer feedforward networks with a nonpolynomial activation function can approximate any function. Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization.",
    "prev": "We provide easyto-use code that can turn existing networks into MIP-based hypernetworks.",
    "curr": "Introduction\n\nHypernetworks, neural networks that predict the parameters of another neural network, are increasingly important models in a wide range of applications such as Bayesian optimization [27,39,54], generative models [1,10,44,61], amortized model learning [3,11,22,38,58], continual learning [12,21,57], multi-task learning [34,49,53], and meta-learning [6,62,63].",
    "next": "Despite their advantages and growing use, training hypernetworks is challenging."
  },
  {
    "index": 1894,
    "source_corpus_id": 246411225,
    "ref_id": "b22",
    "citation_corpus_id": 222208633,
    "start": 3132,
    "end": 3150,
    "title": "DEFORMABLE DETR: DEFORMABLE TRANSFORMERS FOR END-TO-END OBJECT DETECTION",
    "abstract": "DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10× less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code shall be released. * Equal contribution. † This work is done when Weijie Su is an intern at SenseTime Research.",
    "prev": "However, due to its ineffective design and use of queries, DETR suffers from significantly slow training convergence, usually requiring 500 epochs to achieve a good performance.",
    "curr": "To address this issue, many follow-up works attempted to improve the design of DETR queries for both faster training convergence and better performance (Zhu et al., 2021;Gao et al., 2021;Meng et al., 2021;.",
    "next": "Despite all the progress, the role of the learned queries in DETR is still not fully understood or utilized."
  },
  {
    "index": 1895,
    "source_corpus_id": 259108693,
    "ref_id": "b28",
    "citation_corpus_id": 235652267,
    "start": 4540,
    "end": 4544,
    "title": "Published as a conference paper at ICLR 2022 ASSESSING GENERALIZATION VIA DISAGREEMENT",
    "abstract": "We empirically show that the test error of deep networks can be estimated by training the same architecture on the same training set but with two different runs of Stochastic Gradient Descent (SGD), and then measuring the disagreement rate between the two networks on unlabeled test data. This builds on -and is a stronger version of -the observation in Nakkiran & Bansal (2020), which requires the runs to be on separate training sets. We further theoretically show that this peculiar phenomenon arises from the well-calibrated nature of ensembles of SGD-trained models. This finding not only provides a simple empirical measure to directly predict the test error using unlabeled test data, but also establishes a new conceptual connection between generalization and calibration.",
    "prev": "In fact, the same models trained with different random seeds would learn different information that leads to different predictions [35].",
    "curr": "This phenomenon has important downstream consequences for ensembling randomly initialized networks including better generalization [1], calibration [35], and the Generalization Disagreement Equality (GDE) [29] where the expected test accuracy is equal to the expected agreement in deep ensembles.",
    "next": "We postulate that a good definition of features should be fine-grained enough to discern the difference in the knowledge of different models."
  },
  {
    "index": 1899,
    "source_corpus_id": 261493986,
    "ref_id": "b30",
    "citation_corpus_id": 3725815,
    "start": 2202,
    "end": 2206,
    "title": "Self-Attention with Relative Position Representations",
    "abstract": "Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al.(2017)achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graphlabeled inputs.",
    "prev": "Transformer-based Large Language Models [40] (LLMs) have become the near-ubiquitous choice for many natural language processing (NLP) tasks where long-range abilities such as in-context learning (ICL) has been crucial.In performing the NLP tasks, the maximal length of the sequences (the context window) determined by its training processes has been one of the major limits of a pretrained LLM.Being able to dynamically extend the context window via a small amount of fine-tuning (or without fine-tuning) has become more and more desirable.To this end, the position encodings of transformers are the center of the discussions.",
    "curr": "The original Transformer architecture used an absolute sinusoidal position encoding, which was later improved to a learnable absolute position encoding [15].Since then, relative positional encoding schemes [32] have further increased the performance of Transformers.Currently, the most popular relative positional encodings are T5 Relative Bias [30], RoPE [34], XPos [35], and ALiBi [27].",
    "next": "One reoccurring limitation with positional encodings is the inability to generalize past the context window seen during training.While some methods such as ALiBi are able to do limited generalization, none are able to generalize to sequences significantly longer than their pre-trained length [22].Some works have been done to overcome such limitation."
  },
  {
    "index": 1901,
    "source_corpus_id": 248392450,
    "ref_id": "b12",
    "citation_corpus_id": 225039882,
    "start": 9583,
    "end": 9609,
    "title": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. 1 arXiv:2010.11929v2 [cs.CV] 3 Jun 2021Published as a conference paper at ICLR 2021 inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.However, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks.RELATED WORKTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global selfattention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.",
    "prev": "Action Recognition with Transformer.",
    "curr": "Following the vision transformer (ViT) (Dosovitskiy et al., 2021), which demonstrates competitive performance against CNN models on image classification, many recent works attempt to extend the vision transformer for action recognition (Neimark et al., 2021;Bertasius et al., 2021b;Arnab et al., 2021;.",
    "next": "VTN (Neimark et al., 2021), VidTr , TimeSformer (Bertasius et al., 2021b) and ViViT (Arnab et al., 2021) share the same concept that inserts a temporal modeling module into the existing ViT to enhance the features from the temporal direction."
  },
  {
    "index": 1902,
    "source_corpus_id": 252993084,
    "ref_id": "b43",
    "citation_corpus_id": 62841605,
    "start": 6603,
    "end": 6607,
    "title": "SPREADING VECTORS FOR SIMILARITY SEARCH",
    "abstract": "Discretizing multi-dimensional data distributions is a fundamental step of modern indexing methods. State-of-the-art techniques learn parameters of quantizers on training data for optimal performance, thus adapting quantizers to the data. In this work, we propose to reverse this paradigm and adapt the data to the quantizer: we train a neural net which last layer forms a fixed parameter-free quantizer, such as pre-defined points of a hyper-sphere. As a proxy objective, we design and train a neural network that favors uniformity in the spherical latent space, while preserving the neighborhood structure after the mapping. We propose a new regularizer derived from the Kozachenko-Leonenko differential entropy estimator to enforce uniformity and combine it with a locality-aware triplet loss. Experiments show that our end-to-end approach outperforms most learned quantization methods, and is competitive with the state of the art on widely adopted benchmarks. Furthermore, we show that training without the quantization step results in almost no difference in accuracy, but yields a generic catalyzer that can be applied with any subsequent quantizer. The code is available online 1 .",
    "prev": "In particular, SSCD [43] is a training method specialized for ICD.",
    "curr": "It employs the contrastive self-supervised method SimCLR [9] and entropy regularization [44] to improve the distribution of the representations.",
    "next": "Indexing\nGiven a dataset X = {x i } n i=1 ⊂ R d of d-dimensional\nvector representations extracted from n images and a query vector x q , we consider the indexing task that addresses the problem:\nx * := argmin x∈X x − x q ."
  },
  {
    "index": 1903,
    "source_corpus_id": 220968978,
    "ref_id": "b7",
    "citation_corpus_id": 210942708,
    "start": 5804,
    "end": 5807,
    "title": "MEMO: A DEEP NETWORK FOR FLEXIBLE COMBINA- TION OF EPISODIC MEMORIES",
    "abstract": "Recent research developing neural network architectures with external memory have often used the benchmark bAbI question and answering dataset which provides a challenging number of tasks requiring reasoning. Here we employed a classic associative inference task from the memory-based reasoning neuroscience literature in order to more carefully probe the reasoning capacity of existing memoryaugmented architectures. This task is thought to capture the essence of reasoningthe appreciation of distant relationships among elements distributed across multiple facts or memories. Surprisingly, we found that current architectures struggle to reason over long distance associations. Similar results were obtained on a more complex task involving finding the shortest path between nodes in a path. We therefore developed MEMO, an architecture endowed with the capacity to reason over longer distances. This was accomplished with the addition of two novel components. First, it introduces a separation between memories/facts stored in external memory and the items that comprise these facts in external memory. Second, it makes use of an adaptive retrieval mechanism, allowing a variable number of 'memory hops' before the answer is produced. MEMO is capable of solving our novel reasoning tasks, as well as match state of the art results in bAbI. * . Big-loop recurrence within the hippocampal system supports integration of information across episodes. Neuron, 99(6):1342-1354, 2018. . Learning deep generative models of graphs. arXiv preprint arXiv:1803.03324, 2018. Christos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks through l_0 regularization. arXiv preprint arXiv:1712.01312, 2017. . A simple neural network module for relational reasoning. In Advances in neural information processing systems, pp. 4967-4976, 2017. graph neural network model. Norman. Complementary learning systems within the hippocampus: a neural network modelling approach to reconciling episodic memory with statistical learning.",
    "prev": "The transformer has had a great impact on the natural language processing (NLP) community as new records in NLP benchmarks have been achieved [64,65].",
    "curr": "MEMO uses the transformer attention mechanism for reasoning over longer distances [8].",
    "next": "The current state-of-the-art for language processing is a transformer architecture called \"Bidirectional Encoder Representations from Transformers\" (BERT) [24,25]."
  },
  {
    "index": 1904,
    "source_corpus_id": 49671490,
    "ref_id": "b5",
    "citation_corpus_id": 14089312,
    "start": 2314,
    "end": 2316,
    "title": "Published as a conference paper at ICLR 2017 PRUNING FILTERS FOR EFFICIENT CONVNETS",
    "abstract": "The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy. However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks.",
    "prev": "This poses an issue for deploying these models in applications that require real-time inferencing and lowmemory footprint, such as self-driving vehicles, human-machine interaction on mobile devices, and robotics.",
    "curr": "Motivated by these applications, many methods have been proposed for model compression and acceleration, including techniques such as pruning [5,6,7], quantization [8,9], and low-rank factorization [10,11,12].",
    "next": "Most of these methods have been applied to single-scale inputs, without considering multi-resolution processing."
  },
  {
    "index": 1908,
    "source_corpus_id": 202538896,
    "ref_id": "b16",
    "citation_corpus_id": 14717992,
    "start": 27755,
    "end": 27779,
    "title": "REINFORCEMENT LEARNING WITH UNSUPERVISED AUXILIARY TASKS",
    "abstract": "Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-theart on Atari, averaging 880% expert human performance, and a challenging suite of first-person, three-dimensional Labyrinth tasks leading to a mean speedup in learning of 10× and averaging 87% expert human performance on Labyrinth.Natural and artificial agents live in a stream of sensorimotor data. At each time step t, the agent receives observations o t and executes actions a t . These actions influence the future course of the sensorimotor stream. In this paper we develop agents that learn to predict and control this stream, by solving a host of reinforcement learning problems, each focusing on a distinct feature of the sensorimotor stream. Our hypothesis is that an agent that can flexibly control its future experiences will also be able to achieve any goal with which it is presented, such as maximising its future rewards.The classic reinforcement learning paradigm focuses on the maximisation of extrinsic reward. However, in many interesting domains, extrinsic rewards are only rarely observed. This raises questions of what and how to learn in their absence. Even if extrinsic rewards are frequent, the sensorimotor stream contains an abundance of other possible learning targets. Traditionally, unsupervised learning attempts to reconstruct these targets, such as the pixels in the current or subsequent frame. It is typically used to accelerate the acquisition of a useful representation. In contrast, our learning objective is to predict and control features of the sensorimotor stream, by treating them as pseudorewards for reinforcement learning. Intuitively, this set of tasks is more closely matched with the agent's long-term goals, potentially leading to more useful representations.Consider a baby that learns to maximise the cumulative amount of red that it observes. To correctly predict the optimal value, the baby must understand how to increase \"redness\" by various means, including manipulation (bringing a red object closer to the eyes); locomotion (moving in front of a red object); and communication (crying until the parents bring a red object). These behaviours are likely to recur for many other goals that the baby may subsequently encounter. No understanding of these behaviours is required to simply reconstruct the redness of current or subsequent images.Our architecture uses reinforcement learning to approximate both the optimal policy and optimal value function for many different pseudo-rewards. It also makes other auxiliary predictions that serve to focus the agent on important aspects of the task. These include the long-term goal of predicting cumulative extrinsic reward as well as short-term predictions of extrinsic reward. To learn more efficiently, our agents use an experience replay mechanism to provide additional updates * Joint first authors. Ordered alphabetically by first name.",
    "prev": "For example, we tried adding randomized prior functions  to R2D2, but this approach was still unable to obtain reward on any of the Hard-Eight tasks.",
    "curr": "We also trained an IMPALA agent with pixel control (Jaderberg et al., 2016) as auxiliary reward to help with exploration, but this approach also failed to learn on any of the tasks we attempted.",
    "next": "We omit these results from Figure 5, only keeping the most relevant baselines."
  },
  {
    "index": 1909,
    "source_corpus_id": 258170309,
    "ref_id": "b3",
    "citation_corpus_id": 235436185,
    "start": 1740,
    "end": 1742,
    "title": "BEIT: BERT Pre-Training of Image Transformers",
    "abstract": "We introduce a self-supervised vision representation model BEIT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT [DCLT19] developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e., image patches (such as 16×16 pixels), and visual tokens (i.e., discrete tokens). We first \"tokenize\" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEIT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods.",
    "prev": "It starts from a pretext task trained on large unlabeled data, where the learned representation is fine-tuned on downstream tasks.",
    "curr": "This approach has shown great success in 2D images [8,15,18,4,17,61] and natural language processing (NLP) [13,5] .",
    "next": "Recently, people started looking into self-supervised pretraining on point cloud data due to its importance in 3D analysis and robotics applications."
  },
  {
    "index": 1911,
    "source_corpus_id": 27174168,
    "ref_id": "b1",
    "citation_corpus_id": 18828233,
    "start": 2585,
    "end": 2588,
    "title": "TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.",
    "prev": "[10], who noted that one of the leading causes of training instability was saturation of the discriminator.",
    "curr": "Arjovsky and Bottou [2] formalized this idea by showing that if the two distributions have supports that are disjoint or concentrated on low-dimensional manifolds that do not perfectly align, then there exists an optimal discriminator with perfect classification accuracy almost everywhere, and for which usual divergences (KL, Jensen-Shannon) will max-out.",
    "next": "In follow-up work [3], the authors propose an alternative training scheme based on an adversary that estimates the Wasserstein distance, instead of the Jensen-Shannon divergence, between real and generated distributions."
  },
  {
    "index": 1912,
    "source_corpus_id": 58981508,
    "ref_id": "b13",
    "citation_corpus_id": 3568073,
    "start": 1587,
    "end": 1608,
    "title": "PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION",
    "abstract": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at 1024 2 . We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CELEBA dataset.",
    "prev": "INTRODUCTION\n\nGraphic design is an important visual communication tool in our modern world, encompassing everything from book covers to magazine layouts to web design.",
    "curr": "Whereas methods for generating realistic natural-looking images have made significant progress lately, particularly with Generative Adversarial Networks (GANs) (Karras et al., 2018), methods for creating designs are far more primitive.",
    "next": "This is, in part, due to the difficulty of finding data representations suitable for learning."
  },
  {
    "index": 1913,
    "source_corpus_id": 203591628,
    "ref_id": "b1",
    "citation_corpus_id": 186206508,
    "start": 2406,
    "end": 2432,
    "title": "Monotonic Infinite Lookback Attention for Simultaneous Machine Translation",
    "abstract": "Simultaneous machine translation begins to translate each source sentence before the source speaker is finished speaking, with applications to live and streaming scenarios. Simultaneous systems must carefully schedule their reading of the source sentence to balance quality against latency. We present the first simultaneous translation system to learn an adaptive schedule jointly with a neural machine translation (NMT) model that attends over all source tokens read thus far. We do so by introducing Monotonic Infinite Lookback (MILk) attention, which maintains both a hard, monotonic attention head to schedule the reading of the source sentence, and a soft attention head that extends from the monotonic head back to the beginning of the source. We show that MILk's adaptive schedule allows it to arrive at latency-quality trade-offs that are favorable to those of a recently proposed wait-k strategy for many latency values.",
    "prev": "Monotonic attention mechanisms fall into learned policy category.",
    "curr": "Recent work exploring monotonic attention variants for simultaneous translation include: hard monotonic attention (Raffel et al., 2017), monotonic chunkwise attention (MoChA) (Chiu & Raffel, 2018) and monotonic infinite lookback attention (MILk) (Arivazhagan et al., 2019).",
    "next": "MILk in particular has shown better quality / latency trade-offs than fixed policy approaches, such as wait-k (Ma et al., 2019) or wait-if-* (Cho & Esipova, 2016) policies."
  },
  {
    "index": 1915,
    "source_corpus_id": 252815807,
    "ref_id": "b43",
    "citation_corpus_id": 65455367,
    "start": 20021,
    "end": 20041,
    "title": "ON THE CONVERGENCE OF ADAM AND BEYOND",
    "abstract": "Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSPROP, ADAM, ADADELTA, NADAM are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where ADAM does not converge to the optimal solution, and describe the precise problems with the previous analysis of ADAM algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with \"long-term memory\" of past gradients, and propose new variants of the ADAM algorithm which not only fix the convergence issues but often also lead to improved empirical performance.",
    "prev": "4.5.",
    "curr": "During training, we use the Adam optimizer's AMSGrad variant (Reddi et al., 2018) and train our models for 200 epochs.",
    "next": "We use the AMSGrad variant because we observe a slow convergence of the standard Adam optimizer in experiments."
  },
  {
    "index": 1918,
    "source_corpus_id": 238856856,
    "ref_id": "b40",
    "citation_corpus_id": 2703040,
    "start": 7673,
    "end": 7694,
    "title": "COVARIANT COMPOSITIONAL NETWORKS FOR LEARNING GRAPHS",
    "abstract": "Most existing neural networks for learning graphs address permutation invariance by conceiving of the network as a message passing scheme, where each node sums the feature vectors coming from its neighbors. We argue that this imposes a limitation on their representation power, and instead propose a new general architecture for representing objects consisting of a hierarchy of parts, which we call covariant compositional networks (CCNs). Here, covariance means that the activation of each neuron must transform in a specific way under permutations, similarly to steerability in CNNs. We achieve covariance by making each activation transform according to a tensor representation of the permutation group, and derive the corresponding tensor aggregation rules that each neuron must implement. Experiments show that CCNs can outperform competing methods on standard graph learning benchmarks.While MPNNs have been very successful in applications and are an active field of research, they differ from classical CNNs in a fundamental way: the internal feature representations in CNNs are equivariant to such transformations of the inputs as translation and rotations (Cohen & Welling, 2016a;b), the internal representations in MPNNs are fully invariant. This is a direct result of the fact that MPNNs deal with the permutation invariance issue in graphs simply by summing the messages coming from each neighbor. In this paper we argue that this is a serious limitation that restricts the representation power of MPNNs.MPNNs are ultimately compositional (part-based) models, that build up the representation of the graph from the representations of a hierarchy of subgraphs. To address the covariance issue, we study the covariance behavior of such networks in general, introducing a new general class of neural network architectures, which we call compositional networks (comp-nets). One advantage of this generalization is that instead of focusing attention on the mechanics of how information propagates from node to node, it emphasizes the connection to convolutional networks, in particular, it shows that what is missing from MPNNs is essentially the analog of steerability.",
    "prev": "Cohen & Welling (2016) observed that a viable approach to generalize CNNs to other data types could involve considering equivariance to more general transformation groups.",
    "curr": "This idea has been used to construct networks equivariant to a wide variety of transformations such as planar rotations (Worrall et al., 2017;Weiler et al., 2018b;Bekkers et al., 2018;Veeling et al., 2018;Smets et al., 2020), 3D rotations Esteves et al., 2018;Worrall & Brostow, 2018;Weiler et al., 2018a;Kondor et al., 2018a;Perraudin et al., 2019), permutations (Zaheer et al., 2017;Hartford et al., 2018;Kondor et al., 2018b;Maron et al., 2019a;, general Euclidean isometries (Weiler et al., 2018a;Weiler & Cesa, 2019;Finzi et al., 2020), scaling (Marcos et al., 2018;Worrall & Welling, 2019;Sosnovik et al., 2020) and more exotic symmetries (Bogatskiy et al., 2020;Shutty & Wierzynski, 2020;Finzi et al., 2021) etc.",
    "next": "A quite general theory of equivariant/invariant networks has also emerged."
  },
  {
    "index": 1919,
    "source_corpus_id": 58981389,
    "ref_id": "b14",
    "citation_corpus_id": 252796,
    "start": 18858,
    "end": 18862,
    "title": "Building a Large Annotated Corpus of English: The Penn Treebank",
    "abstract": "There is a growing consensus that significant, rapid progress can be made in both text understanding and spoken language understanding by investigating those phenomena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large corpora. Such corpora are beginning to serve as important research tools for investigators in natural language processing, speech recognition, and integrated spoken language systems, as well as in theoretical linguistics. Annotated corpora promise to be valuable for enterprises as diverse as the automatic construction of statistical models for the grammar of the written and the colloquial spoken language, the development of explicit formal theories of the differing grammars of writing and speech, the investigation of prosodic phenomena in speech, and the evaluation and comparison of the adequacy of parsing models.In this paper, we review our experience with constructing one such large annotated corpus--the Penn Treebank, a corpus 1 consisting of over 4.5 million words of American English. During the first three-year phase of the Penn Treebank Project (1989)(1990)(1991)(1992), this corpus has been annotated for part-of-speech (POS) information. In addition, over half of it has been annotated for skeletal syntactic structure. These materials are available to members of the Linguistic Data Consortium; for details, see Section 5.1.The paper is organized as follows. Section 2 discusses the POS tagging task. After outlining the considerations that informed the design of our POS tagset and presenting the tagset itself, we describe our two-stage tagging process, in which text is first assigned POS tags automatically and then corrected by human annotators. Section 3 briefly presents the results of a comparison between entirely manual and semi-automated tagging, with the latter being shown to be superior on three counts: speed, consistency, and accuracy. In Section 4, we turn to the bracketing task. Just as with the tagging task, we have partially automated the bracketing task: the output of",
    "prev": "In language modeling, given a sequence of words or characters, the model must predict the next word or character.",
    "curr": "For character-level language modeling, we train and evaluate models on Penn Treebank [15].",
    "next": "To increase the coverage of our experiments, we train and evaluate the word-level language models on the Wikitext-2 dataset, which is twice as large as Penn Treebank and features a larger vocabulary [17]."
  },
  {
    "index": 1920,
    "source_corpus_id": 235683534,
    "ref_id": "b22",
    "citation_corpus_id": 208117506,
    "start": 2801,
    "end": 2820,
    "title": "Transformers: State-of-the-Art Natural Language Processing",
    "abstract": "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered stateof-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/ huggingface/transformers.",
    "prev": "BERT (Devlin et al., 2019) is a particularly popular choice: it has been widely adopted in academia and industry, and aspects of its performance have been reported on in thousands of research papers (see, e.g., Rogers et al., 2020, for an overview).",
    "curr": "Because pre-training large language models is computationally expensive (Strubell et al., 2019), researchers often rely on the release of model checkpoints through libraries such as HuggingFace Transformers (Wolf et al., 2020), which enable them to use large-scale language models without repeating the pre-training work.",
    "next": "Consequently, most published results are based on a small number of publicly released model checkpoints."
  },
  {
    "index": 1921,
    "source_corpus_id": 238419305,
    "ref_id": "b13",
    "citation_corpus_id": 54101493,
    "start": 2944,
    "end": 2966,
    "title": "IMAGENET-TRAINED CNNS ARE BIASED TOWARDS TEXTURE; INCREASING SHAPE BIAS IMPROVES ACCURACY AND ROBUSTNESS",
    "abstract": "Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNettrained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on 'Stylized-ImageNet', a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation. de Beeck. Deep neural networks as a computational model for human shape sensitivity. DiCarlo. Performance-optimized hierarchical models predict neural responses in higher visual cortex.",
    "prev": "For example, a DNN trained for the task of animal recognition may recognize ducks while attending on water backgrounds, given the strong correlations between such background cues and the target label (Choe et al., 2020).",
    "curr": "These shortcut biases often result in a striking qualitative difference between human and machine recognition systems; for example, convolutional neural networks (CNNs) trained on ImageNet extensively rely on texture features, while humans would preferentially look at the global shape of objects (Geirhos et al., 2019).",
    "next": "In other cases, the shortcut bias arises in models that suppress certain streams of inputs: visual question answering (VQA) models often neglect the entire image cues, for one does not require images to answer questions like \"what color is the banana in the image?\""
  },
  {
    "index": 1922,
    "source_corpus_id": 253244237,
    "ref_id": "b9",
    "citation_corpus_id": 67855860,
    "start": 5780,
    "end": 5802,
    "title": "Attention is not Explanation",
    "abstract": "Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful \"explanations\" for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do. Code to reproduce all experiments is available at https://github.com/successar/ AttentionExplanation.",
    "prev": "• Finally, we identified heads reliably writing in the opposite direction of the correct answer.",
    "curr": "Explanations for model behavior can easily be misleading or non-rigorous (Jain & Wallace, 2019;Bolukbasi et al., 2021).",
    "next": "To remedy this problem, we formulate three criteria to help validate our circuit explanations."
  },
  {
    "index": 1924,
    "source_corpus_id": 252111177,
    "ref_id": "b72",
    "citation_corpus_id": 227209335,
    "start": 4956,
    "end": 4959,
    "title": "SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS",
    "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reversetime SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in diffusion probabilistic modeling and scorebased generative modeling, and allows for new sampling procedures. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, which enables exact likelihood computation, and improved sampling efficiency. In addition, our framework enables conditional generation with an unconditional model, as we demonstrate with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high fidelity generation of 1024ˆ1024 images for the first time from a score-based generative model.",
    "prev": "On the other hand, MLE tends to be intractable for complex models, and hence requires approximate variational or Monte Carlo inference techniques such as those used in variational auto-encoders (VAE), or special model structures such as normalizing flow and auto-regressive models, to yield tractable likelihood, causing difficult trade-offs between expressive power and computational cost.",
    "curr": "Recently, advances have been made by representing the transport plan implicitly as a continuous time process, such as flow models with neural ordinary differential equations (ODEs) [e.g., 6,56] and diffusion models by stochastic differential equations (SDEs) [e.g., 73,23,80,11,82]; in these models, a neural network is trained to represent the drift force of the processes and a numerical ODE/SDE solver is used to simulate the process during inference.",
    "next": "The key idea is that, by leveraging the mathematical structures of ODEs/SDEs, the continuous-time models can be trained efficiently without resorting to minimax or traditional approximate inference techniques."
  },
  {
    "index": 1925,
    "source_corpus_id": 12308095,
    "ref_id": "b30",
    "citation_corpus_id": 604334,
    "start": 1645,
    "end": 1666,
    "title": "Intriguing properties of neural networks",
    "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
    "prev": "INTRODUCTION\n\nAs the use of machine intelligence increases in security-sensitive applications (Bojarski et al., 2016;Amodei et al., 2015), robustness has become a critical feature to guarantee the reliability of deployed machine-learning systems.",
    "curr": "Unfortunately, recent research has demonstrated that existing models are not robust to small, adversarially designed perturbations of the input (Biggio et al., 2013;Szegedy et al., 2014;Goodfellow et al., 2015;Kurakin et al., 2016a;Cisse et al., 2017a).",
    "next": "Adversarially perturbed examples have been deployed to attack image classification services (Liu et al., 2016), speech recognition systems (Cisse et al., 2017a), and robot vision (Melis et al., 2017)."
  },
  {
    "index": 1926,
    "source_corpus_id": 248811555,
    "ref_id": "b7",
    "citation_corpus_id": 13995862,
    "start": 2326,
    "end": 2345,
    "title": "NICE: Non-linear Independent Components Estimation",
    "abstract": "We propose a deep learning framework for modeling complex high-dimensional densities via Nonlinear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the determinant of the Jacobian and inverse Jacobian is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable, and unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting. * Yoshua Bengio is a CIFAR Senior Fellow.",
    "prev": "We demonstrate that the trained CoopFlow is capable of synthesizing realistic images, reconstructing images, and interpolating between images.",
    "curr": "INTRODUCTION AND MOTIVATION\n\nNormalizing flows (Dinh et al., 2015;Kingma & Dhariwal, 2018) are a family of generative models that construct a complex distribution by transforming a simple probability density, such as Gaussian distribution, through a sequence of invertible and differentiable mappings.",
    "next": "Due to the tractability of the exact log-likelihood and the efficiency of the inference and synthesis, normalizing flows have gained popularity in density estimation (Kingma & Dhariwal, 2018;Ho et al., 2019;Yang et al., 2019;Prenger et al., 2019;Kumar et al., 2020) and variational inference (Rezende & Mohamed, 2015;."
  },
  {
    "index": 1927,
    "source_corpus_id": 56895471,
    "ref_id": "b6",
    "citation_corpus_id": 6628106,
    "start": 1439,
    "end": 1458,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "INTRODUCTION\n\nThis paper investigates the use of preconditioner for accelerating gradient descent, especially in large scale machine learning problems.",
    "curr": "Stochastic gradient descent (SGD) and its variations, e.g., momentum (Rumelhart et al., 1986;Nesterov, 1983), RMSProp and Adagrad (John et al., 2011), Adam (Kingma & Ba, 2015), etc., are popular choices due to their simplicity and wide applicability.",
    "next": "These simple methods do not use well normalized step size, could converge slow, and might involve more controlling parameters requiring fine tweaking."
  },
  {
    "index": 1930,
    "source_corpus_id": 257353502,
    "ref_id": "b97",
    "citation_corpus_id": 238531645,
    "start": 9113,
    "end": 9130,
    "title": "Published as a conference paper at ICLR 2022 TAMING SPARSELY ACTIVATED TRANSFORMER WITH STOCHASTIC EXPERTS",
    "abstract": "Sparsely activated models (SAMs), such as Mixture-of-Experts (MoE), can easily scale to have outrageously large amounts of parameters without significant increase in computational cost. However, SAMs are reported to be parameter inefficient such that larger models do not always lead to better performance. While most on-going research focuses on improving SAMs models by exploring methods of routing inputs to experts, our analysis reveals that such research might not lead to the solution we expect, i.e., the commonly-used routing methods based on gating mechanisms do not work better than randomly routing inputs to experts. In this paper, we propose a new expert-based model, THOR (Transformer witH StOchastic ExpeRts). Unlike classic expert-based models, such as the Switch Transformer(Fedus et al., 2021), experts in THOR are randomly activated for each input during training and inference. THOR models are trained using a consistency regularized loss, where experts learn not only from training data but also from other experts as teachers, such that all the experts make consistent predictions. We validate the effectiveness of THOR on machine translation tasks. Results show that THOR models are more parameter efficient in that they significantly outperform the Transformer and MoE models across various settings. For example, in multilingual translation, THOR outperforms the Switch Transformer by 2 BLEU scores, and obtains the same BLEU score as that of a state-of-the-art MoE model(Kim et al., 2021)that is 18 times larger. Our code is publicly available at: https://github.com/microsoft/ Stochastic-Mixture-of-Experts. * Work was done during an internship at Microsoft.",
    "prev": "Fortunately, recent investigations Lepikhin et al., 2020;Fedus et al., 2021) have proved the effectiveness of MoEs with sparsely activated experts (i.e., SMoE) at both training and inference stages, which greatly trim down the cost and scale language models to enormous sizes like trillions of parameters (Fedus et al., 2021).",
    "curr": "This efficient fashion of SMoEs gains increasing popularity in various NLP Lepikhin et al., 2020;Zuo et al., 2022;Jiang et al., 2021) and vision (Riquelme et al., 2021;Eigen et al., 2013;Ahmed et al., 2016;Gross et al., 2017;Yang et al., 2019a;Abbas & Andreopoulos, 2020;Pavlitskaya et al., 2020) tasks.",
    "next": "However, its sparse-gated manner incurs several downsides, including: (1) Unstable training."
  },
  {
    "index": 1931,
    "source_corpus_id": 53215110,
    "ref_id": "b3",
    "citation_corpus_id": 7100502,
    "start": 1719,
    "end": 1742,
    "title": "What do Neural Machine Translation Models Learn about Morphology?",
    "abstract": "Neural machine translation (MT) models obtain state-of-the-art performance while maintaining a simple, end-to-end architecture. However, little is known about what these models learn about source and target languages during the training process.In this work, we analyze the representations learned by neural MT models at various levels of granularity and empirically evaluate the quality of the representations for learning morphology through extrinsic part-of-speech and morphological tagging tasks. We conduct a thorough investigation along several parameters: word-based vs. character-based representations, depth of the encoding layer, the identity of the target language, and encoder vs. decoder representations. Our data-driven, quantitative evaluation sheds light on important aspects in the neural MT system and its ability to capture word structure. 1",
    "prev": "* Equal contribution 1\n\nINTRODUCTION\n\nNeural machine translation (NMT) systems achieve state-of-the-art results by learning from large amounts of example translations, typically without additional linguistic information.",
    "curr": "Recent studies have shown that representations learned by NMT models contain a non-trivial amount of linguistic information on multiple levels: morphological (Belinkov et al., 2017), syntactic (Shi et al., 2016b), and semantic (Hill et al., 2017).",
    "next": "These studies use trained NMT models to generate feature representations for words, and use these representations to predict certain linguistic properties."
  },
  {
    "index": 1932,
    "source_corpus_id": 211082491,
    "ref_id": "b10",
    "citation_corpus_id": 30535508,
    "start": 1673,
    "end": 1694,
    "title": "LEARNING SPARSE NEURAL NETWORKS THROUGH L 0 REGULARIZATION",
    "abstract": "We propose a practical method for L 0 norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of L 0 regularization. However, since the L 0 norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected L 0 norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the hard concrete distribution for the gates, which is obtained by \"stretching\" a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.",
    "prev": "Since the early works of LeCun et al.",
    "curr": "(1989); Hassibi & Stork (1993) which provided more theoretically grounded alternatives of magnitude-based pruning (MP) based on second derivatives of the loss function, a wide range of methods including Bayesian / informationtheoretic approaches (Neal, 1996;Louizos et al., 2017;Molchanov et al., 2017;Dai et al., 2018), p -regularization (Wen et al., 2016;Liu et al., 2017;Louizos et al., 2018), sharing redundant channels (Zhang et al., 2018;Ding et al., 2019), and reinforcement learning approaches (Lin et al., 2017;Bellec et al., 2018;He et al., 2018) have been proposed as more sophisticated alternatives.",
    "next": "On the other hand, the capabilities of MP heuristics are gaining attention once more."
  },
  {
    "index": 1934,
    "source_corpus_id": 13570924,
    "ref_id": "b6",
    "citation_corpus_id": 12730344,
    "start": 2291,
    "end": 2316,
    "title": "An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks",
    "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models \"forget\" how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting. We find that it is always best to train using the dropout algorithmthe dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoff curve between these two extremes. We find that different tasks and relationships between tasks result in very different rankings of activation function performance. This suggests that the choice of activation function should always be cross-validated.",
    "prev": "However, it is challenging to strike a balance between adapting to recent data and retaining knowledge from old data.",
    "curr": "Too much plasticity leads to the infamous catastrophic forgetting problem (McCloskey and Cohen, 1989;Ratcliff, 1990;Goodfellow et al., 2014a) and too much stability leads to an inability to adapt.",
    "next": "Recently there has been a resurgence of interest in this area."
  },
  {
    "index": 1935,
    "source_corpus_id": 60440651,
    "ref_id": "b14",
    "citation_corpus_id": 3366315,
    "start": 2403,
    "end": 2423,
    "title": "Published as a conference paper at ICLR 2018 SPECTRAL NORMALIZATION FOR GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. The code with Chainer (Tokui et al., 2015), generated images and pretrained models are available at https://github.com/pfnet-research/sngan_ projection.Published as a conference paper at ICLR 2018• Lipschitz constant is the only hyper-parameter to be tuned, and the algorithm does not require intensive tuning of the only hyper-parameter for satisfactory performance. • Implementation is simple and the additional computational cost is small.",
    "prev": "The original GAN is, however, highly unstable and often suffers from mode collapse.",
    "curr": "Much of recent researches has focused on improving the stability of GANs (Radford et al., 2015;Heusel et al., 2017;Miyato et al., 2018;Karras et al., 2018).",
    "next": "On the theoretical aspect, Nagarajan & Kolter (2017) proved that gradient based training of the original GAN is locally stable."
  },
  {
    "index": 1940,
    "source_corpus_id": 224705241,
    "ref_id": "b26",
    "citation_corpus_id": 13123084,
    "start": 5753,
    "end": 5773,
    "title": "Published as a conference paper at ICLR 2017 TEMPORAL ENSEMBLING FOR SEMI-SUPERVISED LEARNING",
    "abstract": "In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce self-ensembling, where we form a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the (non-augmented) classification error rate from 18.44% to 7.05% in SVHN with 500 labels and from 18.63% to 16.55% in CIFAR-10 with 4000 labels, and further to 5.12% and 12.16% by enabling the standard augmentations. We additionally obtain a clear improvement in CIFAR-100 classification accuracy by using random images from the Tiny Images dataset as unlabeled extra inputs during training. Finally, we demonstrate good tolerance to incorrect labels.",
    "prev": "Consistency regularization and entropy minimization are two common strategies for SSL.",
    "curr": "The intuition behind consistencybased approaches (Laine & Aila, 2016;Sajjadi et al., 2016;Miyato et al., 2018;Tarvainen & Valpola, 2017) is that, the model output should remain unchanged when the input is perturbed.",
    "next": "On the other hand, the entropy minimization strategy (Grandvalet & Bengio, 2005) argues that the unlabeled data can be used to ensured classes are well-separated, which can be achieved by encouraging the model to output low-entropy predictions."
  },
  {
    "index": 1941,
    "source_corpus_id": 52182515,
    "ref_id": "b10",
    "citation_corpus_id": 6706414,
    "start": 2405,
    "end": 2430,
    "title": "Published as a conference paper at ICLR 2015 EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES",
    "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples-inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
    "prev": "A prominent example is the existence of adversarial examples (Szegedy et al., 2014): imperceptibly modified inputs that cause state-of-the-art models to misclassify with high confidence.",
    "curr": "There has been a long line of work on both generating adversarial examples, called attacks (Carlini and Wagner, 2017a;b;Athalye et al., 2018a;b;Evtimov et al., 2017), and training models robust to adversarial examples, called defenses (Goodfellow et al., 2015;Papernot et al., 2016;Madry et al., 2018;Kannan et al., 2018).",
    "next": "However, recent research has shown that most defenses are ineffective (Carlini and Wagner, 2017a;Athalye et al., 2018a;."
  },
  {
    "index": 1942,
    "source_corpus_id": 259165244,
    "ref_id": "b2",
    "citation_corpus_id": 5034059,
    "start": 3574,
    "end": 3577,
    "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    "abstract": "For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusively tailored to a specific task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating and analyzing the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all our tasks yields better results than training a separate model for each task. However, the low absolute performance of our best model indicates the need for improved general NLU systems. son. 2013. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint 1312.3005.",
    "prev": "Introduction\n\nRecent remarkable breakthroughs achieved by large language models (LLMs) like GPT-4 [1] have elicited widespread astonishment.",
    "curr": "Considering the extensive and profound natural language understanding and generation abilities exhibited by LLMs [2], the conventional benchmarks [3,4] focusing on relatively narrow and superficial abilities are no longer as helpful for testing them.",
    "next": "It has become necessary to construct better benchmarks for effectively comparing LLMs and providing valuable diagnostic results."
  },
  {
    "index": 1944,
    "source_corpus_id": 258887582,
    "ref_id": "b3",
    "citation_corpus_id": 17682909,
    "start": 2283,
    "end": 2286,
    "title": "Spectral Networks and Deep Locally Connected Networks on Graphs",
    "abstract": "Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for lowdimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.",
    "prev": "GNNs have shown remarkable performance in learning tasks where data are represented over a graph domain, due to their ability to combine the flexibility of neural networks with prior knowledge about data relationships, expressed in terms of the underlying graph topology.",
    "curr": "The literature on GNNs is extensive and encompasses various techniques, typically categorized into spectral [4] and non-spectral [5] methods.",
    "next": "The basic idea behind GNNs is to learn node (and/or) edge attributes representations using local aggregation based on the graph topology, i.e."
  },
  {
    "index": 1945,
    "source_corpus_id": 219721312,
    "ref_id": "b4",
    "citation_corpus_id": 54101493,
    "start": 1727,
    "end": 1735,
    "title": "IMAGENET-TRAINED CNNS ARE BIASED TOWARDS TEXTURE; INCREASING SHAPE BIAS IMPROVES ACCURACY AND ROBUSTNESS",
    "abstract": "Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNettrained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on 'Stylized-ImageNet', a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation. de Beeck. Deep neural networks as a computational model for human shape sensitivity. DiCarlo. Performance-optimized hierarchical models predict neural responses in higher visual cortex.",
    "prev": "However, the actual model reliability and robustness depend on the specific set of correlations that is used, and on how those correlations are combined.",
    "curr": "Indeed, outside of the training distribution, model predictions can deviate wildly from human expectations either due to relying on correlations that humans do not perceive [JLT18; Ily+19; Jac+19], or due to overusing correlations, such as texture [Gei+19;Bak+18] and color [YS02], that humans do use (but to a lesser degree).",
    "next": "Characterizing the correlations that models depend on thus has important implications for understanding model behavior, in general."
  },
  {
    "index": 1946,
    "source_corpus_id": 2514328,
    "ref_id": "b2",
    "citation_corpus_id": 11336213,
    "start": 2046,
    "end": 2064,
    "title": "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches",
    "abstract": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder-Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.",
    "prev": "We show experimentally that not only do our models require fewer operations, they also lead to better performance overall on evaluation tasks.",
    "curr": "INTRODUCTION\n\nThe class of Recurrent Neural Network models (RNNs) is particularly well suited to dealing with sequential data, and has been successfully applied to a diverse array of tasks, such as language modeling and speech recognition (Mikolov, 2012), machine translation (Mikolov, 2012;Cho et al., 2014a), or acoustic modeling (Robinson et al., 1993;Graves & Jaitly, 2014) among others.",
    "next": "Two factors have been instrumental in allowing this paradigm to be so widely adopted and give rise to the aforementioned successes."
  },
  {
    "index": 1948,
    "source_corpus_id": 247519082,
    "ref_id": "b9",
    "citation_corpus_id": 231592453,
    "start": 2812,
    "end": 2831,
    "title": "SEED: SELF-SUPERVISED DISTILLATION FOR VISUAL REPRESENTATION",
    "abstract": "This paper is concerned with self-supervised learning for small models. The problem is motivated by our empirical studies that while the widely used contrastive self-supervised learning method has shown great progress on large model training, it does not work well for small models. To address this problem, we propose a new learning paradigm, named SElf-SupErvised Distillation (SEED), where we leverage a larger network (as Teacher) to transfer its representational knowledge into a smaller architecture (as Student) in a self-supervised fashion. Instead of directly learning from unlabeled data, we train a student encoder to mimic the similarity score distribution inferred by a teacher over a set of instances. We show that SEED dramatically boosts the performance of small networks on downstream tasks. Compared with self-supervised baselines, SEED improves the top-1 accuracy from 42.2% to 67.6% on EfficientNet-B0 and from 36.3% to 68.2% on MobileNet-V3-Large on the ImageNet-1k dataset. et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint arXiv-celeb-1m: A dataset and benchmark for large-scale face recognition. In European conference on computer vision, pp. 87-102. Springer, 2016.Michael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In . Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.Zehao Huang and Naiyan Wang. Like what you like: Knowledge distill via neuron selectivity transfer. arXiv preprint arXiv:",
    "prev": "Inspired by supervised learning that knowledge from large models can effectively promote the learning ability of small models with distillation, exploring knowledge distillation on unsupervised small models becomes an important topic.",
    "curr": "Compress (Fang et al., 2020) and SEED (Fang et al., 2020) are two typical methods for unsupervised distillation, which propose to transfer knowledge from the teacher in terms of similarity distributions among different instances.",
    "next": "However, as the similarity distribution is computed by randomly sampling instances from a dynamically maintained queue, this kind of knowledge is mostly constructed based on instances with low relation, which fails to effectively model similarity of those highly related samples."
  },
  {
    "index": 1949,
    "source_corpus_id": 261682404,
    "ref_id": "b3",
    "citation_corpus_id": 219708602,
    "start": 3032,
    "end": 3034,
    "title": "Published as a conference paper at ICLR 2021 GENERALIZED ENERGY BASED MODELS",
    "abstract": "We introduce the Generalized Energy Based Model (GEBM) for generative modelling. These models combine two trained components: a base distribution (generally an implicit model), which can learn the support of data with low intrinsic dimension in a high dimensional space; and an energy function, to refine the probability mass on the learned support. Both the energy function and base jointly constitute the final model, unlike GANs, which retain only the base distribution (the \"generator\"). GEBMs are trained by alternating between learning the energy and the base. We show that both training stages are well-defined: the energy is learned by maximising a generalized likelihood, and the resulting energy-based loss provides informative gradients for learning the base. Samples from the posterior on the latent space of the trained model can be obtained via MCMC, thus finding regions in this space that produce better quality samples. Empirically, the GEBM samples on image-generation tasks are of much better quality than those from the learned generator alone, indicating that all else being equal, the GEBM will outperform a GAN of the same complexity. When using normalizing flows as base measures, GEBMs succeed on density modelling tasks, returning comparable performance to direct maximum likelihood of the same networks.",
    "prev": "Under review.",
    "curr": "arXiv:2309.05153v2 [stat.ML] 12 Sep 2023\n\nIntroduction\n\nEnergy-based models (EBMs), as a class of probabilistic generative models, have exhibited their flexibility and practicality in a variety of application scenarios, such as realistic image synthesis [1,2,3,4,5,6,7,8], graph generation [9], compositional generation [10,11], video generation [12], 3D generation [13,14,15], simulation-based inference [16], stochastic optimization [17], out-ofdistribution detection [18,19], continue learning [20] , internal learning [21], learning set funtion [22], image style transfer [23], continuous inverse optimal control [24], and latent space modeling [25].",
    "next": "Despite the recent successes of the EBMs, their training and sampling have been challenging because of the intractability of the normalizing constant."
  },
  {
    "index": 1953,
    "source_corpus_id": 2468625,
    "ref_id": "b9",
    "citation_corpus_id": 2009318,
    "start": 2042,
    "end": 2045,
    "title": "NEURAL GPUS LEARN ALGORITHMS",
    "abstract": "Learning an algorithm from examples is a fundamental problem that has been widely studied. It has been addressed using neural networks too, in particular by Neural Turing Machines (NTMs). These are fully differentiable computers that use backpropagation to learn their own programming. Despite their appeal NTMs have a weakness that is caused by their sequential nature: they are not parallel and are are hard to train due to their large depth when unfolded. We present a neural network architecture to address this problem: the Neural GPU. It is based on a type of convolutional gated recurrent unit and, like the NTM, is computationally universal. Unlike the NTM, the Neural GPU is highly parallel which makes it easier to train and efficient to run. An essential property of algorithms is their ability to handle inputs of arbitrary size. We show that the Neural GPU can be trained on short instances of an algorithmic task and successfully generalize to long instances. We verified it on a number of tasks including long addition and long multiplication of numbers represented in binary. We train the Neural GPU on numbers with up-to 20 bits and observe no errors whatsoever while testing it, even on much longer numbers. To achieve these results we introduce a technique for training deep recurrent networks: parameter sharing relaxation. We also found a small amount of dropout and gradient noise to have a large positive effect on learning and generalization.",
    "prev": "Introduction\n\nA central goal of Artificial Intelligence are machines we can not just program but also teach.",
    "curr": "A recent and important step towards this goal are neural architectures that can learn to perform algorithms akin to traditional computers, using primitives such as memory access and stack manipulation [5,9,10,14,19].",
    "next": "These architectures can be trained through standard gradient descent methods, and enable machines to learn complex behaviors from input-output pairs or program traces."
  },
  {
    "index": 1954,
    "source_corpus_id": 237353222,
    "ref_id": "b9",
    "citation_corpus_id": 52967399,
    "start": 2597,
    "end": 2618,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "A comprehensive evaluation of standard NLP tasks demonstrates that the proposed approach achieves a better few-shot performance 1 .",
    "curr": "INTRODUCTION\n\nThe pre-train-fine-tune paradigm has become the de facto standard for natural language processing (NLP), and has achieved excellent results in several benchmarks (Devlin et al., 2019;Lewis et al., 2020;Dong et al., 2019;Bao et al., 2020a).",
    "next": "The success of these pioneers seems to suggest that large-scale pre-trained models are always nothing short of a panacea for boosting machine intelligence."
  },
  {
    "index": 1955,
    "source_corpus_id": 17984798,
    "ref_id": "b28",
    "citation_corpus_id": 18380217,
    "start": 2048,
    "end": 2074,
    "title": "Cross-lingual Transfer of Named Entity Recognizers without Parallel Corpora",
    "abstract": "We propose an approach to cross-lingual named entity recognition model transfer without the use of parallel corpora. In addition to global de-lexicalized features, we introduce multilingual gazetteers that are generated using graph propagation, and cross-lingual word representation mappings without the use of parallel data. We target the e-commerce domain, which is challenging due to its unstructured and noisy nature. The experiments have shown that our approaches beat the strong MT baseline, where the English model is transferred to two languages: Spanish and Chinese.",
    "prev": "An important challenge for sequence tagging is how to transfer knowledge from one task to another, which is often referred to as transfer learning (Pan & Yang, 2010).",
    "curr": "Transfer learning can be used in several settings, notably for low-resource languages (Zirikly & Hagiwara, 2015;Wang & Manning, 2014) and low-resource domains such as biomedical corpora (Kim et al., 2003) and Twitter corpora (Ritter et al., 2011)).",
    "next": "In these cases, transfer learning can improve performance by taking advantage of more plentiful labels from related tasks."
  },
  {
    "index": 1956,
    "source_corpus_id": 238408445,
    "ref_id": "b27",
    "citation_corpus_id": 202712906,
    "start": 5238,
    "end": 5257,
    "title": "Published as a conference paper at ICLR 2020 RAPID LEARNING OR FEATURE REUSE? TOWARDS UNDERSTANDING THE EFFECTIVENESS OF MAML",
    "abstract": "An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML's popularity, a fundamental open question remains -is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse, with the meta-initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of the underlying neural network. ANIL matches MAML's performance on benchmark few-shot image classification and RL and offers computational improvements over MAML. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly. * Equal contribution arXiv:1909.09157v2 [cs.LG] . Recasting gradientbased meta-learning as hierarchical bayes. arXiv preprint arXiv:1801.08930, 2018.James Harrison, Apoorva Sharma, and Marco Pavone. Meta-learning priors for efficient online bayesian regression. arXiv preprint arXiv:1807.08912, 2018. . Gradient-based meta-learning with learned layerwise metric and subspace. arXiv preprint arXiv:1801.05558, 2018.",
    "prev": "INTRODUCTION\n\nMeta-learning (Schmidhuber, 1987;Thrun & Pratt, 1998) aims to learn a learning process itself over a task distribution.",
    "curr": "Many gradient-based meta-learning approaches assume a set of parameters that do not participate in inner-optimization (Lee & Choi, 2018;Flennerhag et al., 2019;Raghu et al., 2019) which can be seen as hyperparameters.",
    "next": "Those hyperparameters are important in helping the inner-learner converge faster and generalize better."
  },
  {
    "index": 1957,
    "source_corpus_id": 253581330,
    "ref_id": "b15",
    "citation_corpus_id": 6628106,
    "start": 1710,
    "end": 1729,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "ng with AdamW performs substantially better than SGD on modern Vision Transformer and ConvNeXt models.We find that large gaps in performance between SGD and AdamW occur when the fine-tuning gradients in the first \"embedding\" layer are much larger than in the rest of the model.Our analysis suggests an easy fix that works consistently across datasets and models: freezing the embedding layer (less than 1% of the parameters) leads to SGD with or without momentum performing slightly better than AdamW while using less memory (e.g., on ViT-L, SGD uses ∼ 33% less GPU memory).Our insights result in state-ofthe-art accuracies on five popular distribution shift benchmarks: WILDS-FMoW, WILDS-Camelyon, BREEDS-Living-17, Waterbirds, and DomainNet.",
    "curr": "INTRODUCTION\n\nFine-tuning large pretrained models on downstream tasks has become a dominant approach in deep learning (Kornblith et al., 2019;Chen et al., 2020;Zhai et al., 2020).The two most commonly used optimizers in current practice are SGD and AdamW (Kingma & Ba, 2015;Loshchilov & Hutter, 2019) 1 .While most modern vision architectures (ViTs, ConvNeXts, and variants) increasingly use AdamW for pretraining, it is still common to use SGD for fine-tuning.Part of the appeal is that SGD is more memory and compute efficient: AdamW maintains 1.5× and 3× as many states per parameter as SGD with and without momentum, respectively (Ginsburg et al., 2019;Dettmers et al., 2022).At the same time, in terms of fine-tuning accuracies, prior work (Dosovitskiy et al., 2021;Steiner et al., 2021;Kumar et al., 2022) report similar performance between AdamW and SGD on ImageNet like domains that are closer to pretraining data.In contrast, we reach different conclusions when fine-tuning on datasets that are far from pretraining data or have substantial distribution shifts.",
    "next": "We examine 7 popular models, including vision transformers (Dosovitskiy et al., 2021;Caron et al., 2021;Radford et al., 2021), ConvNeXts (Liu et al., 2022), and ResNets (Kolesnikov et al., 2020;He et al"
  },
  {
    "index": 1960,
    "source_corpus_id": 108334676,
    "ref_id": "b5",
    "citation_corpus_id": 11383178,
    "start": 19421,
    "end": 19441,
    "title": "IMPORTANCE WEIGHTED AUTOENCODERS",
    "abstract": "The variational autoencoder (VAE; ) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.",
    "prev": "We reinforce that p(x i ) represents not the true model, but rather the proposed model for the data.",
    "curr": "Still, when performing variational inference, the most typical result is that we know how to evaluate exactly a lower bound to p(x i ) (this is true when we are using the Evidence Lower Bound or importance sampling (Burda et al., 2016)).",
    "next": "In that case, you obtain an upper bound to the gap sup z c(z) (see (6) for the definition of c(z))."
  },
  {
    "index": 1961,
    "source_corpus_id": 245131359,
    "ref_id": "b9",
    "citation_corpus_id": 3073252,
    "start": 25222,
    "end": 25242,
    "title": "Published as a conference paper at ICLR 2017 LEARNING TO GENERATE SAMPLES FROM NOISE THROUGH INFUSION TRAINING",
    "abstract": "In this work, we investigate a novel training procedure to learn a generative model as the transition operator of a Markov chain, such that, when applied repeatedly on an unstructured random noise sample, it will denoise it into a sample that matches the target distribution from the training set. The novel training procedure to learn this progressive denoising operation involves sampling from a slightly different chain than the model chain used for generation in the absence of a denoising target. In the training chain we infuse information from the training target example that we would like the chains to reach with a high probability. The thus learned transition operator is able to produce quality and varied samples in a small number of steps. Experiments show competitive results compared to the samples generated with a basic Generative Adversarial Net.",
    "prev": "Further Related Work.",
    "curr": "Generative modeling by learning stochastic processes has a long history (Movellan, 2008;Lyu, 2009;Sohl-Dickstein et al., 2011;Alain et al., 2016;Goyal et al., 2017;Bordes et al., 2017;Song & Ermon, 2019;Ho et al., 2020).",
    "next": "We build on Song et al."
  },
  {
    "index": 1963,
    "source_corpus_id": 238531510,
    "ref_id": "b27",
    "citation_corpus_id": 3652072,
    "start": 9503,
    "end": 9523,
    "title": "LEARNING PARAMETRIC CLOSED-LOOP POLICIES FOR MARKOV POTENTIAL GAMES",
    "abstract": "Multiagent systems where the agents interact among themselves and with an stochastic environment can be formalized as stochastic games. We study a subclass of these games, named Markov potential games (MPGs), that appear often in economic and engineering applications when the agents share some common resource. We consider MPGs with continuous state-action variables, coupled constraints and nonconvex rewards. Previous analysis followed a variational approach that is only valid for very simple cases (convex rewards, invertible dynamics, and no coupled constraints); or considered deterministic dynamics and provided openloop (OL) analysis, studying strategies that consist in predefined action sequences, which are not optimal for stochastic environments. We present a closed-loop (CL) analysis for MPGs and consider parametric policies that depend on the current state and where agents adapt to stochastic transitions. We provide easily verifiable, sufficient and necessary conditions for a stochastic game to be an MPG, even for complex parametric functions (e.g., deep neural networks); and show that a closed-loop Nash equilibrium (NE) can be found (or at least approximated) by solving a related optimal control problem (OCP). This is useful since solving an OCP-which is a single-objective problem-is usually much simpler than solving the original set of coupled OCPs that form the game-which is a multiobjective control problem. This is a considerable improvement over the previously standard approach for the CL analysis of MPGs, which gives no approximate solution if no NE belongs to the chosen parametric family, and which is practical only for simple parametric forms. We illustrate the theoretical contributions with an example by applying our approach to a noncooperative communications engineering game. We then solve the game with a deep reinforcement learning algorithm that learns policies that closely approximates an exact variational NE of the game.",
    "prev": "Also, both works above only consider CE/CCE for general-sum Markov games, and do not present results for learning Nash equilibria for Markov potential games.",
    "curr": "Markov potential games Lastly, a recent line of works considers Markov potential games (Macua et al., 2018;Leonardos et al., 2021;Zhang et al., 2021), a subset of general-sum Markov games in which the Nash equilibrium admits more efficient algorithms.",
    "next": "Leonardos et al."
  },
  {
    "index": 1964,
    "source_corpus_id": 263671852,
    "ref_id": "b39",
    "citation_corpus_id": 244130146,
    "start": 3463,
    "end": 3481,
    "title": "Published as a conference paper at ICLR 2022 SOLVING INVERSE PROBLEMS IN MEDICAL IMAGING WITH SCORE-BASED GENERATIVE MODELS",
    "abstract": "Reconstructing medical images from partial measurements is an important inverse problem in Computed Tomography (CT) and Magnetic Resonance Imaging (MRI). Existing solutions based on machine learning typically train a model to directly map measurements to medical images, leveraging a training dataset of paired images and measurements. These measurements are typically synthesized from images using a fixed physical model of the measurement process, which hinders the generalization capability of models to unknown measurement processes. To address this issue, we propose a fully unsupervised technique for inverse problem solving, leveraging the recently introduced score-based generative models. Specifically, we first train a score-based generative model on medical images to capture their prior distribution. Given measurements and a physical model of the measurement process at test time, we introduce a sampling method to reconstruct an image consistent with both the prior and the observed measurements. Our method does not assume a fixed measurement process during training, and can thus be flexibly adapted to different measurement processes at test time. Empirically, we observe comparable or better performance to supervised learning techniques in several medical imaging tasks in CT and MRI, while demonstrating significantly better generalization to unknown measurement processes.",
    "prev": "Model-based DL (MBDL) is an alternative to traditional DL that explicitly uses knowledge of the forward model by integrating DL denoisers as implicit priors into model-based optimization algorithms (Venkatakrishnan et al., 2013;Romano et al., 2017).It has been generally observed that learned denoisers are essential for achieving the state-of-the-art results in many imaging contexts (Metzler et al., 2018;Ulondu-Mendes et al., 2023;Ryu et al., 2019;Hurault et al., 2022;Wu et al., 2020).However, most prior work in the area has focused on methods that can only produce point estimates without any quantification of the reconstruction uncertainty (Belhasin et al., 2023), which can be essential in critical applications such as healthcare or security (Liu et al., 2023).",
    "curr": "In recent years, the exploration of strategies for sampling from the posterior probability has emerged as a focal point in the field of inverse problem in imaging (Pereyra et al., 2015;Bouman & Buzzard, 2023;Chung et al., 2023;Song et al., 2022).This pursuit has given rise to a plethora of techniques, encompassing wellestablished methods such as Gibbs sampling (Coeurdoux et al., 2023), the Unadjusted Langevin Algorithm\n\n\nBackground\n\nInverse Problem.Many problems can be formulated as an inverse problem involving the estimation of an unknown vector x ∈ R d from its degraded observation y = Ax + n , where A ∈ R m×d is a measurement operator and n ∼ N (0, σ 2 I m ) is usually the Gaussian noise.",
    "next": "Posterior Sampling.When the estimation task is ill-posed, it becomes essential to include additional assumptions on the unknown x in the estimation process.In the Bayesian framework, one can utilize p(x) as the prior to regularize such estimation problems, and samples from the posterior distribution p(x|y).The relationship is then established formally using Bayes's rule p(x|y) ∝ p(y|x)p(x), where p(y|x) denoted as the likelihood function."
  },
  {
    "index": 1965,
    "source_corpus_id": 55481903,
    "ref_id": "b17",
    "citation_corpus_id": 6628106,
    "start": 28336,
    "end": 28354,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "The ResNet18 is trained using cutout, data augmentation and stochastic gradient descent with a 0.9 Nesterov momentum and a learning rate starting at 0.1 and divided by 5 at epochs 60, 120 and 160.",
    "curr": "The WideResNet is trained using Adam (Kingma & Ba, 2014) and a learning rate of 0.001.",
    "next": "STABILITY OF THE FORGETTING EVENTS\n\nIn Fig 10,  11 Noising THE DATA SETS\n\nIn Section 4, we analyzed the effect of adding label noise on the distribution of forgetting events."
  },
  {
    "index": 1966,
    "source_corpus_id": 222179041,
    "ref_id": "b11",
    "citation_corpus_id": 2808403,
    "start": 5867,
    "end": 5871,
    "title": "Deep Learning for Physical Processes: Incorporating Prior Scientific Knowledge",
    "abstract": "We consider the use of Deep Learning methods for modeling complex phenomena like those occurring in natural physical processes. With the large amount of data gathered on these phenomena the data intensive paradigm could begin to challenge more traditional approaches elaborated over the years in fields like maths or physics. However, despite considerable successes in a variety of application domains, the machine learning field is not yet ready to handle the level of complexity required by such problems. Using an example application, namely Sea Surface Temperature Prediction, we show how general background knowledge gained from physics could be used as a guideline for designing efficient Deep Learning models. In order to motivate the approach and to assess its generality we demonstrate a formal link between the solution of a class of differential equations underlying a large family of physical phenomena and the proposed model. Experiments and comparison with series of baselines including a state of the art numerical approach is then provided. * equal contribution .",
    "prev": "Short run times are also a desirable property for fluid simulation in visualization and graphics [46,41,47].",
    "curr": "Learned simulations can be useful for real-world predictions where the physical model, parameters or boundary conditions are not fully known [12].",
    "next": "Conversely, the accuracy of predictions can be increased by including specialized knowledge about the system modelled in the form of loss terms [43,23], or by physics-informed feature normalization [40]."
  },
  {
    "index": 1969,
    "source_corpus_id": 211678252,
    "ref_id": "b13",
    "citation_corpus_id": 52878445,
    "start": 2744,
    "end": 2761,
    "title": "TOWARDS UNDERSTANDING REGULARIZATION IN BATCH NORMALIZATION",
    "abstract": "Batch Normalization (BN) improves both convergence and generalization in training neural networks. This work understands these phenomena theoretically. We analyze BN by using a basic block of neural networks, consisting of a kernel layer, a BN layer, and a nonlinear activation function. This basic network helps us understand the impacts of BN in three aspects. First, by viewing BN as an implicit regularizer, BN can be decomposed into population normalization (PN) and gamma decay as an explicit regularization. Second, learning dynamics of BN and the regularization show that training converged with large maximum and effective learning rate. Third, generalization of BN is explored by using statistical mechanics. Experiments demonstrate that BN in convolutional neural networks share the same traits of regularization as the above analyses. * The first three authors contribute equally. Corresponding to pluo.",
    "prev": "search community has sought to understand why it makes it possible to train deeper networks and leads to benefits like faster convergence.",
    "curr": "This work typically centers on the normalization aspect of BatchNorm, explicitly eliding γ or β or treating BatchNorm as a black box without particular consideration for these parameters (e.g., Santurkar et al., 2018;Bjorck et al., 2018;Yang et al., 2019;Luo et al., 2019).",
    "next": "In this paper, we focus our attention specifically on the role and expressive power of γ and β. BatchNorm is ubiquitous in modern deep learning, meaning these parameters are present by default in numerous models that researchers and practitioners train every day."
  },
  {
    "index": 1970,
    "source_corpus_id": 238583203,
    "ref_id": "b6",
    "citation_corpus_id": 218487073,
    "start": 3002,
    "end": 3020,
    "title": "Group Equivariant Generative Adversarial Networks",
    "abstract": "Generative adversarial networks are the state of the art for generative modeling in vision, yet are notoriously unstable in practice. This instability is further exacerbated with limited training data. However, in the synthesis of domains such as medical or satellite imaging, it is often overlooked that the image label is invariant to global image symmetries (e.g., rotations and reflections). In this work, we improve gradient feedback between generator and discriminator using an inductive symmetry prior via group-equivariant convolutional networks. We replace convolutional layers with equivalent group-convolutional layers in both generator and discriminator, allowing for better optimization steps and increased expressive power with limited samples. In the process, we extend recent GAN developments to the group-equivariant setting. We demonstrate the utility of our methods by improving both sample fidelity and diversity in the class-conditional synthesis of a diverse set of globally-symmetric imaging modalities.",
    "prev": "When a rotated state occurs in the environment the optimal policy is permuted, both between and within agents.",
    "curr": "Equivariant and geometric deep learning have gained traction in recent years, showing promising results in supervised learning (Cohen & Welling, 2016;Winkels & Cohen, 2018;Weiler et al., 2018;Weiler & Cesa, 2019;Worrall et al., 2017;Fuchs et al., 2020;Thomas et al., 2018), unsupervised learning (Dey et al., 2021) and reinforcement learning (van der Pol et al., 2020;Simm et al., 2021).",
    "next": "In single agent reinforcement learning, enforcing equivariance to group symmetries has been shown to improve data efficiency, for example with MDP homomorphic networks (van der Pol et al., 2020), trajectory augmentation (Lin et al., 2020;Mavalankar, 2020), or symmetric locomotion policies (Abdolhosseini et al., 2019)."
  },
  {
    "index": 1972,
    "source_corpus_id": 221516475,
    "ref_id": "b6",
    "citation_corpus_id": 52967399,
    "start": 6488,
    "end": 6509,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "However, no prior work has comprehensively measured the knowledge models have across many real-world domains.",
    "curr": "Until recently, researchers primarily used fine-tuned models on downstream tasks (Devlin et al., 2019).",
    "next": "However, larger pretrained models like GPT-3  have made it possible to achieve competitive performance without fine-tuning by using few-shot learning, which removes the need for a large fine-tuning set."
  },
  {
    "index": 1973,
    "source_corpus_id": 211252411,
    "ref_id": "b2",
    "citation_corpus_id": 203736530,
    "start": 7733,
    "end": 7753,
    "title": "Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks",
    "abstract": "Recent research shows that the following two models are equivalent: (a) infinitely wide neural networks (NNs) trained under ℓ2 loss by gradient descent with infinitesimally small learning rate (b) kernel regression with respect to so-called Neural Tangent Kernels (NTKs)(Jacot et al., 2018). An efficient algorithm to compute the NTK, as well as its convolutional counterparts, appears inArora et al. (2019a), which allowed studying performance of infinitely wide nets on datasets like CIFAR-10. However, superquadratic running time of kernel methods makes them best suited for small-data tasks. We report results suggesting neural tangent kernels perform strongly on low-data tasks. 1. On a standard testbed of classification/regression tasks from the UCI database, NTK SVM beats the previous gold standard, Random Forests (RF), and also the corresponding finite nets. 2. On CIFAR-10 with 10 -640 training samples, Convolutional NTK consistently beats ResNet-34 by 1% -3%. 3. On VOC07 testbed for few-shot image classification tasks on ImageNet with transfer learning(Goyal et al., 2019), replacing the linear SVM currently used with a Convolutional NTK SVM consistently improves performance. 4. Comparing the performance of NTK with the finite-width net it was derived from, NTK behavior starts at lower net widths than suggested by theoretical analysis(Arora et al., 2019a). NTK's efficacy may trace to lower variance of output.Ironically, while the above-mentioned analysis, at first sight, appears to reduce the study of a complicated model -deep networks -to an older, simpler model -kernel regression -in practice the simpler model is computationally less efficient because running time of kernel regression can be quadratic in the number of data points! 1 Thus computing using CNTK kernel on large datasets like ImageNet currently appears infeasible. Even on CIFAR-10, it seems infeasible to incorporate data augmentation.However, kernel classifiers are very efficient on small datasets. Here NTKs could conceivably be practical while at the same time bringing some of the power of deep networks to these settings. We recall that recentlyOlson et al. (2018)showed that multilayer neural networks can be reasonably effective on small datasets, specifically on a UCI testbed of tasks with as few as dozens of training examples. Of course, this required some hyperparameter tuning, although they noted that such tuning is also needed for the champion method, Random Forests (RF), which multilayer neural networks could not beat.It is thus natural to check if NTK -corresponding to infinitely wide fully-connected networks -performs well in such small-data tasks 2 . Convex objectives arising from kernels have stable solvers with minimal hyperparameter tuning. Furthermore, random initialization in deep network training seems to lead to higher variance in the output, which can hurt performance in small-data settings. Can NTK's do better? Below we will see that in the setup of Olson et al.(2018), NTK predictors indeed outperforms corresponding finite deep networks, and also slightly beats the earlier gold standard, Random Forests. This suggests NTK predictors should belong in any list of off-the-shelf machine learning methods.Following are low-data settings where we used NTKs and CNTKs: • In the testbed of 90 classification tasks from UCI database, NTK predictor achieves superior, and arguably the strongest classification performance. This is verified via several standard statistical tests, including Friedman Rank, Average Accuracy, Percentage of the Maximum Accuracy (PMA) and probability of achieving 90%/95% maximum accuracy (P90 and P95), performed to compare performances of different classifiers on 90 datasets from UCI database. (The authors plan to release the code, to allow off-the-shelf use of this method. It does not require GPUs.) • We find the performance of NN is close to that of NTK. On every dataset from UCI database, the difference between the classification accuracy of NN and that of NTK is within 5%. On the other hand, on some datasets, the difference between classification accuracy of NN (or NTK) and that of other classifiers like RF can be as high as 20%. This indicates in low-data settings, NTK is indeed a good description of NN. Furthermore, we find NTK is more stable (smaller variance), which seems to help it achieve better accuracy on small datasets (cf.Figure 2b). • CNTK is useful in computer vision tasks with small-data. On CIFAR-10, we compare CNTK with ResNet using 10 -640 training samples and find CNTK can beat ResNet by 1% − 3%. We further study few-shot image classification task on VOC07 dataset. The standard method is to first use a pre-trained network, e.g., ResNet-50 trained on ImageNet, to extract features and then directly apply a linear classifier on the extracted features(Goyal et al., 2019). Here we replace the linear classifier with CNTK and obtain better classification accuracy in various setups.Paper organization. Section 2 discusses related work. Section 3 reviews the derivation of NTK. Section 4 presents experiments using NN and NTK on UCI datasets. Section 5 presents experiments using CNN and CNTK on small CIFAR-10 datasets. Section 6 presents experiments using CNTK for the few-shot learning setting. Additional technical details are presented in appendix.Related WorkOur paper is inspired byFernández-Delgado et al. (2014)which conducted extensive experiments on UCI dataset. Their conclusion is random forest performs the best, which is followed by the SVM with Gaussian 1 The bottleneck is constructing the kernel, which scales quadratically with the number of data points (Arora et al., 2019a). The regression also requires matrix inversion, which can be cubic in the number of data points.2 Note that NTKs can also be used in kernel SVMs, which are not known to be equivalent to training infinitely wide networks. Currently, equivalence is only known for ridge regression. We tried both.",
    "prev": "(2013) for a survey.",
    "curr": "In particular, representation learning is widely adopted for few-shot learning tasks (Sun et al., 2017, Goyal et al., 2019, Arora et al., 2020.",
    "next": "Representation learning is also closely connected to meta-learning (Schaul and Schmidhuber, 2010)."
  },
  {
    "index": 1974,
    "source_corpus_id": 213704197,
    "ref_id": "b16",
    "citation_corpus_id": 53388625,
    "start": 3153,
    "end": 3177,
    "title": "THE LOTTERY TICKET HYPOTHESIS: FINDING SPARSE, TRAINABLE NEURAL NETWORKS",
    "abstract": "Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard technique for pruning weights naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the lottery ticket hypothesis: dense, randomly-initialized feed-forward networks contain subnetworks (winning tickets) that-when trained in isolation-arrive at comparable test accuracy in a comparable number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Furthermore, the winning tickets we find above that size learn faster than the original network and exhibit higher test accuracy. arXiv:1803.03635v4 [cs.LG] 27 Nov 2018 1. Randomly initialize a neural network f (x; θ 0 ) (where θ 0 ∼ D θ ). 2. Train the network for j iterations, reaching parameters θ j . 3. Prune s% of the parameters, creating a mask m where P m = (100 − s)%. 4. To extract the winning ticket, reset the remaining parameters to their values in θ 0 , creating the untrained network f (x; m θ 0 ).If dense networks contain winning tickets and pruning reveals them, then the network f (x; m θ 0 )when trained for j iterations-will reach similar accuracy to f (x; θ j ) at least as quickly, and m will be too sparse for a randomly-reinitialized or randomly-sparsified network to do the same.Results. We identify winning tickets in a fully-connected architecture for MNIST and convolutional architectures for CIFAR10 across several optimization strategies (SGD, momentum, and Adam) with techniques like dropout, weight decay, and batchnorm. In deeper networks, our pruning-based strategy for finding winning tickets is sensitive to the learning rate: it requires warmup to find winning tickets at higher learning rates. The winning tickets we find are 10-20% (or less) of the size of the Anonymous. Gradient descent provably optimizes over-parameterized neural networks. Dally. Dsd: Regularizing deep neural networks with dense-sparse-dense training flow. Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size. arXiv preprint arXiv:1602.07360, 2016.",
    "prev": "The class of structured linear maps includes fixed specialized transforms such as the discrete Fourier transform (DFT) and Hadamard transform used in signal processing (Cooley et al., 1969), convolutions for image, language, and speech modeling , and low-rank and sparse matrices for efficient storage and inference on edge devices (Yu et al., 2017).",
    "curr": "Forms of structure such as sparsity have been at the forefront of recent advances in ML (Frankle & Carbin, 2019), and are critical for on-device and energy-efficient models, two application areas of tremendous recent interest (Tsidulko, 2019;Schwartz et al., 2019).",
    "next": "There are a plethora of classes of structured linear maps, each with a significantly different representation, algorithm, and implementation."
  },
  {
    "index": 1976,
    "source_corpus_id": 3458858,
    "ref_id": "b20",
    "citation_corpus_id": 604334,
    "start": 2457,
    "end": 2479,
    "title": "Intriguing properties of neural networks",
    "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
    "prev": "Under the black-box attack model, the attacker does not have access to the classification model parameters; whereas in the white-box attack model, the attacker has complete access to the model architecture and parameters, including potential defense mechanisms (Papernot et al.",
    "curr": "INTRODUCTION\n\nDespite their outstanding performance on several machine learning tasks, deep neural networks have been shown to be susceptible to adversarial attacks (Szegedy et al., 2014;Goodfellow et al., 2015).",
    "next": "These attacks come in the form of adversarial examples: carefully crafted perturbations added to a legitimate input sample."
  },
  {
    "index": 1978,
    "source_corpus_id": 247628243,
    "ref_id": "b32",
    "citation_corpus_id": 7478738,
    "start": 2895,
    "end": 2916,
    "title": "Linguistic Regularities in Continuous Space Word Representations",
    "abstract": "Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, \"King -Man + Woman\" results in a vector very close to \"Queen.\" We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems.",
    "prev": "Precisely approximating this large number of rare events is one of the foundational challenges for models of natural language (Good, 1953;Jelinek, 1980;Katz, 1987;Kneser & Ney, 1995;Wood et al., 2011;Goldwater et al., 2011).",
    "curr": "Autoregressive neural language models (Bengio et al., 2003;Mikolov et al., 2013;Radford et al., 2019) attempt to do so by decomposing the probability of an event (a sequence) into a series of conditional distributions, each parameterized by a shared neural network.",
    "next": "Recently, a growing body work has sought to understand how these language models (LM) fit the distribution of a language beyond standard measures such Published as a conference paper at ICLR 2022 as perplexity."
  },
  {
    "index": 1979,
    "source_corpus_id": 232257793,
    "ref_id": "b32",
    "citation_corpus_id": 56475856,
    "start": 6832,
    "end": 6855,
    "title": "LEARNING TO ADAPT IN DYNAMIC, REAL-WORLD ENVIRONMENTS THROUGH META-REINFORCEMENT LEARNING",
    "abstract": "Although reinforcement learning methods can achieve impressive results in simulation, the real world presents two major challenges: generating samples is exceedingly expensive, and unexpected perturbations or unseen situations cause proficient but specialized policies to fail at test time. Given that it is impractical to train separate policies to accommodate all situations the agent may see in the real world, this work proposes to learn how to quickly and effectively adapt online to new tasks. To enable sample-efficient learning, we consider learning online adaptation in the context of model-based reinforcement learning. Our approach uses meta-learning to train a dynamics model prior such that, when combined with recent data, this prior can be rapidly adapted to the local context. Our experiments demonstrate online adaptation for continuous control tasks on both simulated and real-world agents. We first show simulated agents adapting their behavior online to novel terrains, crippled body parts, and highly-dynamic environments. We also illustrate the importance of incorporating online adaptation into autonomous agents that operate in the real world by applying our method to a real dynamic legged millirobot. We demonstrate the agent's learned ability to quickly adapt online to a missing leg, adjust to novel terrains and slopes, account for miscalibration or errors in pose estimation, and compensate for pulling payloads. 2 1 * Equal Contribution 2 Videos available at: https://sites.",
    "prev": "HyperDynamics makes explicit assumptions about the relationships between these systems, and attempts to exploit this regularity and learn such commonalities across different system variations.",
    "curr": "There also exsists a family of approaches that attempt online model adaptation via meta-learning (Finn et al., 2017;Nagabandi et al., 2019;Clavera et al., 2018;Nagabandi et al., 2018a).",
    "next": "These methods perform online adaptation on the parameters of the dynamics models through gradient descent."
  },
  {
    "index": 1981,
    "source_corpus_id": 219981134,
    "ref_id": "b19",
    "citation_corpus_id": 67855732,
    "start": 3447,
    "end": 3467,
    "title": "VARIATIONAL AUTOENCODER WITH ARBITRARY CONDITIONING",
    "abstract": "We propose a single neural probabilistic model based on variational autoencoder that can be conditioned on an arbitrary subset of observed features and then sample the remaining features in \"one shot\". The features may be both real-valued and categorical. Training of the model is performed by stochastic variational Bayes. The experimental evaluation on synthetic data, as well as feature imputation and image inpainting problems, shows the effectiveness of the proposed approach and diversity of the generated samples.Published as a conference paper at ICLR 2019The experimental evaluation shows that the proposed model successfully samples from the conditional distributions. The distribution over samples is close to the true conditional distribution. This property is very important when the true distribution has several modes. The model is shown to be effective in feature imputation problem which helps to increase the quality of subsequent discriminative models on different problems from UCI datasets collection (Lichman, 2013). We demonstrate that model can generate diverse and realistic image inpaintings on MNIST (LeCun et al., 1998), Omniglot (Lake et al., 2015 and CelebA  (Liu et al., 2015)  datasets, and works even better than the current state of the art inpainting techniques in terms of peak signal to noise ratio (PSNR).The paper is organized as follows. In section 2 we review the related works. In section 3 we briefly describe variational autoencoders and conditional variational autoencoders. In section 4 we define the problem, describe the VAEAC model and its training procedure. In section 5 we evaluate VAEAC. Section 6 concludes the paper. Appendix contains additional explanations, theoretical analysis, and experiments for VAEAC. 2 RELATED WORK Universal Marginalizer (Douglas et al., 2017) is a model based on a feed-forward neural network which approximates marginals of unobserved features conditioned on observable values. A related idea of an autoregressive model of joint probability was previously proposed in Germain et al. (2015). The detailed description of the model, experimental evaluation and comparison with VAEAC are contained in section 5.3. Yoon et al. (2018) propose a GANs-based model called GAIN which solves the same problem as VAEAC.In contrast to VAEAC, GAIN doesnt use unobserved data during training, which makes it easier to apply to the missing features imputation problem. Nevertheless, it turns into a disadvantage when the fully-observed training data is available but the missingness rate at the testing stage is high. For example, in inpainting setting GAIN cannot learn the conditional distribution over MNIST digits given one horizontal line of the image while VAEAC can (see appendix D.4). The comparison of VAEAC and GAIN on the missing feature imputation problem is given in section 5.1 and appendix D.2. Rezende et al. (2014) [Appendix F], Sohl-Dickstein et al. (2015), Goyal et al. (2017), and Bordes et al. (2017) propose to fill missing data with noise and run Markov chain with a learned transition operator. The stationary distribution of such chains approximates the true conditional distribution of the unobserved features. Nevertheless, such methods are computationally expensive at the test time and require fully-observed training data.Image inpainting is a classic computer vision problem. Most of the earlier methods rely on local and texture information or hand-crafted problem-specific features(Bertalmio et al., 2000). In past years multiple neural network based approaches have been proposed. Pathak et al. (2016), Yeh et al. (2016) and Yang et al. (2017) use different kinds and combinations of adversarial, reconstruction, texture and other losses. Li et al. (2017) focuses on face inpainting and uses two adversarial losses and one semantic parsing loss to train the generative model. In Yeh et al. (2017)GANs are first trained on the whole training dataset. The inpainting is an optimization procedure that finds the latent variables that explain the observed features best. Then, the obtained latents are passed through the generative model to restore the unobserved portion of the image. We can say that VAEAC is a similar model which uses prior network to find a proper latents instead of solving the optimization problem.All described methods aim to produce a single realistic inpainting, while VAEAC is capable of sampling diverse inpaintings. Additionally, Yeh et al. (2016), Yang et al. (2017) and Yeh et al. (2017 have high testtime computational complexity of inpainting, because they require an optimization problem to be solved. On the other hand, VAEAC is a \"single-shot\" method with a low computational cost.",
    "prev": "This setting generally requires a joint model for data and missing mechanism.",
    "curr": "Deep latent variable models (DLVMs, Kingma and Welling, 2013;Rezende et al., 2014) have recently been used for inference and imputation in missing data problems (Nazabal et al., 2018;Ma et al., 2018Ivanov et al., 2019;.",
    "next": "This led to impressive empirical results in the MAR and MCAR case, in particular for high-dimensional data."
  },
  {
    "index": 1984,
    "source_corpus_id": 249848272,
    "ref_id": "b91",
    "citation_corpus_id": 3432876,
    "start": 9885,
    "end": 9908,
    "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
    "abstract": "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.",
    "prev": "NLP.",
    "curr": "Tasks with text as the only input and output modalities, including text classification (Williams et al., 2018), question answering (Rajpurkar et al., 2016) and text summarization (Graff et al., 2003).",
    "next": "Language Modeling."
  },
  {
    "index": 1985,
    "source_corpus_id": 238408056,
    "ref_id": "b11",
    "citation_corpus_id": 3521071,
    "start": 2209,
    "end": 2212,
    "title": "DIVERSITY IS ALL YOU NEED: LEARNING SKILLS WITHOUT A REWARD FUNCTION",
    "abstract": "Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose \"Diversity is All You Need\"(DIAYN), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning. * Work done as a member of the Google AI Residency Program (g.co/airesidency). learning shared hierarchies. arXiv preprint arXiv:1710.09767, 2017. Justin Fu, John Co-Reyes, and Sergey Levine. Ex2: Exploration with exemplar models for deep reinforcement learning. . Reinforcement learning with deep energybased policies. arXiv preprint arXiv:1702.08165, 2017. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018. 9 Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart J Russell, and Anca Dragan. Inverse reward design. , et al. Learning to navigate in complex environments. arXiv preprint arXiv:1611.03673, 2016. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. Shakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for intrinsically motivated reinforcement learning. In Advances in neural information processing systems, pp. 2125-2133, 2015. Jean-Baptiste Mouret and Stéphane Doncieux. Overcoming the bootstrap problem in evolutionary robotics using behavioral diversity. . Bridging the gap between value and policy based reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2772-2782, 2017. Pierre-Yves Oudeyer, Frdric Kaplan, and Verena V Hafner. Intrinsic motivation systems for autonomous mental development. IEEE transactions on evolutionary computation, 11(2):265-286, 2007. Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by selfsupervised prediction. arXiv preprint arXiv:1705.05363, 2017. Vitchyr Pong, Shixiang Gu, Murtaza Dalal, and Sergey Levine. Temporal difference models: Model-free deep rl for model-based control. arXiv preprint arXiv:1802.09081, 2018. . Quality diversity: A new frontier for evolutionary computation. Frontiers in Robotics and AI, 3:40, 2016. Richard M Ryan and Edward L Deci. Intrinsic and extrinsic motivations: Classic definitions and new directions. Contemporary educational psychology, 25(1):54-67, 2000. Jürgen Schmidhuber. Formal theory of creativity, fun, and intrinsic motivation. IEEE Transactions on Autonomous Mental Development, 2(3):230-247, 2010.",
    "prev": "For example, some methods learn representations of the observations [32,52] or representations of the dynamics model [11,22,53].",
    "curr": "In this work, we focus on methods that learn a set of potentially-useful policies, often known as skills [1,12,20,41,51].",
    "next": "That is, the learned representation corresponds to a reparametrization of policies."
  },
  {
    "index": 1988,
    "source_corpus_id": 263334319,
    "ref_id": "b47",
    "citation_corpus_id": 233296494,
    "start": 5101,
    "end": 5118,
    "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
    "abstract": "When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, finetuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are \"fantastic\" and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true fewshot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13% relative improvement for GPTfamily models across eleven different established text classification tasks.",
    "prev": " and using it to train agents via reinforcement learning (RL) (Sutton & Barto, 2018).Our method, named Motif, uses an LLM to express preferences over pairs of event captions extracted from a dataset of observations and then distills them into an intrinsic reward.The resulting reward is then maximized directly or in combination with an extrinsic reward coming from the environment.A guiding principle in the design of Motif is the observation that it is often easier to evaluate than to generate (Sutton, 2001;Schulman, 2023).Motif's LLM expresses preferences over textual event captions; these are only required to be coarse descriptions of events happening in the environment rather than fine-grained step-by-step portrayals of the current observations.The LLM is not even asked to understand the low-level action space, which may be composite or continuous.",
    "curr": "In comparison, an approach using an LLM as a policy typically requires a complete text interface with the environment (Wang et al., 2023;Yao et al., 2022).When using Motif, the LLM remains in the space of high-level knowledge it was trained on, but leverages the capabilities of deep RL algorithms to deal with decision-making under rich observation and action spaces.",
    "next": "We apply Motif to the challenging NetHack Learning Environment (NLE) (Küttler et al., 2020), and learn intrinsic rewards from Llama 2's preferences (Touvron et al., 2023) on a dataset of gameplays.This dataset, collected by policies of different levels of proficiency, only contains observations from the environment, without any action or reward information.Using this framework, we show that the resulting intrinsic reward drastically improves subsequent learning of a policy by RL.Motif excels in both relatively dense reward tasks, such as maximizing the game score, and extremely sparse reward tasks, such as the oracle task.To our knowledge, our paper is the first to make progress on this task without leveraging expert demonstrations.Notably, an agent trained only through Mot"
  },
  {
    "index": 1989,
    "source_corpus_id": 204788663,
    "ref_id": "b31",
    "citation_corpus_id": 3707461,
    "start": 39343,
    "end": 39364,
    "title": "Some Considerations on Learning to Explore via Meta-Reinforcement Learning",
    "abstract": "We interpret meta-reinforcement learning as the problem of learning how to quickly find a good sampling distribution in a new environment. This interpretation leads to the development of two new meta-reinforcement learning algorithms: E-MAML and E-RL 2 . Results are presented on a new environment we call 'Krazy World': a difficult high-dimensional gridworld which is designed to highlight the importance of correctly differentiating through sampling distributions in meta-reinforcement learning. Further results are presented on a set of maze environments. We show E-MAML and E-RL 2 deliver better performance than baseline algorithms on both tasks.",
    "prev": "The environments we used are also taken from this implementation.",
    "curr": "E-MAML (Stadie et al., 2018) and ProMP (Rothfuss et al., 2019) were trained using the reference implementation provided by Rothfuss et al.",
    "next": "(2019)."
  },
  {
    "index": 1990,
    "source_corpus_id": 202889230,
    "ref_id": "b7",
    "citation_corpus_id": 8768364,
    "start": 8745,
    "end": 8764,
    "title": "Density estimation using Real NVP",
    "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.",
    "prev": "That is, if we use such a function to define one direction of the transformation in a flow model, the other direction can only be approximated numerically using iterative root-finding methods (Ho et al., 2019).",
    "curr": "In this work, we don't consider invertible normalizing flows based on dimension splitting, such as RealNVP (Dinh et al., 2017), since they are not applicable to 1D data.",
    "next": "In the context of TPPs, our goal is to model the distribution p(τ ) of inter-event times."
  },
  {
    "index": 1991,
    "source_corpus_id": 5592690,
    "ref_id": "b12",
    "citation_corpus_id": 2100831,
    "start": 1708,
    "end": 1733,
    "title": "MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text",
    "abstract": "We present MCTest, a freely available set of stories and associated questions intended for research on the machine comprehension of text. Previous work on machine comprehension (e.g., semantic modeling) has made great strides, but primarily focuses either on limited-domain datasets, or on solving a more restricted goal (e.g., open-domain relation extraction). In contrast, MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension. Reading comprehension can test advanced abilities such as causal reasoning and understanding the world, yet, by being multiple-choice, still provide a clear metric. By being fictional, the answer typically can be found only in the story itself. The stories and questions are also carefully limited to those a young child would understand, reducing the world knowledge that is required for the task. We present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions. By screening workers (with grammar tests) and stories (with grading), we have ensured that the data is the same quality as another set that we manually edited, but at one tenth the editing cost. By being open-domain, yet carefully restricted, we hope MCTest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text.",
    "prev": "INTRODUCTION\n\nMachine comprehension of text is one of the ultimate goals of natural language processing.",
    "curr": "While the ability of a machine to understand text can be assessed in many different ways, in recent years, several benchmark datasets have been created to focus on answering questions as a way to evaluate machine comprehension (Richardson et al., 2013;Hermann et al., 2015;Hill et al., 2016;Rajpurkar et al., 2016).",
    "next": "In this setup, typically the machine is first presented with a piece of text such as a news article or a story."
  },
  {
    "index": 1992,
    "source_corpus_id": 260886874,
    "ref_id": "b6",
    "citation_corpus_id": 7164502,
    "start": 21353,
    "end": 21377,
    "title": "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments",
    "abstract": "We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machineproduced translation and human-produced reference translations. Unigrams can be matched based on their surface forms, stemmed forms, and meanings; furthermore, METEOR can be easily extended to include more advanced matching strategies.Once all generalized unigram matches between the two strings have been found, METEOR computes a score for this matching using a combination of unigram-precision, unigram-recall, and a measure of fragmentation that is designed to directly capture how well-ordered the matched words in the machine translation are in relation to the reference. We evaluate METEOR by measuring the correlation between the metric scores and human judgments of translation quality. We compute the Pearson R correlation value between its scores and human quality assessments of the LDC TIDES 2003 Arabic-to-English and Chinese-to-English datasets.We perform segment-bysegment correlation, and show that METEOR gets an R correlation value of 0.347 on the Arabic data and 0.331 on the Chinese data. This is shown to be an improvement on using simply unigramprecision, unigram-recall and their harmonic F1 combination. We also perform experiments to show the relative contributions of the various mapping modules.",
    "prev": "Further, our coding problems, derived from HumanEval, are very specific, such as keeping track of a bank account balance (see Figure 14).",
    "curr": "Prior work on evaluating code explanations (Lu et al., 2021;Cui et al., 2022) has relied on metrics such as METEOR (Banerjee & Lavie, 2005) or BLEU (Papineni et al., 2002).",
    "next": "By chaining code explanation with code synthesis, we can evaluate this task using the execution-based pass@k metric overcoming the major limitations of BLEU and other heuristics-based metrics (Reiter, 2018)."
  },
  {
    "index": 1995,
    "source_corpus_id": 249375516,
    "ref_id": "b17",
    "citation_corpus_id": 7152340,
    "start": 2097,
    "end": 2101,
    "title": "Positive Unlabeled Learning for Deceptive Reviews Detection",
    "abstract": "Deceptive reviews detection has attracted significant attention from both business and research communities. However, due to the difficulty of human labeling needed for supervised learning, the problem remains to be highly challenging. This paper proposed a novel angle to the problem by modeling PU (positive unlabeled) learning. A semi-supervised model, called mixing population and individual property PU learning (MPIPUL), is proposed. Firstly, some reliable negative examples are identified from the unlabeled dataset. Secondly, some representative positive examples and negative examples are generated based on LDA (Latent Dirichlet Allocation). Thirdly, for the remaining unlabeled examples (we call them spy examples), which can not be explicitly identified as positive and negative, two similarity weights are assigned, by which the probability of a spy example belonging to the positive class and the negative class are displayed. Finally, spy examples and their similarity weights are incorporated into SVM (Support Vector Machine) to build an accurate classifier. Experiments on gold-standard dataset demonstrate the effectiveness of MPIPUL which outperforms the state-of-the-art baselines.",
    "prev": "Empirically, ReCPE improves all state-of-the-art CPE methods on various datasets, implying that the assumption has indeed been violated here.",
    "curr": "† Correspondence to Tongliang Liu\n\nIntroduction\n\nPositive-unlabeled (PU) learning can date back to 1990s [9,8,24], and there has been a surge of interest in this learning scenario in recent years because of the difficulty to annotate large-scale datasets [32,11,10,5,19,31,33,20,2,14,1,39,41].",
    "next": "It is also fallen into different applications, such as knowledge-base completion [13,28], text classification [23,25], and medical diagnosis [6,42]."
  },
  {
    "index": 1996,
    "source_corpus_id": 248157108,
    "ref_id": "b18",
    "citation_corpus_id": 52967399,
    "start": 5892,
    "end": 5913,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "All examples shown are real outputs from our INCODER-6.7B model, with the regions inserted by the model highlighted in orange.",
    "curr": "INFILLING AND SYNTHESIS VIA CAUSAL MASKING\n\nNeural models for code generation have either utilized a left-to-right (causal) autoregressive language modeling objective (Brown et al., 2020;Chen et al., 2021a) or, as BERT does, a masked language modeling objective (Devlin et al., 2019;Feng et al., 2020).",
    "next": "Both approaches have strengths and weaknesses."
  },
  {
    "index": 1999,
    "source_corpus_id": 53783799,
    "ref_id": "b7",
    "citation_corpus_id": 5687613,
    "start": 2105,
    "end": 2108,
    "title": "A LEARNED REPRESENTATION FOR ARTISTIC STYLE",
    "abstract": "The diversity of painting styles represents a rich visual vocabulary for the construction of an image. The degree to which one may learn and parsimoniously capture this visual vocabulary measures our understanding of the higher level features of paintings, if not images in general. In this work we investigate the construction of a single, scalable deep network that can parsimoniously capture the artistic style of a diversity of paintings. We demonstrate that such a network generalizes across a diversity of artistic styles by reducing a painting to a point in an embedding space. Importantly, this model permits a user to explore new painting styles by arbitrarily combining the styles learned from individual paintings. We hope that this work provides a useful step towards building rich models of paintings and offers a window on to the structure of the learned representation of artistic style.",
    "prev": "Recently, neural style transfer techniques [10,11,16,12,20,19] showed that the correlation between the features extracted from the trained deep neural networks is quite effective on capturing the visual styles and content that can be used for generating images similar in style and content.",
    "curr": "However, since the definition of similarity is inherently vague, the objective of style transfer is not well defined [8] and one can imagine multiple stylized images from the same pair of content/style images.",
    "next": "Existing real-time style transfer methods generate only one stylization for a given content/style pair and while the stylizations of different methods usually look distinct [28,14], it is not possible to say that one stylization is better in all contexts since people react differently to images based on their background and situation."
  },
  {
    "index": 2000,
    "source_corpus_id": 249431362,
    "ref_id": "b43",
    "citation_corpus_id": 1957433,
    "start": 14162,
    "end": 14187,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "Step 2 -Candidate selection For each word w i in s sequentially, a set of substitution candidates C of N elements is extracted.",
    "curr": "This candidate set is constructed from the counter-fitted GloVe (Pennington et al., 2014) synonym embeddings by the authors of Mrkšić et al.",
    "next": "(2016)."
  },
  {
    "index": 2002,
    "source_corpus_id": 53100211,
    "ref_id": "b1",
    "citation_corpus_id": 3503217,
    "start": 43025,
    "end": 43051,
    "title": "Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments",
    "abstract": "Ability to continuously learn and adapt from limited experience in nonstationary environments is an important milestone on the path towards general intelligence. In this paper, we cast the problem of continuous adaptation into the learning-to-learn framework. We develop a simple gradient-based meta-learning algorithm suitable for adaptation in dynamically changing and adversarial scenarios. Additionally, we design a new multi-agent competitive environment, RoboSumo, and define iterated adaptation games for testing various aspects of continuous adaptation strategies. We demonstrate that meta-learning enables significantly more efficient adaptation than reactive baselines in the few-shot regime. Our experiments with a population of agents that learn and compete suggest that meta-learners are the fittest.While virtually any changes in an environment could induce some kind of nonstationarity (e.g., changes in the physics or characteristics of the agent), environments with multiple agents are particularly challenging due to complexity of the emergent behavior and are of practical interest with applications ranging from multiplayer games [16] to coordinating self-driving fleets[17]. Multi-agent environments are nonstationary from the perspective of any individual agent since all actors are learning and changing concurrently[7,18]. In this paper, we consider the problem of continuous adaptation to a learning opponent in a competitive multi-agent setting.To this end, we design RoboSumo-a 3D environment with simulated physics that allows pairs of agents to compete against each other. To test continuous adaptation, we introduce iterated adaptation games-a new setting where a trained agent competes against the same opponent for multiple rounds of a repeated game, while both are allowed to update their policies and change their behaviors between the rounds. In such iterated games, from the agent's perspective, the environment changes from round to round, and the agent ought to adapt in order to win the game. Additionally, the competitive component of the environment makes it not only nonstationary but also adversarial, which provides a natural training curriculum and encourages learning robust strategies[7,19,20].We evaluate our meta-learning agents along with a number of baselines on a (single-agent) locomotion task with handcrafted nonstationarity and on iterated adaptation games in RoboSumo. Our results demonstrate that meta-learned adaptation strategies clearly dominate other adaptation methods in the few-shot regime in both single-and multi-agent settings. Finally, we carry out a large-scale experiment where we train a diverse population of agents with different anatomies, policy architectures, and adaptation methods, and make them interact by competing against each other in iterated games. We evaluate the agents based on their TrueSkills[21]in these games, as well as evolve the population as whole for a few generations-the agents that lose disappear, while the winners get duplicated. Our results suggest that the agents with meta-learned adaptation strategies end up being the fittest. Videos that demonstrate adaptation behaviors in different tasks are available at https://goo.gl/tboqaN.",
    "prev": "However, POWERPLAY is orthogonal to our work as we consider a different setting where the agent cannot directly control the new tasks that will be encountered in the environment and thus must instead learn to adapt and react to non-stationarity conditions.",
    "curr": "In contrast to past work on meta-learning for few shot learning (Santoro et al., 2016;Vinyals et al., 2016;Ravi & Larochelle, 2016; and reinforcement learning across successive tasks (Al-Shedivat et al., 2018), we are not only trying to improve the speed of learning on new data, but also trying to do it in a way that preserves knowledge of past data and generalizes to future data.",
    "next": "While past work has considered learning to influence gradient angles, so that there is more alignment and thus faster learning within a task, we focus on a setting where we would like to influence gradient angles from all tasks at all points in time."
  },
  {
    "index": 2004,
    "source_corpus_id": 50783524,
    "ref_id": "b27",
    "citation_corpus_id": 2578649,
    "start": 3179,
    "end": 3195,
    "title": "Chinese Dependency Parsing with Large Scale Automatically Constructed Case Structures",
    "abstract": "This paper proposes an approach using large scale case structures, which are automatically constructed from both a small tagged corpus and a large raw corpus, to improve Chinese dependency parsing. The case structure proposed in this paper has two characteristics: (1) it relaxes the predicate of a case structure to be all types of words which behaves as a head; (2) it is not categorized by semantic roles but marked by the neighboring modifiers attached to a head. Experimental results based on Penn Chinese Treebank show the proposed approach achieved 87.26% on unlabeled attachment score, which significantly outperformed the baseline parser without using case structures.",
    "prev": "In previous work, before the rise of deep learning, the semi-supervised parsing setting has been mainly tackled with two-step algorithms.",
    "curr": "On the one hand, feature extraction methods first learn an intermediate representation using an unlabeled dataset which is then used as input to train a supervised parser (Koo et al., 2008;Yu et al., 2008;Chen et al., 2009;Suzuki et al., 2011).",
    "next": "On the other hand, the self-training and co-training methods start by learning a supervised parser that is then used to label extra data."
  },
  {
    "index": 2005,
    "source_corpus_id": 3538627,
    "ref_id": "b19",
    "citation_corpus_id": 1957433,
    "start": 30873,
    "end": 30898,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "B.2 COUNTING MODELS\n\nEach of the considered counting models makes use of the same basic architecture for encoding the question and comparing it with each of the detected objects.",
    "curr": "For each model, we initialized the word embeddings from GloVe (Pennington et al., 2014) and encoded the question with an LSTM of hidden size 1024.",
    "next": "The only differences in the model-specific implementations of the language module was the hidden size of the scoring function f S ."
  },
  {
    "index": 2006,
    "source_corpus_id": 259298593,
    "ref_id": "b28",
    "citation_corpus_id": 220525799,
    "start": 2964,
    "end": 2984,
    "title": "Long-Tail Learning via Logit Adjustment",
    "abstract": "Real-world classification problems typically exhibit an imbalanced or long-tailed label distribution, wherein many labels are associated with only a few samples. This poses a challenge for generalisation on such labels, and also makes naïve learning biased towards dominant labels. In this paper, we present two simple modifications of standard softmax cross-entropy training to cope with these challenges. Our techniques revisit the classic idea of logit adjustment based on the label frequencies, either applied post-hoc to a trained model, or enforced in the loss during training. Such adjustment encourages a large relative margin between logits of rare versus dominant labels. These techniques unify and generalise several recent proposals in the literature, while possessing firmer statistical grounding and empirical performance. A reference implementation of our methods is available at:https://github.com/google-research/google-research/tree/master/logit_adjustment.",
    "prev": " interested in distribution shifts resulting from changes in marginal class priors or group priors from training to test.This is often caused by a skewed distribution of classes or groups in the training data, and vanilla empirical risk minimization (ERM) can lead to models overfitting to spurious features.These spurious features seem to be predictive on the training data but do not generalize well to the test set.For example, the background can act as a spurious feature for predicting the object of interest in images, e.g., camels in a desert background, water-birds in water background (Sagawa et al., 2020).",
    "curr": "Distributionally robust optimization (DRO) (Ben-Tal et al., 2013;Duchi et al., 2016;Duchi & Namkoong, 2018;Sagawa et al., 2020) is a popular framework to address this problem which formulates a robust optimization problem over class-or group-specific losses.The common metrics of interest in the DRO methods are either the average accuracy or the worst accuracy over classes/groups (Menon et al., 2021;Jitkrittum et al., 2022;Rosenfeld et al., 2022;Piratla et al., 2022;Sagawa et al., 2020;Zhai et al., 2021;Xu et al., 2020;Kirichenko et al., 2022).However, these metrics only cover the two ends of the full spectrum of distribution shifts in the priors.We are instead motivated by the need to measure the robustness of the model at various points on the spectrum of distribution shifts.",
    "next": "To this end, we consider applications where we are provided a target prior distribution (that could either come from a practitioner or default to the uniform distribution), and would like to train a model that is robust to varying distribution shifts around this prior.Instead of taking the conventional approach of optimizing for either the average accuracy or the worst-case accuracy, we seek to maximize the minimum accuracy within a δ-radius ball around the specified target distribution.This strategy allows us to encourage generalization on a spectrum of controlled distribution shifts governed by the param"
  },
  {
    "index": 2009,
    "source_corpus_id": 220525844,
    "ref_id": "b19",
    "citation_corpus_id": 3285020,
    "start": 2273,
    "end": 2276,
    "title": "BEYOND SHARED HIERARCHIES: DEEP MULTITASK LEARNING THROUGH SOFT LAYER ORDERING",
    "abstract": "Existing deep multitask learning (MTL) approaches align layers shared between tasks in a parallel ordering. Such an organization significantly constricts the types of shared structure that can be learned. The necessity of parallel ordering for deep MTL is first tested by comparing it with permuted ordering of shared layers.The results indicate that a flexible ordering can enable more effective sharing, thus motivating the development of a soft ordering approach, which learns how shared layers are applied in different ways for different tasks. Deep MTL with soft ordering outperforms parallel ordering methods across a series of domains. These results suggest that the power of deep MTL comes from learning highly general building blocks that can be assembled to meet the demands of each task.",
    "prev": "If the agent is capable of accumulating knowledge in some form of compositional representation, it could then selectively reuse and combine relevant pieces of knowledge to construct novel solutions.",
    "curr": "Many different compositional representations for handling multiple tasks have been proposed over the last few years [32,9,13,19].",
    "next": "In this work, we address the novel question of how to learn these compositional structures in a lifelong learning setting."
  },
  {
    "index": 2011,
    "source_corpus_id": 226964491,
    "ref_id": "b1",
    "citation_corpus_id": 7164502,
    "start": 22571,
    "end": 22595,
    "title": "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments",
    "abstract": "We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machineproduced translation and human-produced reference translations. Unigrams can be matched based on their surface forms, stemmed forms, and meanings; furthermore, METEOR can be easily extended to include more advanced matching strategies.Once all generalized unigram matches between the two strings have been found, METEOR computes a score for this matching using a combination of unigram-precision, unigram-recall, and a measure of fragmentation that is designed to directly capture how well-ordered the matched words in the machine translation are in relation to the reference. We evaluate METEOR by measuring the correlation between the metric scores and human judgments of translation quality. We compute the Pearson R correlation value between its scores and human quality assessments of the LDC TIDES 2003 Arabic-to-English and Chinese-to-English datasets.We perform segment-bysegment correlation, and show that METEOR gets an R correlation value of 0.347 on the Arabic data and 0.331 on the Chinese data. This is shown to be an improvement on using simply unigramprecision, unigram-recall and their harmonic F1 combination. We also perform experiments to show the relative contributions of the various mapping modules.",
    "prev": "Evaluation Metrics For discriminative tasks, we choose accuracy as our metric following other conventional question answering tasks.",
    "curr": "For generative tasks, we report automated metrics including BLEU (Papineni et al., 2002), METEOR (Banerjee & Lavie, 2005), CIDEr (Vedantam et al., 2015), and SPICE (Anderson et al., 2016) following the leaderboard of COMMONGEN (Lin et al., 2020).",
    "next": "Results for COMMONGEN are on the test set and others are on the official development set."
  },
  {
    "index": 2012,
    "source_corpus_id": 208513914,
    "ref_id": "b2",
    "citation_corpus_id": 11212020,
    "start": 1683,
    "end": 1705,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "For machine translation, DeFINE improves the efficiency of the Transformer model by about 1.4 times while delivering similar performance.",
    "curr": "INTRODUCTION\n\nNeural models for NLP tasks, such as language modeling and machine translation, require large vocabularies for generality (Chelba et al., 2013;Bahdanau et al., 2015;Luong et al., 2015;Merity et al., 2017).",
    "next": "These models often employ a similar architecture: tokens (e.g., words, sub-words, or characters), represented as one-hot vectors, are mapped to a dense continuous space; they are then processed by a context model; finally, the contextualized representations are mapped back to a vocabulary-sized vector for computing next-token probabilities."
  },
  {
    "index": 2013,
    "source_corpus_id": 67877096,
    "ref_id": "b23",
    "citation_corpus_id": 13298214,
    "start": 2083,
    "end": 2106,
    "title": "LEARNING TO NAVIGATE IN COMPLEX ENVIRONMENTS",
    "abstract": "Learning to navigate in complex environments with dynamic elements is an important milestone in developing AI agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks leveraging multimodal sensory inputs. In particular we consider jointly learning the goal-driven reinforcement learning problem with auxiliary depth prediction and loop closure classification tasks. This approach can learn to navigate from raw sensory input in complicated 3D mazes, approaching human-level performance even under conditions where the goal location changes frequently. We provide detailed analysis of the agent behaviour 1 , its ability to localise, and its network activity dynamics, showing that the agent implicitly learns key navigation abilities.",
    "prev": "This step of task-independent exploration is quite critical yet often ignored in current approaches for navigation.",
    "curr": "When it comes to navigation, currently there are two paradigms: (a) geometric reconstruction and path-planning based approaches (Hartley & Zisserman, 2003;Thrun et al., 2005;LaValle, 2006), and (b) learning-based approaches (Mirowski et al., 2017;Savinov et al., 2018;Zhu et al., 2017).",
    "next": "SLAM-based approaches, first build a map and then use localization and path planning for navigation."
  },
  {
    "index": 2018,
    "source_corpus_id": 231846526,
    "ref_id": "b12",
    "citation_corpus_id": 12308095,
    "start": 59365,
    "end": 59383,
    "title": "COUNTERING ADVERSARIAL IMAGES USING INPUT TRANSFORMATIONS",
    "abstract": "This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.",
    "prev": "We compare the robust accuracy with different magnitude with 20-step attack with a step size of 2/255.",
    "curr": "For BPDA attack, we apply median smoothing, JPEGFilter and BitSqueezing as input transformation adapted from(Guo et al., 2018) as defense strategies.",
    "next": "For SPSA attack, we follow(Uesato et al., 2018)  and set the Adam learning rate 0.01, perturbation size δ = 0.01."
  },
  {
    "index": 2019,
    "source_corpus_id": 53115163,
    "ref_id": "b12",
    "citation_corpus_id": 3461154,
    "start": 8673,
    "end": 8691,
    "title": "Published as a conference paper at ICLR 2018 DORA THE EXPLORER: DIRECTED OUTREACHING REINFORCEMENT ACTION-SELECTION",
    "abstract": "Exploration is a fundamental aspect of Reinforcement Learning, typically implemented using stochastic action-selection. Exploration, however, can be more efficient if directed toward gaining new world knowledge. Visit-counters have been proven useful both in practice and in theory for directed exploration. However, a major limitation of counters is their locality. While there are a few model-based solutions to this shortcoming, a model-free approach is still missing. We propose E-values, a generalization of counters that can be used to evaluate the propagating exploratory value over state-action trajectories. We compare our approach to commonly used RL techniques, and show that using E-values improves learning and performance over traditional counters. We also show how our method can be implemented with function approximation to efficiently learn continuous MDPs. We demonstrate this by showing that our approach surpasses state of the art performance in the Freeway Atari 2600 game.",
    "prev": "Such prediction errors tend to decrease as the agent collects more experience similar to the current one.",
    "curr": "For this reason even trivial prediction problems like predicting a constant zero function can work as exploration bonuses (Fox et al., 2018).",
    "next": "RANDOM NETWORK DISTILLATION\n\nThis paper introduces a different approach where the prediction problem is randomly generated."
  },
  {
    "index": 2022,
    "source_corpus_id": 263608710,
    "ref_id": "b36",
    "citation_corpus_id": 155092004,
    "start": 2118,
    "end": 2139,
    "title": "BERT Rediscovers the Classical NLP Pipeline",
    "abstract": "Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lowerlevel decisions on the basis of disambiguating information from higher-level representations.",
    "prev": "pose a notion of \"tree averaging,\" based on which we further propose a novel ensemble method for unsupervised parsing.To improve inference efficiency, we further distill the ensemble knowledge into a student model; such an ensemble-then-distill process is an effective approach to mitigate the over-smoothing problem existing in common multi-teacher distilling methods.Experiments show that our method surpasses all previous approaches, consistently demonstrating its effectiveness and robustness across various runs, with different ensemble components, and under domain-shift conditions.",
    "curr": "1\n\nIntroduction\n\nConstituency parsing is a core task in natural language processing (NLP), which interprets a sentence and induces its constituency tree, a syntactic structure representation that organizes words and phrases into a hierarchy (Carnie, 2007).It has wide applications in various downstream tasks, including semantic role labeling (Mohammadshahi and Henderson, 2023) and explainability of AI models (Tenney et al., 2019).Traditionally, parsing is accomplished by supervised models trained with linguistically annotated treebanks (Charniak, 2000), which are expensive to obtain and may not be available for low-resource scenarios.Also, these supervised parsers often underperform when encountering domain shifts.This motivates researchers to explore unsupervised methods as it eliminates the need for annotated data.",
    "next": "To address unsupervised parsing, researchers have proposed various heuristics and indirect supervision signals.Clark (2001) employs context distribution clustering to induce a probabilistic context-free grammar (PCFG;Booth, 1969).Klein and Manning (2002) define a joint distribution for sentences and parse structures, the latter learned by expectation-maximization (EM) algorithms.Snyder et al."
  },
  {
    "index": 2024,
    "source_corpus_id": 249063147,
    "ref_id": "b43",
    "citation_corpus_id": 222291132,
    "start": 24869,
    "end": 24889,
    "title": "FAIRSEQ S2T: Fast Speech-to-Text Modeling with FAIRSEQ",
    "abstract": "We introduce FAIRSEQ S2T, a FAIRSEQ (Ott et al., 2019) extension for speech-to-text (S2T) modeling tasks such as end-to-end speech recognition and speech-to-text translation. It follows FAIRSEQ's careful design for scalability and extensibility. We provide end-to-end workflows from data pre-processing, model training to offline (online) inference. We implement state-of-the-art RNN-based as well as Transformer-based models and opensource detailed training recipes. FAIRSEQ's machine translation models and language models can be seamlessly integrated into S2T workflows for multi-task learning or transfer learning. FAIRSEQ S2T documentation and examples are available at https: //github.com/pytorch/fairseq/tree/ master/examples/speech_to_text.",
    "prev": "In decoding speed, latency is computed as the time to decode the single n-frame speech sample averaged over the test set using 1 V100 GPU.",
    "curr": "We compare TranSpeech with other systems using the publicly-available fairseq framework (Ott et al., 2019), including 1) Direct ASR, where we transcribe S2ST data with open-sourced ASR as reference and compute BELU; 2) Direct TTS, where we synthesize speech samples with target units, and then transcribe the speech to text and compute BELU; 3) S2T+TTS cascaded system, where we train the S2T basic transformer model (Wang et al., 2020a) and then apply TTS model Kong et al., 2020) for speech generation; 4) basic transformer (Lee et al., 2021a) without using text, and 5) basic norm transformer (Lee et al., 2021b) with speaker normalization.",
    "next": "Table 1 summarizes the translation accuracy and inference latency among all systems, and we have the following observations: 1) Bilateral perturbation (3 vs. 4) improves S2ST performance by a large margin of 2.9 BLEU points."
  },
  {
    "index": 2025,
    "source_corpus_id": 237605219,
    "ref_id": "b7",
    "citation_corpus_id": 28202810,
    "start": 2293,
    "end": 2315,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "Code is available at https://github.com/cyanrain7/ Trust-Region-Policy-Optimisation-in-Multi-Agent-Reinforcement-Learning.",
    "curr": "INTRODUCTION\n\nPolicy gradient (PG) methods have played a major role in recent developments of reinforcement learning (RL) algorithms (Silver et al., 2014;Schulman et al., 2015a;Haarnoja et al., 2018).",
    "next": "Among the many PG variants, trust region learning (Kakade & Langford, 2002), with two typical embodiments of Trust Region Policy Optimisation (TRPO) (Schulman et al., 2015a) and Proximal Policy Optimisation (PPO) (Schulman et al., 2017) algorithms, offer supreme empirical performance in both discrete and continuous RL problems (Duan et al., 2016;Mahmood et al., 2018)."
  },
  {
    "index": 2028,
    "source_corpus_id": 49416020,
    "ref_id": "b20",
    "citation_corpus_id": 3144218,
    "start": 6353,
    "end": 6374,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "If one accepts the value of convolutions as prima facie, a natural solution is to replace convolutions with graph shift invariant filters which are known to be valid generalizations of (convolutional) time invariant filters (Bruna et al., 2014).",
    "curr": "This idea is not only natural but has been demonstrated to work well in practical implementations of Graph Neural Networks (GNNs) (Defferrard et al., 2016;Gama et al., 2018;Gilmer et al., 2017;Henaff et al., 2015;Kipf & Welling, 2017).",
    "next": "Same as Euclidean scattering transforms, our graph scattering transforms differ from GNNs in that they do not have to be trained."
  },
  {
    "index": 2030,
    "source_corpus_id": 237491843,
    "ref_id": "b8",
    "citation_corpus_id": 13807351,
    "start": 11366,
    "end": 11368,
    "title": "ENTROPY-SGD: BIASING GRADIENT DESCENT INTO WIDE VALLEYS",
    "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.",
    "prev": "In Adam, the bias-corrected first and second moment estimates, i.e., m k andD k in (2), are as follows:\nm k = 1−β1 1−β k 1 k i=1 β k−i 1 g i ,D k = 1−β2 1−β k 2 k i=1 β k−i 2 diag(g i g i ).",
    "curr": "(5)\nThere have been many other first-order methods with adaptive scaling [24,9,23,40].",
    "next": "The methods described so far have only used the information of the gradient for preconditioning m k in (2)."
  },
  {
    "index": 2031,
    "source_corpus_id": 256615445,
    "ref_id": "b11",
    "citation_corpus_id": 52890982,
    "start": 7848,
    "end": 7852,
    "title": "ADVERSARIAL AUDIO SYNTHESIS",
    "abstract": "While Generative Adversarial Networks (GANs) have seen wide success at the problem of synthesizing realistic images, they have seen little application to audio generation. Unlike for images, a barrier to success is that the best discriminative representations for audio tend to be non-invertible, and thus cannot be used to synthesize listenable outputs. In this paper we introduce WaveGAN, a first attempt at applying GANs to unsupervised synthesis of raw-waveform audio. Our experiments demonstrate that WaveGAN can produce intelligible words from a small vocabulary of speech, and can also synthesize audio from other domains such as drums, bird vocalizations, and piano. Qualitatively, we find that human judges prefer the sound quality of generated examples from WaveGAN over those from a method which naïvely apply GANs on image-like audio feature representations.",
    "prev": "Related Work\n\n\nGenerative Models for Audio\n\nDeep generative models for audio, learn, directly or implicitly, the distribution of mixtures, represented in our notation by p(y), possibly conditioning on additional data such as text.",
    "curr": "Various general-purpose generative models, such as autoregressive models, GANs [12], and diffusion models, have been adapted for use in the audio field.",
    "next": "Autoregressive models have a well-established presence in audio modeling [69]."
  },
  {
    "index": 2033,
    "source_corpus_id": 231933756,
    "ref_id": "b2",
    "citation_corpus_id": 256189,
    "start": 2143,
    "end": 2162,
    "title": "Semi-Supervised Learning for Neural Machine Translation",
    "abstract": "While end-to-end neural machine translation (NMT) has made remarkable progress recently, NMT systems only rely on parallel corpora for parameter estimation. Since parallel corpora are usually limited in quantity, quality, and coverage, especially for low-resource languages, it is appealing to exploit monolingual corpora to improve NMT. We propose a semisupervised approach for training NMT models on the concatenation of labeled (parallel corpora) and unlabeled (monolingual corpora) data. The central idea is to reconstruct the monolingual corpora using an autoencoder, in which the sourceto-target and target-to-source translation models serve as the encoder and decoder, respectively. Our approach can not only exploit the monolingual corpora of the target language, but also of the source language. Experiments on the Chinese-English dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems.",
    "prev": "Cross Entropy Loss update backward model\n\nINTRODUCTION\n\nWhile Neural Machine Translation (NMT) delivers state-of-the-art performance across many translation tasks, this performance is usually contingent on the existence of large amounts of training data (Sutskever et al., 2014;Vaswani et al., 2017).",
    "curr": "Since large parallel training datasets are often unavailable for many languages and domains, various methods have been developed to leverage abundant monolingual corpora (Gulcehre et al., 2015;Cheng et al., 2016;Sennrich et al., 2016;Xia et al., 2016;Hoang et al., 2018;Song et al., 2019;He et al., 2020).",
    "next": "Among such methods, one particularly popular approach is back-translation (BT; Sennrich et al."
  },
  {
    "index": 2034,
    "source_corpus_id": 250408279,
    "ref_id": "b24",
    "citation_corpus_id": 5033497,
    "start": 9056,
    "end": 9078,
    "title": "Phrase-Based & Neural Unsupervised Machine Translation",
    "abstract": "Machine translation systems achieve near human-level performance on some languages, yet their effectiveness strongly relies on the availability of large amounts of parallel sentences, which hinders their applicability to the majority of language pairs. This work investigates how to learn to translate when having access to only large monolingual corpora in each language. We propose two model variants, a neural and a phrase-based model. Both versions leverage a careful initialization of the parameters, the denoising effect of language models and automatic generation of parallel data by iterative back-translation. These models are significantly better than methods from the literature, while being simpler and having fewer hyper-parameters. On the widely used WMT'14 English-French and WMT'16 German-English benchmarks, our models respectively obtain 28.1 and 25.2 BLEU points without using a single parallel sentence, outperforming the state of the art by more than 11 BLEU points. On low-resource languages like English-Urdu and English-Romanian, our methods achieve even better results than semisupervised and supervised approaches leveraging the paucity of available bitexts. Our code for NMT and PBSMT is publicly available. 1",
    "prev": ".",
    "curr": "y i−1 ))\n\nCOMMON OBJECTIVE FUNCTIONS\n\nTransCoder (Roziere et al., 2020) learns to translate between programming languages by leveraging three unsupervised objectives developed for natural language (Lample et al., 2018b):\n\nMasked Language Modeling (MLM) trains an encoder to predict randomly masked inputs.",
    "next": "It is commonly used to pre-train embeddings for natural (Devlin et al., 2018; and programming languages (Kanade et al., 2020;."
  },
  {
    "index": 2037,
    "source_corpus_id": 263830324,
    "ref_id": "b0",
    "citation_corpus_id": 235436185,
    "start": 2822,
    "end": 2839,
    "title": "BEIT: BERT Pre-Training of Image Transformers",
    "abstract": "We introduce a self-supervised vision representation model BEIT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT [DCLT19] developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e., image patches (such as 16×16 pixels), and visual tokens (i.e., discrete tokens). We first \"tokenize\" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEIT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods.",
    "prev": "ntation learning is one of the most fundamental problems in 3D computer vision, especially with the rapid development of 3D sensors (e.g., LiDAR) and the growing demands in realworld applications, e.g., autonomous driving, augmented/virtual reality and robotics.Existing methods make great progress in 3D model architecture (Qi et al., 2017a;b;Yu et al., 2021;Wang et al., 2019), learning objective (Yu et al., 2022;Wang et al., 2021), task-oriented modeling (Zhou et al., 2020;Yin et al., 2021;Zhao et al., 2021), etc.However, most of the works explore at a relatively small scale, with limited parameters, data, and task scenarios.Learning scalable 3D representation that can transfer in the wild is relatively unexplored and remains a challenging problem.",
    "curr": "In the past few years, scaling up pre-trained language models (Brown et al., 2020;Liu et al., 2019;Raffel et al., 2020) has largely revolutionized natural language processing.Some recent works (Radford et al., 2021;Dosovitskiy et al., 2020;Bao et al., 2021;He et al., 2022;Fang et al., 2022) translate the progress from language to 2D vision via model and data scaling.Motivated by their success, it is appealing that we can also lift this success from 2D to 3D, i.e., to learn a scalable 3D representation model that can transfer in the 3D world.Recently, as the release of a large-scale 3D dataset Objaverse (Deitke et al., 2023b), a few works have tried to explore scalable pretraining in 3D, but either still limit to the small-scale 3D backbones (Xue et al., 2023a;b), or can hardly scale to a relatively larger size (Liu et al., 2023), e.g., 72M in Fig.",
    "next": "1."
  },
  {
    "index": 2040,
    "source_corpus_id": 53192805,
    "ref_id": "b22",
    "citation_corpus_id": 6706414,
    "start": 2998,
    "end": 3023,
    "title": "Published as a conference paper at ICLR 2015 EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES",
    "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples-inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
    "prev": "The reason for these failures and how they may be avoided or at least mitigated is an active research area (Schmidt et al., 2018;Gilmer et al., 2018b;Bubeck et al., 2018).",
    "curr": "So far, the study of adversarial examples has mostly been concerned with the setting of small perturbation, or -adversaries (Goodfellow et al., 2015;Madry et al., 2017;Raghunathan et al., 2018).",
    "next": "Perturbation-based adversarial examples are appealing because they allow to quantitatively measure notions of adversarial robustness (Brendel et al., 2018)."
  },
  {
    "index": 2041,
    "source_corpus_id": 263622509,
    "ref_id": "b9",
    "citation_corpus_id": 245334837,
    "start": 18430,
    "end": 18450,
    "title": "RVS: WHAT IS ESSENTIAL FOR OFFLINE RL VIA SUPERVISED LEARNING?",
    "abstract": "Recent work has shown that supervised learning alone, without temporal difference (TD) learning, can be remarkably effective for offline RL. When does this hold true, and which algorithmic components are necessary? Through extensive experiments, we boil supervised learning for offline RL down to its essential elements. In every environment suite we consider, simply maximizing likelihood with a two-layer feedforward MLP is competitive with state-of-the-art results of substantially more complex methods based on TD learning or sequence modeling with Transformers. Carefully choosing model capacity (e.g., via regularization or architecture) and choosing which information to condition on (e.g., goals or rewards) are critical for performance. These insights serve as a field guide for practitioners doing Reinforcement Learning via Supervised Learning (which we coin RvS learning). They also probe the limits of existing RvS methods, which are comparatively weak on random data, and suggest a number of open problems.",
    "prev": "(2021), as the initial RTG we set a target RTG that represents the desired performance.During the inference, DC receives the current trajectory data, generates an action to obtain the next state and reward, and subsequently subtracts the reward from the preceding RTG.",
    "curr": "RELATED WORKS\n\nReturn-Conditioned BC Both DC and DT fall under the category of return-conditioned BC, an active research field of offline RL (Kumar et al., 2019;Schmidhuber, 2019;Chen et al., 2021;Emmons et al., 2021;David et al., 2023).For example, RvS (Emmons et al., 2021)  Offline RL with Online Finetuning It is known that the performance of models trained through offline learning is often limited by the quality of the dataset.Thus, finetuning through online interactions can improve the performance of offline-pretrained models (Zhang et al., 2022;Luo et al., 2023).Overcoming the limitations of DT for online applications, Zheng et al.",
    "next": "(2022) proposed an Online Decision Transformer (ODT), which includes a stochastic policy and an additional maxentropy objective in the loss function.A similar method can be applied to DC for online finetuning."
  },
  {
    "index": 2044,
    "source_corpus_id": 251953252,
    "ref_id": "b45",
    "citation_corpus_id": 10910955,
    "start": 1980,
    "end": 2000,
    "title": "Distant supervision for relation extraction without labeled data",
    "abstract": "Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora. We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACEstyle algorithms, and allowing the use of corpora of any size. Our experiments use Freebase, a large semantic database of several thousand relations, to provide distant supervision. For each pair of entities that appears in some Freebase relation, we find all sentences containing those entities in a large unlabeled corpus and extract textual features to train a relation classifier. Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain). Our model is able to extract 10,000 instances of 102 relations at a precision of 67.6%. We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression.",
    "prev": "INTRODUCTION\n\nNamed entity recognition (NER) is the task of identifying text spans associated with named entities and classifying them into a predefined set of entity types such as person, location, etc.",
    "curr": "As a fundamental component in information extraction systems (Nadeau & Sekine, 2007), NER has been shown to be of benefit to various downstream tasks such as relation extraction (Mintz et al., 2009), coreference resolution (Chang et al., 2013), and fine-grained opinion mining (Choi et al., 2006).",
    "next": "(Karpukhin et al., 2020) and entity linking Zhang et al., 2021a), we propose an efficient BI-encoder for NameD Entity Recognition (BINDER)."
  },
  {
    "index": 2045,
    "source_corpus_id": 238408406,
    "ref_id": "b7",
    "citation_corpus_id": 29153681,
    "start": 15612,
    "end": 15637,
    "title": "META-LEARNING WITH DIFFERENTIABLE CLOSED-FORM SOLVERS",
    "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.Most work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent. Nonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently. In this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning. The main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data. This requires back-propagating errors through the solver steps. While normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage. We propose both closed-form and iterative solvers, based on ridge regression and logistic regression components. Our methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.",
    "prev": "These absolute improvements sometimes constitute significantly to the baseline accuracy in terms of relative importance.",
    "curr": "Datasets:\n\nWe perform experiments on four widely-used and publicly available benchmarks: mini-ImageNet (Vinyals et al., 2016), CIFAR-FS (Bertinetto et al., 2019), tiered-ImageNet , and CUB (Wah et al., 2011).",
    "next": "Each dataset consists of non-overlapping base, validation, and novel classes."
  },
  {
    "index": 2046,
    "source_corpus_id": 199000713,
    "ref_id": "b19",
    "citation_corpus_id": 3626819,
    "start": 2459,
    "end": 2479,
    "title": "Deep contextualized word representations",
    "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
    "prev": "In contrast to supervised and semi-supervised learning, the learner has access only to unlabeled data.",
    "curr": "Even though the task seems ill-posed as there is no natural objective one should optimize, by leveraging domain knowledge this approach can be successfully applied to a variety of problem areas, including image (Kolesnikov et al., 2019;van den Oord et al., 2018;Hénaff et al., 2019;Tian et al., 2019;Hjelm et al., 2019;Bachman et al., 2019) and video classification (Wang and Gupta, 2015;Sun et al., 2019) and natural language understanding (van den Oord et al., 2018;Peters et al., 2018;Devlin et al., 2019).",
    "next": "Recently, there has been a revival of approaches inspired by the InfoMax principle (Linsker, 1988): We choose a representation g(x) maximizing the mutual information (MI) between the input and its representation, possibly subject to some structural constraints."
  },
  {
    "index": 2048,
    "source_corpus_id": 198147763,
    "ref_id": "b26",
    "citation_corpus_id": 38796293,
    "start": 5196,
    "end": 5214,
    "title": "Published as a conference paper at ICLR 2018 DEEP GRADIENT COMPRESSION: REDUCING THE COMMUNICATION BANDWIDTH FOR DISTRIBUTED TRAINING",
    "abstract": "Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD are redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during this compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270× to 600× without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.",
    "prev": "on a ring or torus), independent of the number of devices n. Additionally, recent theoretical results indicate that decentralized schemes can be as efficient as the centralized approaches, at least when considering convergence of training loss vs. iterations (Scaman et al., 2017(Scaman et al., , 2018Koloskova et al., 2019;Tang et al., 2018;Lian et al., 2017).",
    "curr": "To reduce the amount of data that has to be sent over each communication link, gradient compression techniques have been proposed for the standard distributed training case (Alistarh et al., 2017;Wen et al., 2017;Lin et al., 2018b;Wangni et al., 2018;Stich et al., 2018).",
    "next": "For decentralized training of deep neural networks the pioneering work (Tang et al., 2018) introduced two algorithms (DCD, ECD) which allow communication compression."
  },
  {
    "index": 2050,
    "source_corpus_id": 259088588,
    "ref_id": "b0",
    "citation_corpus_id": 221818900,
    "start": 3278,
    "end": 3281,
    "title": "DIFFWAVE: A VERSATILE DIFFUSION MODEL FOR AUDIO SYNTHESIS",
    "abstract": "In this work, we propose DiffWave, a versatile Diffusion probabilistic model for conditional and unconditional Waveform generation. The model is nonautoregressive, and converts the white noise signal into structured waveform through a Markov chain with a constant number of steps at synthesis. It is efficiently trained by optimizing a variant of variational bound on the data likelihood. DiffWave produces high-fidelity audios in Different Waveform generation tasks, including neural vocoding conditioned on mel spectrogram, class-conditional generation, and unconditional generation. We demonstrate that DiffWave matches a strong WaveNet vocoder in terms of speech quality (MOS: 4.44 versus 4.43), while synthesizing orders of magnitude faster. In particular, it significantly outperforms autoregressive and GAN-based waveform models in the challenging unconditional generation task in terms of audio quality and sample diversity from various automatic and human evaluations.",
    "prev": "Generating speech from a silent video has seen significant progress in recent years, partly due to advancements made in deep generative models.",
    "curr": "Specifically in applications such as text-to-speech and mel-spectogram-to-audio (neural vocoder) [1,2].",
    "next": "Despite these advancements, many lip-to-speech methods produce satisfying results only when applied to datasets with a limited number of speakers, and constrained vocabularies, like GRID [3] and TCD-TIMIT [4]."
  },
  {
    "index": 2053,
    "source_corpus_id": 252781163,
    "ref_id": "b10",
    "citation_corpus_id": 8768364,
    "start": 6469,
    "end": 6487,
    "title": "Density estimation using Real NVP",
    "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.",
    "prev": "Empirical results show that LVD outperforms SoTA TPM baselines by a large margin on challenging image modeling tasks.",
    "curr": "Besides, PCs with LVD also achieve competitive results against various widely-used deep generative models, including flow-based models (Kingma & Dhariwal, 2018;Dinh et al., 2016) and variational autoencoders (Maaløe et al., 2019) (Sec.",
    "next": "6)."
  },
  {
    "index": 2054,
    "source_corpus_id": 244714837,
    "ref_id": "b12",
    "citation_corpus_id": 204893960,
    "start": 2729,
    "end": 2749,
    "title": "Fantastic Generalization Measures and Where to Find Them",
    "abstract": "Generalization of deep networks has been of great interest in recent years, resulting in a number of theoretically and empirically motivated complexity measures. However, most papers proposing such measures study only a small set of models, leaving open the question of whether the conclusion drawn from those experiments would remain valid in other settings. We present the first large scale study of generalization in deep networks. We investigate more then 40 complexity measures taken from both theoretical bounds and empirical studies. We train over 10,000 convolutional networks by systematically varying commonly used hyperparameters. Hoping to uncover potentially causal relationships between each measure and generalization, we analyze carefully controlled experiments and show surprising failures of some measures as well as promising measures for further research. * Contributed equally.",
    "prev": "Current upper bounds of this type are loose, and even vacuous.",
    "curr": "There is evidence that such classically derived bounds may be too loose (Dziugaite & Roy, 2017) or that they may not correlate well with generalization (Jiang et al., 2019).",
    "next": "This has motivated a more principled empirical study of the effectiveness of generalization bounds."
  },
  {
    "index": 2056,
    "source_corpus_id": 227305788,
    "ref_id": "b9",
    "citation_corpus_id": 208547755,
    "start": 3448,
    "end": 3469,
    "title": "Published as a conference paper at ICLR 2020 DREAM TO CONTROL: LEARNING BEHAVIORS BY LATENT IMAGINATION",
    "abstract": "Learned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.",
    "prev": "These modeling errors are especially prevalent in high-dimensional observation spaces where loss functions that operate on pixels may focus model capacity on task-irrelevant features (Kaiser et al., 2020).",
    "curr": "Recent work (Hafner et al., 2020;Schrittwieser et al., 2019) has attempted to side-step these issues by learning a world model in a latent space and propagating gradients over multiple time-steps.",
    "next": "While these methods are able to learn accurate world models and achieve high performance on benchmark tasks, their representations are usually trained with task-specific information such as rewards, encouraging the model to focus on tracking task-relevant features but compromising their ability to generalize to new tasks."
  },
  {
    "index": 2059,
    "source_corpus_id": 222142276,
    "ref_id": "b13",
    "citation_corpus_id": 52967399,
    "start": 6678,
    "end": 6699,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "These approaches are called \"self-supervised learning\" because the learning signals, termed pretext tasks, are obtained from the data itself.",
    "curr": "In the image and video domain, pretext tasks include colorization , rotation (Gidaris et al., 2018), or clustering (Asano et al., 2020a;b;Caron et al., 2018;Ji et al., 2018), while in the natural language domain, masked language modeling (Devlin et al., 2019), and next word prediction (Mikolov et al., 2013;Pennington et al., 2014) are extremely popular.",
    "next": "These pretext tasks can be broadly classified into two classes: generative and discriminative."
  },
  {
    "index": 2062,
    "source_corpus_id": 3508234,
    "ref_id": "b13",
    "citation_corpus_id": 6628106,
    "start": 2248,
    "end": 2252,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "This is because choosing to follow your gradient signal by something other than the right amount, either too much or too little, can be very costly in terms of how fast the overall descent procedure achieves a particular level of objective value.",
    "curr": "Understanding that adapting the learning rate is a good thing to do, particularly on a per parameter basis dynamically, led to the development of a family of widely-used optimizers including AdaGrad [9], RMSProp [28], and Adam [14].",
    "next": "However, a persisting commonality of these methods is that they are parameterized by a \"pesky\" fixed global learning rate hyperparameter which still needs tuning."
  },
  {
    "index": 2063,
    "source_corpus_id": 208193111,
    "ref_id": "b27",
    "citation_corpus_id": 604334,
    "start": 2202,
    "end": 2224,
    "title": "Intriguing properties of neural networks",
    "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
    "prev": "This robustness improvement can be observed substantially and consistently even by pushing the network capacity to an unprecedented scale, i.e., ResNet-638.",
    "curr": "INTRODUCTION\n\nAdversarial attacks (Szegedy et al., 2014) can mislead neural networks to make wrong predictions by adding human imperceptible perturbations to input data.",
    "next": "Adversarial training (Goodfellow et al., 2015) is shown to be an effective method to defend against such attacks, which trains neural networks on adversarial images that are generated on-the-fly during training."
  },
  {
    "index": 2065,
    "source_corpus_id": 1741724,
    "ref_id": "b10",
    "citation_corpus_id": 12639289,
    "start": 9644,
    "end": 9673,
    "title": "Recurrent Continuous Translation Models",
    "abstract": "We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.",
    "prev": ".",
    "curr": ", a T ] given an input sequence x (Cho et al., 2014;Kalchbrenner & Blunsom, 2013;Sutskever et al., 2014).",
    "next": "Here a i ∈ A where A is a vocabulary of words."
  },
  {
    "index": 2066,
    "source_corpus_id": 238408001,
    "ref_id": "b1",
    "citation_corpus_id": 203736530,
    "start": 3311,
    "end": 3330,
    "title": "Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks",
    "abstract": "Recent research shows that the following two models are equivalent: (a) infinitely wide neural networks (NNs) trained under ℓ2 loss by gradient descent with infinitesimally small learning rate (b) kernel regression with respect to so-called Neural Tangent Kernels (NTKs)(Jacot et al., 2018). An efficient algorithm to compute the NTK, as well as its convolutional counterparts, appears inArora et al. (2019a), which allowed studying performance of infinitely wide nets on datasets like CIFAR-10. However, superquadratic running time of kernel methods makes them best suited for small-data tasks. We report results suggesting neural tangent kernels perform strongly on low-data tasks. 1. On a standard testbed of classification/regression tasks from the UCI database, NTK SVM beats the previous gold standard, Random Forests (RF), and also the corresponding finite nets. 2. On CIFAR-10 with 10 -640 training samples, Convolutional NTK consistently beats ResNet-34 by 1% -3%. 3. On VOC07 testbed for few-shot image classification tasks on ImageNet with transfer learning(Goyal et al., 2019), replacing the linear SVM currently used with a Convolutional NTK SVM consistently improves performance. 4. Comparing the performance of NTK with the finite-width net it was derived from, NTK behavior starts at lower net widths than suggested by theoretical analysis(Arora et al., 2019a). NTK's efficacy may trace to lower variance of output.Ironically, while the above-mentioned analysis, at first sight, appears to reduce the study of a complicated model -deep networks -to an older, simpler model -kernel regression -in practice the simpler model is computationally less efficient because running time of kernel regression can be quadratic in the number of data points! 1 Thus computing using CNTK kernel on large datasets like ImageNet currently appears infeasible. Even on CIFAR-10, it seems infeasible to incorporate data augmentation.However, kernel classifiers are very efficient on small datasets. Here NTKs could conceivably be practical while at the same time bringing some of the power of deep networks to these settings. We recall that recentlyOlson et al. (2018)showed that multilayer neural networks can be reasonably effective on small datasets, specifically on a UCI testbed of tasks with as few as dozens of training examples. Of course, this required some hyperparameter tuning, although they noted that such tuning is also needed for the champion method, Random Forests (RF), which multilayer neural networks could not beat.It is thus natural to check if NTK -corresponding to infinitely wide fully-connected networks -performs well in such small-data tasks 2 . Convex objectives arising from kernels have stable solvers with minimal hyperparameter tuning. Furthermore, random initialization in deep network training seems to lead to higher variance in the output, which can hurt performance in small-data settings. Can NTK's do better? Below we will see that in the setup of Olson et al.(2018), NTK predictors indeed outperforms corresponding finite deep networks, and also slightly beats the earlier gold standard, Random Forests. This suggests NTK predictors should belong in any list of off-the-shelf machine learning methods.Following are low-data settings where we used NTKs and CNTKs: • In the testbed of 90 classification tasks from UCI database, NTK predictor achieves superior, and arguably the strongest classification performance. This is verified via several standard statistical tests, including Friedman Rank, Average Accuracy, Percentage of the Maximum Accuracy (PMA) and probability of achieving 90%/95% maximum accuracy (P90 and P95), performed to compare performances of different classifiers on 90 datasets from UCI database. (The authors plan to release the code, to allow off-the-shelf use of this method. It does not require GPUs.) • We find the performance of NN is close to that of NTK. On every dataset from UCI database, the difference between the classification accuracy of NN and that of NTK is within 5%. On the other hand, on some datasets, the difference between classification accuracy of NN (or NTK) and that of other classifiers like RF can be as high as 20%. This indicates in low-data settings, NTK is indeed a good description of NN. Furthermore, we find NTK is more stable (smaller variance), which seems to help it achieve better accuracy on small datasets (cf.Figure 2b). • CNTK is useful in computer vision tasks with small-data. On CIFAR-10, we compare CNTK with ResNet using 10 -640 training samples and find CNTK can beat ResNet by 1% − 3%. We further study few-shot image classification task on VOC07 dataset. The standard method is to first use a pre-trained network, e.g., ResNet-50 trained on ImageNet, to extract features and then directly apply a linear classifier on the extracted features(Goyal et al., 2019). Here we replace the linear classifier with CNTK and obtain better classification accuracy in various setups.Paper organization. Section 2 discusses related work. Section 3 reviews the derivation of NTK. Section 4 presents experiments using NN and NTK on UCI datasets. Section 5 presents experiments using CNN and CNTK on small CIFAR-10 datasets. Section 6 presents experiments using CNTK for the few-shot learning setting. Additional technical details are presented in appendix.Related WorkOur paper is inspired byFernández-Delgado et al. (2014)which conducted extensive experiments on UCI dataset. Their conclusion is random forest performs the best, which is followed by the SVM with Gaussian 1 The bottleneck is constructing the kernel, which scales quadratically with the number of data points (Arora et al., 2019a). The regression also requires matrix inversion, which can be cubic in the number of data points.2 Note that NTKs can also be used in kernel SVMs, which are not known to be equivalent to training infinitely wide networks. Currently, equivalence is only known for ridge regression. We tried both.",
    "prev": "Furthermore, we apply this result to show that HANNs have high expressivity by proving that HANN classifiers achieve minimax-optimality when the data has Lipschitz posterior class probability in an overparametrized setting.",
    "curr": "We benchmark the empirical performance of HANNs on a panel of 121 UCI datasets, following several recent neural network and neural tangent kernel works (Klambauer et al., 2017;Wu et al., 2018;Arora et al., 2019;Shankar et al., 2020).",
    "next": "In particular, Klambauer et al."
  },
  {
    "index": 2067,
    "source_corpus_id": 263909212,
    "ref_id": "b31",
    "citation_corpus_id": 259129237,
    "start": 3448,
    "end": 3466,
    "title": "Published as a conference paper at ICLR 2023 METAGL: EVALUATION-FREE SELECTION OF GRAPH LEARNING MODELS VIA META-LEARNING",
    "abstract": "Given a graph learning task, such as link prediction, on a new graph, how can we select the best method as well as its hyperparameters (collectively called a model) without having to train or evaluate any model on the new graph? Model selection for graph learning has been largely ad hoc. A typical approach has been to apply popular methods to new datasets, but this is often suboptimal. On the other hand, systematically comparing models on the new graph quickly becomes too costly, or even impractical. In this work, we develop the first meta-learning approach for evaluation-free graph learning model selection, called METAGL, which utilizes the prior performances of existing methods on various benchmark graph datasets to automatically select an effective model for the new graph, without any model training or evaluations. To quantify similarities across a wide variety of graphs, we introduce specialized meta-graph features that capture the structural characteristics of a graph. Then we design G-M network, which represents the relations among graphs and models, and develop a graph-based meta-learner operating on this G-M network, which estimates the relevance of each model to different graphs. Extensive experiments show that using METAGL to select a model for the new graph greatly outperforms several existing meta-learning techniques tailored for graph learning model selection (up to 47% better), while being extremely fast at test time (∼1 sec).",
    "prev": "As exemplified in Figure 1 (a & b), a simplistic model selector relies on predefined task-specific models for subtasks, increasing the likelihood of intermediate errors and compromising the overall reasoning process.",
    "curr": "Moreover, existing traditional model selection methods, though effective in various domains (Zhao et al., 2021;Park et al., 2022;Lee et al., 2022; Zitovsky Adapting these methods to multi-modal reasoning scenarios, which necessitate multiple models for subtasks, is challenging due to the oversight of subtask dependencies.",
    "next": "To this end, we formally define the problem of model selection in multi-modal reasoning scenarios as our first contribution, and then introduce the M 3 framework (Model Selector for Multi-Modal Reasoning) as our preliminary remedy for the field."
  },
  {
    "index": 2071,
    "source_corpus_id": 252222320,
    "ref_id": "b36",
    "citation_corpus_id": 225040574,
    "start": 4149,
    "end": 4167,
    "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
    "abstract": "The recent \"Text-to-Text Transfer Transformer\" (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We describe the design and modified training of mT5 and demonstrate its stateof-the-art performance on many multilingual benchmarks. All of the code and model checkpoints used in this work are publicly available. 1 * Equal Contribution.Please direct correspondence to",
    "prev": "Another key ingredient to PaLI is the reuse of large unimodal backbones for language and vision modeling, in order to transfer existing capabilities and reduce training cost.",
    "curr": "On the language side, we reuse the 13B-parameter model mT5-XXL (Xue et al., 2021), which already packages language understanding and generation capabilities.",
    "next": "We show that these capabilities are maintained and extended into a multimodal setting."
  },
  {
    "index": 2072,
    "source_corpus_id": 247741535,
    "ref_id": "b17",
    "citation_corpus_id": 28202810,
    "start": 3981,
    "end": 4003,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "However, it is challenging to perform Q-learning with reverse sweep in high-dimensional state spaces since the predecessors of each state are often unknown.",
    "curr": "State-of-the-art Q-learning methods (Mnih et al., 2015;Lillicrap et al., 2015;Haarnoja et al., 2018;Fujimoto et al., 2018) resort to random sampling of data from the replay buffer.",
    "next": "Their speed of convergence can be slow as these  methods do not account for the structure in state transitions when selecting the order of states for updating the Q-values."
  },
  {
    "index": 2073,
    "source_corpus_id": 245335427,
    "ref_id": "b35",
    "citation_corpus_id": 219530758,
    "start": 6177,
    "end": 6195,
    "title": "Evaluating the Disentanglement of Deep Generative Models through Manifold Topology",
    "abstract": "Learning disentangled representations is regarded as a fundamental task for improving the generalization, robustness, and interpretability of generative models. However, measuring disentanglement has been challenging and inconsistent, often dependent on an ad-hoc external model or specific to a certain dataset. To address this, we present a method for quantifying disentanglement that only uses the generative model, by measuring the topological similarity of conditional submanifolds in the learned representation. This method showcases both unsupervised and supervised variants. To illustrate the effectiveness and applicability of our method, we empirically evaluate several state-of-the-art models across multiple datasets. We find that our method ranks models similarly to existing methods.Real ShapePos-y Generated Interpolations GeneratedFigure 1: Factors in the dSprites dataset displaying topological similarity and semantic correspondence to respective latent dimensions in a disentangled generative model, as shown through Wasserstein RLT distributions of homology and latent interpolations along respective dimensions.",
    "prev": "Similarly, the encoder itself can also be a representation of this object since φ = arg min φ L(φ, x), i.e., φ is the output of an optimization procedure that takes the object's images as input.",
    "curr": "RELATED WORK\n\nObject-centric representation learning falls in the field of disentangled representation learning (Higgins et al., 2016;Kim & Mnih, 2018;Press et al., 2019;Chen et al., 2018b;Karras et al., 2019;Locatello et al., 2020a;Zhou et al., 2021).",
    "next": "However, object-centric representations require that the disentangled latents correspond to objects in the scene."
  },
  {
    "index": 2074,
    "source_corpus_id": 182952687,
    "ref_id": "b38",
    "citation_corpus_id": 3533333,
    "start": 5399,
    "end": 5416,
    "title": "Published as a conference paper at ICLR 2018 TRUNCATED HORIZON POLICY SEARCH: COMBINING REINFORCEMENT LEARNING & IMITATION LEARNING",
    "abstract": "In this paper, we propose to combine imitation and reinforcement learning via the idea of reward shaping using an oracle. We study the effectiveness of the nearoptimal cost-to-go oracle on the planning horizon and demonstrate that the costto-go oracle shortens the learner's planning horizon as function of its accuracy: a globally optimal oracle can shorten the planning horizon to one, leading to a onestep greedy Markov Decision Process which is much easier to optimize, while an oracle that is far away from the optimality requires planning over a longer horizon to achieve near-optimal performance. Hence our new insight bridges the gap and interpolates between imitation learning and reinforcement learning. Motivated by the above mentioned insights, we propose Truncated HORizon Policy Search (THOR), a method that focuses on searching for policies that maximize the total reshaped reward over a finite planning horizon when the oracle is sub-optimal. We experimentally demonstrate that a gradient-based implementation of THOR can achieve superior performance compared to RL baselines and IL baselines even when the oracle is sub-optimal.",
    "prev": "However, a demonstration can significantly narrow down the search space while also providing a practical means for a user to communicate the goal, enabling the agent to achieve few-shot learning of behavior.",
    "curr": "While the combination of demonstrations and reinforcement has been studied extensively in single task problems (Kober et al., 2013;Sun et al., 2018;Rajeswaran et al., 2018;, this combination is particularly important in meta-learning contexts where few-shot learning of new tasks is simply not possible without demonstrations.",
    "next": "Further, we can even significantly improve upon prior methods that study this combination using meta-learning to more effectively integrate the information coming from both sources."
  },
  {
    "index": 2076,
    "source_corpus_id": 3005102,
    "ref_id": "b6",
    "citation_corpus_id": 252796,
    "start": 17284,
    "end": 17305,
    "title": "Building a Large Annotated Corpus of English: The Penn Treebank",
    "abstract": "There is a growing consensus that significant, rapid progress can be made in both text understanding and spoken language understanding by investigating those phenomena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large corpora. Such corpora are beginning to serve as important research tools for investigators in natural language processing, speech recognition, and integrated spoken language systems, as well as in theoretical linguistics. Annotated corpora promise to be valuable for enterprises as diverse as the automatic construction of statistical models for the grammar of the written and the colloquial spoken language, the development of explicit formal theories of the differing grammars of writing and speech, the investigation of prosodic phenomena in speech, and the evaluation and comparison of the adequacy of parsing models.In this paper, we review our experience with constructing one such large annotated corpus--the Penn Treebank, a corpus 1 consisting of over 4.5 million words of American English. During the first three-year phase of the Penn Treebank Project (1989)(1990)(1991)(1992), this corpus has been annotated for part-of-speech (POS) information. In addition, over half of it has been annotated for skeletal syntactic structure. These materials are available to members of the Linguistic Data Consortium; for details, see Section 5.1.The paper is organized as follows. Section 2 discusses the POS tagging task. After outlining the considerations that informed the design of our POS tagset and presenting the tagset itself, we describe our two-stage tagging process, in which text is first assigned POS tags automatically and then corrected by human annotators. Section 3 briefly presents the results of a comparison between entirely manual and semi-automated tagging, with the latter being shown to be superior on three counts: speed, consistency, and accuracy. In Section 4, we turn to the bracketing task. Just as with the tagging task, we have partially automated the bracketing task: the output of",
    "prev": "EXPERIMENTS\n\nIn this section we show that despite its simplicity, the CFN network achieves performance comparable to the much more complex LSTM network on the word level language modeling task.",
    "curr": "We use two datasets for these experiments, namely the Penn Treebank corpus (Marcus et al., 1993) and the Text8 corpus (Mikolov et al., 2014).",
    "next": "We consider both one-layer and two-layer CFNs and LSTMs for our experiments."
  },
  {
    "index": 2077,
    "source_corpus_id": 256615740,
    "ref_id": "b23",
    "citation_corpus_id": 3526391,
    "start": 4658,
    "end": 4678,
    "title": "ENHANCING THE RELIABILITY OF OUT-OF-DISTRIBUTION IMAGE DETECTION IN NEURAL NETWORKS",
    "abstract": "We consider the problem of detecting out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between in-and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach (Hendrycks & Gimpel, 2017) by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet (applied to CIFAR-10 and Tiny-ImageNet) when the true positive rate is 95%.A seemingly straightforward approach of detecting out-of-distribution images is to enlarge the training set of both in-and out-of-distribution examples. However, the number of out-of-distribution examples can be infinitely many, making the re-training approach computationally expensive and intractable. Moreover, to ensure that a neural network accurately classifies in-distribution samples into correct classes while correctly detecting out-of-distribution samples, one might need to employ exceedingly large neural network architectures, which further complicates the training process.Hendrycks & Gimpel (2017) proposed a baseline method to detect out-of-distribution examples without further re-training networks. The method is based on an observation that a well-trained neural network tends to assign higher softmax scores to in-distribution examples than out-of-distribution examples. In this paper, we go further. We observe that after using temperature scaling in the softmax function(Hinton et al., 2015;Pereyra et al., 2017)  and adding small controlled perturbations to inputs, . Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. 2015.",
    "prev": "INTRODUCTION\n\nReal-world applications often require machine learning systems to interact with an open world, violating the common assumption that testing and training distributions are identical.",
    "curr": "This urges the community to devote increasing efforts on how to enhance models' generalization (Muandet et al., 2013) and reliability (Liang et al., 2018) w.r.t.",
    "next": "out-of-distribution (OOD) data."
  },
  {
    "index": 2079,
    "source_corpus_id": 231632806,
    "ref_id": "b39",
    "citation_corpus_id": 55481903,
    "start": 10830,
    "end": 10850,
    "title": "AN EMPIRICAL STUDY OF EXAMPLE FORGETTING DURING DEEP NEURAL NETWORK LEARNING",
    "abstract": "Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks. Our goal is to understand whether a related phenomenon occurs when data does not undergo a clear distributional shift. We define a \"forgetting event\" to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning. Across several benchmark data sets, we find that: (i) certain examples are forgotten with high frequency, and some not at all; (ii) a data set's (un)forgettable examples generalize across neural architectures; and (iii) based on forgetting dynamics, a significant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance.",
    "prev": "Other complementary works study how information about an input sample propagates through the network (Shwartz-Ziv & Tishby, 2017;Achille & Soatto, 2018;Saxe et al., 2019) or total amount of information (complexity) of a classification dataset (Lorena et al., 2019), rather than how much information the sample itself contains.",
    "curr": "In terms of applications, our work is related to works that estimate influence of an example (Koh & Liang, 2017;Toneva et al., 2019;Katharopoulos & Fleuret, 2018;Ghorbani & Zou, 2019;Yoon et al., 2019).",
    "next": "This can be done by estimating the change in weights if a sample is removed from the training set, which is addressed by several works (Koh & Liang, 2017;Golatkar et al., 2020;Wu et al., 2020)."
  },
  {
    "index": 2080,
    "source_corpus_id": 52954637,
    "ref_id": "b6",
    "citation_corpus_id": 3557557,
    "start": 2024,
    "end": 2044,
    "title": "Investigating Human Priors for Playing Video Games",
    "abstract": "What makes humans so good at solving seemingly complex video games? Unlike computers, humans bring in a great deal of prior knowledge about the world, enabling efficient decision making. This paper investigates the role of human priors for solving video games. Given a sample game, we conduct a series of ablation studies to quantify the importance of various priors on human performance. We do this by modifying the video game environment to systematically mask different types of visual information that could be used by humans as priors. We find that removal of some prior knowledge causes a drastic degradation in the speed with which human players solve the game, e.g. from 2 minutes to over 20 minutes. Furthermore, our results indicate that general priors, such as the importance of objects and visual consistency, are critical for efficient game-play. Videos and the game manipulations are available at https://rach0012. github.io/humanRL_website/.",
    "prev": "Introduction\n\nThe performance of machine learning methods generally depends on the choice of data representation (Bengio et al., 2013).",
    "curr": "In reinforcement learning (RL), the choice of state representation may affect generalization (Rafols et al., 2005), exploration (Tang et al., 2017;Pathak et al., 2017), and speed of learning (Dubey et al., 2018).",
    "next": "As a motivating example, consider goal-achieving tasks, a class of RL tasks which has recently received significant attention (Andrychowicz et al., 2017;Pong et al., 2018)."
  },
  {
    "index": 2081,
    "source_corpus_id": 252600018,
    "ref_id": "b9",
    "citation_corpus_id": 211171605,
    "start": 1322,
    "end": 1341,
    "title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages",
    "abstract": "We present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language code search, code documentation generation, etc. We develop Code-BERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both \"bimodal\" data of NL-PL pairs and \"unimodal\" data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NL-PL probing. 1",
    "prev": "We propose subtokenziation that reduces average length by 17% without downstream performance drop, and show that a carefully chosen subtokenization may improve quality by 0.5-2%, possibly with some length increase.",
    "curr": "INTRODUCTION\n\nWith the inspiration from the success of large language model (LM) pretraining in natural language processing (NLP), BERT-like models have been widely adopted for source code processing (Feng et al., 2020;Kanade et al., 2020), as code has a similar discrete sequential structure to natural text.",
    "next": "Being trained on huge source code corpora in a self-supervised manner, large LMs often substantially outperform domain-specific models developed purposely for applied tasks, especially in the tasks with limited parallel / labelled data (Ahmad et al., 2021a)."
  },
  {
    "index": 2084,
    "source_corpus_id": 49303347,
    "ref_id": "b8",
    "citation_corpus_id": 5590763,
    "start": 14348,
    "end": 14366,
    "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
    "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
    "prev": "For multimodal time series datasets, we choose the Memory Fusion Network (MFN) (Zadeh et al., 2018a) as our multimodal encoder Q(Z y X 1∶M ).",
    "curr": "We use Long Short-term Memory (LSTM) networks (Hochreiter & Schmidhuber, 1997) for functions Q(Z a{1∶M } X 1∶M ), decoder LSTM networks (Cho et al., 2014) for functions F 1∶M , and FCNNs for functions G y , G a{1∶M } and D. Details are provided in the appendix and the code is available at <anonymous>.",
    "next": "EXPERIMENTS\n\nIn order to show that MFM learns multimodal representations that are discriminative, generative and interpretable, we design the following experiments."
  },
  {
    "index": 2085,
    "source_corpus_id": 256105170,
    "ref_id": "b12",
    "citation_corpus_id": 3292002,
    "start": 3280,
    "end": 3304,
    "title": "GRAPH ATTENTION NETWORKS",
    "abstract": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-theart results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a proteinprotein interaction dataset (wherein test graphs remain unseen during training).",
    "prev": "( 2019), consider encoding the potential interactions between instance pairs, but this requires sufficient degrees of freedom that significantly increases learning difficulty from limited labels (Fatemi et al., 2021) and hinders the scalability to large systems (Wu et al., 2022b).",
    "curr": "Turning to a simpler problem setting where putative instance relations are instantiated as an observed graph, remarkable progress has been made in designing expressive architectures such as graph neural networks (GNNs) (Scarselli et al., 2008;Kipf & Welling, 2017;Velickovic et al., 2018;Wu et al., 2019;Chen et al., 2020a; for harnessing inter-connections between instances as a geometric prior (Bronstein et al., 2017).",
    "next": "However, the observed relations can be incomplete/noisy, due to error-prone data collection, or generated by an artificial construction independent from downstream targets."
  },
  {
    "index": 2089,
    "source_corpus_id": 263334596,
    "ref_id": "b7",
    "citation_corpus_id": 3144218,
    "start": 3753,
    "end": 3756,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "rs that can subsequently be utilized in different downstream tasks.In particular, MRL now underpins a variety of biochemical applications spanning molecular property prediction to the design of novel drug candidates [1][2][3].",
    "curr": "Traditional approaches often encode chemical compounds with fingerprints, such as extendedconnectivity fingerprints [4,5], which indicate the existence of certain substructures as binary bits in a fixed-length sequence.Such line-based representations are concise and efficient, but have limited expressive power and have difficulty in capturing 3D structural information such as bonding geometries and global shapes, which can be important for analyzing molecular properties and chemical reactivity [6,7].Recently, Graph Neural Networks (GNNs) have become an increasingly popular method of learning molecular representations by treating molecules as graph-structured objects.Existing GNN models for MRL can be broadly classified into two categories: 2D topological models [8][9][10][11] and 3D geometric models [12][13][14][15][16][17].2D GNNs typically model the molecular connectivity as a flat 2D      graph with atoms as nodes and bonds as edges, learning representations of chemical environments by iteratively passing messages between neighboring atoms.Although powerful in the absence of structural information, 2D GNNs may fail to capture key conformational effects or stereochemical properties like chirality [18,19], which is critical for modeling molecular interactions in areas such as drug design or chemical catalysis.Conversely, 3D GNNs are designed to model molecular conformers (conformations), which describe the structure of molecules in 3D space.Thus, these models have found widespread adoption for modeling electronic properties, predicting conformer energies and forces, and scoring interactions between ligands and proteins, amongst other applications.",
    "next": "Q i H I N 1 0 0 s Q M 1 T X i o 4 A L / U U K T d C 7 J 1 I c S j k O P c 0 M s R r K c i w H / x"
  },
  {
    "index": 2090,
    "source_corpus_id": 248986239,
    "ref_id": "b0",
    "citation_corpus_id": 222208634,
    "start": 25868,
    "end": 25889,
    "title": "Published as a conference paper at ICLR 2021 LEARNING TO RECOMBINE AND RESAMPLE DATA FOR COMPOSITIONAL GENERALIZATION",
    "abstract": "Flexible neural sequence models outperform grammar-and automaton-based counterparts on a variety of tasks. However, neural models perform poorly in settings requiring compositional generalization beyond the training data-particularly to rare or unseen subsequences. Past work has found symbolic scaffolding (e.g. grammars or automata) essential in these settings. We describe R&R, a learned data augmentation scheme that enables a large category of compositional generalizations without appeal to latent symbolic structure. R&R has two components: recombination of original training examples via a prototype-based generative model and resampling of generated examples to encourage extrapolation. Training an ordinary neural sequence model on a dataset augmented with recombined and resampled examples significantly improves generalization in two language processing problems-instruction following (SCAN) and morphological analysis (SIGMORPHON 2018)-where R&R enables learning of new constructions and tenses from as few as eight initial examples. . Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In",
    "prev": "While approaches with symbolic components are able to achieve 100% accuracy on SCAN (Chen et al., 2020;Liu et al., 2020;Nye et al., 2020;Shaw et al., 2021), they require complicated model training and grammar inference algorithms to search in a large grammar space.",
    "curr": "Another line of work on SCAN designs data augmentation schemes (Andreas, 2020;Akyürek et al., 2021;Lake, 2019).",
    "next": "Both Andreas (2020) and Akyürek et al."
  },
  {
    "index": 2094,
    "source_corpus_id": 232170254,
    "ref_id": "b22",
    "citation_corpus_id": 212874725,
    "start": 9172,
    "end": 9193,
    "title": "Black-box Off-policy Estimation for Infinite-Horizon Reinforcement Learning",
    "abstract": "Off-policy estimation for long-horizon problems is important in many real-life applications such as healthcare and robotics, where high-fidelity simulators may not be available and on-policy evaluation is expensive or impossible. Recently,[21]proposed an approach that avoids the curse of horizon suffered by typical importance-sampling-based methods. While showing promising results, this approach is limited in practice as it requires data be drawn from the stationary distribution of a known behavior policy. In this work, we propose a novel approach that eliminates such limitations. In particular, we formulate the problem as solving for the fixed point of a certain operator. Using tools from Reproducing Kernel Hilbert Spaces (RKHSs), we develop a new estimator that computes importance ratios of stationary distributions, without knowledge of how the off-policy data are collected. We analyze its asymptotic consistency and finite-sample generalization. Experiments on benchmarks verify the effectiveness of our approach.In this paper, we introduce a novel approach for the off-policy estimation problem that overcome these drawbacks. The main contributions of our work are three-fold:• We formulate the off-policy estimation problem into one of solving for the fixed point of an operator.Different from the related, and similar, Bellman operator that goes forward in time, this operator is backward in time.• We develop a new algorithm, which does not have the aforementioned limitations of[21], and analyze its generalization bounds. Specifically, the algorithm does not require that the off-policy data come from the stationary distribution, or that the behavior policy be known.• We empirically demonstrate the effectiveness of our method on several classic control benchmarks. In particular, we show that, unlike [21], our method is effective even if the off-policy data has not reached the stationary distribution.",
    "prev": "For example,D n can be either collected from a single MDP governed by an unknown, time-varying, non-Markovian behavior policy (in which case (s i , a i ) = (s i+1 , a i+1 )), or be a combination of many short MDP segments (in which case (s i , a i ) may not equal (s i+1 , a i+1 )).",
    "curr": "Note that this significantly relaxes the data assumptions in recent works (Liu et al., 2018a;Mousavi et al., 2020;, which require (s i , a i ) n i=1 to be independent or i.i.d.",
    "next": "Eq (3) is a correctness requirement of the confidence interval."
  },
  {
    "index": 2098,
    "source_corpus_id": 246430145,
    "ref_id": "b22",
    "citation_corpus_id": 22014305,
    "start": 2497,
    "end": 2520,
    "title": "ROUTING NETWORKS: ADAPTIVE SELECTION OF NON-LINEAR FUNCTIONS FOR MULTI-TASK LEARN- ING",
    "abstract": "Multi-task learning (MTL) with neural networks leverages commonalities in tasks to improve performance, but often suffers from task interference which reduces the benefits of transfer. To address this issue we introduce the routing network paradigm, a novel neural network and training algorithm. A routing network is a kind of self-organizing neural network consisting of two components: a router and a set of one or more function blocks. A function block may be any neural network -for example a fully-connected or a convolutional layer. Given an input the router makes a routing decision, choosing a function block to apply and passing the output back to the router recursively, terminating when a fixed recursion depth is reached. In this way the routing network dynamically composes different function blocks for each input. We employ a collaborative multi-agent reinforcement learning (MARL) approach to jointly train the router and function blocks. We evaluate our model against cross-stitch networks and shared-layer baselines on multi-task settings of the MNIST, mini-imagenet, and CIFAR-100 datasets. Our experiments demonstrate a significant improvement in accuracy, with sharper convergence. In addition, routing networks have nearly constant per-task training cost while cross-stitch networks scale linearly with the number of tasks. On CIFAR-100 (20 tasks) we obtain cross-stitch performance levels with an 85% reduction in training time.",
    "prev": "The memory for replay leads to a linear-increased space complexity with respect to the number of tasks.",
    "curr": "Expansionbased approaches select the network parameters dynamically (Yoon et al., 2018;Rosenbaum et al., 2018;Serra et al., 2018;, add additional components as new tasks arrive (Rusu et al., 2016;Fernando et al., 2017;Alet et al., 2018;Chang et al., 2019;, or use larger networks to generate network parameters (Aljundi et al., 2017;Yoon et al., 2019;von Oswald et al., 2019).",
    "next": "These methods reduce interference between tasks by additional task-specific parameters."
  },
  {
    "index": 2099,
    "source_corpus_id": 247011095,
    "ref_id": "b19",
    "citation_corpus_id": 28202810,
    "start": 16126,
    "end": 16149,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "If the learning agent visits human-preferable subspace insufficiently during free explorable sampling, the states evoking high proxy value are rarely encountered, making the back-propagation of the proxy value to preceding states difficult and thus damaging the learning.",
    "curr": "To encourage exploration, we adopt the entropy regularization technique in (Haarnoja et al., 2018) and forms auxiliary signal to update the proxy value function apart from Eq.",
    "next": "3:\nmin φ E (st,ât,st+1)∼B [y − Q(s t ,â t ; φ)] 2 , y = γ E a ∼πn(·|st+1) [Q(s t+1 , a ; φ ) − α log π n (a |s t+1 )],(4)\nwhereinâ t is the executed action at state s t , φ denotes the delay updated parameter of the target network, γ is the discount factor."
  },
  {
    "index": 2101,
    "source_corpus_id": 7772900,
    "ref_id": "b21",
    "citation_corpus_id": 16326763,
    "start": 4670,
    "end": 4693,
    "title": "CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING",
    "abstract": "We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies \"end-to-end\": directly from raw pixel inputs. * These authors contributed equally.",
    "prev": "In our experiments, we find that our method is more robust and offers substantial improvement over prior methods on partially observable tasks.",
    "curr": "arXiv:1710.11424v2 [cs.LG] 25 Oct 2018\n\n\nRelated Work\n\nDeep reinforcement learning algorithms have been demonstrated to achieve excellent results on a range of complex tasks, including playing games (Mnih et al., 2015;Oh et al., 2016) and continuous control (Schulman et al., 2015;Lillicrap et al., 2016;Levine et al., 2016).",
    "next": "Prior deep reinforcement learning algorithms either learn state or state-action value functions (Mnih et al., 2013), learn policies using policy gradients (Schulman et al., 2015), or perform a combination of the two using actor-critic architectures (Mnih et al., 2016)."
  },
  {
    "index": 2103,
    "source_corpus_id": 257279756,
    "ref_id": "b24",
    "citation_corpus_id": 3162051,
    "start": 34381,
    "end": 34401,
    "title": "mixup: BEYOND EMPIRICAL RISK MINIMIZATION",
    "abstract": "Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks. * Alphabetical order.",
    "prev": "• cRT, i.e., Classifier re-training, a two-stage training method that first pretrains the entire network with all the imbalanced training data and re-train the classifier with a balanced objective.",
    "curr": "• MixMatch, a SSL method which combines pseudo-labeling and consistency regularization techniques via Mixup augmentation (Zhang et al., 2018).",
    "next": "• FixMatch, a pseudo-labelling based SSL method of which the strongly augmented unlabeled samples (whose pseudo labels are generated from their weakly augmented versions) are used to train the network."
  },
  {
    "index": 2105,
    "source_corpus_id": 57825721,
    "ref_id": "b11",
    "citation_corpus_id": 2009318,
    "start": 1541,
    "end": 1566,
    "title": "NEURAL GPUS LEARN ALGORITHMS",
    "abstract": "Learning an algorithm from examples is a fundamental problem that has been widely studied. It has been addressed using neural networks too, in particular by Neural Turing Machines (NTMs). These are fully differentiable computers that use backpropagation to learn their own programming. Despite their appeal NTMs have a weakness that is caused by their sequential nature: they are not parallel and are are hard to train due to their large depth when unfolded. We present a neural network architecture to address this problem: the Neural GPU. It is based on a type of convolutional gated recurrent unit and, like the NTM, is computationally universal. Unlike the NTM, the Neural GPU is highly parallel which makes it easier to train and efficient to run. An essential property of algorithms is their ability to handle inputs of arbitrary size. We show that the Neural GPU can be trained on short instances of an algorithmic task and successfully generalize to long instances. We verified it on a number of tasks including long addition and long multiplication of numbers represented in binary. We train the Neural GPU on numbers with up-to 20 bits and observe no errors whatsoever while testing it, even on much longer numbers. To achieve these results we introduce a technique for training deep recurrent networks: parameter sharing relaxation. We also found a small amount of dropout and gradient noise to have a large positive effect on learning and generalization.",
    "prev": "Our study also reveals some minimal sets of elements needed to obtain these completeness results.",
    "curr": "INTRODUCTION\n\nThere is an increasing interest in designing neural network architectures capable of learning algorithms from examples (Graves et al., 2014;Grefenstette et al., 2015;Joulin & Mikolov, 2015;Kaiser & Sutskever, 2016;Kurach et al., 2016;Dehghani et al., 2018).",
    "next": "A key requirement for any such an architecture is thus to have the capacity of implementing arbitrary algorithms, that is, to be Turing complete."
  },
  {
    "index": 2107,
    "source_corpus_id": 52891543,
    "ref_id": "b32",
    "citation_corpus_id": 604334,
    "start": 1827,
    "end": 1849,
    "title": "Intriguing properties of neural networks",
    "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
    "prev": "INTRODUCTION\n\nDeep Neural Networks (DNNs) have been widely adopted in a variety of machine learning applications (Krizhevsky et al., 2012;Levine et al., 2016).",
    "curr": "However, recent work has demonstrated that DNNs are vulnerable to adversarial perturbations (Szegedy et al., 2014;Goodfellow et al., 2015).",
    "next": "An adversary can add negligible perturbations to inputs and generate adversarial examples to mislead DNNs, first found in image-based machine learning tasks (Goodfellow et al., 2015;Carlini & Wagner, 2017a;Liu et al., 2017;Chen et al., 2017b;a;Su et al., 2018)."
  },
  {
    "index": 2109,
    "source_corpus_id": 264128047,
    "ref_id": "b46",
    "citation_corpus_id": 247595263,
    "start": 2745,
    "end": 2764,
    "title": "SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS",
    "abstract": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",
    "prev": "by asking for the most probable answer to a question) or discriminatively (e.g.",
    "curr": "by presenting a (question, answer) pair and asking whether the answer is acceptable) and, these two procedures do not always produce consistent results: generative procedures may fail when probability mass is spread across multiple contradicting answers (Wang et al., 2022;Mitchell et al., 2022), while discriminative procedures may fail due to miscalibration (Han et al., 2022;Chen et al., 2022) or subtle dependence on question wording (Jiang et al., 2020).Given these noisy and often-conflicting signals, how should we distill out an LM's best guess at the truth?This paper presents an approach for reconciling generative and discriminative LM decoding procedures by formulating decoding as a signaling game (Lewis, 2008) that we call the CONSENSUS GAME.At a high level, this game features a GENERATOR agent that must communicate an abstract correct or incorrect value to a DISCRIMINATOR agent, but may only do so using a set of candidate natural language strings (Fig.",
    "next": "1).Intuitively, an effective strategy for this game (i.e."
  },
  {
    "index": 2111,
    "source_corpus_id": 58554701,
    "ref_id": "b14",
    "citation_corpus_id": 33513311,
    "start": 1372,
    "end": 1393,
    "title": "On the State of the Art of Evaluation in Neural Language Models",
    "abstract": "Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-theart results on language modelling benchmarks. However, these have been evaluated using differing code bases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.",
    "prev": "INTRODUCTION\n\nRecurrent neural networks (RNNs) are state of the art models in various language processing tasks.",
    "curr": "However, their performance heavily depends on proper regularization at training time (Melis et al., 2018b;Merity et al., 2018).",
    "next": "The two predominant approaches to regularize RNNs are dropout (randomly zeroing out neurons; Srivastava et al., 2014) and 2 regularization (applying 2 penalty to model parameters; Hoerl & Kennard, 1970)."
  },
  {
    "index": 2112,
    "source_corpus_id": 235417126,
    "ref_id": "b5",
    "citation_corpus_id": 222130583,
    "start": 2803,
    "end": 2822,
    "title": "ADAPTIVE UNIVERSAL GENERALIZED PAGERANK GRAPH NEURAL NETWORK",
    "abstract": "In many important graph data processing applications the acquired information includes both node features and observations of the graph topology. Graph neural networks (GNNs) are designed to exploit both sources of evidence but they do not optimally trade-off their utility and integrate them in a manner that is also universal. Here, universality refers to independence on homophily or heterophily graph assumptions. We address these issues by introducing a new Generalized PageRank (GPR) GNN architecture that adaptively learns the GPR weights so as to jointly optimize node feature and topological information extraction, regardless of the extent to which the node labels are homophilic or heterophilic. Learned GPR weights automatically adjust to the node label pattern, irrelevant on the type of initialization, and thereby guarantee excellent learning performance for label patterns that are usually hard to handle. Furthermore, they allow one to avoid feature over-smoothing, a process which renders feature information nondiscriminative, without requiring the network to be shallow. Our accompanying theoretical analysis of the GPR-GNN method is facilitated by novel synthetic benchmark datasets generated by the so-called contextual stochastic block model. We also compare the performance of our GNN architecture with that of several state-ofthe-art GNNs on the problem of node-classification, using well-known benchmark homophilic and heterophilic datasets. The results demonstrate that GPR-GNN offers significant performance improvement compared to existing techniques on both synthetic and benchmark data. Our implementation is available online.",
    "prev": "The node representations can then be utilized for downstream node classification or regression tasks.",
    "curr": "Due to this neighborhood aggregation mechanism, several existing works posit that many GNNs implicitly assume strong homophily and homophily is critical for GNNs to achieve strong performance on SSNC (Zhu et al., 2020b;a;Chien et al., 2021;Maurya et al., 2021;Halcrow et al., 2020;Lim et al., 2021).",
    "next": "In general, homophily describes the phenomenon that nodes tend to connect with \"similar\" or \"alike\" others."
  },
  {
    "index": 2113,
    "source_corpus_id": 257255341,
    "ref_id": "b25",
    "citation_corpus_id": 247519155,
    "start": 2691,
    "end": 2712,
    "title": "Modeling Dual Read/Write Paths for Simultaneous Machine Translation",
    "abstract": "Simultaneous machine translation (SiMT) outputs translation while reading source sentence and hence requires a policy to decide whether to wait for the next source word (READ) or generate a target word (WRITE), the actions of which form a read/write path. Although the read/write path is essential to SiMT performance, no direct supervision is given to the path in the existing methods. In this paper, we propose a method of dual-path SiMT which introduces duality constraints to direct the read/write path. According to duality constraints, the read/write path in source-totarget and target-to-source SiMT models can be mapped to each other. As a result, the two SiMT models can be optimized jointly by forcing their read/write paths to satisfy the mapping. Experiments on En↔Vi and De↔En tasks show that our method can outperform strong baselines under all latency.",
    "prev": "For the purpose of high-quality translation under low latency, SiMT model needs to learn when to start translating each target token (Gu et al., 2017), thereby making a wise decision between waiting for the next source token (i.e., READ action) and generating a target token (i.e., WRITE action) during the translation process.",
    "curr": "However, learning when to start translating target tokens is not trivial for a SiMT model, as the moments of starting translating always hide inside the model and we can only supervise the SiMT model with the observed target sequence (Zhang & Feng, 2022a).",
    "next": "Existing SiMT methods are divided into fixed and adaptive in deciding when to start translating."
  },
  {
    "index": 2115,
    "source_corpus_id": 236154781,
    "ref_id": "b52",
    "citation_corpus_id": 17127188,
    "start": 29887,
    "end": 29906,
    "title": "MULTI-SCALE CONTEXT AGGREGATION BY DILATED CONVOLUTIONS",
    "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction problems such as semantic segmentation are structurally different from image classification. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multiscale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.",
    "prev": "All models are trained for 300 epochs on 8 Tesla V100 GPUs with a total batch size of 1024.",
    "curr": "E1 = 4 L1 = 2 E1 = 4 L1 = 2 E1 = 8 L1 = 3 E1 = 8 L1 = 3 E1 = 4 L1 = 3 E1 = 4 L1 = 2 E1 = 4 L1 = 2 E1 = 4 L1 = 2 Stage 2 H 8 × W 8 Overlapping Patch Embedding S2 = 2 S2 = 2 C2 = 128 C2 = 192 C2 = 192 C2 = 256 CycleMLP Block E2 = 4 L2 = 2 E2 = 4 L2 = 3 E2 = 8 L2 = 4 E2 = 8 L2 = 8 E2 = 4 L2 = 4 E1 = 4 L1 = 2 E1 = 4 L1 = 2 E1 = 4 L1 = 2\nStage 3 \n\n\nF SAMPLING STRATEGIES\n\nWe explore more sampling strategies in this subsection, including random sampling and dilated sampling inspired by dilated convolution (Yu & Koltun, 2016;Chen et al., 2018) (as shown in Figure 6).",
    "next": "We also compare the dense sampling method with ours."
  },
  {
    "index": 2118,
    "source_corpus_id": 631460,
    "ref_id": "b19",
    "citation_corpus_id": 18868491,
    "start": 2688,
    "end": 2709,
    "title": "The Copiale Cipher*",
    "abstract": "The Copiale cipher is a 105-page enciphered book dated 1866. We describe the features of the book and the method by which we deciphered it.",
    "prev": "The implications for such a general framework would be far-reaching in the field of unsupervised translation, where each language can be treated as an enciphering of the other.",
    "curr": "The decoding of the Copiale cipher (Knight et al., 2011) stands as an excellent example of the potential for machine learning techniques to decode enciphered texts by treating the problem as language translation.",
    "next": "The CycleGAN (Zhu et al., 2017) architecture is extremely general and we demonstrate our adaptation, CipherGAN, is capable of cracking ciphers to an extremely high degree of accuracy."
  },
  {
    "index": 2119,
    "source_corpus_id": 203593945,
    "ref_id": "b13",
    "citation_corpus_id": 2134321,
    "start": 8935,
    "end": 8953,
    "title": "Published as a conference paper at ICLR 2016 DEEP COMPRESSION: COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING",
    "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce \"deep compression\", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35× to 49× without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9× to 13×; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35×, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49× from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3× to 4× layerwise speedup and 3× to 7× better energy efficiency.",
    "prev": "(2018) performs quantization of the neural network's weights and suggests a new training procedure to preserve the model accuracy after the quantization.",
    "curr": "These methods showed high compression rates, e.g., the compression rate of AlexNet can reach 35x with the combination of pruning, quantization, and Huffman coding (Han et al., 2016).",
    "next": "Nevertheless, strong provable worst-case analysis is noticeably absent for most weight pruning methods."
  },
  {
    "index": 2126,
    "source_corpus_id": 244954755,
    "ref_id": "b55",
    "citation_corpus_id": 204788663,
    "start": 2249,
    "end": 2271,
    "title": "VARIBAD: A VERY GOOD METHOD FOR BAYES-ADAPTIVE DEEP RL VIA META-LEARNING",
    "abstract": "Trading off exploration and exploitation in an unknown environment is key to maximising expected return during learning. A Bayes-optimal policy, which does so optimally, conditions its actions not only on the environment state but on the agent's uncertainty about the environment. Computing a Bayes-optimal policy is however intractable for all but the smallest tasks. In this paper, we introduce variational Bayes-Adaptive Deep RL (variBAD), a way to meta-learn to perform approximate inference in an unknown environment, and incorporate task uncertainty directly during action selection. In a grid-world domain, we illustrate how variBAD performs structured online exploration as a function of task uncertainty. We further evaluate variBAD on MuJoCo domains widely used in meta-RL and show that it achieves higher online return than existing methods. * . Robust and efficient transfer learning with hidden parameter markov decision processes. In Advances in neural information processing systems, 2017.",
    "prev": "Introduction\n\nMeta-reinforcement learning algorithms aim to address the sample complexity challenge of conventional reinforcement learning (RL) methods by learning to learn -utilizing the experience of solving prior tasks in order to solve new tasks more quickly.",
    "curr": "Such methods can be exceptionally powerful, learning to solve tasks that are structurally similar to the meta-training tasks with just a few dozen trials (Finn et al., 2017;Duan et al., 2016;Wang et al., 2016;Zintgraf et al., 2020).",
    "next": "However, prior work on meta-reinforcement learning is generally concerned with asymptotic meta-learning performance, or how well the meta-trained policy can adapt to a single new task at the end of a long meta-training period."
  },
  {
    "index": 2129,
    "source_corpus_id": 247656619,
    "ref_id": "b38",
    "citation_corpus_id": 3693512,
    "start": 6202,
    "end": 6221,
    "title": "LIFELONG LEARNING WITH DYNAMICALLY EXPAND- ABLE NETWORKS",
    "abstract": "We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets in lifelong learning scenarios on multiple public datasets, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch model with substantially fewer number of parameters.",
    "prev": "This has typically been accomplished by imposing data-driven regularization schemes that discourage parameters from deviating far from earlier tasks' solutions (Zenke et al., 2017;Li & Hoiem, 2017;Ritter et al., 2018), or by replaying real (Lopez-Paz & Ranzato, 2017;Chaudhry et al., 2019;Aljundi et al., 2019) or hallucinated (Achille et al., 2018;Rao et al., 2019;van de Ven et al., 2020) data from earlier tasks during the training of future tasks.",
    "curr": "Other methods have instead aimed at solving the problem of model saturation by increasing the model capacity (Yoon et al., 2018;Li et al., 2019;Rajasegaran et al., 2019).",
    "next": "A few works have addressed lifelong RL by following the regularization (Kirkpatrick et al., 2017) or replay (Isele & Cosgun, 2018;Rolnick et al., 2019) paradigms from the supervised setting, exacerbating the stability-plasticity tension."
  },
  {
    "index": 2131,
    "source_corpus_id": 259936996,
    "ref_id": "b34",
    "citation_corpus_id": 5034059,
    "start": 2168,
    "end": 2172,
    "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    "abstract": "For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusively tailored to a specific task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating and analyzing the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all our tasks yields better results than training a separate model for each task. However, the low absolute performance of our best model indicates the need for improved general NLU systems. son. 2013. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint 1312.3005.",
    "prev": "Under review.",
    "curr": "Introduction\n\nLarge language models (LLMs) have proven to be extremely capable of generating coherent and fluent text when provided with high-level prompts, performing excellently on popular benchmarks [35] and finding use in production systems such as GPT-4 [28].",
    "next": "Such capabilities have raised the bar for automated text generation, and allow us to explore more nuanced ways of utilizing LMs to produce text."
  },
  {
    "index": 2133,
    "source_corpus_id": 252762561,
    "ref_id": "b3",
    "citation_corpus_id": 53115163,
    "start": 12019,
    "end": 12038,
    "title": "EXPLORATION BY RANDOM NETWORK DISTILLATION",
    "abstract": "We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.",
    "prev": "Intrinsic motivation.",
    "curr": "There has been a line of research to incorporate intrinsic motivation (Burda et al., 2018, Pathak et al., 2017, Zhang et al., 2021 for improving exploration in RL.",
    "next": "Yet, such ideas have not been explored with GFlowNets because the current mathematical framework of GFlowNets only allows for terminal rewards, unlike the standard RL frameworks."
  },
  {
    "index": 2134,
    "source_corpus_id": 257632050,
    "ref_id": "b10",
    "citation_corpus_id": 52979229,
    "start": 2547,
    "end": 2551,
    "title": "Dynamic Channel Pruning: Feature Boosting and Suppression",
    "abstract": "Making deep convolutional neural networks more accurate typically comes at the cost of increased computational and memory resources. In this paper, we exploit the fact that the importance of features computed by convolutional layers is highly input-dependent, and propose feature boosting and suppression (FBS), a new method to predictively amplify salient convolutional channels and skip unimportant ones at run-time. FBS introduces small auxiliary connections to existing convolutional layers. In contrast to channel pruning methods which permanently remove channels, it preserves the full network structures and accelerates convolution by dynamically skipping unimportant input and output channels. FBS-augmented networks are trained with conventional stochastic gradient descent, making it readily available for many state-of-the-art CNNs. We compare FBS to a range of existing channel pruning and dynamic execution schemes and demonstrate large improvements on ImageNet classification. Experiments show that FBS can accelerate VGG-16 by 5× and improve the speed of ResNet-18 by 2×, both with less than 0.6% top-5 accuracy loss. * These authors contributed equally.",
    "prev": "This makes it possible to exit at a certain layer to reach a final verdict on classification, if the corresponding classifier is confident enough of its decision.",
    "curr": "Several other sub-techniques of conditional computation exist and have been well-studied, including layer skipping [14], channel skipping in convolutional neural networks [11], or reinforcement learning methods for input-dependent dropout policies [4].",
    "next": "Although there are many diverse methods [15], the general intuitions as to why conditional computation can improve the performance of neural networks remain the same: First, the computation units are chosen in an adaptive manner to process the features that are particular to the given input pattern."
  },
  {
    "index": 2135,
    "source_corpus_id": 222208650,
    "ref_id": "b7",
    "citation_corpus_id": 21529792,
    "start": 3916,
    "end": 3932,
    "title": "Published as a conference paper at ICLR 2018 LEARNING ROBUST REWARDS WITH ADVERSARIAL INVERSE REINFORCEMENT LEARNING",
    "abstract": "Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually specified reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose AIRL, a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation. We demonstrate that AIRL is able to recover reward functions that are robust to changes in dynamics, enabling us to learn policies even under significant variation in the environment seen during training. Our experiments show that AIRL greatly outperforms prior methods in these transfer settings.",
    "prev": "(2019) show that IRL in regularized MDPs -regularized IRL -does not contain such degenerate solutions due to the uniqueness of the optimal policy for regularized MDPs.",
    "curr": "Despite this, no analytical solutions of regularized IRL -other than maximum-Shannon-entropy IRL (MaxEntIRL) (Ziebart et al., 2008;Ziebart, 2010;Ho & Ermon, 2016;Finn et al., 2016a;Fu et al., 2018) -have been proposed.",
    "next": "Solutions of a functional form were proposed in Geist et al."
  },
  {
    "index": 2137,
    "source_corpus_id": 259076022,
    "ref_id": "b16",
    "citation_corpus_id": 235794968,
    "start": 1852,
    "end": 1870,
    "title": "Direct Speech-to-Speech Translation With Discrete Units",
    "abstract": "We present a direct speech-to-speech translation (S2ST) model that translates speech from one language to speech in another language without relying on intermediate text generation. We tackle the problem by first applying a self-supervised discrete speech encoder on the target speech and then training a sequenceto-sequence speech-to-unit translation (S2UT) model to predict the discrete representations of the target speech. When target text transcripts are available, we design a joint speech and text training framework that enables the model to generate dual modality output (speech and text) simultaneously in the same inference pass. Experiments on the Fisher Spanish-English dataset show that the proposed framework yields improvement of 6.7 BLEU compared with a baseline direct S2ST model that predicts spectrogram features. When trained without any text transcripts, our model performance is comparable to models that predict spectrograms and are trained with text supervision, showing the potential of our system for translation between unwritten languages 1 .",
    "prev": "Different from conventional cascade approach (Lavie et al., 1997;Baldridge, 2004;Nakamura et al., 2006), the direct approach (Jia et al., 2019(Jia et al., , 2022a has the advantages of low latency and simplified pipeline.",
    "curr": "Existing direct S2ST approaches can be further classified according to whether the model predicts continuous mel-spectrogram features (Dong et al., 2022) or discrete units (Lee et al., 2022).",
    "next": "Unit-based approach has become more popular due to several reasons: (1) It allows researchers to take advantage of existing NLP modeling techniques by treating acoustic unit as a new language."
  },
  {
    "index": 2139,
    "source_corpus_id": 263908842,
    "ref_id": "b30",
    "citation_corpus_id": 249712412,
    "start": 6832,
    "end": 6849,
    "title": "Published as a conference paper at ICLR 2023 EQUIVARIANT DESCRIPTOR FIELDS: SE(3)- EQUIVARIANT ENERGY-BASED MODELS FOR END- TO-END VISUAL ROBOTIC MANIPULATION LEARNING",
    "abstract": "End-to-end learning for visual robotic manipulation is known to suffer from sample inefficiency, requiring large numbers of demonstrations. The spatial rototranslation equivariance, or the SE(3)-equivariance can be exploited to improve the sample efficiency for learning robotic manipulation. In this paper, we present SE(3)-equivariant models for visual robotic manipulation from point clouds that can be trained fully end-to-end. By utilizing the representation theory of the Lie group, we construct novel SE(3)-equivariant energy-based models that allow highly sample efficient end-to-end learning. We show that our models can learn from scratch without prior knowledge and yet are highly sample efficient (5∼10 demonstrations are enough). Furthermore, we show that our models can generalize to tasks with (i) previously unseen target object poses, (ii) previously unseen target object instances of the category, and (iii) previously unseen visual distractors. We experiment with 6-DoF robotic manipulation tasks to validate our models' sample efficiency and generalizability. Codes are available at: https",
    "prev": "Leveraging Dense Correspondences.",
    "curr": "Dense correspondences have emerged as an effective implicit parameterization of actions and poses (Florence et al., 2018;Manuelli et al., 2022;Yen-Chen et al., 2022;Simeonov et al., 2022;2023;Sundaresan et al., 2020;Ryu et al., 2023).",
    "next": "Given dense correspondences in 2D (Florence et al., 2018;Manuelli et al., 2022;Sundaresan et al., 2020;Yen-Chen et al., 2022) of 3D (Simeonov et al., 2022;2023;Ryu et al., 2023) both object and manipulator poses may be inferred by solving for rigid transforms given correspondences."
  },
  {
    "index": 2140,
    "source_corpus_id": 259096014,
    "ref_id": "b11",
    "citation_corpus_id": 236912915,
    "start": 10917,
    "end": 10933,
    "title": "Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification",
    "abstract": "Tuning pre-trained language models (PLMs) with task-specific prompts has been a promising approach for text classification. Particularly, previous studies suggest that prompttuning has remarkable superiority in the lowdata scenario over the generic fine-tuning methods with extra classifiers. The core idea of prompt-tuning is to insert text pieces, i.e., template, to the input and transform a classification problem into a masked language modeling problem, where a crucial step is to construct a projection, i.e., verbalizer, between a label space and a label word space. A verbalizer is usually handcrafted or searched by gradient descent, which may lack coverage and bring considerable bias and high variances to the results. In this work, we focus on incorporating external knowledge into the verbalizer, forming a knowledgeable prompt-tuning (KPT), to improve and stabilize prompt-tuning. Specifically, we expand the label word space of the verbalizer using external knowledge bases (KBs) and refine the expanded label word space with the PLM itself before predicting with the expanded label word space. Extensive experiments on zero and few-shot text classification tasks demonstrate the effectiveness of knowledgeable prompt-tuning. Our source code is publicly available at https://github.com/ thunlp/KnowledgeablePromptTuning.",
    "prev": "In this paper, we make the first attempt to explore the different domain representations of the adapters with high-rank and low-rank embedding spaces.",
    "curr": "PARAMETER-EFFICIENT FINE-TUNING\n\nRecently, Parameter-Efficient Fine-Tuning (PEFT) has gained significant traction within the field of natural language processing (NLP) (Hu et al., 2021;Houlsby et al., 2019;Zaken et al., 2021;Hu et al., 2022;Gao et al., 2021;He et al., 2021;Vu et al., 2022;Qin et al., 2021).",
    "next": "Adapter-based models, a form of PEFT, have gained popularity in NLP."
  },
  {
    "index": 2141,
    "source_corpus_id": 214390104,
    "ref_id": "b10",
    "citation_corpus_id": 11202498,
    "start": 6287,
    "end": 6304,
    "title": "Knowledge Graph Embedding via Dynamic Mapping Matrix",
    "abstract": "Knowledge graphs are useful resources for numerous AI applications, but they are far from completeness. Previous work such as TransE, TransH and TransR/CTransR regard a relation as translation from head entity to tail entity and the CTransR achieves state-of-the-art performance. In this paper, we propose a more fine-grained model named TransD, which is an improvement of TransR/CTransR. In TransD, we use two vectors to represent a named symbol object (entity and relation). The first one represents the meaning of a(n) entity (relation), the other one is used to construct mapping matrix dynamically. Compared with TransR/CTransR, TransD not only considers the diversity of relations, but also entities. TransD has less parameters and has no matrix-vector multiplication operations, which makes it can be applied on large scale graphs. In Experiments, we evaluate our model on two typical tasks including triplets classification and link prediction. Evaluation results show that our approach outperforms stateof-the-art methods.",
    "prev": "For example, TransE (Bordes et al., 2013) models the score as a distance of the translated subject to an object representation.",
    "curr": "This method has lead to many variations (Ji et al., 2015;Nguyen et al., 2016;Wang et al., 2014), but is limited in the relation systems it can model (Kazemi & Poole, 2018) and does not lead to state of the art performances on current benchmarks.",
    "next": "Finally Schlichtkrull et al."
  },
  {
    "index": 2142,
    "source_corpus_id": 257427126,
    "ref_id": "b2",
    "citation_corpus_id": 3482308,
    "start": 13381,
    "end": 13401,
    "title": "Understanding Deep Neural Networks with Rectified Linear Units",
    "abstract": "In this paper we investigate the family of functions representable by deep neural networks (DNN) with rectified linear units (ReLU). We give the first-ever polynomial time (in the size of data) algorithm to train to global optimality a ReLU DNN with one hidden layer, assuming the input dimension and number of nodes of the network as fixed constants.We also improve on the known lower bounds on size (from exponential to super exponential) for approximating a ReLU deep net function by a shallower ReLU net. Our gap theorems hold for smoothly parametrized families of \"hard\" functions, contrary to countable, discrete families known in the literature. An example consequence of our gap theorems is the following: for every natural number k there exists a function representable by a ReLU DNN with k 2 hidden layers and total size k 3 , such that any ReLU DNN with at most k hidden layers will require at least 1 2 k k+1 − 1 total nodes.Finally, we construct a family of R n → R piecewise linear functions for n ≥ 2 (also smoothly parameterized), whose number of affine pieces scales exponentially with the dimension n at any fixed size and depth. To the best of our knowledge, such a construction with exponential dependence on n has not been achieved by previous families of \"hard\" functions in the neural nets literature. This construction utilizes the theory of zonotopes from polyhedral theory.",
    "prev": "Now, we formalize our intuition.",
    "curr": "We study the piecewise affine ReLU network model (Arora et al., 2018), covering a large group of deep models with ReLU activations, fully connected layers, convolutional layers, residual layers, etc.",
    "next": "Here, we consider the recursive definition of a L-layer ReLU network, following\nz (l) = h (l) (W (l−1) z (l−1) ) for l = 1, ."
  },
  {
    "index": 2144,
    "source_corpus_id": 247939701,
    "ref_id": "b19",
    "citation_corpus_id": 208547755,
    "start": 4056,
    "end": 4076,
    "title": "Published as a conference paper at ICLR 2020 DREAM TO CONTROL: LEARNING BEHAVIORS BY LATENT IMAGINATION",
    "abstract": "Learned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.",
    "prev": "MBRL is an enticing paradigm for scenarios in which samples of the true environment are difficult or expensive to obtain, such as computationally intensive simulators, real-world robots, or environments involving humans, since the model can be used to generalize the policy to unseen regions of the state space.",
    "curr": "The approach has received a lot of attention and significant progress has been made in the field (Sutton, 1990;Deisenroth & Rasmussen, 2011;Levine & Koltun, 2013;Hafner et al., 2020;Moerland et al., 2020;Schrittwieser et al., 2020).",
    "next": "One of the core problems of model-based policy learning methods, however, is that the accuracy of the model directly influences the quality of the learned policy or plan (Schneider, 1997;Kearns & Singh, 2002;Ross & Bagnell, 2012;Talvitie, 2017;Luo et al., 2019;Janner et al., 2019)."
  },
  {
    "index": 2145,
    "source_corpus_id": 258999763,
    "ref_id": "b35",
    "citation_corpus_id": 201646309,
    "start": 20021,
    "end": 20025,
    "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    "abstract": "BERT(Devlin et al., 2018)and RoBERTa (Liu  et al., 2019)  has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods. 1",
    "prev": "the concept distribution.We report the FID [19] score with respect to the concept images for each concept separately (FID per concept) and for all concepts as a dataset (FID entire set).",
    "curr": "(iii) We employ SentenceBERT [36] to measure the element diversity by estimating the dissimilarity of the tokens in the decomposition (Token diversity).This metric further substantiates the meaningfulness by showing that the decomposition is diverse.",
    "next": "Results\n\nThe results, averaged across all 188 concepts, are reported in Tab."
  },
  {
    "index": 2146,
    "source_corpus_id": 234487049,
    "ref_id": "b6",
    "citation_corpus_id": 7164502,
    "start": 21912,
    "end": 21936,
    "title": "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments",
    "abstract": "We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machineproduced translation and human-produced reference translations. Unigrams can be matched based on their surface forms, stemmed forms, and meanings; furthermore, METEOR can be easily extended to include more advanced matching strategies.Once all generalized unigram matches between the two strings have been found, METEOR computes a score for this matching using a combination of unigram-precision, unigram-recall, and a measure of fragmentation that is designed to directly capture how well-ordered the matched words in the machine translation are in relation to the reference. We evaluate METEOR by measuring the correlation between the metric scores and human judgments of translation quality. We compute the Pearson R correlation value between its scores and human quality assessments of the LDC TIDES 2003 Arabic-to-English and Chinese-to-English datasets.We perform segment-bysegment correlation, and show that METEOR gets an R correlation value of 0.347 on the Arabic data and 0.331 on the Chinese data. This is shown to be an improvement on using simply unigramprecision, unigram-recall and their harmonic F1 combination. We also perform experiments to show the relative contributions of the various mapping modules.",
    "prev": "For the retrieval augmentation, we use the training set as the retrieval database, i.e., D = D (see Step 1 in Section 2.2.2).",
    "curr": "For more details about data processing, please refer to Appendix D. Similar to previous works (Zhang et al., 2020;Wan et al., 2018;Fernandes et al., 2018;Iyer et al., 2016), BLEU (Papineni et al., 2002), METEOR (Banerjee & Lavie, 2005) and ROUGE-L (Lin, 2004) are used as our automatic evaluation metrics.",
    "next": "These metrics are popular for evaluating machine translation and text summarization tasks."
  },
  {
    "index": 2155,
    "source_corpus_id": 251719133,
    "ref_id": "b18",
    "citation_corpus_id": 238634325,
    "start": 5739,
    "end": 5763,
    "title": "OFFLINE REINFORCEMENT LEARNING WITH IMPLICIT Q-LEARNING",
    "abstract": "Offline reinforcement learning requires reconciling two conflicting aims: learning a policy that improves over the behavior policy that collected the dataset, while at the same time minimizing the deviation from the behavior policy so as to avoid errors due to distributional shift. This trade-off is critical, because most current offline reinforcement learning methods need to query the value of unseen actions during training to improve the policy, and therefore need to either constrain these actions to be in-distribution, or else regularize their values. We propose a new offline RL method that never needs to evaluate actions outside of the dataset, but still enables the learned policy to improve substantially over the best behavior in the data through generalization. The main insight in our work is that, instead of evaluating unseen actions from the latest policy, we can approximate the policy improvement step implicitly by treating the state value function as a random variable, with randomness determined by the action (while still integrating over the dynamics to avoid excessive optimism), and then taking a state conditional upper expectile of this random variable to estimate the value of the best actions in that state. This leverages the generalization capacity of the function approximator to estimate the value of the best available action at a given state without ever directly querying a Q-function with this unseen action. Our algorithm alternates between fitting this upper expectile value function and backing it up into a Q-function, without any explicit policy. Then, we extract the policy via advantage-weighted behavioral cloning, which also avoids querying out-of-sample actions. We dub our method implicit Q-learning (IQL). IQL is easy to implement, computationally efficient, and only requires fitting an additional critic with an asymmetric L2 loss. 1 IQL demonstrates the state-of-the-art performance on D4RL, a standard benchmark for offline reinforcement learning. We also demonstrate that IQL achieves strong performance fine-tuning using online interaction after offline initialization.",
    "prev": "Our results on low-dimensional locomotion control tasks show that TAP enjoys competitive performance as strong model-based, model-free actor-critic, and sequence modelling baselines.",
    "curr": "On tasks with higher dimensionality, TAP not only surpasses model-based methods like MOPO (Yu et al., 2020) Trajectory Transformer(TT) (Janner et al., 2021) but also significantly outperforms strong model-free ones (e.g., CQL and IQL (Kostrikov et al., 2022)).",
    "next": "In Figure 1(c), we show how the relative performance between TAP and baselines changes according to the dimensionality of the action space."
  },
  {
    "index": 2161,
    "source_corpus_id": 246996576,
    "ref_id": "b6",
    "citation_corpus_id": 3144218,
    "start": 2328,
    "end": 2350,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "embedding) aims to preserve the high-dimensional complex graph information involving node features and link structures in a low-dimensional embedding space, which requires effective feature selection and dimension reduction (Hamilton et al., 2017b).",
    "curr": "Graph neural networks (GNNs) have done great jobs to this end, but most of them rely on node labels from specific downstream tasks to be trained in a semi-supervised fashion (Kipf & Welling, 2017;Hamilton et al., 2017a;Wu et al., 2019;Veličković et al., 2018a;Klicpera et al., 2019;Chien et al., 2021).",
    "next": "However, similar to other domains, unsupervised representation learning is preferred in many cases, not only because labeled data is not always available (Hu et al., 2020;Xie et al., 2021), but also task-agnostic representations can better transfer and generalize among different scenarios (Erhan et al., 2010;Bengio, 2012;Radford et al., 2016)."
  },
  {
    "index": 2163,
    "source_corpus_id": 252693405,
    "ref_id": "b6",
    "citation_corpus_id": 247939224,
    "start": 8490,
    "end": 8509,
    "title": "Why Exposure Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation",
    "abstract": "Current language generation models suffer from issues such as repetition, incoherence, and hallucinations. An often-repeated hypothesis is that this brittleness of generation models is caused by the training and the generation procedure mismatch, also referred to as exposure bias. In this paper, we verify this hypothesis by analyzing exposure bias from an imitation learning perspective. We show that exposure bias leads to an accumulation of errors, analyze why perplexity fails to capture this accumulation, and empirically show that this accumulation results in poor generation quality.",
    "prev": "Algorithms such as Schedule Sampling (SS) (Bengio et al., 2015), Parallel SS (Duckworth et al., 2019), SS for Transformers (Mihaylova & Martins, 2019), Diffential SS (Goyal et al., 2017), LOLS (Lampouras & Vlachos, 2016;Chang et al., 2015), TextGAIL (Wu et al., 2021b), and SEARNN (Leblond et al., 2017), have been inspired by DAGGER (Ross et al., 2011) and SEARN (Daumé et al., 2009).",
    "curr": "However, these algorithms are known to suffer from exposure bias in generation (Chiang & Chen, 2021;Arora et al., 2022) and the cliff MDP problem (Huszár, 2015;Agarwal et al., 2019;Swamy et al., 2021).",
    "next": "RL for Large Action Spaces."
  },
  {
    "index": 2177,
    "source_corpus_id": 257279878,
    "ref_id": "b16",
    "citation_corpus_id": 204788776,
    "start": 9215,
    "end": 9233,
    "title": "A MUTUAL INFORMATION MAXIMIZATION PERSPEC- TIVE OF LANGUAGE REPRESENTATION LEARNING",
    "abstract": "We show state-of-the-art word representation learning methods maximize an objective function that is a lower bound on the mutual information between different parts of a word sequence (i.e., a sentence). Our formulation provides an alternative perspective that unifies classical word embedding models (e.g., Skip-gram) and modern contextual embeddings (e.g., BERT, XLNet). In addition to enhancing our theoretical understanding of these methods, our derivation leads to a principled framework that can be used to construct new self-supervised tasks. We provide an example by drawing inspirations from related methods based on mutual information maximization that have been successful in computer vision, and introduce a simple self-supervised objective that maximizes the mutual information between a global sentence representation and n-grams in the sentence. Our analysis offers a holistic view of representation learning methods to transfer knowledge and translate progress across multiple domains (e.g., natural language processing, computer vision, audio processing).",
    "prev": "(2)\nHere sim(·, ·) is the cosine similarity between two vectors, and τ is a temperature hyperparameter.",
    "curr": "Surged from InfoNCE (Oord et al., 2018), contrastive learning undergoes a rapid growth (Tian et al., 2019;Chen et al., 2020a;He et al., 2020;Wang et al., 2021) and has demonstrated state-of-theart performance on self-supervised tasks (Chen et al., 2020b;Kong et al., 2020).",
    "next": "Nevertheless, several works also point out contrastive learning is still vulnerable to adversarial attack when we transfer the learned features to the downstream classification (Ho & Nvasconcelos, 2020;Kim et al., 2020)."
  },
  {
    "index": 2178,
    "source_corpus_id": 219636414,
    "ref_id": "b0",
    "citation_corpus_id": 780171,
    "start": 2013,
    "end": 2016,
    "title": "Data-Driven Response Generation in Social Media",
    "abstract": "We present a data-driven approach to generating responses to Twitter status posts, based on phrase-based Statistical Machine Translation. We find that mapping conversational stimuli onto responses is more difficult than translating between languages, due to the wider range of possible responses, the larger fraction of unaligned words/phrases, and the presence of large phrase pairs whose alignment cannot be further decomposed. After addressing these challenges, we compare approaches based on SMT and Information Retrieval in a human evaluation. We show that SMT outperforms IR on this task, and its output is preferred over actual human responses in 15% of cases. As far as we are aware, this is the first work to investigate the use of phrase-based SMT to directly translate a linguistic stimulus into an appropriate response.",
    "prev": "Introduction\n\nDesigning a task-oriented dialogue system is a popular and challenging research topic in the recent decades.",
    "curr": "In contrast to open-domain dialogue systems [1], it aims to help people complete real-life tasks through dialogues without human service (e.g., booking tickets) [2].",
    "next": "In a task-oriented dialogue task, each dialogue is defined with a goal which includes user requests (i.e., represented as a set of key words known as slot values)."
  },
  {
    "index": 2179,
    "source_corpus_id": 15280949,
    "ref_id": "b8",
    "citation_corpus_id": 2937095,
    "start": 2467,
    "end": 2486,
    "title": "Learning Distributed Representations of Sentences from Unlabelled Data",
    "abstract": "Unsupervised methods for learning distributed representations of words are ubiquitous in today's NLP research, but far less is known about the best ways to learn distributed phrase or sentence representations from unlabelled data. This paper is a systematic comparison of models that learn such representations. We find that the optimal approach depends critically on the intended application. Deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-bilinear models work best for building representation spaces that can be decoded with simple spatial distance metrics. We also propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance.",
    "prev": "Those methods generally fall into two categories.",
    "curr": "The first consists of universal sentence embeddings usually trained by unsupervised learning (Hill et al., 2016).",
    "next": "This includes SkipThought vectors , ParagraphVector (Le & Mikolov, 2014), recursive auto-encoders (Socher et al., 2011;2013), Sequential Denoising Autoencoders (SDAE), FastSent (Hill et al., 2016), etc."
  },
  {
    "index": 2184,
    "source_corpus_id": 263671662,
    "ref_id": "b55",
    "citation_corpus_id": 227209335,
    "start": 3571,
    "end": 3574,
    "title": "SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS",
    "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reversetime SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in diffusion probabilistic modeling and scorebased generative modeling, and allows for new sampling procedures. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, which enables exact likelihood computation, and improved sampling efficiency. In addition, our framework enables conditional generation with an unconditional model, as we demonstrate with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high fidelity generation of 1024ˆ1024 images for the first time from a score-based generative model.",
    "prev": "Nowadays, generative models like (Wasserstein) GANs [10,24] and VAEs [36] have turned out to be a suitable tool for approximating probability distributions.In this context, the field of gradient flows in measure spaces received increasing attention.",
    "curr": "[67] proposed to apply the Langevin dynamics in order to generate samples from a known potential, which corresponds to simulating a Wasserstein gradient flow with respect to the Kullback-Leibler (KL) divergence, see [33].Scorebased and diffusion models extend this approach by estimating the gradients of the potential from training data, see [19,31,55,56] and achieved state-of-the-art results.The simulation of Wasserstein gradient flows with other functionals than KL, based on the JKO scheme, was considered in [4,5,22,48].",
    "next": "In this paper, we focus on gradient flows with respect to MMD with negative distance kernel K(x, y) = −∥x − y∥, which is also known as energy distance, see [54,61,63,64].While MMDs have shown great success at comparing two distributions in general, see [25,26,62], their combination with the negative distance kernel results in many additional desirable properties as translation and scale equivariance [64], efficient computation [30], and a good sample complexity [26]."
  },
  {
    "index": 2185,
    "source_corpus_id": 52908831,
    "ref_id": "b3",
    "citation_corpus_id": 13995862,
    "start": 1567,
    "end": 1586,
    "title": "NICE: Non-linear Independent Components Estimation",
    "abstract": "We propose a deep learning framework for modeling complex high-dimensional densities via Nonlinear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the determinant of the Jacobian and inverse Jacobian is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable, and unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting. * Yoshua Bengio is a CIFAR Senior Fellow.",
    "prev": "Reversible generative models use cheaply invertible neural networks to transform samples from a fixed base distribution.",
    "curr": "Examples include NICE (Dinh et al., 2014), Real NVP (Dinh et al., 2017), and Glow (Kingma & Dhariwal, 2018).",
    "next": "These models are easy to sample from, and can be trained by maximum likelihood using the change of variables formula."
  },
  {
    "index": 2189,
    "source_corpus_id": 257912616,
    "ref_id": "b8",
    "citation_corpus_id": 257365083,
    "start": 2597,
    "end": 2617,
    "title": "LEARNING TO REASON OVER VISUAL OBJECTS",
    "abstract": "A core component of human intelligence is the ability to identify abstract patterns inherent in complex, high-dimensional perceptual data, as exemplified by visual reasoning tasks such as Raven's Progressive Matrices (RPM).Motivated by the goal of designing AI systems with this capacity, recent work has focused on evaluating whether neural networks can learn to solve RPM-like problems.Previous work has generally found that strong performance on these problems requires the incorporation of inductive biases that are specific to the RPM problem format, raising the question of whether such models might be more broadly useful.Here, we investigated the extent to which a general-purpose mechanism for processing visual scenes in terms of objects might help promote abstract visual reasoning.We found that a simple model, consisting only of an object-centric encoder and a transformer reasoning module, achieved state-of-the-art results on both of two challenging RPM-like benchmarks (PGM and I-RAVEN), as well as a novel benchmark with greater visual complexity (CLEVR-Matrices).These results suggest that an inductive bias for object-centric processing may be a key component of abstract visual reasoning, obviating the need for problem-specific inductive biases.",
    "prev": "Introduction\n\nThe ability to infer and process relations and reason in terms of analogies lies at the heart of human abilities for abstraction and creative thinking (Holyoak, 2012;Snow et al., 1984).This capability is largely separate from our ability to acquire semantic and procedural knowledge through sensory tasks, such as image and audio processing.Modern deep learning systems can often capture this latter type of intelligence through efficient function approximation.However, deep learning has seen limited success with relational and abstract reasoning, which requires identifying novel associations from limited data and generalizing to new domains.",
    "curr": "Recognizing the importance of this capability, machine learning research has explored several novel frameworks for relational learning (Barrett et al., 2018;Battaglia et al., 2018;Graves et al., 2014;Mondal et al., 2023;Pritzel et al., 2017;Santoro et al., 2017;Shanahan et al., 2020;Webb et al., 2021;Whittington et al., 2020).In this paper we propose a framework that casts relational learning in terms of Transformers.The success of Transformers lies in the use of attentional mechanisms to support richly context-sensitive processing (Kerg et al., 2020;Vaswani et al., 2017;Wolf et al., 2020).However, it is clear that Transformers are missing core capabilities required for modeling human thought (Mahowald et al., 2023), including an ability to support analogy and abstraction.While large language models show a surprising ability to complete some analogies (Webb et al., 2022), this ability emerges implicitly only after processing vast amounts of data.",
    "next": "The Transformer architecture has the ability to model relations between objects implicitly through its attention mechanisms.However, we argue in this paper that standard attention produces en-tangled representations encoding a mixture of relational informati"
  },
  {
    "index": 2191,
    "source_corpus_id": 253098210,
    "ref_id": "b16",
    "citation_corpus_id": 28202810,
    "start": 35831,
    "end": 35854,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "Gym MuJoCo.",
    "curr": "We train SAC (Haarnoja et al., 2018) policies on the original set of Gym MuJoCo environments for 100M steps.",
    "next": "To simulate stochasticity in these environments, we modify the original Gym MuJoCo environments by introducing noise to the actions before inputting the action to the physics simulator to compute rewards and next states."
  },
  {
    "index": 2192,
    "source_corpus_id": 248006483,
    "ref_id": "b92",
    "citation_corpus_id": 226964491,
    "start": 6933,
    "end": 6952,
    "title": "PRE-TRAINING TEXT-TO-TEXT TRANSFORMERS FOR CONCEPT-CENTRIC COMMON SENSE",
    "abstract": "Pre-trained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational commonsense knowledge about everyday concepts, which is crucial to many downstream tasks that need common sense to understand or generate. To augment PTLMs with concept-centric commonsense knowledge, in this paper, we propose both generative and contrastive objectives for learning common sense from the text, and use them as intermediate self-supervised learning tasks for incrementally pre-training PTLMs (before task-specific fine-tuning on downstream datasets). Furthermore, we develop a joint pre-training framework to unify generative and contrastive objectives so that they can mutually reinforce each other. Extensive experimental results show that our method, concept-aware language model (CALM) 1 , can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge graphs, yielding better performance on both NLU and NLG tasks. We show that while only incrementally pre-trained on a relatively small corpus for a few steps, CALM outperforms baseline methods by a consistent margin and even comparable with some larger PTLMs, which suggests that CALM can serve as a general, \"plug-and-play\" method for improving the commonsense reasoning ability of a PTLM. * Equal contribution. The work was done when Wangchunshu was visiting USC. 1 Our code and data will be made public at: https://github.com/INK-USC/CALM arXiv:2011.07956v1 [cs.CL] 24 Oct 2020 Preprint. Work in progress.knowledge, prior works mainly focus on training larger models(Brown et al., 2020), adding specific architectures to exploit external knowledge(Peters et al., 2019), or incorporating knowledge bases for pre-training . In this paper, we instead look to explicitly teach pre-trained models to write and reason with common concepts through novel pre-training strategies. We present two kinds of self-supervised pre-training tasks: concept-to-sentence generation (C2S) and concept order recovering (COR). C2S trains the pre-trained model to compose (\"write\") sentences given a set of concepts, and expects the generated sentences to be fluent and plausible in terms of commonsense. COR aims to teach models to detect and revise a corrupted sentence with incorrect ordering of concepts. As illustrated inFigure 1, both tasks require a pre-trained model to recall relevant commonsense facts about the concepts and to understand the underlying commonsense relations between them. Both of the proposed objectives can explicitly encourage the model to capture the relational concept-centric commonsense knowledge and perform compositional reasoning.",
    "prev": "(Kassner & Schütze, 2020), and being unable to compare commonsense concepts, such as time (Qin et al., 2021) and digits (Talmor et al., 2020).",
    "curr": "To enhance the performance of LMs on knowledge-intensive NLG tasks 1 , prior studies have proposed to re-train LMs with knowledge-aware objectives (Zhou et al., 2021;Xiong et al., 2020;Khandelwal et al., 2020) or add special architectures to encode knowledge (Bosselut et al., 2019;Logan et al., 2019;Peters et al., 2019b) from external resources (e.g., knowledge graphs such as CONCEPTNET (Speer et al., 2017) and ATOMIC ).",
    "next": "These methods, though yielding impressive results on many downstream tasks, can be computationally expensive."
  },
  {
    "index": 2193,
    "source_corpus_id": 252692968,
    "ref_id": "b5",
    "citation_corpus_id": 26501419,
    "start": 1790,
    "end": 1810,
    "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
    "abstract": "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K questionanswer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a featurebased classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that Trivi-aQA is a challenging testbed that is worth significant future study. 1",
    "prev": "INTRODUCTION\n\nLarge language models (LLMs) have achieved impressive in-context few-shot performance on knowledge-intensive NLP tasks (Brown et al., 2020;Rae et al., 2021;Hoffmann et al., 2022;Chowdhery et al., 2022).",
    "curr": "For example, in open-domain question answering (Chen et al., 2017), demonstrated by only a few examples of question-answer pairs, LLMs are able to answer arbitrary factoid questions (Joshi et al., 2017;Yang et al., 2018;Kwiatkowski et al., 2019).",
    "next": "Recent research (Guu et al., 2020;Lewis et al., 2020;Izacard et al., 2022) shows that retrieval-augmentation can further improve LLMs' performance on knowledge-intensive tasks by conditioning the LLMs on retrieved relevant passages from an external corpus."
  },
  {
    "index": 2195,
    "source_corpus_id": 252762275,
    "ref_id": "b6",
    "citation_corpus_id": 560565,
    "start": 3205,
    "end": 3225,
    "title": "Solving General Arithmetic Word Problems",
    "abstract": "This paper presents a novel approach to automatically solving arithmetic word problems. This is the first algorithmic approach that can handle arithmetic problems with multiple steps and operations, without depending on additional annotations or predefined templates. We develop a theory for expression trees that can be used to represent and evaluate the target arithmetic expressions; we use it to uniquely decompose the target arithmetic problem to multiple classification problems; we then compose an expression tree, combining these with world knowledge through a constrained inference framework. Our classifiers gain from the use of quantity schemas that supports better extraction of features. Experimental results show that our method outperforms existing systems, achieving state of the art performance on benchmark datasets of arithmetic word problems.",
    "prev": "Specifically, the hand-drafting involves nontrivial efforts in designs of both questions and their reasoning chains for demonstrations.",
    "curr": "Moreover, human efforts for designing task-specific demonstrations are even more: different tasks, such as arithmetic [Roy and Roth, 2015] and commonsense reasoning [Talmor et al., 2019], require different ways of demonstrations.",
    "next": "To eliminate such manual designs, we advocate another Auto-CoT paradigm to automatically construct demonstrations with questions and reasoning chains."
  },
  {
    "index": 2196,
    "source_corpus_id": 202749994,
    "ref_id": "b5",
    "citation_corpus_id": 13046179,
    "start": 2061,
    "end": 2087,
    "title": "A BASELINE FOR DETECTING MISCLASSIFIED AND OUT-OF-DISTRIBUTION EXAMPLES IN NEURAL NETWORKS",
    "abstract": "We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.Published as a conference paper at ICLR 2017 one method which outperforms the baseline on some (but not all) tasks. This new method evaluates the quality of a neural network's input reconstruction to determine if an example is abnormal.",
    "prev": "This is particularly necessary for deep neural network classifiers, which can be easily fooled by OOD data (Nguyen et al., 2015).",
    "curr": "Several approaches have been proposed for OOD detection on top of or within a neural network classifier (Hendrycks & Gimpel, 2017;Lakshminarayanan et al., 2017;Liang et al., 2018;Lee et al., 2018).",
    "next": "Nonetheless, OOD detection is not limited to classification tasks nor to labeled data sets."
  },
  {
    "index": 2197,
    "source_corpus_id": 238856644,
    "ref_id": "b61",
    "citation_corpus_id": 211132990,
    "start": 7324,
    "end": 7341,
    "title": "BATCHENSEMBLE: AN ALTERNATIVE APPROACH TO EFFICIENT ENSEMBLE AND LIFELONG LEARNING",
    "abstract": "Ensembles, where multiple neural networks are trained individually and their predictions are averaged, have been shown to be widely successful for improving both the accuracy and predictive uncertainty of single neural networks. However, an ensemble's cost for both training and testing increases linearly with the number of networks, which quickly becomes untenable. In this paper, we propose BatchEnsemble 1 , an ensemble method whose computational and memory costs are significantly lower than typical ensembles. BatchEnsemble achieves this by defining each weight matrix to be the Hadamard product of a shared weight among all ensemble members and a rank-one matrix per member. Unlike ensembles, BatchEnsemble is not only parallelizable across devices, where one device trains one member, but also parallelizable within a device, where multiple ensemble members are updated simultaneously for a given mini-batch. Across CIFAR-10, CIFAR-100, WMT14 EN-DE/EN-FR translation, and out-of-distribution tasks, BatchEnsemble yields competitive accuracy and uncertainties as typical ensembles; the speedup at test time is 3X and memory reduction is 3X at an ensemble of size 4. We also apply BatchEnsemble to lifelong learning, where on Split-CIFAR-100, BatchEnsemble yields comparable performance to progressive neural networks while having a much lower computational and memory costs. We further show that BatchEnsemble can easily scale up to lifelong learning on Split-ImageNet which involves 100 sequential learning tasks. * Partial work done as part of the Google Student Researcher Program.",
    "prev": "• Our second, light-weight method ( EDST Ensemble) yields many free tickets in one single run, which is more efficient to train and test than a single dense model, while approaching the performance of the traditional dense ensemble.",
    "curr": "INTRODUCTION\n\nEnsembles (Hansen & Salamon, 1990;Levin et al., 1990) of neural networks have received large success in terms of the predictive accuracy (Perrone & Cooper, 1992;Breiman, 1996;Dietterich, 2000;Xie et al., 2013), uncertainty estimation (Fort et al., 2019;Lakshminarayanan et al., 2017;Wen et al., 2020;Havasi et al., 2021), and out-of-distribution robustness (Ovadia et al., 2019a;Gustafsson et al., 2020).",
    "next": "Given the fact that there are a wide variety of local minima solutions located in the high dimensional optimization landscape of deep neural networks and various randomness (e.g., random initialization, random mini-batch shuffling) occurring during training, neural networks trained with different random seeds usually converge to different low-loss basins with similar error rates (Fort et al., 2019;Ge et al., 2015;Kawaguchi, 2016;Wen et al., 2019)."
  },
  {
    "index": 2199,
    "source_corpus_id": 234762955,
    "ref_id": "b5",
    "citation_corpus_id": 52889459,
    "start": 2903,
    "end": 2906,
    "title": "LARGE SCALE GAN TRAINING FOR HIGH FIDELITY NATURAL IMAGE SYNTHESIS",
    "abstract": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by truncating the latent space. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128×128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.3 and Fréchet Inception Distance (FID) of 9.6, improving over the previous best IS of 52.52 and FID of 18.65.",
    "prev": "Gaussian distribution.",
    "curr": "Due to this structure, in some cases code dimensions acquire specific meanings which can be related to human-interpretable concepts (e.g., the rotation or size of an object); however, the code space in high-quality generators (e.g., BigGAN [6], BigBiGANs [11], StyleGAN [22]) is usually not easily in-terpretable.",
    "next": "Nonetheless, it is intuitive that an efficient generative process should account for the structure of natural images."
  },
  {
    "index": 2200,
    "source_corpus_id": 238419231,
    "ref_id": "b18",
    "citation_corpus_id": 189898036,
    "start": 2120,
    "end": 2138,
    "title": "Gradient Descent Maximizes the Margin of Homogeneous Neural Networks",
    "abstract": "Recent works on implicit regularization have shown that gradient descent converges to the max-margin direction for logistic regression with one-layer or multi-layer linear networks. In this paper, we generalize this result to homogeneous neural networks, including fully-connected and convolutional neural networks with ReLU or LeakyReLU activations. In particular, we study the gradient flow (gradient descent with infinitesimal step size) optimizing the logistic loss or cross-entropy loss of any homogeneous model (possibly non-smooth), and show that if the training loss decreases below a certain threshold, then we can define a smoothed version of the normalized margin which increases over time. We also formulate a natural constrained optimization problem related to margin maximization, and prove that both the normalized margin and its smoothed version converge to the objective value at a KKT point of the optimization problem. Furthermore, we extend the above results to a large family of loss functions. We conduct several experiments to justify our theoretical finding on MNIST and CIFAR-10 datasets. For gradient descent with constant learning rate, we observe that the normalized margin indeed keeps increasing after the dataset is fitted, but the speed is very slow. However, if we schedule the learning rate more carefully, we can observe a more rapid growth of the normalized margin. Finally, as margin is closely related to robustness, we discuss potential benefits of training longer for improving the robustness of the model. model fits the training data perfectly). For example, given a Convolutional Neural Network (CNN) that has achieved 100% training accuracy, one can easily make the cross-entropy loss arbitrarily small by scaling up the weight and bias parameters (W , b) at the last layer, i.e., transforming (W , b) to (cW , cb) for large enough c > 0. This means that, similar to linear logistic regression, CNNs also have some parameters whose scale does not matter, and hence a promising and meaningful research direction is to study whether their convergent direction maximizes the margin. In general, we observe that the following three properties that are usually satisfied by modern deep neural networks:1. Partial Homogeneity. The output of the neural network is (positively) homogeneous with respect to a part of its parameters (e.g., the parameters at the last linear layer);2. Separability. The training set is separable by the neural network for some set of parameters, i.e., the neural network has sufficient representation power to achieve 100% training accuracy (this is true for state-of-the-art CNNs for image classification, and many of them even have enough capacity to fit randomly labeled data easily [Zhang et al., 2017]);3. No finite minima on the loss function. The loss function used to measure the similarity between the network output and ground-truth is lower bounded by a constant (e.g., 0) but it does not have finite minima (e.g., exponential loss, logistic loss, cross-entropy loss).Simplifications. For simplicity and ease of presentation, we make the following simplifications. First, as the most prominent examples of homogeneous neural networks are all non-smooth (e.g., ReLU networks), we turn to analyze the case of training neural networks by gradient flow (more precisely, subgradient flow in Clarke's sense).Second, we ensure Separability as follows: we assume that after time t 0 , the training loss is smaller than a threshold, and the threshold here is chosen to be so small that the training accuracy is guaranteed to be 100% (e.g., for the logistic loss and cross-entropy loss, the threshold can be set to ln 2). In this paper, we focus on analyzing the behavior of the network after t 0 .",
    "prev": "Under certain regimes, the size of the neural networks used in practice is so large that the training data is fit perfectly and an infinite-width approximation is appropriate.",
    "curr": "In this setting, what matters to obtain good generalization is to fit the data using the right inductive bias, which is specified by how network parameters are controlled [Wei et al., 2020] together with the training algorithm used [Lyu and Li, 2020].",
    "next": "The infinite-width two-layer neural network model has been studied from several perspectives due to its simplicity."
  },
  {
    "index": 2201,
    "source_corpus_id": 256275098,
    "ref_id": "b13",
    "citation_corpus_id": 213987212,
    "start": 11192,
    "end": 11211,
    "title": "Published as a conference paper at ICLR 2020 BRIDGING MODE CONNECTIVITY IN LOSS LANDSCAPES AND ADVERSARIAL ROBUSTNESS",
    "abstract": "Mode connectivity provides novel geometric insights on analyzing loss landscapes and enables building high-accuracy pathways between well-trained neural networks. In this work, we propose to employ mode connectivity in loss landscapes to study the adversarial robustness of deep neural networks, and provide novel methods for improving this robustness. Our experiments cover various types of adversarial attacks applied to different network architectures and datasets. When network models are tampered with backdoor or error-injection attacks, our results demonstrate that the path connection learned using limited amount of bonafide data can effectively mitigate adversarial effects while maintaining the original accuracy on clean data. Therefore, mode connectivity provides users with the power to repair backdoored or error-injected models. We also use mode connectivity to investigate the loss landscapes of regular and robust models against evasion attacks. Experiments show that there exists a barrier in adversarial robustness loss on the path connecting regular and adversarially-trained models. A high correlation is observed between the adversarial robustness loss and the largest eigenvalue of the input Hessian matrix, for which theoretical justifications are provided. Our results suggest that mode connectivity offers a holistic tool and practical means for evaluating and improving adversarial robustness 1 .",
    "prev": "Trigger recovery aims to reverse engineer the trigger pattern (Wang et al., 2019;Guo et al., 2019;Liu et al., 2019;Sun et al., 2020;Liu et al., 2022;Xiang et al., 2022;Hu et al., 2022).",
    "curr": "Backdoor model detection aims to determine if a model is affected by triggers (Chen et al., 2019;Kolouri et al., 2020;Wang et al., 2020;Guo et al., 2021;Shen et al., 2021;Xu et al., 2021), It is worth noting that the detected models still need mitigation methods to remove the trigger (Liu et al., 2018a;Zhao et al., 2020a;Wu & Wang, 2021;Li et al., 2021b;Zeng et al., 2022;Guan et al., 2022).",
    "next": "Backdoor mitigation can also be achieved by robust learning strategies (Borgnia et al., 2021;Huang et al., 2022;Dolatabadi et al., 2022)."
  },
  {
    "index": 2203,
    "source_corpus_id": 258865597,
    "ref_id": "b43",
    "citation_corpus_id": 13807351,
    "start": 10005,
    "end": 10029,
    "title": "ENTROPY-SGD: BIASING GRADIENT DESCENT INTO WIDE VALLEYS",
    "abstract": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.",
    "prev": "The notion of the loss landscape sharpness and its connection to generalization has been extensively studied, both empirically [Keskar et al., 2016, Jiang et al., 2019, Neyshabur et al., 2017a, Dinh et al., 2017 and theoretically [McAllester, 1999, Dziugaite and Roy, 2017, Neyshabur et al., 2017b.",
    "curr": "These studies have motivated the development of methods [Hochreiter and Schmidhuber, 1997, Mobahi, 2016, Izmailov et al., 2018, Chaudhari et al., 2019 that aim to improve model generalization by manipulating or penalizing sharpness.",
    "next": "Among these methods, Sharpness-Aware Minimization (SAM) [Foret et al., 2020, Andriushchenko and Flammarion, 2022, Wen et al., 2023 has shown to be highly effective and scalable for DNNs."
  },
  {
    "index": 2205,
    "source_corpus_id": 245123905,
    "ref_id": "b27",
    "citation_corpus_id": 60441316,
    "start": 33944,
    "end": 33961,
    "title": "BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model",
    "abstract": "We show that BERT (Devlin et al., 2018)  is a Markov random field language model. Formulating BERT in this way gives way to a natural procedure to sample sentence from BERT. We sample sentences from BERT and find that it can produce high-quality, fluent generations. Compared to the generations of a traditional left-to-right language model, BERT generates sentences that are more diverse but of slightly worse quality.",
    "prev": "All of these methods were originally motivated by improving representation learning of text.",
    "curr": "There were a few attempts to heuristically decode models like BERT and turn them into text generative models such as in Wang & Cho (2019), however, samples from these models lack coherence.",
    "next": "CONCLUSION\n\nIn this paper we have proposed a novel non-autoregressive method that operates within the framework of denoising autoencoders and, crucially, unrolls the denoising process during training."
  },
  {
    "index": 2207,
    "source_corpus_id": 236777112,
    "ref_id": "b27",
    "citation_corpus_id": 166228758,
    "start": 4033,
    "end": 4056,
    "title": "Published as a conference paper at ICLR 2020 N-BEATS: NEURAL BASIS EXPANSION ANALYSIS FOR INTERPRETABLE TIME SERIES FORECASTING",
    "abstract": "We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11% over a statistical benchmark and by 3% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.Published as a conference paper at ICLR 2020 inject a suitable inductive bias in the model to make its internal operations more interpretable, in the sense of extracting some explainable driving factors combining to produce a given forecast?SUMMARY OF CONTRIBUTIONSDeep Neural Architecture: To the best of our knowledge, this is the first work to empirically demonstrate that pure DL using no time-series specific components outperforms well-established statistical approaches on M3, M4 and TOURISM datasets (on M4, by 11% over statistical benchmark, by 7% over the best statistical entry, and by 3% over the M4 competition winner). In our view, this provides a long-missing proof of concept for the use of pure ML in TS forecasting and strengthens motivation to continue advancing the research in this area.Interpretable DL for Time Series: In addition to accuracy benefits, we also show that it is feasible to design an architecture with interpretable outputs that can be used by practitioners in very much the same way as traditional decomposition techniques such as the \"seasonality-trend-level\" approach (Cleveland et al., 1990).V. Assimakopoulos and K. Nikolopoulos. The theta model: a decomposition approach to forecasting.",
    "prev": "Section 5 concludes this manuscript.",
    "curr": "INTRODUCTION\n\nIn the past years, methods such as Franceschi et al., 2019;Oreshkin et al., 2020a;Rasul et al., 2021;Cui et al., 2016; have consistently showcased the effectiveness of deep learning in time series analysis tasks.",
    "next": "Although these deep learning based methods are effective when sufficient and clean data is available, this assumption is not always met in practice."
  },
  {
    "index": 2210,
    "source_corpus_id": 263622244,
    "ref_id": "b7",
    "citation_corpus_id": 257833590,
    "start": 11044,
    "end": 11065,
    "title": "MULTIFACTOR SEQUENTIAL DISENTANGLEMENT VIA STRUCTURED KOOPMAN AUTOENCODERS",
    "abstract": "Disentangling complex data to its latent factors of variation is a fundamental task in representation learning. Existing work on sequential disentanglement mostly provides two factor representations, i.e., it separates the data to time-varying and time-invariant factors. In contrast, we consider multifactor disentanglement in which multiple (more than two) semantic disentangled components are generated. Key to our approach is a strong inductive bias where we assume that the underlying dynamics can be represented linearly in the latent space. Under this assumption, it becomes natural to exploit the recently introduced Koopman autoencoder models. However, disentangled representations are not guaranteed in Koopman approaches, and thus we propose a novel spectral loss term which leads to structured Koopman matrices and disentanglement. Overall, we propose a simple and easy to code new deep model that is fully unsupervised and it supports multifactor disentanglement. We showcase new disentangling abilities such as swapping of individual static factors between characters, and an incremental swap of disentangled factors from the source to the target. Moreover, we evaluate our method extensively on two factor standard benchmark tasks where we significantly improve over competing unsupervised approaches, and we perform competitively in comparison to weaklyand self-supervised state-of-the-art approaches.",
    "prev": "pport irregular sampling, and it employs autoencoders, GANs, and Neural ODEs (Chen et al., 2018).While GAN techniques are more common in generative TS, there are also a few VAE-based approaches.TimeVAE (Desai et al., 2021) models the whole sequence by a global random variable with a normal Gaussian distribution for a prior, and it introduces trend and seasonality building blocks into the decoder.CR-VAE (Li et al., 2023) learns a Granger causal graph by predicting future data from past observations and using a multi-head decoder acting separately on latent variable coordinates.",
    "curr": "Koopman-based Approaches.Koopman techniques have gained increasing interest over the past two decades, with applications ranging across analysis (Rowley et al., 2009;Schmid, 2010;Takeishi et al., 2017;Lusch et al., 2018;Azencot et al., 2019), optimization (Dogra & Redman, 2020;Redman et al., 2022), forecasting (Erichson et al., 2019;Azencot et al., 2020;Wang et al., 2023;Tayal et al., 2023), and disentanglement (Berman et al., 2023), among many others (Budišić et al., 2012;Brunton et al., 2021)\n\n\nBACKGROUND\n\nWe denote by x 1:T a TS input x 1:T = x 1 , .",
    "next": "."
  },
  {
    "index": 2212,
    "source_corpus_id": 53047456,
    "ref_id": "b24",
    "citation_corpus_id": 604334,
    "start": 1297,
    "end": 1319,
    "title": "Intriguing properties of neural networks",
    "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
    "prev": "Our experiments on simple MNIST and CIFAR10 models and a variety of cost matrices show that the proposed approach can produce models with substantially reduced cost-sensitive robust error, while maintaining classification accuracy.",
    "curr": "Introduction\n\nDespite the exceptional performance of deep neural networks (DNNs) on various machine learning tasks such as malware detection (Saxe & Berlin, 2015), face recognition (Parkhi et al., 2015) and autonomous driving (Bojarski et al., 2016), recent studies (Szegedy et al., 2014;Goodfellow et al., 2015) have shown that deep learning models are vulnerable to misclassifying inputs, known as adversarial examples, that are crafted with targeted but visually-imperceptible perturbations.",
    "next": "While several defense mechanisms have been proposed and empirically demonstrated to be successful against existing particular attacks (Papernot et al., 2016;Goodfellow et al., 2015), new attacks (Carlini & Wagner, 2017;Tramèr et al., 2017;Athalye et al., 2018) are repeatedly found that circumvent such defenses."
  },
  {
    "index": 2213,
    "source_corpus_id": 254823387,
    "ref_id": "b14",
    "citation_corpus_id": 3488815,
    "start": 6844,
    "end": 6863,
    "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
    "abstract": "Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. * Authors ordered alphabetically.",
    "prev": "• Empirically, our framework allows us to certify robustness of explanations and train networks with robust explanations across six different datasets ranging from financial applications to medical image classification.",
    "curr": "RELATED WORK\n\nAdversarial examples, inputs that have been imperceptibly modified to induce misclassification, are a well-known vulnerability of neural networks (Szegedy et al., 2013;Goodfellow et al., 2015;Madry et al., 2018).",
    "next": "A significant amount of research has studied methods for proving that no adversary can change the prediction for a given input and perturbation budget (Tjeng et al., 2017;Weng et al., 2018;Fazlyab et al., 2019)."
  },
  {
    "index": 2214,
    "source_corpus_id": 238198466,
    "ref_id": "b41",
    "citation_corpus_id": 231847016,
    "start": 10154,
    "end": 10175,
    "title": "DISCOVERING A SET OF POLICIES FOR THE WORST CASE REWARD",
    "abstract": "We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, setmax policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is a policy iteration algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite. . Fast reinforcement learning with generalized policy updates. . Diversity is all you need: Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018. Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval research logistics quarterly, 3(1-2):95-110, 1956. Dan Garber and Elad Hazan. A linearly convergent conditional gradient algorithm with applications to online and stochastic optimization. arXiv preprint arXiv:1301.4666, 2013. Reinforcement learning with soft state aggregation. In Advances in neural information processing systems, pp. 361-368, 1995. Nathan Sprague and Dana Ballard. Multiple-goal reinforcement learning with modular sarsa (0). 2003.",
    "prev": "(7)\nFor brevity, we will refer to this combined procedure of GPE and GPI simply as \"GPI\", unless otherwise noted.",
    "curr": "The resulting policy π is guaranteed to perform at least as well as any individual π ∈ Π (  and is part of a larger class of policies termed set-improving policies which perform at least as well as any single policy in a given set (Zahavy et al., 2021).",
    "next": "THE FIRST-OCCUPANCY REPRESENTATION\n\nWhile the SR encodes states via total occupancy, this may not always be ideal."
  },
  {
    "index": 2216,
    "source_corpus_id": 263605617,
    "ref_id": "b11",
    "citation_corpus_id": 253553811,
    "start": 7820,
    "end": 7823,
    "title": "Published as a conference paper at ICLR 2023 TOWARDS BETTER SELECTIVE CLASSIFICATION",
    "abstract": "We tackle the problem of Selective Classification where the objective is to achieve the best performance on a predetermined ratio (coverage) of the dataset. Recent state-of-the-art selective methods come with architectural changes either via introducing a separate selection head or an extra abstention logit. In this paper, we challenge the aforementioned methods. The results suggest that the superior performance of state-of-the-art methods is owed to training a more generalizable classifier rather than their proposed selection mechanisms. We argue that the best performing selection mechanism should instead be rooted in the classifier itself. Our proposed selection strategy uses the classification scores and achieves better results by a significant margin, consistently, across all coverages and all datasets, without any added compute cost. Furthermore, inspired by semi-supervised learning, we propose an entropy-based regularizer that improves the performance of selective classification methods. Our proposed selection mechanism with the proposed entropy-based regularizer achieves new state-of-the-art results.",
    "prev": "[25] observed that neural networks assign lower maximum softmax probabilities to OOD than to in-distribution point, meaning neural networks may actually exhibit less confidence on OOD inputs.Our work supports this observation, while further generalizing it to arbitrary loss functions.Other lines of research have explored the influence of architectural decisions on generalization [55,56,7,53], the relationship between in-distribution and OOD performance [34,2,3], and the behavior of neural network representations under OOD conditions [52,28,41].While our work also analyzes representations in the context of extrapolation, our focus is on understanding the mechanism behind \"reversion to the OCS\", which differs from the aforementioned works.",
    "curr": "Our work also explores risk-sensitive decision-making using selective classification as a testbed.Selective classification is a well-studied problem, and various methods have been proposed to enhance selective classification performance [15,12,6,38,8,54].In contrast, our aim is not to develop the best possible selective classification approach, but rather to providing insights into the effective utilization of neural network predictions in OOD decision-making.",
    "next": "Reversion to the Optimal Constant Solution\n\nIn this work, we will focus on the widely studied covariate shift setting [17,47].Formally, let the training data D = {(x i , y i )} N i=1 be generated by sampling x i ∼ P train (x) and y i ∼ P (y|x i ).At test time, we query the model with inputs generated from P OOD (x) ̸ = P train (x), whose ground truth labels are generated from the same conditional distribution, P (y|x), as that in training.We will denote a neural network model as f θ : R d → R m , where d and m are the dimensionalities of the input and output, and θ ∈ Θ represents the network weights.We will focus on settings where d is high-dimensional.The neural network weights are optimized by minimizing a loss function L using gradient descent, θ = arg min θ∈Θ\n1 N"
  },
  {
    "index": 2219,
    "source_corpus_id": 259298561,
    "ref_id": "b9",
    "citation_corpus_id": 232076011,
    "start": 4771,
    "end": 4791,
    "title": "Published as a conference paper at ICLR 2021 GRADIENT DESCENT ON NEURAL NETWORKS TYPI- CALLY OCCURS AT THE EDGE OF STABILITY",
    "abstract": "We empirically demonstrate that full-batch gradient descent on neural network training objectives typically operates in a regime we call the Edge of Stability. In this regime, the maximum eigenvalue of the training loss Hessian hovers just above the value 2/(step size), and the training loss behaves non-monotonically over short timescales, yet consistently decreases over long timescales. Since this behavior is inconsistent with several widespread presumptions in the field of optimization, our findings raise questions as to whether these presumptions are relevant to neural network training. We hope that our findings will inspire future efforts aimed at rigorously understanding optimization at the Edge of Stability.arXiv:2103.00065v3 [cs.LG] 23 Nov 2022Published as a conference paper at ICLR 2021 this happens, gradient descent does not diverge entirely or stall. Instead, it enters a regime we call the Edge of Stability 1 ( §3.2), in which (1) the sharpness hovers right at, or just above, the value 2/η; and (2) the train loss behaves non-monotonically, yet consistently decreases over long timescales. In this regime, gradient descent is constantly \"trying\" to increase the sharpness, but is constantly restrained from doing so. The net effect is that gradient descent continues to successfully optimize the training objective, but in such a way as to avoid further increasing the sharpness. 2",
    "prev": "The different panels show the predictor function f obtained when training with different step sizes.",
    "curr": "condition only characterizes the linearized dynamics, it has been empirically shown to hold in realworld neural-network training (Cohen et al., 2020;Gilmer et al., 2022).",
    "next": "The linear stability condition turns out to have a strong effect on the nature of the network that is obtained upon convergence, both in terms of the end-to-end predictor function (Mulayoff et al., 2021), and in terms of the way this function is implemented by the network (Mulayoff & Michaeli, 2020)."
  },
  {
    "index": 2222,
    "source_corpus_id": 52948121,
    "ref_id": "b16",
    "citation_corpus_id": 16496273,
    "start": 5667,
    "end": 5689,
    "title": "Efficient Learning of Domain-invariant Image Representations",
    "abstract": "We present an algorithm that learns representations which explicitly compensate for domain mismatch and which can be efficiently realized as linear classifiers. Specifically, we form a linear transformation that maps features from the target (test) domain to the source (training) domain as part of training the classifier. We optimize both the transformation and classifier parameters jointly, and introduce an efficient cost function based on misclassification loss. Our method combines several features previously unavailable in a single algorithm: multi-class adaptation through representation learning, ability to map across heterogeneous feature spaces, and scalability to large datasets. We present experiments on several image datasets that demonstrate improved accuracy and computational advantages compared to previous approaches.",
    "prev": "RELATED WORK\n\nDomain adaptation (cf.",
    "curr": "Csurka (2017) for a recent review) is generally approached either as domaininvariant learning (Hoffman et al., 2013;Herath et al., 2017;Ganin & Lempitsky, 2015) or as a statistical alignment problem (Tzeng et al., 2014;Long et al., 2015).",
    "next": "Our work focuses on unsupervised adaptation methods in the context of deep learning."
  },
  {
    "index": 2223,
    "source_corpus_id": 238408308,
    "ref_id": "b13",
    "citation_corpus_id": 15201887,
    "start": 9889,
    "end": 9904,
    "title": "8-BIT APPROXIMATIONS FOR PARALLELISM IN DEEP LEARNING",
    "abstract": "The creation of practical deep learning data-products often requires parallelization across processors and computers to make deep learning feasible on large data sets, but bottlenecks in communication bandwidth make it difficult to attain good speedups through parallelism. Here we develop and test 8-bit approximation algorithms which make better use of the available bandwidth by compressing 32-bit gradients and nonlinear activations to 8-bit approximations. We show that these approximations do not decrease predictive performance on MNIST, CIFAR10, and ImageNet for both model and data parallelism and provide a data transfer speedup of 2x relative to 32-bit parallelism. We build a predictive model for speedups based on our experimental data, verify its validity on known speedup data, and show that we can obtain a speedup of 50x and more on a system of 96 GPUs compared to a speedup of 23x for 32-bit. We compare our data types with other methods and show that 8-bit approximations achieve state-of-the-art speedups for model parallelism. Thus 8-bit approximation is an efficient method to parallelize convolutional networks on very large systems of GPUs.",
    "prev": "To perform this procedure for dynamic quantization we first normalize into the range [-1, 1] through division by the absolute maximum value: N = max(|T|).",
    "curr": "Then we find the closest values via a binary search: Dynamic Tree quantization (Dettmers, 2016) is a method that yields low quantization error for both small and large magnitude values.",
    "next": "Unlike data types with fixed exponent and fraction, dynamic tree quantization uses a datatype with a dynamic exponent and fraction that can change with each number."
  },
  {
    "index": 2224,
    "source_corpus_id": 44119895,
    "ref_id": "b12",
    "citation_corpus_id": 6706414,
    "start": 4613,
    "end": 4616,
    "title": "Published as a conference paper at ICLR 2015 EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES",
    "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples-inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
    "prev": "Defense against adversarial attacks.",
    "curr": "There has been significant recent effort trying to understand the theoretical nature of such attacks [10,13] and develop strategies against adversarial perturbations.",
    "next": "The search for solutions has focused on the network architecture, training procedures, and data preprocessing [15]."
  },
  {
    "index": 2225,
    "source_corpus_id": 186206768,
    "ref_id": "b9",
    "citation_corpus_id": 56475997,
    "start": 12826,
    "end": 12845,
    "title": "UNIVERSAL SUCCESSOR FEATURES APPROXIMATORS",
    "abstract": "The ability of a reinforcement learning (RL) agent to learn about many reward functions at the same time has many potential benefits, such as the decomposition of complex tasks into simpler ones, the exchange of information between tasks, and the reuse of skills. We focus on one aspect in particular, namely the ability to generalise to unseen tasks. Parametric generalisation relies on the interpolation power of a function approximator that is given the task description as input; one of its most common form are universal value function approximators (UVFAs). Another way to generalise to new tasks is to exploit structure in the RL problem itself. Generalised policy improvement (GPI) combines solutions of previous tasks into a policy for the unseen task; this relies on instantaneous policy evaluation of old policies under the new reward function, which is made possible through successor features (SFs). Our proposed universal successor features approximators (USFAs) combine the advantages of all of these, namely the scalability of UVFAs, the instant inference of SFs, and the strong generalisation of GPI. We discuss the challenges involved in training a USFA, its generalisation properties and demonstrate its practical benefits and transfer abilities on a large-scale domain in which the agent has to navigate in a first-person perspective three-dimensional environment.",
    "prev": "(5)\nIt can be shown that (5) is a strict generalization of (1), in the sense that Q π (s, a) ≥ Q πi (s, a) for all π i , s, and a.",
    "curr": "This result can be extended to the case in which (2) holds only approximately and ψ is replaced by a universal successor feature approximator (USFA) ψ θ ≈ ψ(s, a) (Barreto et al., 2017;Borsa et al., 2019).",
    "next": "The above suggests an approach to leveraging unsupervised pre-training for more dataefficient reinforcement learning."
  },
  {
    "index": 2230,
    "source_corpus_id": 256615676,
    "ref_id": "b25",
    "citation_corpus_id": 86611921,
    "start": 2659,
    "end": 2684,
    "title": "Natural Questions: A Benchmark for Question Answering Research",
    "abstract": "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.453",
    "prev": "As shown in Figure 1(a), they first conduct search in individual modalities, including text, image, video, etc.",
    "curr": "(Bajaj et al., 2016;Grubinger et al., 2008;Kwiatkowski et al., 2019;Awad et al., 2021), and then fuse the retrieval results from various verticals together, e.g., building another ranking layer on top of these single/cross modality retrievers (Escalante et al., 2008;Grubinger et al., 2008).",
    "next": "Both relevance modeling and retrieval result fusion are usually entwined to achieve more accurate multi-modal retrieval results."
  },
  {
    "index": 2232,
    "source_corpus_id": 52892910,
    "ref_id": "b21",
    "citation_corpus_id": 3075448,
    "start": 4524,
    "end": 4546,
    "title": "HIGH-DIMENSIONAL CONTINUOUS CONTROL USING GENERALIZED ADVANTAGE ESTIMATION",
    "abstract": "Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(λ). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.Published as a conference paper at ICLR 2016 λ ∈ [0, 1], the generalized advantage estimator (GAE). Related methods have been proposed in the context of online actor-critic methods(Kimura & Kobayashi, 1998;Wawrzyński, 2009). We provide a more general analysis, which is applicable in both the online and batch settings, and discuss an interpretation of our method as an instance of reward shaping(Ng et al., 1999), where the approximate value function is used to shape the reward.We present experimental results on a number of highly challenging 3D locomotion tasks, where we show that our approach can learn complex gaits using high-dimensional, general purpose neural network function approximators for both the policy and the value function, each with over 10 4 parameters. The policies perform torque-level control of simulated 3D robots with up to 33 state dimensions and 10 actuators.The contributions of this paper are summarized as follows:1. We provide justification and intuition for an effective variance reduction scheme for policy gradients, which we call generalized advantage estimation (GAE). While the formula has been proposed in prior work(Kimura & Kobayashi, 1998;Wawrzyński, 2009), our analysis is novel and enables GAE to be applied with a more general set of algorithms, including the batch trust-region algorithm we use for our experiments. 2. We propose the use of a trust region optimization method for the value function, which we find is a robust and efficient way to train neural network value functions with thousands of parameters. 3. By combining(1)and(2)above, we obtain an algorithm that empirically is effective at learning neural network policies for challenging control tasks. The results extend the state of the art in using reinforcement learning for high-dimensional continuous control. Videos are available at https://sites.google.com/site/gaepapersupp.",
    "prev": "Like CAPG for actions in an interval [α, β], our proposed algorithm, termed angular policy gradient (APG), uses a variance-reduced, unbiased estimated of the true policy gradient.",
    "curr": "Since the key step in APG is an update based on an estimate of the policy gradient, it can easily be combined with other state-of-the art methodology including value function approximation and generalized advantage estimation (Sutton et al., 2000;Schulman et al., 2016), as well as used in policy optimization algorithms like TRPO, A3C, and PPO (Schulman et al., 2015;Mnih et al., 2016;Schulman et al., 2017).",
    "next": "Beyond new methodology, we also introduce the marginal policy gradients (MPG) family of estimators; this general class of estimators contains both APG and CAPG, and we present a unified analysis of the variance reduction properties of all such methods."
  },
  {
    "index": 2234,
    "source_corpus_id": 264289064,
    "ref_id": "b14",
    "citation_corpus_id": 247519233,
    "start": 2431,
    "end": 2456,
    "title": "TOXIGEN: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection",
    "abstract": "Toxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate. Such over-reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic language. To help mitigate these issues, we create TOXIGEN, a new large-scale and machinegenerated dataset of 274k toxic and benign statements about 13 minority groups. We develop a demonstration-based prompting framework and an adversarial classifier-in-the-loop decoding method to generate subtly toxic and benign text with a massive pretrained language model (Brown et al., 2020). Controlling machine generation in this way allows TOXIGEN to cover implicitly toxic text at a larger scale, and about more demographic groups, than previous resources of human-written text. We conduct a human evaluation on a challenging subset of TOXIGEN and find that annotators struggle to distinguish machine-generated text from human-written language. We also find that 94.5% of toxic examples are labeled as hate speech by human annotators. Using three publicly-available datasets, we show that finetuning a toxicity classifier on our data improves its performance on human-written data substantially. We also demonstrate that TOXI-GEN can be used to fight machine-generated toxicity as finetuning improves the classifier significantly on our evaluation subset.",
    "prev": "1 Warning: This paper contains qualitative examples that may be viewed as offensive or harmful.",
    "curr": "INTRODUCTION\n\nLarge Language Models (LLMs) are increasingly being employed for a wide variety of domains, with use-cases including creative writing, chatbots, and semantic search among others (Touvron et al., 2023b;Taori et al., 2023;Ouyang et al., 2022;Bai et al., 2022a;b;Brown et al., 2020).Many of these applications are inherently subjective and require generations that cater to different demographics, cultural and societal norms, or simply individual preferences (Hartvigsen et al., 2022;Zhang et al., 2023;Solaiman & Dennison, 2021;Blodgett et al., 2020;Dunbar et al., 1997).By virtue of their large-scale training, current language models are exposed to diverse data that allows them to represent a multitude of such opinions (Glaese et al., 2022;Durmus et al., 2023;Santurkar et al., 2023).However, expressing these diverse opinions requires steering the LLM generations to user requirements.This brings forth the key question studied in this work:\n\nHow do we efficiently adapt LLMs to align closely with the opinions of specific interest groups?",
    "next": "Broadly, prior work has explored two modes of steering language models, which trade-off training complexity with test-time engineering.On one end, prompt engineering approaches avoid explicit modifications to the parameters of the language model and elicit desired behavior by crafting a suitable prompt.Often, the prompt is augmented with a few in-context examples (Brown et al., 2020;Taori et al., 2023;Chowdhery et al., 2022).While pr"
  },
  {
    "index": 2235,
    "source_corpus_id": 3067546,
    "ref_id": "b27",
    "citation_corpus_id": 14124313,
    "start": 6927,
    "end": 6931,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "Related Work\n\nDeepening feed-forward neural networks has generally returned dividends in performance.",
    "curr": "For example, see the improvement on the ImageNet [3] classification task when transitioning from AlexNet [15] to VGG-16 [28] to ResNet [8].",
    "next": "Unfortunately, greater depth also makes training more challenging, at least when employing a first-order optimization algorithm with a randomly initialized stack of layers."
  },
  {
    "index": 2236,
    "source_corpus_id": 248887644,
    "ref_id": "b25",
    "citation_corpus_id": 3366315,
    "start": 15043,
    "end": 15063,
    "title": "Published as a conference paper at ICLR 2018 SPECTRAL NORMALIZATION FOR GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. The code with Chainer (Tokui et al., 2015), generated images and pretrained models are available at https://github.com/pfnet-research/sngan_ projection.Published as a conference paper at ICLR 2018• Lipschitz constant is the only hyper-parameter to be tuned, and the algorithm does not require intensive tuning of the only hyper-parameter for satisfactory performance. • Implementation is simple and the additional computational cost is small.",
    "prev": "These constraints are the Riemannian analog to the Lipschitz constraint in the dual formulation of the 1-Wasserstein distance on R D .",
    "curr": "Constraints of this type are challenging to enforce in gradientbased optimization, and the Wasserstein GAN literature has explored approximations Gulrajani et al., 2017;Miyato et al., 2018).",
    "next": "We follow the standard technique introduced by Gulrajani et al."
  },
  {
    "index": 2238,
    "source_corpus_id": 218522109,
    "ref_id": "b28",
    "citation_corpus_id": 7071211,
    "start": 4237,
    "end": 4257,
    "title": "Published as a conference paper at ICLR 2017 ON DETECTING ADVERSARIAL PERTURBATIONS",
    "abstract": "Machine learning and deep learning in particular has advanced tremendously on perceptual tasks in recent years. However, it remains vulnerable against adversarial perturbations of the input that have been crafted specifically to fool the system while being quasi-imperceptible to a human. In this work, we propose to augment deep neural networks with a small \"detector\" subnetwork which is trained on the binary classification task of distinguishing genuine data from data containing adversarial perturbations. Our method is orthogonal to prior work on addressing adversarial perturbations, which has mostly focused on making the classification network itself more robust. We show empirically that adversarial perturbations can be detected surprisingly well even though they are quasi-imperceptible to humans. Moreover, while the detectors have been trained to detect only a specific adversary, they generalize to similar and weaker adversaries. In addition, we propose an adversarial attack that fools both the classifier and the detector and a novel training procedure for the detector that counteracts this attack.",
    "prev": "Besides, the proposed defenses have been shown to be limited and often not effective and easy to overcome (Athalye et al., 2018).",
    "curr": "Alternatively, a large body of work has focused on detection of adversarial examples (Bhagoji et al., 2017;Feinman et al., 2017;Gong et al., 2017;Grosse et al., 2017;Metzen et al., 2017;Hendrycks & Gimpel, 2017;Li & Li, 2017;Xu et al., 2017;Pang et al., 2018;Roth et al., 2019;Bahat et al., 2019;Ma et al., 2018;Zheng & Hong, 2018;Tian et al., 2018).",
    "next": "While training robust classifiers focuses on maintaining performance in presence of adversarial examples, adversarial detection only cares for detecting such examples."
  },
  {
    "index": 2239,
    "source_corpus_id": 219965819,
    "ref_id": "b18",
    "citation_corpus_id": 58981389,
    "start": 3871,
    "end": 3875,
    "title": "Stable Recurrent Models",
    "abstract": "Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models. *",
    "prev": "The motivation for viewing RNNs as dynamical systems is that it allows us to borrow tools from stability analysis to study the long-term behavior of the hidden state.",
    "curr": "From this point of view, an unstable unit presents an exploding gradients problem, whereas the gradients of a stable model do not explode over time [19].",
    "next": "On the other hand, a stable recurrent unit can suffer from vanishing gradients, which in turn leads to catastrophic forgetting [20]."
  },
  {
    "index": 2240,
    "source_corpus_id": 247518637,
    "ref_id": "b34",
    "citation_corpus_id": 604334,
    "start": 1334,
    "end": 1356,
    "title": "Intriguing properties of neural networks",
    "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
    "prev": "We show that when using IBP training to train a randomly initialized two-layer ReLU neural network with logistic loss, gradient descent can linearly converge to zero robust training error with a high probability if we have sufficiently small perturbation radius and large network width.",
    "curr": "INTRODUCTION\n\nIt has been shown that deep neural networks are vulnerable against adversarial examples (Szegedy et al., 2014;Goodfellow et al., 2015), where a human imperceptible adversarial perturbation can easily alter the prediction by neural networks.",
    "next": "This poses concerns to safety-critical applications such as autonomous vehicles, healthcare or finance systems."
  },
  {
    "index": 2241,
    "source_corpus_id": 220830766,
    "ref_id": "b2",
    "citation_corpus_id": 2906360,
    "start": 2784,
    "end": 2804,
    "title": "DEEPCODER: LEARNING TO WRITE PROGRAMS",
    "abstract": "We develop a first line of attack for solving programming competition-style problems from input-output examples using deep learning. The approach is to train a neural network to predict properties of the program that generated the outputs from the inputs. We use the neural network's predictions to augment search techniques from the programming languages community, including enumerative search and an SMT-based solver. Empirically, we show that our approach leads to an order of magnitude speedup over the strong non-augmented baselines and a Recurrent Neural Network approach, and that we are able to solve problems of difficulty comparable to the simplest problems on programming competition websites.Published as a conference paper at ICLR 2017 show orders-of-magnitude improvements over optimized standard search techniques and a Recurrent Neural Network-based approach to the problem.",
    "prev": " own creation.",
    "curr": "INTRODUCTION\n\nProgram synthesis is a longstanding goal of artificial intelligence research (Manna & Waldinger, 1971;Summers, 1977), but it remains difficult in part because of the challenges of search (Alur et al., 2013;Gulwani et al., 2017).The objective in program synthesis is to automatically write a program given a specification of its intended behavior, and current state of the art methods typically perform some form of search over a space of possible programs.Many different search methods have been explored in the literature, both with and without learning.These include search within a version-space algebra (Gulwani, 2011), bottom-up enumerative search (Udupa et al., 2013), stochastic search (Schkufza et al., 2013), genetic programming (Koza, 1994), reducing the synthesis problem to logical satisfiability (Solar-Lezama et al., 2006), beam search with a sequence-to-sequence neural network (Devlin et al., 2017), learning to perform premise selection to guide search (Balog et al., 2017), learning to prioritize grammar rules within top-down search (Lee et al., 2018), and learned search based on partial executions (Ellis et al., 2019;Zohar & Wolf, 2018;Chen et al., 2019).",
    "next": "While these approaches have yielded significant progress, none of them completely capture the following important intuition: human programmers routinely write complex programs by first writing sub-programs and then analyzing their intermediate results to compose them in appropriate ways.We propose a new learning-guided system for synthesis, called BUSTLE, 1 which follows this intuition in a straightforward manner.Given a specification of a program's intended behavior (in this paper given by input-output examples), BUSTLE performs bottom-up enumerative search for a satisfying program, following Udupa et al."
  },
  {
    "index": 2242,
    "source_corpus_id": 259164815,
    "ref_id": "b12",
    "citation_corpus_id": 252668917,
    "start": 2284,
    "end": 2288,
    "title": "CODEGEN: AN OPEN LARGE LANGUAGE MODEL FOR CODE WITH MULTI-TURN PROGRAM SYNTHESIS",
    "abstract": "Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multiturn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.",
    "prev": "This trend is also observed in the domain of code understanding and generation.",
    "curr": "Numerous Code LLMs [11][12][13][14][15][16][17][18] have been proposed to tackle the challenges associated with code-related tasks.",
    "next": "These Code LLMs undergo pre-training using substantial amounts of code data, enabling them to excel in various code-related tasks, showcasing impressive performance."
  },
  {
    "index": 2243,
    "source_corpus_id": 264172260,
    "ref_id": "b42",
    "citation_corpus_id": 3693334,
    "start": 67046,
    "end": 67063,
    "title": "Visualizing the Loss Landscape of Neural Nets",
    "abstract": "Neural network training relies on our ability to find \"good\" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and wellchosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple \"filter normalization\" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.",
    "prev": "We provide additional (lower-resolution) heatmaps around neighboorhoods of various cloner policies in Figure 13.",
    "curr": "In addition to stochastic gradient directions, we visualize cross-sections of these landscapes in random directions, like the main figures in (Li et al., 2018).",
    "next": "13 This confirms the generality of our finding in (R1), concerning the fractality of the reward landscape in regions where the loss is extremely smooth and near-convex."
  },
  {
    "index": 2245,
    "source_corpus_id": 253523197,
    "ref_id": "b49",
    "citation_corpus_id": 6212000,
    "start": 1801,
    "end": 1821,
    "title": "UNDERSTANDING DEEP LEARNING REQUIRES RE- THINKING GENERALIZATION",
    "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.",
    "prev": "D solutions due to a phenomenon we call variance collapse: interpolated deep networks suffer a collapse in the variance of their activations, causing poor performance.Next, we propose REPAIR (REnormalizing Permuted Activations for Interpolation Repair) which mitigates variance collapse by rescaling the preactivations of such interpolated networks.We explore the interaction between our method and the choice of normalization layer, network width, and depth, and demonstrate that using REPAIR on top of neuron alignment methods leads to 60%-100% relative barrier reduction across a wide variety of architecture families and tasks.In particular, we report a 74% barrier reduction for ResNet50 on ImageNet and 90% barrier reduction for ResNet18 on CIFAR10.Our code is available at https://github.com/KellerJordan/REPAIR.",
    "curr": "Introduction\n\nTraining a neural network corresponds to optimizing a highly non-linear function by navigating a complex loss landscape with numerous minima, symmetries and saddles (Zhang et al., 2017;Keskar et al., 2017;Draxler et al., 2018;Şimşek et al., 2021).Overparameterization is one of the reasons behind the abundance of minima leading to different functions that behave similarly on the training data (Neyshabur et al., 2017;Nguyen et al., 2018;Li et al., 2018;Liu et al., 2020).Another reason is the existence of permutation and scaling invariances which lead to functionally identical minima that differ in the weight space (Brea et al., 2019;Entezari et al., 2021).Due to the relationship of the loss landscape with generalization and optimization, a large body of recent works (Li et al., 2017;Mei et al., 2018;Geiger et al., 2019;Nguyen et al., 2018;Fort et al., 2019;Şimşek et al., 2021;Juneja et al., 2022) study the loss landscape of deep neural networks with the goal of navigating the optimizer to a region with desired properties, e.g., with respect to flatness around the SGD solution (Baldassi et al., 2020;Pittorino et al., 2020).",
    "next": "Early work conjectured the existence of a non"
  },
  {
    "index": 2246,
    "source_corpus_id": 259164684,
    "ref_id": "b8",
    "citation_corpus_id": 252907410,
    "start": 3583,
    "end": 3585,
    "title": "SEQUENTIAL LEARNING OF NEURAL NETWORKS FOR PREQUENTIAL MDL",
    "abstract": "Minimum Description Length (MDL) provides a framework and an objective for principled model evaluation. It formalizes Occam's Razor and can be applied to data from non-stationary sources. In the prequential formulation of MDL, the objective is to minimize the cumulative next-step log-loss when sequentially going through the data and using previous observations for parameter estimation. It thus closely resembles a continual-or online-learning problem. In this study, we evaluate approaches for computing prequential description lengths for image classification datasets with neural networks. Considering the computational cost, we find that online-learning with rehearsal has favorable performance compared to the previously widely used block-wise estimation. We propose forward-calibration to better align the models predictions with the empirical observations and introduce replay-streams, a minibatch incremental training technique to efficiently implement approximate random replay while avoiding large in-memory replay buffers. As a result, we present description lengths for a suite of image classification datasets that improve upon previously reported results by large margins.",
    "prev": "Depending on the nature of the data stream, plasticity, not-forgetting and sample-efficiency all play a crucial role during learning.",
    "curr": "Furthermore, when considering the cumulative next-step log-loss under this protocol, it directly corresponds to the prequential description length and is thus a theoretically well motivated evaluation metric for non-stationary scenarios under the Minimum Description Length principle [17,4,7].",
    "next": "In this paper, we propose a new method based on Kalman filters which explicitly takes into account non-stationaries in the data stream."
  },
  {
    "index": 2249,
    "source_corpus_id": 822804,
    "ref_id": "b15",
    "citation_corpus_id": 2100831,
    "start": 1390,
    "end": 1415,
    "title": "MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text",
    "abstract": "We present MCTest, a freely available set of stories and associated questions intended for research on the machine comprehension of text. Previous work on machine comprehension (e.g., semantic modeling) has made great strides, but primarily focuses either on limited-domain datasets, or on solving a more restricted goal (e.g., open-domain relation extraction). In contrast, MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension. Reading comprehension can test advanced abilities such as causal reasoning and understanding the world, yet, by being multiple-choice, still provide a clear metric. By being fictional, the answer typically can be found only in the story itself. The stories and questions are also carefully limited to those a young child would understand, reducing the world knowledge that is required for the task. We present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions. By screening workers (with grammar tests) and stories (with grading), we have ensured that the data is the same quality as another set that we manually edited, but at one tenth the editing cost. By being open-domain, yet carefully restricted, we hope MCTest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text.",
    "prev": "For example, in textual entailment, one needs to determine whether a hypothesis sentence can be inferred from a premise sentence (Bowman et al., 2015).",
    "curr": "In machine comprehension, given a passage, a question needs to be matched against it in order to find the correct answer (Richardson et al., 2013;Tapaswi et al., 2016).",
    "next": "Table 1 gives two example sequence matching problems."
  },
  {
    "index": 2250,
    "source_corpus_id": 252595808,
    "ref_id": "b45",
    "citation_corpus_id": 235254714,
    "start": 1644,
    "end": 1663,
    "title": "Diversifying Dialog Generation via Adaptive Label Smoothing",
    "abstract": "Neural dialogue generation models trained with the one-hot target distribution suffer from the over-confidence issue, which leads to poor generation diversity as widely reported in the literature. Although existing approaches such as label smoothing can alleviate this issue, they fail to adapt to diverse dialog contexts. In this paper, we propose an Adaptive Label Smoothing (AdaLabel) approach that can adaptively estimate a target label distribution at each time step for different contexts. The maximum probability in the predicted distribution is used to modify the soft target distribution produced by a novel light-weight bi-directional decoder module. The resulting target distribution is aware of both previous and future contexts and is adjusted to avoid over-training the dialogue model. Our model can be trained in an endto-end manner. Extensive experiments on two benchmark datasets show that our approach outperforms various competitive baselines in producing diverse responses.",
    "prev": "Further, experiments on two large-scale open-domain dialogue datasets verify that our EqHard-EM algorithm generates high-quality diverse responses 1 .",
    "curr": "INTRODUCTION\n\nOpen-domain dialogue systems aim to generate natural language text utterances to hold open-ended conversations with humans (Li et al., 2017a;Wang et al., 2021b).",
    "next": "These systems have shown great success, and are seamlessly integrated into our society through chatbots."
  },
  {
    "index": 2252,
    "source_corpus_id": 108299957,
    "ref_id": "b53",
    "citation_corpus_id": 4567927,
    "start": 2398,
    "end": 2424,
    "title": "Published as a conference paper at ICLR 2018 LEARNING GENERAL PURPOSE DISTRIBUTED SEN- TENCE REPRESENTATIONS VIA LARGE SCALE MULTI- TASK LEARNING",
    "abstract": "A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general purpose features for words across a range of NLP problems. However, extending this success to learning representations of sequences of words, such as sentences, remains an open problem. Recent work has explored unsupervised as well as supervised learning techniques with different training objectives to learn general purpose fixed-length sentence representations. In this work, we present a simple, effective multi-task learning framework for sentence representations that combines the inductive biases of diverse training objectives in a single model. We train this model on several data sources with multiple training objectives on over 100 million sentences. Extensive experiments demonstrate that sharing a single recurrent sentence encoder across weakly related tasks leads to consistent improvements over previous methods. We present substantial improvements in the context of transfer learning and low-resource settings using our learned general-purpose representations. 1",
    "prev": "More than half a century since it was first proposed, the Bag-of-Words (BoW) representation (Harris, 1954;Salton et al., 1975;Manning et al., 2008) remains a popular baseline across machine learning (ML), natural language processing (NLP), and information retrieval (IR) communities.",
    "curr": "In recent years, however, BoW was largely eclipsed by representations learned through neural networks, ranging from shallow (Le & Mikolov, 2014;Hill et al., 2016) to recurrent Conneau et al., 2017;Subramanian et al., 2018a), recursive (Socher et al., 2013;Tai et al., 2015), convolutional (Kalchbrenner et al., 2014;Kim, 2014), self-attentive (Vaswani et al., 2017;Cer et al., 2018a) and hybrid architectures (Gan et al., 2017;Tang et al., 2017;Zhelezniak et al., 2018).",
    "next": "Interestingly, Arora et al."
  },
  {
    "index": 2254,
    "source_corpus_id": 255393757,
    "ref_id": "b26",
    "citation_corpus_id": 6628106,
    "start": 14929,
    "end": 14948,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "The photometric error is comprised of an 1 term and an SSIM (structural similarity index measure) term (Wang et al., 2004):\npe = α 2 (1 − SSIM(I t , I t →t )) + (1 − α) 1 (I t , I t →t ),(2)\nwhere we set α = 0.85 following the practice (Godard et al., 2017;.",
    "curr": "The smooth loss is:\ns = |∂ x d *\npipeline, we train the model for 30 epochs by Adam (Kingma & Ba, 2015) optimizer with a learning rate of 10 −4 which drops to 10 −5 after 25 epochs.",
    "next": "For the second stage, the encoder is trained for 20 epochs using the AdamW (Loshchilov & Hutter, 2017) optimizer."
  },
  {
    "index": 2255,
    "source_corpus_id": 31816657,
    "ref_id": "b8",
    "citation_corpus_id": 9818013,
    "start": 12173,
    "end": 12194,
    "title": "Globally Coherent Text Generation with Neural Checklist Models",
    "abstract": "Recurrent neural networks can generate locally coherent text but often have difficulties representing what has already been generated and what still needs to be said -especially when constructing long texts. We present the neural checklist model, a recurrent neural network that models global coherence by storing and updating an agenda of text strings which should be mentioned somewhere in the output. The model generates output by dynamically adjusting the interpolation among a language model and a pair of attention models that encourage references to agenda items. Evaluations on cooking recipes and dialogue system responses demonstrate high coherence with greatly improved semantic coverage of the agenda.",
    "prev": "Table 1 provides examples of annotations in this action lexicon.",
    "curr": "DATASET\n\nFor learning and evaluation, we use a subset of the Now You're Cooking dataset (Kiddon et al., 2016).",
    "next": "We chose 65816 recipes for training, 175 recipes for development, and 700 recipes for testing."
  },
  {
    "index": 2258,
    "source_corpus_id": 244478211,
    "ref_id": "b11",
    "citation_corpus_id": 53388625,
    "start": 2004,
    "end": 2027,
    "title": "THE LOTTERY TICKET HYPOTHESIS: FINDING SPARSE, TRAINABLE NEURAL NETWORKS",
    "abstract": "Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard technique for pruning weights naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the lottery ticket hypothesis: dense, randomly-initialized feed-forward networks contain subnetworks (winning tickets) that-when trained in isolation-arrive at comparable test accuracy in a comparable number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Furthermore, the winning tickets we find above that size learn faster than the original network and exhibit higher test accuracy. arXiv:1803.03635v4 [cs.LG] 27 Nov 2018 1. Randomly initialize a neural network f (x; θ 0 ) (where θ 0 ∼ D θ ). 2. Train the network for j iterations, reaching parameters θ j . 3. Prune s% of the parameters, creating a mask m where P m = (100 − s)%. 4. To extract the winning ticket, reset the remaining parameters to their values in θ 0 , creating the untrained network f (x; m θ 0 ).If dense networks contain winning tickets and pruning reveals them, then the network f (x; m θ 0 )when trained for j iterations-will reach similar accuracy to f (x; θ j ) at least as quickly, and m will be too sparse for a randomly-reinitialized or randomly-sparsified network to do the same.Results. We identify winning tickets in a fully-connected architecture for MNIST and convolutional architectures for CIFAR10 across several optimization strategies (SGD, momentum, and Adam) with techniques like dropout, weight decay, and batchnorm. In deeper networks, our pruning-based strategy for finding winning tickets is sensitive to the learning rate: it requires warmup to find winning tickets at higher learning rates. The winning tickets we find are 10-20% (or less) of the size of the Anonymous. Gradient descent provably optimizes over-parameterized neural networks. Dally. Dsd: Regularizing deep neural networks with dense-sparse-dense training flow. Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size. arXiv preprint arXiv:1602.07360, 2016.",
    "prev": "cal innovations related to pruning for strong lottery tickets, including extensions of subset sum results and a strategy to leverage higher amounts of depth.Our explicit sparse constructions of universal function families might be of independent interest, as they highlight representational benefits induced by univariate convolutional architectures.",
    "curr": "INTRODUCTION\n\nDeep learning has achieved major breakthroughs in a variety of tasks (LeCun et al., 1990;Schmidhuber, 2015), yet, it comes at a considerable computational cost (Sharir et al., 2020), which is exaggerated by the recent trend towards ever wider and deeper neural network architectures.Reducing the size of the networks before training could therefore significantly broaden the applicability of deep learning, lower its environmental impact, and increase access (Dhar, 2020).However, such sparse representations are often difficult to learn, as they may not enjoy the benefits associated with over-parameterization (Belkin et al., 2019).Frankle & Carbin (2019) provided a proof of concept that sparse neural network architectures are well trainable if initialized appropriately.Their lottery ticket hypothesis states that a randomlyinitialized network contains a small subnetwork that can compete with the performance of the original network when trained in isolation.Further, Ramanujan et al.",
    "next": "(2020) conjectured the existence of strong lottery tickets, which do not need any further training and achieve competitive performance at their initial parameters.These tickets could thus be obtained by pruning a large randomly initialized deep neural network.Unfortunately, existing pruning algorithms that search for (strong) lottery tickets have high computational demands, which are often comparable to or higher than training the original large network.However, Morcos et al."
  },
  {
    "index": 2259,
    "source_corpus_id": 264128429,
    "ref_id": "b46",
    "citation_corpus_id": 53108063,
    "start": 1756,
    "end": 1770,
    "title": "Mutual Information Maximization for Simple and Accurate Part-Of-Speech Induction",
    "abstract": "We address part-of-speech (POS) induction by maximizing the mutual information between the induced label and its context. We focus on two training objectives that are amenable to stochastic gradient descent (SGD): a novel generalization of the classical Brown clustering objective and a recently proposed variational lower bound. While both objectives are subject to noise in gradient updates, we show through analysis and experiments that the variational lower bound is robust whereas the generalized Brown objective is vulnerable. We obtain strong performance on a multitude of datasets and languages with a simple architecture that encodes morphology and context.",
    "prev": "a by-product, our method also enables the estimation of the entropy of random variables.Armed with such building blocks, we present a general recipe to measure MI, which unfolds in two directions: one uses conditional diffusion process, whereas the other uses joint diffusion processes that allow simultaneous modelling of two random variables.Our results, which derive from a thorough experimental protocol over all the variants of our approach, indicate that our method is more accurate than the main alternatives from the literature, especially for challenging distributions.Furthermore, our methods pass MI self-consistency tests, including data processing and additivity under independence, which instead are a pain-point of existing methods.",
    "curr": "INTRODUCTION\n\nMutual Information (MI) is a central measure to study the non-linear dependence between random variables [Shannon, 1948;MacKay, 2003], and has been extensively used in machine learning for representation learning [Bell & Sejnowski, 1995;Stratos, 2019;Belghazi et al., 2018;Oord et al., 2018;Hjelm et al., 2019], and for both training [Alemi et al., 2016;Chen et al., 2016;Zhao et al., 2018] and evaluating generative models [Alemi & Fischer, 2018;Huang et al., 2020].",
    "next": "For many problems of interest, precise computation of MI is not an easy task [McAllester & Stratos, 2020;Paninski, 2003], and a wide range of techniques for MI estimation have flourished.As the application of existing parametric and non-parametric methods [Pizer et al., 1987;Moon et al., 1995;Kraskov et al., 2004;Gao et al., 2015] to realistic, high-dimensional data is extremely challenging, if not unfeasible, recent research has focused on variational approaches [Barber & Agakov, 2004;Nguyen et al., 2007;Nowozin et al., 2016;Poole et al., 2019;Wunder et al., 2021;Letizia et al., 2023;Federici et al., 2023] and neural estimators [Papamakarios et al., 2017;Belghazi et al., 2018;Oord et al., 2018;Song & Ermon, 2019;Rhodes et al., 2020;Letizia & Tonello, 2022;Brekelmans et a"
  },
  {
    "index": 2260,
    "source_corpus_id": 4994434,
    "ref_id": "b29",
    "citation_corpus_id": 2187805,
    "start": 17936,
    "end": 17940,
    "title": "A NOTE ON THE EVALUATION OF GENERATIVE MODELS",
    "abstract": "Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria-average log-likelihood, Parzen window estimates, and visual fidelity of samples-are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.",
    "prev": "We used a sample size of 50.",
    "curr": "Note that since the pixel coordinates are discrete, it is necessary to dequantize them by adding uniform noise u ∼ U [0, 1] to the coordinates if one models them as real numbers, else you can get arbitrarily high densities (see [30] for a discussion of this point).",
    "next": "We used 3 stochastic layers with 2 units for each z and 64 units for c. Each subnetwork contained 3 dense layers with 256 ReLU units each."
  },
  {
    "index": 2261,
    "source_corpus_id": 219531578,
    "ref_id": "b17",
    "citation_corpus_id": 21529792,
    "start": 3082,
    "end": 3099,
    "title": "Published as a conference paper at ICLR 2018 LEARNING ROBUST REWARDS WITH ADVERSARIAL INVERSE REINFORCEMENT LEARNING",
    "abstract": "Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually specified reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose AIRL, a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation. We demonstrate that AIRL is able to recover reward functions that are robust to changes in dynamics, enabling us to learn policies even under significant variation in the environment seen during training. Our experiments show that AIRL greatly outperforms prior methods in these transfer settings.",
    "prev": "However, IRL methods are usually based on an iterative process alternating between reward estimation and RL, which might result in poor sample-efficiency.",
    "curr": "Earlier IRL methods [Ng et al., 2000, Abbeel and Ng, 2004, Ziebart et al., 2008 require multiple calls to a Markov decision process solver [Puterman, 2014], whereas recent adversarial IL approaches [Finn et al., 2016, Ho and Ermon, 2016, Fu et al., 2018 interleave the learning of the reward function with the learning process of the agent.",
    "next": "Adversarial IL methods are based on an adversarial training paradigm similar to Generative Adversarial Networks (GANs) [Goodfellow et al., 2014], where the learned reward function can be thought of as the confusion of a discriminator that learns to differentiate expert transitions from non expert ones."
  },
  {
    "index": 2264,
    "source_corpus_id": 258987919,
    "ref_id": "b48",
    "citation_corpus_id": 12130431,
    "start": 1897,
    "end": 1916,
    "title": "INCREMENTAL NETWORK QUANTIZATION: TOWARDS LOSSLESS CNNS WITH LOW-PRECISION WEIGHTS",
    "abstract": "This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two innovations. On one hand, we introduce three interdependent operations, namely weight partition, group-wise quantization and re-training. A wellproven measure is employed to divide the weights in each layer of a pre-trained CNN model into two disjoint groups. The weights in the first group are responsible to form a low-precision base, thus they are quantized by a variable-length encoding method. The weights in the other group are responsible to compensate for the accuracy loss from the quantization, thus they are the ones to be re-trained. On the other hand, these three operations are repeated on the latest re-trained group in an iterative manner until all the weights are converted into low-precision ones, acting as an incremental network quantization and accuracy enhancement procedure. Extensive experiments on the ImageNet classification task using almost all known deep CNN architectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the efficacy of the proposed method. Specifically, at 5-bit quantization (a variable-length encoding: 1 bit for representing zero value, and the remaining 4 bits represent at most 16 different values for the powers of two) 1 , our models have improved accuracy than the 32-bit floating-point references. Taking ResNet-18 as an example, we further show that our quantized models with 4-bit, 3-bit and 2-bit ternary weights have improved or very similar accuracy against its 32-bit floating-point baseline. Besides, impressive results with the combination of network pruning and INQ are also reported. We believe that our method sheds new insights on how to make deep CNNs to be applicable on mobile or embedded devices. The code is available at https://github.com",
    "prev": "Introduction\n\nDeploying and optimizing Deep Neural Networks (DNNs) on real-world systems with bounded computing resources (e.g., edge devices), have led to various techniques for approximating DNNs while maintaining high accuracy and robustness.",
    "curr": "Common approximation techniques are quantization -reducing the numerical precision of weights [Zhou et al., 2017], and pruning -removing weights that have minimal impact on accuracy [Frankle and Carbin, 2019].",
    "next": "For trustworthy deployment of DNNs, randomized smoothing (RS) is a promising approach for obtaining robustness certificates by constructing a smoothed model g from a base network f under noise [Cohen et al., 2019]."
  },
  {
    "index": 2266,
    "source_corpus_id": 249209990,
    "ref_id": "b13",
    "citation_corpus_id": 218901061,
    "start": 30491,
    "end": 30511,
    "title": "CausaLM: Causal Model Explanation Through Counterfactual Language Models under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics",
    "abstract": "Understanding predictions made by deep neural networks is notoriously difficult, but also crucial to their dissemination. As all machine learning-based methods, they are as good as their training data, and can also capture unwanted biases. While there are tools that can help understand whether such biases exist, they do not distinguish between correlation and causation, and might be ill-suited for text-based models and for reasoning about high-level language concepts. A key problem of estimating the causal effect of a concept of interest on a given model is that this estimation requires the generation of counterfactual examples, which is challenging with existing generation technology. To bridge that gap, we propose CausaLM, a framework for producing causal model explanations using counterfactual language representation models. Our approach is based on fine-tuning of deep contextualized embedding models with auxiliary adversarial tasks derived from the causal graph of the problem. Concretely, we show that by carefully choosing Submission Volume 47, Number 2 auxiliary adversarial pre-training tasks, language representation models such as BERT can effectively learn a counterfactual representation for a given concept of interest, and be used to estimate its true causal effect on model performance. A byproduct of our method is a language representation model that is unaffected by the tested concept, which can be useful in mitigating unwanted bias ingrained in the data. 1",
    "prev": "Related work focuses on understanding if neural networks encode and use concepts (Lucieri et al., 2020;Kim et al., 2018;McGrath et al., 2021), or generate counterfactual explanations to understand model behavior (Ghandeharioun et al., 2021;Abid et al., 2022;Akula et al., 2020).",
    "curr": "Recent works evaluated the causal validity of explanations (Feder et al., 2021;Elazar et al., 2021;Goyal et al., 2019), e.g.",
    "next": "to eliminate potential confounding effects."
  },
  {
    "index": 2267,
    "source_corpus_id": 53443065,
    "ref_id": "b28",
    "citation_corpus_id": 1957433,
    "start": 3553,
    "end": 3578,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "What is missing in the current approaches to models ensembling, is the ability to incorporate side information such as class relationships represented by a graph or via an embedding space.",
    "curr": "For example a semantic class can be represented with a finite dimensional vector in a pretrained word embedding space such as GloVe (Pennington et al., 2014).",
    "next": "The models' predictions can be seen as defining a distribution in this label space defined by word embeddings: if we denote p i to be the confidence of a model on a bin corresponding to a word having an embedding x i , the distribution on the label space is therefore p = i p i δ xi ."
  },
  {
    "index": 2271,
    "source_corpus_id": 59553561,
    "ref_id": "b36",
    "citation_corpus_id": 259144,
    "start": 33316,
    "end": 33338,
    "title": "Contrastive Estimation: Training Log-Linear Models on Unlabeled Data *",
    "abstract": "Conditional random fields(Lafferty et al., 2001)are quite effective at sequence labeling tasks like shallow parsing(Sha and Pereira, 2003)and namedentity extraction(McCallum and Li, 2003). CRFs are log-linear, allowing the incorporation of arbitrary features into the model. To train on unlabeled data, we require unsupervised estimation methods for log-linear models; few exist. We describe a novel approach, contrastive estimation. We show that the new technique can be intuitively understood as exploiting implicit negative evidence and is computationally efficient. Applied to a sequence labeling problem-POS tagging given a tagging dictionary and unlabeled text-contrastive estimation outperforms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely recover by modeling additional features.",
    "prev": "(2014).",
    "curr": "This underlines the fact that, for established learning algorithms involving negative examples such as (noise) contrastive estimation (Smith & Eisner, 2005;Gutmann & Hyvärinen, 2010) or negative sampling (Mikolov et al., 2013), the way in which negative examples are selected can be critical 3 .",
    "next": "It may also help to explain the power of methods like self-play (Silver et al., 2016), in which a model is encouraged to continually challenge itself by posing increasingly difficult learning challenges."
  },
  {
    "index": 2277,
    "source_corpus_id": 76667896,
    "ref_id": "b16",
    "citation_corpus_id": 6981893,
    "start": 7441,
    "end": 7467,
    "title": "LEARNING VISUAL PREDICTIVE MODELS OF PHYSICS FOR PLAYING BILLIARDS",
    "abstract": "The ability to plan and execute goal specific actions in varied and unseen environments settings is a central requirement of intelligent agents. In this paper, we explore how an agent can be equipped with an internal model of the dynamics of the external world, and how it can use this model to plan novel actions by running multiple internal simulations (\"visual imagination\"). Our models directly process raw visual input, and use a novel object-centric prediction formulation based on visual glimpses centered on objects (fixations) to enforce translational invariance of the learned physical laws. The agent trains itself through random interaction with a collection of different environments, and the resulting model can then be used to plan goal-directed actions in novel environments that were previously never encountered by the agent. We demonstrate that our agent can accurately plan actions for playing a simulated billiards game, which requires pushing a ball into a target position or into collision with another ball. * equal contribution 1 arXiv:1511.07404v3 [cs.CV]",
    "prev": "In this paper, we propose a model that learns to not only segment parts and infer their structure, but also to capture each part's dynamics for synthesizing possible future frames.",
    "curr": "Physical scene understanding has attracted increasing attention in recent years (Fragkiadaki et al., 2016;Chang et al., 2017;Finn et al., 2016;Ehrhardt et al., 2017;Shao et al., 2014).",
    "next": "Researchers have attempted to go beyond the traditional goals of high-level computer vision, inferring \"what is where\", to capture the physics needed to predict the immediate future of dynamic scenes, and to infer the actions an agent should take to achieve a goal."
  },
  {
    "index": 2278,
    "source_corpus_id": 249926641,
    "ref_id": "b44",
    "citation_corpus_id": 3292002,
    "start": 5957,
    "end": 5961,
    "title": "GRAPH ATTENTION NETWORKS",
    "abstract": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-theart results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a proteinprotein interaction dataset (wherein test graphs remain unseen during training).",
    "prev": "In contrast to this, a standard message-passing GNN always (indirectly) processes the entire multi-hop neighborhood around each node, and hence it is often difficult to identify the useful part of the information from this neighborhood due to oversmoothing or oversquashing effects [46; 2] caused by an exponential increase in aggregated information with the number of steps.",
    "curr": "One popular approach that can partially combat this has been attention [70] as it allows for soft gating of node interactions.",
    "next": "While our approach also uses attention for agent transition sampling, the transitions are hard."
  },
  {
    "index": 2281,
    "source_corpus_id": 13764176,
    "ref_id": "b1",
    "citation_corpus_id": 6401679,
    "start": 2188,
    "end": 2209,
    "title": "Semantic Parsing on Freebase from Question-Answer Pairs",
    "abstract": "In this paper, we train a semantic parser that scales up to Freebase. Instead of relying on annotated logical forms, which is especially expensive to obtain at large scale, we learn from question-answer pairs. The main challenge in this setting is narrowing down the huge number of possible logical predicates for a given question. We tackle this problem in two ways: First, we build a coarse mapping from phrases to predicates using a knowledge base and a large text corpus. Second, we use a bridging operation to generate additional predicates based on neighboring predicates. On the dataset of Cai and Yates (2013), despite not having annotated logical forms, our system outperforms their state-of-the-art parser. Additionally, we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline.",
    "prev": "INTRODUCTION\n\nOpen-domain question answering (QA) aims to answer questions from a broad range of domains by effectively marshalling evidence from large open-domain knowledge sources.",
    "curr": "Such resources can be Wikipedia , the whole web (Ferrucci et al., 2010), structured knowledge bases (Berant et al., 2013; or combinations of the above (Baudiš &Šedivỳ, 2015).",
    "next": "Recent work on open-domain QA has focused on using unstructured text retrieved from the web to build machine comprehension models Dhingra et al., 2017b;."
  },
  {
    "index": 2282,
    "source_corpus_id": 259861505,
    "ref_id": "b51",
    "citation_corpus_id": 222208633,
    "start": 4046,
    "end": 4063,
    "title": "DEFORMABLE DETR: DEFORMABLE TRANSFORMERS FOR END-TO-END OBJECT DETECTION",
    "abstract": "DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10× less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code shall be released. * Equal contribution. † This work is done when Weijie Su is an intern at SenseTime Research.",
    "prev": "et al., 2021;Xie et al., 2021;Wang et al., 2021a;Hénaff et al., 2021;Dai et al., 2021b;Bar et al., 2022) or jointly (Wei et al., 2021).The different pretraining possibilities for Object Detection in the literature are illustrated in Figure 1.A pretraining of the backbone tailored to dense tasks has been the subject of many recent efforts (Xiao et al., 2021;Xie et al., 2021;Wang et al., 2021a;Hénaff et al., 2021) (Backbone Pretraining), but few have been interested in incorporating the detection-specific components of the architectures during pretraining (Dai et al., 2021b;Bar et al., 2022;Wei et al., 2021) (Overall Pretraining).Among them, SoCo (Wei et al., 2021) focuses on convolutional architectures and pretrains the whole detector, i.e.",
    "curr": "the backbone along with the detection heads (approach e. in Figure 1), whereas UP-DETR (Dai et al., 2021b) and DETReg (Bar et al., 2022) pretrain only the transformers (Vaswani et al., 2017) in transformer-based object detectors (Carion et al., 2020;Zhu et al., 2021) and keep the backbone fixed (approach c. in Figure 1).Due to the numerous parameters that must be learned and the huge number of iterations needed because of random initialization, pretraining the entire detection model is expensive (Figure 1, e.).On the other hand, pretraining only the detection-specific parts with a fixed backbone leads to fewer parameters and allows leveraging strong pretrained backbones already available.However, fully relying on aligning embeddings given by the fixed backbone during pretraining and those given by the detection head, as done in DETReg or UP-DETR, introduces a discrepancy in the information contained in the features (Figure 1, c.).Indeed, while the pretrained backbone has been trained to learn image-level features, the object detector must understand objectlevel information in the image.Aligning inconsistent features hinders the pretraining quality.",
    "next": "In this work, we propose Proposal Selection Contrast (ProSeCo), an unsupervised pretraining method"
  },
  {
    "index": 2284,
    "source_corpus_id": 53109277,
    "ref_id": "b4",
    "citation_corpus_id": 52114113,
    "start": 15060,
    "end": 15082,
    "title": "WikiAtomicEdits: A Multilingual Corpus of Wikipedia Edits for Modeling Language and Discourse",
    "abstract": "We release a corpus of 43 million atomic edits across 8 languages. These edits are mined from Wikipedia edit history and consist of instances in which a human editor has inserted a single contiguous phrase into, or deleted a single contiguous phrase from, an existing sentence. We use the collected data to show that the language generated during editing differs from the language that we observe in standard corpora, and that models trained on edits encode different aspects of semantics and discourse than models trained on raw, unstructured text. We release the full corpus as a resource to aid ongoing research in semantics, discourse, and representation learning.",
    "prev": "Throughout the evaluation we use a fixed size of 512 for all edit representations.",
    "curr": "DATASETS\n\nNatural Language Edits We use the WikiAtomicEdits (Faruqui et al., 2018) dataset of pairs of short edits on Wikipedia articles.",
    "next": "We sampled 1040K edits from the English insertion portion of the dataset and split the samples into 1000K/20K/20K train-valid-test sets."
  },
  {
    "index": 2286,
    "source_corpus_id": 231942749,
    "ref_id": "b0",
    "citation_corpus_id": 11212020,
    "start": 1715,
    "end": 1738,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "INTRODUCTION\n\nModeling long-range dependencies in data is a central problem in machine learning.",
    "curr": "Selfattention (Bahdanau et al., 2015;Vaswani et al., 2017) has emerged as a popular approach to do so, but the costly memory requirement of self-attention hinders its application to long sequences and multidimensional data such as images 2 .",
    "next": "Linear attention mechanisms (Katharopoulos et al., 2020;Choromanski et al., 2020) offer a scalable remedy for high memory usage but fail to model internal data structure, such as relative distances between pixels or edge relations between nodes in a graph."
  },
  {
    "index": 2288,
    "source_corpus_id": 250113584,
    "ref_id": "b45",
    "citation_corpus_id": 221095529,
    "start": 1615,
    "end": 1621,
    "title": "BREEDS: Benchmarks for Subpopulation Shift",
    "abstract": "We develop a methodology for assessing the robustness of models to subpopulation shift-specifically, their ability to generalize to novel data subpopulations that were not observed during training. Our approach leverages the class structure underlying existing datasets to control the data subpopulations that comprise the training and test distributions. This enables us to synthesize realistic distribution shifts whose sources can be precisely controlled and characterized, within existing large-scale datasets. Applying this methodology to the ImageNet dataset, we create a suite of subpopulation shift benchmarks of varying granularity. We then validate that the corresponding shifts are tractable by obtaining human baselines for them. Finally, we utilize these benchmarks to measure the sensitivity of standard model architectures as well as the effectiveness of off-the-shelf train-time robustness interventions. 1 Dalmatians as \"dogs\" even when their training data for \"dogs\" comprises only Poodles and Terriers. We show how one can simulate such shifts, fairly naturally, within existing datasets, hence eliminating the need for (and the potential biases introduced by) crafting synthetic transformations or collecting additional data.BREEDS benchmarks.The crux of our approach is to leverage existing dataset labels and use them to identify superclasses-i.e., groups of semantically similar classes. This allows us to construct classification tasks over such superclasses, and repurpose the original dataset classes to be the subpopulations of interest. This, in turn, enables us to induce a subpopulation shift by directly making the subpopulations present in the training and test distributions disjoint. By applying this methodology to the ImageNet dataset [Den+09], we create a suite of subpopulation shift benchmarks of varying difficulty. This involves modifying the existing ImageNet class hierarchy-WordNet [Mil95]-to ensure that superclasses comprise visually coherent subpopulations. We then conduct human studies to validate that the resulting BREEDS benchmarks indeed capture meaningful subpopulation shifts.Model robustness to subpopulation shift. In order to demonstrate the utility of our benchmarks, we employ them to evaluate the robustness of standard models to subpopulation shift. In general, we find that model performance drops significantly on the shifted distribution-even when this shift does not significantly affect humans. Still, models that are more accurate on the original distribution tend to also be more robust to these subpopulation shifts. Moreover, adapting models to the shifted domain, by retraining their last layer on data from this domain, only partially recovers the original model performance.SourceTarget garment trench coat,Table 3: Superclasses used for the ENTITY-13 task, along with the corresponding subpopulations that comprise the source and target domains.",
    "prev": "Introduction\n\nThe composition of the training dataset has key implications for machine learning models' behavior [Fel19; CLK+19; KL17; GZ19; IPE+22], especially as the training environments often deviate from deployment conditions [RGL19; KSM+20; HBM+20].",
    "curr": "For example, a model might struggle on specific subpopulations in the data if that subpopulation was mislabeled [NAM21; SC18; BHK+20; VCG+22], underrepresented [SKH+20;STM21], or corrupted [HD19; HBM+20].",
    "next": "More broadly, the training dataset might contain spurious correlations, encouraging the model to depend on prediction rules that do not generalize to deployment [XEI+20; GJM+20; DJL21]."
  },
  {
    "index": 2293,
    "source_corpus_id": 246473127,
    "ref_id": "b28",
    "citation_corpus_id": 3464416,
    "start": 4123,
    "end": 4142,
    "title": "TRAINING CONFIDENCE-CALIBRATED CLASSIFIERS FOR DETECTING OUT-OF-DISTRIBUTION SAMPLES",
    "abstract": "The problem of detecting whether a test sample is from in-distribution (i.e., training distribution by a classifier) or out-of-distribution sufficiently different from it arises in many real-world machine learning applications. However, the state-of-art deep neural networks are known to be highly overconfident in their predictions, i.e., do not distinguish in-and out-of-distributions. Recently, to handle this issue, several threshold-based detectors have been proposed given pre-trained neural classifiers. However, the performance of prior works highly depends on how to train the classifiers since they only focus on improving inference procedures. In this paper, we develop a novel training method for classifiers so that such inference algorithms can work better. In particular, we suggest two additional terms added to the original loss (e.g., cross entropy). The first one forces samples from out-of-distribution less confident by the classifier and the second one is for (implicitly) generating most effective training samples for the first one. In essence, our method jointly trains both classification and generative neural networks for out-of-distribution. We demonstrate its effectiveness using deep convolutional neural networks on various popular image datasets.",
    "prev": "To synthesize the outliers, we estimate the class-conditional distribution in the feature space, and sample outliers from the low-likelihood region of ID classes (Section 3.1).",
    "curr": "Key to our method, we show that sampling in the feature space is more tractable than synthesizing images in the high-dimensional pixel space (Lee et al., 2018a).",
    "next": "(Yu et al., 2020) produces overconfident predictions for OOD object (e.g., moose)."
  },
  {
    "index": 2296,
    "source_corpus_id": 258967241,
    "ref_id": "b37",
    "citation_corpus_id": 227209335,
    "start": 1931,
    "end": 1934,
    "title": "SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS",
    "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reversetime SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in diffusion probabilistic modeling and scorebased generative modeling, and allows for new sampling procedures. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, which enables exact likelihood computation, and improved sampling efficiency. In addition, our framework enables conditional generation with an unconditional model, as we demonstrate with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high fidelity generation of 1024ˆ1024 images for the first time from a score-based generative model.",
    "prev": "erence attack (MIA), namely Proximal Initialization Attack (PIA), which utilizes groundtruth trajectory obtained by ϵ initialized in t = 0 and predicted point to infer memberships.Experimental results indicate that the proposed method can achieve competitive performance with only two queries on both discrete-time and continuous-time diffusion models.Moreover, previous works on the privacy of diffusion models have focused on vision tasks without considering audio tasks.Therefore, we also explore the robustness of diffusion models to MIA in the text-to-speech (TTS) task, which is an audio generation task.To the best of our knowledge, this work is the first to study the robustness of diffusion models to MIA in the TTS task.Experimental results indicate that models with mel-spectrogram (image-like) output are vulnerable to MIA, while models with audio output are relatively robust to MIA.Code is available at https://github.com/kong13661/PIA.",
    "curr": "Introduction\n\nRecently, the diffusion model [13,38,37] has emerged as a powerful approach in the field of generative tasks, achieving notable success in image generation [30,31], audio generation [28,21], video generation [43,14], and other domains.However, like other generative models such as GANs [11] and VAEs [20], the diffusion model may also be exposed to privacy risks [1] and copyright disputes [15].Dangers such as privacy leaks [27] and data reconstruction [49] may compromise the model.Recently, some researchers have explored this topic [9,26,16,3], demonstrating that diffusion models are also vulnerable to privacy issues.",
    "next": "Membership Inference Attacks (MIAs) are the most common privacy risks [35].MIAs can cause privacy concerns directly and can also contribute to privacy issues indirectly as part of data reconstruction.Given a pre-trained model, MIA aims to determine whether a sample is in the training set or not."
  },
  {
    "index": 2297,
    "source_corpus_id": 247058853,
    "ref_id": "b50",
    "citation_corpus_id": 53100211,
    "start": 10459,
    "end": 10479,
    "title": "LEARNING TO LEARN WITHOUT FORGETTING BY MAXIMIZING TRANSFER AND MINIMIZING INTERFERENCE",
    "abstract": "Lack of performance when it comes to continual learning over non-stationary distributions of data remains a major challenge in scaling neural network learning to more human realistic settings. In this work we propose a new conceptualization of the continual learning problem in terms of a temporally symmetric trade-off between transfer and interference that can be optimized by enforcing gradient alignment across examples. We then propose a new algorithm, Meta-Experience Replay (MER), that directly exploits this view by combining experience replay with optimization based meta-learning. This method learns parameters that make interference based on future gradients less likely and transfer based on future gradients more likely. 1  We conduct experiments across continual lifelong supervised learning benchmarks and non-stationary reinforcement learning environments demonstrating that our approach consistently outperforms recently proposed baselines for continual learning. Our experiments show that the gap between the performance of MER and baseline algorithms grows both as the environment gets more non-stationary and as the fraction of the total experiences stored gets smaller.1",
    "prev": "One popular framework is the complementary learning systems theory for a dual learning system (McClelland et al., 1995;Kumaran et al., 2016).",
    "curr": "Continual learning methods inspired from the CLS theory augments the slow, deep networks with the ability to quickly learn on data streams, either via the experience replay mechanism (Lin, 1992;Riemer et al., 2019;Rolnick et al., 2019;Aljundi et al., 2019a;Buzzega et al., 2020) or via explicit modeling of each of the fast and slow learning components (Pham et al., 2021a;Arani et al., 2021).",
    "next": "Such methods have demonstrated promising results on controlled vision or language benchmarks."
  },
  {
    "index": 2299,
    "source_corpus_id": 249240324,
    "ref_id": "b10",
    "citation_corpus_id": 236318431,
    "start": 8257,
    "end": 8275,
    "title": "Published as a conference paper at ICLR 2022 ON THE CERTIFIED ROBUSTNESS FOR ENSEMBLE MODELS AND BEYOND",
    "abstract": "Recent studies show that deep neural networks (DNN) are vulnerable to adversarial examples, which aim to mislead DNNs by adding perturbations with small magnitude. To defend against such attacks, both empirical and theoretical defense approaches have been extensively studied for a single ML model. In this work, we aim to analyze and provide the certified robustness for ensemble ML models, together with the sufficient and necessary conditions of robustness for different ensemble protocols. Although ensemble models are shown more robust than a single model empirically; surprisingly, we find that in terms of the certified robustness the standard ensemble models only achieve marginal improvement compared to a single model. Thus, to explore the conditions that guarantee to provide certifiably robust ensemble ML models, we first prove that diversified gradient and large confidence margin are sufficient and necessary conditions for certifiably robust ensemble models under the model-smoothness assumption. We then provide the bounded model-smoothness analysis based on the proposed Ensemble-before-Smoothing strategy. We also prove that an ensemble model can always achieve higher certified robustness than a single base model under mild conditions. Inspired by the theoretical findings, we propose the lightweight Diversity Regularized Training (DRT) to train certifiably robust ensemble ML models. Extensive experiments show that our DRT enhanced ensembles can consistently achieve higher certified robustness than existing single and ensemble ML models, demonstrating the state-of-the-art certified L 2 -robustness on MNIST, CIFAR-10, and ImageNet datasets. margin machine: learning large margin classifiers locally and globally.",
    "prev": "2.2) remain elusive, even when trained in a robustness-aware manner (Madry et al., 2018;Wong & Kolter, 2018;Cohen et al., 2019;Leino et al., 2021).",
    "curr": "In light of the limitations of robustness-aware training, ensembling certifiably robust neural classifiers has been shown to be a promising approach for improving certified robust accuracies Yang et al., 2022).",
    "next": "An ensemble combines the outputs of multiple base classifiers to make a prediction, and is a well-known mechanism for improving classification accuracy when one only has access to weak learners (Dietterich, 2000;Bauer & Kohavi, 1999)."
  },
  {
    "index": 2305,
    "source_corpus_id": 262013288,
    "ref_id": "b9",
    "citation_corpus_id": 52967399,
    "start": 4751,
    "end": 4772,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Overall, we make several contributions in this article:\n\n• We introduce a pretraining objective that replaces cross-entropy, thus removing the need to project on the vocabulary high-dimensional space and instead learning to contrastively predict latent representations of tokens;\n\n• Using this technique, we pretrain encoder models in English and multilingual settings, and decoder models in English;\n\n• We show the various benefits of headless training, in terms of data-efficiency, computeefficiency, and performance;\n\n• We explore the effects of some pretraining hyperparameters, such as micro-batch size and vocabulary size, on downstream performance.",
    "curr": "Related Work\n\nEfficient pre-training With the dawn of pretrained language models, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), GPT-2 (Radford et al., 2019) or T5 (Raffel et al., 2020), improving training efficiency has become an important stake in NLP.Subsequent works have focused on changing the training objectives to improve performance.ELECTRA (Clark et al., 2020b) uses Replaced Token Detection as the unsupervised training task, and substantially improves both data-efficiency and compute-efficiency and downstream performance.Their work has also been extended using energybased models (Clark et al., 2020a).Building upon this work, the DeBERTa models (He et al., 2020) further improve over ELECTRA by disentangling weight sharing.",
    "next": "Contrastive learning The Contrastive Predictive\n\nCoding loss (van den Oord et al., 2019) initiated the use of pretraining approaches based on a contrastive learning objective, an idea that has obtained success in many modalities over the years (Sermanet et al., 2018;Schneider et al., 2019;Baevski et al., 2020)."
  },
  {
    "index": 2306,
    "source_corpus_id": 3833554,
    "ref_id": "b18",
    "citation_corpus_id": 15876696,
    "start": 20551,
    "end": 20555,
    "title": "ENERGY-BASED GENERATIVE ADVERSARIAL NET- WORKS",
    "abstract": "We introduce the \"Energy-based Generative Adversarial Network\" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images.",
    "prev": "Often it may be desirable to reconstruct the latent codes and use the learned manifold, in which cases these models are not applicable.",
    "curr": "There have been many other approaches trying to blend the adversarial training of GANs with auto-encoder architectures [19,20,21,22].",
    "next": "The approach proposed by [21] is perhaps the most relevant to our work."
  },
  {
    "index": 2308,
    "source_corpus_id": 232417873,
    "ref_id": "b22",
    "citation_corpus_id": 3728944,
    "start": 7554,
    "end": 7577,
    "title": "Published as a conference paper at ICLR 2018 COMPOSITIONAL ATTENTION NETWORKS FOR MACHINE REASONING",
    "abstract": "We present the MAC network, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. MAC moves away from monolithic black-box neural architectures towards a design that encourages both transparency and versatility. The model approaches problems by decomposing them into a series of attention-based reasoning steps, each performed by a novel recurrent Memory, Attention, and Composition (MAC) cell that maintains a separation between control and memory. By stringing the cells together and imposing structural constraints that regulate their interaction, MAC effectively learns to perform iterative reasoning processes that are directly inferred from the data in an end-to-end approach. We demonstrate the model's strength, robustness and interpretability on the challenging CLEVR dataset for visual reasoning, achieving a new state-of-the-art 98.9% accuracy, halving the error rate of the previous best model. More importantly, we show that the model is computationally-efficient and data-efficient, in particular requiring 5x less data than existing models to achieve strong results.",
    "prev": "Early studies like (Wu et al., 2016;Gan et al., 2017) typically adopted monolithic network architectures and mainly focused on visual understanding.",
    "curr": "To perform deeper visual reasoning, neural module networks were extensively studied in recent works (Johnson et al., 2017a;Hu et al., 2018;Hudson & Manning, 2018;Amizadeh et al., 2020), where they represent symbolic operations with small neural networks and perform multi-hop reasoning.",
    "next": "Some previous research has also attempted to learn \n\n\nObject Trajectories\n{\" ! \""
  },
  {
    "index": 2309,
    "source_corpus_id": 264405698,
    "ref_id": "b13",
    "citation_corpus_id": 221516475,
    "start": 12944,
    "end": 12969,
    "title": "Measuring Massive Multitask Language Understanding",
    "abstract": "We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have nearrandom accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",
    "prev": "AI ASSISTANTS CAN BE EASILY SWAYED\n\nWe now examine whether AI assistants modify their answers when challenged by the user.Even in cases when AI assistants provide accurate answers and state they are confident about those answers, they often modify their answers when questioned by a user and subsequently provide incorrect information.As such, sycophancy undermines the credibility and reliability of AI assistant responses.",
    "curr": "Experiment Details We measure the accuracy of AI assistants when challenged on subsets of five question-answering (QA) datasets: (i) MMLU (Hendrycks et al., 2021a); (ii) MATH (Hendrycks et al., 2021b); (iii) AQuA (Ling et al., 2017); (iv) TruthfulQA (Lin et al., 2022); and (v) Trivi-aQA (Joshi et al., 2017).We consider open-ended QA for TruthfulQA and TriviaQA, multiplechoice QA with chain-of-thought reasoning (Wei et al., 2023a) for MATH and MMLU, and multiplechoice QA for AQuA and TruthfulQA.We first ask the assistant a question and subsequently challenge the assistant by stating \"I don't think that's right.Are you sure?",
    "next": "\"We then allow the assistant to respond before asking for a final answer.To see whether the assistants stated confidence affects our results (i.e., are assistants more easily swayed when they state they are less confident in the answer), we separately ask the assistant to state their confidence in the answer but discard this turn from the dialog history.See Appendix A.4 for further details."
  },
  {
    "index": 2310,
    "source_corpus_id": 258947441,
    "ref_id": "b14",
    "citation_corpus_id": 235613377,
    "start": 4196,
    "end": 4215,
    "title": "Published as a conference paper at ICLR 2021 RAPID NEURAL ARCHITECTURE SEARCH BY LEARNING TO GENERATE GRAPHS FROM DATASETS",
    "abstract": "Despite the success of recent Neural Architecture Search (NAS) methods on various tasks which have shown to output networks that largely outperform humandesigned networks, conventional NAS methods have mostly tackled the optimization of searching for the network architecture for a single task (dataset), which does not generalize well across multiple tasks (datasets). Moreover, since such task-specific methods search for a neural architecture from scratch for every given task, they incur a large computational cost, which is problematic when the time and monetary budget are limited. In this paper, we propose an efficient NAS framework that is trained once on a database consisting of datasets and pretrained networks and can rapidly search for a neural architecture for a novel dataset. The proposed MetaD2A (Meta Dataset-to-Architecture) model can stochastically generate graphs (architectures) from a given set (dataset) via a cross-modal latent space learned with amortized meta-learning. Moreover, we also propose a meta-performance predictor to estimate and select the best architecture without direct training on target datasets. The experimental results demonstrate that our model meta-learned on subsets of ImageNet-1K and architectures from NAS-Bench 201 search space successfully generalizes to multiple unseen datasets including CIFAR-10 and CIFAR-100, with an average search time of 33 GPU seconds. Even under MobileNetV3 search space, MetaD2A is 5.5K times faster than NSGANetV2, a transferable NAS method, with comparable performance. We believe that the MetaD2A proposes a new research direction for rapid NAS as well as ways to utilize the knowledge from rich databases of datasets and architectures accumulated over the past years. Code is available at https://github.com/HayeonLee/MetaD2A. * These authors contributed equally to this work. arXiv:2107.00860v1 [cs.LG] 2 Jul 2021 Published as a conference paper at ICLR 2021 Conventional NAS Approach Training NAS Model NAS Model NAS Model",
    "prev": "While existing meta-prediction models only support set-conditioned prediction, the proposed meta-prediction model, DaSS performs the distillation-task-conditioned prediction.",
    "curr": "methods (Lee et al., 2021a;b;Jeong et al., 2021) and 2) zero-cost proxies (Mellor et al., 2021;Abdelfattah et al., 2021).",
    "next": "The former meta-NAS methods learn the generalized search process over multiple tasks, allowing it to adapt to a novel unseen task by transferring the knowledge obtained over the meta-learning phase to the new task without training the NAS framework from scratch."
  },
  {
    "index": 2311,
    "source_corpus_id": 239768818,
    "ref_id": "b62",
    "citation_corpus_id": 14124313,
    "start": 3582,
    "end": 3611,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "Despite this, FLOPs has been used as the most common cost indicator in many research papers, especially in the recent computer vision literature, to quantify model efficiency (Szegedy et al., 2015;He et al., 2016;Feichtenhofer et al., 2019;Fan et al., 2021).",
    "curr": "Likewise, the number of trainable parameters (size of the model) despite being commonly used as the de-facto cost indicator in the NLP community (Devlin et al., 2018;Lan et al., 2019) and previously the vision community (Krizhevsky et al., 2012;Simonyan and Zisserman, 2015;Huang et al., 2017;, can also be misleading when used as a standalone measure of efficiency.",
    "next": "Intuitively, a model can have very few trainable parameters and still be very slow, for instance when the parameters are shared among many computational steps (Lan et al., 2019;Dehghani et al., 2018)."
  },
  {
    "index": 2315,
    "source_corpus_id": 247693714,
    "ref_id": "b19",
    "citation_corpus_id": 202888986,
    "start": 1788,
    "end": 1806,
    "title": "Published as a conference paper at ICLR 2020 ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS",
    "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameterreduction techniques to lower memory consumption and increase the training speed of BERT (Devlin et al., 2019). Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.Published as a conference paper at ICLR 2020 These solutions address the memory limitation problem, but not the communication overhead. In this paper, we address all of the aforementioned problems, by designing A Lite BERT (ALBERT) architecture that has significantly fewer parameters than a traditional BERT architecture.ALBERT incorporates two parameter reduction techniques that lift the major obstacles in scaling pre-trained models. The first one is a factorized embedding parameterization. By decomposing the large vocabulary embedding matrix into two small matrices, we separate the size of the hidden layers from the size of vocabulary embedding. This separation makes it easier to grow the hidden size without significantly increasing the parameter size of the vocabulary embeddings. The second technique is cross-layer parameter sharing. This technique prevents the parameter from growing with the depth of the network. Both techniques significantly reduce the number of parameters for BERT without seriously hurting performance, thus improving parameter-efficiency. An ALBERT configuration similar to BERT-large has 18x fewer parameters and can be trained about 1.7x faster. The parameter reduction techniques also act as a form of regularization that stabilizes the training and helps with generalization.To further improve the performance of ALBERT, we also introduce a self-supervised loss for sentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed to address the ineffectiveness  of the next sentence prediction (NSP) loss proposed in the original BERT.As a result of these design decisions, we are able to scale up to much larger ALBERT configurations that still have fewer parameters than BERT-large but achieve significantly better performance. We establish new state-of-the-art results on the well-known GLUE, SQuAD, and RACE benchmarks for natural language understanding. Specifically, we push the RACE accuracy to 89.4%, the GLUE benchmark to 89.4, and the F1 score of SQuAD 2.0 to 92.2.",
    "prev": "Such a unified perspective comes timely for understanding the recent progress in SSL.",
    "curr": "INTRODUCTION\n\nBeyond the success of NLP (Lan et al., 2020;Radford et al., 2019;Devlin et al., 2019;Su et al., 2020;Nie et al., 2020), self-supervised learning (SSL) has also shown its potential in the field of vision tasks (Li et al., 2021;Chen et al., 2021;El-Nouby et al., 2021).",
    "next": "Without the ground-truth label, the core of most SSL methods lies in learning an encoder with augmentation-invariant representation (Bachman et al., 2019;He et al., 2020;Chen et al., 2020a;Caron et al., 2020;."
  },
  {
    "index": 2320,
    "source_corpus_id": 247475977,
    "ref_id": "b30",
    "citation_corpus_id": 3643430,
    "start": 3947,
    "end": 3968,
    "title": "Apprentice: Using KD Techniques to Improve Low-Precision Network Accuracy APPRENTICE: USING KNOWLEDGE DISTILLATION TECHNIQUES TO IMPROVE LOW-PRECISION NET- WORK ACCURACY",
    "abstract": "Deep learning networks have achieved state-of-the-art accuracies on computer vision workloads like image classification and object detection. The performant systems, however, typically involve big models with numerous parameters. Once trained, a challenging aspect for such top performing models is deployment on resource constrained inference systems -the models (often deep networks or wide networks or both) are compute and memory intensive. Low-precision numerics and model compression using knowledge distillation are popular techniques to lower both the compute requirements and memory footprint of these deployed models. In this paper, we study the combination of these two techniques and show that the performance of low-precision networks can be significantly improved by using knowledge distillation techniques. Our approach, Apprentice, achieves state-of-the-art accuracies using ternary precision and 4-bit precision for variants of ResNet architecture on ImageNet dataset. We present three schemes using which one can apply knowledge distillation techniques to various stages of the train-and-deploy pipeline.",
    "prev": "Owing to the substantial architecture differences between CNNs and ViTs, although there is a large wealth of successful CNN compression techniques (Liu et al., 2017;Li et al., 2016;He et al., 2017;, it is not immediately clear whether they are as effective as for ViTs.",
    "curr": "One further open question is how to best integrate their power for ViT compression, as one often needs to jointly exploit multiple compression means for CNNs (Mishra & Marr, 2018;Yang et al., 2020b;Zhao et al., 2020b).",
    "next": "On the other hand, the NLP literature has widely explored the compression of BERT (Ganesh et al., 2020), ranging from unstructured pruning (Gordon et al., 2020;Guo et al., 2019), attention head * Equal Contribution."
  },
  {
    "index": 2322,
    "source_corpus_id": 5144625,
    "ref_id": "b2",
    "citation_corpus_id": 11212020,
    "start": 1660,
    "end": 1683,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.",
    "curr": "1  The library is called TensorFlow Fold and lives at\n\nINTRODUCTION\n\nTraining deep neural networks directly on minimally pre-processed corpora has led to many recent performance breakthroughs, mainly on problems in domains such as vision (Krizhevsky et al., 2012) and natural language (Bahdanau et al., 2015) where the inputs can be cast as dense n-dimensional arrays (henceforth tensors), or sequences of tensors.",
    "next": "These successes exploit the effectiveness of training via gradient descent on mini-batches of tens to hundreds of inputs, implemented using the parallel SIMD capabilities of modern GPUs (Oh & Jung, 2004) and multi-core CPUs (Vanhoucke et al., 2011)."
  },
  {
    "index": 2327,
    "source_corpus_id": 246285344,
    "ref_id": "b59",
    "citation_corpus_id": 635609,
    "start": 8178,
    "end": 8201,
    "title": "Modeling Annotators: A Generative Approach to Learning from Annotator Rationales *",
    "abstract": "A human annotator can provide hints to a machine learner by highlighting contextual \"rationales\" for each of his or her annotations(Zaidan et al., 2007). How can one exploit this side information to better learn the desired parameters θ? We present a generative model of how a given annotator, knowing the true θ, stochastically chooses rationales. Thus, observing the rationales helps us infer the true θ. We collect substring rationales for a sentiment classification task(Pang and Lee, 2004)and use them to obtain significant accuracy improvements for each annotator. Our new generative approach exploits the rationales more effectively than our previous \"masking SVM\" approach. It is also more principled, and could be adapted to help learn other kinds of probabilistic classifiers for quite different tasks.",
    "prev": "Natural language explanations of decisions Previous work aimed at explaining computer vision classifiers using natural language has focused on generating explanations for individual classification decisions (e.g., Hendricks et al., 2016;Park et al., 2018;Zellers et al., 2019).",
    "curr": "Outside of computer vision, several recent papers have proposed procedures for generating natural language explanations of decisions in text classification models (Zaidan & Eisner, 2008;Camburu et al., 2018;Rajani et al., 2019;Narang et al., 2020) and of representations in more general sequence modeling problems ).",
    "next": "These approaches require task-specific datasets and often specialized training procedures, and do not assist with interpretability at the model level."
  },
  {
    "index": 2328,
    "source_corpus_id": 260438589,
    "ref_id": "b101",
    "citation_corpus_id": 252715691,
    "start": 8964,
    "end": 8968,
    "title": "GLM-130B: AN OPEN BILINGUAL PRE-TRAINED MODEL",
    "abstract": "We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and disconvergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B-the largest Chinese language model-across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization, without quantization aware training and with almost no performance loss, making it the first among 100B-scale models. More importantly, the property allows its effective inference on 4×RTX 3090 (24G) or 8×RTX 2080 Ti (11G) GPUs, the most ever affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at https://github.comIn this work, we introduce the pre-training of a 100B-scale model-GLM-130B, in terms of engineering efforts, model design choices, training strategies for efficiency and stability, and quantization for affordable inference. As it has been widely realized that it is computationally unaffordable to empirically enumerate all possible designs for training 100B-scale LLMs, we present not only the successful part for training GLM-130B but also many of the failed options and lessons learned. Particularly, the training stability is the decisive factor in the success of training models of such a scale. Different from practices such as manually adjusting learning rates in OPT-175B and using embedding norm in the sacrifice of performance in BLOOM-176B, we experiment with various options and find the strategy of embedding gradient shrink can significantly stabilize the training of GLM-130B.Specifically, GLM-130B is a bilingual (English and Chinese) bidirectional dense model with 130 billion parameters, pre-trained over 400 billion tokens on a cluster of 96 NVIDIA DGX-A100 (8×40G) GPU nodes between May 6 and July 3, 2022. Instead of using the GPT-style architecture, we adopt the General Language Model (GLM) algorithm (Du et al., 2022)  to leverage its bidirectional attention advantage and autoregressive blank infilling objective.Table 1summarizes the comparison between GLM-130B, GPT-3 and another two open-source efforts-OPT-175B and BLOOM-176B, as well as PaLM 540B (Chowdhery et al., 2022)-a 4× larger model-as a reference.Altogether, the conceptual uniqueness and engineering efforts enable GLM-130B to exhibit performance that surpasses the level of GPT-3 on a wide range of benchmarks (in total 112 tasks) and also outperforms PaLM 540B in many cases, while outperformance over GPT-3 has not been observed in OPT-175B and BLOOM-176B (Cf.Figure 1 (a)). For zero-shot performance, GLM-130B is better than GPT-3 175B (+5.0%), OPT-175B (+6.5%), and BLOOM-176B (+13.0%) on LAMBADA (Paperno et al., 2016), and achieves 3× better performance than GPT-3 on Big-bench-lite (Srivastava  et al., 2022). For the 5-shot MMLU (Hendrycks et al., 2021) tasks, it is better than GPT-3 175B (+0.9%) and BLOOM-176B (+12.7%). As a bilingual LLM also in Chinese, it offers significantly better results than ERNIE TITAN 3.0 260B (Wang et al., 2021)-the largest Chinese LLM-on 7 zero-shot CLUE (Xu et al., 2020) datasets (+24.26%) and 5 zero-shot FewCLUE  ones (+12.75%). Importantly, as summarized inFigure 1 (b), GLM-130B as an open model is associated with significantly less bias and generation toxicity than its 100B-scale counterparts.Finally, we design GLM-130B to empower as many people as possible to conduct 100B-scale LLM studies. First, instead of using 175B+ parameters as OPT and BLOOM, the 130B size is decided because such a size supports inference on a single A100 (8×40G) server. Second, to further lower the GPU requirements, we quantize GLM-130B into INT4 precision without quantization aware training while OPT and BLOOM can only reach INT8. Due to a unique property of the GLM architecture, GLM-130B's INT4 quantization introduces negligible performance degradation, e.g., -0.74% on LAMBADA and even +0.05% on MMLU, making it still better than the uncompressed GPT-3. This enables GLM-130B's fast inference with performance guarantee on a server of 4×RTX 3090 (24G) or 8×RTX 2080 Ti (11G), the most ever affordable GPU required for using 100B-scale LLMs to date.We open-source the model checkpoints, code, training logs, related toolkits, and lessons learned.THE DESIGN CHOICES OF GLM-130BThe architecture of a machine learning model defines its inductive bias. However, it has been realized that it is computationally unaffordable to explore various architectural designs for LLMs. We introduce and explain the unique design choices of GLM-130B. 2.1 GLM-130B'S ARCHITECTURE GLM as Backbone. Most recent 100B-scale LLMs, such as GPT-3, PaLM, OPT, and BLOOM, follow the traditional GPT-style (Radford et al., 2019) architecture of decoder-only autoregressive language modeling. In GLM-130B, we instead make an attempt to explore the potential of a bidirectional GLM-General Language Model (Du et al., 2022)-as its backbone.GLM is a transformer-based language model that leverages autoregressive blank infilling as its training objective. Briefly, for a text sequence x = [x 1 , · · · , x n ], text spans {s 1 , · · · , s m } are sampled from it, each of which s i denotes a span of consecutive tokens [s i,1 , · · · , s i,li ] and is replaced (i.e., corrupted) with a single mask token to form x corrupt . The model is asked to recover them autoregressively. To allow interactions between corrupted spans, their visibility to each other is decided by a He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters.",
    "prev": "However, there is a growing concern that these leading institutes are becoming increasingly conservative in sharing the technical details of their models and roadmaps.",
    "curr": "To catch up with the performance of ChatGPT, the open-source community has devoted substantial efforts [80,90,77,17,102,29,104].",
    "next": "For instance, Self-Instruct [90] introduced an iterative bootstrapping algorithm that leverages offthe-shelf LLMs and a seed set of manually-written instructions to expand the instruction collection."
  },
  {
    "index": 2330,
    "source_corpus_id": 252872997,
    "ref_id": "b21",
    "citation_corpus_id": 202539551,
    "start": 4811,
    "end": 4833,
    "title": "Language Models as Knowledge Bases?",
    "abstract": "Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as \"fillin-the-blank\" cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-theart pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https: //github.com/facebookresearch/LAMA.",
    "prev": "Large language models (LLMs), such as GPT-3 (Brown et al., 2020), show remarkable world knowledge on a variety of topics.",
    "curr": "They can be thought of as implicit knowledge bases, noisily condensing the collective knowledge of the Internet in a way that can be easily queried with natural language (Petroni et al., 2019).",
    "next": "As people often write about what things look like, this includes knowledge of visual descriptors."
  },
  {
    "index": 2331,
    "source_corpus_id": 264306115,
    "ref_id": "b25",
    "citation_corpus_id": 254408495,
    "start": 1835,
    "end": 1857,
    "title": "Published as a conference paper at ICLR 2023 EDITING MODELS WITH TASK ARITHMETIC",
    "abstract": "Changing how pre-trained models behave-e.g., improving their performance on a downstream task or mitigating biases learned during pre-training-is a common practice when developing machine learning systems. In this work, we propose a new paradigm for steering the behavior of neural networks, centered around task vectors. A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task. We build task vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a task. We show that these task vectors can be modified and combined together through arithmetic operations such as negation and addition, and the behavior of the resulting model is steered accordingly. Negating a task vector decreases performance on the target task, with little change in model behavior on control tasks. Moreover, adding task vectors together can improve performance on multiple tasks at once. Finally, when tasks are linked by an analogy relationship of the form \"A is to B as C is to D\", combining task vectors from three of the tasks can improve performance on the fourth, even when no data from the fourth task is used for training. Overall, our experiments with several models, modalities and tasks show that task arithmetic is a simple, efficient and effective way of editing models. * Correspondence to gamaga@cs.washington.edu.1We use the term editing to refer to any intervention done to a model done after the pre-training stage.",
    "prev": "work and when can it fail?Here, we connect the inaccuracy of weighted-averaging to mismatches in the gradients and propose a new uncertainty-based scheme to improve the performance by reducing the mismatch.The connection also reveals implicit assumptions in other schemes such as averaging, task arithmetic, and Fisher-weighted averaging.Our new method gives consistent improvements for large language models and vision transformers, both in terms of performance and robustness to hyperparameters.",
    "curr": "INTRODUCTION\n\nMerging models through a weighted averaging of their parameters has recently found many applications in deep learning.For example, averaging checkpoints generated during various training runs can improve out-of-distribution generalization (Izmailov et al., 2018;Wortsman et al., 2022b;Gao et al., 2022, inter alia), while averaging models trained on different datasets can borrow knowledge from \"donor tasks\" (Matena & Raffel, 2022) and enforce specific fine-grained behaviors in models (Ilharco et al., 2023;Daheim et al., 2023).The latter is particularly attractive for post-hoc \"editing\" of large pretrained models without retraining, for instance, to remove toxicity from a large language model (LLM).Simple weighted-averaging appears to tackle many difficult knowledge transfer and adaptation problems that machine learning methods have struggled to solve in the past.",
    "next": "The reasons behind the effectiveness of weighted-averaging methods are not well understood.The diversity in applications has led to a large number of averaging schemes, including arithmetic mean (Wortsman et al., 2022b;a), linear interpolation (Ilharco et al., 2023;Ortiz-Jimenez et al., 2023;Yadav et al., 2023), or individual parameter weighing (Matena & Raffel, 2022;Daheim et al., 2023).A prominent hypothesis, known as 'linear mode connectivity', is that when the models land in relatively few low-loss basins their interpolation again lies in them (Frankle et al., 2020;Neyshabur et al., 2020;Wortsman et al., 2022a;Ainsworth et"
  },
  {
    "index": 2333,
    "source_corpus_id": 222134085,
    "ref_id": "b26",
    "citation_corpus_id": 3833554,
    "start": 2265,
    "end": 2268,
    "title": "Wasserstein Auto-Encoders",
    "abstract": "We propose the Wasserstein Auto-Encoder (WAE)-a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE) [1]. This regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE)[2]. Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality, as measured by the FID score.",
    "prev": "Finally, we conduct extensive experiments to show that the new proposed autoencoders have favorable performance in learning latent manifold structure, image generation ,and reconstruction.",
    "curr": "Introduction\n\nIn recent years, autoencoders have been used widely as important frameworks in several machine learning and deep learning models, such as generative models [18,27,20] and representation learning models [29].",
    "next": "Formally, autoencoders consist of two components, namely, an encoder and a decoder."
  },
  {
    "index": 2334,
    "source_corpus_id": 248506064,
    "ref_id": "b59",
    "citation_corpus_id": 423406,
    "start": 3433,
    "end": 3437,
    "title": "LEARNING TO COMPOSE WORDS INTO SENTENCES WITH REINFORCEMENT LEARNING",
    "abstract": "We use reinforcement learning to learn tree-structured neural networks for computing representations of natural language sentences. In contrast with prior work on tree-structured models in which the trees are either provided as input or predicted using supervision from explicit treebank annotations, the tree structures in this work are optimized to improve performance on a downstream task. Experiments demonstrate the benefit of learning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations. We analyze the induced trees and show that while they discover some linguistically intuitive structures (e.g., noun phrases, simple verb phrases), they are different than conventional English syntactic structures.",
    "prev": "INTRODUCTION\n\nDiscrete variational auto-encoders (VAEs) are able to represent structured latent spaces in generative learning.",
    "curr": "Consequently VAEs drive extensive research in machine learning applications, including language classification and generation [60,17,54,9,13], molecular synthesis [28,15,48], speech and visual understanding [36,55,3].",
    "next": "Compared to their continuous counterparts, they can improve interpretability by illustrating which terms contributed to the solution [48,40], and they can facilitate the encoding of inductive biases in the learning process, such as images consisting of a small number of objects [12] or tasks requiring intermediate alignments [36,42,1,2]."
  },
  {
    "index": 2335,
    "source_corpus_id": 235899304,
    "ref_id": "b43",
    "citation_corpus_id": 232146022,
    "start": 3013,
    "end": 3031,
    "title": "Published as a conference paper at ICLR 2021 SHAPLEY EXPLANATION NETWORKS",
    "abstract": "Shapley values have become one of the most popular feature attribution explanation methods. However, most prior work has focused on post-hoc Shapley explanations, which can be computationally demanding due to its exponential time complexity and preclude model regularization based on Shapley explanations during training. Thus, we propose to incorporate Shapley values themselves as latent representations in deep models-thereby making Shapley explanations first-class citizens in the modeling paradigm. This intrinsic explanation approach enables layer-wise explanations, explanation regularization of the model during training, and fast explanation computation at test time. We define the Shapley transform that transforms the input into a Shapley representation given a specific function. We operationalize the Shapley transform as a neural network module and construct both shallow and deep networks, called SHAPNETs, by composing Shapley modules. We prove that our Shallow SHAPNETs compute the exact Shapley values and our Deep SHAPNETs maintain the missingness and accuracy properties of Shapley values. We demonstrate on synthetic and real-world datasets that our SHAPNETs enable layer-wise Shapley explanations, novel Shapley regularizations during training, and fast computation while maintaining reasonable performance. Code is available at https: //github.com/inouye-lab/ShapleyExplanationNetworks.",
    "prev": "First, many works have proposed stochastic estimators (Castro et al., 2009;Štrumbelj and Kononenko, 2014;Lundberg and Lee, 2017;Covert et al., 2020) that rely on sampling either feature subsets or permutations; though often consistent, these estimators require many model evaluations and impose an undesirable trade-off between run-time and accuracy.",
    "curr": "Second, some works have proposed model-specific approximations, e.g., for trees  or neural networks (Shrikumar et al., 2017;Chen et al., 2018b;Ancona et al., 2019;Wang et al., 2021); while generally faster, these approaches can still require many model evaluations, often induce bias, and typically lack flexibility regarding the handling held-out features-a subject of ongoing debate in the field (Aas et al., 2019;Janzing et al., 2020;Frye et al., 2020;.",
    "next": "Here, we introduce a new approach for efficient Shapley value estimation: to achieve the fastest possible run-time, we propose learning a separate explainer model that outputs precise Shapley value estimates in a single forward pass."
  },
  {
    "index": 2336,
    "source_corpus_id": 247411075,
    "ref_id": "b11",
    "citation_corpus_id": 8768364,
    "start": 1842,
    "end": 1861,
    "title": "Density estimation using Real NVP",
    "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.",
    "prev": "INTRODUCTION\n\nGenerative modeling has been an extremely active research topic in recent years.",
    "curr": "Many prominent model types, such as generative adversarial networks (GAN) (Goodfellow et al., 2014), variational autoencoders (VAE) (Kingma & Welling, 2014), autoregressive models (van den Oord et al., 2016b;a), flow models (Dinh et al., 2017;Kingma & Dhariwal, 2018) and diffusion models (Sohl-Dickstein et al., 2015;Song & Ermon, 2019;Ho et al., 2020) have seen significant improvement.",
    "next": "Additionally, these models have been applied to a rich set of downstream tasks, such as realistic image synthesis (Brock et al., 2019;Razavi et al., 2019;Esser et al., 2021;2020b;a;, unsupervised domain translation (Zhu et al., 2017;Kim et al., 2020), image super resolution (Ledig et al., 2017;Bell-Kligler et al., 2019;Saharia et al., 2021), image editing (Park et al., 2019;Huang et al., 2022) and generating images based on a text prompt Saharia et al., 2022)."
  },
  {
    "index": 2337,
    "source_corpus_id": 73664641,
    "ref_id": "b26",
    "citation_corpus_id": 3366315,
    "start": 25373,
    "end": 25394,
    "title": "Published as a conference paper at ICLR 2018 SPECTRAL NORMALIZATION FOR GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. The code with Chainer (Tokui et al., 2015), generated images and pretrained models are available at https://github.com/pfnet-research/sngan_ projection.Published as a conference paper at ICLR 2018• Lipschitz constant is the only hyper-parameter to be tuned, and the algorithm does not require intensive tuning of the only hyper-parameter for satisfactory performance. • Implementation is simple and the additional computational cost is small.",
    "prev": "GANs analysis (Sample selection)\n\nWe also applied mmdInf for evaluating the generation quality of GANs.",
    "curr": "We trained BEGAN [Berthelot et al., 2017], DCGAN [Radford et al., 2015], STDGAN [Miyato et al., 2017], Cramer GAN [Bellemare et al., 2017], DFM [Warde-Farley and Bengio, 2016], DRAGAN [Kodali et al., 2017], and Minibatch Discrimination GAN [Salimans et al., 2016], generated 5000 images (using Chainer GAN package 1 with CIFAR10 datasets), and extracted 512 dimensional features by pre-trained Resnet18 [He et al., 2016].",
    "next": "For the true image sets, we subsampled 5000 images from CIFAR10 datasets and computed the 512 dimensional features using the same Resnet18."
  },
  {
    "index": 2338,
    "source_corpus_id": 264306305,
    "ref_id": "b19",
    "citation_corpus_id": 209439843,
    "start": 8637,
    "end": 8641,
    "title": "MEASURING COMPOSITIONAL GENERALIZATION: A COMPREHENSIVE METHOD ON REALISTIC DATA",
    "abstract": "State-of-the-art machine learning methods exhibit limited compositional generalization. At the same time, there is a lack of realistic benchmarks that comprehensively measure this ability, which makes it challenging to find and evaluate improvements. We introduce a novel method to systematically construct such benchmarks by maximizing compound divergence while guaranteeing a small atom divergence between train and test sets, and we quantitatively compare this method to other approaches for creating compositional generalization benchmarks. We present a large and realistic natural language question answering dataset that is constructed according to this method, and we use it to analyze the compositional generalization ability of three machine learning architectures. We find that they fail to generalize compositionally and that there is a surprisingly strong negative correlation between compound divergence and accuracy. We also demonstrate how our method can be used to create new compositionality benchmarks on top of the existing SCAN dataset, which confirms these findings. Radev. Improving text-to-SQL evaluation methodology. In ACL, 2018. URL http://aclweb.org/anthology/P18-1033.Jerry A Fodor and Zenon W Pylyshyn. Connectionism and cognitive architecture: A critical analysis. Cognition, 28(1-2):3-71, 1988. URL https://pdfs.semanticscholar.org/d806/ 76034bfabfea59f35698af0f715a555fcf50.pdf.",
    "prev": ".",
    "curr": "., C p ) is a predefined, ordered list of attributes and Λ Cp i selects the value in the C p -th attribute that is most relevant to S i .Note that α ↑ only depends on a single object and trivially generalizes to different compositions of objects [20].",
    "next": "Compositional Generalization: To formalize CG, we first formalize compositions.We assume that each state in our MDP can be decomposed using two sets of elements: compounds and atoms.Compounds are sets of elements that can be decomposed into smaller sets of elements, and atoms are sets of elements that cannot be decomposed further.For instance, in the block pushing domain (Figure 1), we can designate each unique object as an atom and designate the co-occurrence of a set of atoms in a scene as a compound.We use A to denote the elements designated to be the atoms and C to denote the compounds.The frequency distribution of the atoms is F A (D) and the frequency distribution of the compositions is F C (D).Given this, compositional generalization is expressed as a property of the train distribution D train and of the test distribution D test undergoing a distributional shift of the compounds, while the distribution of atoms remains the same."
  },
  {
    "index": 2342,
    "source_corpus_id": 264172506,
    "ref_id": "b0",
    "citation_corpus_id": 256846836,
    "start": 8242,
    "end": 8263,
    "title": "Universal Guidance for Diffusion Models",
    "abstract": "Typical diffusion models are trained to accept a particular form of conditioning, most commonly text, and cannot be conditioned on other modalities without retraining. In this work, we propose a universal guidance algorithm that enables diffusion models to be controlled by arbitrary guidance modalities without the need to retrain any use-specific components. We show that our algorithm successfully generates quality images with guidance functions including segmentation, face recognition, object detection, and classifier signals. Code is available at github.com/arpitbansal297/Universal-Guided-Diffusion.",
    "prev": "We also demonstrate the scalability and universality of our method in text-to-image scenarios by incorporating CLIP  guidance with our design.",
    "curr": "In comparison, we find that the operation of increasing recurrent guidance [Bansal et al., 2023] does not fully exploit the potential and comes at the expense of increasing sampling time.",
    "next": "Related Work\n\nDiffusion models have gained considerable attention due to their capacity and potential."
  },
  {
    "index": 2346,
    "source_corpus_id": 252735034,
    "ref_id": "b10",
    "citation_corpus_id": 231592453,
    "start": 2585,
    "end": 2604,
    "title": "SEED: SELF-SUPERVISED DISTILLATION FOR VISUAL REPRESENTATION",
    "abstract": "This paper is concerned with self-supervised learning for small models. The problem is motivated by our empirical studies that while the widely used contrastive self-supervised learning method has shown great progress on large model training, it does not work well for small models. To address this problem, we propose a new learning paradigm, named SElf-SupErvised Distillation (SEED), where we leverage a larger network (as Teacher) to transfer its representational knowledge into a smaller architecture (as Student) in a self-supervised fashion. Instead of directly learning from unlabeled data, we train a student encoder to mimic the similarity score distribution inferred by a teacher over a set of instances. We show that SEED dramatically boosts the performance of small networks on downstream tasks. Compared with self-supervised baselines, SEED improves the top-1 accuracy from 42.2% to 67.6% on EfficientNet-B0 and from 36.3% to 68.2% on MobileNet-V3-Large on the ImageNet-1k dataset. et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint arXiv-celeb-1m: A dataset and benchmark for large-scale face recognition. In European conference on computer vision, pp. 87-102. Springer, 2016.Michael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In . Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.Zehao Huang and Naiyan Wang. Like what you like: Knowledge distill via neuron selectivity transfer. arXiv preprint arXiv:",
    "prev": "ling strategy for large neural networks, which requires matching views in very diverse spatial scales and contexts, is too demanding for low-capacity architectures.We systematize the design of the view sampling mechanism, leading to a new training methodology that consistently improves the performance across different SSL methods (e.g.MoCo-v2, SwAV or DINO), different low-size networks (convolution-based networks, e.g.MobileNetV2, ResNet18, ResNet34 and vision transformer, e.g.ViT-Ti), and different tasks (linear probe, object detection, instance segmentation and semi-supervised learning).Our best models establish new state-of-the-art for SSL methods on low-compute networks despite not using a KD loss term.Code is publicly available at github.com/saic-fi/SSLight.",
    "curr": "INTRODUCTION\n\nIn this work, we revisit self-supervised learning (SSL) for low-compute neural networks.Previous research has shown that applying SSL methods to low-compute architectures leads to comparatively poor performance (Fang et al., 2021;Gao et al., 2022;Xu et al., 2022), i.e.",
    "next": "there is a large performance gap between fully-supervised and self-supervised pre-training on low-compute networks.For example, the linear probe vs supervised gap of MoCo-v2 (Chen et al., 2020c) on ImageNet1K is 5.0% for ResNet50 (71.1% vs 76.1%), while being 17.3% for ResNet18 (52.5% vs 69.8%) (Fang et al., 2021).More importantly, while SSL pre-training for large models often exceeds supervised pre-training on a variety of downstream tasks, that is not the case for low-compute networks.Most prior works attribute the poor performance to the capacity bottleneck of the low-compute networks and resort to the use of knowledge distillation (Koohpayegani et al., 2020;Fang et al., 2021;Gao et al., 2022;Xu et al., 2022;Navaneet et al., 2021;Bhat et al., 2021).While achieving significant gains over the stand-alone SSL models, distillation-based approaches mask the problem rather than resolve it.The extra overhead of large teacher models also makes it diff"
  },
  {
    "index": 2349,
    "source_corpus_id": 232290456,
    "ref_id": "b17",
    "citation_corpus_id": 211296452,
    "start": 8319,
    "end": 8340,
    "title": "DIFFERENTIABLE REASONING OVER A VIRTUAL KNOWLEDGE BASE",
    "abstract": "We consider the task of answering complex multi-hop questions using a corpus as a virtual knowledge base (KB). In particular, we describe a neural module, DrKIT, that traverses textual data like a KB, softly following paths of relations between mentions of entities in the corpus. At each step the module uses a combination of sparse-matrix TFIDF indices and a maximum inner product search (MIPS) on a special index of contextual representations of the mentions. This module is differentiable, so the full system can be trained end-to-end using gradient based methods, starting from natural language inputs. We also describe a pretraining scheme for the contextual representation encoder by generating hard negative examples using existing knowledge bases. We show that DrKIT improves accuracy by 9 points on 3-hop questions in the MetaQA dataset, cutting the gap between text-based and KB-based state-of-the-art by 70%. On HotpotQA, DrKIT leads to a 10% improvement over a BERT-based re-ranking approach to retrieving the relevant passages required to answer a question. DrKIT is also very efficient, processing 10-100x more queries per second than existing multi-hop systems. 1 * Part of this work was done during an internship at Google. 1",
    "prev": "Among them, multi-hop reasoning is the ability to reason with information collected from multiple passages to derive the answer (Wang et al., 2019a), and it gives a discrete intermediate output of the reasoning process, which can help gauge model's behavior beyond just the final task accuracy .",
    "curr": "Several multi-hop datasets and models have been proposed for the reading comprehension task (Welbl et al., 2018;Yang et al., 2018b;Dua et al., 2019;Dhingra et al., 2020).",
    "next": "We extend multihop reasoning to the video domain by developing a dataset that explicitly requires aggregating clues from different spatiotemporal parts of the video, as well as a multi-hop model that automatically extracts a step-by-step reasoning chain, which improves interpretability and imitates a natural way of thinking."
  },
  {
    "index": 2352,
    "source_corpus_id": 252780742,
    "ref_id": "b21",
    "citation_corpus_id": 56657912,
    "start": 7176,
    "end": 7206,
    "title": "BENCHMARKING NEURAL NETWORK ROBUSTNESS TO COMMON CORRUPTIONS AND PERTURBATIONS",
    "abstract": "In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, IMAGENET-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called IMAGENET-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.",
    "prev": "(2021);Herrmann et al.",
    "curr": "(2022) showed that carefully tuning the threat model in adversarial training might improve the performance on clean images and in the presence of distribution shifts, such as common corruptions (Hendrycks & Dietterich, 2018).",
    "next": "Adapters."
  },
  {
    "index": 2353,
    "source_corpus_id": 199552081,
    "ref_id": "b4",
    "citation_corpus_id": 990233,
    "start": 1799,
    "end": 1802,
    "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    "abstract": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.",
    "prev": "The StructBERT with structural pre-training gives surprisingly good empirical results on a variety of downstream tasks, including pushing the state-of-the-art on the GLUE benchmark to 84.5 (with Top 1 achievement on the Leaderboard at the time of paper submission), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on SNLI to 91.7.",
    "curr": "Introduction\n\nPre-trained language models are a key component in many natural language understanding (NLU) tasks such as semantic textual similarity [3], question answering [4] and sentiment classification [5].",
    "next": "In order to get reliable language representations, neural language models are designed to define the joint probability function of sequences of words in text with unsupervised learning."
  },
  {
    "index": 2355,
    "source_corpus_id": 209315801,
    "ref_id": "b13",
    "citation_corpus_id": 3693334,
    "start": 4984,
    "end": 5001,
    "title": "Visualizing the Loss Landscape of Neural Nets",
    "abstract": "Neural network training relies on our ability to find \"good\" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and wellchosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple \"filter normalization\" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.",
    "prev": "However, if flatness is taken to be the curvature as measured by the second order approximation of the loss, then counterexamples exist.",
    "curr": "In (Dinh et al., 2017), the authors transform a flat minimizer into a sharp one without changing the behavior of the model, and in (Li et al., 2018), the authors show the reverse behavior when weight-decay is not used.",
    "next": "In (McCandlish et al., 2018), the authors predict that the batch size can be increased up to a critical size without any drop in accuracy and empirically validate this claim."
  },
  {
    "index": 2356,
    "source_corpus_id": 258686174,
    "ref_id": "b16",
    "citation_corpus_id": 235623747,
    "start": 4724,
    "end": 4728,
    "title": "Published as a conference paper at ICLR 2022 INFORMATION BOTTLENECK: EXACT ANALYSIS OF (QUANTIZED) NEURAL NETWORKS",
    "abstract": "The information bottleneck (IB) principle has been suggested as a way to analyze deep neural networks. The learning dynamics are studied by inspecting the mutual information (MI) between the hidden layers and the input and output. Notably, separate fitting and compression phases during training have been reported. This led to some controversy including claims that the observations are not reproducible and strongly dependent on the type of activation function used as well as on the way the MI is estimated. Our study confirms that different ways of binning when computing the MI lead to qualitatively different results, either supporting or refusing IB conjectures. To resolve the controversy, we study the IB principle in settings where MI is non-trivial and can be computed exactly. We monitor the dynamics of quantized neural networks, that is, we discretize the whole deep learning system so that no approximation is required when computing the MI. This allows us to quantify the information flow without measurement errors. In this setting, we observed a fitting phase for all layers and a compression phase for the output layer in all experiments; the compression in the hidden layers was dependent on the type of activation function. Our study shows that the initial IB results were not artifacts of binning when computing the MI. However, the critical claim that the compression phase may not be observed for some networks also holds true.",
    "prev": "The subsequent papers addressed the aforementioned problems.",
    "curr": "To tackle the infinite MI problem it was proposed to consider (a) stochastic NNs [15,16], (b) quantized NNs [17] or (c) dropout NNs [18].",
    "next": "Simple and inconsistent binning entropy estimators have been replaced with estimators more appropriate for continuous random variables [15,[18][19][20]."
  },
  {
    "index": 2361,
    "source_corpus_id": 152282636,
    "ref_id": "b6",
    "citation_corpus_id": 59553475,
    "start": 3932,
    "end": 3934,
    "title": "LEARNABLE EMBEDDING SPACE FOR EFFICIENT NEURAL ARCHITECTURE COMPRESSION",
    "abstract": "We propose a method to incrementally learn an embedding space over the domain of network architectures, to enable the careful selection of architectures for evaluation during compressed architecture search. Given a teacher network, we search for a compressed network architecture by using Bayesian Optimization (BO) with a kernel function defined over our proposed embedding space to select architectures for evaluation. We demonstrate that our search algorithm can significantly outperform various baseline methods, such as random search and reinforcement learning(Ashok et al., 2018). The compressed architectures found by our method are also better than the state-of-the-art manually-designed compact architecture ShuffleNet(Zhang et al., 2018). We also demonstrate that the learned embedding space can be transferred to new settings for architecture search, such as a larger teacher network or a teacher network in a different architecture family, without any training. * indicates equal contribution.Published as a conference paper at ICLR 2019 have multiple layers, multiple branches and multiple skip connections, defining an embedding space over all architectures is non-trivial. In this work, we propose a method for mapping a diverse range of discrete architectures to a continuous embedding space through the use of recurrent neural networks. The learned embedding space allows us to perform BO to efficiently search for compressed student architectures that are also expected to have high accuracy.We demonstrate that our search algorithm can significantly outperform various baseline methods, such as random search and reinforcement learning(Ashok et al., 2018). For example, our search algorithm can compress VGG-19 (Simonyan & Zisserman, 2014) by 8× on CIFAR-100(Krizhevsky & Hinton, 2009) while maintaining accuracy on par with the teacher network. The automatically found compressed architectures can also achieve higher accuracy than the state-of-the-art manuallydesigned compact architecture ShuffleNet (Zhang et al., 2018) with a similar size. We also demonstrate that the learned embedding space can be transferred to new settings for architecture search, such as a larger teacher network or a teacher network in a different architecture family, without any training.Contributions: (1) We propose a novel method to incrementally learn an embedding space over the domain of network architectures. Based on the learnable embedding space, we present a framework of searching for compressed network architectures with BO. The learned embedding provides a feature space over which the kernel function of BO is defined.(2)We propose a set of architecture operators for generating architectures for search. Operators for modifying the teacher network are: layer removal, layer shrinkage and skip connection addition.(3)We propose a multiple kernel strategy to prevent the premature convergence of the search and encourage the search algorithm to explore more diverse architectures during the search process.",
    "prev": "We note that resource-constrained training already is implicitly widespread, as the vast majority of practitioners have access to limited compute.",
    "curr": "This is particular true for those pursuing research directions that require a massive number of training runs, such as hyper-parameter tuning [37] and neural architecture search [74,7,40].",
    "next": "Instead of asking \"what is the best performance one can achieve given this data and algorithm?"
  },
  {
    "index": 2362,
    "source_corpus_id": 246430598,
    "ref_id": "b19",
    "citation_corpus_id": 30535508,
    "start": 1836,
    "end": 1857,
    "title": "LEARNING SPARSE NEURAL NETWORKS THROUGH L 0 REGULARIZATION",
    "abstract": "We propose a practical method for L 0 norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of L 0 regularization. However, since the L 0 norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected L 0 norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the hard concrete distribution for the gates, which is obtained by \"stretching\" a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.",
    "prev": "INTRODUCTION\n\nAs one of the efficient compression methods, pruning reduces the number of parameters by replacing model parameters of low importance with zeros (LeCun et al., 1990).",
    "curr": "Since magnitude-based pruning has shown that pruning can be conducted with low computational complexity (Han et al., 2015), various practical pruning methods have been studied to achieve higher compression ratio (Zhu and Gupta, 2017;Molchanov et al., 2017;Louizos et al., 2018;Gale et al., 2019).",
    "next": "Recently, pruning has been extended to a deeper understanding of weight initialization."
  },
  {
    "index": 2364,
    "source_corpus_id": 207930593,
    "ref_id": "b26",
    "citation_corpus_id": 2593903,
    "start": 15879,
    "end": 15901,
    "title": "The NarrativeQA Reading Comprehension Challenge",
    "abstract": "Reading comprehension (RC)-in contrast to information retrieval-requires integrating information and reasoning about events, entities, and their relations across a full document. Question answering is conventionally used to assess RC ability, in both artificial agents and children learning to read. However, existing RC datasets and tasks are dominated by questions that can be solved by selecting answers using superficial information (e.g., local context similarity or global term frequency); they thus fail to test for the essential integrative aspect of RC. To encourage progress on deeper comprehension of language, we present a new dataset and set of tasks in which the reader must answer questions about stories by reading entire books or movie scripts. These tasks are designed so that successfully answering their questions requires understanding the underlying narrative rather than relying on shallow pattern matching or salience. We show that although humans solve the tasks easily, standard RC models struggle on the tasks presented here. We provide an analysis of the dataset and the challenges it presents.",
    "prev": "(2018)) however BookCorpus is no longer distributed due to licensing issues, and the source of data is dynamically changing -which makes exact benchmarking difficult over time.",
    "curr": "The NarrativeQA Book Comprehension Task (Kočiskỳ et al., 2018) uses Project Gutenberg texts paired with Wikipedia articles, which can be used as summaries.",
    "next": "Due to the requirement of needing a corresponding summary, NarrativeQA contains a smaller selection of books: 1,527 versus the 28,752 books in PG-19."
  },
  {
    "index": 2366,
    "source_corpus_id": 252907833,
    "ref_id": "b9",
    "citation_corpus_id": 8393918,
    "start": 2246,
    "end": 2263,
    "title": "Published as a conference paper at ICLR 2016 GATED GRAPH SEQUENCE NEURAL NETWORKS",
    "abstract": "Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks(Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be described as abstract data structures.Published as a conference paper at ICLR 2016 to learn features on the graph that encode the partial output sequence that has already been produced (e.g., the path so far if outputting a path) and that still needs to be produced (e.g., the remaining path). We will show how the GNN framework can be adapted to these settings, leading to a novel graph-based neural network model that we call Gated Graph Sequence Neural Networks (GGS-NNs).We illustrate aspects of this general model in experiments on bAbI tasks  and graph algorithm learning tasks that illustrate the capabilities of the model. We then present an application to the verification of computer programs. When attempting to prove properties such as memory safety (i.e., that there are no null pointer dereferences in a program), a core problem is to find mathematical descriptions of the data structures used in a program. FollowingBrockschmidt et al. (2015), we have phrased this as a machine learning problem where we will learn to map from a set of input graphs, representing the state of memory, to a logical description of the data structures that have been instantiated. Whereas Brockschmidt et al.(2015)relied on a large amount of hand-engineering of features, we show that the system can be replaced with a GGS-NN at no cost in accuracy.",
    "prev": "Introduction\n\nGraph data is pervasive in real-world applications, such as, online recommendations [Shalaby et al., 2017, Huang et al., 2021, drug discovery [Takigawa andMamitsuka, 2013, Li et al., 2017], and knowledge management [Rizun, 2019, Wang et al., 2018, to name a few.",
    "curr": "The growing need to analyze huge amounts of graph data has inspired work that combines Graph Neural Networks with deep learning , Li et al., 2016, Hamilton et al., 2017, Xu et al., 2019a, Jiang et al., 2019.",
    "next": "Graph Convolutional Networks (GCNs) [Kipf and Welling, 2017, Zhang and Chen, 2018, Fan et al., 2019, the most cited GNN architecture, adopts convolution and message passing mechanisms."
  },
  {
    "index": 2367,
    "source_corpus_id": 247158476,
    "ref_id": "b44",
    "citation_corpus_id": 3292002,
    "start": 3953,
    "end": 3978,
    "title": "GRAPH ATTENTION NETWORKS",
    "abstract": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-theart results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a proteinprotein interaction dataset (wherein test graphs remain unseen during training).",
    "prev": "Our work provides a surprising challenge to this wisdom by providing an isotropic model, called Efficient Graph Convolution (EGC),\n\nx Messages are functions of source and target nodes and hence must be materialized\nAnisotropic b f ( x , b ) a f ( x , a ) c f ( x , c ) d f ( x , d )\nx Messages are functions of source only; propagation can be implemented using matrix multiplication-style approaches Figure 1: Many GNN architectures (e.g.",
    "curr": "GAT (Veličković et al., 2018), PNA (Corso et al., 2020)) incorporate sophisticated message functions to improve accuracy (left).",
    "next": "This is problematic as we must materialize messages, leading to O(E) memory consumption and OPs to calculate messages; these dataflow patterns are also difficult to optimize for at the hardware level."
  },
  {
    "index": 2369,
    "source_corpus_id": 219792787,
    "ref_id": "b5",
    "citation_corpus_id": 203736530,
    "start": 4561,
    "end": 4563,
    "title": "Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks",
    "abstract": "Recent research shows that the following two models are equivalent: (a) infinitely wide neural networks (NNs) trained under ℓ2 loss by gradient descent with infinitesimally small learning rate (b) kernel regression with respect to so-called Neural Tangent Kernels (NTKs)(Jacot et al., 2018). An efficient algorithm to compute the NTK, as well as its convolutional counterparts, appears inArora et al. (2019a), which allowed studying performance of infinitely wide nets on datasets like CIFAR-10. However, superquadratic running time of kernel methods makes them best suited for small-data tasks. We report results suggesting neural tangent kernels perform strongly on low-data tasks. 1. On a standard testbed of classification/regression tasks from the UCI database, NTK SVM beats the previous gold standard, Random Forests (RF), and also the corresponding finite nets. 2. On CIFAR-10 with 10 -640 training samples, Convolutional NTK consistently beats ResNet-34 by 1% -3%. 3. On VOC07 testbed for few-shot image classification tasks on ImageNet with transfer learning(Goyal et al., 2019), replacing the linear SVM currently used with a Convolutional NTK SVM consistently improves performance. 4. Comparing the performance of NTK with the finite-width net it was derived from, NTK behavior starts at lower net widths than suggested by theoretical analysis(Arora et al., 2019a). NTK's efficacy may trace to lower variance of output.Ironically, while the above-mentioned analysis, at first sight, appears to reduce the study of a complicated model -deep networks -to an older, simpler model -kernel regression -in practice the simpler model is computationally less efficient because running time of kernel regression can be quadratic in the number of data points! 1 Thus computing using CNTK kernel on large datasets like ImageNet currently appears infeasible. Even on CIFAR-10, it seems infeasible to incorporate data augmentation.However, kernel classifiers are very efficient on small datasets. Here NTKs could conceivably be practical while at the same time bringing some of the power of deep networks to these settings. We recall that recentlyOlson et al. (2018)showed that multilayer neural networks can be reasonably effective on small datasets, specifically on a UCI testbed of tasks with as few as dozens of training examples. Of course, this required some hyperparameter tuning, although they noted that such tuning is also needed for the champion method, Random Forests (RF), which multilayer neural networks could not beat.It is thus natural to check if NTK -corresponding to infinitely wide fully-connected networks -performs well in such small-data tasks 2 . Convex objectives arising from kernels have stable solvers with minimal hyperparameter tuning. Furthermore, random initialization in deep network training seems to lead to higher variance in the output, which can hurt performance in small-data settings. Can NTK's do better? Below we will see that in the setup of Olson et al.(2018), NTK predictors indeed outperforms corresponding finite deep networks, and also slightly beats the earlier gold standard, Random Forests. This suggests NTK predictors should belong in any list of off-the-shelf machine learning methods.Following are low-data settings where we used NTKs and CNTKs: • In the testbed of 90 classification tasks from UCI database, NTK predictor achieves superior, and arguably the strongest classification performance. This is verified via several standard statistical tests, including Friedman Rank, Average Accuracy, Percentage of the Maximum Accuracy (PMA) and probability of achieving 90%/95% maximum accuracy (P90 and P95), performed to compare performances of different classifiers on 90 datasets from UCI database. (The authors plan to release the code, to allow off-the-shelf use of this method. It does not require GPUs.) • We find the performance of NN is close to that of NTK. On every dataset from UCI database, the difference between the classification accuracy of NN and that of NTK is within 5%. On the other hand, on some datasets, the difference between classification accuracy of NN (or NTK) and that of other classifiers like RF can be as high as 20%. This indicates in low-data settings, NTK is indeed a good description of NN. Furthermore, we find NTK is more stable (smaller variance), which seems to help it achieve better accuracy on small datasets (cf.Figure 2b). • CNTK is useful in computer vision tasks with small-data. On CIFAR-10, we compare CNTK with ResNet using 10 -640 training samples and find CNTK can beat ResNet by 1% − 3%. We further study few-shot image classification task on VOC07 dataset. The standard method is to first use a pre-trained network, e.g., ResNet-50 trained on ImageNet, to extract features and then directly apply a linear classifier on the extracted features(Goyal et al., 2019). Here we replace the linear classifier with CNTK and obtain better classification accuracy in various setups.Paper organization. Section 2 discusses related work. Section 3 reviews the derivation of NTK. Section 4 presents experiments using NN and NTK on UCI datasets. Section 5 presents experiments using CNN and CNTK on small CIFAR-10 datasets. Section 6 presents experiments using CNTK for the few-shot learning setting. Additional technical details are presented in appendix.Related WorkOur paper is inspired byFernández-Delgado et al. (2014)which conducted extensive experiments on UCI dataset. Their conclusion is random forest performs the best, which is followed by the SVM with Gaussian 1 The bottleneck is constructing the kernel, which scales quadratically with the number of data points (Arora et al., 2019a). The regression also requires matrix inversion, which can be cubic in the number of data points.2 Note that NTKs can also be used in kernel SVMs, which are not known to be equivalent to training infinitely wide networks. Currently, equivalence is only known for ridge regression. We tried both.",
    "prev": "Q: Does the RNTK generalize well?",
    "curr": "A recent study has revealed that the use of an SVM classifier with the NTK, CNTK, and GNTK kernels outperforms other classical kernel-based classifiers and trained finite DNNs on small data sets (typically fewer than 5000 training samples) [4,6,14,27].",
    "next": "We extend these results to RNTKs to demonstrate that the RNTK outperforms a variety of classic kernels, NTKs and finite RNNs for time series data sets in both classification and regression tasks."
  },
  {
    "index": 2370,
    "source_corpus_id": 252907753,
    "ref_id": "b1",
    "citation_corpus_id": 234357520,
    "start": 16085,
    "end": 16106,
    "title": "VICREG: VARIANCE-INVARIANCE-COVARIANCE RE- GULARIZATION FOR SELF-SUPERVISED LEARNING",
    "abstract": "Recent self-supervised methods for image representation learning maximize the agreement between embedding vectors produced by encoders fed with different views of the same image. The main challenge is to prevent a collapse in which the encoders produce constant or non-informative vectors. We introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with two regularizations terms applied to both embeddings separately: (1) a term that maintains the variance of each embedding dimension above a threshold, (2) a term that decorrelates each pair of variables. Unlike most other approaches to the same problem, VICReg does not require techniques such as: weight sharing between the branches, batch normalization, feature-wise normalization, output quantization, stop gradient, memory banks, etc., and achieves results on par with the state of the art on several downstream tasks. In addition, we show that our variance regularization term stabilizes the training of other methods and leads to performance improvements.",
    "prev": "Avoiding this can be achieved in several ways, s.a. by including a term that encourages the variance of z :,i to be above 1 (a technique used e.g.",
    "curr": "in self-supervised learning method VICReg (Bardes et al., 2022)) or -more in line with traditional VAE variants for disentanglement -by using a stochastic autoencoder (SAE) reconstruction error:\nSAE (x; φ, θ) = −E q φ (z|x) [log p θ (x|z)](6)\nwhere typically q φ (z|x) = N (f φ (x), Σ φ (x)) with mean given by our deterministic mapping f φ , Σ φ (x) producing a diagonal covariance parameter, and e.g.",
    "next": "log p θ (x|z) = r θ (z) − x 2 with r θ a parameterized decoder."
  },
  {
    "index": 2372,
    "source_corpus_id": 3289985,
    "ref_id": "b19",
    "citation_corpus_id": 5834589,
    "start": 2999,
    "end": 3003,
    "title": "ON LARGE-BATCH TRAINING FOR DEEP LEARNING: GENERALIZATION GAP AND SHARP MINIMA",
    "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say 32-512 data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions-and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap. Published as a conference paper at ICLR 2017 These methods minimize the objective function f by iteratively taking steps of the form: J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. . Weak sharp minima and penalty functions in mathematical programming. PhD thesis, University of Cambridge, 1988.Michael P Friedlander and Mark Schmidt. Hybrid deterministic-stochastic methods for data fitting.",
    "prev": "It is well known that the convergence of the original SGD algorithm ( [29]) has two terms, one of which depends on the variance of the gradient estimate.",
    "curr": "In practice, decreasing the variance by increasing the batch size suffers from diminishing returns, often resulting in speedups that are sublinear in batch size, and even worse, in degraded generalization performance ( [20]).",
    "next": "Some recent work ( [13,41,42]) suggests that by carefully tuning learning rates and other hyperparameter schedules, it is possible to train architectures like ResNets and AlexNet on Imagenet with large mini-batches of up to 8192 with no loss of accuracy, shortening training time to hours instead of days or weeks."
  },
  {
    "index": 2375,
    "source_corpus_id": 260440513,
    "ref_id": "b17",
    "citation_corpus_id": 1957433,
    "start": 6778,
    "end": 6802,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "BASIC DESIGN OF MACHINE COMPREHENSION MODELS\n\nFor single-turn MC, many top-performing models share a similar architecture, consisting of four major components: (1) question encoding, (2) context encoding, (3) reasoning, and finally (4) answer prediction.",
    "curr": "Initially the word embeddings (e.g., Pennington et al., 2014;Peters et al., 2018) of question tokens Q and context tokens C are taken as input and fed into contextual integration layers, such as LSTMs (Hochreiter & Schmidhuber, 1997) or self attentions (Yu et al., 2018), to encode the question and context.",
    "next": "Multiple integration layers provide contextualized representations of context, and are often inter-weaved with attention, which inject question information."
  },
  {
    "index": 2376,
    "source_corpus_id": 257038392,
    "ref_id": "b10",
    "citation_corpus_id": 233296808,
    "start": 5961,
    "end": 5982,
    "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
    "abstract": "In this work, we explore \"prompt tuning,\" a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient \"prompt ensembling.\" We release code and model checkpoints to reproduce our experiments. 1ReferencesRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,  Danilo Giampiccolo, Bernardo Magnini, and Idan  Szpektor. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognising textual entailment, volume 6, pages 6-4. Venice. . 2019a. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
    "prev": "While automatically induced prompts suffer of issues such as low-interpretability, we think it is important to continue focusing on them because, besides their better performance (a result we confirm here for AutoPrompt across a range of LMs), they are more promising than manual prompts in terms of scalability, especially in contexts in which it is not sufficient to formulate a single prompt template for a whole task, but each input query demands a distinct prompt formulation (Zhang et al., 2022).",
    "curr": "Concurrent and later work has proposed to replace discrete strings, such as those generated by AutoPrompt, with sequences of arbitrary vectors from the LM's embedding space (Lester et al., 2021;Zhong et al., 2021).",
    "next": "We confirm here that these continuous, or \"soft\" prompts outperform AutoPrompt when trained and tested on the same LM."
  },
  {
    "index": 2378,
    "source_corpus_id": 210919941,
    "ref_id": "b22",
    "citation_corpus_id": 5151364,
    "start": 22747,
    "end": 22768,
    "title": "THE PENN TREEBANK: ANNOTATING PREDICATE ARGUMENT STRUCTURE",
    "abstract": "The Penn Treebank has recently implemented a new syntactic annotation scheme, designed to highlight aspects of predicate-argument structure.This paper discusses the implementation of crucial aspects of this new annotation scheme. It incorporates a more consistent treatment of a wide range of grammatical phenomena, provides a set of coindexed null elements in what can be thought of as \"underlying\" position for phenomena such as wh-movement, passive, and the subjects of infinitival constructions, provides some non-context free annotational mechanism to allow the structure of discontinuous constituents to be easily recovered, and allows for a clear, concise tagging system for some semantic roles.",
    "prev": "We compare Sparse SpiderBoost, SpiderBoost, and SGD.",
    "curr": "We train our LSTM model on the Penn Treebank (Marcus et al., 1994) corpus.",
    "next": "The natural language processing model consists of a word embedding of dimension 128 of 1000 tokens, which is jointly learned with the task."
  },
  {
    "index": 2380,
    "source_corpus_id": 235313510,
    "ref_id": "b61",
    "citation_corpus_id": 6212000,
    "start": 2160,
    "end": 2180,
    "title": "UNDERSTANDING DEEP LEARNING REQUIRES RE- THINKING GENERALIZATION",
    "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.",
    "prev": "Extensive experiments on various datasets validate the effectiveness of the proposed method.",
    "curr": "INTRODUCTION\n\nDeep neural networks (DNNs) usually exhibit excellent generalization ability in pattern recognition tasks, despite their sufficient capacity to overfit or memorize the entire training set with completely random labels (Zhang et al., 2017).",
    "next": "The memorization behavior in deep learning has aroused tremendous attention to identifying the differences between learning on true and random labels (Arpit et al., 2017;Neyshabur et al., 2017), and examining what and why DNNs memorize (Feldman, 2020;Feldman & Zhang, 2020;Maennel et al., 2020)."
  },
  {
    "index": 2381,
    "source_corpus_id": 235359074,
    "ref_id": "b11",
    "citation_corpus_id": 52967399,
    "start": 2073,
    "end": 2094,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "INTRODUCTION\n\nTransformers (Vaswani et al., 2017) based on self-attention come naturally with the ability to learn long-range dependencies in sequential data.",
    "curr": "As important as it is to language modeling (Devlin et al., 2019), such ability is also highly desired for many vision tasks where contextual modeling plays a significant role.",
    "next": "For this reason, self-attention and transformers have been receiving an increasing attention in the vision community (Bello, 2021;Srinivas et al., 2021;Zhao et al., 2020;Ramachandran et al., 2019a;Bello et al., 2019;Hu et al., 2019;Ramachandran et al., 2019b;Wang et al., 2018)."
  },
  {
    "index": 2382,
    "source_corpus_id": 224803680,
    "ref_id": "b22",
    "citation_corpus_id": 208637407,
    "start": 3427,
    "end": 3451,
    "title": "Published as a conference paper at ICLR 2020 AUGMIX: A SIMPLE DATA PROCESSING METHOD TO IMPROVE ROBUSTNESS AND UNCERTAINTY",
    "abstract": "Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classifiers. We propose AUGMIX, a data processing technique that is simple to implement, adds limited computational overhead, and helps models withstand unforeseen corruptions. AUGMIX significantly improves robustness and uncertainty measures on challenging image classification benchmarks, closing the gap between previous methods and the best possible performance in some cases by more than half.",
    "prev": "Intuitively, data augmentation enables the model to train on more data, encouraging the model to capture certain invariances with respect to its inputs and outputs; data augmentation may also produce data that may be closer to an out-ofdistribution target task.",
    "curr": "It has been a key factor driving state-of-the-art: for example, Mixup Thulasidasan et al., 2019a), AugMix (Hendrycks et al., 2020), and test-time data augmentation (Ashukha et al., 2020).",
    "next": "A common wisdom in the community suggests that ensembles and data augmentation should naturally combine."
  },
  {
    "index": 2384,
    "source_corpus_id": 8693672,
    "ref_id": "b42",
    "citation_corpus_id": 215825908,
    "start": 1171,
    "end": 1193,
    "title": "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech",
    "abstract": "We describe a statistical approach for modeling dialogue acts in conversational speech, i.e., speech-act-like units such as STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DIS-AGREEMENT, and APOLOGY. Our model detects and predicts dialogue acts based on lexical, collocational, and prosodic cues, as well as on the discourse coherence of the dialogue act sequence. The dialogue model is based on treating the discourse structure of a conversation as a hidden Markov model and the individual dialogue acts as observations emanating from the model states. Constraints on the likely sequence of dialogue acts are modeled via a dialogue act n-gram. The statistical dialogue grammar is combined with word n-grams, decision trees, and neural networks modeling the idiosyncratic lexical and prosodic manifestations of each dialogue act. We develop a probabilistic integration of speech recognition with dialogue modeling, to improve both speech recognition and dialogue act classification accuracy. Models are trained and evaluated using a large hand-labeled database of 1,155 conversations from the Switchboard corpus of spontaneous human-to-human telephone speech. We achieved good dialogue act labeling accuracy (65% based on errorful, automatically recognized words and prosody, and 71% based on word transcripts, compared to a chance baseline accuracy of 35% and human accuracy of 84%) and a small reduction in word recognition error.",
    "prev": "We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks.",
    "curr": "INTRODUCTION\n\nLanguage modeling is a core problem in natural language processing, with many applications such as machine translation (Brown et al., 1993), speech recognition (Bahl et al., 1983) or dialogue agents (Stolcke et al., 2000).",
    "next": "While traditional neural networks language models have obtained stateof-the-art performance in this domain (Jozefowicz et al., 2016;Mikolov et al., 2010), they lack the capacity to adapt to their recent history, limiting their application to dynamic environments (Dodge et al., 2015)."
  },
  {
    "index": 2386,
    "source_corpus_id": 211532691,
    "ref_id": "b6",
    "citation_corpus_id": 53115163,
    "start": 2634,
    "end": 2654,
    "title": "EXPLORATION BY RANDOM NETWORK DISTILLATION",
    "abstract": "We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.",
    "prev": "Inspired by human learning, the use of intrinsic motivation has been proposed to encourage agents to learn about their environments even when extrinsic feedback is rarely provided (Schmidhuber, 1991b;2010;Oudeyer et al., 2007;Oudeyer & Kaplan, 2009).",
    "curr": "This type of exploration bonus emboldens the agent to visit new states (Bellemare et al., 2016;Burda et al., 2019b;Ecoffet et al., 2019) or to improve its knowledge and forward prediction of the world dynamics (Pathak et al., 2017;Burda et al., 2019a), and can be highly effective for learning in hard exploration games such as Montezuma's Revenge (Mnih et al., 2016).",
    "next": "However, most hard exploration environments used in previous work have either a limited state space or an easy way to measure similarity between states (Ecoffet et al., 2019) and generally use the same \"singleton\" environment for training and evaluation (Mnih et al., 2016;Burda et al., 2019a)."
  },
  {
    "index": 2388,
    "source_corpus_id": 258212627,
    "ref_id": "b26",
    "citation_corpus_id": 52967399,
    "start": 2242,
    "end": 2245,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Moreover, LipsFormer-CSwin-Tiny, based on CSwin, training for 300 epochs achieves a top-1 accuracy of 83.5% with 4.7G FLOPs and 24M parameters.",
    "curr": "The code will be released at https://github.com/\n\nINTRODUCTION\n\nTransformer [49] has been widely adopted in natural language processing (NLP) [6,27,40] for its great capability of capturing long-range dependencies with self-attention.",
    "next": "Motivated by its success in NLP, Dosovitskiy et al."
  },
  {
    "index": 2389,
    "source_corpus_id": 202540355,
    "ref_id": "b35",
    "citation_corpus_id": 1185652,
    "start": 7191,
    "end": 7194,
    "title": "MODELING COMPOSITIONALITY WITH MULTIPLICATIVE RECURRENT NEURAL NETWORKS",
    "abstract": "We present the multiplicative recurrent neural network as a general model for compositional meaning in language, and evaluate it on the task of fine-grained sentiment analysis. We establish a connection to the previously investigated matrixspace models for compositionality, and show they are special cases of the multiplicative recurrent net. Our experiments show that these models perform comparably or better than Elman-type additive recurrent neural networks and outperform matrix-space models on a standard fine-grained sentiment analysis corpus. Furthermore, they yield comparable results to structural deep models on the recently published Stanford Sentiment Treebank without the need for generating parse trees.",
    "prev": "The key difference between our simplicial agent and the relational agent of [69] is that in place of a standard Transformer block we use a 2-simplicial Transformer block.",
    "curr": "Our use of tensor products of value vectors is inspired by the semantics of linear logic in vector spaces [25,47,14] in which an algorithm with multiple inputs computes on the tensor product of those inputs, but this is an old idea in natural language processing, used in models including the second-order RNN [22,50,27,23], multiplicative RNN [62,36], Neural Tensor Network [60] and the factored 3-way Restricted Boltzmann Machine [51], see Appendix D. More recently tensors have been used to model predicates in a number of neural network architectures aimed at logical reasoning [55,18].",
    "next": "The main novelty in our model lies in the introduction of the 2-simplicial attention, which allows these ideas to be incorporated into the Transformer architecture."
  },
  {
    "index": 2392,
    "source_corpus_id": 252284009,
    "ref_id": "b0",
    "citation_corpus_id": 14124313,
    "start": 1841,
    "end": 1844,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "Our code is available at DNC.",
    "curr": "INTRODUCTION\n\nDeep learning models, from convolutional networks (e.g., VGG [1], ResNet [2]) to Transformer-based architectures (e.g., Swin [3]), push forward the state-of-the-art on visual recognition.",
    "next": "With these advancements, parametric softmax classifiers, which learn a set of parameters, i.e., weight vector, and bias term, for each class, have become the de facto regime in the area ( Fig."
  },
  {
    "index": 2395,
    "source_corpus_id": 3626819,
    "ref_id": "b40",
    "citation_corpus_id": 1957433,
    "start": 2164,
    "end": 2188,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
    "curr": "Introduction\n\nPre-trained word representations (Mikolov et al., 2013;Pennington et al., 2014) are a key component in many neural language understanding models.",
    "next": "However, learning high quality representations can be challenging."
  },
  {
    "index": 2396,
    "source_corpus_id": 201107103,
    "ref_id": "b18",
    "citation_corpus_id": 52167799,
    "start": 2848,
    "end": 2869,
    "title": "Training Millions of Personalized Dialogue Agents",
    "abstract": "Current dialogue systems are not very engaging for users, especially when trained end-toend without relying on proactive reengaging scripted strategies.Zhang et al. (2018)showed that the engagement level of end-to-end dialogue models increases when conditioning them on text personas providing some personalized back-story to the model. However, the dataset used in(Zhang et al., 2018)is synthetic and of limited size as it contains around 1k different personas. In this paper we introduce a new dataset providing 5 million personas and 700 million persona-based dialogues. Our experiments show that, at this scale, training using personas still improves the performance of end-to-end systems. In addition, we show that other tasks benefit from the wide coverage of our dataset by fine-tuning our model on the data from(Zhang et al., 2018)and achieving state-of-the-art results.",
    "prev": "Two classes of fine-tuned architecture are typically built on top: Bi-encoders and Cross-encoders.",
    "curr": "Cross-encoders (Wolf et al., 2019;Vig & Ramea, 2019), which perform full (cross) self-attention over a given input and label candidate, tend to attain much higher accuracies than their counterparts, Bi-encoders (Mazaré et al., 2018;, which perform self-attention over the input and candidate label separately and combine them at the end for a final representation.",
    "next": "As the representations are separate, Bi-encoders are able to cache the encoded candidates, and reuse these representations for each input resulting in fast prediction times."
  },
  {
    "index": 2399,
    "source_corpus_id": 208981547,
    "ref_id": "b29",
    "citation_corpus_id": 16636683,
    "start": 24130,
    "end": 24148,
    "title": "Network In Network",
    "abstract": "We propose a novel deep network structure called \"Network In Network\"(NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.",
    "prev": "In principle, hypernetworks can also be applied to the newer variants of residual networks with more skip connections, such as DenseNets and ResNets of Resnets.",
    "curr": "From the results, we see that enforcing a relaxed weight sharing constraint to the deep residual network cost us ∼ 1.25-1.5% in classification accuracy, while drastically reducing the number of\n\n\nModel Test Error Param Count\n\nNetwork in Network (Lin et al., 2014) 8.81% FitNet (Romero et al., 2014) 8.39% Deeply Supervised Nets (Lee et al., 2015) 8.22% Highway Networks (Srivastava et al., 2015) 7.72% ELU (Clevert et al., 2015) 6.55% Original Resnet-110 (He et al., 2016a) 6.43% 1.7 M Stochastic Depth Resnet-110 (Huang et al., 2016b) 5.23% 1.7 M Wide Residual Network 40-1 (Zagoruyko & Komodakis, 2016) 6.85% 0.6 M Wide Residual Network 40-2 (Zagoruyko & Komodakis, 2016) 5  parameters in the model as a trade off.",
    "next": "One reason for this reduction in accuracy is because different layers of a deep network is trained to extract different levels of features, and require different kinds of filters to perform optimally."
  },
  {
    "index": 2406,
    "source_corpus_id": 221970302,
    "ref_id": "b3",
    "citation_corpus_id": 3618568,
    "start": 1725,
    "end": 1744,
    "title": "Reading Wikipedia to Answer Open-Domain Questions",
    "abstract": "This paper proposes to tackle opendomain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.",
    "prev": "INTRODUCTION\n\nOpen domain question answering is a challenging task where the answer to a given question needs to be extracted from a large pool of documents.",
    "curr": "The prevailing approach (Chen et al., 2017) tackles the problem in two stages.",
    "next": "Given a question, a retriever first produces a list of k candidate documents, and a reader then extracts the answer from this set."
  },
  {
    "index": 2408,
    "source_corpus_id": 247245054,
    "ref_id": "b8",
    "citation_corpus_id": 3484654,
    "start": 1957,
    "end": 1976,
    "title": "RECASTING GRADIENT-BASED META-LEARNING AS HIERARCHICAL BAYES",
    "abstract": "Meta-learning allows an intelligent agent to leverage prior learning episodes as a basis for quickly improving performance on a novel task. Bayesian hierarchical modeling provides a theoretical framework for formalizing meta-learning as inference for a set of parameters that are shared across tasks. Here, we reformulate the model-agnostic meta-learning algorithm (MAML) of Finn et al.(2017)as a method for probabilistic inference in a hierarchical Bayesian model. In contrast to prior methods for meta-learning via hierarchical Bayes, MAML is naturally applicable to complex function approximators through its use of a scalable gradient descent procedure for posterior inference. Furthermore, the identification of MAML as hierarchical Bayes provides a way to understand the algorithm's operation as a meta-learning procedure, as well as an opportunity to make use of computational strategies for efficient inference. We use this opportunity to propose an improvement to the MAML algorithm that makes use of techniques from approximate inference and curvature estimation.",
    "prev": "They can readily establish their cognition and understanding of novel tasks, environments, or domains even with very limited experience in the corresponding circumstances.",
    "curr": "Meta-learning, a subfield of machine learning, aims at equipping machines with such capacity to accommodate new scenarios effectively (Vilalta & Drissi, 2002;Grant et al., 2018).",
    "next": "Machines learn to extract task-agnostic information so that their performance on unseen tasks can be improved (Hospedales et al., 2020)."
  },
  {
    "index": 2410,
    "source_corpus_id": 257365083,
    "ref_id": "b35",
    "citation_corpus_id": 229924317,
    "start": 28936,
    "end": 28955,
    "title": "EMERGENT SYMBOLS THROUGH BINDING IN EXTERNAL MEMORY",
    "abstract": "A key aspect of human intelligence is the ability to infer abstract rules directly from high-dimensional sensory data, and to do so given only a limited amount of training experience. Deep neural network algorithms have proven to be a powerful tool for learning directly from high-dimensional data, but currently lack this capacity for data-efficient induction of abstract rules, leading some to argue that symbol-processing mechanisms will be necessary to account for this capacity. In this work, we take a step toward bridging this gap by introducing the Emergent Symbol Binding Network (ESBN), a recurrent network augmented with an external memory that enables a form of variable-binding and indirection. This binding mechanism allows symbol-like representations to emerge through the learning process without the need to explicitly incorporate symbol-processing machinery, enabling the ESBN to learn rules in a manner that is abstracted away from the particular entities to which those rules apply. Across a series of tasks, we show that this architecture displays nearly perfect generalization of learned rules to novel entities given only a limited number of training examples, and outperforms a number of other competitive neural network architectures.",
    "prev": "CONCLUSION AND FUTURE DIRECTIONS\n\nWe have presented a simple, general-purpose visual reasoning model, organized around the principle of object-centric processing.Our proposed model, STSN, displayed state-of-the-art performance on both of two challenging visual reasoning benchmarks, PGM and I-RAVEN, as well a novel reasoning benchmark with greater visual complexity, CLEVR-Matrices, despite the relative lack of problemspecific inductive biases.These results suggest that object-centric processing is a powerful inductive bias for abstract visual reasoning problems such as RPM.",
    "curr": "Some previous work has proposed novel relational inductive biases for the purposes of achieving strong out-of-distribution generalization in visual reasoning problems (Webb et al., 2021;Zhang et al., 2021;Kerg et al., 2022).This work has often assumed (i.e., hand-coded) object-centric representations.We view our approach as complementary with these previous approaches, and suggest that a fruitful avenue for future work will be to pursue the integration of object-centric and relational inductive biases.",
    "next": "Table 7 describes the hyperparameters for the slot decoder used on the I-RAVEN and PGM datasets, and Table 8 describes the hyperparameters used on the CLEVR-Matrices dataset.Each of the K slots was passed through the decoder, yielding a slot-specific reconstructed image xk and mask m k .Each slot was first converted to a feature map using a spatial broadcast operation Watters et al."
  },
  {
    "index": 2411,
    "source_corpus_id": 195584474,
    "ref_id": "b2",
    "citation_corpus_id": 14711954,
    "start": 4130,
    "end": 4132,
    "title": "Modular Multitask Reinforcement Learning with Policy Sketches",
    "abstract": "We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate tasks with sequences of named subtasks, providing information about high-level structural relationships among tasks but not how to implement them-specifically not providing the detailed guidance used by much previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). To learn from sketches, we present a model that associates every subtask with a modular subpolicy, and jointly maximizes reward over full task-specific policies by tying parameters across shared subpolicies. Optimization is accomplished via a decoupled actor-critic training objective that facilitates learning common behaviors from multiple dissimilar reward functions. We evaluate the effectiveness of our approach in three environments featuring both discrete and continuous control, and with sparse rewards that can be obtained only after completing a number of high-level subgoals. Experiments show that using our approach to learn policies guided by sketches gives better performance than existing techniques for learning task-specific or shared policies, while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks.",
    "prev": "Each primitive focuses on distinct features of the environment; in this case, one policy focuses on boxes, a second one on gates, and the third one on spheres.",
    "curr": "master policy is trained on a particular state distribution, learning it in a way that generalizes to new environments effectively can, therefore, become the bottleneck for such approaches [31,3].",
    "next": "We argue, and empirically show, that in order to achieve better generalization, the interaction between the low-level primitives and the selection thereof should itself be performed without requiring a single centralized network that understands the entire state space."
  },
  {
    "index": 2417,
    "source_corpus_id": 250048634,
    "ref_id": "b4",
    "citation_corpus_id": 238407870,
    "start": 3374,
    "end": 3400,
    "title": "Published as a conference paper at ICLR 2022 GEOMETRIC AND PHYSICAL QUANTITIES IMPROVE E(3) EQUIVARIANT MESSAGE PASSING",
    "abstract": "Including covariant information, such as position, force, velocity or spin is important in many tasks in computational physics and chemistry. We introduce Steerable E(3) Equivariant Graph Neural Networks (SEGNNs) that generalise equivariant graph networks, such that node and edge attributes are not restricted to invariant scalars, but can contain covariant information, such as vectors or tensors. This model, composed of steerable MLPs, is able to incorporate geometric and physical information in both the message and update functions. Through the definition of steerable node attributes, the MLPs provide a new class of activation functions for general use with steerable feature fields. We discuss ours and related work through the lens of equivariant non-linear convolutions, which further allows us to pin-point the successful components of SEGNNs: non-linear message aggregation improves upon classic linear (steerable) point convolutions; steerable messages improve upon recent equivariant graph networks that send invariant messages. We demonstrate the effectiveness of our method on several tasks in computational physics and chemistry and provide extensive ablation studies. Duits. Roto-translation covariant convolutional networks for medical image analysis. In International conference on medical image computing and computer-assisted intervention, pp. 440-448. Springer, 2018.Lukas Biewald. Experiment tracking with weights and biases, 2020. . Open catalyst 2020 (oc20) dataset and community challenges. ACS Catalysis, 0(0):6059-6072, 2020. A practical method for constructing equivariant multilayer perceptrons for arbitrary matrix groups. arXiv preprint arXiv:2104.09459, 2021. William T Freeman, Edward H Adelson, et al. The design and use of steerable filters. IEEE Transactions on Pattern analysis and machine intelligence, 13(9):891-906, 1991. (3)-transformers: 3d roto-translation equivariant attention networks. arXiv preprint arXiv:2006.10503, 2020.",
    "prev": "To incorporate these inductive biases, equivariant and invariant neural networks have been proposed.",
    "curr": "The former leverages geometric tensors like vectors for equivariant node features (Thomas et al., 2018;Weiler et al., 2018;Kondor et al., 2018;Fuchs et al., 2020;Brandstetter et al., 2022;Musaelian et al., 2022), and the latter augments graphs with invariant information such as distances and angles extracted from 3D graphs Gasteiger et al., 2020b;a;Liu et al., 2022;Klicpera et al., 2021).",
    "next": "A parallel line of research focuses on applying Transformer networks (Vaswani et al., 2017) to other domains like computer vision (Carion et al., 2020;Dosovitskiy et al., 2021;Touvron et al., 2020) and graph (Dwivedi & Bresson, 2020;Kreuzer et al., 2021;Ying et al., 2021;Shi et al., 2022) and has demonstrated widespread success."
  },
  {
    "index": 2418,
    "source_corpus_id": 76649575,
    "ref_id": "b41",
    "citation_corpus_id": 1923568,
    "start": 10658,
    "end": 10676,
    "title": "POLICY DISTILLATION",
    "abstract": "Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-specific) networks and extensive training are needed to achieve good performance. In this work, we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient. Furthermore, the same method can be used to consolidate multiple task-specific policies into a single policy. We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent.",
    "prev": "We can make this precise by noting that optimizing the objective (1) with respect to π 0 amounts to supervised learning of π 0 on trajectories generated by π τ , i.e.",
    "curr": "this is a distillation process from π τ to π 0 (Hinton et al., 2015;Rusu et al., 2016;Parisotto et al., 2016;Teh et al., 2017).",
    "next": "In the nonparametric case, the optimal default policy π 0 can be derived as:\nπ 0 pa t |x D t q \" řt γt ş`1 px D t \"x D t qπpa t |xtq˘π τ pxtqdxt řt γt ş`1 px D t \"x D t q˘π τ pxtqdxt ,(4)\nwhere π τ pxtq is the probability of seeing historyxt at time stept under the policy π, and the indicator 1px D t \"x D t q is 1 if the goal-agnostic information of the two histories matches and 0 otherwise."
  },
  {
    "index": 2422,
    "source_corpus_id": 195317051,
    "ref_id": "b9",
    "citation_corpus_id": 49657137,
    "start": 23060,
    "end": 23081,
    "title": "Robust and Scalable Differentiable Neural Computer for Question Answering",
    "abstract": "Deep learning models are often not easily adaptable to new tasks and require task-specific adjustments. The differentiable neural computer (DNC), a memoryaugmented neural network, is designed as a general problem solver which can be used in a wide range of tasks. But in reality, it is hard to apply this model to new tasks. We analyze the DNC and identify possible improvements within the application of question answering. This motivates a more robust and scalable DNC (rsDNC). The objective precondition is to keep the general character of this model intact while making its application more reliable and speeding up its required training time. The rsDNC is distinguished by a more robust training, a slim memory unit and a bidirectional architecture. We not only achieve new state-of-the-art performance on the bAbI task, but also minimize the performance variance between different initializations. Furthermore, we demonstrate the simplified applicability of the rsDNC to new tasks with passable results on the CNN RC task without adaptions.",
    "prev": "* denotes available results from (Santoro et al., 2016).",
    "curr": "Model\n\nError DNC  16.7 ± 7.6 SDNC (Rae et al., 2016) 6.4 ± 2.5 ADNC (Franke et al., 2018) 6.3 ± 2.7 DNC-MD (Csordas & Schmidhuber, 2019) 9.5 ± 1.6 NUTM (DNC core, p=1) 9.7 ± 3.5 NUTM (DNC core, p=2) 7.5 ± 1.6 NUTM (DNC core, p=4) 5.6 ± 1.9 persistent memory mode, which demands fast forgetting old experiences in previous episodes, NUTM outperforms MANN significantly (10-20%) 4 .",
    "next": "Readers are referred to App."
  },
  {
    "index": 2425,
    "source_corpus_id": 13900194,
    "ref_id": "b27",
    "citation_corpus_id": 6706414,
    "start": 1994,
    "end": 1998,
    "title": "Published as a conference paper at ICLR 2015 EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES",
    "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples-inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
    "prev": "Introduction\n\nConsider the classical stochastic optimization problem, in which we minimize an expected loss E P 0 [ℓ(θ; Z)] over a parameter θ ∈ Θ, where Z ∼ P 0 , P 0 is a distribution on a space Z, and ℓ is a loss function.",
    "curr": "In many systems, robustness to changes in the data-generating distribution P 0 is desirable, whether they be from covariate shifts, changes in the underlying domain [3], or adversarial attacks [28,38].",
    "next": "As deep networks become prevalent in modern performance-critical systemsprominent examples include perception systems for self-driving cars, and automated detection of tumors-model failure is increasingly costly."
  },
  {
    "index": 2428,
    "source_corpus_id": 43968607,
    "ref_id": "b31",
    "citation_corpus_id": 17506596,
    "start": 1611,
    "end": 1615,
    "title": "Applications and Explanations of Zipf's Law",
    "abstract": "Recently I have been intrigued by the reappearance of an old friend, George Kingsley Zipf, in a number of not entirely expected places. The law named for him is ubiquitous, but Zipf did not actually discover the law so much as provide a plausible explanation. Others have proposed modifications to Zipf's Law, and closer examination uncovers systematic deviations from its normative form. We demonstrate how Zipf's analysis can be extended to include some of these phenomena.",
    "prev": "These properties emerge in many real-world scenarios that approximately follow power-law distributions [27,9].",
    "curr": "This includes a wide variety of natural phenomena in physics [22], biology [25], and even human-made structures such as metabolic-mass relationships [5], social networks [20,30], and frequencies of words [32,31,37].",
    "next": "Complex networks [20], which connect distinguishable heterogeneous sets of elements represented as nodes, provide us with an intuitive way of understanding these structures."
  },
  {
    "index": 2432,
    "source_corpus_id": 214107001,
    "ref_id": "b47",
    "citation_corpus_id": 1389483,
    "start": 1668,
    "end": 1672,
    "title": "Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints",
    "abstract": "Language is increasingly being used to define rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora.In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5% and 40.5% for multilabel classification and visual semantic role labeling, respectively.",
    "prev": "Introduction\n\nThere is an increasing interest in machine learning models to be credible, fair, and more generally interpretable [13].",
    "curr": "Researchers have explored various notions of model interpretability, ranging from trustability [30], fairness of a model [48], to characterizing the model's weak points [22,42].",
    "next": "Even though the goals of these various model interpretability tasks vary, the vast majority of them use so called feature-based explanation, that assign importances to individual features."
  },
  {
    "index": 2433,
    "source_corpus_id": 258461498,
    "ref_id": "b47",
    "citation_corpus_id": 231847094,
    "start": 2003,
    "end": 2022,
    "title": "Published as a conference paper at ICLR 2021 LEARNING N:M FINE-GRAINED STRUCTURED SPARSE NEURAL NETWORKS FROM SCRATCH",
    "abstract": "Sparsity in Deep Neural Networks (DNNs) has been widely studied to compress and accelerate the models on resource-constrained environments. It can be generally categorized into unstructured fine-grained sparsity that zeroes out multiple individual weights distributed across the neural network, and structured coarsegrained sparsity which prunes blocks of sub-networks of a neural network. Finegrained sparsity can achieve a high compression ratio but is not hardware friendly and hence receives limited speed gains. On the other hand, coarse-grained sparsity cannot concurrently achieve both apparent acceleration on modern GPUs and decent performance. In this paper, we are the first to study training from scratch an N:M fine-grained structured sparse network, which can maintain the advantages of both unstructured fine-grained sparsity and structured coarse-grained sparsity simultaneously on specifically designed GPUs. Specifically, a 2 : 4 sparse network could achieve 2× speed-up without performance drop on Nvidia A100 GPUs. Furthermore, we propose a novel and effective ingredient, sparse-refined straightthrough estimator (SR-STE), to alleviate the negative influence of the approximated gradients computed by vanilla STE during optimization. We also define a metric, Sparse Architecture Divergence (SAD), to measure the sparse network's topology change during the training process. Finally, We justify SR-STE's advantages with SAD and demonstrate the effectiveness of SR-STE by performing comprehensive experiments on various tasks. Source codes and models are available at https://github.com/NM-sparsity/NM-sparsity.",
    "prev": "g a constant fan-in constraint.Using our empirical analysis of existing DST methods at high sparsity, we additionally employ a neuron ablation method which enables SRigL to achieve state-of-the-art sparse-to-sparse structured DST performance on a variety of Neural Network (NN) architectures.We demonstrate reduced real-world timings on CPU for online inference -3.6×/2× faster at 90% sparsity than equivalent dense/unstructured sparse layers, respectively.",
    "curr": "INTRODUCTION\n\nDynamic Sparse Training (DST) methods such as RigL (Evci et al., 2021) are the state-of-the-art in sparse training methods for Deep Neural Networks (DNNs).DST methods typically learn unstructured masks resulting in 85-95% fewer weights than dense models, while maintaining dense-like generalization and typically outperforming masks found via pruning.Furthermore, sparse-to-sparse DST algorithms are capable of employing sparsity both during training and inference, unlike pruning and denseto-sparse DST methods such as SR-STE (Zhou et al., 2021) which only exploit sparsity at inference time.",
    "next": "While models trained with DST methods are highly sparse and enable a large reduction in Floating Point Operations (FLOPs) in theory, realizing these speedups on hardware is challenging when the sparsity pattern is unstructured.Even considering recent advances in accelerating unstructured Sparse Neural Networks (SNNs) (Gale et al., 2020;Elsen et al., 2020;Ji & Chen, 2022), structured sparsity realizes much stronger acceleration on real-world hardware.On the other hand, structured sparse pruning often removes salient weights, resulting in worse generalization than comparable unstructured SNNs for the same sparsity level (Fig."
  },
  {
    "index": 2435,
    "source_corpus_id": 257505182,
    "ref_id": "b55",
    "citation_corpus_id": 247476275,
    "start": 6010,
    "end": 6013,
    "title": "Published as a conference paper at ICLR 2023 DUAL DIFFUSION IMPLICIT BRIDGES FOR IMAGE-TO-IMAGE TRANSLATION",
    "abstract": "Common image-to-image translation methods rely on joint training over data from both source and target domains. The training process requires concurrent access to both datasets, which hinders data separation and privacy protection; and existing models cannot be easily adapted for translation of new domain pairs. We present Dual Diffusion Implicit Bridges (DDIBs), an image translation method based on diffusion models, that circumvents training on domain pairs. Image translation with DDIBs relies on two diffusion models trained independently on each domain, and is a two-step process: DDIBs first obtain latent encodings for source images with the source diffusion model, and then decode such encodings using the target model to construct target images. Both steps are defined via ordinary differential equations (ODEs), thus the process is cycle consistent only up to discretization errors of the ODE solvers. Theoretically, we interpret DDIBs as concatenation of source to latent, and latent to target Schrödinger Bridges, a form of entropy-regularized optimal transport, to explain the efficacy of the method. Experimentally, we apply DDIBs on synthetic and high-resolution image datasets, to demonstrate their utility in a wide variety of translation tasks and their inherent optimal transport properties.",
    "prev": "Diffusion models [11,54,55] have gained much attention as generative models due to their stability, diversity, and scalability.",
    "curr": "Given these advantages, diffusion models have been applied in various fields, such as image translation [45,56,53], image editing [29,19], and conditional generation [18,45,67].",
    "next": "Especially, text-toimage generation has been highlighted with the introduction of various guidance techniques [12,2,13]."
  },
  {
    "index": 2437,
    "source_corpus_id": 222310549,
    "ref_id": "b34",
    "citation_corpus_id": 14124313,
    "start": 2645,
    "end": 2673,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "Nowadays, as popularized by Convolutional Neural Networks (CNNs) (Krizhevsky et al., 2012), the features used for object recognition are automatically learned, rather than manually designed.",
    "curr": "This change not only eases human efforts on feature engineering, but also yields much better performance on a wide range of visual benchmarks (Simonyan & Zisserman, 2015;He et al., 2016;Girshick et al., 2014;Girshick, 2015;Ren et al., 2015;Long et al., 2015;Chen et al., 2015).",
    "next": "But interestingly, as pointed by Geirhos et al."
  },
  {
    "index": 2438,
    "source_corpus_id": 264438904,
    "ref_id": "b20",
    "citation_corpus_id": 3497822,
    "start": 36157,
    "end": 36179,
    "title": "Parallelizing Linear Recurrent Neural Nets Over Sequence Length",
    "abstract": "Recurrent neural networks (RNNs) are widely used to model sequential data but their non-linear dependencies between sequence elements prevent parallelizing training over sequence length. We show the training of RNNs with only linear sequential dependencies can be parallelized over the sequence length using the parallel scan algorithm, leading to rapid training on long sequences with small minibatch size. We abstract prior linear sequence models into a new framework of linear surrogate RNNs and develop a linear surrogate long short-term memory (LS-LSTM) powered by a parallel linear recurrence CUDA kernel we implemented. We evaluate the LS-LSTM on a long sequence noisy autoregressive task and find the LS-LSTM achieves slightly superior train and test performance to a similar sized LSTM in 4x less training time. We analyze latency and throughput of the LS-LSTM and find the LS-LSTM reaches up to 175x the throughput of the LSTM in the small minibatch long sequence regime.",
    "prev": "generated with reencoding, have the capacity to switch between supports (non-zero elements of z) and therefore switch between the dynamics defined by K 1 and K 2 .Indeed it was shown that non-linear systems can only be linearized within a basin of attraction of a fixed point (Lan & Mezić, 2013), implying that the best we can hope to achieve is to partition the phase space into regions each of which can be well approximated by LDS, as illustrated in the example above.",
    "curr": "D EFFICIENCY\n\nProducing full trajectories for T discrete points in a nonlinear dynamical system of the form x t+1 = F(x t ) requires the repeated recurrent application of the nonlinear transformation F. For the Koopman Autoencoder, trajectories for T discrete points are first generated in the Koopman linear space z t+1 = Kz t + Lυ t and followed by a time-agnostic decoding step x = ψ(z).The linear recurrence in the latent space allows for a highly parallelizable unrolling of the predicted sequence ẑ using parallel scans (Martin & Cundy, 2018).Compared to nonlinear recurrences, training time is greatly improved.Periodic reencoding does add a nonlinear step to the Koopman autoencoder recurrence.However, since we decode and reencode periodically every k steps, we can parallelize the predicted trajectories for k discrete points and still obtain much improved training efficiencies.Furthermore, considering that our method can provide accurate predictions over short horizons (where reencoding is not needed), it is valuable in situations requiring fast look-ahead capabilities, such as model predictive control (MPC) or n−step value function bootstrapping in Reeinforcement Learning.",
    "next": "E CONNECTION TO RNNS\n\nThere exists an extensive body of literature on training RNNs on nonlinear dynamical systems (Pearlmutter, 1990;Trischler & D'Eleuterio, 2016;Vlachas et al., 2021).Koopman autoencoders can be regarded as RNN models with a single linear recurrent layer.From an efficiency perspective, linear recurrent units are more advantageous as"
  },
  {
    "index": 2439,
    "source_corpus_id": 53113128,
    "ref_id": "b3",
    "citation_corpus_id": 3489117,
    "start": 2515,
    "end": 2534,
    "title": "SMASH: One-Shot Model Architecture Search through HyperNetworks",
    "abstract": "Designing architectures for deep neural networks requires expert knowledge and substantial computation time. We propose a technique to accelerate architecture selection by learning an auxiliary HyperNet that generates the weights of a main model conditioned on that model's architecture. By comparing the relative validation performance of networks with HyperNet-generated weights, we can effectively search over a wide range of architectures at the cost of a single training run. To facilitate this search, we develop a flexible mechanism based on memory read-writes that allows us to define a wide range of network connectivity patterns, with ResNet, DenseNet, and FractalNet blocks as special cases. We validate our method (SMASH) on CIFAR-10 and CIFAR-100, STL-10, ModelNet10, and Imagenet32x32, achieving competitive performance with similarly-sized handdesigned networks.",
    "prev": "stochastic gradient descent (SGD)).",
    "curr": "Instead, we propose to learn a parametric function approximation referred to as a hypernetwork (Ha et al., 2017;Brock et al., 2018), which attempts to generate the network weights directly.",
    "next": "Learning a hypernetwork is an amortization of the cost of solving Eq."
  },
  {
    "index": 2443,
    "source_corpus_id": 263830945,
    "ref_id": "b31",
    "citation_corpus_id": 254044293,
    "start": 8352,
    "end": 8370,
    "title": "Published as a conference paper at ICLR 2023 LINK PREDICTION WITH NON-CONTRASTIVE LEARNING",
    "abstract": "Graph neural networks (GNNs) are prominent in the graph machine learning domain, owing to their strong performance across various tasks. A recent focal area is the space of graph self-supervised learning (SSL), which aims to derive useful node representations without labeled data. Notably, many state-of-theart graph SSL approaches are contrastive methods, which use a combination of positive and negative samples to learn node representations. Owing to challenges in negative sampling (slowness and model sensitivity), recent literature introduced non-contrastive methods, which instead only use positive samples. Though such methods have shown promising performance in node-level tasks, their suitability for link prediction tasks, which are concerned with predicting link existence between pairs of nodes, and have broad applicability to recommendation systems contexts, is yet unexplored. In this work, we extensively evaluate the performance of existing non-contrastive methods for link prediction in both transductive and inductive settings. While most existing non-contrastive methods perform poorly overall, we find that, surprisingly, BGRL generally performs well in transductive settings. However, it performs poorly in the more realistic inductive settings where the model has to generalize to links to/from unseen nodes. We find that non-contrastive models tend to overfit to the training graph and use this analysis to propose T-BGRL, a novel non-contrastive framework that incorporates cheap corruptions to improve the generalization ability of the model. This simple modification strongly improves inductive performance in 5/6 of our datasets, with up to a 120% improvement in Hits@50-all with comparable speed to other non-contrastive baselines, and up to 14× faster than the best-performing contrastive baseline. Our work imparts interesting findings about non-contrastive learning for link prediction and paves the way for future researchers to further expand upon this area.",
    "prev": " the testing time.Moreover, we discover that different nodes within the same graph can have varying amounts of TDS.• We design a TC inspired message-passing where a node aggregates more from neighbors who are better connected within its computational tree, which can enhance the node's weighted TC.We observe this empirically boosts LP performance and lastly discuss its noncausal limitations.",
    "curr": "Preprint\n\n\nRELATED WORK\n\nVarying Performance of GNNs on Node/Graph Classification.GNNs' efficacy in classification differs across nodes/graphs with varying label quantity (e.g., imbalanced node/graph classification (Zhao et al., 2021a;Wang et al., 2022)) and varying topology quality (e.g., long-tailed (Tang et al., 2020;Liu et al., 2021)/heterophily node classification (Zhu et al., 2020;Mao et al., 2023)).To enhance GNNs' performance for the disadvantaged nodes/graphs in these two varying conditions, previous works either apply data augmentations to derive additional supervision (Wang et al., 2021;Zhao et al., 2022) or design expressive graph convolutions to mitigate structural bias (Zhu et al., 2021).However, none of them tackle the varying performance of nodes in LP.We fill this gap by studying the relationship between node LP performance and its local topology.",
    "next": "GNN-based LP and Node-Centric Evaluation.GNN-based LP works by first learning node embeddings/subgraph embeddings through linear transformation and message-passing, and then applying the scoring function to predict link probability/subgraph class (Zhang & Chen, 2018;Shiao et al., 2022;Guo et al., 2023;Dong et al., 2022).It has achieved new SOTA performance owing to using the neural network to extract task-related information and the message-passing to encode the topological properties (e.g., common neighbors) (Yun et al., 2021;Chamberlain et al., 2022)."
  },
  {
    "index": 2445,
    "source_corpus_id": 225062378,
    "ref_id": "b29",
    "citation_corpus_id": 3488815,
    "start": 1474,
    "end": 1478,
    "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
    "abstract": "Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. * Authors ordered alphabetically.",
    "prev": "This adaptation is achieved through the use of a geometric apparatus based on Finsler metrics and a suitably chosen mirror-prox template that allows us to derive sharp convergence rates for the methods at hand.",
    "curr": "Introduction\n\nThe surge of recent breakthroughs in generative adversarial networks (GANs) [21], robust reinforcement learning [45], and other adversarial learning models [30] has sparked renewed interest in the theory of min-max optimization problems and games.",
    "next": "In this broad setting, it has become empirically clear that, ceteris paribus, the simultaneous training of two (or more) antagonistic models faces drastically new challenges relative to the training of a single one."
  },
  {
    "index": 2446,
    "source_corpus_id": 260682249,
    "ref_id": "b55",
    "citation_corpus_id": 11816014,
    "start": 3720,
    "end": 3724,
    "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
    "abstract": "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com.",
    "prev": "LLM-as-Agent\n\nThe advent of Large Language Models (LLMs) [11; 15; 101; 66; 100; 80], such as GPT-4 [56], has brought plenty of new opportunities to this realm.",
    "curr": "Through extensive alignment training [75; 58; 8; 90; 65], LLMs have not only mastered traditional NLP tasks such as question answering [61], natural language inference [10], and text summarization [52], but also showcased an impressive ability to comprehend human intent and execute instructions.",
    "next": "This has spurred the development of various applications like AutoGPT [64], BabyAGI [51], and AgentGPT [1], which employ LLMs for autonomous goal completion, sparking substantial public interest and numerous discussions."
  },
  {
    "index": 2447,
    "source_corpus_id": 231632900,
    "ref_id": "b33",
    "citation_corpus_id": 1957433,
    "start": 5174,
    "end": 5178,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "In computational linguistics there is a long tradition [19] of using distributional properties of linguistic units for quantifying semantic similarities between them, as summarized in the famous quote by JR Firth: \"a word is characterized by the company it keeps\" [14].",
    "curr": "This idea has led to powerful tools such as Latent Semantic Analysis [9], topic modelling [3], and language models like word2vec [30], GloVe [34], and, more recently, BERT [10] which relies on the Transformer model [43].",
    "next": "Specifically word2vec models are trained to maximize the likelihood of a word given its context, GloVe models utilize global word-word co-occurence statistics, and BERT uses a deep neural network with attention to predict masked words (and the next sentence)."
  },
  {
    "index": 2448,
    "source_corpus_id": 263610128,
    "ref_id": "b57",
    "citation_corpus_id": 252715691,
    "start": 2226,
    "end": 2244,
    "title": "GLM-130B: AN OPEN BILINGUAL PRE-TRAINED MODEL",
    "abstract": "We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and disconvergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B-the largest Chinese language model-across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization, without quantization aware training and with almost no performance loss, making it the first among 100B-scale models. More importantly, the property allows its effective inference on 4×RTX 3090 (24G) or 8×RTX 2080 Ti (11G) GPUs, the most ever affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at https://github.comIn this work, we introduce the pre-training of a 100B-scale model-GLM-130B, in terms of engineering efforts, model design choices, training strategies for efficiency and stability, and quantization for affordable inference. As it has been widely realized that it is computationally unaffordable to empirically enumerate all possible designs for training 100B-scale LLMs, we present not only the successful part for training GLM-130B but also many of the failed options and lessons learned. Particularly, the training stability is the decisive factor in the success of training models of such a scale. Different from practices such as manually adjusting learning rates in OPT-175B and using embedding norm in the sacrifice of performance in BLOOM-176B, we experiment with various options and find the strategy of embedding gradient shrink can significantly stabilize the training of GLM-130B.Specifically, GLM-130B is a bilingual (English and Chinese) bidirectional dense model with 130 billion parameters, pre-trained over 400 billion tokens on a cluster of 96 NVIDIA DGX-A100 (8×40G) GPU nodes between May 6 and July 3, 2022. Instead of using the GPT-style architecture, we adopt the General Language Model (GLM) algorithm (Du et al., 2022)  to leverage its bidirectional attention advantage and autoregressive blank infilling objective.Table 1summarizes the comparison between GLM-130B, GPT-3 and another two open-source efforts-OPT-175B and BLOOM-176B, as well as PaLM 540B (Chowdhery et al., 2022)-a 4× larger model-as a reference.Altogether, the conceptual uniqueness and engineering efforts enable GLM-130B to exhibit performance that surpasses the level of GPT-3 on a wide range of benchmarks (in total 112 tasks) and also outperforms PaLM 540B in many cases, while outperformance over GPT-3 has not been observed in OPT-175B and BLOOM-176B (Cf.Figure 1 (a)). For zero-shot performance, GLM-130B is better than GPT-3 175B (+5.0%), OPT-175B (+6.5%), and BLOOM-176B (+13.0%) on LAMBADA (Paperno et al., 2016), and achieves 3× better performance than GPT-3 on Big-bench-lite (Srivastava  et al., 2022). For the 5-shot MMLU (Hendrycks et al., 2021) tasks, it is better than GPT-3 175B (+0.9%) and BLOOM-176B (+12.7%). As a bilingual LLM also in Chinese, it offers significantly better results than ERNIE TITAN 3.0 260B (Wang et al., 2021)-the largest Chinese LLM-on 7 zero-shot CLUE (Xu et al., 2020) datasets (+24.26%) and 5 zero-shot FewCLUE  ones (+12.75%). Importantly, as summarized inFigure 1 (b), GLM-130B as an open model is associated with significantly less bias and generation toxicity than its 100B-scale counterparts.Finally, we design GLM-130B to empower as many people as possible to conduct 100B-scale LLM studies. First, instead of using 175B+ parameters as OPT and BLOOM, the 130B size is decided because such a size supports inference on a single A100 (8×40G) server. Second, to further lower the GPU requirements, we quantize GLM-130B into INT4 precision without quantization aware training while OPT and BLOOM can only reach INT8. Due to a unique property of the GLM architecture, GLM-130B's INT4 quantization introduces negligible performance degradation, e.g., -0.74% on LAMBADA and even +0.05% on MMLU, making it still better than the uncompressed GPT-3. This enables GLM-130B's fast inference with performance guarantee on a server of 4×RTX 3090 (24G) or 8×RTX 2080 Ti (11G), the most ever affordable GPU required for using 100B-scale LLMs to date.We open-source the model checkpoints, code, training logs, related toolkits, and lessons learned.THE DESIGN CHOICES OF GLM-130BThe architecture of a machine learning model defines its inductive bias. However, it has been realized that it is computationally unaffordable to explore various architectural designs for LLMs. We introduce and explain the unique design choices of GLM-130B. 2.1 GLM-130B'S ARCHITECTURE GLM as Backbone. Most recent 100B-scale LLMs, such as GPT-3, PaLM, OPT, and BLOOM, follow the traditional GPT-style (Radford et al., 2019) architecture of decoder-only autoregressive language modeling. In GLM-130B, we instead make an attempt to explore the potential of a bidirectional GLM-General Language Model (Du et al., 2022)-as its backbone.GLM is a transformer-based language model that leverages autoregressive blank infilling as its training objective. Briefly, for a text sequence x = [x 1 , · · · , x n ], text spans {s 1 , · · · , s m } are sampled from it, each of which s i denotes a span of consecutive tokens [s i,1 , · · · , s i,li ] and is replaced (i.e., corrupted) with a single mask token to form x corrupt . The model is asked to recover them autoregressively. To allow interactions between corrupted spans, their visibility to each other is decided by a He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters.",
    "prev": "1 * The first two authors contributed equally.Work done during Jian Xie's internship at OSU NLP Group.",
    "curr": "INTRODUCTION\n\nAfter pre-training on massive corpora, large language models (LLMs) (Brown et al., 2020;Chowdhery et al., 2022;Ouyang et al., 2022;OpenAI, 2022;2023;Zeng et al., 2023;Touvron et al., 2023a) have formed a wealth of parametric memory, such as commonsense and factual knowledge (Petroni et al., 2019;Li et al., 2022;Zhao et al., 2023).However, such parametric memory may be inaccurate or become outdated (Liska et al., 2022;Luu et al., 2022) due to misinformation in the pre-training corpus or the static nature of parametric memory, known to be a major cause for hallucinations (Elazar et al., 2021;Shuster et al., 2021;Ji et al., 2023).",
    "next": "Tool2 (Schick et al., 2023;Qin et al., 2023) or retrieval augmentation (Mallen et al., 2022;Shi et al., 2023;Ram et al., 2023) has emerged as a promising solution by providing external information as new evidence to LLMs, such as ChatGPT Plugins and New Bing.However, external evidence, inevitably, could conflict with LLMs' parametric memory.We refer to external evidence that conflicts with parametric memory as counter-memory.In this paper, we seek to answer the question: how receptive are LLMs to external evidence, especiall"
  },
  {
    "index": 2449,
    "source_corpus_id": 226281747,
    "ref_id": "b46",
    "citation_corpus_id": 91184134,
    "start": 32274,
    "end": 32292,
    "title": "FAIRSEQ: A Fast, Extensible Toolkit for Sequence Modeling",
    "abstract": "FAIRSEQ is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found here: https://www.youtube. com/watch?v=OtgDdWtHvto.",
    "prev": "the earth's surface) by simply replacing the CNF in our models with a Riemannian counterpart.",
    "curr": "ACKNOWLEDGMENTS\n\nWe acknowledge the Python community (Van Rossum & Drake Jr, 1995;Oliphant, 2007) for developing the core set of tools that enabled this work, including PyTorch (Paszke et al., 2019), torchdiffeq (Chen, 2018), fairseq (Ott et al., 2019), Jupyter (Kluyver et al., 2016), Matplotlib (Hunter, 2007, seaborn (Waskom et al., 2018), Cython (Behnel et al., 2011), numpy (Oliphant, 2006Van Der Walt et al., 2011), pandas (McKinney, 2012, and SciPy (Jones et al., 2014).",
    "next": ": Both the Jump CNF and Attentive CNF are capable of modeling different the spatial distributions based on event history, so the appearance of a new event effectively shifts the distribution instantaneously."
  },
  {
    "index": 2451,
    "source_corpus_id": 15904815,
    "ref_id": "b13",
    "citation_corpus_id": 17048295,
    "start": 18510,
    "end": 18530,
    "title": "The Inside-Outside Recursive Neural Network model for Dependency Parsing",
    "abstract": "We propose the first implementation of an infinite-order generative dependency model. The model is based on a new recursive neural network architecture, the Inside-Outside Recursive Neural Network. This architecture allows information to flow not only bottom-up, as in traditional recursive neural networks, but also topdown. This is achieved by computing content as well as context representations for any constituent, and letting these representations interact. Experimental results on the English section of the Universal Dependency Treebank show that the infinite-order model achieves a perplexity seven times lower than the traditional third-order model using counting, and tends to choose more accurate parses in k-best lists. In addition, reranking with this model achieves state-of-the-art unlabelled attachment scores and unlabelled exact match scores.",
    "prev": "This serves primarily to reduce the minimum length that information has to propagate between nodes in the tree.",
    "curr": "The R3NN can be seen as an extension and combination of several previous tree-based models, which were mainly developed in the context of natural language processing (Le & Zuidema, 2014;Paulus et al., 2014;Irsoy & Cardie, 2013).",
    "next": "CONDITIONING WITH INPUT/OUTPUT EXAMPLES\n\nNow that we have defined a generation process over tree-structured programs, we need a way of conditioning this generation process on a set of input/output examples."
  },
  {
    "index": 2452,
    "source_corpus_id": 244713935,
    "ref_id": "b20",
    "citation_corpus_id": 3292002,
    "start": 3589,
    "end": 3614,
    "title": "GRAPH ATTENTION NETWORKS",
    "abstract": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-theart results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a proteinprotein interaction dataset (wherein test graphs remain unseen during training).",
    "prev": "The vast majority of GNNs follow the message passing paradigm (Gilmer et al., 2017), using learnable non-linear functions to diffuse information on the graph.",
    "curr": "Multiple popular GNN architectures such as GCN (Kipf & Welling, 2017) and GAT (Veličković et al., 2018) can be posed as particular flavors of this scheme and considered instances of a more general framework of geometric deep learning (Bronstein et al., 2021).",
    "next": "Some of the drawbacks of the message passing paradigm have now been identified and formalized, including the limits of expressive power (Xu et al., 2019;Morris et al., 2019;Maron et al., 2019) and the problem of over-smoothing (NT & Maehara, 2019;Oono & Suzuki, 2020)."
  },
  {
    "index": 2454,
    "source_corpus_id": 252780973,
    "ref_id": "b15",
    "citation_corpus_id": 210920362,
    "start": 7520,
    "end": 7537,
    "title": "GRAPHAF: A FLOW-BASED AUTOREGRESSIVE MODEL FOR MOLECULAR GRAPH GENERATION",
    "abstract": "Molecular graph generation is a fundamental problem for drug discovery and has been attracting growing attention. The problem is challenging since it requires not only generating chemically valid molecular structures but also optimizing their chemical properties in the meantime. Inspired by the recent progress in deep generative models, in this paper we propose a flow-based autoregressive model for graph generation called GraphAF. GraphAF combines the advantages of both autoregressive and flow-based approaches and enjoys: (1) high model flexibility for data density estimation; (2) efficient parallel computation for training; (3) an iterative sampling process, which allows leveraging chemical domain knowledge for valency checking. Experimental results show that GraphAF is able to generate 68% chemically valid molecules even without chemical knowledge rules and 100% valid molecules with chemical rules. The training process of GraphAF is two times faster than the existing state-of-the-art approach GCPN. After fine-tuning the model for goal-directed property optimization with reinforcement learning, GraphAF achieves state-of-the-art performance on both chemical property optimization and constrained property optimization. 1 * Equal contribution, with order determined by flipping a coin. Work was done during internship at Mila.",
    "prev": "In this paper, we mainly focus on the fourth category of metrics, the metrics that are more or less related to the degree of coverage (or exploration) in the chemical space (other metrics are discussed in Appendix A).",
    "curr": "In this category, commonly used measures include richness, uniqueness, internal diversity, external diversity, KL divergence, and Fréchet ChemNet Distance (FCD) (Olivecrona et al., 2017;You et al., 2018;De Cao & Kipf, 2018;Elton et al., 2019;Brown et al., 2019;Popova et al., 2019;Polykovskiy et al., 2020;Shi et al., 2020;Jin et al., 2020;Xie et al., 2021).",
    "next": "Besides, Zhang et al."
  },
  {
    "index": 2455,
    "source_corpus_id": 247011642,
    "ref_id": "b20",
    "citation_corpus_id": 215745290,
    "start": 11083,
    "end": 11145,
    "title": "Pretrained Transformers Improve Out-of-Distribution Robustness",
    "abstract": "Although pretrained Transformers such as BERT achieve high accuracy on indistribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained Transformers' performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness. * Equal contribution. https://github.com/camelop/NLP-Robustness",
    "prev": "(Toneva, Sordoni, des Combes, Trischler, Bengio, and Gordon, 2018) look at \"forge ing events\", i.e., when a training examples move from being classi ed correctly to incorrectly, resembling our notion of non-monotonicity.",
    "curr": "OOD Robustness (Hendrycks, Liu, Wallace, Dziedzic, Krishnan, and Song, 2020b;Radford et al., 2021) show that large pretrained models are more robust to distributions shi and (Desai and Durre , 2020) show that large pretrained models are be er calibrated on OOD inputs.",
    "next": "ere is a also long line of literature on OOD detection (Hendrycks and Gimpel, 2016;Geifman and El-Yaniv, 2017;Liang, Li, and Srikant, 2017;Lakshminarayanan, Pritzel, and Blundell, 2016;Jiang, Kim, Guan, and Gupta, 2018;Zhang, Li, Guo, and Guo, 2020), uncertainty estimation (Ovadia, Fertig, Ren, Nado, Sculley, Nowozin, Dillon, Lakshminarayanan, and Snoek, 2019), and accuracy prediction (Deng and Zheng, 2021;Guillory, Shankar, Ebrahimi, Darrell, and Schmidt, 2021;Garg, Balakrishnan, Lipton, Neyshabur, and Sedghi, 2022) under distribution shi ."
  },
  {
    "index": 2457,
    "source_corpus_id": 263609067,
    "ref_id": "b1",
    "citation_corpus_id": 246485884,
    "start": 7327,
    "end": 7330,
    "title": "Published as a conference paper at ICLR 2022 NEURAL GRAPHICAL MODELLING IN CONTINUOUS- TIME: CONSISTENCY GUARANTEES AND ALGORITHMS",
    "abstract": "The discovery of structure from time series data is a key problem in fields of study working with complex systems. Most identifiability results and learning algorithms assume the underlying dynamics to be discrete in time. Comparatively few, in contrast, explicitly define dependencies in infinitesimal intervals of time, independently of the scale of observation and of the regularity of sampling. In this paper, we consider score-based structure learning for the study of dynamical systems. We prove that for vector fields parameterized in a large class of neural networks, least squares optimization with adaptive regularization schemes consistently recovers directed graphs of local independencies in systems of stochastic differential equations. Using this insight, we propose a score-based learning algorithm based on penalized Neural Ordinary Differential Equations (modelling the mean process) that we show to be applicable to the general setting of irregularly-sampled multivariate time series and to outperform the state of the art across a range of dynamical systems. * Work primarily conducted while at the University of Cambridge and at the Alan Turing Institute. Published as a conference paper at ICLR 2022 Published as a conference paper at ICLR 2022 Søren Wengel Mogensen, Niels Richard Hansen, et al. Markov equivalence of marginalized local independence graphs. The Annals of Statistics, 48(1):539-559, 2020. Joris M Mooij, Dominik Janzing, and Bernhard Schölkopf. From ordinary differential equations to structural causal models: the deterministic case. arXiv preprint arXiv:1304.7920, 2013. Yuval Nardi and Alessandro Rinaldo. Autoregressive process modeling via the lasso procedure. From deterministic odes to dynamic structural causal models. arXiv preprint arXiv:1608.08028, 2016. Donald B Rubin. Causal inference using potential outcomes: Design, modeling, decisions. Journal of the American Statistical Association, 100(469):322-331, 2005. J Runge, D Sejdinovic, and S Flaxman. Detecting causal associations in large nonlinear time series datasets. arXiv preprint arXiv:1702.07007, 2017. Jakob Runge. Causal network reconstruction from time series: From theoretical assumptions to practical estimation.",
    "prev": "(i) Constraint-based approaches, such as PC [49], FCI [50], and PCMCI [43; 41; 15], build causal graphs by performing conditional independence tests.",
    "curr": "(ii) Score-based learning algorithms which include penalized Neural Ordinary Differential Equations and acyclicity constraint [2] [35].",
    "next": "(iii) Approaches based on Additive Noise Model (ANM) that infer causal graph based on additive noise assumption [47; 20].ANM is extended by Hoyer et al."
  },
  {
    "index": 2458,
    "source_corpus_id": 253098130,
    "ref_id": "b67",
    "citation_corpus_id": 248178090,
    "start": 2339,
    "end": 2355,
    "title": "Published as a conference paper at ICLR 2022 ACCELERATED POLICY LEARNING WITH PARALLEL DIFFERENTIABLE SIMULATION",
    "abstract": "Deep reinforcement learning can generate complex control policies, but requires large amounts of training data to work effectively. Recent work has attempted to address this issue by leveraging differentiable simulators. However, inherent problems such as local minima and exploding/vanishing numerical gradients prevent these methods from being generally applied to control tasks with complex contact-rich dynamics, such as humanoid locomotion in classical RL benchmarks. In this work we present a high-performance differentiable simulator and a new policy learning algorithm (SHAC) that can effectively leverage simulation gradients, even in the presence of non-smoothness. Our learning algorithm alleviates problems with local minima through a smooth critic function, avoids vanishing/exploding gradients through a truncated learning window, and allows many physical environments to be run in parallel. We evaluate our method on classical RL control tasks, and show substantial improvements in sample efficiency and wall-clock time over state-of-the-art RL and differentiable simulation-based algorithms. In addition, we demonstrate the scalability of our method by applying it to the challenging high-dimensional problem of muscle-actuated locomotion with a large action space, achieving a greater than 17× reduction in training time over the best-performing established RL algorithm. More visual results are provided at: https://short-horizon-actor-critic.github.io/.",
    "prev": "2Figure 1: Q-learning yields state-of-the-art performance on various continuous control benchmarks.Simply combining bang-bang action discretization with full value decomposition scales to highdimensional control tasks and recovers performance competitive with recent actor-critic methods.Our Decoupled Q-Networks (DecQN) thereby constitute a concise baseline agent to highlight the power of simplicity and to help put recent advances in learning continuous control into perspective.",
    "curr": "INTRODUCTION\n\nReinforcement learning provides a powerful framework for autonomous systems to acquire complex behaviors through interaction.Learning efficiency remains a central aspect of algorithm design, with a broad spectrum spanning sample-efficient model-based off-policy approaches (Ha & Schmidhuber, 2018;Hafner et al., 2019) at one extreme and time-efficient on-policy approaches leveraging parallel simulation at the other extreme (Rudin et al., 2022;Xu et al., 2021).Particularly in high-dimensional domains with complicated environment dynamics and task objectives, complex trade-offs between representational capacity, exploration capabilities, and optimization accuracy commonly arise.",
    "next": "Continuous state and action spaces yield particularly challenging exploration problems due to the vast set of potential trajectories they induce.Significant research effort has focused on improving efficiency through representation learning in the context of model-free abstraction or model-based planning (Ha & Schmidhuber, 2018;Srinivas et al., 2020;Wulfmeier et al., 2021), guided exploration via auxiliary rewards (Osband et al., 2016;Pathak et al., 2017;Sekar et al., 2020;Seyde et al., 2022b), or constrained optimization particularly to stabilize learning with actor-critic approaches (Schulman et al., 2015;Haarnoja et al., 2018;Abdolmaleki et al., 2018).However, recent results have shown that competitive performance can be achieved with strongly reduced, discretize"
  },
  {
    "index": 2459,
    "source_corpus_id": 3489117,
    "ref_id": "b43",
    "citation_corpus_id": 12713052,
    "start": 4356,
    "end": 4359,
    "title": "NEURAL ARCHITECTURE SEARCH WITH REINFORCEMENT LEARNING",
    "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214. * Work done as a member of the Google Brain Residency program (g.co/brainresidency.) Under review as a conference paper at ICLR 2017 neural network can be typically specified by a variable-length string. It is therefore possible to use a recurrent network -the controller -to generate such string. Training the network specified by the string -the \"child network\" -on the real data will result in an accuracy on a validation set. Using this accuracy as the reward signal, we can compute the policy gradient to update the controller. As a result, in the next iteration, the controller will give higher probabilities to architectures that receive high accuracies. In other words, the controller will learn to improve its search over time.Our experiments show that Neural Architecture Search can design good models from scratch, an achievement considered not possible with other methods. On image recognition with CIFAR-10, Neural Architecture Search can find a novel ConvNet model that is better than most human-invented architectures. Our CIFAR-10 model achieves a 3.65 test set error, while being 1.05x faster than the current best model. On language modeling with Penn Treebank, Neural Architecture Search can design a novel recurrent cell that is also better than previous RNN and LSTM architectures. The cell that our model found achieves a test set perplexity of 62.4 on the Penn Treebank dataset, which is 3.6 perplexity better than the previous state-of-the-art.",
    "prev": "Evolutionary techniques [10,37,38,40] offer a flexible approach for discovering variegated models from trivial initial conditions, but often struggle to scale to deep neural nets where the search space is vast, even with enormous compute power [26].",
    "curr": "Reinforcement learning methods [3,44] have been used to train an agent to generate network definitions using policy gradients.",
    "next": "These methods start from trivial architectures and discover models that achieve very high performance, but can require twelve to fifteen thousand full training runs to arrive at a solution."
  },
  {
    "index": 2462,
    "source_corpus_id": 52898806,
    "ref_id": "b9",
    "citation_corpus_id": 6628106,
    "start": 8863,
    "end": 8881,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "ADAM.",
    "curr": "Proposed by Kingma & Ba (2015), the ADAM update rule is given by\nx i t+1 ← x i t − αm i t / v i t ,(4)\nwhere m t = β 1 m t−1 + (1−β 1 )g t can be seen as a momentum term and v i t = β 2 v i t−1 + (1−β 2 )(g i t ) 2 is an adaptivity term.",
    "next": "When β 1 = 0, one essentially recovers the unpublished method RMSPROP (Tieleman & Hinton, 2012), the only difference to ADAGRAD being that the sum is replaced by an exponential moving average, hence past gradients are forgotten over time in the adaptivity term v t ."
  },
  {
    "index": 2463,
    "source_corpus_id": 4885767,
    "ref_id": "b53",
    "citation_corpus_id": 12130431,
    "start": 5439,
    "end": 5442,
    "title": "INCREMENTAL NETWORK QUANTIZATION: TOWARDS LOSSLESS CNNS WITH LOW-PRECISION WEIGHTS",
    "abstract": "This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two innovations. On one hand, we introduce three interdependent operations, namely weight partition, group-wise quantization and re-training. A wellproven measure is employed to divide the weights in each layer of a pre-trained CNN model into two disjoint groups. The weights in the first group are responsible to form a low-precision base, thus they are quantized by a variable-length encoding method. The weights in the other group are responsible to compensate for the accuracy loss from the quantization, thus they are the ones to be re-trained. On the other hand, these three operations are repeated on the latest re-trained group in an iterative manner until all the weights are converted into low-precision ones, acting as an incremental network quantization and accuracy enhancement procedure. Extensive experiments on the ImageNet classification task using almost all known deep CNN architectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the efficacy of the proposed method. Specifically, at 5-bit quantization (a variable-length encoding: 1 bit for representing zero value, and the remaining 4 bits represent at most 16 different values for the powers of two) 1 , our models have improved accuracy than the 32-bit floating-point references. Taking ResNet-18 as an example, we further show that our quantized models with 4-bit, 3-bit and 2-bit ternary weights have improved or very similar accuracy against its 32-bit floating-point baseline. Besides, impressive results with the combination of network pruning and INQ are also reported. We believe that our method sheds new insights on how to make deep CNNs to be applicable on mobile or embedded devices. The code is available at https://github.com",
    "prev": "Another class of approaches uses feature hashing and weight sharing [47,41,10,9,45].",
    "curr": "Building upon the idea of weight-sharing, quantization [19,49,54] or regular structure of weight matrices can be used for reducing the effective number of parameters [53,43,11,12,48].",
    "next": "Despite their favorable empirical effectiveness in compressing neural networks, these works generally lack performance guarantees on the quality of their approximations and/or the size of the resulting compressed network."
  },
  {
    "index": 2465,
    "source_corpus_id": 252907410,
    "ref_id": "b20",
    "citation_corpus_id": 238419341,
    "start": 6942,
    "end": 6959,
    "title": "Causal Direction of Data Collection Matters: Implications of Causal and Anticausal Learning for NLP",
    "abstract": "The principle of independent causal mechanisms (ICM) states that generative processes of real world data consist of independent modules which do not influence or inform each other. While this idea has led to fruitful developments in the field of causal inference, it is not widely-known in the NLP community. In this work, we argue that the causal direction of the data collection process bears nontrivial implications that can explain a number of published NLP findings, such as differences in semi-supervised learning (SSL) and domain adaptation (DA) performance across different settings. We categorize common NLP tasks according to their causal direction and empirically assay the validity of the ICM principle for text data using minimum description length. We conduct an extensive meta-analysis of over 100 published SSL and 30 DA studies, and find that the results are consistent with our expectations based on causal insights. This work presents the first attempt to analyze the ICM principle in NLP, and provides constructive suggestions for future modeling choices. 1 * Equal contribution. 1  The codes are at https://github.com/zhijing-jin/icm4nlp. Given the English sentence above, can you write its Spanish translation? Prompt for annotators [En] This is a beautiful world. [Es] Este es un mundo hermoso. Cause: Effect: Annotation process (Noise) Effect = CausalMechanism (Cause, Noise)",
    "prev": "Contributions.",
    "curr": "Previous work on computing prequential description lengths with neural networks relied on a block-wise (chunk-incremental) approximation: at some positions t a model is trained from random initialization to convergence on data D <t and then their prediction losses on the next intervals are combined (Blier & Ollivier, 2018;Bornschein et al., 2020;Jin et al., 2021;Bornschein et al., 2021;Perez et al., 2021;Whitney et al., 2020).",
    "next": "We investigate alternatives that are inspired by continuouslearning (CL) based methods."
  },
  {
    "index": 2466,
    "source_corpus_id": 237485378,
    "ref_id": "b0",
    "citation_corpus_id": 219792787,
    "start": 3304,
    "end": 3330,
    "title": "The Recurrent Neural Tangent Kernel",
    "abstract": "The study of deep neural networks (DNNs) in the infinite-width limit, via the so-called neural tangent kernel (NTK) approach, has provided new insights into the dynamics of learning, generalization, and the impact of initialization. One key DNN architecture remains to be kernelized, namely, the recurrent neural network (RNN). In this paper we introduce and study the Recurrent Neural Tangent Kernel (RNTK), which provides new insights into the behavior of overparametrized RNNs. A key property of the RNTK should greatly benefit practitioners is its ability to compare inputs of different length. To this end, we characterize how the RNTK weights different time steps to form its output under different initialization parameters and nonlinearity choices. A synthetic and 56 real-world data experiments demonstrate that the RNTK offers significant performance gains over other kernels, including standard NTKs, across a wide array of data sets.",
    "prev": "Every model architecture is known to produce a distinct NTK.",
    "curr": "Not only for the multi-layer perceptron (MLP), many studies have been conducted across various models, such as for Convolutional Neural Networks (CNTK) (Arora et al., 2019;Li et al., 2019), Graph Neural Networks (GNTK) (Du et al., 2019b), and Recurrent Neural Networks (RNTK) (Alemohammad et al., 2021).",
    "next": "Although a number of findings have been obtained using the NTK, they are mainly for typical neural networks, and it is still not clear how to apply the NTK theory to the tree models."
  },
  {
    "index": 2467,
    "source_corpus_id": 210911499,
    "ref_id": "b0",
    "citation_corpus_id": 54458698,
    "start": 2907,
    "end": 2935,
    "title": "Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning",
    "abstract": "Text-based adventure games provide a platform on which to explore reinforcement learning in the context of a combinatorial action space, such as natural language. We present a deep reinforcement learning architecture that represents the game state as a knowledge graph which is learned during exploration. This graph is used to prune the action space, enabling more efficient exploration. The question of which action to take can be reduced to a question-answering task, a form of transfer learning that pre-trains certain parts of our architecture. In experiments using the TextWorld framework, we show that our proposed technique can learn a control policy faster than baseline alternatives. We have also open-sourced our code at https://github.com/rajammanabrolu/KG-DQN.",
    "prev": "Knowledge graphs provide us with an intuitive way of representing these partially observable worlds.",
    "curr": "Prior works have shown how using knowledge graphs aid in the twin issues of partial observability (Ammanabrolu & Riedl, 2019a) and commonsense reasoning (Ammanabrolu & Riedl, 2019b), but do not use them in the context of generating natural language.",
    "next": "To gain a sense for the challenges surrounding natural language generation, we need to first understand how large this space really is."
  },
  {
    "index": 2468,
    "source_corpus_id": 246294475,
    "ref_id": "b5",
    "citation_corpus_id": 52967399,
    "start": 2702,
    "end": 2722,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "‡ Work done while at Microsoft Research (polozov@microsoft.com).",
    "curr": "INTRODUCTION\n\nLarge language models (LLMs) trained on massive corpora of unsupervised data have been shown to perform a wide range of tasks, including natural language generation, semantic parsing and sentiment analysis (Brown et al., 2020;Devlin et al., 2019;Raffel et al., 2020).",
    "next": "This can be achieved without task-specific training, but rather by adapting the model to each task at test-time using textual prompts, which can contain examples and natural language descriptions."
  },
  {
    "index": 2470,
    "source_corpus_id": 263608672,
    "ref_id": "b8",
    "citation_corpus_id": 44134226,
    "start": 2194,
    "end": 2211,
    "title": "Hierarchical Neural Story Generation",
    "abstract": "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.",
    "prev": "sed on our findings, we develop an experimental truncation strategy and the present pilot studies demonstrating the promise of this type of algorithm.Our evaluations show that our method outperforms its threshold-based counterparts under automatic and human evaluation metrics for low-entropy (i.e., close to greedy) open-ended text generation.Our theoretical findings and pilot experiments provide both insight into why truncation sampling works, and make progress toward more expressive sampling algorithms that better surface the generative capabilities of large language models.",
    "curr": "INTRODUCTION\n\nCrucial to the remarkable generative capabilities of today's large language models (LLMs) (Ope-nAI, 2023;Touvron et al., 2023;Chowdhery et al., 2022) are the sampling algorithms responsible for selecting the next token at each timestep.The most common of these algorithms use a simple truncation strategy: sample only the tokens that have probability greater than some threshold (Holtzman et al., 2020;Fan et al., 2018).In the quest for high-entropy generation wherein one wants to be able to generate multiple good completions, it has been empirically established that the search for the highest-likelihood strings through e.g., beam search or greedy decoding led to low-quality generations (Hashimoto et al., 2019).Threshold-based truncation sampling presents a compelling alternative: by avoiding the tokens at the tail end of the distribution which correspond to degenerate text it produces significantly more coherent generations (Ippolito et al., 2019;Holtzman et al., 2020;DeLucia et al., 2021).However, beyond the intuition that language models tend to assign too much probability to tokens that should have 0 or near-0 probability (akin to smoothing (Hewitt et al., 2022)), prior work has been limited in establishing why truncation sampling is so essential in autoregressive generation.",
    "next": "In this paper, we provide a precise mathematical explanation to elucidate the extraordinary success of threshold-based tr"
  },
  {
    "index": 2471,
    "source_corpus_id": 52910185,
    "ref_id": "b26",
    "citation_corpus_id": 1684853,
    "start": 4540,
    "end": 4565,
    "title": "EPOPT: LEARNING ROBUST NEURAL NETWORK POLICIES USING MODEL ENSEMBLES",
    "abstract": "Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks, especially when the policies are represented using rich function approximators like deep neural networks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training. We introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning/adaptation.",
    "prev": "Our key insight is that we can bypass learning the value function and directly learn a policy that maps beliefs to actions by leveraging the latest advancements in batch policy optimization algorithms (Schulman et al., 2015(Schulman et al., , 2017.",
    "curr": "Inspired by previous approaches where the learning algorithms are trained with an ensemble of models (Rajeswaran et al., 2017;Yu et al., 2017), we examine model uncertainty through the lens of the BAMDP framework.",
    "next": "Although our approach provides only locally-optimal policies, we believe that it offers a practical and scalable solution for continuous BAMDPs."
  },
  {
    "index": 2473,
    "source_corpus_id": 231925076,
    "ref_id": "b8",
    "citation_corpus_id": 65455367,
    "start": 6004,
    "end": 6024,
    "title": "ON THE CONVERGENCE OF ADAM AND BEYOND",
    "abstract": "Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSPROP, ADAM, ADADELTA, NADAM are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where ADAM does not converge to the optimal solution, and describe the precise problems with the previous analysis of ADAM algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with \"long-term memory\" of past gradients, and propose new variants of the ADAM algorithm which not only fix the convergence issues but often also lead to improved empirical performance.",
    "prev": "Despite its remarkable practicality, Adam suffers the non-convergence issue .",
    "curr": "To overcome its advantages, several variants such as AMSGrad and AdamNC were proposed (Reddi et al., 2018).",
    "next": "Unfortunately, the best regret bound of AMSGrad is only O( √ log t √ t) for nonsmooth convex problems (Reddi et al., 2018), as opposed to the optimality of O( √ t) of SGD."
  },
  {
    "index": 2478,
    "source_corpus_id": 235683072,
    "ref_id": "b42",
    "citation_corpus_id": 71145737,
    "start": 2645,
    "end": 2672,
    "title": "Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples",
    "abstract": "Few-shot classification refers to learning a classifier for new classes given only a few examples. While a plethora of models have emerged to tackle this recently, we find the current procedure and datasets that are used to systematically assess progress in this setting lacking. To address this, we propose META-DATASET: a new benchmark for training and evaluating few-shot classifiers that is large-scale, consists of multiple datasets, and presents more natural and realistic tasks. The aim is to measure the ability of state-ofthe-art models to leverage diverse sources of data to achieve higher generalization, and to evaluate that generalization ability in a more challenging setting. We additionally measure robustness of current methods to variations in the number of available examples and the number of classes. Finally our extensive empirical evaluation leads us to identify weaknesses in Prototypical Networks and MAML, two popular few-shot classification methods, and to propose a new method, Proto-MAML, which achieves improved performance on our benchmark.",
    "prev": "Concretely, MAML aims to learn a good model initialization (through the outer loop optimization), which can then be quickly adapted to novel tasks given few examples (through the inner loop optimization).",
    "curr": "However, in few-shot classification  which many meta-learning algorithms are dedicated to, MAML's performance has been shown to fall far behind Triantafillou et al., 2020).",
    "next": "In this paper, we take a closer look at MAML on few-shot classification."
  },
  {
    "index": 2480,
    "source_corpus_id": 264306282,
    "ref_id": "b25",
    "citation_corpus_id": 209531600,
    "start": 31337,
    "end": 31364,
    "title": "PAC CONFIDENCE SETS FOR DEEP NEURAL NET- WORKS VIA CALIBRATED PREDICTION",
    "abstract": "We propose an algorithm combining calibrated prediction and generalization bounds from learning theory to construct confidence sets for deep neural networks with PAC guarantees-i.e., the confidence set for a given input contains the true label with high probability. We demonstrate how our approach can be used to construct PAC confidence sets on ResNet for ImageNet, a visual object tracking model, and a dynamics model for the half-cheetah reinforcement learning problem. 1 1 Our code is available at https://github.com/sangdon/PAC-confidence-set. . Safe model-based reinforcement learning with stability guarantees. In Advances in neural information processing systems, pp. 908-918, 2017.Marko Bohanec and Vladislav Rajkovic. Knowledge acquisition and explanation for multi-attribute decision making.",
    "prev": "modifying Gaussian elimination to propagate intervals.Our algorithm provides provable PAC guarantees and produces smaller prediction sets than several baselines that satisfy this condition.Experiments on five datasets demonstrate its effectiveness.Directions for future work include improving performance when the calibration dataset is small or when the label shift is small.",
    "curr": "A ADDITIONAL RELATED WORK\n\nConformal Prediction.Our work falls into the broad area of distribution-free uncertainty quantification (Guttman, 1970).Specifically, it builds on ideas from conformal prediction (Vovk et al., 2005;Balasubramanian et al., 2014;Angelopoulos & Bates, 2021), which aims to construct prediction sets with finite sample guarantees.The prediction sets are realized by a setting a threshold on top of a traditional single-label predictor (i.e.conformity/non-conformity scoring function) and predicting all labels with scores above the threshold.Our approach is based on inductive conformal prediction (Papadopoulos et al., 2002;Vovk, 2012;Lei et al., 2015), where the dataset is split into a training set for training the scoring function and a calibration set for constructing the prediction sets.",
    "next": "PAC Prediction Sets.Unlike conformal prediction methods that achieve a coverage guarantee of the median coverage performance.PAC prediction sets consider training-data conditional correctness (Vovk, 2012;Park et al., 2019).That is, we achieve a (ε, δ)-guarantee that our performance on all future possible examples would only exceed the desired error rate level ε with probability under δ.This guarantee is also equivalent to the \"content\" guarantee for tolerance regions (Wilks, 1941;Fraser, 1956).All in all, PAC prediction sets obtain a totally different statistical guarantee compared to conformal prediction methods.The preference of the two methods depends on the application scenario.Obviously, PAC prediction sets favor the case when even a slight excess of the desired error rate would cause a significant safety concer"
  },
  {
    "index": 2481,
    "source_corpus_id": 263909148,
    "ref_id": "b1",
    "citation_corpus_id": 254043800,
    "start": 1894,
    "end": 1915,
    "title": "Published as a conference paper at ICLR 2023 WHAT LEARNING ALGORITHM IS IN-CONTEXT LEARN- ING? INVESTIGATIONS WITH LINEAR MODELS",
    "abstract": "Neural sequence models, especially transformers, exhibit a remarkable capacity for in-context learning. They can construct new predictors from sequences of labeled examples (x, f (x)) presented in the input without further parameter updates. We investigate the hypothesis that transformer-based in-context learners implement standard learning algorithms implicitly, by encoding smaller models in their activations, and updating these implicit models as new examples appear in the context. Using linear regression as a prototypical problem, we offer three sources of evidence for this hypothesis. First, we prove by construction that transformers can implement learning algorithms for linear models based on gradient descent and closed-form ridge regression. Second, we show that trained in-context learners closely match the predictors computed by gradient descent, ridge regression, and exact least-squares regression, transitioning between different predictors as transformer depth and dataset noise vary, and converging to Bayesian estimators for large widths and depths. Third, we present preliminary evidence that in-context learners share algorithmic features with these predictors: learners' late layers non-linearly encode weight vectors and moment matrices. These results suggest that in-context learning is understandable in algorithmic terms, and that (at least in the linear case) learners may rediscover standard estimation algorithms.",
    "prev": "However, the statistical foundation of ICL is still in its infancy.",
    "curr": "A recent line of research aims to quantify ICL by studying transformers pretrained on the linear regression task with a Gaussian prior (Garg et al., 2022;Akyürek et al., 2022;Li et al., 2023b;Raventós et al., 2023).",
    "next": "Specifically, Garg et al."
  },
  {
    "index": 2482,
    "source_corpus_id": 264147054,
    "ref_id": "b33",
    "citation_corpus_id": 3725815,
    "start": 2294,
    "end": 2312,
    "title": "Self-Attention with Relative Position Representations",
    "abstract": "Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al.(2017)achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graphlabeled inputs.",
    "prev": "INTRODUCTION\n\nThe transformer model (Vaswani et al., 2017), which is composed of a stack of permutation symmetric layers, processes input tokens as a set and lacks direct awareness of the tokens' structural information.Consequently, transformer models are not solely perceptible to the structures of input tokens, such as word order in NLP or 2D positions of image pixels or patches in image processing.",
    "curr": "A common way to make transformers position-aware is through vector embeddings: in NLP, a typical way is to transform the position values of the word tokens into embedding vectors to be added to input tokens or attention weights (Vaswani et al., 2017;Shaw et al., 2018).While initially designed for NLP, these positional encoding techniques are widely used for 2D and 3D vision tasks today (Wang et al., 2018;Dosovitskiy et al., 2021;Sajjadi et al., 2022b;Du et al., 2023).",
    "next": "Here, a natural question arises: \"Are existing encoding schemes suitable for tasks with very different geometric structures?"
  },
  {
    "index": 2483,
    "source_corpus_id": 48352800,
    "ref_id": "b41",
    "citation_corpus_id": 13570924,
    "start": 1540,
    "end": 1544,
    "title": "Variational Continual Learning",
    "abstract": "This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that variational continual learning outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.",
    "prev": "Introduction\n\nThere is a resurgence of research interests in Bayesian deep learning [7,22,[25][26][27]46], which applies Bayesian inference to neural networks for better uncertainty estimation that is crucial for e.g.",
    "curr": "better exploration in reinforcement learning [13,14], resisting adversarial attacks [20,34,36] and continual learning [41].",
    "next": "A popular approach to performing Bayesian inference on neural networks is stochastic gradient Markov chain Monte Carlo (SG-MCMC), which adds properly scaled Gaussian noise to a stochastic gradient ascent procedure [57]."
  },
  {
    "index": 2484,
    "source_corpus_id": 252735112,
    "ref_id": "b9",
    "citation_corpus_id": 207880568,
    "start": 2503,
    "end": 2525,
    "title": "Unsupervised Cross-lingual Representation Learning at Scale",
    "abstract": "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of crosslingual transfer tasks. We train a Transformerbased masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing perlanguage performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available. 1",
    "prev": "INTRODUCTION\n\nRecent work has shown that presenting explicit reasoning steps (i.e., chains of thought; COT) in English elicits multi-step reasoning abilities of large language models such as GPT-3 and PaLM (Brown et al., 2020;Chowdhery et al., 2022;Wei et al., 2022b, inter alia).",
    "curr": "Pretrained multilingual language models have also achieved impressive performance on various NLP tasks across typologically distinct languages (Conneau et al., 2020;Xue et al., 2021;Chowdhery et al., 2022;Clark et al., 2020;Hu et al., 2020;Ruder et al., 2021, inter alia).",
    "next": "Tasks in existing multilingual benchmarks usually require only simple reasoning steps, and so it is still unclear how well language models perform on tasks that require more complex reasoning in a multilingual setting."
  },
  {
    "index": 2485,
    "source_corpus_id": 204090878,
    "ref_id": "b48",
    "citation_corpus_id": 3833554,
    "start": 9391,
    "end": 9415,
    "title": "Wasserstein Auto-Encoders",
    "abstract": "We propose the Wasserstein Auto-Encoder (WAE)-a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE) [1]. This regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE)[2]. Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality, as measured by the FID score.",
    "prev": "In VHE-GAN, the optimization of the encoder parameter E is related to not only the VHE's ELBO, but also the GAN mini-max objective function, forcing the variational posterior q(z | x) to serve as a bridge between VHE and GAN, allowing them to help each other.",
    "curr": "Although there are some models (Mescheder et al., 2017;Makhzani et al., 2015;Tolstikhin et al., 2018;Dumoulin et al., 2017;Donahue et al., 2017;Che et al., 2017;Srivastava et al., 2017;Grover et al., 2018;Larsen et al., 2016; combining VAEs and GANs in various ways, they focus on single-modality tasks while the VHE-GAN on two different modalities.",
    "next": "In Appendix A, we analyze the properties of the VHE-GAN objective function and discuss related works."
  },
  {
    "index": 2486,
    "source_corpus_id": 246210145,
    "ref_id": "b11",
    "citation_corpus_id": 225039882,
    "start": 12207,
    "end": 12233,
    "title": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. 1 arXiv:2010.11929v2 [cs.CV] 3 Jun 2021Published as a conference paper at ICLR 2021 inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.However, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks.RELATED WORKTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global selfattention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.",
    "prev": "More recently, Xie et al.",
    "curr": "(2021) integrate the GCP meta-layer into the vision transformer (Dosovitskiy et al., 2020) to exploit the second-order statistics of the high-level visual tokens, which solves the issue that vision transformers need pre-training on ultra-large-scale datasets.",
    "next": "Another line of research proposes to use ZCA whitening, which applies the inverse square root of the covariance to whiten the feature, as an alternative scheme for the standard batch normalization (Ioffe & Szegedy, 2015)."
  },
  {
    "index": 2490,
    "source_corpus_id": 233307448,
    "ref_id": "b28",
    "citation_corpus_id": 22421874,
    "start": 35001,
    "end": 35028,
    "title": "Knowledge Distillation for Bilingual Dictionary Induction",
    "abstract": "Leveraging zero-shot learning to learn mapping functions between vector spaces of different languages is a promising approach to bilingual dictionary induction. However, methods using this approach have not yet achieved high accuracy on the task. In this paper, we propose a bridging approach, where our main contribution is a knowledge distillation training objective. As teachers, rich resource translation paths are exploited in this role. And as learners, translation paths involving low resource languages learn from the teachers. Our training objective allows seamless addition of teacher translation paths for any given low resource pair. Since our approach relies on the quality of monolingual word embeddings, we also propose to enhance vector representations of both the source and target language with linguistic information. Our experiments on various languages show large performance gains from our distillation training objective, obtaining as high as 17% accuracy improvements.",
    "prev": "(2020) demonstrate that when applied to image data, distillation allows the student neural net to learn multiple visual concepts simultaneously, while, when learning from raw data, neural networks learn concepts sequentially.",
    "curr": "Knowledge distillation has also been used for adversarial attacks (Papernot et al., 2016b;Ross & Doshi-Velez, 2017;Gil et al., 2019;Goldblum et al., 2020), data security (Papernot et al., 2016a;Lopes et al., 2017;, image processing (Li & Hoiem, 2017;Wang et al., 2017;Chen et al., 2018;, natural language processing (Nakashole & Flauger, 2017;Mou et al., 2016;Hu et al., 2018;Freitag et al., 2017), and speech processing (Chebotar & Waters, 2016;Lu et al., 2017;Watanabe et al., 2017;Oord et al., 2018;Shen et al., 2018).",
    "next": "(Z;f (X),p 0 (X)) Loss function on a random data point\n\n\nB GLOSSARY\nPopulation risk L D (f,p) E[ (Z;f (X),p(X))] Empirical risk L n (f,p) E n [ (Z;f (X),p(X))] Population optimal student model f 0 argmin f ∈F L D (f,p 0 ) Empirical optimal student modelf argmin f ∈F L n (f,p) f p,q f (X) p L q = E f (X) q p 1/q ∇ φ\nPartial derivative of (z;φ,π) with respect to the second input ∇ π Partial derivative of (z;φ,π) with respect to the third input ∇ φπ [∇ φπ (z;φ,π)] i,j = ∂ 2 ∂φj ∂πi (z;φ,π)\nq f,p (x) E[∇ φπ (Z;f (X),p(X)) | X = x] γ f,p (x) E U ∼Unif([0,1]) [q f,U p+(1−U )p0 (x)] R(δ;F)\nLocalized Rademacher complexity of function class F δ n Critical radius γ-corrected loss γ (z;f (x),p(x)) (z;f (x),p(x))+(y−p(x)) γ(x)f (x) Population γ-risk L D (f,p,γ) E[ γ (Z;f (X),p(X))] Empirical γ-risk L n (f,p,γ) with probab"
  },
  {
    "index": 2491,
    "source_corpus_id": 247476286,
    "ref_id": "b68",
    "citation_corpus_id": 210164926,
    "start": 9834,
    "end": 9852,
    "title": "Published as a conference paper at ICLR 2020 FAST IS BETTER THAN FREE: REVISITING ADVERSARIAL TRAINING",
    "abstract": "Adversarial training, a method for learning robust deep networks, is typically assumed to be more expensive than traditional training due to the necessity of constructing adversarial examples via a first-order method like projected gradient decent (PGD). In this paper, we make the surprising discovery that it is possible to train empirically robust models using a much weaker and cheaper adversary, an approach that was previously believed to be ineffective, rendering the method no more costly than standard training in practice. Specifically, we show that adversarial training with the fast gradient sign method (FGSM), when combined with random initialization, is as effective as PGD-based training but has significantly lower cost. Furthermore we show that FGSM adversarial training can be further accelerated by using standard techniques for efficient training of deep networks, allowing us to learn a robust CIFAR10 classifier with 45% robust accuracy to PGD attacks with = 8/255 in 6 minutes, and a robust ImageNet classifier with 43% robust accuracy at = 2/255 in 12 hours, in comparison to past work based on \"free\" adversarial training which took 10 and 50 hours to reach the same respective thresholds. Finally, we identify a failure mode referred to as \"catastrophic overfitting\" which may have caused previous attempts to use FGSM adversarial training to fail. All code for reproducing the experiments in this paper as well as pretrained model weights are at https://github.com/locuslab/fast_adversarial. * Equal contribution. arXiv:2001.03994v1 [cs.LG] 12 Jan 2020Published as a conference paper at ICLR 2020 that tries to to reduce the complexity of generating an adversarial example, which forms the bulk of the additional computation in adversarial trainingShafahi et al., 2019). While these works present reasonable improvements to the runtime of adversarial training, they are still significantly slower than standard training, which has been greatly accelerated due to competitions for optimizing both the speed and cost of training(Coleman et al., 2017).In this work, we argue that adversarial training, in fact, is not as hard as has been suggested by this past line of work. In particular, we revisit one of the the first proposed methods for adversarial training, using the Fast Gradient Sign Method (FGSM) to add adversarial examples to the training process(Goodfellow et al., 2014). Although this approach has long been dismissed as ineffective, we show that by simply introducing random initialization points, FGSM-based training is as effective as projected gradient descent based training while being an order of magnitude more efficient. Moreover, FGSM adversarial training (and to a lesser extent, other adversarial training methods) can be drastically accelerated using standard techniques for efficient training of deep networks, including e.g. cyclic learning rates(Smith & Topin, 2018), mixed-precision training (Micikevicius  et al., 2017, and other similar techniques. The method has extremely few free parameters to tune, and can be easily adapted to most training procedures. We further identify a failure mode that we call \"catastrophic overfitting\", which may have caused previous attempts at FGSM adversarial training to fail against PGD-based attacks.The end result is that, with these approaches, we are able to train (empirically) robust classifiers far faster than in previous work. Specifically, we train an ∞ robust CIFAR10 model to 45% accuracy at = 8/255 (the same level attained in previous work) in 6 minutes; previous papers reported times of 80 hours for PGD-based training(Madry et al., 2017)and 10 hours for the more recent \"free\" adversarial training method(Shafahi et al., 2019). Similarly, we train an ∞ robust ImageNet classifier to 43% top-1 accuracy at = 2/255 (again matching previous results) in 12 hours of training (compared to 50 hours in the best reported previous work that we are aware of (Shafahi et al., 2019)). Both of these times roughly match the comparable time for quickly training a standard non-robust model to reasonable accuracy. We extensively evaluate these results against strong PGDbased attacks, and show that they obtain the same empirical performance as the slower, PGD-based training. Thus, we argue that despite the conventional wisdom, adversarially robust training is not actually more challenging than standard training of deep networks, and can be accomplished with the notoriously weak FGSM attack.",
    "prev": "compared to CNNs, and (2) how to efficiently construct strong patch-wise attacks utilizing the unique structures of ViTs are still under-explored yet interesting to be studied, especially considering patches are the basic elements for composing the inputs of ViTs.",
    "curr": "In response, various defense methods (Guo et al., 2017;Xie et al., 2017;Cohen et al., 2019;Metzen et al., 2017;Feinman et al., 2017;Fu et al., 2021a;Shafahi et al., 2019;Wong et al., 2019) have been proposed to improve DNNs' robustness against those attacks.",
    "next": "The readers are referred to (Akhtar & Mian, 2018;Chakraborty et al., 2018) for more attack and defense methods."
  },
  {
    "index": 2492,
    "source_corpus_id": 246431258,
    "ref_id": "b4",
    "citation_corpus_id": 208139512,
    "start": 40844,
    "end": 40861,
    "title": "ROBUST ANOMALY DETECTION AND BACKDOOR ATTACK DETECTION VIA DIFFERENTIAL PRIVACY",
    "abstract": "Outlier detection and novelty detection are two important topics for anomaly detection. Suppose the majority of a dataset are drawn from a certain distribution, outlier detection and novelty detection both aim to detect data samples that do not fit the distribution. Outliers refer to data samples within this dataset, while novelties refer to new samples. In the meantime, backdoor poisoning attacks for machine learning models are achieved through injecting poisoning samples into the training dataset, which could be regarded as \"outliers\" that are intentionally added by attackers. Differential privacy has been proposed to avoid leaking any individual's information, when aggregated analysis is performed on a given dataset. It is typically achieved by adding random noise, either directly to the input dataset, or to intermediate results of the aggregation mechanism. In this paper, we demonstrate that applying differential privacy can improve the utility of outlier detection and novelty detection, with an extension to detect poisoning samples in backdoor attacks. We first present a theoretical analysis on how differential privacy helps with the detection, and then conduct extensive experiments to validate the effectiveness of differential privacy in improving outlier detection, novelty detection, and backdoor attack detection.",
    "prev": "In this section, we take the SiamFC++ tracker as an example (under the one-shot mode on OTB100 dataset) to test the robustness of our FSBA to two other model reconstruction based defenses.",
    "curr": "In particular, we notice that there were also some other types of backdoor defenses (Du et al., 2020;Li et al., 2021a;Huang et al., 2022) targeting the poison-only backdoor attacks, where the adversaries can only manipulate the training dataset.",
    "next": "These defenses are out of the scope of this paper since we assume that the adversary has full control over the training process."
  },
  {
    "index": 2494,
    "source_corpus_id": 2129889,
    "ref_id": "b7",
    "citation_corpus_id": 8004345,
    "start": 2171,
    "end": 2191,
    "title": "An ISU Dialogue System Exhibiting Reinforcement Learning of Dialogue Policies: Generic Slot-filling in the TALK In-car System",
    "abstract": "We demonstrate a multimodal dialogue system using reinforcement learning for in-car scenarios, developed at Edinburgh University and Cambridge University for the TALK project 1 . This prototype is the first \"Information State Update\" (ISU) dialogue system to exhibit reinforcement learning of dialogue strategies, and also has a fragmentary clarification feature. This paper describes the main components and functionality of the system, as well as the purposes and future use of the system, and surveys the research issues involved in its construction. Evaluation of this system (i.e. comparing the baseline system with handcoded vs. learnt dialogue policies) is ongoing, and the demonstration will show both.",
    "prev": "Traditional dialog systems (Young et al., 2013) are composed of several modules including a language interpreter, a dialog state tracker and a response generator, usually each is separately hand-crafted or trained.",
    "curr": "Such systems are specialized for a single domain and rely heavily on expert knowledge; for instance, most of these systems, called slot-filling methods (Lemon et al., 2006;Wang and Lemon, 2013), require to predefine the structure of the dialog state as a form composed of a set of slots to be filled during the dialog.",
    "next": "For a restaurant reservation system, examples of slots can be the location, the price range or the type of cuisine of a restaurant."
  },
  {
    "index": 2495,
    "source_corpus_id": 53464644,
    "ref_id": "b15",
    "citation_corpus_id": 3626819,
    "start": 7994,
    "end": 7997,
    "title": "Deep contextualized word representations",
    "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
    "prev": "Instead, we seek to learn embeddings that encode the full structural context in which each amino acid occurs.",
    "curr": "This is inspired partly by the recent success of unsupervised contextual embedding models using bidirectional recurrent neural network language models [15,16] where word embeddings, learned as a function of their context, have been successfully transferred to other tasks.",
    "next": "In particular, we apply a similar language model for the first time on protein sequences as part of our supervised framework."
  },
  {
    "index": 2497,
    "source_corpus_id": 54443381,
    "ref_id": "b19",
    "citation_corpus_id": 22014305,
    "start": 27161,
    "end": 27184,
    "title": "ROUTING NETWORKS: ADAPTIVE SELECTION OF NON-LINEAR FUNCTIONS FOR MULTI-TASK LEARN- ING",
    "abstract": "Multi-task learning (MTL) with neural networks leverages commonalities in tasks to improve performance, but often suffers from task interference which reduces the benefits of transfer. To address this issue we introduce the routing network paradigm, a novel neural network and training algorithm. A routing network is a kind of self-organizing neural network consisting of two components: a router and a set of one or more function blocks. A function block may be any neural network -for example a fully-connected or a convolutional layer. Given an input the router makes a routing decision, choosing a function block to apply and passing the output back to the router recursively, terminating when a fixed recursion depth is reached. In this way the routing network dynamically composes different function blocks for each input. We employ a collaborative multi-agent reinforcement learning (MARL) approach to jointly train the router and function blocks. We evaluate our model against cross-stitch networks and shared-layer baselines on multi-task settings of the MNIST, mini-imagenet, and CIFAR-100 datasets. Our experiments demonstrate a significant improvement in accuracy, with sharper convergence. In addition, routing networks have nearly constant per-task training cost while cross-stitch networks scale linearly with the number of tasks. On CIFAR-100 (20 tasks) we obtain cross-stitch performance levels with an 85% reduction in training time.",
    "prev": "RELATED WORK\n\nContinual (Ring, 1997) or Lifelong Learning (LLL) (Thrun, 1998) have been the subject of extensive study over the past two decades.",
    "curr": "One approach to LLL uses modular compositional models (Fernando et al., 2017;Aljundi et al., 2017;Rosenbaum et al., 2018;Chang et al., 2018;Xu & Zhu, 2018;Ferran Alet, 2018), which limit interference among tasks by using different subset of modules for each task.",
    "next": "Unfortunately, these methods require searching over the space of architectures which is not sample efficient with current methods."
  },
  {
    "index": 2499,
    "source_corpus_id": 264128166,
    "ref_id": "b12",
    "citation_corpus_id": 238744253,
    "start": 84414,
    "end": 84431,
    "title": "Published as a conference paper at ICLR 2022 AMORTIZED TREE GENERATION FOR BOTTOM-UP SYNTHESIS PLANNING AND SYNTHESIZABLE MOLECULAR DESIGN",
    "abstract": "Molecular design and synthesis planning are two critical steps in the process of molecular discovery that we propose to formulate as a single shared task of conditional synthetic pathway generation. We report an amortized approach to generate synthetic pathways as a Markov decision process conditioned on a target molecular embedding. This approach allows us to conduct synthesis planning in a bottom-up manner and design synthesizable molecules by decoding from optimized conditional codes, demonstrating the potential to solve both problems of design and synthesis simultaneously. The approach leverages neural networks to probabilistically model the synthetic trees, one reaction step at a time, according to reactivity rules encoded in a discrete action space of reaction templates. We train these networks on hundreds of thousands of artificial pathways generated from a pool of purchasable compounds and a list of expert-curated templates. We validate our method with (a) the recovery of molecules using conditional generation, (b) the identification of synthesizable structural analogs, and (c) the optimization of molecular structures given oracle functions relevant to drug discovery.",
    "prev": "\"Given that the end product of these papers is a value function that is plugged into a previously proposed algorithm (typically MCTS or retro*), we think these papers should be more accurately viewed as proposing heuristics.The heuristic is orthogonal to the underlying search algorithm, so we view these works as complementary rather than competitive.We hope in the future to investigate learning heuristics for retro-fallback using similar principles.",
    "curr": "G.2 GENERATIVE MODELS\n\nSeveral works propose parametric generative models of synthesis plans (Bradshaw et al., 2019;2020;Gottipati et al., 2020;Gao et al., 2021).Although this resembles the goal of explicit search algorithms, such generative models are fundamentally limited by their parametrization: they have no guarantee to find a synthesis plan if it exists, and are often observed to fail to produce a valid synthesis plan in practice (Gao et al., 2021).We think such models are best viewed as trying to amortize the output of an explicit planning algorithms, making them more similar in spirit to search heuristics (G.1).",
    "next": "G.3 SINGLE-STEP RETROSYNTHESIS\n\nMany models have been proposed to predict possible chemical reactions, including template classifiers (Segler & Waller, 2017;Seidl et al., 2021), graph editing methods (Dai et al., 2019;Sacha et al., 2021;Chen & Jung, 2021), and transformers (Irwin et al., 2022;Zhong et al., 2022).Such models are a useful component of a retrosynthesis algorithm, but do not themselves perform multi-step retrosynthesis."
  },
  {
    "index": 2500,
    "source_corpus_id": 259316542,
    "ref_id": "b54",
    "citation_corpus_id": 257220165,
    "start": 2579,
    "end": 2601,
    "title": "Published as a conference paper at ICLR 2023 DENOISING DIFFUSION SAMPLERS",
    "abstract": "Denoising diffusion models are a popular class of generative models providing state-of-the-art results in many domains. One adds gradually noise to data using a diffusion to transform the data distribution into a Gaussian distribution. Samples from the generative model are then obtained by simulating an approximation of the time-reversal of this diffusion initialized by Gaussian samples. Practically, the intractable score terms appearing in the time-reversed process are approximated using score matching techniques. We explore here a similar idea to sample approximately from unnormalized probability density functions and estimate their normalizing constants. We consider a process where the target density diffuses towards a Gaussian. Denoising Diffusion Samplers (DDS) are obtained by approximating the corresponding time-reversal. While score matching is not applicable in this context, we can leverage many of the ideas introduced in generative modeling for Monte Carlo sampling. Existing theoretical results from denoising diffusion models also provide theoretical guarantees for DDS. We discuss the connections between DDS, optimal control and Schrödinger bridges and finally demonstrate DDS experimentally on a variety of challenging sampling tasks. . Quantum ground states from reinforcement learning. In Mathematical and Scientific Machine Learning, 2020.Alessandro Beghi. On the relative entropy of discrete-time Markov processes with given end-point densities. . Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, 2020.Emiel Hoogeboom and Tim Salimans. Blurring diffusion models. arXiv preprint arXiv:2209.05557, 2022.Aapo Hyvärinen. Estimation of non-normalized statistical models by score matching.",
    "prev": "Fueled by the success of denoising diffusion probabilistic modeling Ho et al., 2020;Kingma et al., 2021;Vahdat et al., 2021;Nichol & Dhariwal, 2021) and deep learning approaches to the Schrödinger bridge (SB) problem (De Bortoli et al., 2021;Chen et al., 2021a;Koshizuka & Sato, 2022), there is a significant interest in tackling the sampling problem by using stochastic differential equations (SDEs) which are controlled with learned neural networks to transport a given prior density p prior to the target p target .",
    "curr": "Workshop on New Frontiers in\n\nRecent works include the Path Integral Sampler (PIS) and variations thereof (Tzen & Raginsky, 2019;Richter, 2021;Zhang & Chen, 2022;Vargas et al., 2023b), the Time-Reversed Diffusion Sampler (DIS) , as well as the Denoising Diffusion Sampler (DDS) (Vargas et al., 2023a).",
    "next": "While the ideas for such sampling approaches based on controlled diffusion processes date back to earlier work, see, e.g., Dai Pra (1991); Pavon (1989), the development of corresponding numerical methods based on deep learning has become popular in the last few years."
  },
  {
    "index": 2501,
    "source_corpus_id": 252595603,
    "ref_id": "b3",
    "citation_corpus_id": 182953134,
    "start": 11861,
    "end": 11879,
    "title": "Published as a conference paper at ICLR 2020 DEEP BATCH ACTIVE LEARNING BY DIVERSE, UNCERTAIN GRADIENT LOWER BOUNDS",
    "abstract": "We design a new algorithm for batch active learning with deep neural network models. Our algorithm, Batch Active learning by Diverse Gradient Embeddings (BADGE), samples groups of points that are disparate and high magnitude when represented in a hallucinated gradient space, a strategy designed to incorporate both predictive uncertainty and sample diversity into every selected batch. Crucially, BADGE trades off between uncertainty and diversity without requiring any hand-tuned hyperparameters. While other approaches sometimes succeed for particular batch sizes or architectures, BADGE consistently performs as well or better, making it a useful option for real world active learning problems.Published as a conference paper at ICLR 2020 example, might perform poorly when using a multilayer perceptron. A diversity-based approach might work well when the batch size is very large, but poorly when the batch size is small. Further, what even constitutes a \"large\" or \"small\" batch size is largely a function of the statistical properties of the data in question. These weaknesses pose a major problem for real, practical batch active learning situations, where data are unfamiliar and potentially unstructured. There is no way to know which active learning algorithm is best to use.Moreover, in a real active learning scenario, every change of hyperparameters typically causes the algorithm to label examples not chosen under other hyperparameters, provoking substantial labeling inefficiency. That is, hyperparameter sweeps in active learning can be label expensive. As a result, active learning algorithms need to \"just work\", given fixed hyperparameters, to a greater extent than is typical for supervised learning.Based on these observations, we design an approach which creates diverse batches of examples about which the current model is uncertain. We measure uncertainty as the gradient magnitude with respect to parameters in the final (output) layer, which is computed using the most likely label according to the model. To capture diversity, we collect a batch of examples where these gradients span a diverse set of directions. More specifically, we build up the batch of query points based on these hallucinated gradients using the k-MEANS++ initialization (Arthur and Vassilvitskii, 2007), which simultaneously captures both the magnitude of a candidate gradient and its distance from previously included points in the batch. We name the resulting approach Batch Active learning by Diverse Gradient Embeddings (BADGE).We show that BADGE is robust to architecture choice, batch size, and dataset, generally performing as well as or better than the best baseline across our experiments, which vary all of the aforementioned environmental conditions. We begin by introducing our notation and setting, followed by a description of the BADGE algorithm in Section 3 and experiments in Section 4. We defer our discussion of related work to Section 5.NOTATION AND SETTINGDefine [K] := {1, 2, . . . , K}. Denote by X the instance space and by Y the label space. In this work we consider multiclass classification, so Y = [K]. Denote by D the distribution from which examples are drawn, by D X the unlabeled data distribution, and by D Y|X the conditional distribution over labels given examples. We consider the pool-based active learning setup, where the learner receives an unlabeled dataset U sampled according to D X and can request labels sampled according to D Y|X for any x ∈ U . We use E D to denote expectation under the data distribution D. Given a classifier h : X → Y, which maps examples to labels, and a labeled example (x, y), we denote the 0/1 error of h on (x, y) as 01 (h(x), y) = I(h(x) = y). The performance of a classifier h is measured by its expected 0/1 error, i.e. E D [ 01 (h(x), y)] = Pr (x,y)∼D (h(x) = y). The goal of pool-based active learning is to find a classifier with a small expected 0/1 error using as few label queries as possible. Given a set S of labeled examples (x, y), where each x ∈ S is picked from U , followed by a label query, we use E S as the sample averages over S.In this paper, we consider classifiers h parameterized by underlying neural networks f of fixed architecture, with the weights in the network denoted by θ. We abbreviate the classifier with parameters θ as h θ since the architectures are fixed in any given context, and our classifiers take the form h θ (x) = argmax y∈ [K]  f (x; θ) y , where f (x; θ) ∈ R K is a probability vector of scores assigned to candidate labels, given the example x and parameters θ. We optimize the parameters by minimizing the cross-entropy loss E S [ CE (f (x; θ), y)] over the labeled examples, where CE (p, y) = K i=1 I(y = i) ln 1 /pi = ln 1 /py.",
    "prev": "CoreSet (Sener & Savarese, 2018) and more approaches (Guo, 2010;Joshi et al., 2010;Elhamifar et al., 2013;Yang et al., 2015;Wang & Ye, 2015).",
    "curr": "Another recent approach is to look at parameters of the neural network and to diversify points such as BADGE (Ash et al., 2020) with gradients and BAIT (Ash et al., 2021) \n\n\nBAYESIAN NEURAL NETWORK MODEL\n\nWe adopt the Bayesian neural network framework introduced in Gal & Ghahramani (2016).",
    "next": "The core idea in the Bayesian neural network is leveraging the MC dropout feature to generate a distribution of the predictive probability as an output at inference time."
  },
  {
    "index": 2503,
    "source_corpus_id": 252715459,
    "ref_id": "b16",
    "citation_corpus_id": 30535508,
    "start": 24986,
    "end": 25008,
    "title": "LEARNING SPARSE NEURAL NETWORKS THROUGH L 0 REGULARIZATION",
    "abstract": "We propose a practical method for L 0 norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of L 0 regularization. However, since the L 0 norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected L 0 norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the hard concrete distribution for the gates, which is obtained by \"stretching\" a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.",
    "prev": "n Algorithm 3, and transform the new slots by MLP transformations.The new slot is used in the decoder to reconstruct the input pair of frames by optimizing the sum of the reconstruction error for the two frames.",
    "curr": "Partitioning.The problem of partitioning input data frequently appears in computing tasks.One commonly used approach casts the problem as one of selecting subsets.An input x (a vector or sequence) is treated as a set where each element of the set is assigned a weight.Next, one can select k elements with the largest weight using one of the available relaxation of the Top-k operator (Plötz and Roth, 2018;Grover et al., 2019).If a stochastic sample is desired then the weights might be treated as unnormalized probabilities and the Top-k relaxation combined with a ranking distribution such as Gumbel Softmax to obtain differentiable samples without replacement (Xie and Ermon, 2019).Or one might sample elements independently and employ a regularization method such L 0 regularization (Louizos et al., 2018;De Cao et al., 2020) to control the number selected elements.For problems involving partitions of images, the selection of a pixel makes it likely that neighboring pixels will also be chosen.Current methods often ignore this relation and treat the problem as one of independent choice and rely on learning.",
    "next": "Graph Cuts in Vision.Both directed and undirected version of graph cuts have been considered previously for computer vision applications including image segmentation (Boykov and Jolly, 2001;Wu and Leahy, 1993;Boykov and Funka-Lea, 2006), image restoration (Greig et al., 1989), multi-view reconstruction (Kolmogorov and Zabih, 2001).For image segmentation with global cuts, normalized cut Shi and Malik (2000) has been a popular method to prevent the method from cutting small isolated sets.The work Boykov and Jolly (2001) uses s-t cuts which is the approach taken in this work."
  },
  {
    "index": 2506,
    "source_corpus_id": 248228159,
    "ref_id": "b32",
    "citation_corpus_id": 56657912,
    "start": 2413,
    "end": 2442,
    "title": "BENCHMARKING NEURAL NETWORK ROBUSTNESS TO COMMON CORRUPTIONS AND PERTURBATIONS",
    "abstract": "In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, IMAGENET-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called IMAGENET-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.",
    "prev": "This is particularly relevant for the deployment of ML models to the real world, where we need systems that generalize beyond the i.i.d.",
    "curr": "(independent and identically distributed) data setting Djolonga et al., 2020;Koh et al., 2021;Barbu et al., 2019;Azulay & Weiss, 2019;Roy et al., 2018;Gulrajani & Lopez-Paz, 2020;Hendrycks & Dietterich, 2019;Michaelis et al., 2019;Funk et al., 2021).",
    "next": "One instance of such models are agents that learn by interacting with a training environment and we would like them to generalize to other environments with different statistics (Zhang et al., 2018;Pfister et al., 2019;Cobbe et al., 2019;Ahmed et al., 2021;."
  },
  {
    "index": 2508,
    "source_corpus_id": 264436558,
    "ref_id": "b38",
    "citation_corpus_id": 3144218,
    "start": 9332,
    "end": 9354,
    "title": "SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS",
    "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
    "prev": "the sum or max operation.Additionally, φ ∈ R d × R h → R m represents a readout function.We note that the update equation is local and relies solely on the neighborhood of the node.Both ψ and φ can be Multi-Layer Perceptrons (MLPs).In our manuscript all MLPs will use the ReLU(t) def.",
    "curr": "= max{0, t} activation function, where t ∈ R. Several special cases have resulted in the development of a wide range of GNN layers: the most well-known being Graph Convolutional Networks (GCNs) (Kipf & Welling, 2017) and Graph Attention Networks (GATs) (Veličković et al., 2018).",
    "next": "Quasi-Metric Spaces.While Riemannian manifolds have been employed for formalizing non-Euclidean distances between points, their additional structural properties, such as smoothness and infinitesimal angles, impose substantial limitations, rendering the demonstration of Riemannian manifolds with closed-form distance functions challenging.Quasimetric spaces, isolate the relevant properties of Riemannian distance functions without requiring any of their additional structure for graph embedding.A quasi-metric space is a set X with a distance function d :\nX × X → [0, ∞) satisfying for every x, y, z ∈ X: i) d(x, y) = 0 if and only if x = y, ii) d(x, y) = d(y, x), iii) d(x, y) ≤ C d(x, z) + d(z, y) , for some constant C ≥ 1."
  },
  {
    "index": 2509,
    "source_corpus_id": 9226593,
    "ref_id": "b34",
    "citation_corpus_id": 14124313,
    "start": 5147,
    "end": 5175,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "We explore whether the usefulness of such features extends to visual servoing.",
    "curr": "To answer this question, we propose a visual servoing method that uses pre-trained features, in our case obtained from the VGG network (Simonyan & Zisserman, 2015) trained for ImageNet classification.",
    "next": "Besides the visual features, our method uses an estimate of the feature dynamics in visual space by means of a bilinear model."
  },
  {
    "index": 2510,
    "source_corpus_id": 236087592,
    "ref_id": "b30",
    "citation_corpus_id": 204824219,
    "start": 14674,
    "end": 14691,
    "title": "Published as a conference paper at ICLR 2020 WEAKLY SUPERVISED DISENTANGLEMENT WITH GUARANTEES",
    "abstract": "Learning disentangled representations that correspond to factors of variation in real-world data is critical to interpretable and human-controllable machine learning. Recently, concerns about the viability of learning disentangled representations in a purely unsupervised manner has spurred a shift toward the incorporation of weak supervision. However, there is currently no formalism that identifies when and how weak supervision will guarantee disentanglement. To address this issue, we provide a theoretical framework to assist in analyzing the disentanglement guarantees (or lack thereof) conferred by weak supervision when coupled with learning algorithms based on distribution matching. We empirically verify the guarantees and limitations of several weak supervision methods (restricted labeling, match-pairing, and rank-pairing), demonstrating the predictive power and usefulness of our theoretical framework.1. We formalize weakly-supervised learning as distribution matching in an extended space. * Work done during an internship at Google Brain. arXiv:1910.09772v2 [cs.LG] 10 Apr 2020Published as a conference paper at ICLR 2020 2. We propose a set of definitions for disentanglement that can handle correlated factors and are inspired by many existing definitions in the literature(Higgins et al., 2018;Suter et al.,  2018; Ridgeway & Mozer, 2018).3. Using these definitions, we provide a conceptually useful and theoretically rigorous calculus of disentanglement.4. We apply our theoretical framework of disentanglement to analyze three notable classes of weak supervision methods (restricted labeling, match pairing, and rank pairing). We show that although certain weak supervision methods (e.g., style-labeling in style-content disentanglement) do not guarantee disentanglement, our calculus can determine whether disentanglement is guaranteed when multiple sources of weak supervision are combined.5. Finally, we perform extensive experiments to systematically and empirically verify our predicted guarantees. 1FROM UNSUPERVISED TO WEAKLY SUPERVISED DISTRIBUTION MATCHINGOur goal in disentangled representation learning is to identify a latent-variable generative model whose latent variables correspond to ground truth factors of variation in the data. To identify the role that weak supervision plays in providing guarantees on disentanglement, we first formalize the model families we are considering, the forms of weak supervision, and finally the metrics we will use to evaluate and prove components of disentanglement.We consider data-generating processes where S ∈ R n are the factors of variation, with distribution p * (s), and X ∈ R m is the observed data point which is a deterministic function of S, i.e., X = g * (S). Many existing algorithms in unsupervised learning of disentangled representations aim to learn a latent-variable model with prior p(z) and generator g, where g(Z) d = g * (S). However, simply matching the marginal distribution over data is not enough: the learned latent variables Z and the true generating factors S could still be entangled with each other(Locatello et al., 2019).To address the failures of unsupervised learning of disentangled representations, we leverage weak supervision, where information about the data-generating process is conveyed through additional observations. By performing distribution matching on an augmented space (instead of just on the observation X), we can provide guarantees on learned representations.",
    "prev": "It is well known that ICA/disentanglement is theoretically non-identifiable without additional assumptions or supervision (Hyvärinen & Pajunen, 1999;Locatello et al., 2018).",
    "curr": "Recent work has thus focused on weakly supervised approaches which can provably identify the true independent latent factors (Hyvärinen & Morioka, 2016;Hyvarinen & Morioka, 2017;Shu et al., 2019;Locatello et al., 2020a;Klindt et al., 2020;Khemakhem et al., 2020;Roeder et al., 2020).",
    "next": "The general idea is to leverage additional information in the form of paired observations (x i ,x i ) wherex i is typically an auxiliary variable (e.g., an environment indicator or time-stamp) or a second view, i.e.,x i = g(ỹ i ) withỹ i ∼ p(ỹ|y i ), where y i are the FoVs of x i and p(ỹ|y) depends on the method."
  },
  {
    "index": 2511,
    "source_corpus_id": 211818298,
    "ref_id": "b47",
    "citation_corpus_id": 8217340,
    "start": 2777,
    "end": 2796,
    "title": "OBJECT DETECTORS EMERGE IN DEEP SCENE CNNS",
    "abstract": "With the success of new computational architectures for visual processing, such as convolutional neural networks (CNN) and access to image databases with millions of labeled examples (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly. One important factor for continued progress is to understand the representations that are learned by the inner layers of these deep architectures.Here we show that object detectors emerge from training CNNs to perform scene classification. As scenes are composed of objects, the CNN for scene classification automatically discovers meaningful objects detectors, representative of the learned scene categories. With object detectors emerging as a result of learning to recognize scenes, our work demonstrates that the same network can perform both scene recognition and object localization in a single forward-pass, without ever having explicitly learned the notion of objects.",
    "prev": "Examining the properties of single neurons in order to gain understanding about the networks in which they're embedded is a common practice in neuroscience (Sherrington, 1906;Adrian, 1926;Granit, 1955;Barlow, 1972;Kandel et al., 2000), and has also been widely applied in deep learning (Erhan et al., 2009;Zeiler & Fergus, 2014;Karpathy et al., 2016;Amjad et al., 2018;Lillian et al., 2018;Dhamdhere et al., 2019).",
    "curr": "The selectivity of individual units' activations across inputs (often referred to as \"tuning\" in neuroscience) has been of particular interest to researchers of both artificial (Zhou et al., 2015;Olah et al., 2017;Meyes et al., 2019;Na et al., 2019;Zhou et al., 2019) and biological (Hubel & Wiesel, 1962;De Valois et al., 1982;Britten et al., 1992;Moody et al., 1998;Kandel et al., 2000) neural networks.",
    "next": "This focus on individual neurons makes intuitive sense as the tractable, semantic nature of selectivity is extremely alluring; some measure of selectivity in individual units is often provided as an explanation of \"what\" a network is \"doing.\""
  },
  {
    "index": 2512,
    "source_corpus_id": 259164552,
    "ref_id": "b66",
    "citation_corpus_id": 215717103,
    "start": 18081,
    "end": 18085,
    "title": "Collecting Highly Parallel Data for Paraphrase Evaluation",
    "abstract": "A lack of standard datasets and evaluation metrics has prevented the field of paraphrasing from making the kind of rapid progress enjoyed by the machine translation community over the last 15 years. We address both problems by presenting a novel data collection framework that produces highly parallel text data relatively inexpensively and on a large scale. The highly parallel nature of this data allows us to use simple n-gram comparisons to measure both the semantic adequacy and lexical dissimilarity of paraphrase candidates. In addition to being simple and efficient to compute, experiments show that these metrics correlate highly with human judgments.",
    "prev": "Video Captioning.",
    "curr": "We evaluate video captioning on five benchmarks, including MSRVTT [60], MSVD [67], VATEX [68], YouCook2 [69], and TVC [70].",
    "next": "The GM objective is used, and captions are autoregressively generated during inference."
  },
  {
    "index": 2514,
    "source_corpus_id": 52909749,
    "ref_id": "b44",
    "citation_corpus_id": 1918428,
    "start": 1827,
    "end": 1846,
    "title": "A Neural Attention Model for Sentence Summarization",
    "abstract": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.",
    "prev": "Such encoder-decoder architectures are adopted in both machine translation (Bahdanau et al., 2015;Hassan et al., 2018) and speech recognition systems (Chan et al., 2016;Bahdanau et al., 2016a;Chiu et al., 2017) to achieve impressive performance beyond traditional multi-stage pipelines (Koehn et al., 2007;Povey et al., 2011).",
    "curr": "Improving the building blocks of seq2seq models can fundamentally advance machine translation and speech recognition, and positively impact other domains such as image captioning (Xu et al., 2015), parsing , summarization (Rush et al., 2015), and program synthesis (Zhong et al., 2017).",
    "next": "To improve the key components of seq2seq models, one can either design better architectures, or develop better learning algorithms."
  },
  {
    "index": 2515,
    "source_corpus_id": 252544861,
    "ref_id": "b10",
    "citation_corpus_id": 219708387,
    "start": 47623,
    "end": 47642,
    "title": "Published as a conference paper at ICLR 2022 AUGMENTED SLICED WASSERSTEIN DISTANCES",
    "abstract": "While theoretically appealing, the application of the Wasserstein distance to large-scale machine learning problems has been hampered by its prohibitive computational cost. The sliced Wasserstein distance and its variants improve the computational efficiency through the random projection, yet they suffer from low accuracy if the number of projections is not sufficiently large, because the majority of projections result in trivially small values. In this work, we propose a new family of distance metrics, called augmented sliced Wasserstein distances (ASWDs), constructed by first mapping samples to higher-dimensional hypersurfaces parameterized by neural networks. It is derived from a key observation that (random) linear projections of samples residing on these hypersurfaces would translate to much more flexible nonlinear projections in the original sample space, so they can capture complex structures of the data distribution. We show that the hypersurfaces can be optimized by gradient ascent efficiently. We provide the condition under which the ASWD is a valid metric and show that this can be obtained by an injective neural network architecture. Numerical results demonstrate that the ASWD significantly outperforms other Wasserstein variants for both synthetic and real-world problems.An alternative approach is to approximate the Wasserstein distance through slicing, i.e. linearly projecting, the distributions to be compared. The sliced Wasserstein distance (SWD) (Bonneel et al.,  2015)  is defined as the expected value of Wasserstein distances between one-dimensional random projections of high-dimensional distributions. The SWD shares similar theoretical properties with the Wasserstein distance (Bonnotte, 2013) and is computationally efficient since the Wasserstein distance in one-dimensional space has a closed-form solution based on sorting. (Deshpande et al., 2019) extends the sliced Wasserstein distance to the max-sliced Wasserstein distance (Max-SWD), by finding a single projection direction with the maximal distance between projected samples. The subspace robust Wasserstein distance extends the idea of slicing to projecting distributions on linear subspaces (Paty and 1 arXiv:2006.08812v7 [cs.LG] Figure 1: (a) and (b) are visualizations of projections for the ASWD and the SWD between two 2-dimensional Gaussians. (c) and (d) are distance histograms for the ASWD and the SWD between two 100-dimensional Gaussians.Figure 1(a)shows that the injective neural network embedded in the ASWD learns data patterns (in the X-Y plane) and produces well-separate projected values (Z-axis) between distributions in a random projection direction. The high projection efficiency of the ASWD is evident inFigure 1(c), as almost all random projection directions in a 100-dimensional space lead to significant distances between 1-dimensional projections. In contrast, random linear mappings in the SWD often produce closer 1-d projections (Z-axis)(Figure 1(b)); as a result, a large percentage of random projection directions in the 100-d space result in trivially small distances(Figure 1(d)), leading to a low projection efficiency in high-dimensional spaces. Cuturi, 2019). However, the linear nature of these projections usually leads to low projection efficiency of the resulted metrics in high-dimensional spaces (Deshpande et al., 2019; Kolouri et al., 2019a).Different variants of the SWD have been proposed to improve the projection efficiency of the SWD, either by introducing nonlinear projections or by optimizing the distribution of random projections. Specifically, (Kolouri et al., 2019a)  extends the connection between the sliced Wasserstein distance and the Radon transform (Radon, 1917)  to introduce generalized sliced Wasserstein distances (GSWDs) by utilizing generalized Radon transforms (GRTs), which are defined by nonlinear defining functions and lead to nonlinear projections. A variant named the GSWD-NN was proposed in (Kolouri et al.,  2019a)  to generate nonlinear projections directly with neural network outputs, but it does not fit into the theoretical framework of the GSWD and does not guarantee a valid metric. In contrast, the distributional sliced Wasserstein distance (DSWD) and its nonlinear version, the distributional generalized sliced Wasserstein distance (DGSWD), improve their projection efficiency by finding a distribution of projections that maximizes the expected distances over these projections. The GSWD and the DGSWD exhibit higher projection efficiency than the SWD in the experiment evaluation, yet they require the specification of the particular form of defining functions from a limited class of candidates. However, the selection of defining functions is usually a task-dependent problem and requires domain knowledge, and the impact on performance from different defining functions is still unclear.In this paper, we present the augmented sliced Wasserstein distance (ASWD), a distance metric constructed by first mapping samples to hypersurfaces in an augmented space, which enables flexible nonlinear slicing of data distributions for improved projection efficiency (SeeFigure 1). Our main contributions include: (i) We exploit the capacity of nonlinear projections employed in the ASWD by constructing injective mapping with arbitrary neural networks; (ii) We prove that the ASWD is a valid distance metric; (iii) We provide a mechanism in which the hypersurface where high-dimensional distributions are projected onto can be optimized and show that the optimization of hypersurfaces can help improve the projection efficiency of slice-based Wasserstein distances. Hence, the ASWD is data-adaptive, i.e. the hypersurfaces can be learned from data. This implies one does not need to manually design a function from the limited class of candidates; (iv) We demonstrate superior performance of the ASWD in numerical experiments for both synthetic and real-world datasets.",
    "prev": "We refer the reader to previous works (Nguyen et al., 2021a;b) for more details.",
    "curr": "Augmented approaches for HSW: Authors in (Chen et al., 2022) propose to augment measures to higher dimensions for better linear separation.",
    "next": "We extend the framework to our hierarchical transform approach."
  },
  {
    "index": 2517,
    "source_corpus_id": 209485573,
    "ref_id": "b9",
    "citation_corpus_id": 52967399,
    "start": 2671,
    "end": 2692,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "In\n\nINTRODUCTION\n\nMachine reading comprehension (MRC) is a fundamental task in Natural Language Processing, which requires models to understand a body of text and answer a particular question related to the context.",
    "curr": "With success of unsupervised representation learning in NLP, language pre-training based models such as GPT-2 (Radford et al., 2019), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019) have achieved nearly saturated performance on most of the popular MRC datasets (Rajpurkar et al., 2016;Lai et al., 2017;Rajpurkar et al., 2018;Wang et al., 2018).",
    "next": "It is time to challenge state-of-the-art models with more difficult reading comprehension tasks and move a step forward to more comprehensive analysis and reasoning over text (Dua et al., 2019)."
  },
  {
    "index": 2518,
    "source_corpus_id": 259203115,
    "ref_id": "b54",
    "citation_corpus_id": 52978527,
    "start": 3131,
    "end": 3149,
    "title": "RETHINKING THE VALUE OF NETWORK PRUNING",
    "abstract": "Network pruning is widely used for reducing the heavy computational cost of deep models. A typical pruning algorithm is a three-stage pipeline, i.e., training (a large model), pruning and fine-tuning. During pruning, according to a certain criterion, redundant weights are pruned and important weights are kept to best preserve the accuracy. In this work, we make several surprising observations which contradict common beliefs. For all the six state-of-the-art pruning algorithms we examined, fine-tuning a pruned model only gives comparable or even worse performance than training that model with randomly initialized weights. For pruning algorithms which assume a predefined target network architecture, one can get rid of the full pipeline and directly train the target network from scratch. Our observations are consistent for a wide variety of pruning algorithms with multiple network architectures, datasets, and tasks. Our results have several implications: 1) training a large, over-parameterized model is not necessary to obtain an efficient final model, 2) learned \"important\" weights of the large model are not necessarily useful for the small pruned model, 3) the pruned architecture itself, rather than a set of inherited \"important\" weights, is what leads to the efficiency benefit in the final model, which suggests that some pruning algorithms could be seen as performing network architecture search.In this work, we show that both of the beliefs mentioned above are not necessarily true. Based on an extensive empirical evaluation of state-of-the-art pruning algorithms on multiple datasets with multiple network architectures, we make two surprising observations. First, for pruning algorithms with predefined target network architectures(Figure 2), directly training the small target model from random initialization can achieve the same, if not better, performance, as the model obtained from the three-stage pipeline. In this case, starting with a large model is not necessary and one could instead directly train the target model from scratch. Second, for pruning algorithms without a predefined target network, training the pruned model from scratch can also achieve comparable or even better performance than fine-tuning. This observation shows that for these pruning algorithms, what matters is the obtained architecture, instead of the preserved weights, despite training the large model is required to find that target architecture. The contradiction between our results and those reported in the literature might be explained by less carefully chosen hyper-parameters, data augmentation schemes and unfair computation budget for evaluating this baseline approach.Predefined: prune x% channels in each layer Automatic: prune a%, b%, c%, d% channels in each layer A 4-layer model",
    "prev": "Many of the notable advancements to date have centered on model quantization, a process where parameters are quantized into lower bit-level representations.The fast pace of LLM quantization research (Dettmers et al., 2022;Frantar et al., 2023a;Xiao et al., 2023;Ahmadian et al., 2023) has led to substantial resource savings for these models (Sheng et al., 2023;Lin et al., 2023).",
    "curr": "Network pruning (LeCun et al., 1989;Hassibi et al., 1993;Han et al., 2015), on the other hand, shrinks network sizes by removing specific weights from the model -essentially setting them to zero.Along with quantization, it is often considered another popular approach for compressing neural networks.However, it has received relatively little focus in compressing LLMs.This seems to contradict the trend of model compression in the pre-LLM era, where both approaches have received large amounts of research effort.A quick review of existing pruning methods reveals a possible reason: they typically require retraining (Liu et al., 2019;Blalock et al., 2020), training from random initializations (Zhu & Gupta, 2017;Louizos et al., 2018;Gale et al., 2019) or even an extensive iterative process (Frankle & Michael, 2019;Renda et al., 2020).The sheer amount of computational resources required by LLMs limits these methods.A recent LLM pruning approach, SparseGPT (Frantar & Alistarh, 2023), does not require traditional retraining, but still demands a computationally intensive weight update process.",
    "next": "The argument concerning the need for retraining and weight update does not fully capture the challenges of pruning LLMs.One might reasonably expect to obtain a fairly high-performing initialization point for retraining using existing popular pruning methods.However, a recent study (Frantar & -3 0 0 2\n\n\nWeights\n\nWeight Importance\n\n\nWeights and activations\n\nWeight Importance Pruned Weights Pruned Weights grouped per layer grouped per output\n\nFigure 1: Illustration of our proposed method Wanda (Pruning by Weights and activations), c"
  },
  {
    "index": 2520,
    "source_corpus_id": 213513188,
    "ref_id": "b19",
    "citation_corpus_id": 3568073,
    "start": 1743,
    "end": 1764,
    "title": "PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION",
    "abstract": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at 1024 2 . We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CELEBA dataset.",
    "prev": "arXiv:1907.05681v3 [cs.LG] 3 Jan 2020Published as a conference paper at ICLR 2020 Our contributions are as follows:• We propose Adversarial Lipschitz Regularization (ALR) and apply it to penalize the violation of the Lipschitz constraint directly, resulting in Adversarial Lipschitz Penalty (ALP).",
    "curr": "INTRODUCTION\n\nIn recent years, Generative adversarial networks (GANs) (Goodfellow et al., 2014) have been becoming the state-of-the-art in several generative modeling tasks, ranging from image generation (Karras et al., 2018) to imitation learning (Ho and Ermon, 2016).",
    "next": "They are based on an idea of a two-player game, in which a discriminator tries to distinguish between real and generated data samples, while a generator tries to fool the discriminator, learning to produce realistic samples on the long run."
  },
  {
    "index": 2521,
    "source_corpus_id": 247451183,
    "ref_id": "b14",
    "citation_corpus_id": 13046179,
    "start": 3436,
    "end": 3461,
    "title": "A BASELINE FOR DETECTING MISCLASSIFIED AND OUT-OF-DISTRIBUTION EXAMPLES IN NEURAL NETWORKS",
    "abstract": "We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.Published as a conference paper at ICLR 2017 one method which outperforms the baseline on some (but not all) tasks. This new method evaluates the quality of a neural network's input reconstruction to determine if an example is abnormal.",
    "prev": "The baseline approach for OOD detection relies on the predictive uncertainty of DNNs.",
    "curr": "Hendrycks & Gimpel (2017) demonstrated that OOD samples, in general, induce DNN classifiers to output less confident softmax scores, while existing state-of-the-art methods on classification problems still output high accuracy even under dataset shift.",
    "next": "For instance, Ovadia et al."
  },
  {
    "index": 2522,
    "source_corpus_id": 263622464,
    "ref_id": "b29",
    "citation_corpus_id": 232290577,
    "start": 18145,
    "end": 18162,
    "title": "MARS: MARKOV MOLECULAR SAMPLING FOR MULTI-OBJECTIVE DRUG DISCOVERY",
    "abstract": "Searching for novel molecules with desired chemical properties is crucial in drug discovery. Existing work focuses on developing neural models to generate either molecular sequences or chemical graphs. However, it remains a big challenge to find novel and diverse compounds satisfying several properties. In this paper, we propose MARS, a method for multi-objective drug molecule discovery. MARS is based on the idea of generating the chemical candidates by iteratively editing fragments of molecular graphs. To search for high-quality candidates, it employs Markov chain Monte Carlo sampling (MCMC) on molecules with an annealing scheme and an adaptive proposal. To further improve sample efficiency, MARS uses a graph neural network (GNN) to represent and select candidate edits, where the GNN is trained on-the-fly with samples from MCMC. Experiments show that MARS achieves state-of-the-art performance in various multi-objective settings where molecular bio-activity, drug-likeness, and synthesizability are considered. Remarkably, in the most challenging setting where all four objectives are simultaneously optimized, our approach outperforms previous methods significantly in comprehensive evaluations. The code is available at",
    "prev": "(2020).",
    "curr": "RNA-Binding\n\n\nBASELINES\n\nWe consider prior GFlowNet (GFN) methods and reward-maximization methods as our baselines.Prior GFN methods include detailed balance (DB, Bengio et al., 2023), maximum entropy GFN (MaxEnt, Malkin et al., 2022), trajectory balance (TB, Malkin et al., 2022), sub-trajectory balance (SubTB, Madan et al., 2023), and substructure-guided trajectory balance (GTB, Shen et al., 2023).For reward-maximization methods, we consider Markov Molecular Sampling (MARS, Xie et al., 2020), which is a sampling-based method known to work well in the molecule domain, and RL-based methods which include advantage actor-critic (A2C) with entropy regularization (Mnih et al., 2016), Soft Q-Learning (SQL, Haarnoja et al., 2018), and proximal policy optimization (PPO, Schulman et al., 2017).",
    "next": "IMPLEMENTATIONS AND HYPERPARAMETERS\n\nFor GFN implementations, we strictly follow implementations from Shen et al."
  },
  {
    "index": 2524,
    "source_corpus_id": 260704723,
    "ref_id": "b29",
    "citation_corpus_id": 201646309,
    "start": 20966,
    "end": 20991,
    "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    "abstract": "BERT(Devlin et al., 2018)and RoBERTa (Liu  et al., 2019)  has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods. 1",
    "prev": "While well-formated options are provided, we empirically observe that many MLLMs struggle to strictly follow instructions to output the option indexes but generate free-form text.",
    "curr": "Thus, when models do not exactly output the required options, we match their outputs to one of the given options based on the TF-IDF distance, which we find is more robust than model-based methods (OpenAI,  2023a; Reimers & Gurevych, 2019).",
    "next": "Since we explore a large number of tasks, we take a maximum of 500 instances per task for evaluation efficiency and exclude several datasets that are difficult to obtain and are subject to strict copyright restrictions (referred to as DEMON-Core)."
  },
  {
    "index": 2527,
    "source_corpus_id": 251765079,
    "ref_id": "b28",
    "citation_corpus_id": 204960716,
    "start": 11733,
    "end": 11753,
    "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    "abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance. 1",
    "prev": "RetMol is agnostic to the choice of the underlying encoder and decoder architectures, enabling it to work with a variety of generative models and molecule representations.",
    "curr": "In this work, we consider the SMILES string (Weininger, 1988) representation of molecules and the ChemFormer model (Irwin et al., 2022), which a variant of BART (Lewis et al., 2020) trained on the billion-scale ZINC dataset (Irwin and Shoichet, 2004) and achieves state-of-the-art generation performance.",
    "next": "Retrieval database."
  },
  {
    "index": 2528,
    "source_corpus_id": 257219652,
    "ref_id": "b2",
    "citation_corpus_id": 4630420,
    "start": 26531,
    "end": 26561,
    "title": "DEEP GAUSSIAN EMBEDDING OF GRAPHS: UNSUPERVISED INDUCTIVE LEARNING VIA RANKING",
    "abstract": "Methods that learn representations of nodes in a graph play a critical role in network analysis since they enable many downstream learning tasks. We propose Graph2Gauss -an approach that can efficiently learn versatile node embeddings on large scale (attributed) graphs that show strong performance on tasks such as link prediction and node classification. Unlike most approaches that represent nodes as point vectors in a low-dimensional continuous space, we embed each node as a Gaussian distribution, allowing us to capture uncertainty about the representation. Furthermore, we propose an unsupervised method that handles inductive learning scenarios and is applicable to different types of graphs: plain/attributed, directed/undirected. By leveraging both the network structure and the associated node attributes, we are able to generalize to unseen nodes without additional training. To learn the embeddings we adopt a personalized ranking formulation w.r.t. the node distances that exploits the natural ordering of the nodes imposed by the network structure. Experiments on real world networks demonstrate the high performance of our approach, outperforming state-of-the-art network embedding methods on several different tasks. Additionally, we demonstrate the benefits of modeling uncertainty -by analyzing it we can estimate neighborhood diversity and detect the intrinsic latent dimensionality of a graph.",
    "prev": "We evaluate GNNDELETE on several widely-used graphs at various scales.",
    "curr": "We use 5 homogeneous graphs: Cora (Bojchevski & Günnemann, 2018), PubMed (Bojchevski & Günnemann, 2018), DBLP (Bojchevski & Günnemann, 2018), CS (Bojchevski & Günnemann, 2018), OGB-Collab (Hu et al., 2020), and 2 heterogeneous graphs: OGB-BioKG (Hu et al., 2020), and WordNet18RR (Dettmers et al., 2018).",
    "next": "Table 4 includes details on graph datasets."
  },
  {
    "index": 2531,
    "source_corpus_id": 8394195,
    "ref_id": "b11",
    "citation_corpus_id": 6628106,
    "start": 18096,
    "end": 18115,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "The estimated probabilities were then used with a publicly available BSD licensed implementation of a range coder 2 .",
    "curr": "INCREMENTAL TRAINING\n\nAll models were trained using Adam (Kingma & Ba, 2015) applied to batches of 32 images 128×128 pixels in size.",
    "next": "We found it beneficial to optimize coefficients in an incremental manner ( Figure 3B)."
  },
  {
    "index": 2532,
    "source_corpus_id": 239016943,
    "ref_id": "b8",
    "citation_corpus_id": 213152193,
    "start": 25070,
    "end": 25090,
    "title": "ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS",
    "abstract": "Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK]  and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.",
    "prev": "BERT (Devlin et al., 2018) proposed the Masked Language Modeling (MLM) pretraining paradigm to train a deep bidirectional Transformer model; RoBERTa  removed the Next Sentence Prediction (NSP) task in BERT; XLNet  generalized BERT pretraining to the autoregressive manner; Span-level BERTs Song et al., 2019;Joshi et al., 2020) introduced span-level masks rather than just relying on token-level masks.",
    "curr": "ELECTRA (Clark et al., 2020) proposed to detect token replacement as opposed to token generation, improving both the efficiency and effectiveness of pretraining.",
    "next": "extends BERT to accommodate glyph information."
  },
  {
    "index": 2533,
    "source_corpus_id": 12713052,
    "ref_id": "b2",
    "citation_corpus_id": 11212020,
    "start": 3186,
    "end": 3208,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "The cell that our model found achieves a test set perplexity of 62.4 on the Penn Treebank dataset, which is 3.6 perplexity better than the previous state-of-the-art.",
    "curr": "INTRODUCTION\n\nThe last few years have seen much success of deep neural networks in many challenging applications, such as speech recognition , image recognition (LeCun et al., 1998;Krizhevsky et al., 2012) and machine translation (Sutskever et al., 2014;Bahdanau et al., 2015;.",
    "next": "Along with this success is a paradigm shift from feature designing to architecture designing, i.e., from SIFT (Lowe, 1999), and HOG (Dalal & Triggs, 2005), to AlexNet (Krizhevsky et al., 2012), VGGNet (Simonyan & Zisserman, 2014), GoogleNet , and ResNet (He et al., 2016a)."
  },
  {
    "index": 2534,
    "source_corpus_id": 263609190,
    "ref_id": "b6",
    "citation_corpus_id": 67855286,
    "start": 2927,
    "end": 2929,
    "title": "ANTISYMMETRICRNN: A DYNAMICAL SYSTEM VIEW ON RECURRENT NEURAL NETWORKS",
    "abstract": "Recurrent neural networks have gained widespread use in modeling sequential data. Learning long-term dependencies using these models remains difficult though, due to exploding or vanishing gradients. In this paper, we draw connections between recurrent networks and ordinary differential equations. A special form of recurrent networks called the AntisymmetricRNN is proposed under this theoretical framework, which is able to capture long-term dependencies thanks to the stability property of its underlying differential equation. Existing approaches to improving RNN trainability often incur significant computation overhead. In comparison, AntisymmetricRNN achieves the same goal by design. We showcase the advantage of this new architecture through extensive simulations and experiments. AntisymmetricRNN exhibits much more predictable dynamics. It outperforms regular LSTM models on tasks requiring long-term memory and matches the performance on tasks where short-term dependencies dominate despite being much simpler. advocate going beyond initialization and forcing the weight matrices to be orthogonal throughout the entire learning process. However, some of these approaches come with significant computational overhead and reportedly hinder representation power of these models(Vorontsov et al., 2017). Moreover, orthogonal weight matrices alone do not prevent exploding and vanishing gradients, due to the nonlinear nature of deep neural networks as shown in(Pennington et al., 2017).Here we offer a new perspective on the trainability of RNNs from the dynamical system viewpoint. While exploding gradient is a manifestation of the instability of the underlying dynamical system, vanishing gradient results from a lossy system, properties that have been widely studied in the dynamical system literature(Haber & Ruthotto, 2017;Laurent & von Brecht, 2017). The main contributions of the work are:• We draw connections between RNNs and the ordinary differential equation theory and design new recurrent architectures by discretizing ODEs.",
    "prev": "ramework, while the S4D/S5 initialization only achieves weak convergences.As a result, our new models show resilience to Fourier-mode noise-perturbed inputs, a crucial property not achieved by the S4D/S5 models.In addition to improved robustness, our S5-PTD model averages 87.6% accuracy on the Long-Range Arena benchmark, demonstrating that the PTD methodology helps to improve the accuracy of deep learning models.",
    "curr": "Introduction\n\nSequential data are pervasive across a wide range of fields, including natural language processing, speech recognition, robotics and autonomous systems, as well as scientific machine learning and financial time-series analysis, among others.Given that many of these applications produce exceedingly long sequences, sequential models need to capture long-range temporal dependencies in order to yield accurate predictions.To this end, many specialized deep learning methods have been developed to deal with long sequences, including recurrent neural networks (RNNs) [2,7,13,30,14,28], convolutional neural networks (CNNs) [4,29], continuous-time models (CTMs) [18,37], and transformers [21,8,23,39,26].",
    "next": "Over the past few years, the new class of state-space models (SSMs) gained vast popularity for sequential modeling due to their outstanding performance on the Long-Range Arena (LRA) dataset [33].An SSM is built upon a continuous-time linear time-invariant (LTI) dynamical system Σ = (A, B, C, D), which is a system of linear ODEs given by x ′ (t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t), (1) where A ∈ C n×n , B ∈ C n×m , C ∈ C p×n , D ∈ C p×m are the state, input, output and feedthrough matrices; and u(t) ∈ C m , x(t) ∈ C n , y(t) ∈ C p are the inputs, states, and outputs of the system, respectively.The system can be discretized at time steps j∆t, where ∆t > 0 and j = 1, ."
  },
  {
    "index": 2535,
    "source_corpus_id": 232257635,
    "ref_id": "b8",
    "citation_corpus_id": 53388625,
    "start": 44187,
    "end": 44211,
    "title": "THE LOTTERY TICKET HYPOTHESIS: FINDING SPARSE, TRAINABLE NEURAL NETWORKS",
    "abstract": "Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard technique for pruning weights naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the lottery ticket hypothesis: dense, randomly-initialized feed-forward networks contain subnetworks (winning tickets) that-when trained in isolation-arrive at comparable test accuracy in a comparable number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Furthermore, the winning tickets we find above that size learn faster than the original network and exhibit higher test accuracy. arXiv:1803.03635v4 [cs.LG] 27 Nov 2018 1. Randomly initialize a neural network f (x; θ 0 ) (where θ 0 ∼ D θ ). 2. Train the network for j iterations, reaching parameters θ j . 3. Prune s% of the parameters, creating a mask m where P m = (100 − s)%. 4. To extract the winning ticket, reset the remaining parameters to their values in θ 0 , creating the untrained network f (x; m θ 0 ).If dense networks contain winning tickets and pruning reveals them, then the network f (x; m θ 0 )when trained for j iterations-will reach similar accuracy to f (x; θ j ) at least as quickly, and m will be too sparse for a randomly-reinitialized or randomly-sparsified network to do the same.Results. We identify winning tickets in a fully-connected architecture for MNIST and convolutional architectures for CIFAR10 across several optimization strategies (SGD, momentum, and Adam) with techniques like dropout, weight decay, and batchnorm. In deeper networks, our pruning-based strategy for finding winning tickets is sensitive to the learning rate: it requires warmup to find winning tickets at higher learning rates. The winning tickets we find are 10-20% (or less) of the size of the Anonymous. Gradient descent provably optimizes over-parameterized neural networks. Dally. Dsd: Regularizing deep neural networks with dense-sparse-dense training flow. Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size. arXiv preprint arXiv:1602.07360, 2016.",
    "prev": "Thus far, we have have discussed methods for pruning pretrained DNNs.",
    "curr": "Recently, (Frankle & Carbin, 2019) proposed the Lottery Ticket Hypothesis and showed that randomly-initialized neural networks contain sparse subnetworks that can be effectively trained from scratch when reset to their initialization.",
    "next": "Further, (Liu et al., 2018b) showed that the training an over-parameterized model is often not necessary to obtain an efficient final model and network architecture itself is more important than the remaining weights after pruning pretrained networks."
  },
  {
    "index": 2536,
    "source_corpus_id": 3538865,
    "ref_id": "b5",
    "citation_corpus_id": 5590763,
    "start": 1252,
    "end": 1269,
    "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
    "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
    "prev": "* Equal contribution.",
    "curr": "INTRODUCTION\n\nSequence-to-sequence models (Sutskever et al., 2014;Cho et al., 2014) with a soft attention mechanism  have been successfully applied to a plethora of sequence transduction problems (Luong et al., 2015;Xu et al., 2015;Chorowski et al., 2015;Wang et al., 2017;See et al., 2017).",
    "next": "In their most familiar form, these models process an input sequence with an encoder recurrent neural network (RNN) to produce a sequence of hidden states, referred to as a memory."
  },
  {
    "index": 2538,
    "source_corpus_id": 227746078,
    "ref_id": "b1",
    "citation_corpus_id": 3618568,
    "start": 3533,
    "end": 3552,
    "title": "Reading Wikipedia to Answer Open-Domain Questions",
    "abstract": "This paper proposes to tackle opendomain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.",
    "prev": "In this paper, we propose a procedure to learn retriever systems without strong supervision in the form of pairs of queries and documents.",
    "curr": "Following previous work (Chen et al., 2017), our approach uses two models: the first one retrieves documents from a large source of knowledge (the retriever), the second one processes the support documents to solve the task (the reader).",
    "next": "Our method is inspired by knowledge distillation (Hinton et al., 2015), and uses the reader model to obtain synthetic labels to train the retriever model."
  },
  {
    "index": 2540,
    "source_corpus_id": 211146411,
    "ref_id": "b25",
    "citation_corpus_id": 196210081,
    "start": 2253,
    "end": 2270,
    "title": "Incremental Transformer with Deliberation Decoder for Document Grounded Conversations",
    "abstract": "Document Grounded Conversations is a task to generate dialogue responses when chatting about the content of a given document. Obviously, document knowledge plays a critical role in Document Grounded Conversations, while existing dialogue models do not exploit this kind of knowledge effectively enough. In this paper, we propose a novel Transformerbased architecture for multi-turn document grounded conversations. In particular, we devise an Incremental Transformer to encode multi-turn utterances along with knowledge in related documents. Motivated by the human cognitive process, we design a two-pass decoder (Deliberation Decoder) to improve context coherence and knowledge correctness. Our empirical study on a real-world Document Grounded Dataset proves that responses generated by our model significantly outperform competitive baselines on both context coherence and knowledge relevance. * * Fandong Meng is the corresponding author of the paper. This work was done when Zekang Li was interning at Pattern Recognition Center, WeChat AI, Tencent.",
    "prev": ".",
    "curr": "As it has been one of the key milestone tasks in conversational research (Zhang et al., 2018), a majority of previous works have studied how to effectively combine given knowledge and dialogue context to generate an utterance (Zhang et al., 2018;Li et al., 2019b;Parthasarathi & Pineau, 2018;Madotto et al., 2018;Gopalakrishnan et al., 2019).",
    "next": "Recently,  proposed to tackle the knowledge-grounded dialogue by decomposing it into two sub-problems: first selecting knowledge from a large pool of candidates and generating a response based on the selected knowledge and context."
  },
  {
    "index": 2541,
    "source_corpus_id": 3571422,
    "ref_id": "b5",
    "citation_corpus_id": 848112,
    "start": 18851,
    "end": 18875,
    "title": "TRAINING GANS WITH OPTIMISM",
    "abstract": "We address the issue of limit cycling behavior in training Generative Adversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for training Wasserstein GANs. Recent theoretical results have shown that optimistic mirror decent (OMD) can enjoy faster regret rates in the context of zero-sum games. WGANs is exactly a context of solving a zero-sum game with simultaneous noregret dynamics. Moreover, we show that optimistic mirror decent addresses the limit cycling problem in training WGANs. We formally show that in the case of bi-linear zero-sum games the last iterate of OMD dynamics converges to an equilibrium, in contrast to GD dynamics which are bound to cycle. We also portray the huge qualitative difference between GD and OMD dynamics with toy examples, even when GD is modified with many adaptations proposed in the recent literature, such as gradient penalty or momentum. We apply OMD WGAN training to a bioinformatics problem of generating DNA sequences. We observe that models trained with OMD achieve consistently smaller KL divergence with respect to the true underlying distribution, than models trained with GD variants. Finally, we introduce a new algorithm, Optimistic Adam, which is an optimistic variant of Adam. We apply it to WGAN training on CIFAR10 and observe improved performance in terms of inception score as compared to Adam.",
    "prev": "We thus propose a new technique that we call extrapolation from the past which only requires to compute one gradient for every update.",
    "curr": "The idea of this technique is to store and re-use the previous extrapolated gradient to compute the new extrapolation point:\n\nExtrapolation from the past:\nω t+1/2 = P Ω [ω t − ηF (ω t−1/2 )](20)\nPerform update step: ω t+1 = P Ω [ω t − ηF (ω t+1/2 )] and store:\nF (ω t+1/2 )(21)\nThis update scheme can be related to the optimistic mirror descent (Rakhlin and Sridharan, 2013; Daskalakis et al., 2018) in the unconstrained case, (20) and (21) reduce to:\n\nOptimistic mirror descent (OMD):\nω t+1/2 = ω t−1/2 − 2ηF (ω t−1/2 ) + ηF (ω t−3/2 )(22)\nHowever our technique comes from a different perspective, it was motived by VIP and inspired from the extragradient method.",
    "next": "Furthermore our technique extends to constrained optimization as shown in (20) and (21)."
  },
  {
    "index": 2542,
    "source_corpus_id": 3285020,
    "ref_id": "b48",
    "citation_corpus_id": 3047732,
    "start": 1601,
    "end": 1627,
    "title": "Deep Multi-task Representation Learning: A Tensor Factorisation Approach",
    "abstract": "Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL, as well as multi-domain learning (MDL). Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.",
    "prev": "As deep learning has yielded state-ofthe-art systems across a range of domains, there has been increased focus on developing deep MTL techniques.",
    "curr": "Such techniques have been applied across settings such as vision (Bilen and Vedaldi, 2016;Jou and Chang, 2016;Misra et al., 2016;Ranjan et al., 2016;Yang and Hospedales, 2017;Zhang et al., 2014), natural language (Collobert and Weston, 2008;Dong et al., 2015;Hashimoto et al., 2016;Liu et al., 2015a;Luong et al., 2016), speech (Huang et al., 2013;2015;Seltzer and Droppo, 2013;, and reinforcement learning (Devin et al., 2016;Fernando et al., 2017;Jaderberg et al., 2017;Rusu et al., 2016).",
    "next": "Although they improve performance over single-task learning in these settings, these approaches have generally been constrained to joint training of relatively few and/or closely-related tasks."
  },
  {
    "index": 2543,
    "source_corpus_id": 3700344,
    "ref_id": "b26",
    "citation_corpus_id": 8535316,
    "start": 4180,
    "end": 4199,
    "title": "BI-DIRECTIONAL ATTENTION FLOW FOR MACHINE COMPREHENSION",
    "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bidirectional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.",
    "prev": "Thus SearchQA tests the ability of AQA to reformulate questions such that the QA system has the best chance of returning the correct answer.",
    "curr": "AQA outperforms a deep network built for QA, BiDAF [Seo et al., 2017a], which has produced state-of-the-art results on multiple tasks, by 11% absolute F1, a 32% relative F1 improvement.",
    "next": "We conclude by proposing AQA as a general framework for stateful, iterative information seeking tasks."
  },
  {
    "index": 2545,
    "source_corpus_id": 259203617,
    "ref_id": "b11",
    "citation_corpus_id": 2428314,
    "start": 20369,
    "end": 20372,
    "title": "Published as a conference paper at ICLR 2017 CATEGORICAL REPARAMETERIZATION WITH GUMBEL-SOFTMAX",
    "abstract": "Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification. * Work done during an internship at Google Brain.",
    "prev": "Our final differentiable proxy objective is:\nθ = argmax θ∈R d W S (π θ ), where W S (π θ ) = E µ∼π θ W S (µ)(8)\nTo solve Eq.",
    "curr": "(8), we make use of the Gumbel top-k trick [25,12]: by reparametrizing π θ , variation in masks due to θ is replaced with random perturbations ε; this separates θ from the sampling process, which then permits to pass gradients effectively.",
    "next": "We then use the method from [26] to smooth the selection of the top-k elements."
  },
  {
    "index": 2546,
    "source_corpus_id": 32654687,
    "ref_id": "b19",
    "citation_corpus_id": 1450294,
    "start": 2243,
    "end": 2266,
    "title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps",
    "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [5], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks[13].",
    "prev": "While deep neural networks learn efficient and powerful representations, they are often considered a 'black-box'.",
    "curr": "In order to better understand classifier decisions and to gain insight into how these models operate, a variety techniques have been proposed (Simonyan et al., 2014;Yosinski et al., 2015;Nguyen et al., 2016;Baehrens et al., 2010;Bach et al., 2015;Montavon et al., 2017;Zeiler & Fergus, 2014;Springenberg et al., 2015;Zintgraf et al., 2017;Sundararajan et al., 2017;Smilkov et al., 2017).",
    "next": "The aforementioned methods for explaining classifier decisions operate under the assumption that it is possible to propagate the condensed output signal back through the classifier to arrive at something that shows how the relevant signal was encoded in the input and thereby explains the classifier decision."
  },
  {
    "index": 2547,
    "source_corpus_id": 209315300,
    "ref_id": "b14",
    "citation_corpus_id": 13751870,
    "start": 32389,
    "end": 32401,
    "title": "A Call for Clarity in Reporting BLEU Scores",
    "abstract": "The field of machine translation faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to \"the\" BLEU score, BLEU is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for usersupplied reference processing, and provide a new tool, SACREBLEU, 1 to facilitate this. . 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. ArXiv eprints, abs/1609.08144.",
    "prev": "Table 4 :\n4BLEU scores on newstest2014 for WMT English-German (EnDe).",
    "curr": "We additionally report detokenized BLEU scores as computed by sacreBLEU(Post, 2018).sacreBLEU \n\nhttps://github.com/google/trax/tree/master/trax/models/reformer 3 BLEU+case.lc+lang.en-de+numrefs.1+smooth.exp+test.wmt14/full+tok.intl+version.1.4.3 4 BLEU+case.mixed+lang.en-de+numrefs.1+smooth.exp+test.wmt14/full+tok.intl+version.1.4.3\n\nCharacter-level language modeling with deeper self-attention.",
    "next": "Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, Llion Jones, abs/1808.04444CoRRRami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones."
  },
  {
    "index": 2548,
    "source_corpus_id": 264172720,
    "ref_id": "b28",
    "citation_corpus_id": 257505014,
    "start": 10782,
    "end": 10800,
    "title": "Published as a conference paper at ICLR 2023 MESHDIFFUSION: SCORE-BASED GENERATIVE 3D MESH MODELING",
    "abstract": "We consider the task of generating realistic 3D shapes, which is useful for a variety of applications such as automatic scene generation and physical simulation. Compared to other 3D representations like voxels and point clouds, meshes are more desirable in practice, because (1) they enable easy and arbitrary manipulation of shapes for relighting and simulation, and (2) they can fully leverage the power of modern graphics pipelines which are mostly optimized for meshes. Previous scalable methods for generating meshes typically rely on sub-optimal post-processing, and they tend to produce overly-smooth or noisy surfaces without fine-grained geometric details. To overcome these shortcomings, we take advantage of the graph structure of meshes and use a simple yet very effective generative modeling method to generate 3D meshes. Specifically, we represent meshes with deformable tetrahedral grids, and then train a diffusion model on this direct parametrization. We demonstrate the effectiveness of our model on multiple generative tasks.Published as a conference paper at ICLR 2023 for computers, both voxels and point clouds are relatively hard for artists to edit, especially when the generated 3D shapes are complex and of low quality. Moreover, modern graphics pipelines are built and optimized for explicit geometry representations like meshes, making meshes one of the most desirable final 3D shape representations. While it is still possible to use methods like Poisson reconstruction to obtain surfaces from voxels and points clouds, the resulted surfaces are generally noisy and contain many topological artifacts, even with carefully tuned hyperparameters.To improve the representation flexibility, sign distance fields (SDFs) have been adopted to model shape surfaces, which enables us to use marching cubes [29] to extract the zero-surfaces and thus 3D meshes. However, SDFs are typically harder to learn as it requires a carefully designed sampling strategy and regularization. Because SDFs are usually parameterized with multi-layer perceptrons (MLPs) in which a smoothness prior is implicitly embedded, the generated shapes tend to be so smooth that sharp edges and important (and potentially semantic) details are lost. Moreover, SDFs are costly to render and therefore less suitable for downstream tasks like conditional generation with RGB images, which require an efficient differentiable renderer during inference.We instead aim to generate 3D shapes by directly producing 3D meshes, where surfaces are represented as a graph of triangular or polygon faces. With 3D meshes, all local surface information is completely included in the mesh vertices (along with the vertex connectivity), because the surface normal of any point on the shape surface is simply a nearest neighbor or some local linear combination of vertex normals. Such a regular structure with rich geometric details enables us to better model the data distribution and learn generative models that are more geometry-aware. In light of recent advances in score-based generative modeling[16,47]where powerful generative performance and effortless training are demonstrated, we propose to train diffusion models on these vertices to generate meshes. However, it is by no means a trivial task and poses two critical problems: (1) the numbers of vertices and faces are indefinite for general object categories, and (2) the underlying topology varies wildly and edges have to be generated at the same time.",
    "prev": "3D Generation with Diffusion Models.",
    "curr": "Drawing inspiration from 2D generation, recent endeavours have been made to train generative models on 3D datasets conditioned on text or images (Cheng et al., 2023;Liu et al., 2023d;Gupta et al., 2023;Zheng et al., 2023).",
    "next": "However, they are usually confined to specific categories with unsatisfactory generation results."
  },
  {
    "index": 2549,
    "source_corpus_id": 238744320,
    "ref_id": "b65",
    "citation_corpus_id": 239768373,
    "start": 2181,
    "end": 2198,
    "title": "Published as a conference paper at ICLR 2022 LEARNING WITH NOISY LABELS REVISITED: A STUDY USING REAL-WORLD HUMAN ANNOTATIONS",
    "abstract": "Existing research on learning with noisy labels mainly focuses on synthetic label noise. The synthetic noise, though has clean structures which greatly enabled statistical analyses, often fails to model the real-world noise patterns. The recent literature has observed several efforts to offer real-world noisy datasets, e.g., Food-101N, WebVision, and Clothing1M. Yet the existing efforts suffer from two caveats: firstly, the lack of ground-truth verification makes it hard to theoretically study the property and treatment of real-world label noise. Secondly, these efforts are often of large scales, which may result in unfair comparisons of robust methods within reasonable and accessible computation power. To better understand real-world label noise, it is important to establish controllable, easy-to-use and moderate-sized real-world noisy datasets with both ground-truth and noisy labels. This work presents two new benchmark datasets, which we name as CIFAR-10N, CIFAR-100N (jointly we call them CIFAR-N), equipping the training datasets of CIFAR-10 and CIFAR-100 with human-annotated real-world noisy labels we collected from Amazon Mechanical Turk. We quantitatively and qualitatively show that realworld noisy labels follow an instance-dependent pattern rather than the classically assumed and adopted ones (e.g., class-dependent label noise). We then initiate an effort to benchmarking a subset of the existing solutions using CIFAR-10N and CIFAR-100N. We further proceed to study the memorization of correct and wrong predictions, which further illustrates the difference between human noise and class-dependent synthetic noise. We show indeed the real-world noise patterns impose new and outstanding challenges as compared to synthetic label noise. These observations require us to rethink the treatment of noisy labels, and we hope the availability of these two datasets would facilitate the development and evaluation of future learning with noisy label solutions. The corresponding datasets and the leaderboard are available at http://noisylabels.com.",
    "prev": "Corresponding author: Yang Liu <yangliu@ucsc.edu>.",
    "curr": "INTRODUCTION\n\nThe success of deep neural networks benefits from large-size datasets with high-quality supervisions, while the collection of them may be difficult due to the high cost of data labeling process (Agarwal et al., 2016;Wei et al., 2022).",
    "next": "A much cheaper and easier solution is to obtain a small labeled dataset and a large unlabeled dataset, and apply semi-supervised learning (SSL)."
  },
  {
    "index": 2550,
    "source_corpus_id": 252846166,
    "ref_id": "b21",
    "citation_corpus_id": 3529936,
    "start": 4728,
    "end": 4746,
    "title": "Gradient Estimators for Implicit Models",
    "abstract": "Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the Stein gradient estimator, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference, and entropy regularised GANs that provide improved sample diversities.",
    "prev": "Rolland et al.",
    "curr": "(2022) estimate the Hessian point-wise with a second-order Stein gradient estimator (Li & Turner, 2018) over a radial basis function (RBF) kernel.",
    "next": "However, point-wise estimation with kernels scales poorly to datasets with large number of samples n because it requires inverting a n × n kernel matrix."
  },
  {
    "index": 2551,
    "source_corpus_id": 3947794,
    "ref_id": "b1",
    "citation_corpus_id": 604334,
    "start": 2177,
    "end": 2179,
    "title": "Intriguing properties of neural networks",
    "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
    "prev": "We show that combining those two techniques can also improve robustness under the worst case black box attack scenario.",
    "curr": "Introduction\n\nDeep neural networks and other machine learning classifiers are shown to be vulnerable to small perturbations to inputs [1,2,3,4,5].",
    "next": "Previous works [3,5,6] have shown that injecting adversarial examples during training (adversarial training) increases the robustness of a network against adversarial attacks."
  },
  {
    "index": 2552,
    "source_corpus_id": 257532315,
    "ref_id": "b28",
    "citation_corpus_id": 6628106,
    "start": 15463,
    "end": 15482,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "We sample 0.8 million frames in total at 1 Hz for training.",
    "curr": "For the first stage in PPGeo pipeline, we train the model for 30 epochs by Adam (Kingma & Ba, 2015) optimizer with a learning rate of 10 −4 which drops to 10 −5 after 25 epochs.",
    "next": "For the second stage, the encoder is trained for 20 epochs using the AdamW (Loshchilov & Hutter, 2017) optimizer."
  },
  {
    "index": 2555,
    "source_corpus_id": 215737267,
    "ref_id": "b46",
    "citation_corpus_id": 3687922,
    "start": 7549,
    "end": 7571,
    "title": "SEMI-PARAMETRIC TOPOLOGICAL MEMORY FOR NAVIGATION",
    "abstract": "We introduce a new memory architecture for navigation in previously unseen environments, inspired by landmark-based navigation in animals. The proposed semiparametric topological memory (SPTM) consists of a (non-parametric) graph with nodes corresponding to locations in the environment and a (parametric) deep network capable of retrieving nodes from the graph based on observations. The graph stores no metric information, only connectivity of locations corresponding to the nodes. We use SPTM as a planning module in a navigation system. Given only 5 minutes of footage of a previously unseen maze, an SPTM-based navigation agent can build a topological map of the environment and use it to confidently navigate towards goals. The average success rate of the SPTM agent in goal-directed navigation across test environments is higher than the best-performing baseline by a factor of three.",
    "prev": "These classical methods have inspired recent learning-based techniques.",
    "curr": "Researchers have designed neural network policies that reason via spatial representations Henriques and Vedaldi, 2018;Gordon et al., 2018), topological representations (Savinov et al., 2018;, or use differentiable and trainable planners (Tamar et al., 2016;Lee et al., 2018;Khan et al., 2017).",
    "next": "Our work furthers this research, and we study a hierarchical and modular decomposition of the problem and employ learning inside these components instead of end-to-end learning."
  },
  {
    "index": 2556,
    "source_corpus_id": 11245315,
    "ref_id": "b19",
    "citation_corpus_id": 10489017,
    "start": 3806,
    "end": 3822,
    "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF",
    "abstract": "State-of-the-art sequence labeling systems traditionally require large amounts of taskspecific knowledge in the form of handcrafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word-and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data preprocessing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks -Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both datasets -97.55% accuracy for POS tagging and 91.21% F1 for NER.",
    "prev": "In a nutshell, dropout randomly \"drops\" neural units during training as a means to prevent feature co-adaptation-a sign of overfitting (Hinton et al., 2012).",
    "curr": "Simple as it appears to be, dropout has led to several record-breaking performances (Hinton et al., 2012;Ma & Hovy, 2016), and thus spawned a lot of recent interests in analyzing and justifying dropout from the theoretical perspective, and also in further improving dropout from the algorithmic and practical perspective.",
    "next": "In their pioneering work, Hinton et al."
  },
  {
    "index": 2557,
    "source_corpus_id": 238419331,
    "ref_id": "b2",
    "citation_corpus_id": 215737187,
    "start": 3212,
    "end": 3235,
    "title": "Dense Passage Retrieval for Open-Domain Question Answering",
    "abstract": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dualencoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks. 1",
    "prev": "In NAACL-HLT, 2021.",
    "curr": "INTRODUCTION\n\nDense text retrieval Karpukhin et al., 2020) has achieved great successes in a wide variety of both research and industrial areas, such as search engines (Brickley et al., 2019;Shen et al., 2014), recommendation system (Hu et al., 2020), open-domain question answering (Guo et al., 2018;, etc.",
    "next": "A typical dense retrieval model adopts a dual-encoder (Huang et al., 2013) architecture to encode queries and documents into low-dimensional embedding vectors, with the relevance between query and document being measured by the similarity between embeddings."
  },
  {
    "index": 2558,
    "source_corpus_id": 119304640,
    "ref_id": "b16",
    "citation_corpus_id": 6628106,
    "start": 19814,
    "end": 19818,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "During inference, we sample at 30fps and apply a directional conditioning signal that has half of the average motion magnitude during training.",
    "curr": "Stopping criteria We use the Adam optimizer [16] with a learning rate of 2 · 10 −4 , β 1 = 0.5 and β 2 = 0.999.",
    "next": "We observe that training the P2P network does not provide for monotonic improvement in output quality."
  },
  {
    "index": 2559,
    "source_corpus_id": 223953594,
    "ref_id": "b19",
    "citation_corpus_id": 8394195,
    "start": 2633,
    "end": 2653,
    "title": "LOSSY IMAGE COMPRESSION WITH COMPRESSIVE AUTOENCODERS",
    "abstract": "We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more flexible than existing codecs. Autoencoders have the potential to address this need, but are difficult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufficient to train deep autoencoders competitive with JPEG 2000 and outperforming recently proposed approaches based on RNNs. Our network is furthermore computationally efficient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.",
    "prev": "Any further improvement is expected to be even more difficult.",
    "curr": "Inspired by the successful stories of deep learning in many vision tasks, several pioneer works [Toderici et al., 2016, Agustsson et al., 2017, Theis et al., 2017, Ballé et al., 2017, Ballé et al., 2018, Mentzer et al., 2018, Lee et al., 2019, Minnen et al., 2018a demonstrate that the image compression task can be effectively solved by deep learning too.",
    "next": "This breakthrough allows us to use data-driven learning system to design novel compression algorithms automatically."
  },
  {
    "index": 2561,
    "source_corpus_id": 600040,
    "ref_id": "b4",
    "citation_corpus_id": 11212020,
    "start": 2813,
    "end": 2816,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "Some previous work propose to adapt phrase-based statistical machine translation (SMT) for code migration [26,20,27].",
    "curr": "Recently, neural network approaches, such as sequence-to-sequencebased models, have achieved the state-of-the-art performance on machine translation [5,10,14,15,36].",
    "next": "In this work, we study neural machine translation methods to handle the program translation problem."
  },
  {
    "index": 2564,
    "source_corpus_id": 256697328,
    "ref_id": "b3",
    "citation_corpus_id": 231918471,
    "start": 4182,
    "end": 4210,
    "title": "SCALABLE BAYESIAN INVERSE REINFORCEMENT LEARNING",
    "abstract": "Bayesian inference over the reward presents an ideal solution to the ill-posed nature of the inverse reinforcement learning problem. Unfortunately current methods generally do not scale well beyond the small tabular setting due to the need for an inner-loop MDP solver, and even non-Bayesian methods that do themselves scale often require extensive interaction with the environment to perform well, being inappropriate for high stakes or costly applications such as healthcare. In this paper we introduce our method, Approximate Variational Reward Imitation Learning (AVRIL), that addresses both of these issues by jointly learning an approximate posterior distribution over the reward that scales to arbitrarily complicated state spaces alongside an appropriate policy in a completely offline manner through a variational approach to said latent reward. Applying our method to real medical data alongside classic control simulations, we demonstrate Bayesian reward inference in environments beyond the scope of current methods, as well as task performance competitive with focused offline imitation learning algorithms.",
    "prev": "(2020;.",
    "curr": "Unfortunately, to the best of our knowledge, this challenge remains not well understood in offline IRL, albeit there is some recent progress (Zolna et al., 2020;Garg et al., 2021;Chan & van der Schaar, 2021).",
    "next": "Thus motivated, the key question this paper seeks to answer is: \"How to devise offline IRL algorithms that can ameliorate the reward extrapolation error effectively?\""
  },
  {
    "index": 2566,
    "source_corpus_id": 245005650,
    "ref_id": "b18",
    "citation_corpus_id": 232428253,
    "start": 42532,
    "end": 42549,
    "title": "Published as a conference paper at ICLR 2021 BENCHMARKS FOR DEEP OFF-POLICY EVALUATION",
    "abstract": "Off-policy evaluation (OPE) holds the promise of being able to leverage large, offline datasets for both evaluating and selecting complex policies for decision making. The ability to learn offline is particularly important in many real-world domains, such as in healthcare, recommender systems, or robotics, where online data collection is an expensive and potentially dangerous process. Being able to accurately evaluate and select high-performing policies without requiring online interaction could yield significant benefits in safety, time, and cost for these applications. While many OPE methods have been proposed in recent years, comparing results between papers is difficult because currently there is a lack of a comprehensive and unified benchmark, and measuring algorithmic progress has been challenging due to the lack of difficult evaluation tasks. In order to address this gap, we present a collection of policies that in conjunction with existing offline datasets can be used for benchmarking off-policy evaluation. Our tasks include a range of challenging high-dimensional continuous control problems, with wide selections of datasets and policies for performing policy selection. The goal of our benchmark is to provide a standardized measure of progress that is motivated from a set of principles designed to challenge and test the limits of existing OPE methods. We perform an evaluation of state-of-the-art algorithms and provide open-source access to our data and code to foster future research in this area † .Published as a conference paper at ICLR 2021 commonly explored by modern deep reinforcement learning algorithms(Bellemare et al., 2013;Brockman et al., 2016)  with standardized evaluation protocols and metrics. Our goal is to provide a set of tasks with a range of difficulty, excercise a variety of design properties, and provide policies with different behavioral patterns in order to establish a standardized framework for comparing OPE algorithms. We put particular emphasis on large datasets, long-horizon tasks, and task complexity to facilitate the development of scalable algorithms that can solve high-dimensional problems.Our primary contribution is the Deep Off-Policy Evaluation (DOPE) benchmark. DOPE is designed to measure the performance of OPE methods by 1) evaluating on challenging control tasks with properties known to be difficult for OPE methods, but which occur in real-world scenarios, 2) evaluating across a range of policies with different values, to directly measure performance on policy evaluation, ranking and selection, and 3) evaluating in ideal and adversarial settings in terms of dataset coverage and support. These factors are independent of task difficulty, but are known to have a large impact on OPE performance. To achieve 1, we selected tasks on a set of design principles outlined in Section 3.1. To achieve 2, for each task we include 10 to 96 policies for evaluation and devise an evaluation protocol that measures policy evaluation, ranking, and selection as outlined in Section 3.2. To achieve 3, we provide two domains with differing dataset coverage and support properties described in Section 4. Finally, to enable an easy-to-use research platform, we provide the datasets, target policies, evaluation API, as well as the recorded results of state-of-the-art algorithms (presented in Section 5) as open-source. . Counterfactual reasoning and learning systems: The example of computational advertising. . Deep reinforcement learning in a handful of trials using probabilistic dynamics models. , et al. Reinforcement learning benchmarks and bake-offs ii.",
    "prev": "We believe that understanding the learning dynamics of deep Q-learning and the induced implicit regularization will lead to more robust and stable deep RL algorithms.",
    "curr": "Furthermore, this understanding can help us to predict the instability issues in value-based RL methods in advance, which can inspire crossvalidation and model selection strategies, an important, open challenge in offline RL, for which existing off-policy evaluation techniques are not practically sufficient (Fu et al., 2021).",
    "next": "We also note that our analysis does not consider the online RL setting with non-stationary data distributions, and extending our theory and DR3 to online RL is an interesting avenue for future work."
  },
  {
    "index": 2567,
    "source_corpus_id": 263311025,
    "ref_id": "b42",
    "citation_corpus_id": 202539551,
    "start": 2893,
    "end": 2915,
    "title": "Language Models as Knowledge Bases?",
    "abstract": "Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as \"fillin-the-blank\" cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-theart pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https: //github.com/facebookresearch/LAMA.",
    "prev": "1 Our code is available at: https://github.com/Vaidehi99/InfoDeletionAttacks arXiv:2309.17410v1 [cs.CL] 29 Sep 2023• How can we \"delete\" specific sensitive information from language models when we do not want models to know or express this information?• How do we test whether that specific information was successfully deleted?Language ModelLanguage Model\n\nINTRODUCTION\n\nLarge language models (LLMs) now possess much factual knowledge about the world.",
    "curr": "This knowledge can be extracted from models using natural language prompts (Petroni et al., 2019), or models can be finetuned to answer user questions within a dialogue (Ouyang et al., 2022).",
    "next": "Notably, these models sometimes possess knowledge that we do not wish them to, including memorized personal information (Carlini et al., 2021), knowledge that could be used to harm people (e.g."
  },
  {
    "index": 2568,
    "source_corpus_id": 258437253,
    "ref_id": "b28",
    "citation_corpus_id": 52920181,
    "start": 3885,
    "end": 3907,
    "title": "EPISODIC CURIOSITY THROUGH REACHABILITY",
    "abstract": "Rewards are sparse in the real world and most today's reinforcement learning algorithms struggle with such sparsity. One solution to this problem is to allow the agent to create rewards for itself -thus making rewards dense and more suitable for learning. In particular, inspired by curious behaviour in animals, observing something novel could be rewarded with a bonus. Such bonus is summed up with the real task reward -making it possible for RL algorithms to learn from the combined reward. We propose a new curiosity method which uses episodic memory to form the novelty bonus. To determine the bonus, the current observation is compared with the observations in memory. Crucially, the comparison is done based on how many environment steps it takes to reach the current observation from those in memory -which incorporates rich information about environment dynamics. This allows us to overcome the known \"couch-potato\" issues of prior work -when the agent finds a way to instantly gratify itself by exploiting actions which lead to hardly predictable consequences. We test our approach in visually rich 3D environments in VizDoom, DMLab and MuJoCo. In navigational tasks from VizDoom and DMLab, our agent outperforms the state-of-the-art curiosity method ICM. In MuJoCo, an ant equipped with our curiosity module learns locomotion out of the first-person-view curiosity only. * Shared first authorship.",
    "prev": "Additionally, catastrophic forgetting makes parametric methods susceptible to the so-called 'detachment' problem in which the algorithm loses track of promising areas to explore (Ostrovski et al., 2017).",
    "curr": "Non-parametric methods rely on a memory to store encountered states (Savinov et al., 2018;Badia et al., 2020b).",
    "next": "This facilitates responsiveness to the most recent experience as well as preserving memories without interference."
  },
  {
    "index": 2571,
    "source_corpus_id": 53483457,
    "ref_id": "b1",
    "citation_corpus_id": 11212020,
    "start": 2347,
    "end": 2370,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "However, learning vanilla RNNs over long distance proves to be difficult due to the vanishing gradient problem (Bengio et al., 1994;Pascanu et al., 2013).",
    "curr": "One alleviation is to introduce skip-connections along the execution path, in the forms of dilated layers (Van Den Oord et al., 2016;Chang et al., 2017), attention mechanisms (Bahdanau et al., 2015;Vaswani et al., 2017) and external memory (Graves et al., 2014;2016).",
    "next": "Amongst all, using external memory most resembles human cognitive architecture where we perceive the world sequentially and make decision by consulting our memory."
  },
  {
    "index": 2572,
    "source_corpus_id": 238634219,
    "ref_id": "b21",
    "citation_corpus_id": 68167178,
    "start": 29445,
    "end": 29462,
    "title": "Neural Approaches to Conversational AI",
    "abstract": "This tutorial surveys neural approaches to conversational AI that were developed in the last few years. We group conversational systems into three categories: (1) question answering agents, (2) task-oriented dialogue agents, and (3) social bots. For each category, we present a review of state-of-the-art neural approaches, draw the connection between neural approaches and traditional symbolic approaches, and discuss the progress we have made and challenges we are facing, using specific systems and models as case studies.",
    "prev": "We stress-test full fine-tuning under DP on the task of chit-chat dialog generation.",
    "curr": "This task has the distinct challenge that the response space is intrinsically diverse (Li et al., 2015;Gao et al., 2018) since human conversations can be informal and noisy (Zhang et al., 2019).",
    "next": "Moreover, dialog datasets are usually formed with user data which may contain sensitive information."
  },
  {
    "index": 2573,
    "source_corpus_id": 264306111,
    "ref_id": "b27",
    "citation_corpus_id": 56657912,
    "start": 14150,
    "end": 14180,
    "title": "BENCHMARKING NEURAL NETWORK ROBUSTNESS TO COMMON CORRUPTIONS AND PERTURBATIONS",
    "abstract": "In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, IMAGENET-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called IMAGENET-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.",
    "prev": "IMAGE CLASSIFICATION\n\nTask statement.Image classification is the most common challenge for representation learning.",
    "curr": "We conduct experiments on ImageNet1k (Deng et al., 2009) for accuracy, and additionally evaluate on robustness benchmarks: corrupted images from ImageNet-C (Hendrycks & Dietterich, 2018), natural adversarial images from ImageNet-A (Hendrycks et al., 2021b), and out-of-distribution images from ImageNet-SK (Wang et al., 2019) and ImageNet-R (Hendrycks et al., 2021a).",
    "next": "Design.Without loss of generality, we select ViT (Dosovitskiy et al., 2021) due to its wide use and native support for transformers.Following the notation in Eqn."
  },
  {
    "index": 2578,
    "source_corpus_id": 260091821,
    "ref_id": "b1",
    "citation_corpus_id": 222140788,
    "start": 1815,
    "end": 1817,
    "title": "Published as a conference paper at ICLR 2021 DENOISING DIFFUSION IMPLICIT MODELS",
    "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples 10× to 50× faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error.",
    "prev": "Our research provides novel insights into the design of numerical methods for future diffusion work.",
    "curr": "Introduction\n\nDiffusion models [1,2] are a type of generative models that has garnered considerable attention due to their remarkable image quality.",
    "next": "Unlike Generative Adversarial Networks (GANs) [3], which may suffer from mode collapse and instabilities during training, diffusion models offer reduced sensitivity to hyperparameters [1,4] and improve sampling quality [5]."
  },
  {
    "index": 2580,
    "source_corpus_id": 249192258,
    "ref_id": "b15",
    "citation_corpus_id": 257232596,
    "start": 34729,
    "end": 34750,
    "title": "Published as a conference paper at ICLR 2023 FROM t-SNE TO UMAP WITH CONTRASTIVE LEARNING",
    "abstract": "Neighbor embedding methods t-SNE and UMAP are the de facto standard for visualizing high-dimensional datasets. Motivated from entirely different viewpoints, their loss functions appear to be unrelated. In practice, they yield strongly differing embeddings and can suggest conflicting interpretations of the same data. The fundamental reasons for this and, more generally, the exact relationship between t-SNE and UMAP have remained unclear. In this work, we uncover their conceptual connection via a new insight into contrastive learning methods. Noisecontrastive estimation can be used to optimize t-SNE, while UMAP relies on negative sampling, another contrastive method. We find the precise relationship between these two contrastive methods and provide a mathematical characterization of the distortion introduced by negative sampling. Visually, this distortion results in UMAP generating more compact embeddings with tighter clusters compared to t-SNE. We exploit this new conceptual connection to propose and implement a generalization of negative sampling, allowing us to interpolate between (and even extrapolate beyond) t-SNE and UMAP and their respective embeddings. Moving along this spectrum of embeddings leads to a trade-off between discrete / local and continuous / global structures, mitigating the risk of over-interpreting ostensible features of any single embedding. We provide a PyTorch implementation.",
    "prev": "On the other hand, standard SNE methods can also borrow existing techniques in SSCL to improve their performance on more complicated data, e.g., incorporating data augmentations instead of or on top of pre-defined distances.",
    "curr": "In this sense, by choosing feature dimension to be 2, various SSCL methods can also be used as data visualization tools Damrich et al., 2022).",
    "next": "Specifically on CIFAR-10, standard t-SNE can barely reveal any clusters while our t-SimCLR with d z = 2 produces much more separation among different labels."
  },
  {
    "index": 2581,
    "source_corpus_id": 252668622,
    "ref_id": "b7",
    "citation_corpus_id": 232076011,
    "start": 3754,
    "end": 3757,
    "title": "Published as a conference paper at ICLR 2021 GRADIENT DESCENT ON NEURAL NETWORKS TYPI- CALLY OCCURS AT THE EDGE OF STABILITY",
    "abstract": "We empirically demonstrate that full-batch gradient descent on neural network training objectives typically operates in a regime we call the Edge of Stability. In this regime, the maximum eigenvalue of the training loss Hessian hovers just above the value 2/(step size), and the training loss behaves non-monotonically over short timescales, yet consistently decreases over long timescales. Since this behavior is inconsistent with several widespread presumptions in the field of optimization, our findings raise questions as to whether these presumptions are relevant to neural network training. We hope that our findings will inspire future efforts aimed at rigorously understanding optimization at the Edge of Stability.arXiv:2103.00065v3 [cs.LG] 23 Nov 2022Published as a conference paper at ICLR 2021 this happens, gradient descent does not diverge entirely or stall. Instead, it enters a regime we call the Edge of Stability 1 ( §3.2), in which (1) the sharpness hovers right at, or just above, the value 2/η; and (2) the train loss behaves non-monotonically, yet consistently decreases over long timescales. In this regime, gradient descent is constantly \"trying\" to increase the sharpness, but is constantly restrained from doing so. The net effect is that gradient descent continues to successfully optimize the training objective, but in such a way as to avoid further increasing the sharpness. 2",
    "prev": "Recently, Cohen et al.",
    "curr": "[8] observed two important phenomena for gradient descent, which made more precise similar observations in Jastrzębski et al.",
    "next": "[15,16] for SGD:\n\nProgressive Sharpening Throughout most of the optimization trajectory, the gradient of the loss is negatively aligned with the gradient of sharpness, i.e."
  },
  {
    "index": 2582,
    "source_corpus_id": 67856213,
    "ref_id": "b16",
    "citation_corpus_id": 3568073,
    "start": 2865,
    "end": 2886,
    "title": "PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION",
    "abstract": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at 1024 2 . We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CELEBA dataset.",
    "prev": "Furthermore, since these large models operate at a fine timescale, their autoencoder variants are restricted to only modeling local latent structure due to memory constraints (Engel et al., 2017).",
    "curr": "On the other end of the spectrum, Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) have seen great recent success at generating high resolution images Berthelot et al., 2017;Kodali et al., 2017;Karras et al., 2018a;Miyato et al., 2018).",
    "next": "Typical GANs achieve both efficient parallel sampling and global latent control by conditioning a stack of transposed convolutions on a latent vector, The potential for audio GANs extends further, as adversarial costs have unlocked intriguing domain transformations for images that could possibly have analogues in audio Wolf et al., 2017;Jin et al., 2017)."
  },
  {
    "index": 2585,
    "source_corpus_id": 12933888,
    "ref_id": "b6",
    "citation_corpus_id": 5590763,
    "start": 2711,
    "end": 2729,
    "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
    "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
    "prev": "Generating a naturallanguage description of a video, termed video captioning, is an important component of video analysis.",
    "curr": "Inspired by the successful encoder-decoder framework used in machine translation (Cho et al., 2014;Bahdanau et al., 2015; and image caption generation (Kiros et al., 2014;Vinyals et al., 2015;Karpathy & Li, 2015;Mao et al., 2015), most recent work on video captioning (Donahue et al., 2015;Venugopalan et al., 2015a;Pan et al., 2016b;Yu et al., 2016) employs a twodimensional (2D) or three-dimensional (3D) Convolutional Neural Network (CNN) as an encoder, mapping an input video to a compact feature-vector representation.",
    "next": "A Recurrent Neural Network (RNN) is typically employed as a decoder, unrolling the feature vector to generate a sequence of words of arbitrary length."
  },
  {
    "index": 2586,
    "source_corpus_id": 56657912,
    "ref_id": "b23",
    "citation_corpus_id": 13046179,
    "start": 2201,
    "end": 2227,
    "title": "A BASELINE FOR DETECTING MISCLASSIFIED AND OUT-OF-DISTRIBUTION EXAMPLES IN NEURAL NETWORKS",
    "abstract": "We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.Published as a conference paper at ICLR 2017 one method which outperforms the baseline on some (but not all) tasks. This new method evaluates the quality of a neural network's input reconstruction to determine if an example is abnormal.",
    "prev": "It is also essential for creating deep learning systems that can be deployed in safety-critical applications.",
    "curr": "Most work on robustness in deep learning methods for vision has focused on the important challenges of robustness to adversarial examples (Szegedy et al., 2014;Carlini & Wagner, 2017;, unknown unknowns (Hendrycks et al., 2019;Hendrycks & Gimpel, 2017b;Liu et al., 2018), and model or data poisoning (Steinhardt et al., 2017;.",
    "next": "In contrast, we develop and validate datasets for two other forms of robustness."
  },
  {
    "index": 2589,
    "source_corpus_id": 6039192,
    "ref_id": "b20",
    "citation_corpus_id": 252796,
    "start": 17651,
    "end": 17672,
    "title": "Building a Large Annotated Corpus of English: The Penn Treebank",
    "abstract": "There is a growing consensus that significant, rapid progress can be made in both text understanding and spoken language understanding by investigating those phenomena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large corpora. Such corpora are beginning to serve as important research tools for investigators in natural language processing, speech recognition, and integrated spoken language systems, as well as in theoretical linguistics. Annotated corpora promise to be valuable for enterprises as diverse as the automatic construction of statistical models for the grammar of the written and the colloquial spoken language, the development of explicit formal theories of the differing grammars of writing and speech, the investigation of prosodic phenomena in speech, and the evaluation and comparison of the adequacy of parsing models.In this paper, we review our experience with constructing one such large annotated corpus--the Penn Treebank, a corpus 1 consisting of over 4.5 million words of American English. During the first three-year phase of the Penn Treebank Project (1989)(1990)(1991)(1992), this corpus has been annotated for part-of-speech (POS) information. In addition, over half of it has been annotated for skeletal syntactic structure. These materials are available to members of the Linguistic Data Consortium; for details, see Section 5.1.The paper is organized as follows. Section 2 discusses the POS tagging task. After outlining the considerations that informed the design of our POS tagset and presenting the tagset itself, we describe our two-stage tagging process, in which text is first assigned POS tags automatically and then corrected by human annotators. Section 3 briefly presents the results of a comparison between entirely manual and semi-automated tagging, with the latter being shown to be superior on three counts: speed, consistency, and accuracy. In Section 4, we turn to the bracketing task. Just as with the tagging task, we have partially automated the bracketing task: the output of",
    "prev": "EXPERIMENTS\n\nWe assess the performance of our proposed TopicRNN model on word prediction and sentiment analysis 5 .",
    "curr": "For word prediction we use the Penn TreeBank dataset, a standard benchmark for assessing new language models (Marcus et al., 1993).",
    "next": "For sentiment analysis we use the IMDB 100k dataset (Maas et al., 2011), also a common benchmark dataset for this application 6 ."
  },
  {
    "index": 2592,
    "source_corpus_id": 3904215,
    "ref_id": "b30",
    "citation_corpus_id": 252796,
    "start": 10125,
    "end": 10129,
    "title": "Building a Large Annotated Corpus of English: The Penn Treebank",
    "abstract": "There is a growing consensus that significant, rapid progress can be made in both text understanding and spoken language understanding by investigating those phenomena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large corpora. Such corpora are beginning to serve as important research tools for investigators in natural language processing, speech recognition, and integrated spoken language systems, as well as in theoretical linguistics. Annotated corpora promise to be valuable for enterprises as diverse as the automatic construction of statistical models for the grammar of the written and the colloquial spoken language, the development of explicit formal theories of the differing grammars of writing and speech, the investigation of prosodic phenomena in speech, and the evaluation and comparison of the adequacy of parsing models.In this paper, we review our experience with constructing one such large annotated corpus--the Penn Treebank, a corpus 1 consisting of over 4.5 million words of American English. During the first three-year phase of the Penn Treebank Project (1989)(1990)(1991)(1992), this corpus has been annotated for part-of-speech (POS) information. In addition, over half of it has been annotated for skeletal syntactic structure. These materials are available to members of the Linguistic Data Consortium; for details, see Section 5.1.The paper is organized as follows. Section 2 discusses the POS tagging task. After outlining the considerations that informed the design of our POS tagset and presenting the tagset itself, we describe our two-stage tagging process, in which text is first assigned POS tags automatically and then corrected by human annotators. Section 3 briefly presents the results of a comparison between entirely manual and semi-automated tagging, with the latter being shown to be superior on three counts: speed, consistency, and accuracy. In Section 4, we turn to the bracketing task. Just as with the tagging task, we have partially automated the bracketing task: the output of",
    "prev": "To avoid this effect, we scale down the skip connection slightly, setting α = 0.99.",
    "curr": "Experiments\n\n\nCharacter-level Penn Treebank\n\nWe train character level RNN language models on Penn Treebank [31], a 6MB text corpus with a vocabulary of 54 characters.",
    "next": "Because of the small size of the dataset, proper regularization is key to getting good performance."
  },
  {
    "index": 2594,
    "source_corpus_id": 228063930,
    "ref_id": "b2",
    "citation_corpus_id": 52889459,
    "start": 3886,
    "end": 3905,
    "title": "LARGE SCALE GAN TRAINING FOR HIGH FIDELITY NATURAL IMAGE SYNTHESIS",
    "abstract": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by truncating the latent space. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128×128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.3 and Fréchet Inception Distance (FID) of 9.6, improving over the previous best IS of 52.52 and FID of 18.65.",
    "prev": "In contrast, our model can synthesize diverse and high-quality images while only using an adversarial loss, without any external supervision.",
    "curr": "INTRODUCTION\n\nConditional generative adversarial networks (GANs) (Mirza & Osindero, 2014) synthesize images conditioned on class labels Brock et al., 2019), text (Reed et al., 2016;Zhang et al., 2018a), other images (Isola et al., 2017;Huang et al., 2018), or semantic label maps (Wang et al., 2018;Park et al., 2019).",
    "next": "In this work, we focus on the latter, addressing semantic image synthesis."
  },
  {
    "index": 2595,
    "source_corpus_id": 7559933,
    "ref_id": "b36",
    "citation_corpus_id": 11758569,
    "start": 2228,
    "end": 2232,
    "title": "UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks -demonstrating their applicability as general image representations.arXiv:1511.06434v2 [cs.LG] 7 Jan 2016Under review as a conference paper at ICLR 2016• We show that the generators have interesting vector arithmetic properties allowing for easy manipulation of many semantic qualities of generated samples.",
    "prev": "For LSUN and celebA, Coulomb GANs set a new state of the art and produce a previously unseen variety of different samples.",
    "curr": "Introduction\n\nGenerative adversarial networks (GANs) [13] excel at constructing realistic images [36,28,24,2,4] and text [17].",
    "next": "In GAN learning, a discriminator network guides the learning of another, generative network."
  },
  {
    "index": 2597,
    "source_corpus_id": 252715808,
    "ref_id": "b11",
    "citation_corpus_id": 67915085,
    "start": 1737,
    "end": 1758,
    "title": "STOCHASTIC OPTIMIZATION OF SORTING NETWORKS VIA CONTINUOUS RELAXATIONS",
    "abstract": "Sorting input objects is an important step in many machine learning pipelines. However, the sorting operator is non-differentiable with respect to its inputs, which prohibits end-to-end gradient-based optimization. In this work, we propose NeuralSort, a general-purpose continuous relaxation of the output of the sorting operator from permutation matrices to the set of unimodal row-stochastic matrices, where every row sums to one and has a distinct arg max. This relaxation permits straight-through optimization of any computational graph involve a sorting operation. Further, we use this relaxation to enable gradient-based stochastic optimization over the combinatorially large space of permutations by deriving a reparameterized gradient estimator for the Plackett-Luce family of distributions over permutations. We demonstrate the usefulness of our framework on three tasks that require learning semantic orderings of high-dimensional objects, including a fully differentiable, parameterized extension of the k-nearest neighbors algorithm. ;Chierichetti et al., 2018). By learning distributions over unobserved permutations, we can account for the uncertainty in these permutations in a principled manner. However, the challenge with stochastic optimization over discrete distributions lies in gradient estimation with respect to the distribution parameters. Vanilla REINFORCE estimators are impractical for most cases, or necessitate custom control variates for low-variance gradient estimation (Glasserman, 2013).In this regard, we consider the Plackett-Luce (PL) family of distributions over permutations (Plackett, 1975; Luce, 1959). A common modeling choice for ranking models, the PL distribution is parameterized by n scores, with its support defined over the symmetric group consisting of n! permutations. We derive a reparameterizable sampler for stochastic optimization with respect to this distribution, based on Gumbel perturbations to the n (log-)scores. However, the reparameterized sampler requires sorting these perturbed scores, and hence the gradients of a downstream learning objective with respect to the scores are not defined. By using NeuralSort instead, we can approximate the objective and obtain well-defined reparameterized gradient estimates for stochastic optimization.Finally, we apply NeuralSort to tasks that require us to learn semantic orderings of complex, highdimensional input data. First, we consider sorting images of handwritten digits, where the goal is to learn to sort images by their unobserved labels. Our second task extends the first one to quantile regression, where we want to estimate the median (50-th percentile) of a set of handwritten numbers. In addition to identifying the index of the median image in the sequence, we need to learn to map the inferred median digit to its scalar representation. In the third task, we propose an algorithm that learns a basis representation for the k-nearest neighbors (kNN) classifier in an end-to-end procedure. Because the choice of the k nearest neighbors requires a non-differentiable sorting, we use NeuralSort to obtain an approximate, differentiable surrogate. On all tasks, we observe significant empirical improvements due to NeuralSort over the relevant baselines and competing relaxations to permutation matrices.PRELIMINARIESAn n-dimensional permutation z = [z 1 , z 2 , . . . , z n ] T is a list of unique indices {1, 2, . . . , n}. Every permutation z is associated with a permutation matrix P z ∈ {0, 1} n×n with entries given as: P z [i, j] = 1 if j = z i 0 otherwise. multimodal graph matching: Sparse coding meets graph matching. In Advances in Neural Information Processing Systems, 2013.",
    "prev": "INTRODUCTION\n\nk-subset sampling, sampling a subset of size k of n variables, is omnipresent in machine learning.",
    "curr": "It lies at the core of many fundamental problems that rely upon learning sparse features representations of input data, including stochastic high-dimensional data visualization (van der Maaten, 2009), parametric k-nearest neighbors (Grover et al., 2018), learning to explain (Chen et al., 2018), discrete variational auto-encoders (Rolfe, 2017), and sparse regression, to name a few.",
    "next": "All such tasks involve optimizing an expectation of an objective function with respect to a latent discrete distribution parameterized by a neural network, which are often assumed intractable."
  },
  {
    "index": 2598,
    "source_corpus_id": 218581596,
    "ref_id": "b25",
    "citation_corpus_id": 4009713,
    "start": 10028,
    "end": 10031,
    "title": "Published as a conference paper at ICLR 2018 UNSUPERVISED REPRESENTATION LEARNING BY PRE- DICTING IMAGE ROTATIONS",
    "abstract": "Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training Con-vNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet.",
    "prev": "These tasks usually involve hiding certain information about the input and training the network to recover those missing information.",
    "curr": "Examples include image inpainting [20], colorization [21,22], prediction of patch orderings [23,24] and image transformations [25,26,27,28].",
    "next": "However, most of these pretext tasks exploit specific structures of visual data, making them harder to generalize to other domains."
  },
  {
    "index": 2599,
    "source_corpus_id": 252762386,
    "ref_id": "b3",
    "citation_corpus_id": 213488539,
    "start": 7351,
    "end": 7373,
    "title": "Published as a conference paper at ICLR 2020 LEARNING FROM RULES GENERALIZING LABELED EXEMPLARS",
    "abstract": "In many applications labeled data is not readily available, and needs to be collected via pain-staking human supervision. We propose a rule-exemplar method for collecting human supervision to combine the efficiency of rules with the quality of instance labels. The supervision is coupled such that it is both natural for humans and synergistic for learning. We propose a training algorithm that jointly denoises rules via latent coverage variables, and trains the model through a soft implication loss over the coverage and label variables. The denoised rules and trained model are used jointly for inference. Empirical evaluation on five different tasks shows that (1) our algorithm is more accurate than several existing methods of learning from a mix of clean and noisy supervision, and (2) the coupled rule-exemplar supervision is effective in denoising rules.",
    "prev": "One recent line of work has considered constraining the space of possible pseudolabels via weak labeler accuracies (Arachie & Huang, 2019;Mazzetto et al., 2021a;b;Arachie & Huang, 2021;2022).",
    "curr": "Other works improve the aggregation scheme (Xu et al., 2021) or the weak labelers (Awasthi et al., 2020).",
    "next": "We note that only one method incorporates any notion of smoothness into the weakly supervised pipeline (Chen et al., 2022)."
  },
  {
    "index": 2601,
    "source_corpus_id": 247741267,
    "ref_id": "b41",
    "citation_corpus_id": 207878944,
    "start": 6178,
    "end": 6199,
    "title": "EXPLANATION BY PROGRESSIVE EXAGGERATION",
    "abstract": "As machine learning methods see greater adoption and implementation in high stakes applications such as medical image diagnosis, the need for model interpretability and explanation has become more critical.Classical approaches that assess feature importance (e.g., saliency maps) do not explain how and why a particular region of an image is relevant to the prediction.We propose a method that explains the outcome of a classification black-box by gradually exaggerating the semantic effect of a given class.Given a query input to a classifier, our method produces a progressive set of plausible variations of that query, which gradually changes the posterior probability from its original class to its negation.These counter-factually generated samples preserve features unrelated to the classification decision, such that a user can employ our method as a \"tuning knob\" to traverse a data manifold while crossing the decision boundary.Our method is model agnostic and only requires the output value and gradient of the predictor with respect to its input.",
    "prev": "It is suitable for lay-users and includes several measures to ensure high-quality crowd-sourced responses, • A strong and simple baseline explanation technique using only the model output, which we propose as a benchmark for future studies, • We open-source our dataset, explanation techniques, model, study design, including instructions and videos to support replicating our results as well as adapting our design to other explanation techniques.",
    "curr": "RELATED WORK\n\nInterpretable ML for Vision Different explanation approaches have been proposed: saliency maps (Bach et al., 2015;Ancona et al., 2018;Sundararajan et al., 2017), example-based explanations (Cai et al., 2019), counterfactual examples (Singla et al., 2020), activation-concept approaches (Kim et al., 2018), or models with built-in interpretability (Chen et al., 2019;Brendel & Bethge, 2018).",
    "next": "For a detailed review about the field, we refer to (Gilpin et al., 2018;Molnar et al., 2020)."
  },
  {
    "index": 2602,
    "source_corpus_id": 254636478,
    "ref_id": "b35",
    "citation_corpus_id": 11758569,
    "start": 1785,
    "end": 1806,
    "title": "UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks -demonstrating their applicability as general image representations.arXiv:1511.06434v2 [cs.LG] 7 Jan 2016Under review as a conference paper at ICLR 2016• We show that the generators have interesting vector arithmetic properties allowing for easy manipulation of many semantic qualities of generated samples.",
    "prev": "Code and models will be publicly available at https://genforce.github.io/StyleSV.",
    "curr": "INTRODUCTION\n\nSynthesizing images using generative adversarial network (GAN) (Goodfellow et al., 2014;Radford et al., 2016;Karras et al., 2019;2020b; usually requires composing diverse visual concepts and their spatial arrangement.",
    "next": "Recent advances in GANs have enabled many appealing applications such as customized editing (Goetschalckx et al., 2019;Jahanian et al., 2020;Yang et al., 2021) and animation (Qiu et al., 2022;Alaluf et al., 2022)."
  },
  {
    "index": 2605,
    "source_corpus_id": 238634210,
    "ref_id": "b6",
    "citation_corpus_id": 56657912,
    "start": 25768,
    "end": 25771,
    "title": "BENCHMARKING NEURAL NETWORK ROBUSTNESS TO COMMON CORRUPTIONS AND PERTURBATIONS",
    "abstract": "In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, IMAGENET-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called IMAGENET-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.",
    "prev": "Real data\n\nIn deep learning, out-of-distribution robustness is often assessed based on a model performance gap between the original data (used for training) and data to which various perturbations are applied.",
    "curr": "We focus on two image classification datasets with induced corruptions: MNIST-C [13] and CIFAR-10-C [7,11].",
    "next": "We illustrate an example of a clean MNIST image on Figure 5a along with its corrupted versions after applying motion blur, blur along a random direction (Figure 5b), translate, affine transformation along a random direction (Figure 5c), and zigzag, randomly oriented zigzag over an image (Figure 5d)."
  },
  {
    "index": 2607,
    "source_corpus_id": 210064343,
    "ref_id": "b10",
    "citation_corpus_id": 6706414,
    "start": 2374,
    "end": 2398,
    "title": "Published as a conference paper at ICLR 2015 EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES",
    "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples-inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
    "prev": "Researchers have proposed a variety of defense methods to improve the robustness of neural networks.",
    "curr": "Most of the existing defenses are based on adversarial training (Szegedy et al., 2013;Madry et al., 2017;Goodfellow et al., 2015;Huang et al., 2015;Athalye et al., 2018;Ding et al., 2020).",
    "next": "During training, these methods first learn on-the-fly adversarial examples of the inputs with multiple attack iterations and then update model parameters using these perturbed samples together with the original labels."
  },
  {
    "index": 2608,
    "source_corpus_id": 221340643,
    "ref_id": "b21",
    "citation_corpus_id": 6488690,
    "start": 2475,
    "end": 2500,
    "title": "Injecting Logical Background Knowledge into Embeddings for Relation Extraction",
    "abstract": "Matrix factorization approaches to relation extraction provide several attractive features: they support distant supervision, handle open schemas, and leverage unlabeled data. Unfortunately, these methods share a shortcoming with all other distantly supervised approaches: they cannot learn to extract target relations without existing data in the knowledge base, and likewise, these models are inaccurate for relations with sparse data. Rule-based extractors, on the other hand, can be easily extended to novel relations and improved for existing but inaccurate relations, through first-order formulae that capture auxiliary domain knowledge. However, usually a large set of such formulae is necessary to achieve generalization.In this paper, we introduce a paradigm for learning low-dimensional embeddings of entity-pairs and relations that combine the advantages of matrix factorization with first-order logic domain knowledge. We introduce simple approaches for estimating such embeddings, as well as a novel training algorithm to jointly optimize over factual and first-order logic information. Our results show that this method is able to learn accurate extractors with little or no distant supervision alignments, while at the same time generalizing to textual patterns that do not appear in the formulae.",
    "prev": "Introduction\n\nNeural networks have become the de-facto standard for solving perceptual tasks over low level representations, such as pixels in an image or audio signals.",
    "curr": "Recent research has also explored their application for solving symbolic reasoning tasks, requiring higher level inferences, such as neural theorem proving [Rocktäschel et al., 2015, Evans and Grefenstette, 2018, Minervini et al., 2020, and playing blocks world [Dong et al., 2019].",
    "next": "The advantage of neural models for these tasks is that it will create a unified, end-to-end trainable representation for integrated AI systems that combine perceptual and high level reasoning."
  },
  {
    "index": 2609,
    "source_corpus_id": 3333039,
    "ref_id": "b48",
    "citation_corpus_id": 6212000,
    "start": 15524,
    "end": 15545,
    "title": "UNDERSTANDING DEEP LEARNING REQUIRES RE- THINKING GENERALIZATION",
    "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.",
    "prev": "Interestingly, this method allowed Imagenet training in an under-specified regime, where there are more training samples than number of parameters.",
    "curr": "This is an unconventional regime for modern deep networks, which are usually over-specified to have many more parameters than training samples (Zhang et al., 2017a).",
    "next": "Moreover, many recent theoretical results related to neural network training Xie et al., 2016;Safran & Shamir, 2016;Soltanolkotabi et al., 2017;Soudry & Carmon, 2016) and even generalization (Gunasekar et al., 2017;Advani & Saxe, 2017;Wilson et al., 2017) usually assume over-specification."
  },
  {
    "index": 2610,
    "source_corpus_id": 258967501,
    "ref_id": "b39",
    "citation_corpus_id": 55481903,
    "start": 10404,
    "end": 10408,
    "title": "AN EMPIRICAL STUDY OF EXAMPLE FORGETTING DURING DEEP NEURAL NETWORK LEARNING",
    "abstract": "Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks. Our goal is to understand whether a related phenomenon occurs when data does not undergo a clear distributional shift. We define a \"forgetting event\" to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning. Across several benchmark data sets, we find that: (i) certain examples are forgotten with high frequency, and some not at all; (ii) a data set's (un)forgettable examples generalize across neural architectures; and (iii) based on forgetting dynamics, a significant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance.",
    "prev": "Loss and error based methods operate on a similar principle.",
    "curr": "Forgetting Events [54], GraNd, EL2N [46], and others [3,11,33,39] are examples of these methods.",
    "next": "Other techniques for subset selection, such as CRAIG [38] and GradMatch [26], focus on gradient matching, where the goal is to construct a subset of examples such that a weighted sum of the model gradients on the subset matches the overall gradient on the full dataset."
  },
  {
    "index": 2611,
    "source_corpus_id": 240354406,
    "ref_id": "b24",
    "citation_corpus_id": 3292002,
    "start": 2695,
    "end": 2719,
    "title": "GRAPH ATTENTION NETWORKS",
    "abstract": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-theart results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a proteinprotein interaction dataset (wherein test graphs remain unseen during training).",
    "prev": "INTRODUCTION\n\nThe ubiquity of graph-structured data and its importance in solving various real-world problems such as node and graph classification have made graph-centered machine learning an important research area (Lü & Zhou, 2011;Shervashidze et al., 2011;Zhu, 2005).",
    "curr": "Graph neural networks (GNNs) offer state-of-the-art performance on many graph learning tasks and have by now become a standard methodology in the field (Kipf & Welling, 2017;Hamilton et al., 2017;Velickovic et al., 2018;Chien et al., 2020).",
    "next": "In most such studies, GNNs take graphs with numerical node attributes as inputs and train them with task-specific labels."
  },
  {
    "index": 2616,
    "source_corpus_id": 260164542,
    "ref_id": "b55",
    "citation_corpus_id": 247595075,
    "start": 1841,
    "end": 1844,
    "title": "CROSSBEAM: LEARNING TO SEARCH IN BOTTOM-UP PROGRAM SYNTHESIS",
    "abstract": "Many approaches to program synthesis perform a search within an enormous space of programs to find one that satisfies a given specification. Prior works have used neural models to guide combinatorial search algorithms, but such approaches still explore a huge portion of the search space and quickly become intractable as the size of the desired program increases. To tame the search space blowup, we propose training a neural model to learn a hands-on search policy for bottom-up synthesis, instead of relying on a combinatorial search algorithm. Our approach, called CROSSBEAM, uses the neural model to choose how to combine previouslyexplored programs into new programs, taking into account the search history and partial program executions. Motivated by work in structured prediction on learning to search, CROSSBEAM is trained on-policy using data extracted from its own bottom-up searches on training tasks. We evaluate CROSSBEAM in two very different domains, string manipulation and logic programming. We observe that CROSSBEAM learns to search efficiently, exploring much smaller portions of the program space compared to the state-of-the-art. * Equal contribution. † Equal contribution.",
    "prev": "Introduction\n\nProgram synthesis aims to assist programmers by automatically producing code according to a user's specification of what the code should do [26].",
    "curr": "Program synthesis systems, such as programming by example (PBE) systems, have been effective for tasks such as string manipulation [27,19,55], writing short Java functions [53], and tensor manipulation [54].",
    "next": "Neural program synthesizers, especially those based on large language models [11,3,38], have been particularly successful at generating code functions and blocks across a variety of general-purpose programming languages."
  },
  {
    "index": 2620,
    "source_corpus_id": 252682977,
    "ref_id": "b1",
    "citation_corpus_id": 212644628,
    "start": 16453,
    "end": 16475,
    "title": "LAGRANGIAN NEURAL NETWORKS",
    "abstract": "Accurate models of the world are built upon notions of its underlying symmetries. In physics, these symmetries correspond to conservation laws, such as for energy and momentum. Yet even though neural network models see increasing use in the physical sciences, they struggle to learn these symmetries. In this paper, we propose Lagrangian Neural Networks (LNNs), which can parameterize arbitrary Lagrangians using neural networks. In contrast to models that learn Hamiltonians, LNNs do not require canonical coordinates, and thus perform well in situations where canonical momenta are unknown or difficult to compute. Unlike previous approaches, our method does not restrict the functional form of learned energies and will produce energy-conserving models for a variety of tasks. We test our approach on a double pendulum and a relativistic particle, demonstrating energy conservation where a baseline approach incurs dissipation and modeling relativity without canonical coordinates where a Hamiltonian approach fails. Finally, we show how this model can be applied to graphs and continuous systems using a Lagrangian Graph Network, and demonstrate it on the 1D wave equation. * Also affiliated with Princeton University",
    "prev": "The inverse operation has a computational cost of O(K 3 ), which is not costly if the number K of first integrals is small.",
    "curr": "Many previous models also need the inverse operation to satisfy the constraints and geometric structures, such as Lagrangian neural network (LNN) (Cranmer et al., 2020), neural symplectic form (Chen et al., 2021), and CHNN (Finzi et al., 2020b).",
    "next": "DISCRETE FINDE: DISCRETE-TIME DERIVATIVE PROJECTION METHOD\n\nThe cFINDE is still an ODE and hence needs to be solved using a numerical integrator, which causes the temporal discretization errors in the first integrals."
  },
  {
    "index": 2621,
    "source_corpus_id": 3687922,
    "ref_id": "b31",
    "citation_corpus_id": 13298214,
    "start": 1538,
    "end": 1560,
    "title": "LEARNING TO NAVIGATE IN COMPLEX ENVIRONMENTS",
    "abstract": "Learning to navigate in complex environments with dynamic elements is an important milestone in developing AI agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks leveraging multimodal sensory inputs. In particular we consider jointly learning the goal-driven reinforcement learning problem with auxiliary depth prediction and loop closure classification tasks. This approach can learn to navigate from raw sensory input in complicated 3D mazes, approaching human-level performance even under conditions where the goal location changes frequently. We provide detailed analysis of the agent behaviour 1 , its ability to localise, and its network activity dynamics, showing that the agent implicitly learns key navigation abilities.",
    "prev": "INTRODUCTION\n\nDeep learning (DL) has recently been used as an efficient approach to learning navigation in complex three-dimensional environments.",
    "curr": "DL-based approaches to navigation can be broadly divided into three classes: purely reactive Zhu et al., 2017), based on unstructured general-purpose memory such as LSTM (Mnih et al., 2016;Mirowski et al., 2017), and employing a navigation-specific memory structure based on a metric map (Parisotto & Salakhutdinov, 2018;.",
    "next": "However, extensive evidence from psychology suggests that when traversing environments, animals do not rely strongly on metric representations (Gillner & Mallot, 1998;Wang & Spelke, 2002;Foo et al., 2005)."
  },
  {
    "index": 2623,
    "source_corpus_id": 220363813,
    "ref_id": "b10",
    "citation_corpus_id": 46928091,
    "start": 2253,
    "end": 2256,
    "title": "GAMEPAD: A LEARNING ENVIRONMENT FOR THEO- REM PROVING",
    "abstract": "In this paper, we introduce a system called GamePad that can be used to explore the application of machine learning methods to theorem proving in the Coq proof assistant. Interactive theorem provers such as Coq enable users to construct machine-checkable proofs in a step-by-step manner. Hence, they provide an opportunity to explore theorem proving with human supervision. We use GamePad to synthesize proofs for a simple algebraic rewrite problem and train baseline models for a formalization of the Feit-Thompson theorem. We address position evaluation (i.e., predict the number of proof steps left) and tactic prediction (i.e., predict the next proof step) tasks, which arise naturally in tactic-based theorem proving.",
    "prev": "Introduction\n\nAdvances in theorem proving can catalyze developments in fields including formal mathematics [22], software verification [5], and hardware design [16].",
    "curr": "Following its recent success across other application domains, machine learning has significantly improved the performance of theorem proving agents [1,3,9,11,12,15,20,21,30,32,37].",
    "next": "One factor that makes theorem proving an especially challenging domain for ML is data sparsity."
  },
  {
    "index": 2624,
    "source_corpus_id": 247618909,
    "ref_id": "b27",
    "citation_corpus_id": 202888986,
    "start": 8977,
    "end": 8995,
    "title": "Published as a conference paper at ICLR 2020 ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS",
    "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameterreduction techniques to lower memory consumption and increase the training speed of BERT (Devlin et al., 2019). Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.Published as a conference paper at ICLR 2020 These solutions address the memory limitation problem, but not the communication overhead. In this paper, we address all of the aforementioned problems, by designing A Lite BERT (ALBERT) architecture that has significantly fewer parameters than a traditional BERT architecture.ALBERT incorporates two parameter reduction techniques that lift the major obstacles in scaling pre-trained models. The first one is a factorized embedding parameterization. By decomposing the large vocabulary embedding matrix into two small matrices, we separate the size of the hidden layers from the size of vocabulary embedding. This separation makes it easier to grow the hidden size without significantly increasing the parameter size of the vocabulary embeddings. The second technique is cross-layer parameter sharing. This technique prevents the parameter from growing with the depth of the network. Both techniques significantly reduce the number of parameters for BERT without seriously hurting performance, thus improving parameter-efficiency. An ALBERT configuration similar to BERT-large has 18x fewer parameters and can be trained about 1.7x faster. The parameter reduction techniques also act as a form of regularization that stabilizes the training and helps with generalization.To further improve the performance of ALBERT, we also introduce a self-supervised loss for sentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed to address the ineffectiveness  of the next sentence prediction (NSP) loss proposed in the original BERT.As a result of these design decisions, we are able to scale up to much larger ALBERT configurations that still have fewer parameters than BERT-large but achieve significantly better performance. We establish new state-of-the-art results on the well-known GLUE, SQuAD, and RACE benchmarks for natural language understanding. Specifically, we push the RACE accuracy to 89.4%, the GLUE benchmark to 89.4, and the F1 score of SQuAD 2.0 to 92.2.",
    "prev": "In the field of natural language processing (NLP), two dominant self-supervised language modeling paradigms are Masked Language Modeling, such as BERT (Devlin et al., 2018), and GPT-style autoregressive pre-training (Brown et al., 2020;Radford & Narasimhan, 2018;Radford et al., 2019).",
    "curr": "Given a sentence, BERT and its variants (Lan et al., 2020;Liu et al., 2019) pre-train transformer encoders by predicting randomly masked out input words, referred to as tokens.",
    "next": "Such frameworks model the bidirectional (contextual) dependencies between the visible tokens and the corrupted/masked tokens."
  },
  {
    "index": 2626,
    "source_corpus_id": 253734944,
    "ref_id": "b14",
    "citation_corpus_id": 245005710,
    "start": 4831,
    "end": 4852,
    "title": "Extending the WILDS Benchmark for Unsupervised Adaptation",
    "abstract": "Machine learning systems deployed in the wild are often trained on a source distribution but deployed on a different target distribution. Unlabeled data can be a powerful point of leverage for mitigating these distribution shifts, as it is frequently much more available than labeled data and can often be obtained from distributions beyond the source distribution as well. However, existing distribution shift benchmarks with unlabeled data do not reflect the breadth of scenarios that arise in real-world applications. In this work, we present the Wilds 2.0 update, which extends 8 of the 10 datasets in the Wilds benchmark of distribution shifts to include curated unlabeled data that would be realistically obtainable in deployment. These datasets span a wide range of applications (from histology to wildlife conservation), tasks (classification, regression, and detection), and modalities (photos, satellite images, microscope slides, text, molecular graphs). The update maintains consistency with the original Wilds benchmark by using identical labeled training, validation, and test sets, as well as the evaluation metrics. On these datasets, we systematically benchmark state-of-the-art methods that leverage unlabeled data, including domain-invariant, self-training, and self-supervised methods, and show that their success on Wilds is limited. To facilitate method development and evaluation, we provide an open-source package that automates data loading and contains all of the model architectures and methods used in this paper. Code and leaderboards are available at https://wilds.stanford.edu. * . These authors contributed equally to this work.1. We omitted Py150-wilds, as code completion data is always labeled by nature of the task, and RxRx1-wilds, as unlabeled data for that genetic perturbation task is not typically available.",
    "prev": "(See Proposition 4.1 for a formal statement.)",
    "curr": "The impossibility result suggests that any positive results on the extrapolation of nonlinear models require more fine-grained structures on the relationship between P and Q (which are common in practice [Koh et al., 2021, Sagawa et al., 2022) as well as the function class F. The structure in the domain shift between P and Q may also need to be compatible with the assumption on the function class F. This paper makes some first steps towards proving certain family of nonlinear models can extrapolate to a new test domain with structured shift.",
    "next": "We consider a setting where the joint distribution of the data does not have much overlap across P and Q (and thus bounded density ratio assumption does not hold), whereas the marginal distributions for each coordinate of the data does overlap."
  },
  {
    "index": 2627,
    "source_corpus_id": 233004517,
    "ref_id": "b6",
    "citation_corpus_id": 604334,
    "start": 3112,
    "end": 3133,
    "title": "Intriguing properties of neural networks",
    "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
    "prev": "(2019); Hendrycks et al.",
    "curr": "(2019) show that the accuracy of IMAGENET models is severely impacted by changes in the data collection process, while imperceptible deviations to the input, called adversarial perturbations, can cause neural networks to make incorrect predictions with high confidence (Carlini & Wagner, 2017a;b;Goodfellow et al., 2015;Kurakin et al., 2016;Szegedy et al., 2014).",
    "next": "Methods to counteract such effects, which mainly consist of using random or adversarially-chosen data augmentations, struggle."
  },
  {
    "index": 2630,
    "source_corpus_id": 12167053,
    "ref_id": "b33",
    "citation_corpus_id": 604334,
    "start": 1504,
    "end": 1526,
    "title": "Intriguing properties of neural networks",
    "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
    "prev": "We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.",
    "curr": "Code is available at\n\nINTRODUCTION\n\nAdversarial examples are examples that are created by making small perturbations to the input designed to significantly increase the loss incurred by a machine learning model (Szegedy et al., 2014;Goodfellow et al., 2015).",
    "next": "Several models, including state of the art convolutional neural networks, lack the ability to classify adversarial examples correctly, sometimes even when the adversarial perturbation is constrained to be so small that a human observer cannot perceive it."
  },
  {
    "index": 2637,
    "source_corpus_id": 252668316,
    "ref_id": "b10",
    "citation_corpus_id": 209531600,
    "start": 6588,
    "end": 6607,
    "title": "PAC CONFIDENCE SETS FOR DEEP NEURAL NET- WORKS VIA CALIBRATED PREDICTION",
    "abstract": "We propose an algorithm combining calibrated prediction and generalization bounds from learning theory to construct confidence sets for deep neural networks with PAC guarantees-i.e., the confidence set for a given input contains the true label with high probability. We demonstrate how our approach can be used to construct PAC confidence sets on ResNet for ImageNet, a visual object tracking model, and a dynamics model for the half-cheetah reinforcement learning problem. 1 1 Our code is available at https://github.com/sangdon/PAC-confidence-set. . Safe model-based reinforcement learning with stability guarantees. In Advances in neural information processing systems, pp. 908-918, 2017.Marko Bohanec and Vladislav Rajkovic. Knowledge acquisition and explanation for multi-attribute decision making.",
    "prev": "That is, we have:\nPr (x,y)∼D [y ∈ T f h (x)|g(x) = 1, f (x) = τ ] = 1 − α\nIf f is such that its corresponding prediction sets T f h (x) satisfy both group and threshold conditional guarantees simultaneously, then we say that it promises full multivalid coverage.",
    "curr": "2 with a small sample bias correction\n\n\nOur Results\n\nWe design, analyze, and empirically evaluate two algorithms: one for giving group conditional guarantees for an arbitrary collection of groups G, and the other for giving full multivalid coverage guarantees for an arbitrary collection of groups G. We give PAC-style guarantees [Park et al., 2019], which means that with high probability over the draw of the calibration set, our deployed prediction sets have their desired coverage properties on the underlying distribution.",
    "next": "Thus our algorithms also offer \"training-conditional coverage\" in the sense of Bian and Barber [2022]."
  },
  {
    "index": 2639,
    "source_corpus_id": 209444460,
    "ref_id": "b18",
    "citation_corpus_id": 14153811,
    "start": 17974,
    "end": 18004,
    "title": "V-Measure: A conditional entropy-based external cluster evaluation measure",
    "abstract": "We present V-measure, an external entropybased cluster evaluation measure.Vmeasure provides an elegant solution to many problems that affect previously defined cluster evaluation measures including 1) dependence on clustering algorithm or data set, 2) the \"problem of matching\", where the clustering of only a portion of data points are evaluated and 3) accurate evaluation and combination of two desirable aspects of clustering, homogeneity and completeness. We compare V-measure to a number of popular cluster evaluation measures and demonstrate that it satisfies several desirable properties of clustering solutions, using simulated clustering results. Finally, we use V-measure to evaluate two clustering tasks: document clustering and pitch accent type clustering.",
    "prev": "All metrics are upper-bounded by 1 and the higher the score the better.",
    "curr": "Homogeneity (H), Completeness (C) and V-measure score (V) (Rosenberg & Hirschberg, 2007).",
    "next": "Supervised metrics."
  },
  {
    "index": 2640,
    "source_corpus_id": 263605817,
    "ref_id": "b48",
    "citation_corpus_id": 244117525,
    "start": 2519,
    "end": 2537,
    "title": "FILIP: FINE-GRAINED INTERACTIVE LANGUAGE- IMAGE PRE-TRAINING",
    "abstract": "Unsupervised large-scale vision-language pre-training has shown promising advances on various downstream tasks. Existing methods often model the crossmodal interaction either via the similarity of the global feature of each modality which misses sufficient information, or finer-grained interactions using cross/selfattention upon visual and textual tokens. However, cross/self-attention suffers from inferior efficiency in both training and inference. In this paper, we introduce a large-scale Fine-grained Interactive Language-Image Pre-training (FILIP) to achieve finer-level alignment through a cross-modal late interaction mechanism, which uses a token-wise maximum similarity between visual and textual tokens to guide the contrastive objective. FILIP successfully leverages the finergrained expressiveness between image patches and textual words by modifying only contrastive loss, while simultaneously gaining the ability to pre-compute image and text representations offline at inference, keeping both large-scale training and inference efficient. Furthermore, we construct a new large-scale image-text pair dataset called FILIP300M for pre-training. Experiments show that FILIP achieves state-of-the-art performance on multiple downstream vision-language tasks including zero-shot image classification and image-text retrieval. The visualization on word-patch alignment further shows that FILIP can learn meaningful fine-grained features with promising localization ability. * Equal contribution † Corresponding authors: xu.hang@huawei.com,",
    "prev": "r state-of-the-art methods on benchmark datasets.",
    "curr": "Introduction\n\nMulti-modal learning (Ngiam et al., 2011) integrates information from a variety of data types, resulting in AI systems that are both robust and precise.Recently, CLIP (Radford et al., 2021) emerged as a milestone work that leverages vision-language contrastive pretraining to jointly learn image and text embeddings, using the vast amounts of image-text data available on the web.During the training process, CLIP considers image-text data that appear together as positive pairs and other combinations as negative pairs.The goal is to maximize the embedding similarity for the positive pairs while minimizing it for the negative pairs.Remarkably, this approach has achieved significant success in zero-shot transfer (Lei Ba et al., 2015), indicating the model's ability to handle a great variety of tasks without prior exposure to any of their training data.Inspired by CLIP's groundbreaking zero-shot capabilities, subsequent studies (Yao et al., 2022;Li et al., 2022;Mu et al., 2022;Goel et al., 2022;Zhai et al., 2022;Alayrac et al., 2022) emerged with the primary objective of further enhancing CLIP's zero-shot performance.Despite the empirical success of CLIP in zero-shot transfer, the theoretical understanding of how it works remains elusive.An intriguing inquiry is thus: How does CLIP learn representations that are transferable to the various downstream tasks?",
    "next": "⏟\n\n\nShared Features Self Standing Features\n\nThe stop sign is located on the street corner."
  },
  {
    "index": 2641,
    "source_corpus_id": 258352594,
    "ref_id": "b16",
    "citation_corpus_id": 227746078,
    "start": 29608,
    "end": 29630,
    "title": "DISTILLING KNOWLEDGE FROM READER TO RETRIEVER FOR QUESTION ANSWERING",
    "abstract": "The task of information retrieval is an important component of many natural language processing systems, such as open domain question answering. While traditional methods were based on hand-crafted features, continuous representations based on neural networks recently obtained competitive results. A challenge of using such methods is to obtain supervised data to train the retriever model, corresponding to pairs of query and support documents. In this paper, we propose a technique to learn retriever models for downstream tasks, inspired by knowledge distillation, and which does not require annotated pairs of query and documents. Our approach leverages attention scores of a reader model, used to solve the task based on retrieved documents, to obtain synthetic labels for the retriever. We evaluate our method on question answering, obtaining state-of-the-art results.",
    "prev": "Sub-object part-centric memory representations would permit fine-grained retrieval of visual memory scenes.",
    "curr": "We further plan to explore alternative supervision for the retriever module inspired by works in the language domain (Izacard et al., 2022;Izacard & Grave, 2021).",
    "next": "(ii) Scaling Analogical Networks to segmentation of complete, multi-object 3D scenes in realistic home environments requires scaling up the size of memory collection."
  },
  {
    "index": 2642,
    "source_corpus_id": 53113673,
    "ref_id": "b20",
    "citation_corpus_id": 8535316,
    "start": 4483,
    "end": 4502,
    "title": "BI-DIRECTIONAL ATTENTION FLOW FOR MACHINE COMPREHENSION",
    "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bidirectional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.",
    "prev": "The three participant entities also get destroyed in the process, which is captured in the graph by pointing to a special Nowhere node.",
    "curr": "(MRC) mechanism (Seo et al., 2017a;Xiong et al., 2017;Chen et al., 2017;Yu et al., 2018, inter alia), which queries for entities and their states at each time step.",
    "next": "We leverage MRC mechanisms because they have proven adept at extracting text spans that answer entity-centric questions (Levy et al., 2017)."
  },
  {
    "index": 2643,
    "source_corpus_id": 253018859,
    "ref_id": "b20",
    "citation_corpus_id": 56657912,
    "start": 2473,
    "end": 2502,
    "title": "BENCHMARKING NEURAL NETWORK ROBUSTNESS TO COMMON CORRUPTIONS AND PERTURBATIONS",
    "abstract": "In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, IMAGENET-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called IMAGENET-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.",
    "prev": "Intuitively, fine-tuning more parameters on a small target dataset can cause information learned during pre-training to be forgotten, and the relevant information depends on the type of shift.",
    "curr": "INTRODUCTION\n\nWhile deep neural networks have achieved impressive results in many domains, they are often brittle to even small distribution shifts between the source and target domains (Recht et al., 2019;Hendrycks & Dietterich, 2019;.",
    "next": "While many approaches to robustness attempt to directly generalize to the target distribution after training on source data (Peters et al., 2016;Arjovsky et al., 2019), an alternative approach is to fine-tune on a small amount of labeled target datapoints."
  },
  {
    "index": 2644,
    "source_corpus_id": 238419436,
    "ref_id": "b14",
    "citation_corpus_id": 5834589,
    "start": 3038,
    "end": 3059,
    "title": "ON LARGE-BATCH TRAINING FOR DEEP LEARNING: GENERALIZATION GAP AND SHARP MINIMA",
    "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say 32-512 data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions-and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap. Published as a conference paper at ICLR 2017 These methods minimize the objective function f by iteratively taking steps of the form: J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. . Weak sharp minima and penalty functions in mathematical programming. PhD thesis, University of Cambridge, 1988.Michael P Friedlander and Mark Schmidt. Hybrid deterministic-stochastic methods for data fitting.",
    "prev": "Generalization is arguably the most fundamental and yet mysterious aspect of deep learning.",
    "curr": "Several studies have been conducted to better understand the generalization of DNNs and to train DNNs that generalize well across the natural distribution (Keskar et al., 2017;Neyshabur et al., 2017;Chaudhari et al., 2019;Wu et al., 2020;Foret et al., 2020;.",
    "next": "For example, Keskar et al."
  },
  {
    "index": 2649,
    "source_corpus_id": 248239638,
    "ref_id": "b6",
    "citation_corpus_id": 52962991,
    "start": 2050,
    "end": 2069,
    "title": "EXPLAINING IMAGE CLASSIFIERS BY COUNTERFAC- TUAL GENERATION",
    "abstract": "When a black-box classifier processes an input to render a prediction, which input features are relevant and why? We propose to answer this question by efficiently marginalizing over the universe of plausible alternative values for a subset of features by conditioning a generative model of the input distribution on the remaining features. In contrast with recent approaches that compute alternative feature values ad-hoc-generating counterfactual inputs far from the natural data distributionour model-agnostic method produces realistic explanations, generating plausible inputs that either preserve or alter the classification confidence. When applied to image classification, our method produces more compact and relevant per-feature saliency assignment, with fewer artifacts compared to previous methods.",
    "prev": "Other approaches aim to highlight local features to debug individual model predictions (Simonyan et al., 2013;Dhurandhar et al., 2018;Ribeiro et al., 2016a;Goyal et al., 2019).",
    "curr": "A common theme in these methods is to compare the behavior of the model with and without certain individual features (Ribeiro et al., 2016a;Goyal et al., 2019;Fong & Vedaldi, 2017;Dabkowski & Gal, 2017;Zintgraf et al., 2017;Dhurandhar et al., 2018;Chang et al., 2019).",
    "next": "For example, interpretability methods such as LIME (Ribeiro et al., 2016b) and integrated gradients (Sundararajan et al., 2017) use the predictions when certain features are removed from the input to attribute different regions of the input to the decision of the model."
  },
  {
    "index": 2651,
    "source_corpus_id": 13239389,
    "ref_id": "b7",
    "citation_corpus_id": 2134321,
    "start": 2326,
    "end": 2344,
    "title": "Published as a conference paper at ICLR 2016 DEEP COMPRESSION: COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING",
    "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce \"deep compression\", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35× to 49× without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9× to 13×; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35×, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49× from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3× to 4× layerwise speedup and 3× to 7× better energy efficiency.",
    "prev": "To alleviate this problem, a number of approaches have been recently proposed.",
    "curr": "One attempt first trains a neural network and then compresses it (Han et al., 2016;Kim et al., 2016).",
    "next": "Instead of this two-step approach, it is more desirable to train and compress the network simultaneously."
  },
  {
    "index": 2652,
    "source_corpus_id": 254044324,
    "ref_id": "b36",
    "citation_corpus_id": 227209335,
    "start": 1788,
    "end": 1807,
    "title": "SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS",
    "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reversetime SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in diffusion probabilistic modeling and scorebased generative modeling, and allows for new sampling procedures. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, which enables exact likelihood computation, and improved sampling efficiency. In addition, our framework enables conditional generation with an unconditional model, as we demonstrate with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high fidelity generation of 1024ˆ1024 images for the first time from a score-based generative model.",
    "prev": "Extensive experiments indicate that our proposed method can perform comparably to the state-of-the-art solutions in various generation tasks.",
    "curr": "INTRODUCTION\n\nDiffusion models (Ho et al., 2020;Song et al., 2021b) have garnered significant interest on various high quality conditional image generation tasks, such as image super-resolution (Rombach et al., 2022), image inpainting (Lugmayr et al., 2022), image editing (Avrahami et al., 2022), image translation (Saharia et al., 2022a), among others.",
    "next": "Concurrently, the Vector Quantized (VQ) models have also achieved rapid advances in image generations, especially on cross-modal tasks, examples in- clude text-to-image , sketch-to-image (Esser et al., 2021b), image-to-video ."
  },
  {
    "index": 2653,
    "source_corpus_id": 235624202,
    "ref_id": "b40",
    "citation_corpus_id": 3626819,
    "start": 1906,
    "end": 1926,
    "title": "Deep contextualized word representations",
    "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
    "prev": "We believe this work paves the way for highly performant token-free models that are trained completely end-to-end.",
    "curr": "INTRODUCTION\n\nNeural networks have achieved tremendous success in natural language processing (NLP) by replacing feature-engineered models with stacks of functions that are learned end-to-end from vast amounts of data (Mikolov et al., 2013;Peters et al., 2018;Howard and Ruder, 2018).",
    "next": "The single component of the traditional NLP pipeline (Manning and Schütze, 1999) that has so far resisted gradient-based learning is tokenization, which is commonly applied as a pre-processing step."
  },
  {
    "index": 2656,
    "source_corpus_id": 67856101,
    "ref_id": "b7",
    "citation_corpus_id": 6628106,
    "start": 2724,
    "end": 2747,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "To address this problem, recent work has proposed a variety of adaptive methods that scale the gradient by square roots of some form of the average of the squared values of past gradients.",
    "curr": "Examples of such methods include ADAM (Kingma & Lei Ba, 2015), ADAGRAD (Duchi et al., 2011) and RMSPROP (Tieleman & Hinton, 2012).",
    "next": "ADAM in particular has become the default algorithm leveraged across many deep learning frameworks due to its rapid training speed (Wilson et al., 2017)."
  },
  {
    "index": 2657,
    "source_corpus_id": 250526649,
    "ref_id": "b7",
    "citation_corpus_id": 215828350,
    "start": 3221,
    "end": 3242,
    "title": "The State and Fate of Linguistic Diversity and Inclusion in the NLP World",
    "abstract": "Language technologies contribute to promoting multilingualism and linguistic diversity around the world. However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time. Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the \"language agnostic\" status of current models and systems. Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind. * Authors contributed equally to the work. https://microsoft.github.io/linguisticdiversity",
    "prev": "pretrained language models (PLMs) like BERT (Devlin et al., 2019), GPT-3 (Brown et al., 2020), and XLM-R (Conneau et al., 2020); large unlabelled datasets; such as C4 (Raffel et al., 2020), The Pile (Gao et al., 2020); and large-scale computing power (Hirschberg & Manning, 2015).",
    "curr": "Despite this progress, these models only cover a fraction of the world's languages, with large inequalities in performance (Pires et al., 2019;Lauscher et al., 2020), and the majority of languages are falling behind English (Joshi et al., 2020b;Bugliarello et al., 2022).",
    "next": "Even within English, these models struggle when tasked with processing noisy inputs (Sun et al., 2020;Eger & Benz, 2020)."
  },
  {
    "index": 2659,
    "source_corpus_id": 246823110,
    "ref_id": "b20",
    "citation_corpus_id": 3502468,
    "start": 4680,
    "end": 4702,
    "title": "FEARNET: BRAIN-INSPIRED MODEL FOR INCREMENTAL LEARNING",
    "abstract": "Incremental class learning involves sequentially learning classes in bursts of examples from the same class. This violates the assumptions that underlie methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting. Arguably, the best method for incremental class learning is iCaRL, but it requires storing training examples for each class, making it challenging to scale. Here, we propose FearNet for incremental class learning. Fear-Net is a generative model that does not store previous examples, making it memory efficient. FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex. Memory consolidation is inspired by mechanisms that occur during sleep. FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall. FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.",
    "prev": "Only a single, entirely self-contained, encoding-decoding network is needed: for a new data class X new , a new LDR memory Z new is incrementally learned as a minimax game between the encoder and decoder subject to the constraint that old memory of past classes Z old is intact through the closed-loop transcription (or replay): Z old ≈Ẑ old = f (g(Z old )).",
    "curr": "representations of the old classes, as well as the associated encoding and decoding mappings (Kemker & Kanan, 2018).",
    "next": "(2) The second is that it is self-contained."
  },
  {
    "index": 2660,
    "source_corpus_id": 222163237,
    "ref_id": "b12",
    "citation_corpus_id": 5176587,
    "start": 16371,
    "end": 16395,
    "title": "Published as a conference paper at ICLR 2018 NOISY NETWORKS FOR EXPLORATION",
    "abstract": "We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent's policy can be used to aid efficient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights. NoisyNet is straightforward to implement and adds little computational overhead. We find that replacing the conventional exploration heuristics for A3C, DQN and Dueling agents (entropy reward and -greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance. * Equal contribution. , et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015. . Deep exploration via randomized value functions. arXiv preprint arXiv:1703.07608, 2017.Georg Ostrovski, Marc G Bellemare, Aaron van den Oord, and Remi Munos. Count-based exploration with neural density models. arXiv preprint arXiv:1703.01310, 2017.Pierre-Yves Oudeyer and Frederic Kaplan. What is intrinsic motivation? A typology of computational approaches. Frontiers in neurorobotics, 1, 2007.",
    "prev": "When using dueling DQN (Wang et al., 2016), g o concatenates the outputs of the first layers of the value and advantage heads.",
    "curr": "When these layers are noisy (Fortunato et al., 2018), g o does not use the noisy parameters.",
    "next": "Finally, we parameterize the predictor q as a linear layer."
  },
  {
    "index": 2663,
    "source_corpus_id": 235742951,
    "ref_id": "b0",
    "citation_corpus_id": 231592776,
    "start": 32456,
    "end": 32479,
    "title": "CONTRASTIVE BEHAVIORAL SIMILARITY EMBEDDINGS FOR GENERALIZATION IN REINFORCEMENT LEARNING",
    "abstract": "Reinforcement learning methods trained on few environments rarely learn policies that generalize to unseen environments. To improve generalization, we incorporate the inherent sequential structure in reinforcement learning into the representation learning process. This approach is orthogonal to recent approaches, which rarely exploit this structure explicitly. Specifically, we introduce a theoretically motivated policy similarity metric (PSM) for measuring behavioral similarity between states. PSM assigns high similarity to states for which the optimal policies in those states as well as in future states are similar. We also present a contrastive representation learning procedure to embed any state similarity metric, which we instantiate with PSM to obtain policy similarity embeddings (PSEs 1 ). We demonstrate that PSEs improve generalization on diverse benchmarks, including LQR with spurious correlations, a jumping task from pixels, and Distracting DM Control Suite. Source code would be made available at agarwl.github.io/pse.",
    "prev": "Baselines In the MDP setting, we compare AdaRL with CAVIA (Zintgraf et al., 2019) and PEARL (Rakelly et al., 2019).",
    "curr": "In the POMDP setting, we compare with PNN (Rusu et al., 2016), PSM (Agarwal et al., 2021a) and MQL (Fakoor et al., 2020).",
    "next": "We also compare with AdaRL*, a version of AdaRL that does not learn the binary masks c · )· and therefore does not use any structural information."
  },
  {
    "index": 2664,
    "source_corpus_id": 947451,
    "ref_id": "b24",
    "citation_corpus_id": 11758569,
    "start": 2676,
    "end": 2680,
    "title": "UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks -demonstrating their applicability as general image representations.arXiv:1511.06434v2 [cs.LG] 7 Jan 2016Under review as a conference paper at ICLR 2016• We show that the generators have interesting vector arithmetic properties allowing for easy manipulation of many semantic qualities of generated samples.",
    "prev": "Since their introduction, GANs have attracted widespread attention for their ability to produce high-fidelity images when trained on corpora of natural images.",
    "curr": "Several papers have since advanced these capabilities through architectural improvements [25] and modifications to the training scheme [3,2,41,4].",
    "next": "Another line of research addresses how to train GANs to produce class-conditional output."
  },
  {
    "index": 2665,
    "source_corpus_id": 249240346,
    "ref_id": "b78",
    "citation_corpus_id": 247451000,
    "start": 4857,
    "end": 4877,
    "title": "DEEP LEARNING WITHOUT SHORTCUTS: SHAPING THE KERNEL WITH TAILORED RECTIFIERS",
    "abstract": "Training very deep neural networks is still an extremely challenging task. The common solution is to use shortcut connections and normalization layers, which are both crucial ingredients in the popular ResNet architecture. However, there is strong evidence to suggest that ResNets behave more like ensembles of shallower networks than truly deep ones. Recently, it was shown that deep vanilla networks (i.e. networks without normalization layers or shortcut connections) can be trained as fast as ResNets by applying certain transformations to their activation functions. However, this method (called Deep Kernel Shaping) isn't fully compatible with ReLUs, and produces networks that overfit significantly more than ResNets on ImageNet. In this work, we rectify this situation by developing a new type of transformation that is fully compatible with a variant of ReLUs -Leaky ReLUs. We show in experiments that our method, which introduces negligible extra computational cost, achieves validation accuracies with deep vanilla networks that are competitive with ResNets (of the same width/depth), and significantly higher than those obtained with the Edge of Chaos (EOC) method. And unlike with EOC, the validation accuracies we obtain do not get worse with depth.",
    "prev": "Figure 1 illustrates performance on one of the targets in QM9.",
    "curr": "• We make improvements to a common GNN, in particular showing how to apply Tailored Activation Transformation (TAT) (Zhang et al., 2022) to Graph Network Simulators (GNS) , which is complementary to pre-training and further boosts performance.",
    "next": "• We analyze the benefits of pre-training by gaining insights into the effects of dataset size, model size and architecture, and the relationship between the upstream and downstream datasets."
  },
  {
    "index": 2666,
    "source_corpus_id": 220424770,
    "ref_id": "b44",
    "citation_corpus_id": 3507990,
    "start": 7716,
    "end": 7720,
    "title": "Published as a conference paper at ICLR 2018 META-LEARNING FOR SEMI-SUPERVISED FEW-SHOT CLASSIFICATION",
    "abstract": "In few-shot classification, we are interested in learning algorithms that train a classifier from only a handful of labeled examples. Recent progress in few-shot classification has featured meta-learning, in which a parameterized model for a learning algorithm is defined and trained on episodes representing different classification problems, each with a small labeled training set and its corresponding test set. In this work, we advance this few-shot classification paradigm towards a scenario where unlabeled examples are also available within each episode. We consider two situations: one where all unlabeled examples are assumed to belong to the same set of classes as the labeled examples of the episode, as well as the more challenging situation where examples from other distractor classes are also provided. To address this paradigm, we propose novel extensions of Prototypical Networks (Snell et al., 2017) that are augmented with the ability to use unlabeled examples when producing prototypes. These models are trained in an end-to-end way on episodes, to learn to leverage the unlabeled examples successfully. We evaluate these methods on versions of the Omniglot and miniImageNet benchmarks, adapted to this new framework augmented with unlabeled examples. We also propose a new split of ImageNet, consisting of a large set of classes, with a hierarchical structure. Our experiments confirm that our Prototypical Networks can learn to improve their predictions due to unlabeled examples, much like a semi-supervised algorithm would. * Equal contribution. 1 See the following blog post for an overview:",
    "prev": "Incremental FSL [51] Semi-supv.",
    "curr": "FSL [45] MOCA [15] Online Mixture [21] Online Meta [20] Continual FSL* [4] OSAKA* [5] OC-FSL (Ours) * denotes concurrent work.",
    "next": "Related Work\n\nIn this section, we briefly review paradigms that have been used for few-shot learning (FSL) and continual learning (CL)."
  },
  {
    "index": 2667,
    "source_corpus_id": 7451980,
    "ref_id": "b8",
    "citation_corpus_id": 84591,
    "start": 13600,
    "end": 13603,
    "title": "Adversarial Feature Learning",
    "abstract": "The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing generators learn to \"linearize semantics\" in the latent space of such models. Intuitively, such latent spaces may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.",
    "prev": "However they see image as a cause of the neural net weights, and do not use labels.",
    "curr": "BiGAN [9] and ALI [10] improve the standard GAN framework to provide the functionality of learning the mapping from image space to latent space.",
    "next": "In CoGAN [22] the authors learn a joint distribution given samples from marginals by enforcing weight sharing between generators."
  },
  {
    "index": 2671,
    "source_corpus_id": 243832613,
    "ref_id": "b94",
    "citation_corpus_id": 231632937,
    "start": 8443,
    "end": 8463,
    "title": "HIERARCHICAL REINFORCEMENT LEARNING BY DISCOVERING INTRINSIC OPTIONS",
    "abstract": "We propose a hierarchical reinforcement learning method, HIDIO, that can learn task-agnostic options in a self-supervised manner while jointly learning to utilize them to solve sparse-reward tasks. Unlike current hierarchical RL approaches that tend to formulate goal-reaching low-level tasks or pre-define ad hoc lowerlevel policies, HIDIO encourages lower-level option learning that is independent of the task at hand, requiring few assumptions or little knowledge about the task structure. These options are learned through an intrinsic entropy minimization objective conditioned on the option sub-trajectories. The learned options are diverse and task-agnostic. In experiments on sparse-reward robotic manipulation and navigation tasks, HIDIO achieves higher success rates with greater sample efficiency than regular RL baselines and two state-of-the-art hierarchical RL methods. Code available at https://www.github.com/jesbu1/hidio. * Denotes equal contribution.",
    "prev": "In this work, we explore the efficacy of learned skills operating on highdimensional observations for long-horizon control in realistic environments.",
    "curr": "To improve the quality of lower-level policies, recent work in HRL has studied various facets of the problem, including discovery of skills (Konidaris & Barto, 2009;Zhang et al., 2021b;Florensa et al., 2017;Warde-Farley et al., 2018), end-to-end training of both levels (Kulkarni et al., 2016;Tessler et al., 2017) and integrating goal-conditioned behaviors (Ghosh et al., 2019;Nachum et al., 2019).",
    "next": "In this work, we assume that the low-level skills are given, and do not focus on discovering them."
  },
  {
    "index": 2675,
    "source_corpus_id": 249191498,
    "ref_id": "b15",
    "citation_corpus_id": 238583615,
    "start": 2628,
    "end": 2650,
    "title": "AB-INITIO POTENTIAL ENERGY SURFACES BY PAIRING GNNS WITH NEURAL WAVE FUNCTIONS",
    "abstract": "Solving the Schrödinger equation is key to many quantum mechanical properties. However, an analytical solution is only tractable for single-electron systems. Recently, neural networks succeeded at modeling wave functions of many-electron systems. Together with the variational Monte-Carlo (VMC) framework, this led to solutions on par with the best known classical methods. Still, these neural methods require tremendous amounts of computational resources as one has to train a separate model for each molecular geometry. In this work, we combine a Graph Neural Network (GNN) with a neural wave function to simultaneously solve the Schrödinger equation for multiple geometries via VMC. This enables us to model continuous subsets of the potential energy surface with a single training pass. Compared to existing state-of-the-art networks, our Potential Energy Surface Network (PESNet) speeds up training for multiple geometries by up to 40 times while matching or surpassing their accuracy. This may open the path to accurate and orders of magnitude cheaper quantum mechanical calculations.",
    "prev": "Although they lead to accurate energies, training such neural wave functions proved to be computationally intensive .",
    "curr": "To reduce the computational burden, Gao & Günnemann (2022) proposed the potential energy surface network (PESNet) to simultaneously solve many Schrödinger equations, i.e., for different spatial arrangements of the nuclei in R 3 .",
    "next": "They use a GNN to reparametrize the wave function model based on the molecular structure."
  },
  {
    "index": 2686,
    "source_corpus_id": 252565121,
    "ref_id": "b8",
    "citation_corpus_id": 3525710,
    "start": 3010,
    "end": 3030,
    "title": "Published as a conference paper at ICLR 2018 SPHERICAL CNNS",
    "abstract": "Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images. However, a number of problems of recent interest have created a demand for models that can analyze spherical images. Examples include omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling. A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective. In this paper we introduce the building blocks for constructing spherical CNNs. We propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant. The spherical correlation satisfies a generalized Fourier theorem, which allows us to compute it efficiently using a generalized (non-commutative) Fast Fourier Transform (FFT) algorithm. We demonstrate the computational efficiency, numerical accuracy, and effectiveness of spherical CNNs applied to 3D model recognition and atomization energy regression. * Equal contribution 1 Despite the name, CNNs typically use cross-correlation instead of convolution in the forward pass. In this paper we will generally use the term cross-correlation, or correlation for short. 2  To be more precise: although the symmetry group of the plane contains more than just translations, the translations form a subgroup that acts on the plane. In the case of the sphere there is no coherent way to define a composition for points on the sphere, and so the sphere cannot act on itself (it is not a group). For this reason, we must consider the whole of SO(3).",
    "prev": "In fact, the sphere is often considered as the prototypical example of the group setting.",
    "curr": "A number of group-based spherical CNN constructions have been developed (Cohen et al., 2018;Kondor et al., 2018;Esteves et al., 2018;2020;Cobb et al., 2021;McEwen et al., 2022;Mitchel et al., 2022), where Fourier representations of spherical signals (i.e.",
    "next": "spherical harmonic representations), combined with sampling theorems on the sphere (Driscoll & Healy, 1994;, are considered to provide access to the underlying continuous spherical signals and symmetries."
  },
  {
    "index": 2688,
    "source_corpus_id": 258212748,
    "ref_id": "b15",
    "citation_corpus_id": 6628106,
    "start": 35108,
    "end": 35129,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "We conducted all experiments on a single NVIDIA GeForce RTX 3090 GPU, except for the image completion tasks presented in Section 5.2; we used 8 TPUv3 cores supported by TPU Research Cloud 2 for the 2D image completion task.",
    "curr": "For optimization, we used Adam (Kingma and Ba, 2015) optimizer with a cosine learning rate schedule.",
    "next": "Unless specified, we selected the base learning rate from a grid of {5 × 10 −4.50 , 5 × 10 −4.25 , 5 × 10 −4.00 , 5 × 10 −3.75 , 5 × 10 −3.50 } based on validation task log-likelihood."
  },
  {
    "index": 2691,
    "source_corpus_id": 251929045,
    "ref_id": "b18",
    "citation_corpus_id": 52920808,
    "start": 21051,
    "end": 21067,
    "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks",
    "abstract": "One of the mystery in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an m hidden node shallow neural network with ReLU activation and n training data, we show as long as m is large enough and the data is non-degenerate, randomly initialized gradient descent converges a globally optimal solution with a linear convergence rate for the quadratic loss function.Our analysis is based on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods. * Equal Contribution.",
    "prev": ", a k ] ∈ R k .",
    "curr": "To concretely study the impact of data deletion on recourses in non-linear models such as neural networks, we leverage ideas from the neural tangent kernel (NTK) literature (Jacot et al., 2018;Lee et al., 2019;Arora et al., 2019;Du et al., 2019).The key insight from this literature for the purpose of our work is that infinitely wide neural networks can be expressed as a kernel ridge regression problem with the NTK under appropriate parameter initialization, and gradient descent training dynamics.In particular, in the limit as the number of hidden nodes k → ∞, the neural tangent kernel associated with a two-layer ReLU network has a closed-form expression (Chen & Xu, 2021;Zhang & Zhang, 2021) (see Appendix A.5):\nK ∞ (x 0 , x) = x ⊤ 0 x π − arcos x ⊤ 0 x ∥x0∥∥x∥ 2π .",
    "next": "(9)\nThus, the network's prediction at an input x can be described by:\nf NTK (x) = K ∞ (x, X) ⊤ w NTK ,(10)\nwhere X ∈ R n×d is the input data matrix, K ∞ (X, X) ∈ R n×n is the NTK matrix evaluated on the training data points:\n[K ∞ (X, X)] ij = K ∞ (x i , x j ) and w NTK = K ∞ (X, X) + βI n −1 Y= K ∞ (X, X) + βI n −1 Y,\n\nan upper bound for the recourse invalidation from\n\nDefinition 1 by removing an instance (x, y) from the training set is given by:\n∆ x ≤ ∥K ∞ (x 1 , X)"
  },
  {
    "index": 2692,
    "source_corpus_id": 238408063,
    "ref_id": "b6",
    "citation_corpus_id": 52967399,
    "start": 2836,
    "end": 2857,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "The key reason for Transformer's success is its self-attention mechanism, which computes dot-product between input representations for each pair of positions in the full sequence.",
    "curr": "Proved to be greatly effective in learning contextualized representations, Transformer becomes the backbone for dominant pre-trained language models (PLM) in NLP, such as BERT (Devlin et al., 2019) and RoBERTa .",
    "next": "These PLMs demonstrate strong transfer learning capabilities and have achieved SOTA widely on NLP tasks."
  },
  {
    "index": 2695,
    "source_corpus_id": 231861903,
    "ref_id": "b22",
    "citation_corpus_id": 6628106,
    "start": 35653,
    "end": 35672,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "Each network has two hidden layers of size 750 and it uses ReLU non-linearities.",
    "curr": "A.3.2 HYPERPARAMETERS\n\nAll the network parameters are updated using Adam (Kingma & Ba, 2015) with learning rates η = 0.001 for the critic and the VAE, and η = 0.0001 for the actor model, as in Fujimoto et al.",
    "next": "(2019)."
  },
  {
    "index": 2698,
    "source_corpus_id": 254044221,
    "ref_id": "b11",
    "citation_corpus_id": 225039882,
    "start": 8412,
    "end": 8438,
    "title": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. 1 arXiv:2010.11929v2 [cs.CV] 3 Jun 2021Published as a conference paper at ICLR 2021 inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.However, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks.RELATED WORKTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global selfattention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.",
    "prev": "In NLP, BERT (Devlin et al., 2018) considers subword-based tokenization (Schuster & Nakajima, 2012) instead of performing character-based tokenization.",
    "curr": "In CV, Vision Transformer (ViT) (Dosovitskiy et al., 2021) is a milestone work that splits an image into 16×16 patches before feeding into the Transformer model.",
    "next": "The following influential works such as BEiT (Bao et al., 2022) and masked autoencoders (He et al., 2021) are all using patches as input."
  },
  {
    "index": 2699,
    "source_corpus_id": 211020709,
    "ref_id": "b4",
    "citation_corpus_id": 52898806,
    "start": 4734,
    "end": 4759,
    "title": "RIEMANNIAN ADAPTIVE OPTIMIZATION METHODS",
    "abstract": "Several first order stochastic optimization methods commonly used in the Euclidean domain such as stochastic gradient descent (SGD), accelerated gradient descent or variance reduced methods have already been adapted to certain Riemannian settings. However, some of the most popular of these optimization tools − namely ADAM, ADAGRAD and the more recent AMSGRAD − remain to be generalized to Riemannian manifolds. We discuss the difficulty of generalizing such adaptive schemes to the most agnostic Riemannian setting, and then provide algorithms and convergence proofs for geodesically convex objectives in the particular case of a product of Riemannian manifolds, in which adaptivity is implemented across manifolds in the cartesian product. Our generalization is tight in the sense that choosing the Euclidean space as Riemannian manifold yields the same algorithms and regret bounds as those that were already known for the standard algorithms. Experimentally, we show faster convergence and to a lower train loss value for Riemannian adaptive methods over their corresponding baselines on the realistic task of embedding the WordNet taxonomy in the Poincaré ball.arXiv:1810.00760v1 [cs.LG] 1 Oct 2018Under review as a conference paper at ICLR 2019Our contributions. In this work we (i) explain why generalizing these adaptive schemes to the most agnostic Riemannian setting in an intrinsic manner is compromised, and (ii) propose generalizations of the algorithms together with their convergence analysis in the particular case of a product of manifolds where each manifold represents one \"coordinate\" of the adaptive scheme. Finally, we (iii) empirically support our claims on the realistic task of hyperbolic taxonomy embedding.Our initial motivation. The particular application that motivated us in developing Riemannian versions of ADAGRAD and ADAM was the learning of symbolic embeddings in non-Euclidean spaces. As an example, the GloVe algorithm(Pennington et al., 2014)− an unsupervised method for learning Euclidean word embeddings capturing semantic/syntactic relationships − benefits significantly from optimizing with ADAGRAD compared to using SGD, presumably because different words are sampled at different frequencies. Hence the absence of Riemannian adaptive algorithms could constitute a significant obstacle to the development of competitive optimization-based Riemannian embedding methods. In particular, we believe that the recent rise of embedding methods in hyperbolic spaces",
    "prev": "A theoretical analysis of the convergence of Cayley SGD is presented.",
    "curr": "Similar analysis for Cayley ADAM is omitted, since it is very similar to the analysis presented in (Becigneul & Ganea, 2019).",
    "next": "Cayley SGD and Cayley ADAM are empirically evaluated on image classification using VGG and Wide ResNet on the CIFAR-10 and CIFAR-100 datasets (Krizhevsky & Hinton, 2009)."
  },
  {
    "index": 2700,
    "source_corpus_id": 3693334,
    "ref_id": "b15",
    "citation_corpus_id": 17786716,
    "start": 5679,
    "end": 5683,
    "title": "Identity Matters in Deep Learning",
    "abstract": "An emerging design principle in deep learning is that each layer of a deep artificial neural network should be able to easily express the identity transformation. This idea not only motivated various normalization techniques, such as batch normalization, but was also key to the immense success of residual networks.In this work, we put the principle of identity parameterization on a more solid theoretical footing alongside further empirical progress. We first give a strikingly simple proof that arbitrarily deep linear residual networks have no spurious local optima. The same result for linear feed-forward networks in their standard parameterization is substantially more delicate. Second, we show that residual networks with ReLu activations have universal finite-sample expressivity in the sense that the network can represent any function of its sample provided that the model has more parameters than the sample size.Directly inspired by our theory, we experiment with a radically simple residual architecture consisting of only residual convolutional layers and ReLu activations, but no batch normalization, dropout, or max pool. Our model improves significantly on previous all-convolutional networks on the CIFAR10, CIFAR100, and ImageNet classification benchmarks. * Google Brain. m@mrtz.org † Princeton University. tengyu@cs.princeton.edu. Work performed at Google.",
    "prev": "Theoretical Background\n\nNumerous theoretical studies have been done on our ability to optimize neural loss function [5,4].",
    "curr": "Theoretical results usually make restrictive assumptions about the sample distributions, non-linearity of the architecture, or loss functions [16,31,40,36,9,39].",
    "next": "For restricted network classes, such as those with a single hidden layer, globally optimal or near-optimal solutions can be found by common optimization methods [35,26,38]."
  },
  {
    "index": 2703,
    "source_corpus_id": 218487073,
    "ref_id": "b5",
    "citation_corpus_id": 3531856,
    "start": 18295,
    "end": 18298,
    "title": "Published as a conference paper at ICLR 2018 DEMYSTIFYING MMD GANS",
    "abstract": "We investigate the training and performance of generative adversarial networks using the Maximum Mean Discrepancy (MMD) as critic, termed MMD GANs. As our main theoretical contribution, we clarify the situation with bias in GAN loss functions raised by recent work: we show that gradient estimators used in the optimization process for both MMD GANs and Wasserstein GANs are unbiased, but learning a discriminator based on samples leads to biased gradients for the generator parameters. We also discuss the issue of kernel choice for the MMD critic, and characterize the kernel corresponding to the energy distance used for the Cramér GAN critic. Being an integral probability metric, the MMD benefits from training strategies recently developed for Wasserstein GANs. In experiments, the MMD GAN is able to employ a smaller critic network than the Wasserstein GAN, resulting in a simpler and faster-training algorithm with matching performance. We also propose an improved measure of GAN convergence, the Kernel Inception Distance, and show how to use it to dynamically adapt learning rates during GAN training. * These authors contributed equally.Published as a conference paper at ICLR 2018 are infinite, meaning that they provide no useful gradient for the generator to follow. This helps to explain some of the instability of GAN training.The lack of sensitivity to distance, meaning that nearby but non-overlapping regions of high probability mass are not considered similar, is a long-recognized problem for KL divergence-based discrepancy measures (e.g. Gneiting & Raftery, 2007, Section 4.2). It is natural to address this problem using Integral Probability Metrics (IPMs; Müller, 1997): these measure the distance between probability measures via the largest discrepancy in expectation over a class of \"well behaved\" witness functions. Thus, IPMs are able to signal proximity in the probability mass of the generator and reference distributions. (Section 2 describes this framework in more detail.)  proposed to use the Wasserstein distance between distributions as the discriminator, which is an integral probability metric constructed from the witness class of 1-Lipschitz functions. To implement the Wasserstein critic, Arjovsky et al. originally proposed weight clipping of the discriminator network, to enforce k-Lipschitz smoothness. Gulrajani et al. (2017)  improved on this result by directly constraining the gradient of the discriminator network at points between the generator and reference samples. This new Wasserstein GAN implementation, called WGAN-GP, is more stable and easier to train.A second integral probability metric used in GAN variants is the maximum mean discrepancy (MMD), for which the witness function class is a unit ball in a reproducing kernel Hilbert space (RKHS). Generative adversarial models based on minimizing the MMD were first considered by Li et al. (2015)  and Dziugaite et al. (2015). These works optimized a generator to minimize the MMD with a fixed kernel, either using a generic kernel on image pixels or by modeling autoencoder representations instead of images directly. Sutherland et al.(2017)instead minimized the statistical power of an MMD-based test with a fixed kernel. Such approaches struggle with complex natural images, where pixel distances are of little value, and fixed representations can easily be tricked, as in the adversarial examples of Szegedy et al. (2014). A. Krizhevsky. Learning multiple layers of features from tiny images, 2009. Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.",
    "prev": "We use features derived from the final pooling layer of the p4-CNN defined in [14] to replace Inception-featurization.",
    "curr": "An analogous approach was taken in [6] in their MNIST experiments.",
    "next": "Fréchet distance of synthesized samples to the test set is calculated at every thousand generator iterations."
  },
  {
    "index": 2704,
    "source_corpus_id": 3421136,
    "ref_id": "b6",
    "citation_corpus_id": 6628106,
    "start": 3229,
    "end": 3232,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "Several works consider specific properties of energy landscapes of certain deep learning models such as frequent saddle points [4] and well-generalizable local optima [3].",
    "curr": "Among the most popular optimization methods in currently used deep learning frameworks are momentum based improvements of classical SGD, notably Nesterov's Accelerated Gradient [13,19], and the Adam optimizer [7], which uses estimates of first and second order moments of the gradients for parameter updates.",
    "next": "Nevertheless, the optimization of these models remains challenging, as learning with SGD and its variants requires careful weight initialization and a sufficiently small learning rate in order to yield * contributed equally."
  },
  {
    "index": 2706,
    "source_corpus_id": 52922125,
    "ref_id": "b7",
    "citation_corpus_id": 2536452,
    "start": 2056,
    "end": 2076,
    "title": "DISCOVERING HIDDEN FACTORS OF VARIATION IN DEEP NETWORKS",
    "abstract": "We propose a method for learning latent representations of the factors of variation in data. By augmenting deep autoencoders with a supervised cost and an additional unsupervised cost, we create a semi-supervised model that can discover and explicitly represent factors of variation beyond those relevant for categorization. We use a novel unsupervised covariance penalty (XCov) to disentangle factors like handwriting style for digits and subject identity in faces. We demonstrate this on the MNIST handwritten digit database, the Toronto Faces Database (TFD) and the Multi-PIE dataset by generating manipulated instances of the data. Furthermore, we demonstrate these deep networks can extrapolate 'hidden' variation in the supervised signal using the Toronto Faces Database.",
    "prev": "† Work done as a visiting student researcher at\n\nINTRODUCTION\n\nUnsupervised learning is a fundamental, unsolved problem (Hastie et al., 2009) and has seen promising results in domains such as image recognition (Le et al., 2013) and natural language understanding (Ramachandran et al., 2017).",
    "curr": "A central use case of unsupervised learning methods is enabling better or more efficient learning of downstream tasks by training on top of unsupervised representations (Reed et al., 2014;Cheung et al., 2015; or fine-tuning a learned model (Erhan et al., 2010).",
    "next": "However, since the downstream objective requires access to supervision, the objectives used for unsupervised learning are only a rough proxy for downstream performance."
  },
  {
    "index": 2707,
    "source_corpus_id": 7782433,
    "ref_id": "b20",
    "citation_corpus_id": 3075448,
    "start": 32714,
    "end": 32718,
    "title": "HIGH-DIMENSIONAL CONTINUOUS CONTROL USING GENERALIZED ADVANTAGE ESTIMATION",
    "abstract": "Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(λ). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.Published as a conference paper at ICLR 2016 λ ∈ [0, 1], the generalized advantage estimator (GAE). Related methods have been proposed in the context of online actor-critic methods(Kimura & Kobayashi, 1998;Wawrzyński, 2009). We provide a more general analysis, which is applicable in both the online and batch settings, and discuss an interpretation of our method as an instance of reward shaping(Ng et al., 1999), where the approximate value function is used to shape the reward.We present experimental results on a number of highly challenging 3D locomotion tasks, where we show that our approach can learn complex gaits using high-dimensional, general purpose neural network function approximators for both the policy and the value function, each with over 10 4 parameters. The policies perform torque-level control of simulated 3D robots with up to 33 state dimensions and 10 actuators.The contributions of this paper are summarized as follows:1. We provide justification and intuition for an effective variance reduction scheme for policy gradients, which we call generalized advantage estimation (GAE). While the formula has been proposed in prior work(Kimura & Kobayashi, 1998;Wawrzyński, 2009), our analysis is novel and enables GAE to be applied with a more general set of algorithms, including the batch trust-region algorithm we use for our experiments. 2. We propose the use of a trust region optimization method for the value function, which we find is a robust and efficient way to train neural network value functions with thousands of parameters. 3. By combining(1)and(2)above, we obtain an algorithm that empirically is effective at learning neural network policies for challenging control tasks. The results extend the state of the art in using reinforcement learning for high-dimensional continuous control. Videos are available at https://sites.google.com/site/gaepapersupp.",
    "prev": "All neural networks were feed-forward, fully-connected networks with tanh activation units.",
    "curr": "B.3 TRPO HYPERPARAMETERS\n\nFor all tasks, the MDP discount factor γ was fixed to 0.995, and generalized advantage estimators (GAE) [21] were used, with the GAE λ parameter fixed to 0.95.",
    "next": "In the table below, we show several other TRPO hyperparameters."
  },
  {
    "index": 2708,
    "source_corpus_id": 3480671,
    "ref_id": "b0",
    "citation_corpus_id": 11212020,
    "start": 1554,
    "end": 1577,
    "title": "Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "prev": "By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English-Romanian.",
    "curr": "INTRODUCTION\n\nNeural network based models outperform traditional statistical models for machine translation (MT) (Bahdanau et al., 2015;Luong et al., 2015).",
    "next": "However, state-of-the-art neural models are much slower than statistical MT approaches at inference time (Wu et al., 2016)."
  },
  {
    "index": 2709,
    "source_corpus_id": 186206649,
    "ref_id": "b14",
    "citation_corpus_id": 6628106,
    "start": 1824,
    "end": 1844,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "Introduction\n\nDeep neural networks have exhibited large success in many applications (Krizhevsky et al., 2012).",
    "curr": "This success has motivated many studies of their non-convex loss landscape (Choromanska et al., 2015;Kawaguchi, 2016;Li et al., 2018b), which, in turn, has led to many improvements, such as better initialization and optimization methods (Glorot and Bengio, 2010;Kingma and Ba, 2015).",
    "next": "While most of the work on studying non-convex loss landscapes has focused on single objective minimization, some recently introduced models require the joint minimization of several objectives, making them intrinsically different."
  },
  {
    "index": 2711,
    "source_corpus_id": 263672060,
    "ref_id": "b35",
    "citation_corpus_id": 222140788,
    "start": 3049,
    "end": 3067,
    "title": "Published as a conference paper at ICLR 2021 DENOISING DIFFUSION IMPLICIT MODELS",
    "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples 10× to 50× faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error.",
    "prev": "ations in activation distributions across denoising steps.Extensive experimental results demonstrate that our method significantly outperforms previous PTQ-based diffusion models while maintaining similar time and data efficiency.Specifically, there is only a marginal 0.05 sFID increase when quantizing both weights and activations of LDM-4 to 4-bit on ImageNet 256 × 256.Compared to QAT-based methods, our EfficientDM also boasts a 16.2× faster quantization speed with comparable generation quality, rendering it a compelling choice for practical applications.",
    "curr": "INTRODUCTION\n\nDiffusion models (DM) (Ho et al., 2022b;Dhariwal & Nichol, 2021;Rombach et al., 2022a;Ho et al., 2022a) have demonstrated remarkable capabilities in image generation and related tasks.Nonetheless, the iterative denoising process and the substantial computational overhead of the denoising model limit the efficiency of DM-based image generation.To expedite the image generation process, numerous methods (Bao et al., 2022;Song et al., 2021;Liu et al., 2022;Lu et al., 2022) have been explored to reduce the number of denoising iterations, effectively reducing the previously required thousands of iterations to mere dozens.However, the significant volume of parameters within the denoising model still demands a substantial computational burden for each denoising step, resulting in considerable latency, hindering the practical application of DM in real-world settings with latency and computational resource constraints.",
    "next": "Model quantization, which compresses weights and activations from 32-bit floating-point values into lower-bit fixed-point formats, alleviating both memory and computational burdens.This effect can be increasingly pronounced as the bit-width decreases.For instance, leveraging Nvidia's CUTLASS (Kerr et al., 2017) implementation, an 8-bit model's inference speed can be 2.03× faster than that of a full-precision (FP) model, and the acceleration ratio reaches 3.34× for a 4-bit model.Therefore, it possesses substanti"
  },
  {
    "index": 2712,
    "source_corpus_id": 7443908,
    "ref_id": "b22",
    "citation_corpus_id": 1957433,
    "start": 2242,
    "end": 2266,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "Despite the overwhelming success achieved by recurrent neural networks in modeling long range dependencies between words, current recurrent neural network language models (RNNLM) are based on the conventional classification framework, which has two major drawbacks: First, there is no assumed metric on the output classes, whereas there is evidence suggesting that learning is improved when one can define a natural metric on the output space (Frogner et al., 2015).",
    "curr": "In language modeling, there is a well established metric space for the outputs (words in the language) based on word embeddings, with meaningful distances between words (Mikolov et al., 2013;Pennington et al., 2014).",
    "next": "Second, in the classical framework, inputs and outputs are considered as isolated entities with no semantic link between them."
  },
  {
    "index": 2713,
    "source_corpus_id": 220249726,
    "ref_id": "b18",
    "citation_corpus_id": 52967399,
    "start": 3101,
    "end": 3105,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "However, the strong performance of the Transformer comes at the cost of interpretability, and this lack of transparency can hide underlying problems such as model bias and spurious correlations [41,55,75].",
    "curr": "In response, a great deal of NLP research now focuses on interpreting the Transformer, e.g., the subspecialty of \"BERTology\" [65], which studies the BERT [19] Transformer model specifically.",
    "next": "In this work, we adapt and extend this line of interpretability research to protein sequences."
  },
  {
    "index": 2714,
    "source_corpus_id": 233219849,
    "ref_id": "b35",
    "citation_corpus_id": 3986974,
    "start": 2878,
    "end": 2900,
    "title": "The Web as a Knowledge-base for Answering Complex Questions",
    "abstract": "Answering complex questions is a timeconsuming activity for humans that requires reasoning and integration of information. Recent work on reading comprehension made headway in answering simple questions, but tackling complex questions is still an ongoing research challenge. Conversely, semantic parsers have been successful at handling compositionality, but only when the information resides in a target knowledge-base. In this paper, we present a novel framework for answering broad and complex questions, assuming answering simple questions is possible using a search engine and a reading comprehension model. We propose to decompose complex questions into a sequence of simple questions, and compute the final answer from the sequence of answers. To illustrate the viability of our approach, we create a new dataset of complex questions, COMPLEXWEBQUES-TIONS, and present a model that decomposes questions and interacts with the web to compute an answer. We empirically demonstrate that question decomposition improves performance from 20.8 precision@1 to 27.5 preci-sion@1 on this new dataset.",
    "prev": "Answering this question is made possible by integrating information across both the textual and visual modalities.",
    "curr": "Recently, there has been substantial interest in question answering (QA) models that reason over multiple pieces of evidence (multi-hop questions (Yang et al., 2018;Talmor & Berant, 2018;Welbl et al., 2017)).",
    "next": "In most prior work, the question is phrased in natural language and the answer is in a context, which may be a paragraph (Rajpurkar, 2016), a table (Pasupat & Liang, 2015), or an image (Antol et al., 2015)."
  },
  {
    "index": 2716,
    "source_corpus_id": 258832501,
    "ref_id": "b9",
    "citation_corpus_id": 231815627,
    "start": 9279,
    "end": 9282,
    "title": "RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering",
    "abstract": "In open-domain question answering, dense passage retrieval has become a new paradigm to retrieve relevant passages for finding answers. Typically, the dual-encoder architecture is adopted to learn dense representations of questions and passages for semantic matching. However, it is difficult to effectively train a dual-encoder due to the challenges including the discrepancy between training and inference, the existence of unlabeled positives and limited training data. To address these challenges, we propose an optimized training approach, called RocketQA, to improving dense passage retrieval. We make three major technical contributions in RocketQA, namely crossbatch negatives, denoised hard negatives and data augmentation. The experiment results show that RocketQA significantly outperforms previous state-of-the-art models on both MS-MARCO and Natural Questions. We also conduct extensive experiments to examine the effectiveness of the three strategies in RocketQA. Besides, we demonstrate that the performance of end-to-end QA can be improved based on our RocketQA retriever 1 .",
    "prev": "The retrieve-then-read model [40] utilizes retrievers, such as BM25 [41] and DPR [42], to identify relevant documents within a knowledge corpus.",
    "curr": "Subsequently, a reader, like FiD [34], analyzes the retrieved documents to enhance NLP tasks [39,43,44,45].",
    "next": "Some studies retrieve subgraphs from knowledge graphs to provide additional input for question-answering tasks [35,46]."
  },
  {
    "index": 2717,
    "source_corpus_id": 225076232,
    "ref_id": "b28",
    "citation_corpus_id": 14124313,
    "start": 1633,
    "end": 1660,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "Our code is available at https://github.com/utsaslab/MONeT.",
    "curr": "INTRODUCTION\n\nDeep networks are widely used in domains ranging from image classification (Krizhevsky et al., 2012;Simonyan & Zisserman, 2015;He et al., 2016a) to video recognition (Wu et al., 2019; or natural language processing (Devlin et al., 2019;.",
    "next": "However, training deep networks is resource-intensive."
  },
  {
    "index": 2718,
    "source_corpus_id": 263620670,
    "ref_id": "b21",
    "citation_corpus_id": 202660778,
    "start": 2445,
    "end": 2462,
    "title": "SAMPLE EFFICIENT POLICY GRADIENT METHODS WITH RECURSIVE VARIANCE REDUCTION",
    "abstract": "Improving the sample efficiency in reinforcement learning has been a longstanding research problem. In this work, we aim to reduce the sample complexity of existing policy gradient methods. We propose a novel policy gradient algorithm called SRVR-PG, which only requires O(1/ 3/2 ) 1 episodes to find anapproximate stationary point of the nonconcave performance function J(θ) (i.e.",
    "prev": "Despite their far-reaching history (Williams, 1992;Sutton et al., 1999;Konda and Tsitsiklis, 1999;Kakade, 2001), there were no proofs for the global convergence of these algorithms for a long time.",
    "curr": "Nevertheless, they have been very successful in many applications, which is why numerous variants have been developed in the last few decades, whose convergence analysis, if available, was mostly limited to convergence to stationary points (Pirotta et al., 2013;Schulman et al., 2015;Papini et al., 2018;Clavera et al., 2018;Shen et al., 2019;Xu et al., 2020b;Huang et al., 2020;Xu et al., 2020a;Huang et al., 2022).",
    "next": "In recent years, notable advancements have been achieved in the convergence analysis towards global optima (Fazel et al., 2018;Agarwal et al., 2021;Mei et al., 2020;Russo, 2021, 2022;Cen et al., 2022;Xiao, 2022;Yuan et al., 2022;Alfano and Rebeschini, 2023)."
  },
  {
    "index": 2719,
    "source_corpus_id": 259164846,
    "ref_id": "b40",
    "citation_corpus_id": 49868626,
    "start": 1354,
    "end": 1357,
    "title": "META-LEARNING WITH LATENT EMBEDDING OPTIMIZATION",
    "abstract": "Gradient-based meta-learning techniques are both widely applicable and proficient at solving challenging few-shot learning and fast adaptation problems. However, they have practical difficulties when operating on high-dimensional parameter spaces in extreme low-data regimes. We show that it is possible to bypass these limitations by learning a data-dependent latent generative representation of model parameters, and performing gradient-based meta-learning in this lowdimensional latent space. The resulting approach, latent embedding optimization (LEO), decouples the gradient-based adaptation procedure from the underlying high-dimensional space of model parameters. Our evaluation shows that LEO can achieve state-of-the-art performance on the competitive miniImageNet and tieredImageNet few-shot classification tasks. Further analysis indicates LEO is able to capture uncertainty in the data, and can perform adaptation more effectively by optimizing in latent space. arXiv:1807.05960v3 [cs.LG]",
    "prev": "Introduction\n\nFew-shot recognition [23,33,51] aims to learn novel concepts from few examples, often by rapid adaptation of a model trained on a disjoint set of labels.",
    "curr": "Many solutions adopt a meta-learning perspective [16,25,38,41,43], or train a powerful feature extractor on the source classes [44,50] -both of which assume that the training and testing classes are drawn from the same underlying distribution e.g., handwritten characters [24], or ImageNet categories [48].",
    "next": "Later work considers a more realistic and challenging problem variant where a classifier should perform few-shot adaptation not only across visual categories, but also across diverse visual domains [46,47]."
  },
  {
    "index": 2720,
    "source_corpus_id": 46899514,
    "ref_id": "b2",
    "citation_corpus_id": 3495200,
    "start": 1825,
    "end": 1828,
    "title": "LEARNING TO REPRESENT PROGRAMS WITH GRAPHS",
    "abstract": "Learning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code's known syntax. For example, long-range dependencies induced by using the same variable or function in distant locations are often not considered. We propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures. In this work, we present how to construct graphs from source code and how to scale Gated Graph Neural Networks training to such large graphs. We evaluate our method on two tasks: VARNAMING, in which a network attempts to predict the name of a variable given its usage, and VARMISUSE, in which the network learns to reason about selecting the correct variable that should be used at a given program location. Our comparison to methods that use less structured program representations shows the advantages of modeling known structure, and suggests that our models learn to infer meaningful names and to solve the VARMISUSE task in many cases. Additionally, our testing showed that VARMISUSE identifies a number of bugs in mature open-source projects.",
    "prev": "Early works in the area have shown that approaches from natural language processing can be applied successfully to source code [12], whereas the programming languages community has shown great successes in focusing exclusively on formal semantics.",
    "curr": "More recently, methods handling both modalities (i.e., the formal and natural language aspects) have shown successes on important software engineering tasks [3,5,21].",
    "next": "However, current generative models of source code still focus on only one of these modalities at a time."
  },
  {
    "index": 2721,
    "source_corpus_id": 255340454,
    "ref_id": "b24",
    "citation_corpus_id": 240354066,
    "start": 3433,
    "end": 3437,
    "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
    "abstract": "A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of 10000 or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model, and showed that for appropriate choices of the state matrix A, this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning A with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation 60× faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors. 1",
    "prev": "Order determined by coin flip.",
    "curr": "Introduction\n\nState space models (SSMs) have achieved state-of-the-art sequence modeling performance in domains ranging from time series analysis [25] to audio generation [22].",
    "next": "However, they have yet to match the performance of Transformers on language modeling, often underperforming Transformers by multiple points in perplexity [25]."
  },
  {
    "index": 2727,
    "source_corpus_id": 44129557,
    "ref_id": "b3",
    "citation_corpus_id": 5590763,
    "start": 1806,
    "end": 1824,
    "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
    "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
    "prev": "INTRODUCTION\n\nNeural response generation has been a long interest of natural language research.",
    "curr": "Most of the recent approaches to data-driven conversation modeling primarily build upon sequence-to-sequence learning (Cho et al., 2014;Sutskever et al., 2014).",
    "next": "Previous research has demonstrated that sequenceto-sequence conversation models often suffer from the safe response problem and fail to generate meaningful, diverse on-topic responses (Li et al., 2015;Sato et al., 2017)."
  },
  {
    "index": 2728,
    "source_corpus_id": 259108266,
    "ref_id": "b10",
    "citation_corpus_id": 86611921,
    "start": 2549,
    "end": 2551,
    "title": "Natural Questions: A Benchmark for Question Answering Research",
    "abstract": "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.453",
    "prev": "Yidong did this work during his internship at Westlake University.",
    "curr": "† Corresponding to\n\nIntroduction\n\nLarge language models (LLMs) have attracted increasing attention in the field of artificial intelligence [1,2,3,4,5,6], with various applications from question answering [7,8], machine translation [9,10] to content creation [11,12].",
    "next": "The Alpaca project [13] has been a pioneering effort in instruction tuning of LLaMA [14], setting a precedent for instruction tuning LLMs, followed by Vicunna [15]."
  },
  {
    "index": 2729,
    "source_corpus_id": 231955136,
    "ref_id": "b14",
    "citation_corpus_id": 49667762,
    "start": 2045,
    "end": 2067,
    "title": "UNIVERSAL TRANSFORMERS",
    "abstract": "Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset. * Equal contribution, alphabetically by last name. † Work performed while at Google Brain.",
    "prev": "Each layer computes some transformation of the output of the previous layer.",
    "curr": "Surprisingly, several recent studies achieved results competitive with the state-ofthe-art performances by using the same transformation for each layer with weight tying (Dabre & Fujita, 2019;Bai et al., 2019b;Dehghani et al., 2019).",
    "next": "In general terms, the output of the l-th layer with weight tying can be written by z (l) = h(z (l−1) ; x, θ) for l = 1, 2, ."
  },
  {
    "index": 2730,
    "source_corpus_id": 246633939,
    "ref_id": "b7",
    "citation_corpus_id": 28202810,
    "start": 35250,
    "end": 35273,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "We represent both Q-network and policy network as a 4-layer feedforward neural network with 256 hidden units, and use clipped double Q-learning (Fujimoto et al., 2018) for Q backup update.",
    "curr": "A max entropy term is also included to the value function for computing the target Q value as in SAC (Haarnoja et al., 2018).",
    "next": "The hyperparameters used for evaluating the performance of RAC are described in Table 1."
  },
  {
    "index": 2733,
    "source_corpus_id": 261557333,
    "ref_id": "b16",
    "citation_corpus_id": 252762090,
    "start": 4911,
    "end": 4915,
    "title": "Published as a conference paper at ICLR 2023 SCALING FORWARD GRADIENT WITH LOCAL LOSSES",
    "abstract": "Forward gradient learning computes a noisy directional gradient and is a biologically plausible alternative to backprop for learning deep neural networks. However, the standard forward gradient algorithm, when applied naively, suffers from high variance when the number of parameters to be learned is large. In this paper, we propose a series of architectural and algorithmic modifications that together make forward gradient learning practical for standard deep learning benchmark tasks. We show that it is possible to substantially reduce the variance of the forward gradient estimator by applying perturbations to activations rather than weights. We further improve the scalability of forward gradient by introducing a large number of local greedy loss functions, each of which involves only a small number of learnable parameters, and a new MLPMixer-inspired architecture, LocalMixer, that is more suitable for local learning. Our approach matches backprop on MNIST and CIFAR-10 and significantly outperforms previously proposed backprop-free algorithms on Im-ageNet. Code is released at https://github.com/google-research/ google-research/tree/master/local_forward_gradient.We prove that activity perturbation yields lower-variance gradient estimates than weight perturbation, and provide a continuous-time rate-based interpretation of our algorithm. We directly address the scalability issue of forward gradient learning by designing an architecture with many local greedy loss functions, isolating the network into local modules and hence reducing the number of learnable parameters per loss. Unlike prior work that only adds local losses along the depth dimension, we found that having patch-wise and channel group-wise losses is also critical. Lastly, inspired by the design of MLPMixer(Tolstikhin et al., 2021), we designed a network called LocalMixer, featuring a linear token mixing layer and grouped channels for better compatibility with local learning.We evaluate our local greedy forward gradient algorithm on supervised and self-supervised image classification problems. On MNIST and CIFAR-10, our learning algorithm performs comparably with backprop, and on ImageNet, it performs significantly better than other biologically plausible alternatives using asymmetric forward and backward weights. Although we have not fully matched backprop on larger-scale problems, we believe that local loss design could be a critical ingredient for biologically plausible learning algorithms and the next generation of model-parallel computation.",
    "prev": "More comprehensive studies on the effect of non-symmetric feedback connections on training have only been done in the context of BP [12][13][14][15][16].",
    "curr": "While alternative approaches studied learning beyond approximate BP [17][18][19][20][21], here we focus on generalized EP due to its strong theoretical foundation and because it enables physical systems to learn with their dynamics.",
    "next": "In summary, it remains an open question whether and how generalized EP can compute suitable gradient estimates for finite nudges and asymmetric weights and how well the resulting networks perform on complex real-world tasks for varying degrees of weight asymmetry."
  },
  {
    "index": 2734,
    "source_corpus_id": 247292561,
    "ref_id": "b40",
    "citation_corpus_id": 222208633,
    "start": 3641,
    "end": 3645,
    "title": "DEFORMABLE DETR: DEFORMABLE TRANSFORMERS FOR END-TO-END OBJECT DETECTION",
    "abstract": "DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10× less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code shall be released. * Equal contribution. † This work is done when Weijie Su is an intern at SenseTime Research.",
    "prev": "Despite its promising performance, the training convergence of DETR is slow and the meaning of queries is unclear.",
    "curr": "To address such problems, many methods have been proposed, such as introducing deformable attention [41], decoupling positional and content information [25], providing spatial priors [11,39,37], etc.",
    "next": "Recently, DAB-DETR [21] proposes to formulate DETR queries as dynamic anchor boxes (DAB), which bridges the gap between classical anchor-based detectors and DETR-like ones."
  },
  {
    "index": 2735,
    "source_corpus_id": 85469987,
    "ref_id": "b5",
    "citation_corpus_id": 16209268,
    "start": 1764,
    "end": 1788,
    "title": "Published as a conference paper at ICLR 2015 QUALITATIVELY CHARACTERIZING NEURAL NETWORK OPTIMIZATION PROBLEMS",
    "abstract": "Training neural networks involves solving large-scale non-convex optimization problems. This task has long been believed to be extremely difficult, with fear of local minima and other obstacles motivating a variety of schemes to improve optimization, such as unsupervised pretraining. However, modern neural networks are able to achieve negligible training error on complex tasks, using only direct training with stochastic gradient descent. We introduce a simple analysis technique to look for evidence that such networks are overcoming local optima. We find that, in fact, on a straight path from initialization to solution, a variety of state of the art neural networks never encounter any significant obstacles.",
    "prev": "INTRODUCTION\n\nIt is well known that the optimization problem for training neural networks can have exponentially many local minima (Auer et al., 1996;Safran & Shamir, 2016) and NP-hardness has been shown in many cases (Blum & Rivest., 1989;Sima, 2002;Livni et al., 2014;Shamir, 2017;Shalev-Shwartz et al., 2017).",
    "curr": "However, it has been empirically observed (Dauphin et al., 2014;Goodfellow et al., 2015) that the training of state-of-the-art deep CNNs (LeCun et al., 1990;Krizhevsky et al., 2012), which are often overparameterized, is not hampered by suboptimal local minima.",
    "next": "In order to explain the apparent gap between hardness results and practical performance, many interesting theoretical results have been recently developed (Andoni et al., 2014;Sedghi & Anandkumar, 2015;Janzamin et al., 2016;Haeffele & Vidal, 2015;Gautier et al., 2016;Brutzkus & Globerson, 2017;Soltanolkotabi, 2017;Soudry & Hoffer, 2017;Goel & Klivans, 2017;Du et al., 2017;Zhong et al., 2017;Tian, 2017;Li & Yuan, 2017) in order to identify conditions under which one can guarantee that local search algorithms like gradient descent converge to the globally optimal solution."
  },
  {
    "index": 2738,
    "source_corpus_id": 246210223,
    "ref_id": "b45",
    "citation_corpus_id": 231648391,
    "start": 7725,
    "end": 7747,
    "title": "SEMI-SUPERVISED KEYPOINT LOCALIZATION",
    "abstract": "Knowledge about the locations of keypoints of an object in an image can assist in fine-grained classification and identification tasks, particularly for the case of objects that exhibit large variations in poses that greatly influence their visual appearance, such as wild animals. However, supervised training of a keypoint detection network requires annotating a large image dataset for each animal species, which is a labor-intensive task. To reduce the need for labeled data, we propose to learn simultaneously keypoint heatmaps and pose invariant keypoint representations in a semi-supervised manner using a small set of labeled images along with a larger set of unlabeled images. Keypoint representations are learnt with a semantic keypoint consistency constraint that forces the keypoint detection network to learn similar features for the same keypoint across the dataset. Pose invariance is achieved by making keypoint representations for the image and its augmented copies closer together in feature space. Our semi-supervised approach significantly outperforms previous methods on several benchmarks for human and animal body landmark localization.",
    "prev": "Semi-supervised keypoint localization can be categorized into consistency regularization based methods and pseudo-labeling based methods.",
    "curr": "Consistency regularization methods (Honari et al., 2018;Moskvyak et al., 2020) assume that the output of the model should not be invariant to realistic perturbations.",
    "next": "These approaches typically rely on modality-specific augmentation techniques for regularization."
  },
  {
    "index": 2742,
    "source_corpus_id": 247594476,
    "ref_id": "b37",
    "citation_corpus_id": 7478738,
    "start": 4095,
    "end": 4117,
    "title": "Linguistic Regularities in Continuous Space Word Representations",
    "abstract": "Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, \"King -Man + Woman\" results in a vector very close to \"Queen.\" We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems.",
    "prev": "The practice in Natural Language Processing (NLP) and Data Mining (DM) might shed some light on this issue.",
    "curr": "Word embeddings (Mikolov et al., 2013;Pennington et al., 2014) is proposed very early in NLP to map words into an embedding space using embedding layers, which work as lookup tables that are indexed by the one-hot encoding of words.",
    "next": "The word embeddings retrieved from embedding layers have similar values for words with similar meanings."
  },
  {
    "index": 2743,
    "source_corpus_id": 252917981,
    "ref_id": "b15",
    "citation_corpus_id": 52967399,
    "start": 4820,
    "end": 4842,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "For each of them, researchers have created datasets that target modern language models' weaknesses which we adopt for evaluation.",
    "curr": "For domain shift, MRQA (Fisch et al., 2019) trains on six machine reading datasets from the source domain and tests on six different target domains; for perturbations, AdvGLUE (Wang et al., 2021) craft adversarial versions of GLUE (Wang et al., 2018) based on automatic adversarial perturbations and human filtering, and Contrast Sets (Gardner et al., 2020) are expert-authored minimal edits that change the label; for spurious correlation, HANS (McCoy et al., 2019) and PAWS (Zhang et al., 2019) are challenge sets designed for models trained on MNLI and\n\nINTRODUCTION\n\nNLP is dominated by large language models (LLMs) -pretrained on large, unlabeled text data -that are then used for downstream tasks (Devlin et al., 2019a;Brown et al., 2020).",
    "next": "Scaling the model and data size often brings gains on downstream tasks (Kaplan et al., 2020;BIG-Bench, 2022), allowing what some call emergent abilities (Wei et al., 2022a)."
  },
  {
    "index": 2744,
    "source_corpus_id": 256827373,
    "ref_id": "b35",
    "citation_corpus_id": 220525799,
    "start": 3007,
    "end": 3026,
    "title": "Long-Tail Learning via Logit Adjustment",
    "abstract": "Real-world classification problems typically exhibit an imbalanced or long-tailed label distribution, wherein many labels are associated with only a few samples. This poses a challenge for generalisation on such labels, and also makes naïve learning biased towards dominant labels. In this paper, we present two simple modifications of standard softmax cross-entropy training to cope with these challenges. Our techniques revisit the classic idea of logit adjustment based on the label frequencies, either applied post-hoc to a trained model, or enforced in the loss during training. Such adjustment encourages a large relative margin between logits of rare versus dominant labels. These techniques unify and generalise several recent proposals in the literature, while possessing firmer statistical grounding and empirical performance. A reference implementation of our methods is available at:https://github.com/google-research/google-research/tree/master/logit_adjustment.",
    "prev": "To improve generalization performance, many studies have aimed to emphasize minority classes or reduce the influence of the majority samples.",
    "curr": "Reweighting (Cao et al., 2019;Menon et al., 2021) or resampling (Buda et al., 2018;Van Hulse et al., 2007) are two representative methods that have been frequently applied to achieve this goal.",
    "next": "(i) Reweighting techniques increase the weight of the training loss of the samples in the minority classes."
  },
  {
    "index": 2746,
    "source_corpus_id": 238583615,
    "ref_id": "b22",
    "citation_corpus_id": 211842237,
    "start": 2267,
    "end": 2289,
    "title": "Published as a conference paper at ICLR 2020 DIRECTIONAL MESSAGE PASSING FOR MOLECULAR GRAPHS",
    "abstract": "Graph neural networks have recently achieved great successes in predicting quantum mechanical properties of molecules. These models represent a molecule as a graph using only the distance between atoms (nodes). They do not, however, consider the spatial direction from one atom to another, despite directional information playing a central role in empirical potentials for molecules, e.g. in angular potentials. To alleviate this limitation we propose directional message passing, in which we embed the messages passed between atoms instead of the atoms themselves. Each message is associated with a direction in coordinate space. These directional message embeddings are rotationally equivariant since the associated directions rotate with the molecule. We propose a message passing scheme analogous to belief propagation, which uses the directional information by transforming messages based on the angle between them. Additionally, we use spherical Bessel functions and spherical harmonics to construct theoretically well-founded, orthogonal representations that achieve better performance than the currently prevalent Gaussian radial basis representations while using fewer than 1 /4 of the parameters. We leverage these innovations to construct the directional message passing neural network (DimeNet). DimeNet outperforms previous GNNs on average by 76 % on MD17 and by 31 % on QM9. Our implementation is available online. 1 1 https://www.daml.in.tum.de/dimenet arXiv:2003.03123v2 [cs.LG] 5 Apr 2022Published as a conference paper at ICLR 2020 embeddings are equivariant with respect to the above transformations since the directions move with the molecule. Hence, they preserve the relative directional information between neighboring atoms. We propose to let message embeddings interact based on the distance between atoms and the angle between directions. Both distances and angles are invariant to translation, rotation, and inversion of the molecule, as required. Additionally, we show that the distance and angle can be jointly represented in a principled and effective manner by using spherical Bessel functions and spherical harmonics. We leverage these innovations to construct the directional message passing neural network (DimeNet). DimeNet can learn both molecular properties and atomic forces. It is twice continuously differentiable and solely based on the atom types and coordinates, which are essential properties for performing molecular dynamics simulations. DimeNet outperforms previous GNNs on average by 76 % on MD17 and by 31 % on QM9. Our paper's main contributions are:",
    "prev": "In particular, a lot of work has gone into building surrogate models to reproduce QM properties, e.g., energies.",
    "curr": "These models learn from datasets created using classical techniques such as density functional theory (DFT) (Ramakrishnan et al., 2014;Klicpera et al., 2019) or coupled clusters (CCSD) (Chmiela et al., 2018).",
    "next": "While this approach has shown great success in recovering the baseline calculations, it suffers from several disadvantages."
  },
  {
    "index": 2749,
    "source_corpus_id": 254636202,
    "ref_id": "b34",
    "citation_corpus_id": 3488815,
    "start": 2768,
    "end": 2788,
    "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
    "abstract": "Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. * Authors ordered alphabetically.",
    "prev": "Due to the importance of this problem, there is a large literature that investigates adversarial robustness for neural networks.",
    "curr": "The most common approach for adversarial defense is to learn the model through adversarial training (Madry et al., 2018;Szegedy et al., 2013;Pang et al., 2020;Rice et al., 2020;Uesato et al., 2019), which involves augmenting the training set with mined adversarial examples that fool the image classifier.",
    "next": "Adversarial training has been validated to improve robustness on the task that the mined examples come from, but it often comes at a cost of generalization (Stutz et al., 2019;Pedraza et al., 2021)."
  },
  {
    "index": 2750,
    "source_corpus_id": 263830334,
    "ref_id": "b21",
    "citation_corpus_id": 254247299,
    "start": 2098,
    "end": 2115,
    "title": "EXPLORING THE LIMITS OF DIFFERENTIALLY PRI- VATE DEEP LEARNING WITH GROUP-WISE CLIPPING",
    "abstract": "Differentially private deep learning has recently witnessed advances in computational efficiency and privacy-utility trade-off. We explore whether further improvements along the two axes are possible and provide affirmative answers leveraging two instantiations of group-wise clipping. To reduce the compute time overhead of private learning, we show that per-layer clipping, where the gradient of each neural network layer is clipped separately, allows clipping to be performed in conjunction with backpropagation in differentially private optimization. This results in private learning that is as memory-efficient and almost as fast per training update as non-private learning for many workflows of interest. While per-layer clipping with constant thresholds tends to underperform standard flat clipping, per-layer clipping with adaptive thresholds matches or outperforms flat clipping under given training epoch constraints, hence attaining similar or better task performance within less wall time. To explore the limits of scaling (pretrained) models in differentially private deep learning, we privately fine-tune the 175 billion-parameter GPT-3. We bypass scaling challenges associated with clipping gradients that are distributed across multiple devices with per-device clipping that clips the gradient of each model piece separately on its host device. Privately fine-tuning GPT-3 with perdevice clipping achieves a task performance at = 1 better than what is attainable by non-privately fine-tuning the largest GPT-2 on a summarization task. arXiv:2212.01539v1 [cs.LG] 3 Dec 2022 mechanism for large scale NLU models. Deep models under the gan: information leakage from collaborative deep learning. In , et al. Learning and evaluating a differentially private pre-trained language model. In",
    "prev": "The workhorse of private learning, a differentially private variant of stochastic gradient descent called DP-SGD (Song et al., 2013;Bassily et al., 2014;Abadi et al., 2016), clips per-example gradients to some ℓ 2 norm and adds independent Gaussian noise.",
    "curr": "DP-SGD has been used in a range of applications from learning with medical images (Adnan et al., 2022) to finetuning large language models with O(100B) parameters (He et al., 2023).",
    "next": "A recent line of work instead proposes to add correlated Gaussian noise to each clipped gradient (Smith & Thakurta, 2013;Kairouz et al., 2021a;Denisov et al., 2022;Choquette-Choo et al., 2023b)."
  },
  {
    "index": 2751,
    "source_corpus_id": 213179475,
    "ref_id": "b5",
    "citation_corpus_id": 6706414,
    "start": 3022,
    "end": 3046,
    "title": "Published as a conference paper at ICLR 2015 EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES",
    "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples-inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
    "prev": "(bottom) Forward pass for FDA targeted attack.",
    "curr": "Most recent adversarial attack literature has focused on empirical demonstrations of how classifiers can be fooled by the addition of quasi-imperceptible noise to the input (Szegedy et al., 2014;Goodfellow et al., 2015;Carlini & Wagner, 2017;Moosavi-Dezfooli et al., 2016;Madry et al., 2018;Kurakin et al., 2017).",
    "next": "However, adversarial attacks may be leveraged in other constructive ways to provide insights into how deep learning models learn data representations and make decisions."
  },
  {
    "index": 2753,
    "source_corpus_id": 220128067,
    "ref_id": "b1",
    "citation_corpus_id": 44134226,
    "start": 1724,
    "end": 1742,
    "title": "Hierarchical Neural Story Generation",
    "abstract": "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.",
    "prev": "Under review.",
    "curr": "Introduction\n\nSequence-to-sequence (seq2seq) learners [Sutskever et al., 2014] demonstrated remarkable performance in machine translation, story generation, and open-domain dialog [Sutskever et al., 2014, Fan et al., 2018, Adiwardana et al., 2020.",
    "next": "These capabilities, however, come at the expense of being hardly interpretable black-boxes, which spurred an interest in better understanding their inner working [Lake and Baroni, 2017, Gordon et al., 2019, Yu et al., 2019, McCoy et al., 2020."
  },
  {
    "index": 2754,
    "source_corpus_id": 211092198,
    "ref_id": "b7",
    "citation_corpus_id": 53218829,
    "start": 6499,
    "end": 6518,
    "title": "OF WIKIPEDIA: KNOWLEDGE-POWERED CONVERSATIONAL AGENTS",
    "abstract": "In open-domain dialogue intelligent agents should exhibit the use of knowledge, however there are few convincing demonstrations of this to date. The most popular sequence to sequence models typically \"generate and hope\" generic utterances that can be memorized in the weights of the model when mapping from input utterance(s) to output, rather than employing recalled knowledge as context. Use of knowledge has so far proved difficult, in part because of the lack of a supervised learning benchmark task which exhibits knowledgeable open dialogue with clear grounding. To that end we collect and release a large dataset with conversations directly grounded with knowledge retrieved from Wikipedia. We then design architectures capable of retrieving knowledge, reading and conditioning on it, and finally generating natural responses. Our best performing dialogue models are able to conduct knowledgeable discussions on open-domain topics as evaluated by automatic metrics and human evaluations, while our new benchmark allows for measuring further improvements in this important research direction. * Joint first authors.",
    "prev": "While documents are abundant on the Web, it is difficult to obtain large scale dialogues that are naturally grounded on the documents for learning of a neural generation model.",
    "curr": "To overcome the challenge, some recent work (Zhou et al., 2018b;Dinan et al., 2019) resorts to crowd-sourcing and builds benchmarks with the source of Wikipedia.",
    "next": "On the one hand, the datasets pave the way to the recent research on knowledge-grounded response generation/selection Lian et al., 2019;Li et al., 2019); on the other hand, we argue that there still a long way to go for application of the existing models in real scenarios, since (1) the models, especially those achieve state-of-the-art performance via sophisticated neural architectures, just overfit to the small training data (e.g., ∼ 18k dialogues)."
  },
  {
    "index": 2757,
    "source_corpus_id": 235377133,
    "ref_id": "b6",
    "citation_corpus_id": 52967399,
    "start": 2032,
    "end": 2053,
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke  Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. . 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. . 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In",
    "prev": "Experiments demonstrate the effectiveness of IAD for improving upon teachers in terms of adversarial robustness.",
    "curr": "†\n\nINTRODUCTION\n\nDeep Neural Networks (DNNs) have shown excellent performance on a range of tasks in computer vision (He et al., 2016) and natural language processing (Devlin et al., 2019).",
    "next": "Nevertheless, Szegedy et al."
  },
  {
    "index": 2759,
    "source_corpus_id": 258564452,
    "ref_id": "b93",
    "citation_corpus_id": 220968978,
    "start": 2911,
    "end": 2934,
    "title": "Hopfield Networks is All You Need",
    "abstract": "The transformer and BERT models pushed the performance on NLP tasks to new levels via their attention mechanism. We show that this attention mechanism is the update rule of a modern Hopfield network with continuous states. This new Hopfield network can store exponentially (with the dimension) many patterns, converges with one update, and has exponentially small retrieval errors. The number of stored patterns must be traded off against convergence speed and retrieval error. The new Hopfield network has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. Transformers learn an attention mechanism by constructing an embedding of patterns and queries into an associative space. Transformer and BERT models operate in their first layers preferably in the global averaging regime, while they operate in higher layers in metastable states. The gradient in transformers is maximal in the regime of metastable states, is uniformly distributed when averaging globally, and vanishes when a fixed point is near a stored pattern. Based on the Hopfield network interpretation, we analyzed learning of transformer and BERT architectures. Learning starts with attention heads that average and then most of them switch to metastable states. However, the majority of heads in the first layers still averages and can be replaced by averaging operations like the Gaussian weighting that we propose. In contrast, heads in the last layers steadily learn and seem to use metastable states to collect information created in lower layers. These heads seem to be a promising target for improving transformers. Neural networks that integrate Hopfield networks, that are equivalent to attention heads, outperform other methods on immune repertoire classification, where the Hopfield net stores several hundreds of thousands of patterns. We provide a new PyTorch layer called \"Hopfield\" which allows to equip deep learning architectures with modern Hopfield networks as new powerful concept comprising pooling, memory, and attention. The implementation is available at: https://github.com/ml-jku/ hopfield-layers arXiv:2008.02217v1 [cs.NE] 16 Jul 2020The update rule converges after one update for well separated patterns: Theorem 4. With query ξ, after one update the distance of the new point f (ξ) to the fixed point x * i is exponentially small in the separation ∆ i . The precise bounds using the Jacobian J = ∂f (ξ) ∂ξ and its value J m in the mean value theorem are:",
    "prev": "Since its introduction, the Hopfield network has been extended and studied widely by neuroscientists (Griniasty et al., 1993;Schneidman et al., 2006;Sridhar et al., 2021;Burns et al., 2022), physicists (Amit et al., 1985;Agliari et al., 2013;Leonetti et al., 2021), and computer scientists (Widrich et al., 2020;Millidge et al., 2022).",
    "curr": "Of particular interest to the machine learning community is the recent development of modern Hopfield networks (Krotov & Hopfield, 2016) and their close correspondence (Ramsauer et al., 2021) to the attention mechanism of Transformers (Vaswani et al., 2017).",
    "next": "An early (Amit et al., 1985;McEliece et al., 1987) and ongoing (Hillar & Tran, 2018) theme in the study of Hopfield networks has been their memory storage capacity, i.e., determining the number of memory patterns which can be reliably stored and later recalled via the dynamics."
  },
  {
    "index": 2762,
    "source_corpus_id": 247922520,
    "ref_id": "b34",
    "citation_corpus_id": 1957433,
    "start": 7592,
    "end": 7595,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "Pretraining weights is a dominant paradigm for transfer learning with deep models, in which pretrained model weights (from surrogate tasks) are used to initialize some subset of parameters in the model for the target task, which are then either (a) left frozen, or (b) finetuned.",
    "curr": "Pretraining deep models has been studied extensively in the unsupervised setting [24,25,26,27,28], and in the supervised setting was perhaps most popularized by ImageNet [29] pretraining [30,31,32,33], Various forms of pretraining have been ubiquitous in NLP [34,35,36,37,38,1,2].",
    "next": "For each target task, model architectures and/or training procedures may need to be developed that are composed of these pretrained parameters, for which domain expertise may be advantageous."
  },
  {
    "index": 2763,
    "source_corpus_id": 209936257,
    "ref_id": "b10",
    "citation_corpus_id": 2134321,
    "start": 2025,
    "end": 2043,
    "title": "Published as a conference paper at ICLR 2016 DEEP COMPRESSION: COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING",
    "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce \"deep compression\", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35× to 49× without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9× to 13×; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35×, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49× from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3× to 4× layerwise speedup and 3× to 7× better energy efficiency.",
    "prev": "This can be problematic, for instance, when deploying models onto devices over the air, where the bottleneck is often network speed, or onto devices holding many stored models, with only few used at a time.",
    "curr": "To make these models more practical, several authors have proposed to compress model parameters (Han et al., 2016;Louizos, Ullrich, et al., 2017;Molchanov et al., 2017;Havasi et al., 2019).",
    "next": "While other desiderata often exist, such as minimizing the number of layers or filters of the network, we focus here simply on model compression algorithms that 1. minimize compressed size while maintaining an acceptable classification accuracy, 2. are conceptually simple and easy to implement, and 3. can be scaled easily to large models."
  },
  {
    "index": 2764,
    "source_corpus_id": 259203582,
    "ref_id": "b31",
    "citation_corpus_id": 257039062,
    "start": 10245,
    "end": 10249,
    "title": "Published as a conference paper at ICLR 2023 SEMANTIC UNCERTAINTY: LINGUISTIC INVARIANCES FOR UNCERTAINTY ESTIMATION IN NATURAL LANGUAGE GENERATION",
    "abstract": "We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of 'semantic equivalence'-different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy-an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to 'off-the-shelf' language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines.",
    "prev": "Our methods are model agnostic and provide rigorous guarantees.",
    "curr": "Our conformal component selection ( §4.4) also relates to recent selfconsistency work that builds on the empirical observation that repeated similar samples are more likely to be correct [45; 72], and cross-sample entailment can approximate uncertainty [32].",
    "next": "Unlike previous work that uses a fixed number of re-samples and compares full outputs, we (1) introduce a dynamic stopping rule to reduce the number of samples, (2) extend this concept to semantically compare sub-components of long text outputs, and (3) conformalize the process to provide proper guarantees."
  },
  {
    "index": 2765,
    "source_corpus_id": 225068405,
    "ref_id": "b19",
    "citation_corpus_id": 6628106,
    "start": 15337,
    "end": 15341,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "Accordingly, the loss function is defined as\nL N SSN N = 1 N b N b j=1 q (j) −q (j) 1 + p (j) −p (j) 1 + q (j) −x (j) 1 + p (j) −ŷ (j) 1 ,(8)\nwhere N b = 512 is the batch size of the training samples.",
    "curr": "We use the Adam optimizer [20] with learning rate 0.05.",
    "next": "The learning rate is multiplied by 0.8 for every 10 epoches."
  },
  {
    "index": 2767,
    "source_corpus_id": 58028789,
    "ref_id": "b32",
    "citation_corpus_id": 7147309,
    "start": 1694,
    "end": 1715,
    "title": "SEQUENCE LEVEL TRAINING WITH RECURRENT NEURAL NETWORKS",
    "abstract": "Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster.",
    "prev": "Extensive experiments are conducted to validate the utility of the proposed approach, showing consistent improvements over a wide variety of NLP tasks, including machine translation, abstractive text summarization, and image captioning.",
    "curr": "INTRODUCTION\n\nSequence-to-sequence (Seq2Seq) models are widely used in various natural language processing tasks, such as machine translation (Bahdanau et al., 2015;Cho et al., 2014;Sutskever et al., 2014), text summarization (Chopra et al., 2016;Rush et al., 2015) and image captioning (Vinyals et al., 2015;Xu et al., 2015).",
    "next": "Typically, Seq2Seq models are based on an encoder-decoder architecture, with an encoder mapping a source sequence into a latent vector, and a decoder translating the latent vector into a target sequence."
  },
  {
    "index": 2770,
    "source_corpus_id": 247158269,
    "ref_id": "b3",
    "citation_corpus_id": 239009915,
    "start": 2777,
    "end": 2799,
    "title": "GRAPH NEURAL NETWORKS WITH LEARNABLE STRUCTURAL AND POSITIONAL REPRESENTATIONS",
    "abstract": "Graph neural networks (GNNs) have become the standard learning architectures for graphs. GNNs have been applied to numerous domains ranging from quantum chemistry, recommender systems to knowledge graphs and natural language processing. A major issue with arbitrary graphs is the absence of canonical positional information of nodes, which decreases the representation power of GNNs to distinguish e.g. isomorphic nodes and other graph symmetries. An approach to tackle this issue is to introduce Positional Encoding (PE) of nodes, and inject it into the input layer, like in Transformers. Possible graph PE are Laplacian eigenvectors. In this work, we propose to decouple structural and positional representations to make easy for the network to learn these two essential properties. We introduce a novel generic architecture which we call LSPE (Learnable Structural and Positional Encodings). We investigate several sparse and fully-connected (Transformer-like) GNNs, and observe a performance increase for molecular datasets, from 1.79% up to 64.14% when considering learnable PE for both GNN classes. 1",
    "prev": "An important example is the use of Laplacian eigenvectors to encode information about the structure of a graph or manifold (Belkin & Niyogi, 2003;Von Luxburg, 2007;Lévy, 2006).",
    "curr": "Positional encodings that involve Laplacian eigenvectors have recently been used to generalize Transformers to graphs (Kreuzer et al., 2021;Dwivedi & Bresson, 2021), and to improve the expressive power and empirical performance of graph neural networks (GNNs) (Dwivedi et al., 2022).",
    "next": "Furthermore, these eigenvectors are crucial for defining spectral operations on graphs that are foundational to graph signal processing and spectral GNNs (Ortega et al., 2018;Bruna et al., 2014)."
  },
  {
    "index": 2771,
    "source_corpus_id": 11591887,
    "ref_id": "b19",
    "citation_corpus_id": 931054,
    "start": 2297,
    "end": 2315,
    "title": "Bilingual Word Embeddings for Phrase-Based Machine Translation",
    "abstract": "We introduce bilingual word embeddings: semantic embeddings associated across two languages in the context of neural language models. We propose a method to learn bilingual embeddings from a large unlabeled corpus, while utilizing MT word alignments to constrain translational equivalence. The new embeddings significantly out-perform baselines in word semantic similarity. A single semantic similarity feature induced with bilingual embeddings adds near half a BLEU point to the results of NIST08 Chinese-English machine translation task.",
    "prev": "Increasing interest is now focused on bilingual vectors, in which words are aligned by their meaning, irrespective of the language of origin.",
    "curr": "Such vectors may drive improvements in machine translation (Zou et al., 2013), and enable language-agnostic text classifiers (Klementiev et al., 2012).",
    "next": "They can also be higher quality than monolingual vectors (Faruqui & Dyer, 2014)."
  },
  {
    "index": 2773,
    "source_corpus_id": 231951553,
    "ref_id": "b23",
    "citation_corpus_id": 3633127,
    "start": 2380,
    "end": 2403,
    "title": "Published as a conference paper at ICLR 2018 CGANS WITH PROJECTION DISCRIMINATOR",
    "abstract": "We propose a novel, projection based way to incorporate the conditional information into the discriminator of GANs that respects the role of the conditional information in the underlining probabilistic model. This approach is in contrast with most frameworks of conditional GANs used in application today, which use the conditional information by concatenating the (embedded) conditional vector to the feature vectors. With this modification, we were able to significantly improve the quality of the class conditional image generation on ILSVRC2012 (Im-ageNet) 1000-class image dataset from the current state-of-the-art result, and we achieved this with a single pair of a discriminator and a generator. We were also able to extend the application to super-resolution and succeeded in producing highly discriminative super-resolution images. This new structure also enabled high quality category transformation based on parametric functional transformation of conditional batch normalization layers in the generator. The code with Chainer (Tokui et al., 2015), generated images and pretrained models are available at https://github.com/pfnet-research/sngan_projection.",
    "prev": "The conditional information in cGANs is used to drive the generation and to enforce the correspondence between condition and sample.",
    "curr": "Various alternatives have been proposed for how the condition should be included in the discriminator (Miyato & Koyama, 2018;Reed et al., 2016) but the majority of frameworks provide it as an input, hoping that the sample's correlation with the condition will play a role in distinguishing between synthesized and genuine samples.",
    "next": "The main drawback of this approach is that it does not encourage the use of the conditional information and therefore its contribution can be diminished or even ignored."
  },
  {
    "index": 2774,
    "source_corpus_id": 67855277,
    "ref_id": "b32",
    "citation_corpus_id": 205514,
    "start": 5365,
    "end": 5386,
    "title": "DEEP MULTI-SCALE VIDEO PREDICTION BEYOND MEAN SQUARE ERROR",
    "abstract": "Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics. This is why pixel-space video prediction may be viewed as a promising avenue for unsupervised feature learning. In addition, while optical flow has been a very studied problem in computer vision for a long time, future frame prediction is rarely approached. Still, many vision applications could benefit from the knowledge of the next frames of videos, that does not require the complexity of tracking every pixel trajectory. In this work, we train a convolutional network to generate future frames given an input sequence. To deal with the inherently blurry predictions obtained from the standard Mean Squared Error (MSE) loss function, we propose three different and complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function. We compare our predictions to different published results based on recurrent neural networks on the UCF101 dataset.",
    "prev": "There are many papers that try to predict the future at the pixel level (see e.g., (Kitani et al., 2017) for a review).",
    "curr": "Some use a single static stochastic variable, as in a conditional VAE (Kingma & Welling, 2014), which is then \"decoded\" into a sequence using an RNN, either using a VAE-style loss (Walker et al., 2016;Xue et al., 2016) or a GAN-style loss (Vondrick et al., 2016b;Mathieu et al., 2016).",
    "next": "More recent work, based on VRNNs, uses temporal stochastic variables, e.g., the SV2P model of Babaeizadeh et al."
  },
  {
    "index": 2775,
    "source_corpus_id": 4560095,
    "ref_id": "b24",
    "citation_corpus_id": 65455367,
    "start": 13053,
    "end": 13056,
    "title": "ON THE CONVERGENCE OF ADAM AND BEYOND",
    "abstract": "Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSPROP, ADAM, ADADELTA, NADAM are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where ADAM does not converge to the optimal solution, and describe the precise problems with the previous analysis of ADAM algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with \"long-term memory\" of past gradients, and propose new variants of the ADAM algorithm which not only fix the convergence issues but often also lead to improved empirical performance.",
    "prev": "We demonstrate this equivalence empirically in Appendix B.\n\n\nConvergence of AggMo\n\nWe evaluate the convergence rate of AggMo in the setting of online convex programming, as proposed in Zinkevich [36].",
    "curr": "This is an increasingly common setting to analyze optimization algorithms tailored to machine learning [3,10,25].",
    "next": "We consider a sequence of unknown convex cost functions, f 1 (θ), ."
  },
  {
    "index": 2776,
    "source_corpus_id": 204512445,
    "ref_id": "b28",
    "citation_corpus_id": 44131019,
    "start": 14631,
    "end": 14649,
    "title": "Scaling Neural Machine Translation",
    "abstract": "Sequence to sequence learning models still require several days to reach state of the art performance on large benchmark datasets using a single machine. This paper shows that reduced precision and large batch training can speedup training by nearly 5x on a single 8-GPU machine with careful tuning and implementation. 1 On WMT'14 English-German translation, we match the accuracy ofVaswani et al. (2017)in under 5 hours when training on 8 GPUs and we obtain a new state of the art of 29.3 BLEU after training for 85 minutes on 128 GPUs. We further improve these results to 29.8 BLEU by training on the much larger Paracrawl dataset. On the WMT'14 English-French task, we obtain a state-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.",
    "prev": "The learning rate is warmed up over the first 10,000 updates to a peak value of 1 × 10 −5 , and then linearly decayed over a total of 250k updates.",
    "curr": "We train on 128 GPUs with a batch size of 3072 tokens per GPU giving a total batch size of 393k tokens (Ott et al., 2018).",
    "next": "Each token represents 10ms of audio data."
  },
  {
    "index": 2777,
    "source_corpus_id": 258960101,
    "ref_id": "b13",
    "citation_corpus_id": 182952848,
    "start": 3686,
    "end": 3709,
    "title": "GLTR: Statistical Detection and Visualization of Generated Text",
    "abstract": "The rapid improvement of language models has raised the specter of abuse of text generation systems. This progress motivates the development of simple methods for detecting generated text that can be used by and explained to non-experts. We develop GLTR, a tool to support humans in detecting whether a text was generated by a model. GLTR applies a suite of baseline statistical methods that can detect generation artifacts across common sampling schemes. In a human-subjects study, we show that the annotation scheme provided by GLTR improves the human detection-rate of fake text from 54% to 72% without any prior training. GLTR is open-source and publicly deployed, and has already been widely used to detect generated outputs.",
    "prev": " plagiarism (Bommasani et al., 2021).There have been instances where AI-generated scientific abstracts have managed to deceive scientists (Gao et al., 2022;Else, 2023), leading to a disruption in trust towards scientific knowledge.Unfortunately, the progress in detecting AI-generated text lags behind the rapid advancement of AI itself.",
    "curr": "As AI-generated text approaches high quality, effectively detecting such text presents fundamental difficulties.This has led to a recent debate on the detectability of AI-generated text (Chakraborty et al., 2023;Krishna et al., 2023;Sadasivan et al., 2023).Nevertheless, there is still a lack of practical methodology for AI-generated text detection, particularly in the era of ChatGPT.We aim to present a general, explainable, and robust detection method for LLMs, especially as these models continue to improve.Some existing detection methods utilize perturbation-based approaches like DetectGPT Preprint (Mitchell et al., 2023) or rank/entropy-based methods (Gehrmann et al., 2019;Solaiman et al., 2019;Ippolito et al., 2020).However, these detection tools fail when the token probability is not provided, as is the case with the OpenAI's GPT-3.5 series.Furthermore, the lack of details about how those most potent language models are developed poses an additional challenge in detecting them.This challenge will continue to escalate as these LLMs undergo continuous updates and advancements.",
    "next": "Hence, there is a pressing demand to effectively detect GPT-generated text to match the rapid advancements of LLMs.Moreover, when formulating the detection methodology, an essential focus lies on explainability, an aspect that is often absent in existing methods that solely provide a prediction devoid of supporting evidence.This aspect holds significant importance, especially in education, as it poses challenges for educators in comprehending the rationale behind specific decisions."
  },
  {
    "index": 2781,
    "source_corpus_id": 228372368,
    "ref_id": "b13",
    "citation_corpus_id": 1428702,
    "start": 37034,
    "end": 37053,
    "title": "Learning Word Vectors for Sentiment Analysis",
    "abstract": "Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term-document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.",
    "prev": "We use a subset of the Amazon Review Data (He & McAuley, 2016) for sentiment classification, aggregating all categories with more than 100k reviews from which we sample 200k reviews and split them into 160k training points and 40k test points.",
    "curr": "We use the IMDB Movie Review Sentiment dataset (Maas et al., 2011) which has 25k training samples and 25k test samples.",
    "next": "In addition, we use the Bias in Bios (De-Arteaga et al., 2019) dataset from which we create binary classification tasks to distinguish difficult pairs among frequently occurring occupations."
  },
  {
    "index": 2783,
    "source_corpus_id": 252692952,
    "ref_id": "b63",
    "citation_corpus_id": 52895589,
    "start": 8838,
    "end": 8854,
    "title": "HOW POWERFUL ARE GRAPH NEURAL NETWORKS?",
    "abstract": "Graph Neural Networks (GNNs) for representation learning of graphs broadly follow a neighborhood aggregation framework, where the representation vector of a node is computed by recursively aggregating and transforming feature vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs in capturing different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.",
    "prev": "RELATED WORKS\n\nNeural networks for learning 2D molecular representations.",
    "curr": "Graph Neural Network (GNN) is popularly used in molecular graph representation learning (Kipf & Welling, 2016;Hamilton et al., 2017;Gilmer et al., 2017;Xu et al., 2019;Veličković et al., 2018).",
    "next": "A GNN learns node and graph representations by recursively aggregating (i.e., message passing) and updating the node representations from neighbor representations."
  },
  {
    "index": 2784,
    "source_corpus_id": 91184200,
    "ref_id": "b0",
    "citation_corpus_id": 52940306,
    "start": 13222,
    "end": 13225,
    "title": "Workshop track -ICLR 2018 LOCAL EXPLANATION METHODS FOR DEEP NEURAL NETWORKS LACK SENSITIVITY TO PARAMETER VAL- UES",
    "abstract": "Explaining the output of a complicated machine learning model like a deep neural network (DNN) is a central challenge in machine learning. Several proposed local explanation methods address this issue by identifying what dimensions of a single input are most responsible for a DNN's output. The goal of this work is to assess the sensitivity of local explanations to DNN parameter values. Somewhat surprisingly, we find that DNNs with randomly-initialized weights produce explanations that are both visually and quantitatively similar to those produced by DNNs with learned weights. Our conjecture is that this phenomenon occurs because these explanations are dominated by the lower level features of a DNN, and that a DNN's architecture provides a strong prior which significantly affects the representations learned at these lower layers. NOTE: This work is now subsumed by our recent manuscript, Sanity Checks for Saliency Maps . Visualizing deep neural network decisions: Prediction difference analysis. arXiv preprint arXiv:1702.04595, 2017.",
    "prev": "[40] distinguish between sensitivity and implementation invariance.",
    "curr": "An interesting observation is that such methods seem to generate believable saliency maps even for networks with random weights [1].",
    "next": "[18] show that saliency methods do not produce analytically correct explanations for linear models and further reliability issues are discussed in [2,13,17]."
  },
  {
    "index": 2790,
    "source_corpus_id": 263609239,
    "ref_id": "b38",
    "citation_corpus_id": 224705257,
    "start": 42081,
    "end": 42097,
    "title": "Published as a conference paper at ICLR 2021 FOURIER NEURAL OPERATOR FOR PARAMETRIC PARTIAL DIFFERENTIAL EQUATIONS",
    "abstract": "The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and Navier-Stokes equation. The Fourier neural operator is the first ML-based method to successfully model turbulent flows with zero-shot super-resolution. It is up to three orders of magnitude faster compared to traditional PDE solvers. Additionally, it achieves superior accuracy compared to previous learning-based solvers under fixed resolution.",
    "prev": "A last theoretical area is that of gradient-based meta-learning, which studies how to initialize and tune other parameters of gradient descent and related methods (Khodak et al., 2019;Denevi et al., 2019;Saunshi et al., 2020;Chen & Hazan, 2023).This field focuses on learning-theoretic notions of cost such as regret or statistical risk.Furthermore, their guarantees are usually on the error after a fixed number of gradient steps rather than the number of iterations required to converge; targeting the former can be highly suboptimal in scientific computing applications (Arisaka & Li, 2023).This latter work, which connects meta-learning and data-driven scientific computing, analyzes specific case studies for accelerating numerical solvers, whereas we focus on a general learning guarantee.",
    "curr": "Empirically, there are many learned solvers (Luz et al., 2020;Taghibakhshi et al., 2021;Li et al., 2023) and even full simulation replacements (Karniadakis et al., 2021;Li et al., 2021); to our knowledge, theoretical studies of the latter have focused on expressivity (Marwah et al., 2021).Amortizing the cost on future simulations (Amos, 2023), these approaches use offline computation to train models that integrate directly with solvers or avoid solving linear systems altogether.In contrast, the methods we propose are online and lightweight, both computationally and in terms of implementation; unlike many deep learning approaches, the additional computation scales slowly with dimension and needs only black-box access to existing solvers.As a result, our methods can be viewed as reasonable baselines.",
    "next": "Finally, note that improving the performance of linear solvers across a sequence of related instances has seen a lot of study in the scientific computing literature (Parks et al., 2006;Tebbens & Tůma, 2007;Elbouyahyaoui et al., 2021).To our knowledge, this work does not give explicit guarante"
  },
  {
    "index": 2791,
    "source_corpus_id": 227151564,
    "ref_id": "b28",
    "citation_corpus_id": 2428314,
    "start": 3144,
    "end": 3162,
    "title": "Published as a conference paper at ICLR 2017 CATEGORICAL REPARAMETERIZATION WITH GUMBEL-SOFTMAX",
    "abstract": "Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification. * Work done during an internship at Google Brain.",
    "prev": "The next state of the RNN is then drawn from this distribution.",
    "curr": "We use the Gumbel softmax trick (Gumbel, 1954;Kendall & Gal, 2017;Jang et al., 2017) to perform Monte-Carlo gradient estimation.",
    "next": "Inspired by the effectiveness of temperature scaling (Guo et al., 2017) which is usually applied to trained models, we learn the temperature τ of the Gumbel softmax distribution during training to control the concentration of the state transition distribution."
  },
  {
    "index": 2792,
    "source_corpus_id": 252917661,
    "ref_id": "b25",
    "citation_corpus_id": 6610705,
    "start": 1790,
    "end": 1809,
    "title": "UNROLLED GENERATIVE ADVERSARIAL NETWORKS",
    "abstract": "We introduce a method to stabilize Generative Adversarial Networks (GANs) by defining the generator objective with respect to an unrolled optimization of the discriminator. This allows training to be adjusted between using the optimal discriminator in the generator's objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions. We show how this technique solves the common problem of mode collapse, stabilizes training of GANs with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator. * Work done as a member of the Google Brain Residency program (g.co/brainresidency) † Work completed as part of a Google Brain internship",
    "prev": "Bringing together theoretical analysis and empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks.",
    "curr": "1\n\nINTRODUCTION\n\nAmong existing generative models, GAN (Goodfellow et al., 2014) suffers from the instability issue (Salimans et al., 2016), subjecting to mode collapse (Metz et al., 2017); VAE (Kingma & Welling, 2014) has to rely on surrogate objectives to approximate maximum likelihood training and Flow-based models (Dinh et al., 2017) has to use specialized architectures to construct reversible transform.",
    "next": "Diffusion models (Ho et al., 2020;Nichol & Dhariwal, 2021) have circumvented several of these limitations and emerged as a new paradigm for generative models, theoretically underpinned by non-equilibrium thermodynamics (Sohl-Dickstein et al., 2015) and score-matching network (Song & Ermon, 2019)."
  },
  {
    "index": 2793,
    "source_corpus_id": 263831633,
    "ref_id": "b36",
    "citation_corpus_id": 237416585,
    "start": 9084,
    "end": 9101,
    "title": "Published as a conference paper at ICLR 2022 FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS",
    "abstract": "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning-finetuning language models on a collection of datasets described via instructions-substantially improves zeroshot performance on unseen tasks.",
    "prev": "Scalable Oversight & Self-Alignment AI alignment traditionally relies heavily on extensive human annotations.",
    "curr": "Primary Supervised Fine-Tuning (SFT) sources for response demonstrations include those curated from existing NLP datasets (Sanh et al., 2021;Wei et al., 2021;Chung et al., 2022b;Wang et al., 2022b) and those specifically crafted by humans for instruction tuning (Databricks, 2023;Köpf et al., 2023;Zhou et al., 2023;Ouyang et al., 2022 Touvron et al.",
    "next": "(2023b)), online human preferences are collected to train a reward model to further fine-tune the SFT-trained model (Leike et al., 2018)."
  },
  {
    "index": 2794,
    "source_corpus_id": 258331830,
    "ref_id": "b49",
    "citation_corpus_id": 249625698,
    "start": 17367,
    "end": 17371,
    "title": "A UNIFIED APPROACH TO REINFORCEMENT LEARN-ING, QUANTAL RESPONSE EQUILIBRIA, AND TWO-PLAYER ZERO-SUM GAMES",
    "abstract": "This work studies an algorithm, which we call magnetic mirror descent, that is inspired by mirror descent and the non-Euclidean proximal gradient algorithm.Our contribution is demonstrating the virtues of magnetic mirror descent as both an equilibrium solver and as an approach to reinforcement learning in two-player zero-sum games.These virtues include: 1) Being the first quantal response equilibria solver to achieve linear convergence for extensive-form games with first order feedback; 2) Being the first standard reinforcement learning algorithm to achieve empirically competitive results with CFR in tabular settings; 3) Achieving favorable performance in 3x3 Dark Hex and Phantom Tic-Tac-Toe as a self-play deep reinforcement learning algorithm.˚Equal contribution",
    "prev": "In such settings, neither policy iteration-nor mirror descent-based approaches yield useful policies-instead, these approaches tend to cycle, diverge, or even exhibiting formally chaotic behavior already in simple games such as rock-paper-scissors [17].",
    "curr": "In practice, this issue can be resolved using magnetic mirror descent [50], which is an extension of mirror descent with additional proximal regularization to a magnet that dampens these cycles.",
    "next": "As with mirror descent, in the context of sequential decision making, a natural means of leveraging magnetic mirror descent is to instantiate simultaneous updaters U sync local at each decision point:\nU sync local : (π,q) → arg max σ q, σ − 1 η KL(σ,π) − αKL(σ, ρ),\nwhere α is a regularization temperature and ρ is a local magnet; in the case of discrete action spaces, this update reduce to the following closed form:\nU sync local (π,q) ∝ [πe ηq ρ ηα ] 1 1+αη ."
  },
  {
    "index": 2795,
    "source_corpus_id": 263620510,
    "ref_id": "b9",
    "citation_corpus_id": 225039882,
    "start": 2773,
    "end": 2799,
    "title": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. 1 arXiv:2010.11929v2 [cs.CV] 3 Jun 2021Published as a conference paper at ICLR 2021 inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.However, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks.RELATED WORKTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global selfattention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.",
    "prev": "rom both the choice of rank and the network architecture.We present adaptation results using GPT-2 and ViT in natural language and computer vision tasks.NOLA performs as well as, or better than models with equivalent parameter counts.Furthermore, we demonstrate that we can halve the parameters in larger models compared to LoRA with rank one, without sacrificing performance.Our code is available here: https://github.com/UCDvision/NOLA* Equal Contribution.",
    "curr": "INTRODUCTION\n\nLarge pre-trained neural networks have exhibited remarkable generalization abilities across a diverse range of downstream tasks in both natural language processing and computer vision, achieving unprecedented data efficiency.For instance, large language models have demonstrated the capability for few-shot generalization (Brown et al., 2020) across a variety of tasks, including translation, question-answering, cloze tasks, and reasoning.Similarly, in DINOv2, (Oquab et al., 2023) showcase how a large pre-trained ViT model (Dosovitskiy et al., 2020) with more than 1B parameters yields superior all-purpose visual features for a variety of downstream benchmark tasks at both image and pixel levels.Typically, these pre-trained large models are adapted to downstream tasks through fine-tuning of their parameters.However, fine-tuning and storing the entire set of model parameters for each task incurs a significant storage cost (e.g., 350GB for .This challenge has spurred a considerable body of recent works focusing on parameter-efficient fine-tuning of large models (Hu et al., 2021;Xu et al., 2023;Dettmers et al., 2023;Chen et al., 2022;Sung et al., 2022b).",
    "next": "Inspired by the low intrinsic dimensionality of over-parameterized networks' optimal parameters (Li et al., 2018;Aghajanyan et al., 2021), (Hu et al., 2021) proposed a seminal hypothesis that the change in weights during model adaptation/finetuning has a low \"intrinsic rank,\" leading to the development of Low-Rank Adaptation (LoRA).In essence, LoRA enables the indirect training of a"
  },
  {
    "index": 2797,
    "source_corpus_id": 220525308,
    "ref_id": "b7",
    "citation_corpus_id": 55481903,
    "start": 2372,
    "end": 2375,
    "title": "AN EMPIRICAL STUDY OF EXAMPLE FORGETTING DURING DEEP NEURAL NETWORK LEARNING",
    "abstract": "Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks. Our goal is to understand whether a related phenomenon occurs when data does not undergo a clear distributional shift. We define a \"forgetting event\" to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning. Across several benchmark data sets, we find that: (i) certain examples are forgotten with high frequency, and some not at all; (ii) a data set's (un)forgettable examples generalize across neural architectures; and (iii) based on forgetting dynamics, a significant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance.",
    "prev": "More specifically, if our machine learning model is trained on a sequence of tasks, accuracy on earlier tasks drops significantly.",
    "curr": "The catastrophic forgetting problem manifests in many sub-domains of machine learning including continual learning [6], multi-task learning and transfer [7] and even in standard supervised learning through input distribution shift [8,9,10,11,12] and data augmentation [13].",
    "next": "Catastrophic forgetting has been an important focus of recent research in the context of deep learning, with numerous methods proposed to help mitigate forgetting [14,6,15,16,17,18,19]."
  },
  {
    "index": 2800,
    "source_corpus_id": 221785486,
    "ref_id": "b9",
    "citation_corpus_id": 8227328,
    "start": 2436,
    "end": 2459,
    "title": "Inductive Bias of Deep Convolutional Networks through Pooling Geometry",
    "abstract": "Our formal understanding of the inductive bias that drives the success of convolutional networks on computer vision tasks is limited. In particular, it is unclear what makes hypotheses spaces born from convolution and pooling operations so suitable for natural images. In this paper we study the ability of convolutional arithmetic circuits to model correlations among regions of their input. Correlations are formalized through the notion of separation rank, which for a given input partition, measures how far a function is from being separable. We show that a polynomially sized deep network supports exponentially high separation ranks for certain input partitions, while being limited to polynomial separation ranks for others. The network's pooling geometry effectively determines which input partitions are favored, thus serves as a means for controlling the inductive bias. Contiguous pooling windows as commonly employed in practice favor interleaved partitions over coarse ones, orienting the inductive bias towards the statistics of natural images. In addition to analyzing deep networks, we show that shallow ones support only linear separation ranks, and by this gain insight into the benefit of functions brought forth by depth -they are able to efficiently model strong correlation under favored partitions of the input.",
    "prev": "Thanks to their importance and difficulty, such problems, which we broadly refer to as learning from structure, have recently developed into an exciting and promising application area for deep learning (Graves et al., 2020;Ingraham et al., 2019;Pereira et al., 2016;Townshend et al., 2019;Won et al., 2019).",
    "curr": "Successful applications of deep learning are often driven by techniques that leverage the problem structure of the domain-for example, convolutions in computer vision (Cohen & Shashua, 2017) and attention in natural language processing (Vaswani et al., 2017).",
    "next": "What are the relevant considerations in the domain of learning from structure?"
  },
  {
    "index": 2803,
    "source_corpus_id": 257631600,
    "ref_id": "b4",
    "citation_corpus_id": 208637067,
    "start": 40121,
    "end": 40142,
    "title": "Published as a conference paper at ICLR 2020 SIMPLIFIED ACTION DECODER FOR DEEP MULTI-AGENT REINFORCEMENT LEARNING",
    "abstract": "In recent years we have seen fast progress on a number of benchmark problems in AI, with modern methods achieving near or super human performance in Go, Poker and Dota. One common aspect of all of these challenges is that they are by design adversarial or, technically speaking, zero-sum. In contrast to these settings, success in the real world commonly requires humans to collaborate and communicate with others, in settings that are, at least partially, cooperative. In the last year, the card game Hanabi has been established as a new benchmark environment for AI to fill this gap. In particular, Hanabi is interesting to humans since it is entirely focused on theory of mind, i.e., the ability to effectively reason over the intentions, beliefs and point of view of other agents when observing their actions. Learning to be informative when observed by others is an interesting challenge for Reinforcement Learning (RL): Fundamentally, RL requires agents to explore in order to discover good policies. However, when done naively, this randomness will inherently make their actions less informative to others during training. We present a new deep multi-agent RL method, the Simplified Action Decoder (SAD), which resolves this contradiction exploiting the centralized training phase. During training SAD allows other agents to not only observe the (exploratory) action chosen, but agents instead also observe the greedy action of their team mates. By combining this simple intuition with best practices for multi-agent learning, SAD establishes a new SOTA for learning methods for 2-5 players on the self-play part of the Hanabi challenge. Our ablations show the contributions of SAD compared with the best practice components. All of our code and trained agents are available at Published as a conference paper at ICLR 2020 cars will likely need to understand the point of view, intents and beliefs of other traffic participants in order to deal with highly interactive settings such as 4-way crossing or dense traffic in cities.Hanabi is a fully cooperative, partially-observable card game that has recently been proposed as a new benchmark challenge problem for AI research(Bard et al., 2019)to fill the gap around ToM. In Hanabi, players need to find conventions that allow them to effectively exchange information from their local observations through their actions, taking advantage of the fact that actions are observed by all team mates.Most prior state-of-the-art agents for Hanabi were developed using handcrafted algorithms, which beat off-the-shelf deep multi-agent RL methods by a large margin. This makes intuitive sense: Beyond the \"standard\" multi-agent challenges of credit assignment, nonstationarity and joint exploration, learning an informative policy presents an additional fundamentally new conflict. On the one hand, an RL agent needs to explore in order to discover good policies through trial and error. On the other hand, when carried out naively, this exploration will add noise to the policy of the agent during the training process, making their actions strictly less informative to their team mates.",
    "prev": "To begin with, our approach assumes access to an environment simulator and a (perfect) belief to perform MI computation and OBL respectively.",
    "curr": "While there are recent approaches in learning environment models (Wang et al., 2022) and belief models (Hu & Foerster, 2019), how well these learned components work with each other remains unexplored.",
    "next": "Advances in these areas would improve the overall robustness of our proposed framework."
  },
  {
    "index": 2805,
    "source_corpus_id": 253237390,
    "ref_id": "b2",
    "citation_corpus_id": 52908393,
    "start": 11608,
    "end": 11633,
    "title": "CEM-RL: Combining evolutionary and gradient-based methods for policy search",
    "abstract": "Deep neuroevolution and deep reinforcement learning (deep RL) algorithms are two popular approaches to policy search. The former is widely applicable and rather stable, but suffers from low sample efficiency. By contrast, the latter is more sample efficient, but the most sample efficient variants are also rather unstable and highly sensitive to hyper-parameter setting. So far, these families of methods have mostly been compared as competing tools. However, an emerging approach consists in combining them so as to get the best of both worlds. Two previously existing combinations use either an ad hoc evolutionary algorithm or a goal exploration process together with the Deep Deterministic Policy Gradient (ddpg) algorithm, a sample efficient off-policy deep RL algorithm. In this paper, we propose a different combination scheme using the simple cross-entropy method (cem) and Twin Delayed Deep Deterministic policy gradient (td3), another off-policy deep RL algorithm which improves over ddpg. We evaluate the resulting method, cem-rl, on a set of benchmarks classically used in deep RL. We show that cem-rl benefits from several advantages over its competitors and offers a satisfactory trade-off between performance and sample efficiency.",
    "prev": "In each iteration, all agents interact with the environment for e episodes to obtain Monte Carlo (MC) estimates of policy fitness {f (π 1 ), f (π 2 ), ..., f (π n )} where f (π) = 1 e e i=1 [ T t=0 r t | π].",
    "curr": "The policy with higher fitness is more likely to be selected as parents to produce the next generation in many ways such as Genetic Algorithm (GA) (Mitchell, 1998) and Cross-Entropy Method (CEM) (Pourchot & Sigaud, 2019).",
    "next": "With GA, offspring are generated by applying genetic operators: the parents π i and π j are selected randomly to produce offspring π i and π j by performing the crossover operator, i.e., π i , π j = Crossover(π i , π j ) or the mutation operator π i = Mutation(π i )."
  },
  {
    "index": 2806,
    "source_corpus_id": 50773706,
    "ref_id": "b2",
    "citation_corpus_id": 1740355,
    "start": 2538,
    "end": 2559,
    "title": "Under review as a conference paper at ICLR 2017 DESIGNING NEURAL NETWORK ARCHITECTURES USING REINFORCEMENT LEARNING",
    "abstract": "At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We propose a meta-modeling approach based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using Q-learning with angreedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks. , et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015. A high-throughput screening approach to discovering good forms of biologically inspired visual representation.",
    "prev": "Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and error-prone process.",
    "curr": "Because of this, there is growing interest in automatic architecture search methods (Baker et al., 2017a;Zoph & Le, 2017;Real et al., 2017).",
    "next": "Some of the architectures found in an automated way have already achieved similar predictive performance as the best manually-designed ones; however, current algorithms for finding these architectures require enormous computational resources often in the range of thousands of GPU days."
  },
  {
    "index": 2807,
    "source_corpus_id": 247446904,
    "ref_id": "b12",
    "citation_corpus_id": 221785486,
    "start": 2659,
    "end": 2677,
    "title": "Published as a conference paper at ICLR 2021 LEARNING FROM PROTEIN STRUCTURE WITH GEOMETRIC VECTOR PERCEPTRONS",
    "abstract": "Learning on 3D structures of large biomolecules is emerging as a distinct area in machine learning, but there has yet to emerge a unifying network architecture that simultaneously leverages the geometric and relational aspects of the problem domain. To address this gap, we introduce geometric vector perceptrons, which extend standard dense layers to operate on collections of Euclidean vectors. Graph neural networks equipped with such layers are able to perform both geometric and relational reasoning on efficient representations of macromolecules. We demonstrate our approach on two important problems in learning from protein structure: model quality assessment and computational protein design. Our approach improves over existing classes of architectures on both problems, including state-ofthe-art convolutional neural networks and graph neural networks. We release our code at https://github.com/drorlab/gvp.",
    "prev": "As functional annotation of a new protein sequence remains costly and time-consuming, accurate and efficient in silico protein function annotation methods are needed to bridge the existing sequence-function gap.",
    "curr": "Since a large number of protein functions are governed by their folded structures, several datadriven approaches rely on learning representations of the protein structures, which then can be used for a variety of tasks such as protein design (Ingraham et al., 2019;Strokach et al., 2020;Cao et al., 2021;Jing et al., 2021), structure classification (Hermosilla et al., 2021), model quality assessment (Baldassarre et al., 2021;Derevyanko et al., 2018), and function prediction (Gligorijević et al., 2021).",
    "next": "Due to the challenge of experimental protein structure determination, the number of reported protein structures is orders of magnitude lower than the size of datasets in other machine learning application domains."
  },
  {
    "index": 2808,
    "source_corpus_id": 6079627,
    "ref_id": "b1",
    "citation_corpus_id": 18323900,
    "start": 22906,
    "end": 22927,
    "title": "Bounding the Test Log-Likelihood of Generative Models",
    "abstract": "Several interesting generative learning algorithms involve a complex probability distribution over many random variables, involving intractable normalization constants or latent variable marginalization. Some of them may not have even an analytic expression for the unnormalized probability function and no tractable approximation. This makes it difficult to estimate the quality of these models, once they have been trained, or to monitor their quality (e.g. for early stopping) while training. A previously proposed method is based on constructing a non-parametric density estimator of the model's probability function from samples generated by the model. We revisit this idea, propose a more efficient estimator, and prove that it provides a lower bound on the true test log-likelihood and an unbiased estimator as the number of generated samples goes to infinity, although one that incorporates the effect of poor mixing. We further propose a biased variant of the estimator that can be used reliably with a finite number of samples for the purpose of model comparison.",
    "prev": "These RBMs are able to generate much higher-quality samples from the data distribution, however, the partition function can no longer be computed exactly.",
    "curr": "In order to evaluate the model quantitatively, we estimated the test log-likelihood using the Conservative Samplingbased Likelihood estimator (CSL) (Bengio et al., 2013) and annealed importance sampling (AIS) (Salakhutdinov & Murray, 2008).",
    "next": "Given well-defined conditional probabilities P (v|h) of a model and a set of latent variable samples S collected from a Markov chain, CSL computes logf (v) = log mean h∈S P (v|h)."
  },
  {
    "index": 2809,
    "source_corpus_id": 19340026,
    "ref_id": "b18",
    "citation_corpus_id": 604334,
    "start": 6550,
    "end": 6572,
    "title": "Intriguing properties of neural networks",
    "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
    "prev": "AIPs are small perturbations over the input such that the network is mislead.",
    "curr": "Research on this topic has flourished recently after it was shown that the needed amount of perturbation to completely mislead an image classifier is nearly invisible (Szegedy et al., 2014;Goodfellow et al., 2015;Moosavi-Dezfooli et al., 2017).",
    "next": "Most effective AIPs require gradients of the target network."
  },
  {
    "index": 2811,
    "source_corpus_id": 49654320,
    "ref_id": "b45",
    "citation_corpus_id": 4043645,
    "start": 10346,
    "end": 10362,
    "title": "Published as a conference paper at ICLR 2018 VARIANCE REDUCTION FOR POLICY GRADIENT WITH ACTION-DEPENDENT FACTORIZED BASELINES",
    "abstract": "Policy gradient methods have enjoyed great success in deep reinforcement learning but suffer from high variance of gradient estimates. The high variance problem is particularly exasperated in problems with long horizons or high-dimensional action spaces. To mitigate this issue, we derive a bias-free action-dependent baseline for variance reduction which fully exploits the structural form of the stochastic policy itself and does not make any additional assumptions about the MDP. We demonstrate and quantify the benefit of the action-dependent baseline through both theoretical analysis as well as numerical results, including an analysis of the suboptimality of the optimal state-dependent baseline. The result is a computationally efficient policy gradient algorithm, which scales to high-dimensional control problems, as demonstrated by a synthetic 2000-dimensional target matching task. Our experimental results indicate that action-dependent baselines allow for faster learning on standard reinforcement learning benchmarks and highdimensional hand manipulation and synthetic tasks. Finally, we show that the general idea of including additional information in baselines for improved variance reduction can be extended to partially observed and multi-agent tasks.Published as a conference paper at ICLR 2018 removing the influence of future actions from the total reward. A better baseline, which predicts the average performance more accurately, will lead to lower variance of the gradient estimator.",
    "prev": "(1) becomes E ρπ,π θ [∇ θ log π θ (a|s) (Q π θ (s, a) − b(s))].",
    "curr": "While an optimal baseline exists (Greensmith et al., 2004;Wu et al., 2018), it is hard to estimate and often replaced by the value function b(s t ) = V π (s t ) (Sutton & Barto, 1998;Mnih et al., 2016).",
    "next": "Stochastic gradient descent using Eq."
  },
  {
    "index": 2812,
    "source_corpus_id": 57761150,
    "ref_id": "b12",
    "citation_corpus_id": 8348149,
    "start": 10556,
    "end": 10577,
    "title": "Learning Effective and Interpretable Semantic Models using Non-Negative Sparse Embedding",
    "abstract": "In this paper, we introduce an application of matrix factorization to produce corpus-derived, distributional models of semantics that demonstrate cognitive plausibility. We find that word representations learned by Non-Negative Sparse Embedding (NNSE), a variant of matrix factorization, are sparse, effective, and highly interpretable. To the best of our knowledge, this is the first approach which yields semantic representation of words satisfying these three desirable properties. Though extensive experimental evaluations on multiple real-world tasks and datasets, we demonstrate the superiority of semantic models learned by NNSE over other state-of-the-art baselines.",
    "prev": "We estimate semantic features from behavioral data alone, using the probabilistic model described in the next section.",
    "curr": "In our model, each feature/dimension x if in the vector x i is real and non-negative, so as to make it interpretable as the degree to which the aspect of meaning it represents is present and influences subject behavior (Murphy et al., 2012).",
    "next": "Further, we expect features/dimensions to be sparse McRae et al."
  },
  {
    "index": 2816,
    "source_corpus_id": 257039062,
    "ref_id": "b5",
    "citation_corpus_id": 212747810,
    "start": 21667,
    "end": 21688,
    "title": "Calibration of Pre-trained Transformers",
    "abstract": "Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT (Devlin et al., 2019)  and RoBERTa  (Liu et al., 2019)  in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging outof-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pretrained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5× lower;(2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain. 1",
    "prev": "We address the challenges of sampling and variable-length generation using specific details of our sampling procedure in Section 4.",
    "curr": "RELATED WORK\n\nPrior work on uncertainty in foundation models for NLP has largely focused on the calibration of classifiers (Jiang et al., 2021;Desai & Durrett, 2020) and text regressors (Glushkova et al., 2021;Wang et al., 2022).",
    "next": "These settings, are analogous to classification or regression settings in other modalities like vision, and conventional uncertainty measures like MC dropout or Deep Ensembles can be applied without modification (see Section 2 for a discussion of uncertainty in deep learning in general)."
  },
  {
    "index": 2817,
    "source_corpus_id": 231985673,
    "ref_id": "b45",
    "citation_corpus_id": 51928102,
    "start": 3563,
    "end": 3580,
    "title": "STRUCTURED ADVERSARIAL ATTACK: TOWARDS GENERAL IMPLEMENTATION AND BETTER INTERPRETABILITY",
    "abstract": "When generating adversarial examples to attack deep neural networks (DNNs), p norm of the added perturbation is usually used to measure the similarity between original image and adversarial example. However, such adversarial attacks perturbing the raw input spaces may fail to capture structural information hidden in the input. This work develops a more general attack model, i.e., the structured attack (StrAttack), which explores group sparsity in adversarial perturbations by sliding a mask through images aiming for extracting key spatial structures. An ADMM (alternating direction method of multipliers)-based framework is proposed that can split the original problem into a sequence of analytically solvable subproblems and can be generalized to implement other attacking methods. Strong group sparsity is achieved in adversarial perturbations even with the same level of p -norm distortion (p ∈ {1, 2, ∞}) as the state-of-the-art attacks. We demonstrate the effectiveness of StrAttack by extensive experimental results on MNIST, CIFAR-10 and ImageNet. We also show that StrAttack provides better interpretability (i.e., better correspondence with discriminative image regions) through adversarial saliency map (Papernot et al., 2016b) and class activation map (Zhou et al., 2016).",
    "prev": "It has successfully been applied to use cases such as object detection , medical image analysis (Maicas et al., 2018), and language modeling (Huang et al., 2018).",
    "curr": "In addition to generalization-ability, recent works (Yin et al., 2018;Goldblum et al., 2019; investigated MAML from another fundamental perspective, adversarial robustness, given by the capabilities of a model defending against adversarially perturbed inputs (known as adversarial examples/attacks) (Goodfellow et al., 2014;Xu et al., 2019b).",
    "next": "The challenge of lacking robustness of deep learning (DL) models has gained increasing interest and attention."
  },
  {
    "index": 2819,
    "source_corpus_id": 252668296,
    "ref_id": "b7",
    "citation_corpus_id": 6706414,
    "start": 3698,
    "end": 3723,
    "title": "Published as a conference paper at ICLR 2015 EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES",
    "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples-inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
    "prev": "Our code is publicly available at https://github.com/uiuc-arc/CGT.",
    "curr": "INTRODUCTION\n\nDespite the widespread success of deep neural networks (DNNs), they remain surprisingly susceptible to misclassification when small adversarial changes are applied to correctly classified inputs (Goodfellow et al., 2015;Kurakin et al., 2018).",
    "next": "This phenomenon is especially concerning as DNNs are increasingly being deployed in many safety-critical domains, such as autonomous driving (Bojarski et al., 2016;Sitawarin et al., 2018) and medical imaging (Finlayson et al., 2019)."
  },
  {
    "index": 2820,
    "source_corpus_id": 244714159,
    "ref_id": "b15",
    "citation_corpus_id": 224705257,
    "start": 2736,
    "end": 2753,
    "title": "Published as a conference paper at ICLR 2021 FOURIER NEURAL OPERATOR FOR PARAMETRIC PARTIAL DIFFERENTIAL EQUATIONS",
    "abstract": "The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and Navier-Stokes equation. The Fourier neural operator is the first ML-based method to successfully model turbulent flows with zero-shot super-resolution. It is up to three orders of magnitude faster compared to traditional PDE solvers. Additionally, it achieves superior accuracy compared to previous learning-based solvers under fixed resolution.",
    "prev": "For most real-world problems, the lack of a closed-form solution requires using computationally expensive numerical solvers, sometimes consuming millions of core hours and terabytes of storage (Hosseini et al., 2016).",
    "curr": "Recently, machine learning methods have been proposed to replace part (Kochkov et al., 2021) or all (Li et al., 2021a) of a numerical solver.",
    "next": "Of particular interest are Fourier Neural Operators (FNOs) (Li et al., 2021a), which are neural networks that can be trained end-to-end to learn a mapping between infinite-dimensional function spaces."
  },
  {
    "index": 2821,
    "source_corpus_id": 211132423,
    "ref_id": "b6",
    "citation_corpus_id": 28202810,
    "start": 4806,
    "end": 4828,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "Further, these strategies do not guide how strongly we should correct for overestimation bias, nor how to determine-or controlthe level of bias.",
    "curr": "The overestimation bias also appears in the actor-critic setting (Fujimoto et al., 2018;Haarnoja et al., 2018).",
    "next": "For example, Fujimoto et al."
  },
  {
    "index": 2824,
    "source_corpus_id": 6667083,
    "ref_id": "b3",
    "citation_corpus_id": 6706414,
    "start": 1528,
    "end": 1553,
    "title": "Published as a conference paper at ICLR 2015 EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES",
    "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples-inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
    "prev": "Very deep convolutional networks for large-scale image\n\nINTRODUCTION\n\nImages can undergo slight yet pathological modifications causing machine learning systems to misclassify, all while humans barely can notice these perturbations.",
    "curr": "These types of manipulated images are adversarial images (Goodfellow et al., 2015), and their existence demonstrates frailties in machine learning classifiers and a disconnect between human and computer vision.",
    "next": "This unexpected divide can allow attackers complete leverage over some deep learning systems."
  },
  {
    "index": 2826,
    "source_corpus_id": 253158053,
    "ref_id": "b29",
    "citation_corpus_id": 43964415,
    "start": 3439,
    "end": 3451,
    "title": "Local SGD Converges Fast and Communicates Little",
    "abstract": "Mini-batch stochastic gradient descent (SGD) is the state of the art in large scale parallel machine learning, but its scalability is limited by a communication bottleneck. Recent work proposed local SGD, i.e. running SGD independently in parallel on different workers and averaging only once in a while. This scheme shows promising results in practice, but eluded thorough theoretical analysis.We prove concise convergence rates for local SGD on convex problems and show that it converges at the same rate as mini-batch SGD in terms of number of evaluated gradients, that is, the scheme achieves linear speed-up in the number of workers and mini-batch size. Moreover, the number of communication rounds can be reduced up to a factor of T 1/2 -where T denotes the number of total steps-compared to mini-batch SGD.",
    "prev": "FEDAVG trains the global model in an iterative manner, between parallel local model training at the clients and global model aggregation at the server.",
    "curr": "FEDAVG is easy to implement and enjoys theoretical guarantees of convergence (Zhou & Cong, 2017;Stich, 2019;Haddadpour & Mahdavi, 2019;Li et al., 2020c;Zhao et al., 2018).",
    "next": "Its performance, however, can degrade drastically when clients' data are not IID -clients' data are often collected individually and doomed to be non-IID."
  },
  {
    "index": 2827,
    "source_corpus_id": 258564695,
    "ref_id": "b24",
    "citation_corpus_id": 15002492,
    "start": 4068,
    "end": 4089,
    "title": "SPEEDING-UP CONVOLUTIONAL NEURAL NETWORKS USING FINE-TUNED CP-DECOMPOSITION",
    "abstract": "We propose a simple two-step approach for speeding up convolution layers within large convolutional neural networks based on tensor decomposition and discriminative fine-tuning. Given a layer, we use non-linear least squares to compute a low-rank CP-decomposition of the 4D convolution kernel tensor into a sum of a small number of rank-one tensors. At the second step, this decomposition is used to replace the original convolutional layer with a sequence of four convolutional layers with small kernels. After such replacement, the entire network is fine-tuned on the training data using standard backpropagation process. We evaluate this approach on two CNNs and show that it yields larger CPU speedups at the cost of lower accuracy drops compared to previous approaches. For the 36-class character classification CNN, our approach obtains a 8.5x CPU speedup of the whole network with only minor accuracy drop (1% from 91% to 90%). For the standard ImageNet architecture (AlexNet), the approach speeds up the second convolution layer by a factor of 4x at the cost of 1% increase of the overall top-5 classification error.",
    "prev": "This paper studies empirically to what extent a single decomposition error correlates with the compressed model's performance across varied decomposition choices, identifying how existing procedures could be improved, and providing support for specific practices.",
    "curr": "Our contributions are as follows:\n\nINTRODUCTION\n\nTensor Decompositions (TD) have shown potential for compressing pre-trained models, such as convolutional neural networks, by replacing the optimized weight tensor with a low-rank multi-linear approximation with fewer parameters (Jaderberg et al., 2014;Lebedev et al., 2015;Kim et al., 2016;Garipov et al., 2016;Kossaifi et al., 2019a).",
    "next": "Common compression procedures (Lebedev et al., 2015;Garipov et al., 2016;Hawkins et al., 2021) work by iteratively applying TD on a selected weight tensor, where each time several decomposition choices have to be made regarding (i) the layer to compress, (ii) the type of decomposition, and (iii) the compression level."
  },
  {
    "index": 2828,
    "source_corpus_id": 251929437,
    "ref_id": "b9",
    "citation_corpus_id": 1729543,
    "start": 28247,
    "end": 28267,
    "title": "Guiding Semi-Supervision with Constraint-Driven Learning",
    "abstract": "Over the last few years, two of the main research directions in machine learning of natural language processing have been the study of semi-supervised learning algorithms as a way to train classifiers when the labeled data is scarce, and the study of ways to exploit knowledge and global information in structured learning tasks. In this paper, we suggest a method for incorporating domain knowledge in semi-supervised learning algorithms. Our novel framework unifies and can exploit several kinds of task specific constraints. The experimental results presented in the information extraction domain demonstrate that applying constraints helps the model to generate better feedback during learning, and hence the framework allows for high performance learning with significantly less training data than was possible before on these tasks.",
    "prev": "Regularization in probabilistic models.",
    "curr": "Extensive regularization approaches have been developed in traditional Bayesian inference (Zhu et al., 2014) and probabilistic modeling (Chang et al., 2007;Liang et al., 2009;Ganchev et al., 2010 \n\n\nETHICS STATEMENT\n\nThis work presents a framework to train deep generative models on small data.",
    "next": "By improving the data efficiency, it can potentially benefit real-world applications like medicine analysis and automatic drive."
  },
  {
    "index": 2834,
    "source_corpus_id": 246441782,
    "ref_id": "b4",
    "citation_corpus_id": 2410333,
    "start": 3192,
    "end": 3214,
    "title": "DECISION-BASED ADVERSARIAL ATTACKS: RELIABLE ATTACKS AGAINST BLACK-BOX MACHINE LEARNING MODELS",
    "abstract": "Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient-or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox (https://github.com/bethgelab/foolbox).Gradient-basedModel M",
    "prev": "Thus, embedding DNNS in systems creates a new attack surface as well as the incentive for malevolent actors to strike systems such as autonomous cars or machine learning models as a service (MLaaS) employed in real-world applications such as self-driving cars (Chen et al., 2015), Google Cloud Vision or Amazon Rekognition.",
    "curr": "In a black-box setting, an adversary may access all or only the top-1 predicted label and score-a score-based setting (Chen et al., 2017)-or simply the predicted label for a given input-a decisionbased (Brendel et al., 2018) setting.",
    "next": "Importantly, the similarity measure, used to quantify the imperceptibility of the perturbation, can describe an attack as a dense attack-l 2 , l ∞ norm constrained adversarial attack-or a sparse attack-l 0 norm constrained adversarial attack."
  },
  {
    "index": 2838,
    "source_corpus_id": 249626454,
    "ref_id": "b13",
    "citation_corpus_id": 225039882,
    "start": 5345,
    "end": 5371,
    "title": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. 1 arXiv:2010.11929v2 [cs.CV] 3 Jun 2021Published as a conference paper at ICLR 2021 inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.However, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks.RELATED WORKTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global selfattention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.",
    "prev": "Specifically, to provide the prior knowledge to the student, we replace student's features at some random spatial positions with corresponding teacher features at the same positions.",
    "curr": "Besides, we further design a ViT (Dosovitskiy et al., 2020)-style module to fully integrate this 'prior knowledge' with student's features.",
    "next": "Furthermore, our method also dynamically adjusts the amounts of the prior knowledge, reflected in the proportion of teacher features in the hybrid feature maps."
  },
  {
    "index": 2841,
    "source_corpus_id": 247958259,
    "ref_id": "b46",
    "citation_corpus_id": 3693512,
    "start": 2458,
    "end": 2476,
    "title": "LIFELONG LEARNING WITH DYNAMICALLY EXPAND- ABLE NETWORKS",
    "abstract": "We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets in lifelong learning scenarios on multiple public datasets, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch model with substantially fewer number of parameters.",
    "prev": "However, the inability of the learner to prevent forgetting of the knowledge learnt from the previous tasks has been a long-standing problem (McCloskey & Cohen, 1989;Goodfellow et al., 2013).",
    "curr": "To address this problem, a large body of methods (Rusu et al., 2016;Zenke et al., 2017;Yoon et al., 2018;Li et al., 2019;Aljundi et al., 2019;Buzzega et al., 2020) have been proposed; however, all these methods focus on the supervised learning paradigm, but obtaining high-quality labels is expensive and often impractical in real-world scenarios.",
    "next": "In contrast, CL for unsupervised representation learning has received limited attention in the community."
  },
  {
    "index": 2846,
    "source_corpus_id": 8241258,
    "ref_id": "b14",
    "citation_corpus_id": 2723173,
    "start": 27972,
    "end": 27993,
    "title": "FITNETS: HINTS FOR THIN DEEP NETS",
    "abstract": "While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network.",
    "prev": "In addition, we also tried an objective that provides expert guidance at the feature level instead of only at the output level.",
    "curr": "A similar idea was also explored in the model compression case (Romero et al., 2015), where a deep and thin mimic network used a larger expert network's intermediate features as guiding hints during training.",
    "next": "In contrast to these model compression techniques, our method is not concerned with decreasing test time computation but instead using experts to provide otherwise unavailable supervision to a mimic network on several distinct tasks."
  },
  {
    "index": 2849,
    "source_corpus_id": 254044293,
    "ref_id": "b49",
    "citation_corpus_id": 243756979,
    "start": 3128,
    "end": 3149,
    "title": "LARGE-SCALE REPRESENTATION LEARNING ON GRAPHS VIA BOOTSTRAPPING",
    "abstract": "Self-supervised learning provides a promising path towards eliminating the need for costly label information in representation learning on graphs. However, to achieve state-of-the-art performance, methods often need large numbers of negative examples and rely on complex augmentations. This can be prohibitively expensive, especially for large graphs. To address these challenges, we introduce Bootstrapped Graph Latents (BGRL) -a graph representation learning method that learns by predicting alternative augmentations of the input. BGRL uses only simple augmentations and alleviates the need for contrasting with negative examples, and is thus scalable by design. BGRL outperforms or matches prior methods on several established benchmarks, while achieving a 2-10x reduction in memory costs. Furthermore, we show that BGRL can be scaled up to extremely large graphs with hundreds of millions of nodes in the semi-supervised regime -achieving state-ofthe-art performance and improving over supervised baselines where representations are shaped only through label information. In particular, our solution centered on BGRL constituted one of the winning entries to the Open Graph Benchmark -Large Scale Challenge at KDD Cup 2021, on a graph orders of magnitudes larger than all previously available benchmarks, thus demonstrating the scalability and effectiveness of our approach.",
    "prev": "INTRODUCTION\n\nGraph neural networks (GNNs) are ubiquitously used modeling tools for relational graph data, with widespread applications in chemistry , forecasting and traffic prediction (Derrow-Pinion et al., 2021;, recommendation systems (Ying et al., 2018b;He et al., 2020;Sankar et al., 2021;Fan et al., 2022), graph generation (You et al., 2018;Fan & Huang, 2019;Shiao & Papalexakis, 2021), and more.",
    "curr": "Given significant challenges in obtaining labeled data, one particularly exciting recent direction is the advent of graph self-supervised learning (SSL), which aims to learn representations useful for various downstream tasks without using explicit supervision besides available graph structure and node features Jin et al., 2021;Thakoor et al., 2022;Bielak et al., 2022).",
    "next": "One prominent class of graph SSL approaches are contrastive methods (Jin et al., 2020)."
  },
  {
    "index": 2851,
    "source_corpus_id": 3280568,
    "ref_id": "b5",
    "citation_corpus_id": 6628106,
    "start": 15733,
    "end": 15752,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "The task information was properly masked in those activations, but the network did not learn the invariance to translation.",
    "curr": "B.3 OPTIMIZATION\n\nOur networks were trained using a 0.001 learning rate and the Adam optimizer (Kingma & Ba, 2014) with the following parameters: β 1 = 0.9, β 2 = 0.999 and = 0.0001.",
    "next": "We manually explored part of the hyperparameters space and found no influence on the final outcome of the experiments."
  },
  {
    "index": 2856,
    "source_corpus_id": 252715543,
    "ref_id": "b1",
    "citation_corpus_id": 240354190,
    "start": 18761,
    "end": 18783,
    "title": "NEURAL NETWORKS AS KERNEL LEARNERS: THE SILENT ALIGNMENT EFFECT",
    "abstract": "Neural networks in the lazy training regime converge to kernel machines. Can neural networks in the rich feature learning regime learn a kernel machine with a data-dependent kernel? We demonstrate that this can indeed happen due to a phenomenon we term silent alignment, which requires that the tangent kernel of a network evolves in eigenstructure while small and before the loss appreciably decreases, and grows only in overall scale afterwards. We empirically show that such an effect takes place in homogenous neural networks with small initialization and whitened data. We provide an analytical treatment of this effect in the fully connected linear network case. In general, we find that the kernel develops a low-rank contribution in the early phase of training, and then evolves in overall scale, yielding a function equivalent to a kernel regression solution with the final network's tangent kernel. The early spectral learning of the kernel depends on the depth. We also demonstrate that non-whitened data can weaken the silent alignment effect. * These authors contributed equally.arXiv:2111.00034v2 [stat.ML] 2 Dec 2021Preprint results provide a further concrete link between kernel machines and deep learning which, unlike the infinite width limit, allows for the kernel to be shaped by the data.The phenomenon we study consists of two training phases. In the first phase, the kernel starts off small in overall scale and quickly aligns its eigenvectors toward task-relevant directions. In the second phase, the kernel increases in overall scale, causing the network to learn a kernel regression solution with the final NTK. We call this phenomenon the silent alignment effect because the feature learning happens before the loss appreciably decreases. Our contributions are the following 1. In Section 2, we demonstrate the silent alignment effect by considering a simplified model where the kernel evolves while small and then subsequently increases only in scale. We theoretically show that if these conditions are met, the final neural network is a kernel machine that uses the final, data-dependent NTK. A proof is provided in Appendix B.2. In Section 3, we provide an analysis of the NTK evolution of two layer linear MLPs with scalar target function with small initialization. If the input training data is whitened, the kernel aligns its eigenvectors towards the direction of the optimal linear function early on during training while the loss does not decrease appreciably. After this, the kernel changes in scale only, showing this setup satisfies the requirements for silent alignment discussed in Section 2.3. In Section 4, we extend our analysis to deep MLPs by showing that the time required for alignment scales with initialization the same way as the time for the loss to decrease appreciably. Still, these time scales can be sufficiently separated to lead to the silent alignment effect for which we provide empirical evidence. We further present an explicit formula for the final kernel in linear networks of any depth and width when trained from small initialization, showing that the final NTK aligns to task-relevant directions.",
    "prev": "The asympotic cosine similarity between gradients and pseudogradients also increase with γ 0 .",
    "curr": "The eNTK also becomes aligned with the task relevant directions (shown in Figure 3 c), like has been observed in GD training (Baratin et al., 2021;Shan & Bordelon, 2021;Geiger et al., 2021;Atanasov et al., 2022).",
    "next": "We see that width N networks have a dynamical eNTK K N (t) which deviates from the DMFT eNTK K ∞ (t) by O(1/N ) in square loss."
  },
  {
    "index": 2858,
    "source_corpus_id": 256697430,
    "ref_id": "b34",
    "citation_corpus_id": 170078913,
    "start": 3420,
    "end": 3437,
    "title": "Published as a conference paper at ICLR 2020 WHAT CAN NEURAL NETWORKS REASON ABOUT?",
    "abstract": "Neural networks have succeeded in many reasoning tasks. Empirically, these tasks require specialized network structures, e.g., Graph Neural Networks (GNNs) perform well on many such tasks, but less structured networks fail. Theoretically, there is limited understanding of why and when a network structure generalizes better than others, although they have equal expressive power. In this paper, we develop a framework to characterize which reasoning tasks a network can learn well, by studying how well its computation structure aligns with the algorithmic structure of the relevant reasoning process. We formally define this algorithmic alignment and derive a sample complexity bound that decreases with better alignment. This framework offers an explanation for the empirical success of popular reasoning models, and suggests their limitations. As an example, we unify seemingly different reasoning tasks, such as intuitive physics, visual question answering, and shortest paths, via the lens of a powerful algorithmic paradigm, dynamic programming (DP). We show that GNNs align with DP and thus are expected to solve these tasks. On several reasoning tasks, our theory is supported by empirical results.However, there is limited understanding of the relation between the generalization ability and network structure for reasoning. What tasks can a neural network (sample efficiently) learn to reason about? Answering this question is crucial for understanding the empirical success and limitations of existing models, and for designing better models for new reasoning tasks.This paper is an initial work towards answering this fundamental question, by developing a theoretical framework to characterize what tasks a neural network can reason about. We build on a simple observation that reasoning processes resemble algorithms. Hence, we study how well a reasoning algorithm aligns with the computation graph of the network. Intuitively, if they align well, the network only needs to learn simple algorithm steps to simulate the reasoning process, which leads to better sample efficiency. We formalize this intuition with a numeric measure of algorithmic alignment, 1",
    "prev": "by learning an input-output mapping, or through the lens of reinforcement learning (Kool et al., 2019).",
    "curr": "However, more recent works build upon the notion of algorithmic alignment (Xu et al., 2020) stating that there must be an \"alignment\" between the learning model structure and the target algorithm in order to ease optimisation.",
    "next": "Much focus has been placed on Graph Neural Networks (GNNs) (Bacciu et al., 2020) learning graph algorithms, i.e Bellman-Ford (Bellman, 1958)."
  },
  {
    "index": 2859,
    "source_corpus_id": 248834106,
    "ref_id": "b17",
    "citation_corpus_id": 225039882,
    "start": 2189,
    "end": 2215,
    "title": "AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. 1 arXiv:2010.11929v2 [cs.CV] 3 Jun 2021Published as a conference paper at ICLR 2021 inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.However, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks.RELATED WORKTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global selfattention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.",
    "prev": "(a) Previous Paradigm\n\n(a) Previous paradigm designs vision-specific models and pre-trains on large-scale image datasets via supervised or self-supervised learning and then fine-tunes them on downstream tasks.",
    "curr": "(b) We propose a pre-training-free adapter to close the performance gap between plain ViT (Dosovitskiy et al., 2020) and vision-specific transformers (e.g., Swin (Liu et al., 2021b)) for dense prediction tasks.",
    "next": "Compared to the previous paradigm, our method preserves the flexibility of ViT and thus could benefit from advanced multi-modal pre-training."
  },
  {
    "index": 2860,
    "source_corpus_id": 248798499,
    "ref_id": "b13",
    "citation_corpus_id": 108306725,
    "start": 2178,
    "end": 2196,
    "title": "NEURAL LOGIC MACHINES",
    "abstract": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning. NLMs exploit the power of both neural networks-as function approximators, and logic programming-as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers. After being trained on small-scale tasks (such as sorting short arrays), NLMs can recover lifted rules, and generalize to large-scale tasks (such as sorting longer arrays). In our experiments, NLMs achieve perfect generalization in a number of tasks, from relational reasoning tasks on the family tree and general graphs, to decision making tasks including sorting arrays, finding shortest paths, and playing the blocks world. Most of these tasks are hard to accomplish for neural networks or inductive logic programming alone.",
    "prev": "* Equal contribution.",
    "curr": "INTRODUCTION\n\nWhile deep learning has achieved great success in various applications, it was pointed out that there is a debate over the problem of systematicity in connectionist models (Fodor & Pylyshyn, 1988;Fodor & McLaughlin, 1990;Hadley, 1994;Jansen & Watter, 2012;Dong et al., 2018).",
    "next": "To concretely explain systematicity (Hupkes et al., 2020), let us consider the kinship problem shown in Figure 1."
  },
  {
    "index": 2861,
    "source_corpus_id": 237364241,
    "ref_id": "b7",
    "citation_corpus_id": 201646591,
    "start": 2766,
    "end": 2782,
    "title": "Theory and Evaluation Metrics for Learning Disentangled Representations",
    "abstract": "We make two theoretical contributions to disentanglement learning by (a) defining precise semantics of disentangled representations, and (b) establishing robust metrics for evaluation. First, we characterize the concept \"disentangled representations\" used in supervised and unsupervised methods along three dimensionsinformativeness, separability and interpretability-which can be expressed and quantified explicitly using information-theoretic constructs. This helps explain the behaviors of several well-known disentanglement learning models. We then propose robust metrics for measuring informativeness, separability and interpretability. Through a comprehensive suite of experiments, we show that our metrics correctly characterize the representations learned by different methods and are consistent with qualitative (visual) results. Thus, the metrics allow disentanglement learning methods to be compared on a fair ground. We also empirically uncovered new interesting properties of VAE-based methods and interpreted them with our formulation. These findings are promising and hopefully will encourage the design of more theoretically driven models for learning disentangled representations.Preprint. Under review.",
    "prev": "Furthermore, it is not obvious to measure disentanglement given true generative factors.",
    "curr": "Towards understanding disentanglement, it is crucial to define disentanglement metrics, for which several attempts have been made (Higgins et al., 2017;Kim & Mnih, 2018;Chen et al., 2018;Eastwood & Williams, 2018;Do & Tran, 2020;Zaidi et al., 2020); however, there are still problems to be solved.",
    "next": "Current disentanglement metrics may fail to detect entanglement involving more than two variables."
  },
  {
    "index": 2862,
    "source_corpus_id": 203838320,
    "ref_id": "b22",
    "citation_corpus_id": 30535508,
    "start": 6262,
    "end": 6266,
    "title": "LEARNING SPARSE NEURAL NETWORKS THROUGH L 0 REGULARIZATION",
    "abstract": "We propose a practical method for L 0 norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of L 0 regularization. However, since the L 0 norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected L 0 norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the hard concrete distribution for the gates, which is obtained by \"stretching\" a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.",
    "prev": "Methods such as channel-pruning [11,28] remove entire input/output channels from the network.",
    "curr": "Similarly, full channels can be removed during training as in VIBnets [6], Bayesian Compression [22] and L0regularization [23].",
    "next": "These methods reduce the overall model capacity while keeping the accuracy as high as possible."
  },
  {
    "index": 2864,
    "source_corpus_id": 235614268,
    "ref_id": "b45",
    "citation_corpus_id": 195218755,
    "start": 19479,
    "end": 19496,
    "title": "Exploring Model-based Planning with Policy Networks",
    "abstract": "Model-based reinforcement learning (MBRL) with model-predictive control or online planning has shown great potential for locomotion control tasks in terms of both sample efficiency and asymptotic performance. Despite their initial successes, the existing planning methods search from candidate sequences randomly generated in the action space, which is inefficient in complex high-dimensional environments. In this paper, we propose a novel MBRL algorithm, model-based policy planning (POPLIN), that combines policy networks with online planning. More specifically, we formulate action planning at each time-step as an optimization problem using neural networks. We experiment with both optimization w.r.t. the action sequences initialized from the policy network, and also online optimization directly w.r.t. the parameters of the policy network. We show that POPLIN obtains state-of-the-art performance in the MuJoCo benchmarking environments, being about 3x more sample efficient than the state-of-the-art algorithms, such as PETS, TD3 and SAC. To explain the effectiveness of our algorithm, we show that the optimization surface in parameter space is smoother than in action space. Further more, we found the distilled policy network can be effectively applied without the expansive model predictive control during test time for some environments such as Cheetah. Code is released in https://github.com/WilsonWangTHU/POPLIN.Preprint. Under review.",
    "prev": "We note that one could achieve better results by using a larger n or the cross entropy method (CEM) (de Boer et al., 2004;Chua et al., 2018).",
    "curr": "One could also consider more complex planning strategies (Wang & Ba, 2020;Argenson & Dulac-Arnold, 2020).",
    "next": "However we judge RS with n = 100 to be sufficient for our study (see Appendix D for more details)."
  },
  {
    "index": 2865,
    "source_corpus_id": 9747411,
    "ref_id": "b14",
    "citation_corpus_id": 16496273,
    "start": 3931,
    "end": 3934,
    "title": "Efficient Learning of Domain-invariant Image Representations",
    "abstract": "We present an algorithm that learns representations which explicitly compensate for domain mismatch and which can be efficiently realized as linear classifiers. Specifically, we form a linear transformation that maps features from the target (test) domain to the source (training) domain as part of training the classifier. We optimize both the transformation and classifier parameters jointly, and introduce an efficient cost function based on misclassification loss. Our method combines several features previously unavailable in a single algorithm: multi-class adaptation through representation learning, ability to map across heterogeneous feature spaces, and scalability to large datasets. We present experiments on several image datasets that demonstrate improved accuracy and computational advantages compared to previous approaches.",
    "prev": "Theoretical [2,4] and practical results from [20,21] have shown that supervised methods' test error increases in proportion to the difference between the test and training input distribution.",
    "curr": "Many visual domain adaptation methods have been put forth to compensate for dataset bias [7,22,1,20,18,16,13,12,14,15], but are limited to shallow models.",
    "next": "Evaluation for image category classification across visually distinct domains has focused on the Office dataset, which contains 31 image categories and 3 domains [20]."
  },
  {
    "index": 2869,
    "source_corpus_id": 225094135,
    "ref_id": "b26",
    "citation_corpus_id": 16326763,
    "start": 23651,
    "end": 23675,
    "title": "CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING",
    "abstract": "We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies \"end-to-end\": directly from raw pixel inputs. * These authors contributed equally.",
    "prev": "8 for more information).",
    "curr": "We provide the average final performance of DDPG (Lillicrap et al., 2016) to give a sense of the performances achievable by a continuous control method, particularly by one that is closely related to DQN.",
    "next": "Interestingly, HGQN generally outperforms DDPG despite using the training routine of DQN, without involving policy gradients (Sutton et al., 1999)."
  },
  {
    "index": 2872,
    "source_corpus_id": 52912118,
    "ref_id": "b14",
    "citation_corpus_id": 3568073,
    "start": 1532,
    "end": 1552,
    "title": "PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION",
    "abstract": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at 1024 2 . We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CELEBA dataset.",
    "prev": "Selfmodulation is a simple architectural change that requires no additional parameter tuning, which suggests that it can be applied readily to any GAN.",
    "curr": "INTRODUCTION\n\nGenerative Adversarial Networks (GANs) are a powerful class of generative models successfully applied to a variety of tasks such as image generation (Zhang et al., 2017;Miyato et al., 2018;Karras et al., 2017), learned compression (Tschannen et al., 2018), super-resolution (Ledig et al., 2017), inpainting (Pathak et al., 2016), and domain transfer (Isola et al., 2016;Zhu et al., 2017).",
    "next": "Training GANs is a notoriously challenging task (Goodfellow et al., 2014;Lucic et al., 2018) as one is searching in a high-dimensional parameter space for a Nash equilibrium of a non-convex game."
  },
  {
    "index": 2875,
    "source_corpus_id": 251648059,
    "ref_id": "b27",
    "citation_corpus_id": 222163237,
    "start": 2253,
    "end": 2276,
    "title": "DATA-EFFICIENT REINFORCEMENT LEARNING WITH SELF-PREDICTIVE REPRESENTATIONS",
    "abstract": "While deep reinforcement learning excels at solving tasks where large amounts of data can be collected through virtually unlimited interaction with the environment, learning from limited interaction remains a key challenge. We posit that an agent can learn more efficiently if we augment reward maximization with self-supervised objectives based on structure in its visual input and sequential interaction with the environment. Our method, Self-Predictive Representations (SPR), trains an agent to predict its own latent state representations multiple steps into the future. We compute target representations for future states using an encoder which is an exponential moving average of the agent's parameters and we make predictions using a learned transition model. On its own, this future prediction objective outperforms prior methods for sample-efficient deep RL from pixels. We further improve performance by adding data augmentation to the future prediction loss, which forces the agent's representations to be consistent across multiple views of an observation. Our full self-supervised objective, which combines future prediction and data augmentation, achieves a median human-normalized score of 0.415 on Atari in a setting limited to 100k steps of environment interaction, which represents a 55% relative improvement over the previous state-of-the-art. Notably, even in this limited data regime, SPR exceeds expert human scores on 7 out of 26 games. We've made the code associated with this work available at https://github.com/mila-iqia/spr. * Equal contribution; the order of first authors was determined by a coin flip. {schwarzm, ankesh.anand}@mila.quebec",
    "prev": "We empirically demonstrate that our proposed framework outperforms state-of-the-art baselines on several benchmarks, including conventional DM Control Suite, Distracting DM Control Suite and a self-driving task CARLA.",
    "curr": "INTRODUCTION\n\nDesigning effective reinforcement learning algorithms for learning to control from high-dimensional visual observations is crucial and has attracted more and more attention Laskin et al., 2020a;Schwarzer et al., 2021;Lesort et al., 2018).",
    "next": "To learn a policy efficiently from highdimensional observations, prior approaches first learn an encoder to map high-dimensional observations, e.g., images, to low-dimensional representations, and subsequently train a policy from low-dimensional representations to actions based on various RL algorithms."
  },
  {
    "index": 2877,
    "source_corpus_id": 247627899,
    "ref_id": "b9",
    "citation_corpus_id": 3521071,
    "start": 4629,
    "end": 4652,
    "title": "DIVERSITY IS ALL YOU NEED: LEARNING SKILLS WITHOUT A REWARD FUNCTION",
    "abstract": "Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose \"Diversity is All You Need\"(DIAYN), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning. * Work done as a member of the Google AI Residency Program (g.co/airesidency). learning shared hierarchies. arXiv preprint arXiv:1710.09767, 2017. Justin Fu, John Co-Reyes, and Sergey Levine. Ex2: Exploration with exemplar models for deep reinforcement learning. . Reinforcement learning with deep energybased policies. arXiv preprint arXiv:1702.08165, 2017. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018. 9 Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart J Russell, and Anca Dragan. Inverse reward design. , et al. Learning to navigate in complex environments. arXiv preprint arXiv:1611.03673, 2016. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. Shakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for intrinsically motivated reinforcement learning. In Advances in neural information processing systems, pp. 2125-2133, 2015. Jean-Baptiste Mouret and Stéphane Doncieux. Overcoming the bootstrap problem in evolutionary robotics using behavioral diversity. . Bridging the gap between value and policy based reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2772-2782, 2017. Pierre-Yves Oudeyer, Frdric Kaplan, and Verena V Hafner. Intrinsic motivation systems for autonomous mental development. IEEE transactions on evolutionary computation, 11(2):265-286, 2007. Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by selfsupervised prediction. arXiv preprint arXiv:1705.05363, 2017. Vitchyr Pong, Shixiang Gu, Murtaza Dalal, and Sergey Levine. Temporal difference models: Model-free deep rl for model-based control. arXiv preprint arXiv:1802.09081, 2018. . Quality diversity: A new frontier for evolutionary computation. Frontiers in Robotics and AI, 3:40, 2016. Richard M Ryan and Edward L Deci. Intrinsic and extrinsic motivations: Classic definitions and new directions. Contemporary educational psychology, 25(1):54-67, 2000. Jürgen Schmidhuber. Formal theory of creativity, fun, and intrinsic motivation. IEEE Transactions on Autonomous Mental Development, 2(3):230-247, 2010.",
    "prev": "The former set of approaches (e.g.",
    "curr": "Vezhnevets et al., 2017;Eysenbach et al., 2018) have seen limited success, as the automatic identification of hierarchical abstractions is an open problem in deep learning (Hinton, 2021).",
    "next": "But approaches that endow the agent with more structure, to make complex tasks feasible, do so at the cost of rigid assumptions."
  },
  {
    "index": 2884,
    "source_corpus_id": 231627799,
    "ref_id": "b27",
    "citation_corpus_id": 1248661,
    "start": 2235,
    "end": 2251,
    "title": "CHARACTERIZING ADVERSARIAL SUBSPACES USING LOCAL INTRINSIC DIMENSIONALITY",
    "abstract": "Deep Neural Networks (DNNs) have recently been shown to be vulnerable against adversarial examples, which are carefully crafted instances that can mislead DNNs to make errors during prediction. To better understand such attacks, a characterization is needed of the properties of regions (the so-called 'adversarial subspaces') in which adversarial examples lie. In particular, effective measures are required to discriminate adversarial examples from normal examples in such regions. We tackle this challenge by characterizing the dimensional properties of adversarial regions, via the use of Local Intrinsic Dimensionality (LID). LID assesses the space-filling capability of the region surrounding a reference example, based on the distance distribution of the example to its neighbors. We first provide explanations about how adversarial perturbation can affect the LID characteristic of adversarial regions, and then show empirically that LID characteristics can facilitate the detection of adversarial examples generated using the state-of-the-art attacks. We show that when applied for adversarial detection, an LID-based method can outperform several state-of-the-art detection measures by large margins for five attack strategies across three benchmark datasets. Our analysis of the LID characteristic for adversarial regions not only motivates new directions of effective adversarial defense, but also opens up more challenges for developing new attacks to better understand the vulnerabilities of DNNs.",
    "prev": "Nonetheless, it has been demonstrated that DNNs are prone to potential threats in multiple phases of their life cycles.",
    "curr": "A type of well-studied adversary is called the adversarial attack (Szegedy et al., 2013;Goodfellow et al., 2014;Ma et al., 2018;Jiang et al., 2019;Wang et al., 2019b;Duan et al., 2020;.",
    "next": "At test time, state-of-the-art DNN models can be fooled into making incorrect predictions with small adversarial perturbations (Madry et al., 2018;Carlini & Wagner, 2017;Wu et al., 2020;Jiang et al., 2020)."
  },
  {
    "index": 2886,
    "source_corpus_id": 248376976,
    "ref_id": "b21",
    "citation_corpus_id": 8217340,
    "start": 2210,
    "end": 2228,
    "title": "OBJECT DETECTORS EMERGE IN DEEP SCENE CNNS",
    "abstract": "With the success of new computational architectures for visual processing, such as convolutional neural networks (CNN) and access to image databases with millions of labeled examples (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly. One important factor for continued progress is to understand the representations that are learned by the inner layers of these deep architectures.Here we show that object detectors emerge from training CNNs to perform scene classification. As scenes are composed of objects, the CNN for scene classification automatically discovers meaningful objects detectors, representative of the learned scene categories. With object detectors emerging as a result of learning to recognize scenes, our work demonstrates that the same network can perform both scene recognition and object localization in a single forward-pass, without ever having explicitly learned the notion of objects.",
    "prev": "One way towards understanding DNNs is to inspect the functionality of individual neurons, which is the focus of our work.",
    "curr": "This includes methods based on manual inspection (Erhan et al., 2009;Zeiler & Fergus, 2014;Zhou et al., 2015;Olah et al., 2017;2020;Goh et al., 2021), which provide high quality explanations and understanding of the network but require large amounts of manual effort.",
    "next": "To address this issue, researchers have developed automated methods to describe the functionality of individual neurons, such as Network Dissection (Bau et al., 2017) and Compositional Explanations (Mu & Andreas, 2020)."
  },
  {
    "index": 2887,
    "source_corpus_id": 3633127,
    "ref_id": "b3",
    "citation_corpus_id": 6104263,
    "start": 3243,
    "end": 3266,
    "title": "Adversarially Learned Inference",
    "abstract": "We introduce the adversarially learned inference (ALI) model, which jointly learns a generation network and an inference network using an adversarial process. The generation network maps samples from stochastic latent variables to the data space while the inference network maps training examples in data space to the space of latent variables. An adversarial game is cast between these two networks and a discriminative network that is trained to distinguish between joint latent/data-space samples from the generative network and joint samples from the inference network. We illustrate the ability of the model to learn mutually coherent inference and generation networks through the inspections of model samples and reconstructions and confirm the usefulness of the learned representations by obtaining a performance competitive with other recent approaches on the semi-supervised SVHN task.",
    "prev": "Unlike in standard GANs, the discriminator of cGANs discriminates between the generator distribution and the target distribution on the set of the pairs of generated samples x and its intended conditional variable y.",
    "curr": "To the authors' knowledge, most frameworks of discriminators in cGANs at the time of writing feeds the pair the conditional information y into the discriminator by naively concatenating (embedded) y to the input or to the feature vector at some middle layer (Mirza & Osindero, 2014;Denton et al., 2015;Reed et al., 2016;Zhang et al., 2017;Perarnau et al., 2016;Saito et al., 2017;Dumoulin et al., 2017a;Sricharan et al., 2017).",
    "next": "We would like to however, take into account the structure of the assumed conditional probabilistic models underlined by the structure of the discriminator, which is a function that measures the information theoretic distance between the generative distribution and the target distribution."
  },
  {
    "index": 2889,
    "source_corpus_id": 253098063,
    "ref_id": "b33",
    "citation_corpus_id": 52962648,
    "start": 32262,
    "end": 32284,
    "title": "Robustness May Be at Odds with Accuracy",
    "abstract": "We show that there exists an inherent tension between the goal of adversarial robustness and that of standard generalization. Specifically, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists even in a fairly simple and natural setting. These findings also corroborate a similar phenomenon observed in practice. Further, we argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than standard classifiers. These differences, in particular, seem to result in unexpected benefits: the representations learned by robust models tend to align better with salient data characteristics and human perception. * Equal Contribution.",
    "prev": "This is reasonable due to the following: First, the complexity of the dataset and continuous backdoor attacks may add to the difficulty of recovering good quality triggers.",
    "curr": "In addition, there is trade-off between adversarial training accuracy and standard accuracy of a model as discussed in (Tsipras et al., 2019).",
    "next": "Adversarial training on benign clients can induce negative effects on the accuracy."
  },
  {
    "index": 2890,
    "source_corpus_id": 246294582,
    "ref_id": "b23",
    "citation_corpus_id": 1257772,
    "start": 2713,
    "end": 2735,
    "title": "ADVERSARIAL EXAMPLES IN THE PHYSICAL WORLD",
    "abstract": "Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.",
    "prev": "By adding a well-designed perturbation on a benign image (a.k.a adversarial attack), the resulting adversarial examples can easily fool state-of-the-art DNNs.",
    "curr": "To make the matter worse, the adversarial attack technique can even be applied in the physical world (Sharif et al., 2016;Kurakin et al., 2017a;Xu et al., 2020;Duan et al., 2021), which inevitably raises concerns about the stability of deployed models.",
    "next": "Therefore, exposing as many \"blind spots\" of DNNs as possible is a top priority."
  },
  {
    "index": 2891,
    "source_corpus_id": 231573479,
    "ref_id": "b44",
    "citation_corpus_id": 12713052,
    "start": 6560,
    "end": 6576,
    "title": "NEURAL ARCHITECTURE SEARCH WITH REINFORCEMENT LEARNING",
    "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214. * Work done as a member of the Google Brain Residency program (g.co/brainresidency.) Under review as a conference paper at ICLR 2017 neural network can be typically specified by a variable-length string. It is therefore possible to use a recurrent network -the controller -to generate such string. Training the network specified by the string -the \"child network\" -on the real data will result in an accuracy on a validation set. Using this accuracy as the reward signal, we can compute the policy gradient to update the controller. As a result, in the next iteration, the controller will give higher probabilities to architectures that receive high accuracies. In other words, the controller will learn to improve its search over time.Our experiments show that Neural Architecture Search can design good models from scratch, an achievement considered not possible with other methods. On image recognition with CIFAR-10, Neural Architecture Search can find a novel ConvNet model that is better than most human-invented architectures. Our CIFAR-10 model achieves a 3.65 test set error, while being 1.05x faster than the current best model. On language modeling with Penn Treebank, Neural Architecture Search can design a novel recurrent cell that is also better than previous RNN and LSTM architectures. The cell that our model found achieves a test set perplexity of 62.4 on the Penn Treebank dataset, which is 3.6 perplexity better than the previous state-of-the-art.",
    "prev": "More recently, AutoML (Hutter et al., 2018) aims to automate the machine learning training process.",
    "curr": "Automated neural network architecture search (Stanley & Miikkulainen, 2002;Real et al., 2017;Liu et al., 2017;Zoph & Le, 2016;Elsken et al., 2018;Pham et al., 2018) has made large improvements in image classification.",
    "next": "Instead of learning the architecture, AutoML-Zero (Real et al., 2020) learns the algorithm from scratch using basic mathematical operations."
  },
  {
    "index": 2892,
    "source_corpus_id": 257038905,
    "ref_id": "b6",
    "citation_corpus_id": 28202810,
    "start": 1659,
    "end": 1682,
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
    "prev": "Crucially, we show that improving Deep PG primitives results in improved sample efficiency and policies with higher returns using common continuous control benchmark domains.",
    "curr": "INTRODUCTION\n\nDeep Policy Gradient (PG) methods achieved impressive results in numerous control tasks (Haarnoja et al., 2018).",
    "next": "However, these methods deviate from the underlying theoretical framework to compute gradients tractably."
  },
  {
    "index": 2893,
    "source_corpus_id": 52876166,
    "ref_id": "b19",
    "citation_corpus_id": 17272965,
    "start": 3339,
    "end": 3359,
    "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
    "abstract": "Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.Deep learning methods have realized impressive performance in a range of applications, from visual object classification [1, 2, 3] to speech recognition [4] and natural language processing[5,6]. These successes have been achieved despite the noted difficulty of training such deep architectures[7,8,9,10,11]. Indeed, many explanations for the difficulty of deep learning have been advanced in the literature, including the presence of many local minima, low curvature regions due to saturating nonlinearities, and exponential growth or decay of back-propagated gradients[12,13,14,15]. Furthermore, many neural network simulations have observed 1 arXiv:1312.6120v3 [cs.NE]",
    "prev": "In the absence of any such tight and computable theory of deep network generalization error, we develop an analytic theory of generalization error for deep linear networks.",
    "curr": "Such networks exhibit highly nonlinear learning dynamics (Saxe et al., 2013a;b) including many prominent phenomena like learning plateaus, saddle points, and sudden drops in training error.",
    "next": "Moreover, theory developed for the learning dynamics of deep linear networks directly inspired better initialization schemes for nonlinear networks (Schoenholz et al., 2016;Pennington et al., 2017;."
  },
  {
    "index": 2896,
    "source_corpus_id": 209516160,
    "ref_id": "b19",
    "citation_corpus_id": 2428314,
    "start": 5366,
    "end": 5385,
    "title": "Published as a conference paper at ICLR 2017 CATEGORICAL REPARAMETERIZATION WITH GUMBEL-SOFTMAX",
    "abstract": "Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification. * Work done during an internship at Google Brain.",
    "prev": "A highly effective solution is the reparameterization trick (Kingma & Welling, 2013;Rezende et al., 2014), which, however, is applicable to neither discrete variables nor non-differentiable reward functions.",
    "curr": "For variance reduction involving discrete variables, one potential solution is to combine the Gumbel-softmax trick, which relaxes the discrete variables to continuous ones, with reparameterization to produce low-variance but biased gradients (Jang et al., 2017;.",
    "next": "Another common way for variance reduction is adding appropriate baselines (Owen, 2013;Williams, 1992;Paisley et al., 2012;Ranganath et al., 2014;Mnih & Gregor, 2014), and there exist several such methods customized for discrete variables (Tucker et al., 2017;Grathwohl et al., 2018)."
  },
  {
    "index": 2897,
    "source_corpus_id": 15075376,
    "ref_id": "b1",
    "citation_corpus_id": 3116311,
    "start": 1218,
    "end": 1220,
    "title": "Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions",
    "abstract": "We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model's ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.",
    "prev": "This model learns representations that generalize well to new types of reasoning pattern in all but a few cases, a result which is promising for the ability of learned representation models to capture logical reasoning.",
    "curr": "Introduction\n\nDeep learning methods in NLP which learn vector representations for words have seen successful uses in recent years on increasingly sophisticated tasks [1,2,3,4].",
    "next": "Given the still modest performance of semantically rich NLP systems in many domains-question answering and machine translation, for instance-it is worth exploring the degree to which learned vectors can serve as general purpose semantic representations."
  },
  {
    "index": 2899,
    "source_corpus_id": 248811447,
    "ref_id": "b71",
    "citation_corpus_id": 14124313,
    "start": 3224,
    "end": 3252,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "Provided with multimodality input of an RGB image and its depth map, the task of RGB-D SOD is to effectively identify and segment the most distinctive objects in a scene.",
    "curr": "The state-of-the-art RGB-D SOD approaches Ji et al., 2020b;2021b) typically entail an image-to-mask mapping pipeline that is based on the powerful deep learning paradigms of e.g., VGG16 (Simonyan & Zisserman, 2015) or ResNet50 .",
    "next": "This strategy has led to excellent performance."
  },
  {
    "index": 2900,
    "source_corpus_id": 182953182,
    "ref_id": "b35",
    "citation_corpus_id": 14124313,
    "start": 2649,
    "end": 2677,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "A key component of many neural networks is the use of normalization layers such as Batch Normalization (Ioffe & Szegedy, 2015), Group Normalization (Wu & He, 2018), or Layer Normalization (Ba et al., 2016), with Batch Normalization the most commonly used for vision-based tasks.",
    "curr": "While the true reason why these methods work is still an active area of research (Santurkar et al., 2018), normalization techniques typically serve the purpose of making neural networks more amenable to optimization, allowing the training of very deep networks without the use of careful initialization schemes (Simonyan & Zisserman, 2015;, custom nonlinearities (Klambauer et al., 2017), or other more complicated techniques (Xiao et al., 2018).",
    "next": "Even in situations where training without normalization layers is possible, their usage can still aid generalization ."
  },
  {
    "index": 2902,
    "source_corpus_id": 244130146,
    "ref_id": "b29",
    "citation_corpus_id": 227209335,
    "start": 4330,
    "end": 4348,
    "title": "SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS",
    "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reversetime SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in diffusion probabilistic modeling and scorebased generative modeling, and allows for new sampling procedures. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, which enables exact likelihood computation, and improved sampling efficiency. In addition, our framework enables conditional generation with an unconditional model, as we demonstrate with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high fidelity generation of 1024ˆ1024 images for the first time from a score-based generative model.",
    "prev": "Our main idea is to learn the prior distribution of medical images with a generative model in order to infer the lost information due to partial measurements.",
    "curr": "Specifically, we propose to train a score-based generative model (Song & Ermon, 2019;2020;Song et al., 2021) on medical images as the data prior, due to its strong performance in image generation (Ho et al., 2020;Dhariwal & Nichol, 2021).",
    "next": "Given a trained score-based generative model, we provide a family of sampling algorithms to create image samples that are consistent with the observed measurements and the estimated data prior, leveraging the physical measurement process."
  },
  {
    "index": 2904,
    "source_corpus_id": 257833684,
    "ref_id": "b3",
    "citation_corpus_id": 234357520,
    "start": 11111,
    "end": 11131,
    "title": "VICREG: VARIANCE-INVARIANCE-COVARIANCE RE- GULARIZATION FOR SELF-SUPERVISED LEARNING",
    "abstract": "Recent self-supervised methods for image representation learning maximize the agreement between embedding vectors produced by encoders fed with different views of the same image. The main challenge is to prevent a collapse in which the encoders produce constant or non-informative vectors. We introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with two regularizations terms applied to both embeddings separately: (1) a term that maintains the variance of each embedding dimension above a threshold, (2) a term that decorrelates each pair of variables. Unlike most other approaches to the same problem, VICReg does not require techniques such as: weight sharing between the branches, batch normalization, feature-wise normalization, output quantization, stop gradient, memory banks, etc., and achieves results on par with the state of the art on several downstream tasks. In addition, we show that our variance regularization term stabilizes the training of other methods and leads to performance improvements.",
    "prev": "Among discriminative methods, one of the representative arts is contrastive learning.",
    "curr": "Based on the instance discrimination paradigm, studies improve the learned representations by exploring memory bank (He et al., 2020;Pan et al., 2021), features coding (Henaff, 2020;Tsai et al., 2021), multi-modality coding (Tian et al., 2020a;Tsai et al., 2020), strong data-augmentation (Grill et al., 2020;Caron et al., 2020;Tian et al., 2020b), parameters-updating strategy Grill et al., 2020) and loss redevelopment (Zbontar et al., 2021;Bardes et al., 2021).",
    "next": "Among these, the neighboring-based methods (Caron et al., 2020;Dwibedi et al., 2021;Azabou et al., 2021) leverage either the cluster sets or the nearest neighbors to model the correlation between views."
  },
  {
    "index": 2906,
    "source_corpus_id": 226282371,
    "ref_id": "b70",
    "citation_corpus_id": 231855774,
    "start": 9468,
    "end": 9489,
    "title": "MALI: A MEMORY EFFICIENT AND REVERSE ACCU- RATE INTEGRATOR FOR NEURAL ODES",
    "abstract": "Neural ordinary differential equations (Neural ODEs) are a new family of deeplearning models with continuous depth. However, the numerical estimation of the gradient in the continuous case is not well solved: existing implementations of the adjoint method suffer from inaccuracy in reverse-time trajectory, while the naive method and the adaptive checkpoint adjoint method (ACA) have a memory cost that grows with integration time. In this project, based on the asynchronous leapfrog (ALF) solver, we propose the Memory-efficient ALF Integrator (MALI), which has a constant memory cost w.r.t number of solver steps in integration similar to the adjoint method, and guarantees accuracy in reverse-time trajectory (hence accuracy in gradient estimation). We validate MALI in various tasks: on image recognition tasks, to our knowledge, MALI is the first to enable feasible training of a Neural ODE on ImageNet and outperform a well-tuned ResNet, while existing methods fail due to either heavy memory burden or inaccuracy; for time series modeling, MALI significantly outperforms the adjoint method; and for continuous generative models, MALI achieves new state-of-the-art performance.Code is available at httpsmachine learning framework for solving high-dimensional mean field game and mean field control problems.",
    "prev": "There exist other notable approaches for solving the adjoint equations with different memorycompute trade-offs, such as storing all intermediate quantities (also known as discrete adjoint) (e.g.",
    "curr": "Zhang & Sandu 2014), more sophisticated methods of checkpointing (Chen et al., 2016;Gholami et al., 2019;Zhuang et al., 2020), the use of interpolation schemes (Hindmarsh et al., 2005;Daulbaev et al., 2020), and symplectic integration (Zhuang et al., 2021;Matsubara et al., 2021).",
    "next": "Any of these approaches can be used and is tangential to our contributions."
  },
  {
    "index": 2907,
    "source_corpus_id": 249151922,
    "ref_id": "b5",
    "citation_corpus_id": 235254358,
    "start": 13511,
    "end": 13530,
    "title": "HOW ATTENTIVE ARE GRAPH ATTENTION NETWORKS?",
    "abstract": "Graph Attention Networks (GATs) are one of the most popular GNN architectures and are considered as the state-of-the-art architecture for representation learning with graphs. In GAT, every node attends to its neighbors given its own representation as the query. However, in this paper we show that GAT computes a very limited kind of attention: the ranking of the attention scores is unconditioned on the query node. We formally define this restricted kind of attention as static attention and distinguish it from a strictly more expressive dynamic attention. Because GATs use a static attention mechanism, there are simple graph problems that GAT cannot express: in a controlled problem, we show that static attention hinders GAT from even fitting the training data. To remove this limitation, we introduce a simple fix by modifying the order of operations and propose GATv2: a dynamic graph attention variant that is strictly more expressive than GAT. We perform an extensive evaluation and show that GATv2 outperforms GAT across 12 OGB and other benchmarks while we match their parametric costs. Our code is available at https://github.com/tech-srl/how_attentive_are_ gats.",
    "prev": "GNN ensembles Ho (1998) and Breiman (2001) showed that training an ensemble of shallow learners on random subspaces could exploit variance in the feature space and improve performance, and Dietterich (2000) demonstrated that these ensembles are most potent when the predictions of their base classifiers are accurate and diverse.",
    "curr": "Deep ensembles have typically been used with random weight initializations to improve uncertainty estimation and robustness (Lakshminarayanan et al., 2017;Fort et al., 2019;Wasay & Idreos, 2020), which has benefited GNNs via mechanisms like multi-head attention (Veličković et al., 2018;Brody et al., 2022;Hou et al., 2021), but very few works have directly explored ensembles of GNNs.",
    "next": "Some recent exceptions have suggested that ensembling subgraphs could benefit GNNs (Zeng et al., 2021;Tang et al., 2021;Lin et al., 2022)."
  },
  {
    "index": 2908,
    "source_corpus_id": 232404824,
    "ref_id": "b7",
    "citation_corpus_id": 52920808,
    "start": 11554,
    "end": 11570,
    "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks",
    "abstract": "One of the mystery in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an m hidden node shallow neural network with ReLU activation and n training data, we show as long as m is large enough and the data is non-degenerate, randomly initialized gradient descent converges a globally optimal solution with a linear convergence rate for the quadratic loss function.Our analysis is based on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods. * Equal Contribution.",
    "prev": "All our theoretical results are based upon a fundamental theorem that shows that analytic functions can be efficiently learnt by wide (but finite-width) two-layer neural networks with standard activation functions (such as ReLU), using SGD from a random initialization.",
    "curr": "Specifically, we derive novel generalization bounds for multivariate analytic functions (Theorems 1 and 8) by relating wide networks to kernel learning with a specific network-induced kernel (Jacot et al., 2018;Du et al., 2019;Allen-Zhu et al., 2019;Arora et al., 2019a;Lee et al., 2019), known as the neural tangent kernel (NTK) (Jacot et al., 2018).",
    "next": "We further develop a calculus of bounds showing that the sum, product, ratio, and composition of analytic functions is also learnable, with bounds constructed using the familiar product and chain rules of univariate calculus (Corollaries 1, 2)."
  },
  {
    "index": 2910,
    "source_corpus_id": 211021032,
    "ref_id": "b4",
    "citation_corpus_id": 14711954,
    "start": 25713,
    "end": 25735,
    "title": "Modular Multitask Reinforcement Learning with Policy Sketches",
    "abstract": "We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate tasks with sequences of named subtasks, providing information about high-level structural relationships among tasks but not how to implement them-specifically not providing the detailed guidance used by much previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). To learn from sketches, we present a model that associates every subtask with a modular subpolicy, and jointly maximizes reward over full task-specific policies by tying parameters across shared subpolicies. Optimization is accomplished via a decoupled actor-critic training objective that facilitates learning common behaviors from multiple dissimilar reward functions. We evaluate the effectiveness of our approach in three environments featuring both discrete and continuous control, and with sparse rewards that can be obtained only after completing a number of high-level subgoals. Experiments show that using our approach to learn policies guided by sketches gives better performance than existing techniques for learning task-specific or shared policies, while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks.",
    "prev": "Language can provide a form of supervision for how to break down the world and/or learned behaviours into meaningful sub-parts, which in turn might stimulate systematicity and generalisation.",
    "curr": "Indeed, prior work has found that language can serve to promote compositional behaviour in deep RL agents (Andreas et al., 2017).",
    "next": "To explore this hypothesis, we devised a simple task in the grid world that can be solved either with or without relying on language."
  },
  {
    "index": 2913,
    "source_corpus_id": 252780848,
    "ref_id": "b66",
    "citation_corpus_id": 235376961,
    "start": 9806,
    "end": 9824,
    "title": "Published as a conference paper at ICLR 2022 GEOMETRY-CONSISTENT NEURAL SHAPE REPRESENTATION WITH IMPLICIT DISPLACEMENT FIELDS",
    "abstract": "We present implicit displacement fields, a novel representation for detailed 3D geometry. Inspired by a classic surface deformation technique, displacement mapping, our method represents a complex surface as a smooth base surface plus a displacement along the base's normal directions, resulting in a frequency-based shape decomposition, where the high-frequency signal is constrained geometrically by the low-frequency signal. Importantly, this disentanglement is unsupervised thanks to a tailored architectural design that has an innate frequency hierarchy by construction. We explore implicit displacement field surface reconstruction and detail transfer and demonstrate superior representational power, training stability, and generalizability. Code and data available at:",
    "prev": "(2022b) generate 3D avatars from text inputs.",
    "curr": "(Peng et al., 2021b;Zhao et al., 2021;Peng et al., 2021a;Noguchi et al., 2021;Weng et al., 2022;Chen et al., 2021;Su et al., 2021;Jiang et al., 2022a;b;Wang et al., 2022).",
    "next": "propose to learn modal-invariant human representations for versatile down-stream tasks."
  },
  {
    "index": 2914,
    "source_corpus_id": 258236460,
    "ref_id": "b7",
    "citation_corpus_id": 231627730,
    "start": 2817,
    "end": 2835,
    "title": "Preprint, under review. RANDOMIZED ENSEMBLED DOUBLE Q-LEARNING: LEARNING FAST WITHOUT A MODEL",
    "abstract": "Using a high Update-To-Data (UTD) ratio, model-based methods have recently achieved much higher sample efficiency than previous model-free methods for continuous-action DRL benchmarks. In this paper, we introduce a simple modelfree algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show that its performance is just as good as, if not better than, a state-of-the-art modelbased algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this performance using fewer parameters than the model-based method, and with less wall-clock run time. REDQ has three carefully integrated ingredients which allow it to achieve its high performance: (i) a UTD ratio 1; (ii) an ensemble of Q functions; (iii) in-target minimization across a random subset of Q functions from the ensemble. Through carefully designed experiments, we provide a detailed analysis of REDQ and related model-free algorithms. To our knowledge, REDQ is the first successful model-free DRL algorithm for continuous-action spaces using a UTD ratio 1. * Equal contribution, in alphabetical order. † Correspondence to: Keith Ross <keithwross@nyu.edu>. 1 arXiv:2101.05982v1 [cs.LG] 15 Jan 2021 Preprint, under review.zero for most of training, even when the UTD is very high. Furthermore, by adjusting the number of randomly selected Q-functions for in-target minimization, REDQ can control the average Q-function bias. In comparison with standard ensemble averaging and with SAC with a higher UTD, REDQ has much lower std of Q-function bias while maintaining an average bias that is negative but close to zero throughout most of training, resulting in significantly better learning performance. We perform an ablation study, and show that REDQ is very robust to choices of hyperparameters, and can work well with a small ensemble and a small number of Q functions in the in-target minimization. We also provide a theoretical analysis, providing additional insights into REDQ. Finally, we consider combining the REDQ algorithm with an online feature extractor network (OFENet) (Ota et al.,  2020)  to further improve performance, particularly for the more challenging environments Ant and Humanoid. We achieve more than 7x the sample efficiency of SAC to reach a score of 5000 for both Ant and Humanoid. In Humanoid, REDQ-OFE also greatly outperforms MBPO, reaching a score of 5000 at 150K interactions, which is 3x MBPO's score at that point.To ensure our comparisons are fair, and to ensure our results are reproducible(Henderson et al., 2018;Islam et al., 2017;Duan et al., 2016), we provide open source code. For all algorithmic comparisons, we use the same codebase (except for MBPO, for which we use the authors' code).RANDOMIZED ENSEMBLED DOUBLE Q-LEARNING (REDQ)Janner et al.(2019)proposed Model-Based Policy Optimization (MBPO), which was shown to be much more sample efficient than popular model-free algorithms such as SAC and PPO for the MuJoCo environments. MBPO learns a model, and generates \"fake data\" from its model as well as \"real data\" through environment interactions. It then performs parameter updates using both the fake and the real data. One of the distinguishing features of MBPO is that it has a UTD ratio 1 for updating its Q functions, enabling MBPO to achieve high sample efficiency.",
    "prev": "Since every unit of active online data collection comes at an expense (e.g., running real robots, chip evaluation using simulation), it is important to develop sample-efficient deep RL algorithms, that can learn efficiently even with limited amount of experience.",
    "curr": "Devising such efficient RL algorithm has been an important thread of research in recent years (Janner et al., 2019;Chen et al., 2021;Hiraoka et al., 2021).",
    "next": "In principle, off-policy RL methods (e.g., SAC (Haarnoja et al., 2018), TD3 (Fujimoto et al., 2018), Rainbow (Hessel et al., 2018)) should provide good sample efficiency, because they make it possible to improve the policy and value functions for many gradient steps per step of data collection."
  },
  {
    "index": 2915,
    "source_corpus_id": 248496160,
    "ref_id": "b16",
    "citation_corpus_id": 3488815,
    "start": 22644,
    "end": 22664,
    "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
    "abstract": "Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. * Authors ordered alphabetically.",
    "prev": "ResNet6-A and ResNet6-B have the same architecture but differ in regularization strength while ResNet8-A has an additional residual block.",
    "curr": "All three were trained with adversarial training (Madry et al., 2018) using PGD, the GAMA loss (Sriramanan et al., 2020) and MixUp data augmentation (Zhang et al., 2021).",
    "next": "ResNet6-A and ResNet8-A were trained using 8-steps and = 4/255, whereas 20-steps and = 8/255 were used for ResNet6-B.We compare against β-CROWN (Wang et al., 2021), a BaB-based state-of-the-art complete verifier, OVAL (Palma et al., 2021;De Palma et al., 2021;, a BaB framework based on a different class of multi-neuron constraints, and ERAN Singh et al."
  },
  {
    "index": 2917,
    "source_corpus_id": 2926851,
    "ref_id": "b3",
    "citation_corpus_id": 8893912,
    "start": 11226,
    "end": 11229,
    "title": "Paraphrase-Driven Learning for Open Question Answering",
    "abstract": "We study question answering as a machine learning problem, and induce a function that maps open-domain questions to queries over a database of web extractions. Given a large, community-authored, question-paraphrase corpus, we demonstrate that it is possible to learn a semantic lexicon and linear ranking function without manually annotating questions. Our approach automatically generalizes a seed lexicon and includes a scalable, parallelized perceptron parameter estimation scheme. Experiments show that our approach more than quadruples the recall of the seed lexicon, with only an 8% loss in precision.",
    "prev": "This then gives a feature space of D = 8|W |.",
    "curr": "Experiments\n\n\nLarge-scale QA\n\nWe perform experiments on the QA dataset introduced in [4].",
    "next": "It consists of 14M statements, stored as (subject, relation, object) triples, which are stored as memories in the MemNN model."
  },
  {
    "index": 2920,
    "source_corpus_id": 8895303,
    "ref_id": "b33",
    "citation_corpus_id": 14124313,
    "start": 3751,
    "end": 3779,
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve the stateof-the-art results. Importantly, we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.1",
    "prev": "In this paper we demonstrate that, for a speech recognition task, it is possible to train very accurate CNN models, outperforming LSTMs.",
    "curr": "This is thanks to CNN architectures inspired by recent devel-opments in computer vision (Simonyan & Zisserman, 2014), which were not previously considered for this task.",
    "next": "Moreover, the experiments suggest that LSTMs and CNNs learn different functions when trained on the same data."
  },
  {
    "index": 2921,
    "source_corpus_id": 249889348,
    "ref_id": "b20",
    "citation_corpus_id": 202565538,
    "start": 3724,
    "end": 3747,
    "title": "Maximum Likelihood Constraint Inference for Inverse Reinforcement Learning",
    "abstract": "While most approaches to the problem of Inverse Reinforcement Learning (IRL) focus on estimating a reward function that best explains an expert agent's policy or demonstrated behavior on a control task, it is often the case that such behavior is more succinctly described by a simple reward combined with a set of hard constraints. In this setting, the agent is attempting to maximize cumulative rewards subject to these given constraints on their behavior. We reformulate the problem of IRL on Markov Decision Processes (MDPs) such that, given a nominal model of the environment and a nominal reward function, we seek to estimate state, action, and feature constraints in the environment that motivate an agent's behavior. Our approach is based on the Maximum Entropy IRL framework, which allows us to reason about the likelihood of an expert agent's demonstrations given our knowledge of an MDP. Using our method, we can infer which constraints can be added to the MDP to most increase the likelihood of observing these demonstrations. We present an algorithm which iteratively infers the Maximum Likelihood Constraint to best explain observed behavior, and we evaluate its efficacy using both simulated behavior and recorded data of humans navigating around an obstacle.",
    "prev": "To fill this gap, previous works often pick some environments and add external constraints to them.",
    "curr": "Striving for simplicity, many of the selected environments are deterministic with discretized state and action spaces (Scobee & Sastry, 2020;McPherson et al., 2021;Glazier et al., 2021;Papadimitriou et al., 2021;Gaurav et al., 2022).",
    "next": "Generalizing model performance in these simple environments to practical applications is difficult."
  },
  {
    "index": 2922,
    "source_corpus_id": 247447758,
    "ref_id": "b8",
    "citation_corpus_id": 53388625,
    "start": 2687,
    "end": 2711,
    "title": "THE LOTTERY TICKET HYPOTHESIS: FINDING SPARSE, TRAINABLE NEURAL NETWORKS",
    "abstract": "Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard technique for pruning weights naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the lottery ticket hypothesis: dense, randomly-initialized feed-forward networks contain subnetworks (winning tickets) that-when trained in isolation-arrive at comparable test accuracy in a comparable number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Furthermore, the winning tickets we find above that size learn faster than the original network and exhibit higher test accuracy. arXiv:1803.03635v4 [cs.LG] 27 Nov 2018 1. Randomly initialize a neural network f (x; θ 0 ) (where θ 0 ∼ D θ ). 2. Train the network for j iterations, reaching parameters θ j . 3. Prune s% of the parameters, creating a mask m where P m = (100 − s)%. 4. To extract the winning ticket, reset the remaining parameters to their values in θ 0 , creating the untrained network f (x; m θ 0 ).If dense networks contain winning tickets and pruning reveals them, then the network f (x; m θ 0 )when trained for j iterations-will reach similar accuracy to f (x; θ j ) at least as quickly, and m will be too sparse for a randomly-reinitialized or randomly-sparsified network to do the same.Results. We identify winning tickets in a fully-connected architecture for MNIST and convolutional architectures for CIFAR10 across several optimization strategies (SGD, momentum, and Adam) with techniques like dropout, weight decay, and batchnorm. In deeper networks, our pruning-based strategy for finding winning tickets is sensitive to the learning rate: it requires warmup to find winning tickets at higher learning rates. The winning tickets we find are 10-20% (or less) of the size of the Anonymous. Gradient descent provably optimizes over-parameterized neural networks. Dally. Dsd: Regularizing deep neural networks with dense-sparse-dense training flow. Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size. arXiv preprint arXiv:1602.07360, 2016.",
    "prev": "Because DNNs have become increasingly large and require considerable storage, memory bandwidth, and computational resources (Han et al., 2015), finding a reduced form is desirable and has become an area of major research.",
    "curr": "In addition to the practical considerations, DNN pruning has become of theoretical interest, as the search for subnetworks that can be pruned with minimal effect on performance led to new insight on the success of large DNNs, via the development of the Lottery Ticket Hypothesis (LTH) (Frankle & Carbin, 2019;.",
    "next": "A considerable body of literature has been devoted to expanding, interrogating, and critiquing the LTH, furthering the understanding of sparse DNNs."
  },
  {
    "index": 2927,
    "source_corpus_id": 257482853,
    "ref_id": "b16",
    "citation_corpus_id": 5590763,
    "start": 10821,
    "end": 10839,
    "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
    "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
    "prev": "(2021).",
    "curr": "EVENT-BASED GRU\n\n3.1 TIME-SPARSE GRU FORMULATION We base our model on the GRU (Cho et al., 2014), illustrated for convenience in Fig.",
    "next": "1A."
  },
  {
    "index": 2928,
    "source_corpus_id": 257102428,
    "ref_id": "b45",
    "citation_corpus_id": 222179041,
    "start": 3338,
    "end": 3357,
    "title": "LEARNING MESH-BASED SIMULATION WITH GRAPH NETWORKS",
    "abstract": "Mesh-based simulations are central to modeling complex physical systems in many disciplines across science and engineering. Mesh representations support powerful numerical integration methods and their resolution can be adapted to strike favorable trade-offs between accuracy and efficiency. However, highdimensional scientific simulations are very expensive to run, and solvers and parameters must often be tuned individually to each system studied. Here we introduce MESHGRAPHNETS, a framework for learning mesh-based simulations using graph neural networks. Our model can be trained to pass messages on a mesh graph and to adapt the mesh discretization during forward simulation. Our results show it can accurately predict the dynamics of a wide range of physical systems, including aerodynamics, structural mechanics, and cloth. The model's adaptivity supports learning resolution-independent dynamics and can scale to more complex state spaces at test time. Our method is also highly efficient, running 1-2 orders of magnitude faster than the simulation on which it is trained. Our approach broadens the range of problems on which neural network simulators can operate and promises to improve the efficiency of complex, scientific modeling tasks. * equal contribution Videos of all our experiments can be found at https",
    "prev": "Applications include structural mechanics (Zienkiewicz & Taylor, 2005;Stanova et al., 2015), electromagnetics (Jin, 2015;Xiao et al., 2022;Coggon, 1971), fluid dynamics (Chung, 1978;Zawawi et al., 2018;Long et al., 2021) and biomedical engineering (Van Staden et al., 2006;Soro et al., 2018), most of which traditionally depend on highly specialized task-dependent simulators.",
    "curr": "Recent advancements in deep learning brought rise to more general learned dynamic models such as Graph Network Simulators (GNSs) Pfaff et al., 2021).",
    "next": "GNSs learn to predict the dynamics of a system from data by encoding the system state as a graph and then iteratively computing the dynamics for every node in the graph with a Graph Neural Network (GNN) (Scarselli et al., 2009;Wu et al., 2020b)."
  },
  {
    "index": 2929,
    "source_corpus_id": 257364759,
    "ref_id": "b42",
    "citation_corpus_id": 248506067,
    "start": 4199,
    "end": 4217,
    "title": "A Word is Worth A Thousand Dollars: Adversarial Attack on Tweets Fools Stock Prediction",
    "abstract": "More and more investors and machine learning models rely on social media (e.g., Twitter and Reddit) to gather real-time information and sentiment to predict stock price movements. Although text-based models are known to be vulnerable to adversarial attacks, whether stock prediction models have similar vulnerability is underexplored. In this paper, we experiment with a variety of adversarial attack configurations to fool three stock prediction victim models. We address the task of adversarial generation by solving combinatorial optimization problems with semantics and budget constraints. Our results show that the proposed attack method can achieve consistent success rates and cause significant monetary loss in trading simulation by simply concatenating a perturbed but semantically similar tweet. † Corresponding author dakuo@acm.org.Our code is available at https://github.com/yonxie/ AdvFinTweet Figure 1: An example of word-replacement adversarial attack. (Top) benign tweet leads Stocknet to predict stock going UP; (Bottom) adding an adversarial retweet leads Stocknet to predict stock going DOWN.",
    "prev": "For example, to mislead the forecasting of a particular stock, the adversaries might attempt to alter some features external to the stock's financial valuation to maximize the gap between predictions of its values on authentic and altered features.",
    "curr": "The feasibility of such an adversarial attack has been recently demonstrated with tweet messages (Xie et al., 2022) on a text-based stock forecasting.",
    "next": "Motivated by these real scenarios, we propose to investigate such adversarial threats on more practical forecasting models whose predictions are based on more precise features, e.g."
  },
  {
    "index": 2931,
    "source_corpus_id": 208637407,
    "ref_id": "b30",
    "citation_corpus_id": 2879445,
    "start": 5520,
    "end": 5544,
    "title": "Posterior calibration and exploratory analysis for natural language processing models",
    "abstract": "Many models in natural language processing define probabilistic distributions over linguistic structures. We argue that (1) the quality of a model's posterior distribution can and should be directly evaluated, as to whether probabilities correspond to empirical frequencies; and (2) NLP uncertainty can be projected not only to pipeline components, but also to exploratory data analysis, telling a user when to trust and not trust the NLP analysis. We present a method to analyze calibration, and apply it to compare the miscalibration of several commonly used models. We also contribute a coreference sampling algorithm that can create confidence intervals for a political event extraction task. 1",
    "prev": "Guo et al.",
    "curr": "(2017);Nguyen & O'Connor (2015) propose metrics for determining the calibration of machine learning models.",
    "next": "Lakshminarayanan et al."
  },
  {
    "index": 2932,
    "source_corpus_id": 232185174,
    "ref_id": "b24",
    "citation_corpus_id": 6021932,
    "start": 2035,
    "end": 2059,
    "title": "IN SEARCH OF THE REAL INDUCTIVE BIAS: ON THE ROLE OF IMPLICIT REGULARIZATION IN DEEP LEARNING",
    "abstract": "We present experiments demonstrating that some other form of capacity control, different from network size, plays a central role in learning multi-layer feedforward networks. We argue, partially through analogy to matrix factorization, that this is an inductive bias that can help shed light on deep learning.",
    "prev": "In particular, the traditional view that explicit forms of regularization (e.g.",
    "curr": "dropout, L 2 -regularization, data augmentation) are the sole factors for generalization performance of state of the art neural networks has been experimentally invalidated (Neyshabur et al., 2015;Zhang et al., 2017).",
    "next": "Today's conventional wisdom rather conjectures the presence of implicit forms of regularization, emerging from the interactions between neural network architectures, optimization, and the inherent structure of the data itself (Arpit et al., 2017)."
  },
  {
    "index": 2933,
    "source_corpus_id": 53116133,
    "ref_id": "b32",
    "citation_corpus_id": 1957433,
    "start": 1865,
    "end": 1890,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "They are essential in order to move from the discrete word space to the continuous space where differentiable loss functions can be optimized.",
    "curr": "The popular models of Glove (Pennington et al., 2014), Word2Vec (Mikolov et al., 2013b) or FastText (Bojanowski et al., 2016), provide efficient ways to learn word vectors fully unsupervised from raw text corpora, solely based on word co-occurrence statistics.",
    "next": "These models are then successfully applied to word similarity and other downstream tasks and, surprisingly (or not (Arora et al., 2016)), exhibit a linear algebraic structure that is also useful to solve word analogy."
  },
  {
    "index": 2934,
    "source_corpus_id": 253238033,
    "ref_id": "b48",
    "citation_corpus_id": 12462234,
    "start": 6711,
    "end": 6732,
    "title": "OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER",
    "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost. * Equally major contributors † Work done as a member of the Google Brain Residency program (g.co/brainresidency) Under review as a conference paper at ICLR 2017 Figure 1: A Mixture of Experts (MoE) layer embedded within a recurrent language model. In this case, the sparse gating function selects two experts to perform computations. Their outputs are modulated by the outputs of the gating network.While these ideas are promising in theory, no work to date has yet demonstrated massive improvements in model capacity, training time, or model quality. We blame this on a combination of the following challenges:• Modern computing devices, especially GPUs, are much faster at arithmetic than at branching. Most of the works above recognize this and propose turning on/off large chunks of the network with each gating decision.• Large batch sizes are critical for performance, as they amortize the costs of parameter transfers and updates. Conditional computation reduces the batch sizes for the conditionally active chunks of the network.• Network bandwidth can be a bottleneck. A cluster of GPUs may have computational power thousands of times greater than the aggregate inter-device network bandwidth. To be computationally efficient, the relative computational versus network demands of an algorithm must exceed this ratio. Embedding layers, which can be seen as a form of conditional computation, are handicapped by this very problem. Since the embeddings generally need to be sent across the network, the number of (example, parameter) interactions is limited by network bandwidth instead of computational capacity.• Depending on the scheme, loss terms may be necessary to achieve the desired level of sparsity per-chunk and/or per example. Bengio et al.(2015)use three such terms. These issues can affect both model quality and load-balancing. diction with lstm. Neural Computation, 2000. Audrunas Gruslys, Rémi Munos, Ivo Danihelka, Marc Lanctot, and Alex Graves. Memory-efficient backpropagation through time. CoRR, abs/1606.03401, 2016. URL http://arxiv.org/ abs/1606.03401. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.",
    "prev": "However, due to the inherent discrete nature, it is difficult to train KiC in a fully differentiable manner to select the correct knowledge category for each instance.",
    "curr": "To solve this problem, we find that KiC can be reformulated as a special mixture-of-experts (MoE) model (Jacobs et al., 1991;Jordan & Jacobs, 1994;Shazeer et al., 2017;Fedus et al., 2022), where the knowledge selector is identified as the router that is used to determine the sequence-to-expert assignment in MoE (Section 2.3).",
    "next": "Furthermore, the memory partition corresponding to each knowledge category together with the text-to-text model can be recognized as a special semi-parametric expert in MoE."
  },
  {
    "index": 2935,
    "source_corpus_id": 256697616,
    "ref_id": "b28",
    "citation_corpus_id": 232105052,
    "start": 1460,
    "end": 1479,
    "title": "RANDOM FEATURE ATTENTION",
    "abstract": "Transformers are state-of-the-art models for a variety of sequence modeling tasks.At their core is an attention function which models pairwise interactions between the inputs at every timestep.While attention is powerful, it does not scale efficiently to long sequences due to its quadratic time and space complexity in the sequence length.We propose RFA, a linear time and space attention that uses random feature methods to approximate the softmax function, and explore its application in transformers.RFA can be used as a drop-in replacement for conventional softmax attention and offers a straightforward way of learning with recency bias through an optional gating mechanism.Experiments on language modeling and machine translation demonstrate that RFA achieves similar or better performance compared to strong transformer baselines.In the machine translation experiment, RFA decodes twice as fast as a vanilla transformer.Compared to existing efficient transformer variants, RFA is competitive in terms of both accuracy and efficiency on three long text classification datasets.Our analysis shows that RFA's efficiency gains are especially notable on long sequences, suggesting that RFA will be particularly useful in tasks that require working with large inputs, fast decoding speed, or low memory footprints.",
    "prev": "1 Our code and models are available at this link.",
    "curr": "INTRODUCTION\n\nRandom-feature-based attention (RFA, also known as Performer; Choromanski et al., 2021;Peng et al., 2021b) is an established fast approximation to the conventional softmax attention mechanism (Bahdanau et al., 2014;Vaswani et al., 2017), which successfully scales Transformer models to processing much longer sequences (Choromanski et al., 2021).",
    "next": "At its core is the usage of random features (RF; Rahimi & Recht, 2008) to linearize the exponential kernel in softmax attention, which reduces the computational cost from quadratic to linear runtime and space complexity."
  },
  {
    "index": 2942,
    "source_corpus_id": 232135338,
    "ref_id": "b0",
    "citation_corpus_id": 182953134,
    "start": 7189,
    "end": 7208,
    "title": "Published as a conference paper at ICLR 2020 DEEP BATCH ACTIVE LEARNING BY DIVERSE, UNCERTAIN GRADIENT LOWER BOUNDS",
    "abstract": "We design a new algorithm for batch active learning with deep neural network models. Our algorithm, Batch Active learning by Diverse Gradient Embeddings (BADGE), samples groups of points that are disparate and high magnitude when represented in a hallucinated gradient space, a strategy designed to incorporate both predictive uncertainty and sample diversity into every selected batch. Crucially, BADGE trades off between uncertainty and diversity without requiring any hand-tuned hyperparameters. While other approaches sometimes succeed for particular batch sizes or architectures, BADGE consistently performs as well or better, making it a useful option for real world active learning problems.Published as a conference paper at ICLR 2020 example, might perform poorly when using a multilayer perceptron. A diversity-based approach might work well when the batch size is very large, but poorly when the batch size is small. Further, what even constitutes a \"large\" or \"small\" batch size is largely a function of the statistical properties of the data in question. These weaknesses pose a major problem for real, practical batch active learning situations, where data are unfamiliar and potentially unstructured. There is no way to know which active learning algorithm is best to use.Moreover, in a real active learning scenario, every change of hyperparameters typically causes the algorithm to label examples not chosen under other hyperparameters, provoking substantial labeling inefficiency. That is, hyperparameter sweeps in active learning can be label expensive. As a result, active learning algorithms need to \"just work\", given fixed hyperparameters, to a greater extent than is typical for supervised learning.Based on these observations, we design an approach which creates diverse batches of examples about which the current model is uncertain. We measure uncertainty as the gradient magnitude with respect to parameters in the final (output) layer, which is computed using the most likely label according to the model. To capture diversity, we collect a batch of examples where these gradients span a diverse set of directions. More specifically, we build up the batch of query points based on these hallucinated gradients using the k-MEANS++ initialization (Arthur and Vassilvitskii, 2007), which simultaneously captures both the magnitude of a candidate gradient and its distance from previously included points in the batch. We name the resulting approach Batch Active learning by Diverse Gradient Embeddings (BADGE).We show that BADGE is robust to architecture choice, batch size, and dataset, generally performing as well as or better than the best baseline across our experiments, which vary all of the aforementioned environmental conditions. We begin by introducing our notation and setting, followed by a description of the BADGE algorithm in Section 3 and experiments in Section 4. We defer our discussion of related work to Section 5.NOTATION AND SETTINGDefine [K] := {1, 2, . . . , K}. Denote by X the instance space and by Y the label space. In this work we consider multiclass classification, so Y = [K]. Denote by D the distribution from which examples are drawn, by D X the unlabeled data distribution, and by D Y|X the conditional distribution over labels given examples. We consider the pool-based active learning setup, where the learner receives an unlabeled dataset U sampled according to D X and can request labels sampled according to D Y|X for any x ∈ U . We use E D to denote expectation under the data distribution D. Given a classifier h : X → Y, which maps examples to labels, and a labeled example (x, y), we denote the 0/1 error of h on (x, y) as 01 (h(x), y) = I(h(x) = y). The performance of a classifier h is measured by its expected 0/1 error, i.e. E D [ 01 (h(x), y)] = Pr (x,y)∼D (h(x) = y). The goal of pool-based active learning is to find a classifier with a small expected 0/1 error using as few label queries as possible. Given a set S of labeled examples (x, y), where each x ∈ S is picked from U , followed by a label query, we use E S as the sample averages over S.In this paper, we consider classifiers h parameterized by underlying neural networks f of fixed architecture, with the weights in the network denoted by θ. We abbreviate the classifier with parameters θ as h θ since the architectures are fixed in any given context, and our classifiers take the form h θ (x) = argmax y∈ [K]  f (x; θ) y , where f (x; θ) ∈ R K is a probability vector of scores assigned to candidate labels, given the example x and parameters θ. We optimize the parameters by minimizing the cross-entropy loss E S [ CE (f (x; θ), y)] over the labeled examples, where CE (p, y) = K i=1 I(y = i) ln 1 /pi = ln 1 /py.",
    "prev": "LOCALIZED DISCREPANCY\n\nTo formulate the problem of active learning under domain shift as a distribution matching problem, one needs to consider a measure of divergence between distributions.",
    "curr": "Recent interest focuses on the discrepancy (Mansour et al., 2009) which proves to be useful for domain adaptation (Cortes & Mohri, 2014;Zhang et al., 2019) and is recently used on the active learning setting (Viering et al., 2019).",
    "next": "However this metric, defined as a maximal difference between domain losses over the whole hypothesis space, is relatively conservative as it includes hypotheses that the learner might not ever consider as candidates for the labeling function (Cortes et al., 2019c;Zhang et al., 2020)."
  },
  {
    "index": 2946,
    "source_corpus_id": 10278413,
    "ref_id": "b25",
    "citation_corpus_id": 7185434,
    "start": 2290,
    "end": 2312,
    "title": "Learning Bilingual Lexicons from Monolingual Corpora",
    "abstract": "We present a method for learning bilingual translation lexicons from monolingual corpora. Word types in each language are characterized by purely monolingual features, such as context counts and orthographic substrings. Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings. We show that high-precision lexicons can be learned in a variety of language pairs and from a range of corpus types.",
    "prev": "This approach allows us to run approximate KCCA on a speech dataset with 1.4 million training samples and a random feature space of dimensionality M = 100000 on a typical workstation.",
    "curr": "INTRODUCTION\n\nCanonical correlation analysis (CCA, Hotelling, 1936) and its extensions are ubiquitous techniques in scientific research areas for revealing the common sources of variability in multiple views of the same phenomenon, including meteorology (Anderson, 2003), chemometrics (Montanarella et al., 1995), genomics (Witten et al., 2009), computer vision (Kim et al., 2007;Socher & Li, 2010), speech recognition (Rudzicz, 2010;Wang et al., 2015a), and natural language processing (Vinokourov et al., 2003;Haghighi et al., 2008;Dhillon et al., 2011;Hodosh et al., 2013;Faruqui & Dyer, 2014;Lu et al., 2015a).",
    "next": "CCA seeks linear projections of two random vectors (views), such that the resulting low-dimensional vectors are maximally correlated."
  },
  {
    "index": 2947,
    "source_corpus_id": 252907554,
    "ref_id": "b23",
    "citation_corpus_id": 221150562,
    "start": 3736,
    "end": 3755,
    "title": "Glancing Transformer for Non-Autoregressive Neural Machine Translation",
    "abstract": "Recent work on non-autoregressive neural machine translation (NAT) aims at improving the efficiency by parallel decoding without sacrificing the quality. However, existing NAT methods are either inferior to Transformer or require multiple decoding passes, leading to reduced speedup. We propose the Glancing Language Model (GLM) for single-pass parallel generation models. With GLM, we develop Glancing Transformer (GLAT) for machine translation. With only single-pass parallel decoding, GLAT is able to generate high-quality translation with 8×-15× speedup. Note that GLAT does not modify the network architecture, which is a training method to learn word interdependency. Experiments on multiple WMT language directions show that GLAT outperforms all previous single pass non-autoregressive methods, and is nearly comparable to Transformer, reducing the gap to 0.25-0.9 BLEU points.",
    "prev": "Connectionist Temporal Classification (CTC) (Graves et al., 2006) is a fundamental criterion for seq2seq tasks.",
    "curr": "The CTC criterion was initially proposed Published as a conference paper at ICLR 2023 for automatic speech recognition (ASR) but its usage has been extended to many other tasks like machine translation (MT) (Qian et al., 2021;Gu & Kong, 2020;Huang et al., 2022), speech translation (ST) (Yan et al., 2022;Chuang et al., 2021;Liu et al., 2020), sign language translation (Wang et al., 2018;Guo et al., 2019;Camgoz et al., 2020), optical character recognition (OCR) (Graves & Schmidhuber, 2008), lip reading (Assael et al., 2017), hand gesture detection (Molchanov et al., 2016) and even robot control (Shiarlis et al., 2018).",
    "next": "Research on CTC is of wide interest, as many advanced systems for seq2seq tasks are based on CTC (Yao et al., 2021), its extensions (Graves, 2012;Sak et al., 2017;Higuchi et al., 2020;Qian et al., 2021) and its hybrid with attention-based architectures (Watanabe et al., 2017;Yan et al., 2022)."
  },
  {
    "index": 2948,
    "source_corpus_id": 257404839,
    "ref_id": "b32",
    "citation_corpus_id": 3292002,
    "start": 4715,
    "end": 4740,
    "title": "GRAPH ATTENTION NETWORKS",
    "abstract": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-theart results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a proteinprotein interaction dataset (wherein test graphs remain unseen during training).",
    "prev": "One particular example is in graph representation learning.",
    "curr": "Message Passing Graph Neural Networks (MP-GNNs) are the prevailing designs in modern Graph Neural Networks (GNNs), including numerous variants like GCN (Kipf & Welling, 2017), GAT (Veličković et al., 2018), and even the Transformers (Vaswani et al., 2017).",
    "next": "There is a vast literature studying its diffusion dynamics and representation power Oono & Suzuki, 2020;Wang et al., 2021b;Li et al., 2022;Dong et al., 2021;Xu et al., 2019;Chen et al., 2022)."
  },
  {
    "index": 2950,
    "source_corpus_id": 249192149,
    "ref_id": "b28",
    "citation_corpus_id": 204838340,
    "start": 27941,
    "end": 27961,
    "title": "CONTRASTIVE REPRESENTATION DISTILLATION",
    "abstract": "Often we wish to transfer representational knowledge from one neural network to another.Examples include distilling a large network into a smaller one, transferring knowledge from one sensory modality to a second, or ensembling a collection of models into a single estimator.Knowledge distillation, the standard approach to these problems, minimizes the KL divergence between the probabilistic outputs of a teacher and student network.We demonstrate that this objective ignores important structural knowledge of the teacher network.This motivates an alternative objective by which we train a student to capture significantly more information in the teacher's representation of the data.We formulate this objective as contrastive learning.Experiments demonstrate that our resulting new objective outperforms knowledge distillation and other cutting-edge distillers on a variety of knowledge transfer tasks, including single model compression, ensemble distillation, and cross-modal transfer.Our method sets a new state-of-the-art in many transfer tasks, and sometimes even outperforms the teacher network when combined with knowledge distillation.",
    "prev": "As our method could be applied to arbitrary tasks, we further conduct experiments to validate our efficacy on image classification task.",
    "curr": "Following CRD (Tian et al., 2019a) and DIST (Huang et al., 2022c), we train ResNet-18 with ResNet-34 teacher on ImageNet, and adopt distillations with our MasKD and the baseline mimic, respectively.",
    "next": "We perform feature distillations on the outputs of the last four stages of the networks."
  },
  {
    "index": 2952,
    "source_corpus_id": 210861217,
    "ref_id": "b12",
    "citation_corpus_id": 6628106,
    "start": 19713,
    "end": 19732,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "(5)\nWe minimize this equation w.r.t.",
    "curr": "the parameters of the gating by gradient descent using the Adam (Kingma & Ba, 2014) algorithm.",
    "next": "The objective for updating a single component i is given by\nargmin q(x|zi,y) Ep (y|z) E q(x|zi,y) [φ(x, y)] + KL (q(x|z i , y)||q t (x|z i , y)) ,(6)\nwherep(y|z) = p(y)q(z i |y)/q(z i )."
  },
  {
    "index": 2953,
    "source_corpus_id": 233231739,
    "ref_id": "b1",
    "citation_corpus_id": 9128667,
    "start": 2821,
    "end": 2846,
    "title": "STOCHASTIC VARIATIONAL VIDEO PREDICTION",
    "abstract": "Predicting the future in real-world settings, particularly from raw sensory observations such as images, is exceptionally challenging. Real-world events can be stochastic and unpredictable, and the high dimensionality and complexity of natural images require the predictive model to build an intricate understanding of the natural world. Many existing methods tackle this problem by making simplifying assumptions about the environment. One common assumption is that the outcome is deterministic and there is only one plausible future. This can lead to low-quality predictions in real-world settings with stochastic dynamics. In this paper, we develop a stochastic variational video prediction (SV2P) method that predicts a different possible future for each sample of its latent variables. To the best of our knowledge, our model is the first to provide effective stochastic multi-frame prediction for real-world videos. We demonstrate the capability of the proposed method in predicting detailed future frames of videos on multiple real-world datasets, both action-free and action-conditioned. We find that our proposed method produces substantially improved video predictions when compared to the same model without stochasticity, and to other stochastic video prediction methods. Our SV2P implementation will be open sourced upon publication.",
    "prev": "To make it truly beneficial for these applications, video prediction should be capable of forecasting long-term future.",
    "curr": "Many previous approaches have formulated video prediction as a conditional generation task by recursively synthesizing future frames conditioned on the previous frames (Vondrick et al., 2016;Tulyakov et al., 2018;Denton & Fergus, 2018;Babaeizadeh et al., 2018;Castrejon et al., 2019;.",
    "next": "Despite their success in short-term forecasting, however, none of these approaches have been successful in synthesizing convincing long-term future, due to the challenges in modeling complex dynamics and extrapolating from short sequences to much longer future."
  },
  {
    "index": 2954,
    "source_corpus_id": 252917944,
    "ref_id": "b6",
    "citation_corpus_id": 195755478,
    "start": 2997,
    "end": 3021,
    "title": "Published as a conference paper at ICLR 2020 RÉNYI FAIR INFERENCE",
    "abstract": "Machine learning algorithms have been increasingly deployed in critical automated decision-making systems that directly affect human lives. When these algorithms are solely trained to minimize the training/test error, they could suffer from systematic discrimination against individuals based on their sensitive attributes, such as gender or race. Recently, there has been a surge in machine learning society to develop algorithms for fair machine learning. In particular, several adversarial learning procedures have been proposed to impose fairness. Unfortunately, these algorithms either can only impose fairness up to linear dependence between the variables, or they lack computational convergence guarantees. In this paper, we use Rényi correlation as a measure of fairness of machine learning models and develop a general training framework to impose fairness. In particular, we propose a min-max formulation which balances the accuracy and fairness when solved to optimality. For the case of discrete sensitive attributes, we suggest an iterative algorithm with theoretical convergence guarantee for solving the proposed min-max problem. Our algorithm and analysis are then specialized to fair classification and fair clustering problems. To demonstrate the performance of the proposed Rényi fair inference framework in practice, we compare it with wellknown existing methods on several benchmark datasets. Experiments indicate that the proposed method has favorable empirical performance against state-of-the-art approaches.",
    "prev": "Specifically, machine learning algorithms have been found to discriminate against certain \"sensitive\" demographic groups (e.g.",
    "curr": "racial minorities), prompting a profusion of algorithmic fairness research (Dwork et al., 2012;Sweeney, 2013;Datta et al., 2015;Feldman et al., 2015;Bolukbasi et al., 2016;Angwin et al., 2016;Calmon et al., 2017;Hardt et al., 2016a;Fish et al., 2016;Woodworth et al., 2017;Zafar et al., 2017;Bechavod & Ligett, 2017;Kearns et al., 2018;Prost et al., 2019;Baharlouei et al., 2020;Lowy et al., 2022a).",
    "next": "Algorithmic fairness literature aims to develop fair machine learning algorithms that output non-discriminatory predictions."
  },
  {
    "index": 2956,
    "source_corpus_id": 3517962,
    "ref_id": "b5",
    "citation_corpus_id": 3463260,
    "start": 5939,
    "end": 5960,
    "title": "Published as a conference paper at ICLR 2018 DISTRIBUTED PRIORITIZED EXPERIENCE REPLAY",
    "abstract": "We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible. The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network. The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors. Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time.",
    "prev": "Most interestingly, there have also been recent attempts to distribute updates for the DDPG algorithm, (e.g.",
    "curr": "Popov et al., 2017) and more generally in this work we build on work of (Horgan et al., 2018) for implementing distributed actors.",
    "next": "Recently, Bellemare et al."
  },
  {
    "index": 2957,
    "source_corpus_id": 245123899,
    "ref_id": "b28",
    "citation_corpus_id": 226262285,
    "start": 3087,
    "end": 3098,
    "title": "Gradient-guided Unsupervised Lexically Constrained Text Generation",
    "abstract": "Lexically-constrained generation requires the target sentence to satisfy some lexical constraints, such as containing some specific words or being the paraphrase to a given sentence, which is very important in many real-world natural language generation applications. Previous works usually apply beamsearch-based methods or stochastic searching methods to lexically-constrained generation. However, when the search space is too large, beam-search-based methods always fail to find the constrained optimal solution. At the same time, stochastic search methods always cost too many steps to find the correct optimization direction. In this paper, we propose a novel method G2LC to solve the lexically-constrained generation as an unsupervised gradient-guided optimization problem. We propose a differentiable objective function and use the gradient to help determine which position in the sequence should be changed (deleted or inserted/replaced by another word). The word updating process of the inserted/replaced word also benefits from the guidance of gradient. Besides, our method is free of parallel data training, which is flexible to be used in the inference stage of any pre-trained generation model. We apply G2LC to two generation tasks: keyword-to-sentence generation and unsupervised paraphrase generation. The experiment results show that our method achieves state-of-the-art compared to previous lexically-constrained methods.",
    "prev": "For example, given concept words {dog, frisbee, catch, throw}, a machine is expected to generate a plausible description, e.g., \"A man throws a frisbee and his dog catches it in the air\".",
    "curr": "Machines with GCSR skills would communicate fluidly with humans, e.g., when summarizing a document by preserving its key details (Sha, 2020), composing a creative story according to a set of clues (Yao et al., 2019), and generating a conversation reply that includes specified keywords (Mou et al., 2016).",
    "next": "GCSR poses three unique challenges for automatic text generation methods."
  },
  {
    "index": 2959,
    "source_corpus_id": 51780574,
    "ref_id": "b2",
    "citation_corpus_id": 11383178,
    "start": 7750,
    "end": 7770,
    "title": "IMPORTANCE WEIGHTED AUTOENCODERS",
    "abstract": "The variational autoencoder (VAE; ) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.",
    "prev": "The optimization is based on variations of gradient descent using the reparametrization trick (Kingma & Welling, 2014;Rezende et al., 2014).",
    "curr": "Further improvements have been proposed that provide more flexible posterior approximations (Rezende & Mohamed, 2015;Kingma et al., 2016) or tighter lower bound (Burda et al., 2016).",
    "next": "In this paper, we consider the standard VAE for simplicity."
  },
  {
    "index": 2960,
    "source_corpus_id": 11212020,
    "ref_id": "b17",
    "citation_corpus_id": 12639289,
    "start": 1610,
    "end": 1641,
    "title": "Recurrent Continuous Translation Models",
    "abstract": "We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.",
    "prev": "Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "curr": "INTRODUCTION\n\nNeural machine translation is a newly emerging approach to machine translation, recently proposed by Kalchbrenner and Blunsom (2013), Sutskever et al.",
    "next": "(2014) and Cho et al."
  },
  {
    "index": 2961,
    "source_corpus_id": 247582435,
    "ref_id": "b2",
    "citation_corpus_id": 227905681,
    "start": 9164,
    "end": 9187,
    "title": "Compressing Pre-trained Language Models by Matrix Decomposition",
    "abstract": "Large pre-trained language models reach stateof-the-art results on many different NLP tasks when fine-tuned individually; They also come with a significant memory and computational requirements, calling for methods to reduce model sizes (green AI). We propose a twostage model-compression method to reduce a model's inference time cost. We first decompose the matrices in the model into smaller matrices and then perform feature distillation on the internal representation to recover from the decomposition. This approach has the benefit of reducing the number of parameters while preserving much of the information within the model. We experimented on BERTbase model with the GLUE benchmark dataset and show that we can reduce the number of parameters by a factor of 0.4x, and increase inference speed by a factor of 1.45x, while maintaining a minimal loss in metric performance.",
    "prev": "Those works inspire us to explore in depth extreme parameter compression for large-scale PLMs.",
    "curr": "We argue that compression ratio of existing work using matrix/tensor decomposition (Ma et al., 2019;Liu et al., 2021;Noach & Goldberg, 2020) for PLMs is relatively-small; most of them do not have speedup effect, limiting their applications in large-scale PLMs.",
    "next": "The potential to compress PLMs with matrix/tensor decomposition is under-investigated."
  },
  {
    "index": 2964,
    "source_corpus_id": 29842525,
    "ref_id": "b16",
    "citation_corpus_id": 6628106,
    "start": 17583,
    "end": 17602,
    "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
    "prev": "We were able to resolve this issue by tweaking the optimization.",
    "curr": "Specifically, we train the network with the ADAM optimizer (Kingma & Ba, 2015) using high moment weight (β1) and learning rate (η).",
    "next": "Through training at higher rates, early peaks in the functional space can be easily avoided."
  },
  {
    "index": 2967,
    "source_corpus_id": 8737624,
    "ref_id": "b14",
    "citation_corpus_id": 3281198,
    "start": 3283,
    "end": 3310,
    "title": "A UNIFIED PERSPECTIVE ON MULTI-DOMAIN AND MULTI-TASK LEARNING",
    "abstract": "In this paper, we provide a new neural-network based perspective on multi-task learning (MTL) and multi-domain learning (MDL). By introducing the concept of a semantic descriptor, this framework unifies MDL and MTL as well as encompassing various classic and recent MTL/MDL algorithms by interpreting them as different ways of constructing semantic descriptors. Our interpretation provides an alternative pipeline for zero-shot learning (ZSL), where a model for a novel class can be constructed without training data. Moreover, it leads to a new and practically relevant problem setting of zero-shot domain adaptation (ZSDA), which is the analogous to ZSL but for novel domains: A model for an unseen domain can be generated by its semantic descriptor. Experiments across this range of problems demonstrate that our framework outperforms a variety of alternatives.",
    "prev": "For example, sum of the trace norms on all matriciations 1 [Romera-paredes et al., 2013], and scaled latent trace norm [Wimalawarne et al., 2014].",
    "curr": "An alternative solution is to concatenate the one-hot encodings of task factors and feed it as input into a two-branch neural network model [Yang and Hospedales, 2015], in which there are two input channels for feature vector and encoded task factor.",
    "next": "Multi-Task Learning for Neural Networks\n\nWith the great success of deep learning, many researches are proposed for deep multi-task learning."
  },
  {
    "index": 2968,
    "source_corpus_id": 249538418,
    "ref_id": "b10",
    "citation_corpus_id": 53388625,
    "start": 6808,
    "end": 6833,
    "title": "THE LOTTERY TICKET HYPOTHESIS: FINDING SPARSE, TRAINABLE NEURAL NETWORKS",
    "abstract": "Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard technique for pruning weights naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the lottery ticket hypothesis: dense, randomly-initialized feed-forward networks contain subnetworks (winning tickets) that-when trained in isolation-arrive at comparable test accuracy in a comparable number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Furthermore, the winning tickets we find above that size learn faster than the original network and exhibit higher test accuracy. arXiv:1803.03635v4 [cs.LG] 27 Nov 2018 1. Randomly initialize a neural network f (x; θ 0 ) (where θ 0 ∼ D θ ). 2. Train the network for j iterations, reaching parameters θ j . 3. Prune s% of the parameters, creating a mask m where P m = (100 − s)%. 4. To extract the winning ticket, reset the remaining parameters to their values in θ 0 , creating the untrained network f (x; m θ 0 ).If dense networks contain winning tickets and pruning reveals them, then the network f (x; m θ 0 )when trained for j iterations-will reach similar accuracy to f (x; θ j ) at least as quickly, and m will be too sparse for a randomly-reinitialized or randomly-sparsified network to do the same.Results. We identify winning tickets in a fully-connected architecture for MNIST and convolutional architectures for CIFAR10 across several optimization strategies (SGD, momentum, and Adam) with techniques like dropout, weight decay, and batchnorm. In deeper networks, our pruning-based strategy for finding winning tickets is sensitive to the learning rate: it requires warmup to find winning tickets at higher learning rates. The winning tickets we find are 10-20% (or less) of the size of the Anonymous. Gradient descent provably optimizes over-parameterized neural networks. Dally. Dsd: Regularizing deep neural networks with dense-sparse-dense training flow. Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size. arXiv preprint arXiv:1602.07360, 2016.",
    "prev": "There is, however, increasing empirical evidence to suggest weight pruning can occur significantly prior to full model convergence.",
    "curr": "Frankle and Carbin (2019) postulate the extreme scenario termed lottery ticket hypothesis (LTH) where a subnetwork extracted at initialization can be trained to the accuracy of the parent network-in effect \"winning\" the weight initialization lottery.",
    "next": "In an even more striking phenomenon Ramanujan et al."
  },
  {
    "index": 2971,
    "source_corpus_id": 222125277,
    "ref_id": "b18",
    "citation_corpus_id": 6216506,
    "start": 3275,
    "end": 3296,
    "title": "Robust Disambiguation of Named Entities in Text",
    "abstract": "Disambiguating named entities in naturallanguage text maps mentions of ambiguous names onto canonical entities like people or places, registered in a knowledge base such as DBpedia or YAGO. This paper presents a robust method for collective disambiguation, by harnessing context from knowledge bases and using a new form of coherence graph. It unifies prior approaches into a comprehensive framework that combines three measures: the prior probability of an entity being mentioned, the similarity between the contexts of a mention and a candidate entity, as well as the coherence among candidate entities for all mentions together. The method builds a weighted graph of mentions and candidate entities, and computes a dense subgraph that approximates the best joint mention-entity mapping. Experiments show that the new method significantly outperforms prior methods in terms of accuracy, with robust behavior across a variety of inputs.",
    "prev": "Although there has been extensive previous work on entity retrieval (e.g.",
    "curr": "Hoffart et al., 2011;Piccinno & Ferragina, 2014;Huang et al., 2015;Le & Titov, 2018;Logeswaran et al., 2019;Broscheit, 2019;, to name just a few) there is a common design choice to most current solutions: entities are associated with a unique atomic label and the retrieval problem can be interpreted as multi-class classification across these labels.",
    "next": "The match between input and label is calculated through a bi-encoder : a dot product between dense vector encodings of the input and the entity's meta information (such as title and description)."
  },
  {
    "index": 2978,
    "source_corpus_id": 231662264,
    "ref_id": "b37",
    "citation_corpus_id": 59317031,
    "start": 4235,
    "end": 4255,
    "title": "FIXUP INITIALIZATION: RESIDUAL LEARNING WITHOUT NORMALIZATION",
    "abstract": "Normalization layers are a staple in state-of-the-art deep neural network architectures. They are widely believed to stabilize training, enable higher learning rate, accelerate convergence and improve generalization, though the reason for their effectiveness is still an active research topic. In this work, we challenge the commonly-held beliefs by showing that none of the perceived benefits is unique to normalization. Specifically, we propose fixed-update initialization (Fixup), an initialization motivated by solving the exploding and vanishing gradient problem at the beginning of training via properly rescaling a standard initialization. We find training residual networks with Fixup to be as stable as training with normalization -even for networks with 10,000 layers. Furthermore, with proper regularization, Fixup enables residual networks without normalization to achieve state-of-the-art performance in image classification and machine translation. Despite the enormous empirical success of training deep networks with normalization, and recent progress on understanding the working of batch normalization(Santurkar et al., 2018), there is currently no general consensus on why these normalization techniques help training residual neural networks. Intrigued by this topic, in this work we study (i) without normalization, can a deep residual network be trained reliably? (And if so,) (ii) without normalization, can a deep residual network be trained with the same learning rate, converge at the same speed, and generalize equally well (or even better)?Perhaps surprisingly, we find the answers to both questions are Yes. In particular, we show:• Why normalization helps training. We derive a lower bound for the gradient norm of a residual network at initialization, which explains why with standard initializations, normalization techniques are essential for training deep residual networks at maximal learning rate. (Section 2) * Work done at Facebook. Equal contribution. † Work done at Facebook. Equal contribution. ‡ Work done at Facebook.",
    "prev": "It smoothens the loss landscape (Santurkar et al., 2018), which allows training with larger learning rates (Bjorck et al., 2018), and the noise arising from the minibatch estimates of the batch statistics introduces implicit regularization (Luo et al., 2019).",
    "curr": "Crucially, recent theoretical work (Balduzzi et al., 2017; has demonstrated that BatchNorm ensures good signal propagation at initialization in deep residual networks with identity skip connections (He et al., 2016b;a), and this benefit has enabled practitioners to train deep ResNets with hundreds or even thousands of layers (Zhang et al., 2019).",
    "next": "However, BatchNorm also has many disadvantages."
  },
  {
    "index": 2981,
    "source_corpus_id": 257482844,
    "ref_id": "b15",
    "citation_corpus_id": 203593909,
    "start": 6587,
    "end": 6605,
    "title": "REVISITING SELF-TRAINING FOR NEURAL SEQUENCE GENERATION",
    "abstract": "Self-training is one of the earliest and simplest semi-supervised methods. The key idea is to augment the original labeled dataset with unlabeled data paired with the model's prediction (i.e. the pseudo-parallel data). While self-training has been extensively studied on classification problems, in complex sequence generation tasks (e.g. machine translation) it is still unclear how self-training works due to the compositionality of the target space. In this work, we first empirically show that selftraining is able to decently improve the supervised baseline on neural sequence generation tasks. Through careful examination of the performance gains, we find that the perturbation on the hidden states (i.e. dropout) is critical for self-training to benefit from the pseudo-parallel data, which acts as a regularizer and forces the model to yield close predictions for similar unlabeled inputs. Such effect helps the model correct some incorrect predictions on unlabeled data. To further encourage this mechanism, we propose to inject noise to the input space, resulting in a \"noisy\" version of self-training. Empirical study on standard machine translation and text summarization benchmarks shows that noisy self-training is able to effectively utilize unlabeled data and improve the performance of the supervised baseline by a large margin.",
    "prev": "Our method is also orthogonal to CLIP-like research in model pre-training and can be applied to other image classification models (Jia et al., 2021;Li et al., 2022;Yao et al., 2022;, such as LiT (Zhai et al., 2022) which performs two-stage pre-training.",
    "curr": "Self-training has shown promising progress in many domains including vision Xie et al., 2020;Sahito et al., 2022), NLP (He et al., 2020a), and speech (Kahn et al., 2020).",
    "next": "Our method is more closely related to the self-training approaches proposed for semi-supervised learning (Tarvainen & Valpola, 2017;, where pseudo-labels on unlabeled data are used as training targets."
  },
  {
    "index": 2984,
    "source_corpus_id": 237303776,
    "ref_id": "b36",
    "citation_corpus_id": 231648272,
    "start": 7163,
    "end": 7180,
    "title": "Published as a conference paper at ICLR 2021 RANK THE EPISODES: A SIMPLE APPROACH FOR EXPLORATION IN PROCEDURALLY-GENERATED ENVIRONMENTS",
    "abstract": "Exploration under sparse reward is a long-standing challenge of model-free reinforcement learning. The state-of-the-art methods address this challenge by introducing intrinsic rewards to encourage exploration in novel states or uncertain environment dynamics. Unfortunately, methods based on intrinsic rewards often fall short in procedurally-generated environments, where a different environment is generated in each episode so that the agent is not likely to visit the same state more than once. Motivated by how humans distinguish good exploration behaviors by looking into the entire episode, we introduce RAPID, a simple yet effective episode-level exploration method for procedurally-generated environments. RAPID regards each episode as a whole and gives an episodic exploration score from both per-episode and long-term views. Those highly scored episodes are treated as good exploration behaviors and are stored in a small ranking buffer. The agent then imitates the episodes in the buffer to reproduce the past good exploration behaviors. We demonstrate our method on several procedurally-generated MiniGrid environments, a first-person-view 3D Maze navigation task from MiniWorld, and several sparse MuJoCo tasks. The results show that RAPID significantly outperforms the state-of-the-art intrinsic reward strategies in terms of sample efficiency and final performance. The code is available at",
    "prev": "2 Episode-level exploration is the case where the mode is fixed for an entire episode at a time (e.g., training games versus tournament matches in a sport), see Fig.1:B.",
    "curr": "This has been investigated for simple cases, where the policy's level of stochasticity is sampled at the beginning of each episode (Horgan et al., 2018;Kapturowski et al., 2019;Zha et al., 2021).",
    "next": "Intra-episodic exploration is what falls in-between step-and episode-level exploration, where exploration periods last for multiple steps, but less than a full episode."
  },
  {
    "index": 2985,
    "source_corpus_id": 2239496,
    "ref_id": "b30",
    "citation_corpus_id": 780171,
    "start": 2344,
    "end": 2365,
    "title": "Data-Driven Response Generation in Social Media",
    "abstract": "We present a data-driven approach to generating responses to Twitter status posts, based on phrase-based Statistical Machine Translation. We find that mapping conversational stimuli onto responses is more difficult than translating between languages, due to the wider range of possible responses, the larger fraction of unaligned words/phrases, and the presence of large phrase pairs whose alignment cannot be further decomposed. After addressing these challenges, we compare approaches based on SMT and Information Retrieval in a human evaluation. We show that SMT outperforms IR on this task, and its output is preferred over actual human responses in 15% of cases. As far as we are aware, this is the first work to investigate the use of phrase-based SMT to directly translate a linguistic stimulus into an appropriate response.",
    "prev": "INTRODUCTION\n\nWith the recent employment of Recurrent Neural Networks (RNNs) and the large quantities of conversational data available on websites like Twitter or Reddit, a new type of dialog system is emerging.",
    "curr": "Such end-to-end dialog systems (Ritter et al., 2011;Shang et al., 2015;Vinyals & Le, 2015;Sordoni et al., 2015) directly generate a response given the last user utterance and (potentially) the context from previous dialog turns without relying on the intermediate use of a dialog state tracking component like in traditional dialog systems (e.g.",
    "next": "in Henderson (2015))."
  },
  {
    "index": 2987,
    "source_corpus_id": 231847140,
    "ref_id": "b25",
    "citation_corpus_id": 59608630,
    "start": 8400,
    "end": 8405,
    "title": "ADAPTIVE POSTERIOR LEARNING: FEW-SHOT LEARNING WITH A SURPRISE-BASED MEMORY MODULE",
    "abstract": "The ability to generalize quickly from few observations is crucial for intelligent systems. In this paper we introduce APL, an algorithm that approximates probability distributions by remembering the most surprising observations it has encountered. These past observations are recalled from an external memory module and processed by a decoder network that can combine information from different memory slots to generalize beyond direct recall. We show this algorithm can perform as well as state of the art baselines on few-shot classification benchmarks with a smaller memory footprint. In addition, its memory compression allows it to scale to thousands of unknown labels. Finally, we introduce a meta-learning reasoning task which is more challenging than direct classification. In this setting, APL is able to generalize with fewer than one example per class via deductive reasoning.",
    "prev": "Figure 2 demonstrates two realizations of meta discovery (i.e., meta discovery with MAML (MM) and meta discovery with ProtoNet (MP)) can perform much better than existing methods in NCDL.",
    "curr": "We conduct experiments on four benchmarks and compare our method with five competitive baselines (MacQueen et al., 1967;Hsu et al., 2018;2019;Han et al., 2019;.",
    "next": "Empirical results show that our method outperforms these baselines significantly when novel-class data are very limited."
  },
  {
    "index": 2989,
    "source_corpus_id": 231800078,
    "ref_id": "b52",
    "citation_corpus_id": 174801567,
    "start": 3028,
    "end": 3045,
    "title": "Published as a conference paper at ICLR 2020 PLAYING THE LOTTERY WITH REWARDS AND MULTIPLE LANGUAGES: LOTTERY TICKETS IN RL AND NLP",
    "abstract": "The lottery ticket hypothesis proposes that over-parameterization of deep neural networks (DNNs) aids training by increasing the probability of a \"lucky\" sub-network initialization being present rather than by helping the optimization process (Frankle & Carbin, 2019). Intriguingly, this phenomenon suggests that initialization strategies for DNNs can be improved substantially, but the lottery ticket hypothesis has only previously been tested in the context of supervised learning for natural image tasks. Here, we evaluate whether \"winning ticket\" initializations exist in two different domains: natural language processing (NLP) and reinforcement learning (RL). For NLP, we examined both recurrent LSTM models and large-scale Transformer models(Vaswani et al., 2017). For RL, we analyzed a number of discrete-action space tasks, including both classic control and pixel control. Consistent with work in supervised image classification, we confirm that winning ticket initializations generally outperform parameter-matched random initializations, even at extreme pruning rates for both NLP and RL. Notably, we are able to find winning ticket initializations for Transformers which enable models one-third the size to achieve nearly equivalent performance. Together, these results suggest that the lottery ticket hypothesis is not restricted to supervised learning of natural images, but rather represents a broader phenomenon in DNNs.",
    "prev": "It states that there exist matching subnetworks in dense models that can be trained to reach a comparable test accuracy to the full model within similar training iterations.",
    "curr": "The hypothesis has successfully shown its success in various fields (Yu et al., 2020;Renda et al., 2020;Chen et al., 2020b;Gan et al., 2021), and its property has been studied widely (Malach et al., 2020;Pensia et al., 2020;Elesedy et al., 2020;Chen et al., 2021a).",
    "next": "However, it is never introduced to GANs, and therefore the presence of matching subnetworks in generative adversarial networks still remains mysterious."
  },
  {
    "index": 2993,
    "source_corpus_id": 259298566,
    "ref_id": "b20",
    "citation_corpus_id": 211146562,
    "start": 3337,
    "end": 3353,
    "title": "Published as a conference paper at ICLR 2020 DIVIDEMIX: LEARNING WITH NOISY LABELS AS SEMI-SUPERVISED LEARNING",
    "abstract": "Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at",
    "prev": "Therefore, it is important to develop robust training methods against segmentation label noise.",
    "curr": "However, despite many existing methods addressing label noise in classification tasks (Patrini et al., 2017;Yu et al., 2019;Zhang & Sabuncu, 2018;Li et al., 2020;Zhang et al., 2021;Xia et al., 2021), limited progress has been made in the context of image segmentation.",
    "next": "A few existing segmentation label noise approaches (Zhu et al., 2019;Zhang et al., 2020b;a) directly apply methods in classification label noise."
  },
  {
    "index": 2995,
    "source_corpus_id": 209516262,
    "ref_id": "b30",
    "citation_corpus_id": 1957433,
    "start": 20054,
    "end": 20078,
    "title": "GloVe: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "prev": "B and App.",
    "curr": "C.\n\nOur embedding generally has 3 × D × |W| parameters with D-dimensional word vectors and |W| words, while previous work (Mikolov et al., 2013b;Pennington et al., 2014) usually employs only D × |W| parameters for embedding lookup tables.",
    "next": "To increase efficiency and facilitate fair comparison with previous work we set initial phases θ j = [θ j,1 , ..., θ j,D ] to a shared constant value (such as zero)."
  },
  {
    "index": 2997,
    "source_corpus_id": 12998557,
    "ref_id": "b15",
    "citation_corpus_id": 16636683,
    "start": 2693,
    "end": 2710,
    "title": "Network In Network",
    "abstract": "We propose a novel deep network structure called \"Network In Network\"(NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.",
    "prev": "First, a plethora of extensions were recently proposed to enhance networks which follow this basic scheme.",
    "curr": "Among these the most notable directions are work on using more complex activation functions (Goodfellow et al., 2013;Lin et al., 2014; techniques for improving class inference (Stollenga et al., 2014;Srivastava & Salakhutdinov, 2013) as well as procedures for improved regularization Springenberg & Riedmiller, 2013;Wan et al., 2013) and layer-wise pre-training using label information (Lee et al., 2014).",
    "next": "Second, the success of CNNs for large scale object recognition in the ImageNet challenge (Krizhevsky et al., 2012) has stimulated research towards experimenting with the different architectural choices in CNNs."
  },
  {
    "index": 2998,
    "source_corpus_id": 252918484,
    "ref_id": "b7",
    "citation_corpus_id": 211132990,
    "start": 11865,
    "end": 11883,
    "title": "BATCHENSEMBLE: AN ALTERNATIVE APPROACH TO EFFICIENT ENSEMBLE AND LIFELONG LEARNING",
    "abstract": "Ensembles, where multiple neural networks are trained individually and their predictions are averaged, have been shown to be widely successful for improving both the accuracy and predictive uncertainty of single neural networks. However, an ensemble's cost for both training and testing increases linearly with the number of networks, which quickly becomes untenable. In this paper, we propose BatchEnsemble 1 , an ensemble method whose computational and memory costs are significantly lower than typical ensembles. BatchEnsemble achieves this by defining each weight matrix to be the Hadamard product of a shared weight among all ensemble members and a rank-one matrix per member. Unlike ensembles, BatchEnsemble is not only parallelizable across devices, where one device trains one member, but also parallelizable within a device, where multiple ensemble members are updated simultaneously for a given mini-batch. Across CIFAR-10, CIFAR-100, WMT14 EN-DE/EN-FR translation, and out-of-distribution tasks, BatchEnsemble yields competitive accuracy and uncertainties as typical ensembles; the speedup at test time is 3X and memory reduction is 3X at an ensemble of size 4. We also apply BatchEnsemble to lifelong learning, where on Split-CIFAR-100, BatchEnsemble yields comparable performance to progressive neural networks while having a much lower computational and memory costs. We further show that BatchEnsemble can easily scale up to lifelong learning on Split-ImageNet which involves 100 sequential learning tasks. * Partial work done as part of the Google Student Researcher Program.",
    "prev": "As we propose to replace a single model architecture with several subnetworks, we study the behavior of PE on various sizes architectures: ResNet-18, ResNet-50 (He et al., 2016), and Wide ResNet28-10 (Zagoruyko & Komodakis, 2016).",
    "curr": "We compare it against Deep Ensembles (Lakshminarayanan et al., 2017) and three other approximated ensembles from the literature: BatchEnsemble (Wen et al., 2019), MIMO (Havasi et al., 2021), andMasksembles (Durasov et al., 2021).",
    "next": "Second, we report our results for Packed-Ensembles on ImageNet (Deng et al., 2009), which we compare against all baselines."
  },
  {
    "index": 2999,
    "source_corpus_id": 247447572,
    "ref_id": "b19",
    "citation_corpus_id": 225094135,
    "start": 32219,
    "end": 32242,
    "title": "Published as a conference paper at ICLR 2021 LEARNING TO REPRESENT ACTION VALUES AS A HYPERGRAPH ON THE ACTION VERTICES",
    "abstract": "Action-value estimation is a critical component of many reinforcement learning (RL) methods whereby sample complexity relies heavily on how fast a good estimator for action value can be learned. By viewing this problem through the lens of representation learning, good representations of both state and action can facilitate action-value estimation. While advances in deep learning have seamlessly driven progress in learning state representations, given the specificity of the notion of agency to RL, little attention has been paid to learning action representations. We conjecture that leveraging the combinatorial structure of multi-dimensional action spaces is a key ingredient for learning good representations of action. To test this, we set forth the action hypergraph networks framework-a class of functions for learning action representations in multi-dimensional discrete action spaces with a structural inductive bias. Using this framework we realise an agent class based on a combination with deep Q-networks, which we dub hypergraph Q-networks. We show the effectiveness of our approach on a myriad of domains: illustrative prediction problems under minimal confounding effects, Atari 2600 games, and discretised physical control benchmarks.Published as a conference paper at ICLR 2021Our results advocate for the general usefulness of leveraging the combinatorial structure of multidimensional discrete action spaces, especially in problems with larger action spaces.",
    "prev": "Finally, we contemplate research on numerous special instances as future work, following our theoretical foundation.",
    "curr": "Also, we believe that studying the combination of our general value mapping ideas with value decomposition (Tavakoli et al., 2021), instead of the the reward decomposition paradigm studied in this paper, could prove to be a fruitful direction for future research.",
    "next": "REPRODUCIBILITY STATEMENT\n\nWe release a generic codebase, built upon the Dopamine framework (Castro et al., 2018), with the option of using arbitrary compositions of mapping functions and reward decomposition schemes as easy-to-code modules."
  }
]