[
  {
    "index": 0,
    "source_corpus_id": 252715594,
    "ref_id": "b57",
    "citation_corpus_id": 238582653,
    "start": 11564,
    "end": 11568,
    "prev": "Quantization and Losses:\n\nTo learn a discrete latent space, we quantize our encoder outputs into the entries of a learned codebook via the vector quantization (VQ) objective in VQVAEs [45],\nL VQ = sg(z) − e 2 2 + β z − sg(e) 2 2 ,(1)\nwhere sg(x) ≡ x, and d dx sg(x) ≡ 0 is the stop-gradient operator, β is the commitment loss weight, and e is a codebook vector from codebook E. The index to the codebook vector closest to z is found by i = argmin j z − E j 2 2 .",
    "curr": "In addition to the VQ objective, we adopt the factorized and 2normalized codes from ViT-VQGAN [58] to improve codebook usage and reconstruction quality.",
    "next": "To train our model, we use a combination of L 2 loss, image perceptual loss L IP [20,61], video perceptual loss L VP by using the I3D network [6] as feature extractor, and adversarial loss L Adv with StyleGAN architecture [21]."
  },
  {
    "index": 8,
    "source_corpus_id": 212996548,
    "ref_id": "b9",
    "citation_corpus_id": 52967399,
    "start": 2418,
    "end": 2439,
    "prev": "INTRODUCTION\n\nTransformer (Vaswani et al., 2017) is widely used in natural language processing due to its high training efficiency and superior capability in capturing long-distance dependencies.",
    "curr": "Building on top of them, modern state-of-the-art models, such as BERT (Devlin et al., 2019), are able to learn powerful language representations from unlabeled text and even surpass the human performance on the challenging question answering task.",
    "next": "However, the good performance comes at a high computational cost."
  },
  {
    "index": 9,
    "source_corpus_id": 202719276,
    "ref_id": "b8",
    "citation_corpus_id": 6706414,
    "start": 2268,
    "end": 2292,
    "prev": "INTRODUCTION\n\nDeep learning has achieved a remarkable performance breakthrough on various challenging benchmarks in machine learning fields, such as image classification (Krizhevsky et al., 2012) and speech recognition .",
    "curr": "However, recent studies (Szegedy et al., 2014;Goodfellow et al., 2015) have revealed that deep neural network models are strikingly susceptible to adversarial examples, in which small perturbations around the input are sufficient to mislead the predictions of the target model.",
    "next": "Moreover, such perturbations are almost imperceptible to humans and often transfer across diverse models to achieve black-box attacks (Papernot et al., 2017;Liu et al., 2017)."
  },
  {
    "index": 10,
    "source_corpus_id": 220665539,
    "ref_id": "b22",
    "citation_corpus_id": 5834589,
    "start": 3440,
    "end": 3473,
    "prev": "This question is motivated by the surprising realization over the past decade that deep neural network training can be performed almost entirely with first-order stochastic optimization.",
    "curr": "In fact, empirical evidence supports the hypothesis that the regularizing effect of gradient noise assists model generalization (Keskar et al., 2017;Smith and Le, 2018;Hochreiter and Schmidhuber, 1997).",
    "next": "Stochastic gradient descent variants such as AdaGrad (Duchi et al., 2011) and Adam (Kingma and Ba, 2015) form the core of almost all successful optimization techniques for these models, using small subsets of the data to form the noisy gradient estimates."
  },
  {
    "index": 12,
    "source_corpus_id": 264802502,
    "ref_id": "b11",
    "citation_corpus_id": 28202810,
    "start": 2358,
    "end": 2380,
    "prev": " offline RL algorithms conditioned on observation histories suffer from poor sample complexity, in accordance with the above intuition.We then identify sufficient conditions under which offline RL can still be efficient -intuitively, it needs to learn a compact representation of history comprising only features relevant for action selection.We introduce a bisimulation loss that captures the extent to which this happens, and propose that offline RL can explicitly optimize this loss to aid worst-case sample complexity.Empirically, we show that across a variety of tasks either our proposed loss improves performance, or the value of this loss is already minimized as a consequence of standard offline RL, indicating that it correlates well with good performance.",
    "curr": "INTRODUCTION\n\nDeep reinforcement learning (RL) has achieved impressive performance in games (Mnih et al., 2013;Silver et al., 2017;AlphaStar, 2019), robotic locomotion (Schulman et al., 2015;2017), and control (Todorov et al., 2012;Haarnoja et al., 2018).A key challenge in the widespread adoption of RL algorithms is the need for deploying a suboptimal policy in the environment to collect online interactions, which can be detrimental in many applications such as recommender systems (Afsar et al., 2021), healthcare (Shortreed et al., 2011;Wang et al., 2018), and robotics (Kalashnikov et al., 2018).Offline RL aims to learn effective policies entirely from an offline dataset of previously collected demonstrations (Levine et al., 2020), which makes it a promising approach for applications where exploring online from scratch is unsafe or costly.A major reason for the success of offline RL algorithms is their ability to combine components of suboptimal trajectories in the offline dataset using common states, a phenomenon called \"trajectory stitching\" (Fu et al., 2019a;2020).",
    "next": "Most offline RL methods are formulated in a Markov decision process (MDP) where the state is fully observed (Sutton and Barto, 2018).However, in many real-world tasks,"
  },
  {
    "index": 15,
    "source_corpus_id": 253523474,
    "ref_id": "b10",
    "citation_corpus_id": 245906072,
    "start": 3522,
    "end": 3546,
    "prev": "In particular, bounding the smallest eigenvalue of the NTK Gram matrix is a staple technique for establishing convergence guarantees for the optimization (Du et al., 2019a,b;Oymak & Soltanolkotabi, 2020).",
    "curr": "Furthermore, the full spectrum of the NTK Gram matrix governs the dynamics of the empirical risk (Arora et al., 2019b), and the eigenvalues of the associated integral operator characterize the dynamics of the generalization error outside the training set (Bowman & Montufar, 2022;Bowman & Montúfar, 2022).",
    "next": "Moreover, the decay rate of the generalization error for Gaussian process regression using the NTK can be characterized by the decay rate of the spectrum (Caponnetto & De Vito, 2007;Cui et al., 2021;Jin et al., 2022)."
  },
  {
    "index": 23,
    "source_corpus_id": 231632937,
    "ref_id": "b37",
    "citation_corpus_id": 52911937,
    "start": 2092,
    "end": 2114,
    "prev": "In this paper we ask, how do we learn to decompose the task automatically and utilize the decomposition to solve sparse reward problems?",
    "curr": "Deep RL has made great strides solving a variety of tasks recently, with hierarchical RL (hRL) demonstrating promise in solving such sparse reward tasks (Sharma et al., 2019b;Le et al., 2018;Merel et al., 2019;Ranchod et al., 2015).",
    "next": "In hRL, the task is decomposed into a hierarchy of subtasks, where policies at the top of the hierarchy call upon policies below to perform actions to solve their respective subtasks."
  },
  {
    "index": 24,
    "source_corpus_id": 246904522,
    "ref_id": "b38",
    "citation_corpus_id": 5034059,
    "start": 1944,
    "end": 1964,
    "prev": "* Equal contribution.",
    "curr": "INTRODUCTION\n\nOver the past few years, Transformer (Vaswani et al., 2017) has been widely used in various natural language processing (NLP) tasks, including text classification (Wang et al., 2018a), text translation (Ott et al., 2018), question answering (Rajpurkar et al., 2016; and text generation (Brown et al., 2020).",
    "next": "The recent application of Transformer in computer vision (CV) field also demonstrate the potential capacity of Transformer architecture."
  },
  {
    "index": 25,
    "source_corpus_id": 252846609,
    "ref_id": "b18",
    "citation_corpus_id": 52920808,
    "start": 9411,
    "end": 9427,
    "prev": "We call the NTK of a finite network with θ chosen at some point during training the network's empirical NTK.",
    "curr": "Although the empirical NTK cannot exactly model the full training dynamics of finite networks, (Du et al., 2018(Du et al., , 2019a give some non-asymptotic guarantees.",
    "next": "Bi-level optimization with NTK: Let (X d , y d ) and (X p , y p ) denote the clean and poison training examples, respectively, (X t , y t ) denote clean test examples, and (X a , y a ) denote test data with the trigger applied and the target label."
  },
  {
    "index": 26,
    "source_corpus_id": 257834209,
    "ref_id": "b41",
    "citation_corpus_id": 3626819,
    "start": 1592,
    "end": 1612,
    "prev": "In our experiments, SPIN reduces memory requirements, improves accuracy across a range of metalearning tasks, and improves state-of-the-art performance on an important practical problem, genotype imputation.",
    "curr": "INTRODUCTION\n\nRecent advances in deep learning have been driven by large-scale parametric models (Krizhevsky et al., 2012;Peters et al., 2018;Devlin et al., 2019;Ramesh et al., 2022).",
    "next": "Modern parametric models rely on large numbers of weights to capture the signal contained in the training set and to facilitate generalization (Frankle & Carbin, 2018;; as a result, they require non-trivial computational resources (Hoffmann et al., 2022), have limited interpretability (Belinkov, 2022), and impose a significant carbon footprint (Bender et al., 2021)."
  },
  {
    "index": 29,
    "source_corpus_id": 263831863,
    "ref_id": "b21",
    "citation_corpus_id": 49411844,
    "start": 2458,
    "end": 2476,
    "prev": "ation is that the model obtained by the proposed inner optimization can mimic the self-supervised target model.To achieve this, we also introduce the MSE between representations of the inner model and the self-supervised target model on the original full dataset for outer optimization.Lastly, assuming that a feature extractor is fixed, we only optimize a linear head on top of the feature extractor, which allows us to reduce the computational cost and obtain a closedform solution of the head with kernel ridge regression.We empirically validate the effectiveness of our method on various applications involving transfer learning.",
    "curr": "INTRODUCTION\n\nAs a consequence of collecting large-scale datasets and recent advances in parallel data processing, deep models have achieved remarkable success in various machine learning problems.However, some applications such as hyperparameter optimization (Franceschi et al., 2017), continual learning (Lopez-Paz & Ranzato, 2017), or neural architecture search (Liu et al., 2019) require repetitive training processes.In such scenarios, it is prohibitively costly to use all the examples from the huge dataset, which motivates the need to compress the full dataset into a small representative set of examples.Recently, many dataset distillation (or condensation) methods (Wang et al., 2018;Zhao et al., 2021;Zhao & Bilen, 2021;Nguyen et al., 2021a;b;Cazenavette et al., 2022;Zhou et al., 2022;Loo et al., 2022;Zhao & Bilen, 2023) have successfully learned a small number of examples on which we can train a model to achieve performance comparable to the one trained on the full dataset.",
    "next": "Despite the recent success of dataset distillation methods, they are not designed to produce a distilled dataset that can be effectively transferred to downstream tasks (Figure 1-(a)).In other words, we may not achieve meaningful performance improvements when pre-training a model on the distilled dataset and fine-tuning it on the target dataset.However, condensing general-purpose datasets"
  },
  {
    "index": 30,
    "source_corpus_id": 5763832,
    "ref_id": "b28",
    "citation_corpus_id": 6628106,
    "start": 20967,
    "end": 20988,
    "prev": "We use gradient descent with a batch size of 1 robot for optimization, as the problem is not stochastic in nature.",
    "curr": "The parameters are optimized with Adam's rule (Kingma and Ba, 2014) with a learning rate of 0.001.",
    "next": "Every update step with this method takes about 5 s on CPU."
  },
  {
    "index": 32,
    "source_corpus_id": 21850704,
    "ref_id": "b5",
    "citation_corpus_id": 1729177,
    "start": 1836,
    "end": 1854,
    "prev": "There are two prominent types of summarization algorithms.",
    "curr": "First, extractive summarization systems form summaries by copying parts of the input (Neto et al., 2002;Dorr et al., 2003;Nallapati et al., 2017).",
    "next": "Second, abstractive summarization systems generate new phrases, possibly rephrasing or using words that were not in the original text (Chopra et al., 2016;Zeng et al., 2016)."
  },
  {
    "index": 33,
    "source_corpus_id": 239009555,
    "ref_id": "b26",
    "citation_corpus_id": 3536221,
    "start": 3973,
    "end": 3995,
    "prev": "Model-based RL (MBRL) reduces the amount of data required for policy optimization by approximating the environment with a learned model, which we can use to generate simulated state transitions (Sutton, 1990;Racanière et al., 2017;Moerland et al., 2020).",
    "curr": "While early approaches on lowdimensional tasks by Schneider (1997); Deisenroth & Rasmussen (2011) used probabilistic models with closed-form posteriors, recent methods rely on neural networks to scale to complex tasks on discrete (Kaiser et al., 2020) and continuous (Chua et al., 2018;Kurutach et al., 2018) action spaces.",
    "next": "However, the learned representation of the true environment always remains imperfect, which introduces approximation errors to the RL problem (Atkeson & Santamaria, 1997;Abbeel et al., 2006)."
  },
  {
    "index": 36,
    "source_corpus_id": 261697392,
    "ref_id": "b16",
    "citation_corpus_id": 227209335,
    "start": 2939,
    "end": 2942,
    "prev": "However, despite their impressive generation quality, these models often suffer from excessive inference time and computational consumption [5,7,8,9,10].",
    "curr": "This can be attributed to the fact that most of these models are either auto-regressive [13,14,15] or diffusion models [16,17].",
    "next": "For instance, Stable Diffusion, even when using a state-of-the-art sampler [18,19,20], typically requires more than 20 steps to generate acceptable images."
  },
  {
    "index": 38,
    "source_corpus_id": 220302524,
    "ref_id": "b1",
    "citation_corpus_id": 3618568,
    "start": 1983,
    "end": 1986,
    "prev": "Introduction\n\nMany language systems rely on text retrieval as their first step to find relevant information.",
    "curr": "For example, search ranking [1], open domain question answering [2], and fact verification [3,4] all first retrieve relevant documents as the input to their later stage reranking, machine reading, and reasoning models.",
    "next": "All these later-stage models enjoy the advancements of deep learning techniques [5,6], while, in contrast, the first stage retrieval still mainly relies on matching discrete bag-of-words [1,2,3,7]."
  },
  {
    "index": 39,
    "source_corpus_id": 252596001,
    "ref_id": "b20",
    "citation_corpus_id": 209439843,
    "start": 6589,
    "end": 6611,
    "prev": "d and wrote N1 A: that (edited and wrote) (N1) Q: whose N1 was employed by and founded M1 A: whose (N1) (was employed by and founded) (M1) Q: whose N1 married M2 and married N2 A: whose (N1) ((married M2) and (married N2)) Q: that played M2 , played M3 , and played M4 A: that ((played M2) , (played M3) , and (played M4)) Q: that wrote , edited , executive produced , and directed N1 A: that (wrote , edited , executive produced , and directed) (N1) Q: that N3 were written by and art directed by A: that (N3) (were written by and art directed by) Q: Was N1 N2\n\nINTRODUCTION\n\nCompositionality is a key part of human intelligence as it allows us to understand and produce a potentially infinite number of novel combinations of known components (Chomsky, 1957;Montague, 1970;Lake et al., 2017).",
    "curr": "In contrast, standard neural sequence models, transformers and recurrent neural networks, often fail to capture the compositional structure of the problem domain and thus fail to generalize compositionally (Keysers et al., 2020;.",
    "next": "Prior efforts to improve compositional generalization primarily rely on specialized architectures or training procedures (Lake, 2019;Nye et al., 2020;Andreas, 2020;Conklin et al., 2021;Liu et al., 2021)."
  },
  {
    "index": 40,
    "source_corpus_id": 259095643,
    "ref_id": "b15",
    "citation_corpus_id": 10494183,
    "start": 3887,
    "end": 3906,
    "prev": "This can lead to low accuracy or impractical false positive rates that especially impact vulnerable subgroups, such as non-native speakers [Liang et al., 2023].",
    "curr": "One way to enable accurate detection of machine-generated text is through watermarking, where generated text is marked imperceptibly so that its origin can be determined [Atallah et al., 2001, Fang et al., 2017, Kirchenbauer et al., 2023.",
    "next": "Because watermarks rely on subtle patterns in text that are statistically unlikely to be replicated by a human, watermarking enables detectors that achieve high levels of accuracy on relatively short fragments of text."
  },
  {
    "index": 41,
    "source_corpus_id": 247595088,
    "ref_id": "b23",
    "citation_corpus_id": 6628106,
    "start": 20588,
    "end": 20607,
    "prev": "The goal of our first experiments is to give a broad comparison of the proposed HIGs with commonly used optimizers.",
    "curr": "This includes stochastic gradient descent (SGD), Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), RMSprop , Adam (Kingma & Ba, 2015), and Gauss-Newton (GN) applied to mini batches.",
    "next": "The results are shown in figure 2b where all curves show the best runs for each optimizer with suitable hyperparameters independently selected, as explained in the appendix."
  },
  {
    "index": 42,
    "source_corpus_id": 259342096,
    "ref_id": "b43",
    "citation_corpus_id": 237416585,
    "start": 7349,
    "end": 7352,
    "prev": "Among these models, transformer-based language models [49] have emerged as the de facto standard for a wide range of NLP tasks, owing to their unparalleled capabilities in capturing complex linguistic patterns and generalizing across diverse contexts.",
    "curr": "One particularly successful paradigm for training such models is instruction-tuning [44,52,4,28,34,38], which enhances their performance on specific tasks by adapting their pre-trained representations to follow natural language instructions.",
    "next": "While the benefits of Large Language Models (LLMs) are indisputable, their rapidly growing size and computational requirements pose significant challenges in terms of training efficiency, memory footprint, and deployment costs."
  },
  {
    "index": 43,
    "source_corpus_id": 21196492,
    "ref_id": "b20",
    "citation_corpus_id": 11816014,
    "start": 4490,
    "end": 4514,
    "prev": "(2016) show that skip layer connections facilitate signal propagation and alleviate gradient degradation.",
    "curr": "The combination of the deep residual coattention encoder and the mixed objective leads to higher performance across question types, question lengths, and answer lengths on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) compared to our DCN baseline.",
    "next": "The improvement is especially apparent on long questions, which require the model to capture long-range dependencies between the document and the question."
  },
  {
    "index": 45,
    "source_corpus_id": 254926490,
    "ref_id": "b48",
    "citation_corpus_id": 237491751,
    "start": 5512,
    "end": 5531,
    "prev": "Evaluate humans and models on a new benchmark of ambiguously-specified tasks, demonstrating that while pure language models fail to disambiguate the intended task well, sufficiently-large models trained with human feedback data are able to approach or even exceed the performance of our human participants to resolve the ambiguity between tasks 3.",
    "curr": "Show how finetuning on ambiguous in-context prompts and examples can enable traditional language models to surpass the performance of HFD models when evaluated on unseen tasks, providing a promising route towards models that capably manage task ambiguity 2 RELATED WORK\n\n\nAMBIGUITY IN NATURAL LANGUAGE PROCESSING\n\nAmbiguity is a well-studied topic in NLP, with work spanning topics as diverse as search queries (Cronen-Townsend & Croft, 2002;Wang & Agichtein, 2010), question answering (Min et al., 2020;Zhang & Choi, 2021), named entities (Bunescu & Pasca, 2006;Cucerzan, 2007;Dredze et al., 2010), coreference resolution (Webster et al., 2018), machine translation (Stanovsky et al., 2019), and information-seeking dialogues (Aliannejadi et al., 2019;Guo et al., 2021;Aliannejadi et al., 2021;Sun et al., 2022;.",
    "next": "Our work differs from these prior streams of work by studying task ambiguity (Finn et al., 2018;Tamkin et al., 2022c), where the task the agent is being asked to perform is ambiguous, rather than an ambiguous input for a clear task."
  },
  {
    "index": 46,
    "source_corpus_id": 52912260,
    "ref_id": "b22",
    "citation_corpus_id": 14124313,
    "start": 1763,
    "end": 1792,
    "prev": "ckle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task.Yet, even with such meta-learning, the low-data problem in the novel classification task still remains.In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data.TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results.",
    "curr": "INTRODUCTION\n\nRecent breakthroughs in deep learning (Krizhevsky et al., 2012;Simonyan and Zisserman, 2015;He et al., 2016) highly rely on the availability of large amounts of labeled data.However, this reliance on large data increases the burden of data collection, which hinders its potential applications to the low-data regime where the labeled data is rare and difficult to gather.On the contrary, humans have the ability to recognize new objects after observing only one or few instances (Lake et al., 2011).For example, children can generalize the concept of \"apple\" after given a single instance of it.This significant gap between human and deep learning has reawakened the research interest on few-shot learning (Vinyals et al., 2016;Snell et al., 2017;Finn et al., 2017;Ravi and Larochelle, 2017;Lee and Choi, 2018;Xu et al., 2017;Wang et al., 2018).",
    "next": "Few-shot learning aims to learn a classifier that generalizes well with a few examples of each of these classes.Traditional techniques such as fine-tuning (Jia et al., 2014) that work well with deep learning models would severely overfit on this"
  },
  {
    "index": 47,
    "source_corpus_id": 3535369,
    "ref_id": "b8",
    "citation_corpus_id": 6628106,
    "start": 34220,
    "end": 34239,
    "prev": "The number of times we train the value network is fixed to 25, while for the control variate, it was chosen to be a hyperparameter.",
    "curr": "All models were trained using ADAM (Kingma & Ba, 2015), with β 1 = 0.9, β 2 = 0.999, and = 1e − 08.",
    "next": "The baseline A2C case has 2 hyperparameters to tune: the learning rate for the optimizer for the policy and value network."
  },
  {
    "index": 49,
    "source_corpus_id": 247595243,
    "ref_id": "b62",
    "citation_corpus_id": 220363897,
    "start": 9302,
    "end": 9320,
    "prev": "In our imbalanced setting, generative invariance transfer aims to learn transformations from the head classes that apply to the tail classes of the same dataset, and does not assume the data is paired or grouped.",
    "curr": "There is a general interest in obtaining invariances for machine learning models (Benton et al., 2020;Zhou et al., 2021).",
    "next": "Data augmentation (Beymer & Poggio, 1995;Niyogi et al., 1998) can be used to train classifiers to be invariant to certain hand-picked transformations, but requires the practitioner to know and implement those transformations in advance."
  },
  {
    "index": 50,
    "source_corpus_id": 252683543,
    "ref_id": "b6",
    "citation_corpus_id": 44134226,
    "start": 4073,
    "end": 4091,
    "prev": "To avoid this inconsistency, they proposed a self-terminating (ST) language model that uses new parametrization for its classifier rather than usual softmax parametrization.",
    "curr": "They proved that the ST language model is consistent with respect to greedy search, beam search, top-k sampling (Fan et al., 2018) as well as nucleus sampling (Holtzman et al., 2020).",
    "next": "The ST language model increases the termination probability of each sequence monotonically to 1, but this parametrization is not appropriate for learning our natural language."
  },
  {
    "index": 53,
    "source_corpus_id": 202660778,
    "ref_id": "b10",
    "citation_corpus_id": 52920808,
    "start": 8237,
    "end": 8255,
    "prev": "After the ICLR conference submission deadline, Yang & Zhang (2019) revised their paper by adding a new variance reduction algorithm that achieves O(H 2 / 3/2 ) sample complexity, which is also worse than our result by a factor of O(H 2 ).",
    "curr": "Apart from the convergence analysis of the general nonconcave performance functions, there has emerged a line of work Liu et al., 2019; that studies the global convergence of (proximal/trust-region) policy optimization with neural network function approximation, which applies the theory of overparameterized neural networks (Du et al., 2019b;a;Allen-Zhu et al., 2019;Zou et al., 2019;Cao & Gu, 2019) to reinforcement learning.",
    "next": "Notation v 2 denotes the Euclidean norm of a vector v ∈ R d and A 2 denotes the spectral norm of a matrix A ∈ R d×d ."
  },
  {
    "index": 54,
    "source_corpus_id": 235293695,
    "ref_id": "b7",
    "citation_corpus_id": 54443381,
    "start": 2354,
    "end": 2377,
    "prev": "Building a system resembling human learning abilities is a deep-rooted desire since sustainable learning over a long-term period is essential for general artificial intelligence.",
    "curr": "In light of this need, continual learning (CL) (Thrun, 1995), or lifelong learning, tackles a learning scenario where a model continuously learns over a sequence of tasks (Kumar & Daume III, 2012;Li & Hoiem, 2016) within a broad research area, such as classification (Kirkpatrick et al., 2017;Chaudhry et al., 2019a), image generation (Zhai et al., 2019), language learning (Li et al., 2019b;Biesialska et al., 2020), clinical application (Lee & Lee, 2020;Lenga et al., 2020), speech recognition (Sadhu & Hermansky, 2020), and federated learning (Yoon et al., 2021).",
    "next": "A well-known challenge for continual learning is catastrophic forgetting (McCloskey & Cohen, 1989), where the continual learner loses the fidelity for past tasks after adapting the previously learned knowledge to future tasks."
  },
  {
    "index": 56,
    "source_corpus_id": 238582772,
    "ref_id": "b22",
    "citation_corpus_id": 108300573,
    "start": 4309,
    "end": 4331,
    "prev": "This decoupled approach does not fully exploit informative missingness patterns or deal with irregular sampling, thus producing suboptimal Previous studies (Wu et al., 2021;Li et al., 2020a;Zhang et al., 2019) have noted that inter-sensor correlations bring rich information in modeling time series.",
    "curr": "However, only few studies consider relational structure of irregularly sampled time series, and those which do have limited ability in capturing inter-sensor connections (Wu et al., 2021;Shukla & Marlin, 2018).",
    "next": "In contrast, we integrate recent advances in graph neural networks to take advantage of relational structure among sensors."
  },
  {
    "index": 59,
    "source_corpus_id": 220302148,
    "ref_id": "b29",
    "citation_corpus_id": 3300937,
    "start": 2398,
    "end": 2401,
    "prev": "(1)\nWhile ERM is widely used and offers nice statistical properties, it can also perform poorly in practical situations where average performance is not an appropriate surrogate for the objective of interest.",
    "curr": "Significant research has thus been devoted to developing alternatives to traditional ERM for diverse applications, such as learning in the presence of noisy/corrupted data or outliers [25,30], performing classification with imbalanced data [37,38], ensuring that subgroups within a population are treated fairly [36,42,56], or developing solutions with favorable out-of-sample performance [43].",
    "next": "In this paper, we suggest that deficiencies in ERM can be flexibly addressed via a unified framework, tilted empirical risk minimization (TERM)."
  },
  {
    "index": 61,
    "source_corpus_id": 211132990,
    "ref_id": "b32",
    "citation_corpus_id": 3536221,
    "start": 4217,
    "end": 4240,
    "prev": "A further study confirms that deep ensembles generally achieves the best performance on out-of-distribution uncertainty benchmarks (Ovadia et al., 2019;Gustafsson et al., 2019), compared to other methods such as MC-dropout (Gal & Ghahramani, 2015).",
    "curr": "In other applications such as model-based reinforcement learning (Deisenroth & Rasmussen, 2011;Wang et al., 2019), ensembles of neural networks can be used to estimate model uncertainty, leading to better overall performance (Kurutach et al., 2018).",
    "next": "Despite their success on benchmarks, ensembles are limited in practice due to their expensive computational and memory costs, which increase linearly with the ensemble size in both training and testing."
  },
  {
    "index": 64,
    "source_corpus_id": 259375870,
    "ref_id": "b23",
    "citation_corpus_id": 243865663,
    "start": 12470,
    "end": 12488,
    "prev": "Our work focuses on decoder-only models since they are well-suited for text generation and are widely used in LLMs (Brown et al., 2020;Touvron et al., 2023;MosaicML, 2023).",
    "curr": "However, encoder-decoder models have also been extensively studied in the literature in the context of learning arithmetic (Kim et al., 2021;Wang et al., 2021).",
    "next": "Qian et al."
  },
  {
    "index": 65,
    "source_corpus_id": 222141728,
    "ref_id": "b2",
    "citation_corpus_id": 52922363,
    "start": 4476,
    "end": 4496,
    "prev": "(2018); Oymak & Soltanolkotabi (2020), and many more).",
    "curr": "There are also convergence results that focus on linear networks, without nonlinear activations (Bartlett et al., 2018;Arora et al., 2019a;Wu et al., 2019;Du & Hu, 2019;Hu et al., 2020).",
    "next": "These results typically focus on the convergence of loss, hence do not address which of the many global minima is reached."
  },
  {
    "index": 68,
    "source_corpus_id": 253080406,
    "ref_id": "b24",
    "citation_corpus_id": 3626819,
    "start": 7137,
    "end": 7158,
    "prev": "Language models.",
    "curr": "Large language models, such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2018), and GPT-2 (Radford et al., 2019), are able to achieve state-of-the-art performance on many standard NLP benchmarks.",
    "next": "More recent works, such as GPT-3 (Brown et al., 2020), PALM (Chowdhery et al., 2022), and Chinchilla (Hoffmann et al., 2022) further enable few-shot learning from textual prompts."
  },
  {
    "index": 69,
    "source_corpus_id": 259833441,
    "ref_id": "b16",
    "citation_corpus_id": 13046179,
    "start": 5144,
    "end": 5148,
    "prev": "However, in real-world classifier deployment, one may encounter out-of-distribution (OOD) test samples, i.e., samples drawn from some distinct distribution P out = P in (e.g., images of aeroplanes).",
    "curr": "Out-ofdistribution detection is the problem of accurately identifying such OOD samples, and has received considerable study of late [18,30,20,43,23,22,48,51,3,26,52,46,21].",
    "next": "An accurate OOD detector allows one to abstain from making a prediction on OOD samples, rather than making an egregiously incorrect prediction; this yields more reliable and trust-worthy classifiers."
  },
  {
    "index": 75,
    "source_corpus_id": 231918471,
    "ref_id": "b25",
    "citation_corpus_id": 108304275,
    "start": 8791,
    "end": 8815,
    "prev": "Moving within the maximum-entropy RL framework (Ziebart, 2010;Levine, 2018), Ho & Ermon (2016) noted that the full procedure (RL • IRL) can be interpreted equivalently as the minimisation of some divergence between occupancy measures of the imitator and demonstrator:\n\narg min\nπ {ψ * (ρ π − ρ π D ) − H(π)},(1)\nwith H(π) being the discounted causal entropy (Bloem & Bambos, 2014) of the policy and ψ * the Fenchel conjugate of a chosen regulariser on the form of the reward.",
    "curr": "These are typically optimised in an adversarial fashion (Goodfellow et al., 2014) and given the focus on evaluating ρ π this often requires extensive interaction with the environment, otherwise banking on approximations over a replay buffer (Kostrikov et al., 2018) or a reformulation of the divergence to allow for off-policy evaluation (Kostrikov et al., 2019).",
    "next": "Bear in mind that optimal policies within the maximum-entropy framework are parameterised by a Boltzmann distribution:\nπ(a|s) = exp(Q(s, a)) b∈A exp(Q(s, b)) ,(2)\nwith Q(s, a) the soft Q-function, defined recursively via the soft Bellman-equation:\nQ(s, a) R(s, a) + γE s ∼ρπ soft max a Q(s , a )) ."
  },
  {
    "index": 76,
    "source_corpus_id": 7942973,
    "ref_id": "b1",
    "citation_corpus_id": 6015236,
    "start": 1542,
    "end": 1562,
    "prev": "We also show which hyperparameter choices had a significant effect on parsing accuracy, allowing us to achieve large gains over other graph-based approaches.",
    "curr": "INTRODUCTION\n\nDependency parsers-which annotate sentences in a way designed to be easy for humans and computers alike to understand-have been found to be extremely useful for a sizable number of NLP tasks, especially those involving natural language understanding in some way (Bowman et al., 2016;Angeli et al., 2015;Levy & Goldberg, 2014;Toutanova et al., 2016;Parikh et al., 2015).",
    "next": "However, frequent incorrect parses can severely inhibit final performance, so improving the quality of dependency parsers is needed for the improvement and success of these downstream tasks."
  },
  {
    "index": 80,
    "source_corpus_id": 246996534,
    "ref_id": "b1",
    "citation_corpus_id": 52889459,
    "start": 2032,
    "end": 2051,
    "prev": "* Indicates equal contribution arXiv:2202.08937v2 [cs.LG]\n\nINTRODUCTION\n\nThese days, generative adversarial networks (GANs) (Goodfellow et al., 2014) can successfully approximate the high-dimensional distributions of real images.",
    "curr": "The exceptional quality of the state-ofthe-art GANs (Karras et al., 2020b;Brock et al., 2019) makes them a key ingredient in applications, including semantic editing (Isola et al., 2017;Zhu et al., 2018;Shen et al., 2020;Voynov & Babenko, 2020), image processing (Pan et al., 2020;Ledig et al., 2017;Menon et al., 2020), video generation (Wang et al., 2018a), producing high-quality synthetics (Zhang et al., 2021;Voynov et al., 2020).",
    "next": "To extend the success of GANs to the limited-data regime, it is common to use pretraining, i.e., to initialize the optimization process by the GAN checkpoint pretrained on some large dataset."
  },
  {
    "index": 82,
    "source_corpus_id": 247411320,
    "ref_id": "b29",
    "citation_corpus_id": 6212000,
    "start": 1595,
    "end": 1614,
    "prev": "Our code is available at: https://github.com/MSU-MLSys-Lab/DeepAA.",
    "curr": "INTRODUCTION\n\n... Data augmentation (DA) is a powerful technique for machine learning since it effectively regularizes the model by increasing the number and the diversity of data points (Goodfellow et al., 2016;Zhang et al., 2017).",
    "next": "A large body of data augmentation transformations has been proposed (Inoue, 2018;Zhang et al., 2018;DeVries & Taylor, 2017;Yun et al., 2019;Hendrycks et al., 2020;Yan et al., 2020) to improve model performance."
  },
  {
    "index": 83,
    "source_corpus_id": 252682980,
    "ref_id": "b13",
    "citation_corpus_id": 28202810,
    "start": 17633,
    "end": 17656,
    "prev": "Furthermore, plenty of practical algorithms leverage fitted-Q subroutines for updating the critic step (e.g.",
    "curr": "[Schulman et al., 2017, Haarnoja et al., 2018) with different differentiable function choices.",
    "next": "In addition, we also incorporate pessimism for the design."
  },
  {
    "index": 85,
    "source_corpus_id": 249210151,
    "ref_id": "b71",
    "citation_corpus_id": 238419650,
    "start": 2237,
    "end": 2256,
    "prev": "Optimal transport (OT) is a powerful framework to solve mass-moving problems for data distributions which finds many applications in machine learning and computer vision (Bonneel & Digne, 2023).Most existing methods to compute OT plans are designed for discrete distributions (Flamary et al., 2021;Peyré et al., 2019;Cuturi, 2013).These methods have good flexibility: they allow to control the properties of the plan via choosing the cost function.However, discrete methods find an optimal matching between two given (train) sets which does not generalize to new (test) data points.This limits the applications of discrete OT plan methods to scenarios when one needs to generate new data, e.g., image-to-image transfer (Zhu et al., 2017).",
    "curr": "Recent works (Rout et al., 2022;Korotin et al., 2023b;2021b;Fan et al., 2021a;Daniels et al., 2021) propose continuous methods to compute OT plans.Thanks to employing neural networks to parameterize OT solutions, the learned transport plan can be used directly as the generative model in data synthesis (Rout et al., 2022) and unpaired learning (Korotin et al., 2023b;Rout et al., 2022;Daniels et al., 2021;Gazdieva et al., 2022).",
    "next": "Existing continuous OT methods mostly focus on classic cost functions such as ℓ 2 (Korotin et al., 2021b;2023b;Fan et al., 2021a;Gazdieva et al., 2022) which estimate the closeness of input and 1 BACKGROUND AND NOTATIONS\n\nIn this section, we provide key concepts of the optimal transport theory.Throughout the paper, we consider compact X = Y ⊂ R D and P, Q ∈ P(X ), P(Y)."
  },
  {
    "index": 86,
    "source_corpus_id": 252668614,
    "ref_id": "b4",
    "citation_corpus_id": 10550488,
    "start": 7484,
    "end": 7504,
    "prev": "Furthermore, perplexity assigns cnn dailymail even lower scores than the in-domain xsum.",
    "curr": "For translation, the model is trained on WMT15 dataset and evaluated on other WMT test splits (Bojar et al., 2015), OPUS100 (Aulamo & Tiedemann, 2019), and MTNT (Michel & Neubig, 2018).",
    "next": "The in-domain and OOD datasets perplexity densities overlap even more."
  },
  {
    "index": 87,
    "source_corpus_id": 21946795,
    "ref_id": "b23",
    "citation_corpus_id": 9059612,
    "start": 3552,
    "end": 3575,
    "prev": "showed that adversarially trained models can be made robust to white-box attacks (i.e., with knowledge of the model parameters) if the perturbations computed during training closely maximize the model's loss.",
    "curr": "However, prior attempts at scaling this approach to ImageNet-scale tasks (Deng et al., 2009) have proven unsuccessful (Kurakin et al., 2017b).",
    "next": "It is thus natural to ask whether it is possible, at scale, to achieve robustness against the class of black-box adversaries Towards this goal, Kurakin et al."
  },
  {
    "index": 88,
    "source_corpus_id": 219636462,
    "ref_id": "b14",
    "citation_corpus_id": 3693512,
    "start": 2190,
    "end": 2194,
    "prev": "At the heart of catastrophic forgetting is the stability-plasticity dilemma [2,3], where a model exhibits high stability on previously trained tasks, but suffers from low plasticity for the integration of new knowledge (and vice-versa).",
    "curr": "Attempts to overcome this challenge in neural network-based CL can be grouped into three main strategies: regularization methods [4][5][6][7][8][9], memory replay [10][11][12][13], and dynamic network architecture [14][15][16].",
    "next": "In particular, regularization methods that control model weights bear the longest history due to its simplicity and efficiency to control the trade-off for a fixed model capacity."
  },
  {
    "index": 90,
    "source_corpus_id": 252907593,
    "ref_id": "b14",
    "citation_corpus_id": 52967399,
    "start": 2581,
    "end": 2602,
    "prev": "Such coordinated representations are particularly useful for tasks such as cross-modal retrieval.",
    "curr": "Another vetted commonly used self-supervised learning framework is Masked Data Modeling (MDM), which learns a meaningful representation with the pretext task of recovering the original inputs or features from the corrupted ones (Devlin et al., 2019).",
    "next": "Particularly, based on the Audio Spectrogram Transformer (Gong et al., 2021a) and Vision Transformer (Dosovitskiy et al., 2020) backbones, the single-modal Masked Auto-Encoder (MAE)  achieved state-of-the-art (SOTA) performance on images and audio tasks (Huang et al., 2022a) individually."
  },
  {
    "index": 91,
    "source_corpus_id": 203593909,
    "ref_id": "b17",
    "citation_corpus_id": 10480989,
    "start": 2616,
    "end": 2638,
    "prev": "In the context of sequence generation problems, semi-supervised approaches have been shown to work well in some cases.",
    "curr": "For example, back-translation (Sennrich et al., 2015) makes use of the monolingual data on the target side to improve machine translation systems, latent variable models  are employed to incorporate unlabeled source data to facilitate sentence compression (Miao & Blunsom, 2016) or code generation (Yin et al., 2018).",
    "next": "In this work, we revisit a much older and simpler semi-supervised method, self-training (ST, Scudder (1965)), where a base model trained with labeled data acts as a \"teacher\" to label the unannotated data, which is then used to augment the original small training set."
  },
  {
    "index": 92,
    "source_corpus_id": 219708742,
    "ref_id": "b2",
    "citation_corpus_id": 52967399,
    "start": 6440,
    "end": 6461,
    "prev": " activation functions other than ReLU have appeared in the literature.Johnson (2019) shows that if the activation function ρ is uniformly continuous and can be uniformly approximated by a sequence of one-to-one functions, a width-d x network cannot universally approximate C(K, R).Kidger and Lyons (2020) show that if ρ is continuous, nonpolynomial, and continuously differentiable at at least one point (say z) with ρ (z) = 0, then networks of width d x + d y + 1 with activation ρ are dense in C(K, R dy ).Furthermore, Kidger and Lyons (2020) prove that ReLU networks of width d x + d y + 1 are dense in L p (R dx , R dy ).",
    "curr": "Limitations of prior arts.Note that none of the existing works succeeds in closing the gap between the upper bound (at least d x + d y ) and the lower bound (at most d x + 1).This gap is significant especially for applications with high-dimensional codomains (i.e., large d y ) such as image generation (Kingma and Welling, 2013;Goodfellow et al., 2014), language modeling (Devlin et al., 2019;Liu et al., 2019), and molecule generation (Gómez-Bombarelli et al., 2018;Jin et al., 2018).In the prior arts, the main bottleneck for proving an upper bound below d x + d y is that they maintain all d x neurons to store the input and all d y neurons to construct the function output; this means every layer already requires at least d x + d y neurons.In addition, the proof techniques for the lower bounds only consider the input dimension d x regardless of the output dimension d y .",
    "next": "Summary of results\n\nWe mainly focus on characterizing the minimum width of ReLU networks for universal approximation.Nevertheless, our results are not restricted to ReLU networks; they can be generalized to networks with general activation functions.Our contributions can be summarized as follows."
  },
  {
    "index": 93,
    "source_corpus_id": 220768638,
    "ref_id": "b8",
    "citation_corpus_id": 57189211,
    "start": 5159,
    "end": 5162,
    "prev": "Developing a factorized scene representation has been a core research topic in computer vision for decades.",
    "curr": "Most learning-based prior works are supervised, requiring annotated specification such as segmentations [9], patches [5], or simulation engines [27,10].",
    "next": "These supervised approaches face two challenges."
  },
  {
    "index": 94,
    "source_corpus_id": 84186721,
    "ref_id": "b1",
    "citation_corpus_id": 5590763,
    "start": 33895,
    "end": 33913,
    "prev": "At the first time step these messages, m\n\nv , are initialized with the respective atom features shown in Table 4.",
    "curr": "GGNNs then update these messages in a recursive nature:\nm (s) v = GRU   m (s−1) v , i∈Ne1(v) f single m (s−1) i + j∈Ne2(v) f double m (s−1) j + k∈Ne3(v) f triple m (s−1) k  (7)\nWhere GRU is a Gated Recurrent Unit (Cho et al., 2014), the functions N e1 (v), N e2 (v), N e3 (v) index the nodes connected by single, double and triple bonds to node v respectively and f single (·), f double (·) and f triple (·) are linear transformations with learnable parameters.",
    "next": "This process continues for S steps (where we choose S = 4)."
  },
  {
    "index": 95,
    "source_corpus_id": 254535921,
    "ref_id": "b7",
    "citation_corpus_id": 231627730,
    "start": 8406,
    "end": 8424,
    "prev": "icitly using regularization on the learned Q-values (Kumar et al., 2020).The limitation of existing offline RL approaches is that the derived policies can only act under a fixed degree of conservatism, which is determined by an opaque hyperparameter that scales the estimated epistemic uncertainty, and has to be chosen during offline training.This means the policies will be unable to correct their behavior online, even if it becomes evident from online observations that the estimated value function is too pessimistic or optimistic.",
    "curr": "Our algorithm learns confidence-conditioned Q-values that capture all possible degrees of pessimism by conditioning on the confidence level, modeling epistemic uncertainty as a function of confidence.By doing so, instead of committing to one degree of pessimism, we enable policies that adapt how conservative they should behave using the observations they sees during online evaluation.Our approach is related to ensemble (Agarwal et al., 2020;Lee et al., 2021;Chen et al., 2021;An et al., 2021) approaches in that they also predict multiple Q-values to model epistemic uncertainty.However, existing ensemble methods train individual Q-values on the same objective, and rely on different parameter initializations.In contrast, each of our Q-values captures a different confidence-level.In addition, standard ensemble approaches do not consider adaptive policies.Recently, APE-V proposes using ensembles to learn adaptive policies that condition on belief over which value function is most accurate (Ghosh et al., 2022).Our approach considers a similar strategy for adaptation, but explicitly parameterizes the value function by the confidence level, introducing a novel training objective for this purpose.In our experiments, we compare to a method that adapts APE-V to our discrete-action benchmark tasks.Jiang & Huang (2020); Dai et al.",
    "next": "(2020) propose confidence intervals for policy evaluation at specified confidence-levels.We aim to learn a value function across all confide"
  },
  {
    "index": 97,
    "source_corpus_id": 1880070,
    "ref_id": "b33",
    "citation_corpus_id": 780171,
    "start": 6329,
    "end": 6350,
    "prev": "There are many obvious cases where these metrics fail, as they are often incapable of considering the semantic similarity between responses (see Figure 1).",
    "curr": "Despite this, many researchers still use BLEU to evaluate their dialogue models (Ritter et al., 2011;Sordoni et al., 2015b;Li et al., 2015;Galley et al., 2015;Li et al., 2016a), as there are few alternatives available that correlate with human judgements.",
    "next": "While human evaluation should always be used to evaluate dialogue models, it is often too expensive and time-consuming to do this for every model specification (for example, for every combination of model hyperparameters)."
  },
  {
    "index": 98,
    "source_corpus_id": 263829563,
    "ref_id": "b2",
    "citation_corpus_id": 237561567,
    "start": 7856,
    "end": 7873,
    "prev": "he Pile (Gao et al., 2020), and GPT-3 (Brown et al., 2020) introduce various pipelines for extracting quality data from Common Crawl for the purposes of language model training.These pipelines typically consist of three primary steps: text extraction, filtering, and deduplication.",
    "curr": "Text extraction\n\nExtracting plain text from HTML files is a critical step in the creation of Common Crawl-based datasets.The easiest way to extract text from Common Crawl documents is to use the WET corresponding to each webpage, which contains pre-extracted plain text of the webpage.CCNet and C4 both use Common Crawl's WET files.However, the text extracted in WET files may contain too much boilerplate or miss out on important content such as L A T E X equations.It is also possible to extract text directly from the raw HTML found in Common Crawl WARC files.The Pile uses an open source library called jusText (Endrédy & Novák, 2013) to extract text from HTML while RefinedWeb uses a library called Trafilatura (Barbaresi, 2021).These text extraction approaches differ in terms of extraction speed, customization, and their precision and recall for removing boilerplate content.",
    "next": "Filtering The first layer of filtering often involves language identification (Wenzek et al., 2019).Language filtering is used because certain other parts of the pipeline only work for specific languages, and is often done with simple linear classifiers such as from fastText (Joulin et al., 2016).Quality filtering can be done with a combination of perplexity, classifier, and rule-based methods.CCNet uses a 5-gram Kneser-Ney language model implemented in the KenLM library (Heafield, 2011) trained on the target domain.The documents in the dataset are then sorted and filtered by their perplexity under this model.Other datasets such as the one used to train GPT-3 (Brown et al., 2020) use a classifier-based approach.This involves training a classifier on known-high-quality documents, such as those from Wikipedia, as positive examples and unf"
  },
  {
    "index": 100,
    "source_corpus_id": 261696510,
    "ref_id": "b9",
    "citation_corpus_id": 215745286,
    "start": 3723,
    "end": 3744,
    "prev": "Select Implement def transform_grid(grid): out_grid = np.zeros_like(grid) for col in range(grid.shape[1]): non_zeros = \\ grid[:, col][grid[:, col] != 0] if len(non_zeros) > 0: out_grid[-len(non_zeros):,col]= \\ on_zeros return out_grid def transform_grid(grid): return ...\n\nINTRODUCTION\n\nInductive reasoning -the ability to infer general principles from specific examples and apply them to novel situations -is a core aspect of human intelligence (Peirce, 1868).",
    "curr": "Recently, large-scale pre-trained language models have received significant interest for their performance across a diverse range of reasoning tasks such as commonsense, arithmetic and symbolic reasoning (Rajani et al., 2019;Shwartz et al., 2020;Nye et al., 2021;Wei et al., 2022;Marasović et al., 2021;Lampinen et al., 2022;Zelikman et al., 2022;Zhou et al., 2022).",
    "next": "There has been extensive discussion of language models' impressive \"in-context learning\" capabilities, a form of inductive reasoning."
  },
  {
    "index": 101,
    "source_corpus_id": 252367996,
    "ref_id": "b21",
    "citation_corpus_id": 3488815,
    "start": 2196,
    "end": 2216,
    "prev": "In\n\nINTRODUCTION\n\nAs machine learning models are increasingly deployed in security or safety-critical settings, robustness becomes an essential property.",
    "curr": "Adversarial training (Madry et al., 2018) is the state-of-the-art method for improving the adversarial robustness of deep neural networks.",
    "next": "Recent work has made substantial progress in robustness by scaling adversarial training to very large datasets."
  },
  {
    "index": 103,
    "source_corpus_id": 222133031,
    "ref_id": "b9",
    "citation_corpus_id": 6628106,
    "start": 5267,
    "end": 5288,
    "prev": "This class encompasses many known schemes such as gradient descent with momentum, or full-matrix AdaGrad.",
    "curr": "However, methods such as Adam [Kingma and Ba, 2015] or diagonal AdaGrad [Duchi et al., 2011] are not in this class, as the diagonal re-scaling creates iterates x t outside the span of previous gradients.",
    "next": "Although we will focus on the distance to the solution, the results can be extended to other convergence criteria such as F (x t ) 2 ."
  },
  {
    "index": 104,
    "source_corpus_id": 247451000,
    "ref_id": "b6",
    "citation_corpus_id": 231662264,
    "start": 27174,
    "end": 27194,
    "prev": "This suggests that one can compensate for the removal of BN layers, at least in terms of their effect on the behaviour of the network at initialization time, by down-scaling the residual branch of each residual block.",
    "curr": "Arguably, almost all recent work on training deep networks without normalization layers (Zhang et al., 2018;Shao et al., 2020;Bachlechner et al., 2020;Brock et al., 2021a;b) has adopted this idea by introducing multipliers on the residual branches (which may or may not be optimized during training).",
    "next": "In Table 2, we show that one can close most of the gap with standard ResNets by simply adopting the modification in equation 6 without using BN layers."
  },
  {
    "index": 107,
    "source_corpus_id": 264555202,
    "ref_id": "b28",
    "citation_corpus_id": 52115700,
    "start": 30655,
    "end": 30680,
    "prev": "All these works, however, focus on protecting training data, without considering context, and rely heavily on having a well-defined notion of a single record.While this is ideal for tabular data, it is extremely Preprint   hard to define for language, as drawing borders around a unit of language that needs protection is not always feasible (Brown et al., 2022) and different units might need different levels of protection, based on information type and context.Our work, however, differs from existing literature in two main aspects: (1) we focus on the impact that context has on privacy, and how reasoning about this context is crucial in making judgments when it comes to language, and (2) we shift attention away from training data and towards interactions with the model, as providing lengthy history for the model is becoming more and more relevant.",
    "curr": "Theory of Mind (ToM) and LLMs\n\nThe development of ToM abilities has been a long-standing goal in AI research (Nematzadeh et al., 2018;Le et al., 2019;Sap et al., 2019;Shapira et al., 2023b;Kim et al., 2023).Although qualitative assessments might imply a degree of ToM in LLMs (Whang, 2023), more comprehensive quantitative investigations reveal that LLMs still struggle to reason ToM robustly (Sap et al., 2022;Shapira et al., 2023a;Ullman, 2023;Kim et al., 2023).This might account for the poor performance of LLMs on our benchmark.",
    "next": "Ethics and morality for LLMs Revealing secrets often involves making moral decisions in the real world.Many previous works focus on inferring the morality of the behavior based on textual descriptions Preprint of scenarios (Jiang et al., 2021;Zhou et al., 2023a;Forbes et al., 2020), while more works start to integrate social contexts into the machine morality discourse (Kim et al., 2022b;Pyatkin et al., 2023;Jin et al., 2022)."
  },
  {
    "index": 108,
    "source_corpus_id": 245906072,
    "ref_id": "b17",
    "citation_corpus_id": 6212000,
    "start": 2118,
    "end": 2138,
    "prev": "This is surprising from an optimization point-of-view because the ERM problem induced by neural networks is nonconvex (Sontag & Sussmann, 1989, 1991 and can even be NP-Complete in certain cases (Blum & Rivest, 1993).",
    "curr": "Perhaps even more surprising is that the discovered solution can generalize even when the network is able to fit arbitrary labels (Zhang et al., 2017), rendering traditional complexity measures such as Rademacher complexity inadequate.",
    "next": "How does deep learning succeed in the face of pathological behavior by the standards of classical optimization and statistical learning theory?"
  },
  {
    "index": 110,
    "source_corpus_id": 263835059,
    "ref_id": "b22",
    "citation_corpus_id": 231934149,
    "start": 5608,
    "end": 5612,
    "prev": "ts variants [24,26] still being used as static feature descriptors that require domain-specific hyperparameter choices.By contrast, our approach makes the ECT end-to-end trainable, resulting in an efficient and effective shape descriptor that can be integrated into deep learning models.Subsequently, we demonstrate such integrations both on the level of loss terms as well as on the level of novel computational layers.",
    "curr": "In a machine learning context, the choice of model is typically dictated by the type of data.For point clouds, a recent survey [14] outlines a plethora of models for point cloud analysis tasks like classification, many of them being based on learning equivariant functions [41].When additional structure is being present in the form of graphs or meshes, graph neural networks (GNNs) are typically employed for classification tasks [42], with some methods being capable to either learn explicitly on such higher-order domains [3,4,10,15,16] or harness their topological features [23,32].",
    "next": "Mathematical Background\n\nPrior to discussing our method and its implementation, we provide a self-contained description to the Euler Characteristic Transform (ECT).The ECT is often relying on simplicial complexes, the central building blocks in algebraic topology, which are extensively used for calculating homology groups and proving a variety of properties of topological spaces.While numerous variants of simplicial complexes exist, we will focus on those that are embedded in R n .Generally, simplicial complexes are obtained from on a set of points, to which higher-order elements-simplices-such as Figure 1: We construct a simplicial from an image of the MNIST data set (using a Delaunay complex construction on the non-zero pixels).For each choice of direction on S 1 , we obtain a Euler Characteristic Curve.The collection of all these curves constitutes the Euler Characteristic Transform.Existing work typically concatenates all these curves to obtain a static feature vector, where"
  },
  {
    "index": 111,
    "source_corpus_id": 256808748,
    "ref_id": "b26",
    "citation_corpus_id": 210064473,
    "start": 2313,
    "end": 2330,
    "prev": "INTRODUCTION\n\nObjects, and their interactions, are the foundations of human cognition (Spelke & Kinzler, 2007).",
    "curr": "The endowment on making abstractions from perception and organizing them systematically empowers humans the ability to accomplish and generalize across a broad range of tasks, such as scene modeling (Bear et al., 2020), visual reasoning (Yi et al., 2020), and simulating interactions (Bear et al., 2020).",
    "next": "The key to such success lies in the emergence of symbol-like mental representations of object concepts (Whitehead, 1928)."
  },
  {
    "index": 112,
    "source_corpus_id": 53452703,
    "ref_id": "b5",
    "citation_corpus_id": 28971531,
    "start": 2480,
    "end": 2502,
    "prev": "Perone et al.",
    "curr": "(2018) have shown that the best encoding architectures are based on recurrent neural networks (RNNs) (Conneau et al., 2017;Peters et al., 2018) or the Transformer architecture (Cer et al., 2018).",
    "next": "These techniques are, however, substantially more expensive to train and apply than word embeddings (Hill et al., 2016;Cer et al., 2018)."
  },
  {
    "index": 114,
    "source_corpus_id": 232320210,
    "ref_id": "b4",
    "citation_corpus_id": 51979536,
    "start": 1995,
    "end": 2015,
    "prev": "ut also additionally provides a deterministic compressed representation of the input variable, which is useful for inference tasks that require consistent representation.Moreover, it can jointly learn a feature extractor and select features considering each feature dimension's relevance to the target task, which is unattainable by most neural network-based IB methods.We propose an exploration method based on Drop-Bottleneck for reinforcement learning tasks.In a multitude of noisy and reward sparse maze navigation tasks in VizDoom(Kempka et al., 2016)and DM-Lab (Beattie et al., 2016), our exploration method achieves state-of-the-art performance.As a new IB framework, we demonstrate that Drop-Bottleneck outperforms Variational Information Bottleneck (VIB)(Alemi et al., 2017)in multiple aspects including adversarial robustness and dimensionality reduction.",
    "curr": "INTRODUCTION\n\nData with noise or task-irrelevant information easily harm the training of a model; for instance, the noisy-TV problem (Burda et al., 2019a) is one of well-known such phenomena in reinforcement learning.If observations from the environment are modified to contain a TV screen, which changes its channel randomly based on the agent's actions, the performance of curiosity-based exploration methods dramatically degrades (Burda et al., 2019a;b;Kim et al., 2019;Savinov et al., 2019).",
    "next": "The information bottleneck (IB) theory (Tishby et al., 2000;Tishby & Zaslavsky, 2015) provides a framework for dealing with such task-irrelevant information, and has been actively adopted to exploration in reinforcement learning (Kim et al., 2019;Igl et al., 2019).For an input variable X and a target variable Y , the IB theory introduces another variable Z, which is a compressed representation of X.The IB objective trains Z to contain less information about X but more information about Y as possible, where the two are quantified by mutual information terms of I(Z; X) and I(Z; Y ), respectively.IB methods such as Variational Information Bottleneck ("
  },
  {
    "index": 115,
    "source_corpus_id": 219792087,
    "ref_id": "b24",
    "citation_corpus_id": 49411844,
    "start": 2395,
    "end": 2399,
    "prev": "These works usually incur large computation overheads, which make them impractical to use.",
    "curr": "More recent algorithms significantly reduce the search cost including one-shot methods [2,29], a continuous relaxation of the space [25] and network morphisms [5].",
    "next": "In particular, Liu et al."
  },
  {
    "index": 116,
    "source_corpus_id": 247748808,
    "ref_id": "b7",
    "citation_corpus_id": 208857409,
    "start": 2641,
    "end": 2660,
    "prev": "INTRODUCTION\n\nAdversarial Training (AT) is one of the most effective approaches developed so far to improve the robustness of deep neural networks (DNNs) (Madry et al., 2018).",
    "curr": "AT solves a minimax optimization problem, with the inner maximization generating adversarial examples by maximizing the classification loss, and the outer minimization finding model parameters by minimizing the loss on adversarial examples generated from the inner maximization (Wang et al., 2019).",
    "next": "Recently, researchers have noticed that such robust classifiers obtained by AT are able to extract features that are perceptually aligned with humans (Engstrom et al., 2019)."
  },
  {
    "index": 119,
    "source_corpus_id": 256868547,
    "ref_id": "b13",
    "citation_corpus_id": 208248131,
    "start": 4276,
    "end": 4295,
    "prev": "INTRODUCTION\n\nCausal interpretation of the observed time-series data can help answer fundamental causal questions and advance scientific discoveries in various disciplines such as medical and financial fields.",
    "curr": "To enable causal reasoning and counterfactual prediction, researchers in the past decades have been dedicated to discovering causal graphs from observed time-series and made large progress (Gerhardus & Runge, 2020;Tank et al., 2022;Khanna & Tan, 2020;Wu et al., 2022;Pamfil et al., 2020;Löwe et al., 2022;Runge, 2021).",
    "next": "This task is called causal discovery or causal structure learning, which usually formulates causal relationships as Directed Acyclic Graphs (DAGs)."
  },
  {
    "index": 121,
    "source_corpus_id": 11243593,
    "ref_id": "b10",
    "citation_corpus_id": 14915449,
    "start": 6466,
    "end": 6485,
    "prev": "It is also able to generalize to sequences longer than those seen during training.",
    "curr": "Finally, our model also obtains competitive results on the Childrens Book Test (Hill et al., 2016), and performs best among models that read the text in a single pass before receiving knowledge of the question.",
    "next": "MODEL\n\nOur model is designed to process data in sequential form, and consists of three main parts: an input encoder, a dynamic memory and an output layer, which we now describe in detail."
  },
  {
    "index": 123,
    "source_corpus_id": 3525802,
    "ref_id": "b19",
    "citation_corpus_id": 9615470,
    "start": 7912,
    "end": 7928,
    "prev": "Given the first three sentences of a paragraph, choose the next sentence from five sentences later in the paragraph.",
    "curr": "Related to our objective is the local coherence model of Li & Hovy (2014) where a binary classifier is trained to identify coherent/incoherent sentence windows.",
    "next": "In contrast, we only encourage observed contexts to be more plausible than contrastive ones and formulate it as a multi-class classification problem."
  },
  {
    "index": 125,
    "source_corpus_id": 53081529,
    "ref_id": "b42",
    "citation_corpus_id": 11174813,
    "start": 3303,
    "end": 3322,
    "prev": "In our experiments on 9 benchmark text classification datasets and 22 textual similarity tasks, the proposed technique consistently matches or outperforms state-of-the-art techniques, with significantly higher accuracy on problems of short length.",
    "curr": "Introduction\n\nText representation plays an important role in many NLP-based tasks such as document classification and clustering (Zhang et al., 2018;Gui et al., 2016Gui et al., , 2014, sense disambiguation (Gong et al., 2017(Gong et al., , 2018a, machine translation (Mikolov et al., 2013b), document matching (Pham et al., 2015), and sequential alignment (Peng et al., 2016(Peng et al., , 2015.",
    "next": "Since there are no explicit features in text, much work has aimed to develop effective text representations."
  },
  {
    "index": 126,
    "source_corpus_id": 203642015,
    "ref_id": "b22",
    "citation_corpus_id": 53015479,
    "start": 2496,
    "end": 2519,
    "prev": "However, implementing and running MAML continues to be challenging.",
    "curr": "One major complication is that the standard version of MAML requires estimating second derivatives of the RL reward function, which is difficult when using backpropagation on stochastic policies; indeed, the original implementation of MAML (Finn et al., 2017) did so incorrectly, which spurred the development of unbiased higher-order estimators (DiCE, (Foerster et al., 2018)) and further analysis of the credit assignment mechanism in MAML (Rothfuss et al., 2019).",
    "next": "Another challenge arises from the high variance inherent in policy gradient methods, which can be ameliorated through control variates such as in T-MAML (Liu et al., 2019), through careful adaptive hyperparameter tuning (Behl et al., 2019;Antoniou et al., 2019) and learning rate annealing (Loshchilov & Hutter, 2017)."
  },
  {
    "index": 127,
    "source_corpus_id": 256627797,
    "ref_id": "b10",
    "citation_corpus_id": 202573030,
    "start": 2913,
    "end": 2916,
    "prev": "Instead of using a greedy method [9,10] to search splitting rules, soft trees make decision rules soft and simultaneously update the entire model parameters using the gradient method.",
    "curr": "Soft trees have been actively studied in recent years in terms of predictive performance [7,11,12], interpretability [8,13], and potential techniques in real world applications like pre-training and fine-tuning [14,15].",
    "next": "In addition, a soft tree can be interpreted as a Mixture-of-Experts [16][17][18], a practical technique for balancing computational cost and prediction performance."
  },
  {
    "index": 130,
    "source_corpus_id": 221139843,
    "ref_id": "b44",
    "citation_corpus_id": 7186165,
    "start": 30522,
    "end": 30526,
    "prev": "A Glossary of Notation\n\nWe provide a glossary of notation used throughout the paper.Total consistency loss (Eq 3) L : X 2 → R A distance function, used for CycleGAN consistency losses λ\n\nHyperparameter controlling the strength of the consistency loss KL(•)\n\nThe KL divergence JS(•)\n\nThe Jensen-Shannon divergence (Definition 2)\nI(•)\nThe Mutual Information\n\n\nB Extended Related Work\n\nWe provide a comprehensive overview of related work and highlight connections to our work below.",
    "curr": "B.1 Overview of Data Augmentation\n\nData augmentation is widely used for improving the aggregate performance of machine learning models in computer vision [46,79], natural language processing [45,71,95] and audio [18,43].The theoretical motivation for data augmentation is largely based on the tangent propagation formalism [19,73,74,76] which expresses the desired invariances induced by a data augmentation as tangent constraints on the directional derivatives of the learned model.",
    "next": "Early work considered augmentations as image defects [5] or stroke warping [90] for character recognition.Since then, augmentation is considered an essential ingredient in computer vision [47,75], with commonly used augmentations including random flips, rotations and crops [31,46,79].Applications of augmentation in computer vision include object detection [23,98] and scene understanding [22] In natural language processing, common data augmentation techniques include back-translation [71,91], synonym or word substitution [25,44,45,83,95], noising [89], grammar induction [39], text editing [85] and other heuristics [20,72].In speech and audio applications, augmentation is also commonly used, through tec"
  },
  {
    "index": 131,
    "source_corpus_id": 258461359,
    "ref_id": "b0",
    "citation_corpus_id": 67855815,
    "start": 2219,
    "end": 2241,
    "prev": "INTRODUCTION\n\nThe rapid development of neural networks opens the path towards the ambitious goal of universal translation that allows converting information between any languages regardless of data modalities (text, audio or video) (Zhang, 2022).",
    "curr": "While the translation for spoken languages (in text and speech) has gained wide attention (Aharoni et al., 2019;Inaguma et al., 2019;Jia et al., 2019), the study of sign language translation (SLT) -a task translating from sign language videos to spoken language texts -still lags behind despite its significance in facilitating the communication between Deaf communities and spoken language communities (Camgoz et al., 2018;.",
    "next": "SLT represents unique challenges: it demands the capability of video understanding and sequence generation."
  },
  {
    "index": 133,
    "source_corpus_id": 239616399,
    "ref_id": "b13",
    "citation_corpus_id": 231933963,
    "start": 4278,
    "end": 4282,
    "prev": "Such a data-driven approach provides three key benefits: (1) it significantly shortens the recurring cost of running large-scale simulation sweeps, (2) it alleviates the need to explicitly bake in domain knowledge or search space pruning, and (3) it enables data re-use by empowering the designer to optimize accelerators for new unseen applications, by the virtue of effective generalization.",
    "curr": "While data-driven approaches have shown promising results in biology [14,5,57], using offline optimization methods to design accelerators has been challenging Figure 1: Overview of PRIME.",
    "next": "We use a one-time collected dataset of prior accelerator designs, including TPU-style [65], NVDLA-style [42], and ShiDianNao-style [10] accelerators to train a conservative surrogate model, which is used to design accelerators to meet desired goals and constraints."
  },
  {
    "index": 134,
    "source_corpus_id": 220665925,
    "ref_id": "b28",
    "citation_corpus_id": 68137503,
    "start": 2547,
    "end": 2551,
    "prev": "The variational approach is limited in posterior expressiveness while the implicit approach is computationally slow and costly in terms of storage.",
    "curr": "Moreover, specifying meaningful priors in parameter space is known to be difficult due to the complex relationship between weights and functions in deep networks [29].",
    "next": "In this paper, we present a Bayesian approach to FSC based on Gaussian processes (GPs) [36] that enables efficient marginalization over functions rather than model parameters."
  },
  {
    "index": 135,
    "source_corpus_id": 263611938,
    "ref_id": "b21",
    "citation_corpus_id": 252683312,
    "start": 8855,
    "end": 8862,
    "prev": "[Thi+22] attributed grokking to the slingshot mechanism, which can be measured by the cyclic phase transitions between stable and unstable training regimes.Žunkovič and Ilievski [ŽI22] showed a time separation between achieving zero training error and zero test error in a binary classification task on a linearly separable distribution.",
    "curr": "Liu, Michaud, and Tegmark [LMT23] identified a large initialization scale together with weight decay as a mechanism for grokking.",
    "next": "Barak et al."
  },
  {
    "index": 136,
    "source_corpus_id": 213969759,
    "ref_id": "b14",
    "citation_corpus_id": 52055130,
    "start": 3776,
    "end": 3795,
    "prev": "A seminal work is the InfoMax principle (Linsker, 1988), where given an input instance x, the goal of the InfoMax principle is to learn a representation E ψ (x) by maximizing the MI between the input and its representation.",
    "curr": "A growing set of recent works have demonstrated promising empirical performance in unsupervised representation learning via MI maximization (Krause et al., 2010;Hu et al., 2017;Alemi et al., 2018b;Oord et al., 2018;Hjelm et al., 2019).",
    "next": "Another closely related work is the Information Bottleneck method Alemi et al., 2017), where MI is used to limit the contents of representations."
  },
  {
    "index": 137,
    "source_corpus_id": 34984289,
    "ref_id": "b2",
    "citation_corpus_id": 9716222,
    "start": 1855,
    "end": 1881,
    "prev": "* Equal contribution.",
    "curr": "Introduction\n\nRecurrent neural networks (RNNs) have been recently quite successful in structured prediction applications such as machine translation (Sutskever et al., 2014), parsing (Ballesteros et al., 2016) or caption generation (Vinyals et al., 2015).",
    "next": "These models use the same repeated cell (or unit) to output a sequence of tokens one by one."
  },
  {
    "index": 139,
    "source_corpus_id": 235313504,
    "ref_id": "b39",
    "citation_corpus_id": 3488815,
    "start": 2074,
    "end": 2078,
    "prev": "Introduction\n\nThis paper considers minimax optimization min x max y f (x, y) in the context of two-player zero-sum games, where the min-player (controlling x) tries to minimize objective f assuming a worst-case opponent (controlling y) that acts so as to maximize it.",
    "curr": "Minimax optimization naturally arises in a variety of important machine learning paradigms, with the most prominent examples being the training of generative adversarial networks (GANs) [20] and adversarially robust models [40].",
    "next": "These applications commonly engage deep neural networks with various techniques such as convolution, recurrent layers, and batch normalization."
  },
  {
    "index": 140,
    "source_corpus_id": 258714845,
    "ref_id": "b24",
    "citation_corpus_id": 59608630,
    "start": 20594,
    "end": 20620,
    "prev": "We refer to Le (2021) for an extensive overview of memory-based architectures.",
    "curr": "Architectures with external memories have also been used for meta-learning (Vinyals et al., 2016;Santoro et al., 2016) and few-shot learning (Munkhdalai and Yu, 2017;Ramalho and Garnelo, 2018;Ma et al., 2021 (Rogers and Hahn, 2010) and key molecular physical descriptors, which were defined by RDKit (Landrum et al., 2006).",
    "next": "While methods would be allowed to use other representations of the input molecules, such as the molecular graph, we used a concatenation of these ECFPs and RDKit-based descriptors."
  },
  {
    "index": 141,
    "source_corpus_id": 222272443,
    "ref_id": "b28",
    "citation_corpus_id": 166228758,
    "start": 3392,
    "end": 3434,
    "prev": "Model Based (MB) approaches typically rely on partial or ordinary differential equations (PDE/ODE) and stem from a deep understanding of the underlying physical phenomena.",
    "curr": "Machine learning (ML) and deep learning methods are more prior agnostic yet have become state-of-the-art for several spatio-temporal prediction tasks (Shi, Chen, Wang, Yeung, Wong & Woo 2015, Wang, Gao, Long, Wang & Yu 2018, Oreshkin, Carpov, Chapados & Bengio 2020, Donà, Franceschi, Lamprier & Gallinari 2020, and connections have been drawn between deep architectures and numerical ODE solvers, e.g.",
    "next": "neural ODEs (Chen, Rubanova, Bettencourt & Duvenaud 2018, Ayed, de Bézenac, Pajot, Brajard & Gallinari 2019."
  },
  {
    "index": 146,
    "source_corpus_id": 52986403,
    "ref_id": "b20",
    "citation_corpus_id": 6628106,
    "start": 37015,
    "end": 37034,
    "prev": "A mean pooling layer is used to summarize the LSTM outputs across time, followed by a linear projection layer to predict the posterior mean and log variance.",
    "curr": "C DETAILED EXPERIMENTAL SETUP\n\nThe network is trained using the Adam optimizer (Kingma & Ba, 2015), configured with an initial learning rate 10 −3 , and an exponential decay that halved the learning rate every 12.5k steps, beginning after 50k steps.",
    "next": "Table 6 details the list of prior hyperparameters used for each of the four datasets described in Section 4: multi-speaker English data (multi-spk), noisified multi-speaker English data (noisy-multispk), single-speaker story-telling data (audiobooks), and crowd-sourced audiobook data (crowdsourced)."
  },
  {
    "index": 147,
    "source_corpus_id": 253237975,
    "ref_id": "b7",
    "citation_corpus_id": 848112,
    "start": 4188,
    "end": 4213,
    "prev": "networks (Arjovsky et al., 2017;Goodfellow et al., 2014), adversarial learning (Goodfellow et al., 2015;Miller et al., 2020), reinforcement learning (Dai et al., 2017;Modi et al., 2021), sharpness-aware minimization (Foret et al., 2021), domain-adversarial training (Ganin et al., 2016), etc.",
    "curr": "Albeit theoretically underexplored, adaptive methods are widely deployed in these applications in combination with popular minimax optimization algorithms such as (stochastic) gradient descent ascent (GDA), extragradient (EG) (Korpelevich, 1976), and optimistic GDA (Popov, 1980;Rakhlin and Sridharan, 2013); see, e.g., (Daskalakis et al., 2018;Gulrajani et al., 2017;Mishchenko et al., 2020;Reisizadeh et al., 2020), just to list a few.",
    "next": "While it seems natural to directly extend adaptive stepsizes to minimax optimization algorithms, a recent work by Yang et al."
  },
  {
    "index": 150,
    "source_corpus_id": 222209080,
    "ref_id": "b18",
    "citation_corpus_id": 1487550,
    "start": 3081,
    "end": 3097,
    "prev": "Self-training is a common algorithmic paradigm for leveraging unlabeled data with deep networks.",
    "curr": "Self-training methods train a model to fit pseudolabels, that is, predictions on unlabeled data made by a previously-learned model (Yarowsky, 1995;Grandvalet & Bengio, 2005;Lee, 2013).",
    "next": "Recent work also extends these methods to enforce stability of predictions under input transformations such as adversarial perturbations (Miyato et al., 2018) and data augmentation (Xie et al., 2019)."
  },
  {
    "index": 154,
    "source_corpus_id": 238583191,
    "ref_id": "b5",
    "citation_corpus_id": 220249831,
    "start": 23586,
    "end": 23605,
    "prev": "In Figure \n\n\nImproving SSL on Imbalanced Datasets with Regularization\n\nIn this section, we aim to further improve the performance of SSL to close the gap between imbalanced and balanced datasets.",
    "curr": "Many prior works on imbalanced supervised learning regularize the rare classes more strongly, motivated by the observation that the rare classes suffer from more overfitting [Cao et al., 2019[Cao et al., , 2021.",
    "next": "Inspired by these works, we compute the generalization gaps (i.e., the differences between empirical and validation pre-training losses) on frequent and rare classes for the step-imbalance CIFAR-10 datasets (where 5 classes are frequent class with 5000 examples per class and the rest are rare with 50 examples per class)."
  },
  {
    "index": 157,
    "source_corpus_id": 222125116,
    "ref_id": "b9",
    "citation_corpus_id": 5590763,
    "start": 2351,
    "end": 2355,
    "prev": "A variety of approaches have been suggested to mitigate the exploding and vanishing gradient problem.",
    "curr": "These include adding gating mechanisms to the RNN in order to control the flow of information in the network, leading to architectures such as long short-term memory (LSTM) [21] and gated recurring units (GRU) [10], that can overcome the vanishing gradient problem on account of the underlying additive structure.",
    "next": "However, the gradients might still explode and learning very long term dependencies remains a challenge [30]."
  },
  {
    "index": 158,
    "source_corpus_id": 4679427,
    "ref_id": "b1",
    "citation_corpus_id": 18828233,
    "start": 2297,
    "end": 2322,
    "prev": "*\n\nIntroduction\n\nIn order to learn Generative Adversarial Networks (Goodfellow et al., 2014), it is now well established that the generator should mimic the distribution of real data, in the sense of a certain discrepancy measure.",
    "curr": "Discrepancies between distributions that measure the goodness of the fit of the neural generator to the real data distribution has been the subject of many recent studies (Arjovsky & Bottou, 2017;Nowozin et al., 2016;Kaae Sønderby et al., 2017;Mao et al., 2017;Gulrajani et al., 2017;, most of which focus on training stability.",
    "next": "In terms of data modalities, most success was booked in plausible natural image generation after the introduction of Deep Convolutional Generative Adversarial Networks (DCGAN) (Radford et al., 2015)."
  },
  {
    "index": 159,
    "source_corpus_id": 219558760,
    "ref_id": "b36",
    "citation_corpus_id": 52895589,
    "start": 1628,
    "end": 1644,
    "prev": "Under review.",
    "curr": "Introduction\n\nGraph neural networks (GNNs) (Scarselli et al., 2008;Micheli, 2009) have seen growing popularity over the last few years (Duvenaud et al., 2015;Hamilton et al., 2017;Xu et al., 2019).",
    "next": "Many domains can be naturally represented as graphs."
  },
  {
    "index": 160,
    "source_corpus_id": 88514953,
    "ref_id": "b17",
    "citation_corpus_id": 16209268,
    "start": 3397,
    "end": 3421,
    "prev": "Ideally, we would like to quantify the probability to converge to a local minimum as a function of the error at this minimum, where the probability is taken with the respect to the randomness of the initialization of the weights, the data and SGD.",
    "curr": "Specifically, we would like to know, under which conditions this probability is very small if the error is high, as was observed empirically (e.g., (Dauphin et al., 2014;Goodfellow et al., 2015)).",
    "next": "However, this seems to be a daunting task for realistic MNNs, since it requires a characterization of the sizes and distributions of the basins of attraction for all local minima."
  },
  {
    "index": 163,
    "source_corpus_id": 263671510,
    "ref_id": "b31",
    "citation_corpus_id": 247595263,
    "start": 10710,
    "end": 10730,
    "prev": "We also investigated using only GSM8K to create difficult problems, but we found that the new problems were too similar to the original ones, and the large gap to MATH still exists (more information can be found in Appendix C).",
    "curr": "Self-distillation.Given that we do not have ground truth answers for the new problems, we then generate n different LCE solutions as depicted in (Wang et al., 2023a) for each new problem with our initial MathCoder models, keeping only those solutions for which all n answers match (n is set to 3 in this paper), thus ensuring our dataset's quality.We use MathCoder-Initial here because it demonstrates the potential for effective model distillation using a model much weaker than the powerful closed-source models.As MathCoder-Initial already has an accuracy of 77.3% on GSM8K and 44.0% on MATH, it is plausible that distilling it can produce good results.It also reduces the cost compared to using GPT-4.Some examples can be found in Appendix A.",
    "next": "Combining the new data D 1 with the seed data D 0 yields the MathCodeInstruct dataset D = {D 0 , D 1 }.We fine-tune the base Llama-2 (Touvron et al., 2023) and CodeLlama (Rozière et al., 2023) models using MathCodeInstruct to derive our final MathCoder models.For clarity, we refer to the supervised fine-tuning of base Llama-2 as \"MathCoder-L\" and that of CodeLlama as \"MathCoder-CL\", as shown in Fig."
  },
  {
    "index": 165,
    "source_corpus_id": 52978527,
    "ref_id": "b53",
    "citation_corpus_id": 27494814,
    "start": 10231,
    "end": 10249,
    "prev": "Frankle & Carbin (2018) hypothesizes that certain connections, together with their randomly initialized weights, is particularly effective for training, and a pruning algorithm can help find such sub-networks.",
    "curr": "Zhu & Gupta (2018) shows that training a small-dense model cannot achieve the same accuracy as a pruned large-sparse model with identical memory footprint.",
    "next": "In this work, we reveal a different and rather surprising characteristic of network pruning methods: fine-tuning the pruned model with inherited weights is no better than training it from scratch."
  },
  {
    "index": 168,
    "source_corpus_id": 250627720,
    "ref_id": "b15",
    "citation_corpus_id": 231847016,
    "start": 6650,
    "end": 6671,
    "prev": "Multiple tasks In standard multitask RL, there is a (possibly infinite) set of tasks (MDPs) M = {M }, usually presented to the agent by sampling from some task distribution P M ∈ P(M).",
    "curr": "Typical objectives include finding either a single policy or a set of policies which maximize worstor average-case value: max π min M ∈M V π M (Zahavy et al., 2021) or max π E P M V π M (Moskovitz et al., 2022a).",
    "next": "When the emphasis is on decreasing the required sample complexity of learning new tasks, a useful metric is cumulative regret: the agent's total shortfall across training compared to an optimal agent."
  },
  {
    "index": 169,
    "source_corpus_id": 256615813,
    "ref_id": "b27",
    "citation_corpus_id": 3144218,
    "start": 6513,
    "end": 6535,
    "prev": "Bounding Shape Mask R- CNN (Kang et al., 2020) improves performance on object detection and instance segmentation by its bounding shape mask branch.",
    "curr": "BCNet (Ke et al., 2021) uses two GCN (Welling & Kipf, 2016) layers to detect overlapping instances.",
    "next": "Although these algorithms have yielded promising results, they are still restricted in the low-resolution mask representation and thus do not generate high-quality masks."
  },
  {
    "index": 171,
    "source_corpus_id": 263609164,
    "ref_id": "b22",
    "citation_corpus_id": 219530969,
    "start": 33805,
    "end": 33830,
    "prev": "minimising the number of times the policy used changes from one episode to the next.These works are not directly comparable because they study regret-minimisation for finite-horizon MDPs and we study BPI and PE in the discounted setting.Nevertheless, there are works on tabular MDPs (Qiao et al., 2022;Bai et al., 2019;Zhang et al., 2020), linear MDPs (Gao et al., 2021;Wang et al., 2021;Qiao & Wang, 2023) and MDPs with a linear representation for the action values (Qiao et al., 2023).",
    "curr": "The multi-batch learning model has been studied extensively for bandit algorithms (Perchet et al., 2015;Jun et al., 2016;Gao et al., 2019;Esfandiari et al., 2021;Duchi et al., 2018;Han et al., 2020;Ruan et al., 2021).In RL, it has been studied in the regret-minimisation setting for finite-horizon tabular (Zihan et al., 2022) and linear MDPs (Wang et al., 2021) and MDPs under general function approximation (Xiong et al., 2023).A closely related notion is deployment efficiency (Matsushima et al., 2021), which constrains batches to be of a fixed size consisting of trajectories from a single policy.In finite-horizon linear MDPs, it has been shown that BPI can be solved to arbitrary accuracy with a number of deployments independent of the dimension d (Huang et al., 2022;Qiao & Wang, 2023) where the deployed policy is a finite mixture of deterministic policies.Our results suggest that infinite-horizon discounted MDPs under more general linear representation of action-values are fundamentally harder since the number of deployments must scale with dimension.",
    "next": "The policy finetuning setting assumes access to an offline dataset that can be complemented with online trajectories (Xie et al., 2021) but is different from our setting since there is no adaptivity constraint in the online algorithm, i.e."
  },
  {
    "index": 172,
    "source_corpus_id": 44096233,
    "ref_id": "b23",
    "citation_corpus_id": 2100831,
    "start": 5974,
    "end": 5998,
    "prev": "Third, we demonstrate that on synthetic data, DSMN achieves superior performance for answering questions that require visual thinking.",
    "curr": "Related Work\n\nNatural language datasets for QA: Several natural language QA datasets have been proposed to test AI systems on various reasoning abilities (Levesque et al., 2011;Richardson et al., 2013).",
    "next": "Our work differs from them in two key aspects: first, we use synthetic data instead of natural data; and second, we specialize in geometrical reasoning instead of general language understanding."
  },
  {
    "index": 174,
    "source_corpus_id": 53729760,
    "ref_id": "b19",
    "citation_corpus_id": 205514,
    "start": 2521,
    "end": 2543,
    "prev": "INTRODUCTION\n\nGenerative Adversarial Networks (GANs) (Goodfellow et al., 2014) have been able to produce photorealistic images, often indistinguishable from real images.",
    "curr": "This remarkable ability has powered many real-world applications ranging from visual recognition (Wang et al., 2017), to image manipulation (Isola et al., 2017;Zhu et al., 2017), to video prediction (Mathieu et al., 2016).",
    "next": "Since its invention in 2014, many GAN variants have been proposed (Radford et al., 2016;Zhang et al., 2018), often producing more realistic and diverse samples with better training stability."
  },
  {
    "index": 175,
    "source_corpus_id": 220769181,
    "ref_id": "b10",
    "citation_corpus_id": 204512179,
    "start": 1924,
    "end": 1928,
    "prev": "Demonstrative videos are available at https\n\nIntroduction\n\nCooperative multi-agent reinforcement learning (MARL) has achieved great progress in recent years [1][2][3][4][5][6][7].",
    "curr": "Advances in valued-based MARL [8][9][10][11] contribute significantly to the progress, achieving state-of-the-art performance on challenging tasks, such as StarCraft II micromanagement [12].",
    "next": "However, these value-based methods present a major challenge for stability and convergence in multi-agent settings [13], which is further exacerbated in continuous action spaces."
  },
  {
    "index": 177,
    "source_corpus_id": 222341655,
    "ref_id": "b32",
    "citation_corpus_id": 199543783,
    "start": 24270,
    "end": 24291,
    "prev": "it requires two independent samples of s starting from the same (s, a) due to the inner conditional expectation (Baird, 1995), whereas IV regression methods do not suffer from this issue.",
    "curr": "We evaluate DFIV on three BSuite (Osband et al., 2019) tasks: catch, mountain car, and cartpole.",
    "next": "See Section E.6.1 for a description of those tasks."
  },
  {
    "index": 178,
    "source_corpus_id": 252110923,
    "ref_id": "b9",
    "citation_corpus_id": 210839350,
    "start": 5670,
    "end": 5673,
    "prev": "Second, we study how to translate the start of manipulation skills to the navigation reward, which is used to train the navigation skill to connect manipulation skills.",
    "curr": "Note that the goal position in mobile manipulation plays a very different role from that in point-goal [9,10] navigation.",
    "next": "On the one hand, the position of a target object (e.g., on the table or in the fridge) is often not directly navigable; on the other hand, a navigable position close to the goal position can be infeasible due to kinematic and collision constraints."
  },
  {
    "index": 179,
    "source_corpus_id": 211069439,
    "ref_id": "b2",
    "citation_corpus_id": 11212020,
    "start": 2487,
    "end": 2510,
    "prev": "INTRODUCTION\n\nText sequence transduction systems convert a given text sequence from one domain to another.",
    "curr": "These techniques can be applied to a wide range of natural language processing applications such as machine translation (Bahdanau et al., 2015), summarization (Rush et al., 2015), and dialogue response generation (Zhao et al., 2017).",
    "next": "In many cases, however, parallel corpora for the task at hand are scarce."
  },
  {
    "index": 181,
    "source_corpus_id": 232105052,
    "ref_id": "b44",
    "citation_corpus_id": 44131019,
    "start": 3421,
    "end": 3439,
    "prev": "tion, and explore its application in transformers.RFA can be used as a drop-in replacement for conventional softmax attention and offers a straightforward way of learning with recency bias through an optional gating mechanism.Experiments on language modeling and machine translation demonstrate that RFA achieves similar or better performance compared to strong transformer baselines.In the machine translation experiment, RFA decodes twice as fast as a vanilla transformer.Compared to existing efficient transformer variants, RFA is competitive in terms of both accuracy and efficiency on three long text classification datasets.Our analysis shows that RFA's efficiency gains are especially notable on long sequences, suggesting that RFA will be particularly useful in tasks that require working with large inputs, fast decoding speed, or low memory footprints.",
    "curr": "INTRODUCTION\n\nTransformer architectures (Vaswani et al., 2017) have achieved tremendous success on a variety of sequence modeling tasks (Ott et al., 2018;Radford et al., 2018;Parmar et al., 2018;Devlin et al., 2019;Parisotto et al., 2020, inter alia).Under the hood, the key component is attention (Bahdanau et al., 2015), which models pairwise interactions of the inputs, regardless of their distances from each other.This comes with quadratic time and memory costs, making the transformers computationally expensive, especially for long sequences.A large body of research has been devoted to improving their time and memory efficiency (Tay et al., 2020c).Although better asymptotic complexity and prominent gains for long sequences have been achieved (Lee et al., 2019;Child et al., 2019;Beltagy et al., 2020, inter alia), in practice, many existing approaches are less well-suited for moderatelength ones: the additional computation steps required by some approaches can overshadow the time and memory they save (Kitaev et al., 2020;Wang et al., 2020;Roy et al., 2020, inter alia).",
    "next": "This work proposes random feature attention (RFA), an efficient atte"
  },
  {
    "index": 182,
    "source_corpus_id": 257102434,
    "ref_id": "b15",
    "citation_corpus_id": 13046179,
    "start": 10346,
    "end": 10372,
    "prev": "This function should induce a partial order over instances in X .",
    "curr": "The most common and well-known κ function for a classification model f (with softmax at its last layer) is its softmax response values -κ(x,ŷ|f ) f (x)ŷ (Cordella et al., 1995;De Stefano et al., 2000) -which is also widely accepted as a baseline in the OOD literature (Hendrycks & Gimpel, 2017;Hendrycks et al., 2021;Berger et al., 2021;Shalev et al., 2018).",
    "next": "While this is the primary κ we evaluate for the sake of simplicity, various other κ functions, which are also utilized for OOD detection, exist."
  },
  {
    "index": 185,
    "source_corpus_id": 225067229,
    "ref_id": "b32",
    "citation_corpus_id": 216553145,
    "start": 2479,
    "end": 2498,
    "prev": "Human evaluations show that COCO-generated conversations perfectly reflect the underlying user goal with more than 95% accuracy and are as human-like as the original conversations, further strengthening its reliability and promise to be adopted as part of the robustness evaluation of DST models.",
    "curr": "INTRODUCTION\n\nTask-oriented dialogue (TOD) systems have recently attracted growing attention and achieved substantial progress (Zhang et al., 2019b;Peng et al., 2020;Wang et al., 2020b;a), partly made possible by the construction of large-scale datasets (Budzianowski et al., 2018;Byrne et al., 2019;Rastogi et al., 2019).",
    "next": "Dialogue state tracking (DST) is a backbone of TOD systems, where it is responsible for extracting the user's goal represented as a set of slot-value pairs (e.g., (area, center), (food, British)), as illustrated in the upper part of Figure 1."
  },
  {
    "index": 186,
    "source_corpus_id": 215814169,
    "ref_id": "b24",
    "citation_corpus_id": 59310641,
    "start": 7411,
    "end": 7414,
    "prev": "[23] for a review of different pruning strategies.",
    "curr": "Others have worked on lightweight architectures, by modifying existing models [24,25,26] or developing new networks, such as MobileNet [16], ShuffleNet [15], and EfficientNet [4] in vision.",
    "next": "Finally, knowledge distillation [6] has been applied to sentence representation [13,27,28,29,30], to reduce the size of a BERT model [31]."
  },
  {
    "index": 187,
    "source_corpus_id": 264426451,
    "ref_id": "b38",
    "citation_corpus_id": 227209335,
    "start": 2332,
    "end": 2350,
    "prev": "ombined with better hyperparameter tuning, these modifications enable consistency models to achieve FID scores of 2.51 and 3.25 on CIFAR-10 and ImageNet 64 ˆ64 respectively in a single sampling step.These scores mark a 3.5ˆand 4ˆimprovement compared to prior consistency training approaches.Through two-step sampling, we further reduce FID scores to 2.24 and 2.77 on these two datasets, surpassing those obtained via distillation in both one-step and two-step settings, while narrowing the gap between consistency models and other state-of-the-art generative models.",
    "curr": "INTRODUCTION\n\nConsistency models (Song et al., 2023) are an emerging family of generative models that produce high-quality samples using a single network evaluation.Unlike GANs (Goodfellow et al., 2014), consistency models are not trained with adversarial optimization and thus sidestep the associated training difficulty.Compared to score-based diffusion models (Sohl-Dickstein et al., 2015;Song & Ermon, 2019;2020;Ho et al., 2020;Song et al., 2021), consistency models do not require numerous sampling steps to generate high-quality samples.They are trained to generate samples in a single step, but still retain important advantages of diffusion models, such as the flexibility to exchange compute for sample quality through multistep sampling, and the ability to perform zero-shot data editing.",
    "next": "We can train consistency models using either consistency distillation (CD) or consistency training (CT).The former requires pre-training a diffusion model and distilling the knowledge therein into a consistency model.The latter allows us to train consistency models directly from data, establishing them as an independent family of generative models.Previous work (Song et al., 2023) demonstrates that CD significantly outperforms CT.However, CD adds computational overhead to the training process since it requires learning a separate diffusion model.Additionally, distillation limits the sample quality of the consistency model to that of the diffu"
  },
  {
    "index": 191,
    "source_corpus_id": 263829348,
    "ref_id": "b12",
    "citation_corpus_id": 209315300,
    "start": 2766,
    "end": 2786,
    "prev": "INTRODUCTION\n\nTime series forecasting, i.e., predicting future data based on historical observations, has broad realworld applications, such as health, transportation, finance and so on.",
    "curr": "In the past decade, numerous deep neural network architectures have been applied to time series modeling, including convolutional neural networks (CNN) (Bai et al., 2018), recurrent neural networks (RNN) (Siami-Namini et al., 2018), graph neural networks (GNN) (Li et al., 2018;Cao et al., 2020), and Transformers (Liu et al., 2021;Wu et al., 2021;Zhou et al., 2021;Wu et al., 2023;Zhou et al., 2022;Woo et al., 2022;Kitaev et al., 2020;Nie et al., 2023), leading to state-of-the-arts results.",
    "next": "While achieving strong prediction performance, the previous works on time series mostly benefit from the advance in sequence modeling (from RNN and GNN, to transformers) that captures temporal dependencies but overlooks a series of intricate patterns within time series data, such as seasonality, trend, and residual."
  },
  {
    "index": 194,
    "source_corpus_id": 235313882,
    "ref_id": "b19",
    "citation_corpus_id": 153313159,
    "start": 9071,
    "end": 9092,
    "prev": "(2019) replace F c (x) as a weighted sum of features:\nF c (x) = D j=1 x j entmax α (F c ) j = x · entmax α (F c ).",
    "curr": "(2)\nHere F c ∈ R D are the logits for which features to choose, and entmax α (Peters et al., 2019) is the entmax transformation which works like a sparse version of softmax such that the sum of the output equals to 1.",
    "next": "They also replace the I with entmoid which works like a sparse sigmoid that has output values between 0 and 1."
  },
  {
    "index": 198,
    "source_corpus_id": 253255190,
    "ref_id": "b2",
    "citation_corpus_id": 54203451,
    "start": 3304,
    "end": 3326,
    "prev": "In practice, when trained on language data, do transformers instead constrain their computation to look equivalent to a tree-structured bottom-up computation?",
    "curr": "While generalization tests on benchmarks (Lake & Baroni, 2018;Bahdanau et al., 2019;Hupkes et al., 2019;Kim & Linzen, 2020, among others) assess if a transformer's behavior is aligned with tree-like models, they do not measure if the transformer's computation is tree-structured, largely because model behavior on benchmarks could entirely be due to orthogonal properties of the dataset (Patel et al., 2022).",
    "next": "Thus, to understand if transformers implement tree-structured computations, the approach we take is based on directly approximating them with a separate, tree-structured computation."
  },
  {
    "index": 200,
    "source_corpus_id": 229923250,
    "ref_id": "b1",
    "citation_corpus_id": 11212020,
    "start": 2344,
    "end": 2367,
    "prev": "arXiv, 2020.",
    "curr": "INTRODUCTION\n\nSequence-to-Sequence (Seq2Seq) learning  has advanced the state of the art in various natural language processing (NLP) tasks, such as machine translation (Bahdanau et al., 2015;Vaswani et al., 2017;Wu et al., 2019), text summarization (Wang et al., 2019b;Zhang et al., 2020), and grammatical error correction (Kiyono et al., 2019;Kaneko et al., 2020).",
    "next": "Seq2Seq models are generally implemented with an encoder-decoder framework, in which a multi-layer encoder summarizes a source sequence into a sequence of representation and another multi-layer decoder produces the target sequence conditioned on the encoded representation."
  },
  {
    "index": 202,
    "source_corpus_id": 219969405,
    "ref_id": "b19",
    "citation_corpus_id": 71145737,
    "start": 2905,
    "end": 2909,
    "prev": "Recently, Triantafillou et al.",
    "curr": "[20] proposed a benchmark for multi-domain few-shot classification, Meta-Dataset, and highlighted some of the challenges that current methods face when training data is heterogeneous.",
    "next": "Crucially, they found that methods which trained on all available domains would normally obtain improved performance on some domains at the expense of others."
  },
  {
    "index": 203,
    "source_corpus_id": 59279266,
    "ref_id": "b3",
    "citation_corpus_id": 17682909,
    "start": 4146,
    "end": 4166,
    "prev": "Spectral methods leverage graph Fourier transform to convert signals defined in vertex domain into spectral domain, e.g., the space spanned by the eigenvectors of the graph Laplacian matrix, and then filter is defined in spectral domain, maintaining the weight sharing property of CNN.",
    "curr": "As the pioneering work of spectral methods, spectral CNN (Bruna et al., 2014) exploited graph data with the graph Fourier transform to implement convolution operator using convolution theorem.",
    "next": "Some subsequent works make spectral methods spectrum-free (Defferrard et al., 2016;Kipf & Welling, 2017;Khasanova & Frossard, 2017), achieving locality in spatial domain and avoiding high computational cost of the eigendecomposition of Laplacian matrix."
  },
  {
    "index": 204,
    "source_corpus_id": 252815987,
    "ref_id": "b13",
    "citation_corpus_id": 3480671,
    "start": 6503,
    "end": 6520,
    "prev": "For example, including an extra mathematical symbol can push everything one line further down.",
    "curr": "Some datasets also have long-term symbolic dependencies, which may be difficult for non-sequential models to handle, analogous to some of the challenges observed in nonautoregressive machine translation (Gu et al., 2018).",
    "next": "Generation with Diffusion Models Denoising diffusion probabilistic models (DDPM) (Ho et al., 2020) parameterize a probabilistic distribution P (y 0 |x) as a Markov chain P (y t−1 |y t ) with an initial distribution P (y T )."
  },
  {
    "index": 205,
    "source_corpus_id": 209314627,
    "ref_id": "b0",
    "citation_corpus_id": 3651422,
    "start": 47962,
    "end": 47982,
    "prev": "It does not provide a consistent ranking of different ensembling techniques.",
    "curr": "(\nSrivastava et al., 2014;Gal & Ghahramani, 2016), variational inference(Blundell et al., 2015;  Kingma et al., 2015; Louizos & Welling, 2017), batch normalization(Ioffe & Szegedy, 2015; Teye  et al., 2018;Atanov et al., 2019), Laplace approximation(Ritter et al., 2018)  and many more.Snapshot-based methods aim to obtain sets of weights for deep learning models and then to average the predictions across these weights.",
    "next": "The weights can be trained independently (e.g."
  },
  {
    "index": 207,
    "source_corpus_id": 222208810,
    "ref_id": "b12",
    "citation_corpus_id": 969555,
    "start": 13384,
    "end": 13406,
    "prev": "The decoder generates an action sentence a t token-by-token to interact with the game.",
    "curr": "The encoder and decoder are based on a Transformer Seq2Seq model with pointer softmax mechanism (Gulcehre et al., 2016 When playing a game, an agent might get stuck at certain states due to various failures (e.g., action is grammatically incorrect, wrong object name).",
    "next": "The observation for a failed action does not contain any useful feedback, so a fully deterministic model tends to produce the same (wrong) action repeatedly."
  },
  {
    "index": 211,
    "source_corpus_id": 260126025,
    "ref_id": "b39",
    "citation_corpus_id": 249538336,
    "start": 7152,
    "end": 7156,
    "prev": "There are a number of related deep reinforcement learning methods for two-player zero-sum games.",
    "curr": "CFR-based techniques such as Deep CFR [4], DREAM [62], and ESCHER [40], use deep reinforcement learning to approximate CFR.",
    "next": "Policy-gradient techniques such as RPG [60], NeuRD [23], Friction-FoReL [53; 52], and MMD [59], approximate Nash equilibrium via modified actor-critic algorithms."
  },
  {
    "index": 213,
    "source_corpus_id": 226237047,
    "ref_id": "b24",
    "citation_corpus_id": 40100965,
    "start": 1791,
    "end": 1812,
    "prev": "We also demonstrate that the new objective leads to models that are more robust to different levels of noise in the training data, and can generalize better to related tasks with limited labeled task data.",
    "curr": "INTRODUCTION\n\nState-of-the-art for most existing natural language processing (NLP) classification tasks is currently achieved by systems that are first pre-trained on auxiliary language modeling tasks and then fine-tuned on the task of interest with cross-entropy loss (Radford et al., 2019;Howard & Ruder, 2018;Liu et al., 2019;Devlin et al., 2019).",
    "next": "Although commonly used, cross-entropy loss -the KL-divergence between one-hot vectors of labels and the distribution of model's output logits -has several shortcomings."
  },
  {
    "index": 214,
    "source_corpus_id": 52900371,
    "ref_id": "b10",
    "citation_corpus_id": 65455367,
    "start": 29042,
    "end": 29061,
    "prev": "The figure 2 depicts trajectories for one particular seed, while the figure 3 plots means and error bars over these distinct seeds.",
    "curr": "C ADDITIONAL SIMULATIONS C.1 ADAM VERSUS AMSGRAD Plotted in the figure S1 are the comparisons between Adam (Kingma & Ba, 2014) and AMS-Grad (J. Reddi et al., 2018) algorithms with the default hyperparameters α = 10 −3 , (β 1 , β 2 ) = (0.9, 0.999), and = 10 −8 .",
    "next": "The AMSGrad algorithm marginally outperforms the Adam algorithm for the tasks at hand and thus the results with the AMSGrad are presented in the main text."
  },
  {
    "index": 215,
    "source_corpus_id": 226254365,
    "ref_id": "b22",
    "citation_corpus_id": 202712906,
    "start": 2743,
    "end": 2746,
    "prev": "Introduction\n\nTraining, regularizing, and understanding complex neural network models is challenging.",
    "curr": "There remain central open questions on making training faster and more data-efficient [11,23,24], ensuring better generalisation [32] and improving transparency and robustness [2,20].",
    "next": "A promising approach for addressing these questions is learning to teach [35], in which learned auxiliary information about a task is provided to a neural network to inform the training process and help downstream objectives."
  },
  {
    "index": 216,
    "source_corpus_id": 245836975,
    "ref_id": "b25",
    "citation_corpus_id": 5959482,
    "start": 3603,
    "end": 3625,
    "prev": "Zero-shot methods, on the other hand, commonly leverage word embeddings to discover or generate related features between seen and unseen classes (Bucher et al., 2019;Gu et al., 2020) without the need for additional annotations.",
    "curr": "Existing works in this space use standard word embeddings (Mikolov et al., 2013) and focus on the image encoder.",
    "next": "In this work, we present a simple approach to leveraging modern language models to increase the flexibility and generality of semantic segmentation models."
  },
  {
    "index": 218,
    "source_corpus_id": 52890982,
    "ref_id": "b12",
    "citation_corpus_id": 3338083,
    "start": 1887,
    "end": 1912,
    "prev": "However, audio signals have high temporal resolution, and strategies that learn such a latent representation must perform effectively in high dimensions.",
    "curr": "Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) are one such unsupervised strategy for mapping low-dimensional latent vectors to high-dimensional data.",
    "next": "The potential advantages of GAN-based approaches to audio synthesis are numerous."
  },
  {
    "index": 220,
    "source_corpus_id": 256416405,
    "ref_id": "b21",
    "citation_corpus_id": 3488815,
    "start": 4363,
    "end": 4383,
    "prev": "• We compare interpretations of the proof features for standard DNNs and state-of-the-art robustly trained DNNs for the MNIST and CIFAR10 datasets.",
    "curr": "We observe that the proof features corresponding to the standard networks rely on spurious input features while the proofs of adversarially trained DNNs (Madry et al., 2018) filter out some of the spurious features.",
    "next": "In contrast, the networks trained with certifiable training  produce proofs that do not rely on any spurious features but they also miss out on some meaningful features."
  },
  {
    "index": 221,
    "source_corpus_id": 249097375,
    "ref_id": "b6",
    "citation_corpus_id": 211146562,
    "start": 6732,
    "end": 6749,
    "prev": "Semi-supervised learning is also popular and effective on learning with noisy labels in recent years.",
    "curr": "Some works [Li et al., 2020a, Nguyen et al., 2020 first perform clustering on the sample loss and divide the samples into clean ones and noisy ones.",
    "next": "Then drop the labels of the \"noisy samples\" and perform semi-supervised learning on all the samples."
  },
  {
    "index": 224,
    "source_corpus_id": 246607791,
    "ref_id": "b6",
    "citation_corpus_id": 213896662,
    "start": 4842,
    "end": 4858,
    "prev": "Furthermore, although that GANs and VAEs are known to be able to generate high-quality samples from the data distribution, there is no strong evidence that they encode explicit layerwise representations to similar quality as in supervised learning models, which implies that there does not exist a natural way to explicitly extract a representation from intermediate layer activations in unsupervisedly pre-trained generative models.",
    "curr": "Additionally, layer activations alone do not suffice to reach the full power of learned representations hidden in neural network models, as shown in recent works (Mu et al., 2020) that incorporating additional gradients-based features into representation leads to substantial improvement over solely using activations-based features.",
    "next": "In light of these constraints, we are interested in the question: is there a principled method for representation extraction beyond layer activations?"
  },
  {
    "index": 225,
    "source_corpus_id": 257079072,
    "ref_id": "b19",
    "citation_corpus_id": 30535508,
    "start": 15896,
    "end": 15917,
    "prev": "As opposed to when using a straight through estimator (Qiao et al., 2021), non-zero gradients are ensured during the backward pass.",
    "curr": "To overcome training limitations, we use a hard concrete distribution (Qiao et al., 2021;Louizos et al., 2018).",
    "next": "It rescales the weights and the random variable introduced during training prevents from obtaining local minima (Appendix B)."
  },
  {
    "index": 226,
    "source_corpus_id": 262083735,
    "ref_id": "b9",
    "citation_corpus_id": 212747810,
    "start": 20442,
    "end": 20466,
    "prev": "onstructed by the meteorological community.Hallenbeck (1920), for example, presents the performance of a certain rain forecasting method by aggregating results over 6 months into a table: Among the days forecast to have between 10%−20% chance of rain, the table records the true fraction of days which were rainy -and similarly for every forecast interval.This early account of calibration already applies the practice of binning-discretizing predictions into bins, and estimating frequencies conditional on each bin.Plots of these tables turned into binned reliability diagrams (Murphy and Winkler, 1977;DeGroot and Fienberg, 1983), which was recently popularized in the machine learning community by a series of works including Zadrozny and Elkan (2001); Niculescu-Mizil and Caruana (2005); Guo et al.",
    "curr": "(2017).Binned reliability diagrams continue to be used in studies of calibration in machine learning, including in the GPT-4 tech report (Guo et al., 2017;Nixon et al., 2019;Minderer et al., 2021;Desai and Durrett, 2020;?",
    "next": ";OpenAI, 2023)."
  },
  {
    "index": 227,
    "source_corpus_id": 253107476,
    "ref_id": "b20",
    "citation_corpus_id": 3144218,
    "start": 2476,
    "end": 2498,
    "prev": "* Equal contribution.",
    "curr": "INTRODUCTION\n\nDeep learning has made tremendous advances in the past decade, leading to state-of-the-art performance on various learning tasks such as computer vision (He et al., 2016), natural language processing (Devlin et al., 2019) and graph learning (Kipf & Welling, 2017).",
    "next": "While some progress has been made regarding the theoretical understanding of these deep models (Arora et al., 2018;Bartlett et al., 2019;Neyshabur et al., 2015;Dziugaite & Roy, 2017), the considered settings are unfortunately often very restrictive and the insights made are only qualitative or very loose."
  },
  {
    "index": 231,
    "source_corpus_id": 29154793,
    "ref_id": "b12",
    "citation_corpus_id": 26100519,
    "start": 8200,
    "end": 8204,
    "prev": "In [12], the authors have used it for denoising waveforms by predicting the middle ground-truth sample from its noisy input support.",
    "curr": "Recent contributions in Text-To-Speech(TTS) [13,14] have successfully conditioned wavenet on linguistic and acoustic features to obtain state of the art performance.",
    "next": "In our encoder-decoder architecture, we use WaveNet as the output of the decoder, and backpropagate through it down to the encoder."
  },
  {
    "index": 232,
    "source_corpus_id": 247244739,
    "ref_id": "b28",
    "citation_corpus_id": 604334,
    "start": 1399,
    "end": 1402,
    "prev": "E for details).",
    "curr": "Today's best-performing classifiers are vulnerable to adversarial attacks [17,46] and exhibit high robust error : for many inputs, their predictions change under adversarial perturbations, even though the true class stays the same.",
    "next": "For example, in image classification tasks, we distinguish between two categories of such attacks that are contentpreserving [16] (or consistent [38]) if their strength is limited -perceptible and imperceptible perturbations."
  },
  {
    "index": 236,
    "source_corpus_id": 258686472,
    "ref_id": "b42",
    "citation_corpus_id": 208910151,
    "start": 6960,
    "end": 6979,
    "prev": "Related Work\n\nRL with linear function approximation To make MDPs with large state space amenable for provable RL, there has been an explosion of works relying on MDP classes with various linear structures (Jiang et al., 2017;Sun et al., 2019;Du et al., 2021;Jin et al., 2021).",
    "curr": "Among different assumptions made in recent work Wang et al., 2020b;Jin et al., 2020b;Du et al., 2019;Zanette et al., 2020;Ayoub et al., 2020;Jia et al., 2020;Weisz et al., 2021;Zhou et al., 2021;He et al., 2022b;Zhou and Gu, 2022;He et al., 2022a), we consider the linear mixture MDP setting Ayoub et al., 2020;Zhou et al., 2021;Zhang et al., 2021a;He et al., 2022b), where the transition kernel is a linear combination of d given models.",
    "next": "More specifically, we focus on the adversarial linear mixture MDP of He et al."
  },
  {
    "index": 237,
    "source_corpus_id": 264812826,
    "ref_id": "b4",
    "citation_corpus_id": 52967399,
    "start": 2040,
    "end": 2061,
    "prev": "xpressiveness limitations.We show that despite the continuous embedding space being more expressive than the discrete token space, soft-prompting and prefix-tuning are strictly less expressive than full fine-tuning, even with the same number of learnable parameters.Concretely, context-based fine-tuning cannot change the relative attention pattern over the content and can only bias the outputs of an attention layer in a fixed direction.This suggests that while techniques like prompting, in-context learning, soft prompting, and prefixtuning can effectively elicit skills present in the pretrained model, they cannot learn novel tasks that require new attention patterns.",
    "curr": "INTRODUCTION\n\nLanguage model advances are largely driven by larger models and more training data (Kaplan et al., 2020;Rae et al., 2021).Training cutting-edge models is out of reach for most academic researchers, small enterprises, and individuals, and it has become common to instead fine-tune open-source pretrained models (Devlin et al., 2019;Min et al., 2021).Yet, due to escalating computational demands, even fine-tuning of the larger models has become prohibitively expensive (Lialin et al., 2023).",
    "next": "As a result, there is an acute need for more efficient fine-tuning methods, either by sparsely modifying the parameters of the model or modifying its input context.Examples of the first type include adapter modules which introduce a few trainable layers to modify the behaviour of the frozen pretrained network (Rebuffi et al., 2017;Houlsby et al., 2019;Hu et al., 2023).One can also use low-rank updates, which also results in a reduced number of trainable parameters (Hu et al., 2021)."
  },
  {
    "index": 241,
    "source_corpus_id": 254854553,
    "ref_id": "b31",
    "citation_corpus_id": 3488815,
    "start": 3190,
    "end": 3210,
    "prev": "Among a variety of robustness evaluation methods, gradient-based adversarial attack generation makes a tremendous success in the computer vision (CV) domain (Croce & Hein, 2020;Dong et al., 2020).",
    "curr": "For example, the projected gradient descent (PGD)-based methods have been widely used to benchmark the adversarial robustness of CV models (Madry et al., 2018;Zhang et al., 2019b;Shafahi et al., 2019;Wong et al., 2020;Zhang et al., 2019a;Athalye et al., 2018).",
    "next": "However, in the natural language processing (NLP) area, the predominant robustness evaluation tool belongs to query-based attack generation methods Garg & Ramakrishnan, 2020;Li et al., 2019), which do not make the full use of gradient information."
  },
  {
    "index": 242,
    "source_corpus_id": 252531169,
    "ref_id": "b0",
    "citation_corpus_id": 11212020,
    "start": 1690,
    "end": 1713,
    "prev": "INTRODUCTION\n\nSummarization is the process of condensing a source text into a shorter version while preserving its information content.",
    "curr": "Thanks to neural encoder-decoder models (Bahdanau et al., 2015;Sutskever et al., 2014), Transformer-based architectures (Vaswani et al., 2017), and large-scale pretraining (Liu & Lapata, 2019;Zhang et al., 2020;Lewis et al., 2020), the past few years have witnessed a huge leap forward in summarization technology.",
    "next": "Abstractive methods fluently paraphrase the main content of the input, using a vocabulary different from the original document, while extractive approaches are less creative -they produce summaries by identifying and subsequently concatenating the most important sentences in a document -but manage to avoid hallucinations, false statements and inconsistencies."
  },
  {
    "index": 243,
    "source_corpus_id": 245837268,
    "ref_id": "b18",
    "citation_corpus_id": 13046179,
    "start": 2607,
    "end": 2633,
    "prev": "Since there is little prior warning of phase transitions, they pose a challenge to monitoring the safety of ML systems.",
    "curr": "Spurred by this challenge, we propose an anomaly detection task (Hendrycks & Gimpel, 2017;Tack et al., 2020): Can we detect when the true reward starts to drop, while maintaining a low false positive rate in benign cases?",
    "next": "We instantiate our proposed task, POLYNOMALY, for the traffic and COVID environments (Section 5)."
  },
  {
    "index": 245,
    "source_corpus_id": 20472740,
    "ref_id": "b51",
    "citation_corpus_id": 1957433,
    "start": 9627,
    "end": 9652,
    "prev": "Embedding Layer converts each word or phrase to a vector representation and construct the representation matrix for sentences.",
    "curr": "In embedding layer, a model can map tokens to vectors with the pre-trained word representation such as GloVe (Pennington et al., 2014), word2Vec (Mikolov et al., 2013) and fasttext (Joulin et al., 2016).",
    "next": "It can also utilize the preprocessing tool, e.g."
  },
  {
    "index": 247,
    "source_corpus_id": 247594724,
    "ref_id": "b0",
    "citation_corpus_id": 67915085,
    "start": 1475,
    "end": 1478,
    "prev": "INTRODUCTION\n\nRecently, the idea of end-to-end training of neural networks with ordering supervision via continuous relaxation of the sorting function has been presented by Grover et al.",
    "curr": "[1].",
    "next": "The idea of ordering supervision is that the ground truth order of some samples is known while their absolute values remain unsupervised."
  },
  {
    "index": 249,
    "source_corpus_id": 1859294,
    "ref_id": "b10",
    "citation_corpus_id": 5590763,
    "start": 1836,
    "end": 1840,
    "prev": "Such success has been enabled by the appearance of larger datasets, more powerful computing resources and improved architectures and training algorithms.",
    "curr": "Gated units, such as the Long Short-Term Memory [24] (LSTM) and the Gated Recurrent Unit [11] (GRU), were designed to deal with the vanishing gradients problem commonly found in RNNs [8].",
    "next": "These architectures have become popularized thanks to their impressive results in a variety of tasks such as machine translation [5], language modeling [53] or speech recognition [19]."
  },
  {
    "index": 252,
    "source_corpus_id": 9655643,
    "ref_id": "b5",
    "citation_corpus_id": 5590763,
    "start": 1799,
    "end": 1816,
    "prev": "* Work done at Google Brain.",
    "curr": "INTRODUCTION\n\nSequence-to-sequence (seq2seq) models (Sutskever et al., 2014;Cho et al., 2014) with attention  have been successfully applied to many applications including machine translation (Luong et al., 2015;Jean et al., 2015), parsing (Vinyals et al., 2015a), image captioning (Vinyals et al., 2015b;Xu et al., 2015) and Automatic Speech Recognition (ASR) (Chan et al., 2016;Bahdanau et al., 2016a).",
    "next": "Previous work has assumed a fixed deterministic decomposition for each output sequence."
  },
  {
    "index": 254,
    "source_corpus_id": 8728609,
    "ref_id": "b35",
    "citation_corpus_id": 1957433,
    "start": 2450,
    "end": 2474,
    "prev": "1\n\nINTRODUCTION\n\nWord embeddings-continuous-valued vector representations of words-are an almost ubiquitous component of recent natural language processing (NLP) research.",
    "curr": "Word embeddings can be learned using spectral methods (Deerwester et al., 1990) or, more commonly in recent work, via neural networks (Bengio et al., 2003;Mnih & Hinton, 2007;Mikolov et al., 2013;Pennington et al., 2014).",
    "next": "Word embeddings can also be composed to form embeddings of phrases, sentences, or documents Kiros et al., 2015;Wieting et al., 2016;Iyyer et al., 2015)."
  },
  {
    "index": 257,
    "source_corpus_id": 49907212,
    "ref_id": "b12",
    "citation_corpus_id": 9059612,
    "start": 2991,
    "end": 2998,
    "prev": "Introduction\n\nRecent research has shown that neural networks exhibit significant vulnerability to adversarial examples, or slightly perturbed inputs designed to fool the network prediction.",
    "curr": "This vulnerability is present in a wide range of settings, from situations in which inputs are fed directly to classifiers [SZS + 14, CMV + 16] to highly variable real-world environments [KGB16,AEIK18].",
    "next": "Researchers have developed a host of methods to construct such attacks [GSS15, MFF, CW17, MMS + 18], most of which correspond to first order (i.e., gradient based) methods."
  },
  {
    "index": 260,
    "source_corpus_id": 52895589,
    "ref_id": "b14",
    "citation_corpus_id": 3292002,
    "start": 2047,
    "end": 2071,
    "prev": "INTRODUCTION\n\nLearning with graph structured data, such as molecules, social, biological, and financial networks, requires effective representation of their graph structure (Hamilton et al., 2017b).",
    "curr": "Recently, there has been a surge of interest in Graph Neural Network (GNN) approaches for representation learning of graphs (Li et al., 2016;Hamilton et al., 2017a;Kipf & Welling, 2017;Velickovic et al., 2018;Xu et al., 2018).",
    "next": "GNNs broadly follow a recursive neighborhood aggregation (or message passing) scheme, where each node aggregates feature vectors of its neighbors to compute its new feature vector (Gilmer et al., 2017;Xu et al., 2018)."
  },
  {
    "index": 261,
    "source_corpus_id": 222379753,
    "ref_id": "b34",
    "citation_corpus_id": 49868626,
    "start": 8978,
    "end": 8996,
    "prev": "For example, they may assume that features that are discriminative for the base classes are also discriminative for the novel classes, suggesting a metric learning-based approach Qi et al., 2018;Snell et al., 2017;Vinyals et al., 2016;Sung et al., 2018;Hou et al., 2019).",
    "curr": "Alternatively, they may assume that model initializations that lead to rapid convergence on the base classes are also good initializations for the novel classes (Finn et al., 2017;Ravi & Larochelle, 2017;Nichol & Schulman;Rusu et al., 2019;.",
    "next": "Other methods assume that modes of intra-class variation are shared, suggesting the possibility of learned, class-agnostic augmentation policies (Hariharan & Girshick, 2017;Chen et al., 2019b)."
  },
  {
    "index": 263,
    "source_corpus_id": 263334074,
    "ref_id": "b88",
    "citation_corpus_id": 259370805,
    "start": 3118,
    "end": 3137,
    "prev": " further bridges the gap between human proofs and formal proofs by making it easier to impute missing steps.LEGO-Prover advances the stateof-the-art pass rate on miniF2F-valid (48.0% to 57.0%) and miniF2F-test (45.5% to 50.0%).During the proving process, LEGO-Prover also manages to generate over 20,000 skills (theorems/lemmas) and adds them to the growing library.Our ablation study indicates that these newly added skills are indeed helpful for proving theorems, resulting in an improvement from a success rate of 47.1% to 50.4%.We also release our code and all the generated skills.",
    "curr": "1\n\nINTRODUCTION\n\nThe automation of formal reasoning tasks, such as theorem proving and mathematical proof formalization, represents a formidable challenge and an active area of research within the domain of artificial intelligence (Polu & Sutskever, 2020a;Han et al., 2022;Jiang et al., 2022a;First et al., 2023;Bansal et al., 2019;Lample et al., 2022;Jiang et al., 2022b;2021;Zhao et al., 2023;Yang et al., 2023;Wang et al., 2023b;Liu et al., 2023).The process of formalizing mathematical proofs typically relies on human experts to transcribe intricate mathematical concepts into structured formal languages verifiable by interactive theorem prover like Lean (de Moura et al., 2015) or Isabelle (Paulson, 1994).This process, while robust, is often labor-intensive and demands a high level of expertise.",
    "next": "In the past few years, large language models (LLMs) have emerged as a promising avenue, with their capacity to process and produce human-like text, opening doors to the idea of LLM-based neural LEGO Prover (* lemma 1."
  },
  {
    "index": 264,
    "source_corpus_id": 238198403,
    "ref_id": "b3",
    "citation_corpus_id": 221447287,
    "start": 4451,
    "end": 4471,
    "prev": "Among other popular solutions of the disentanglement problem one can mention applying vector quantization technique to the content information (Wu et al., 2020;Wang et al., 2021), utilizing features of Variational AutoEncoders (Luong & Tran, 2021; The model we propose in this paper solves the disentanglement problem by employing the encoder predicting \"average voice\": it is trained to transform mel features corresponding to each phoneme into mel features corresponding to this phoneme averaged across a large multi-speaker dataset.",
    "curr": "As for decoder, in our VC model, it is designed as a part of a Diffusion Probabilistic Model (DPM) since this class of generative models has shown very good results in speech-related tasks like raw waveform generation (Chen et al., 2021a;Kong et al., 2021) and mel feature generation (Popov et al., 2021;Jeong et al., 2021).",
    "next": "However, this decoder choice poses a problem of slow inference because DPM forward pass scheme is iterative and to obtain high-quality results it is typically necessary to run it for hundreds of iterations (Ho et al., 2020;Nichol & Dhariwal, 2021)."
  },
  {
    "index": 268,
    "source_corpus_id": 264490454,
    "ref_id": "b9",
    "citation_corpus_id": 246285344,
    "start": 1626,
    "end": 1649,
    "prev": "iently large model from the Pythia and LLaMA families.Using causal interventions, we show that LMs' internal activations represent binding information by attaching binding ID vectors to corresponding entities and attributes.We further show that binding ID vectors form a continuous subspace, in which distances between binding ID vectors reflect their discernability.Overall, our results uncover interpretable strategies in LMs for representing symbolic knowledge in-context, providing a step towards understanding general in-context reasoning in large-scale LMs.",
    "curr": "INTRODUCTION\n\nModern language models (LMs) excel at many reasoning benchmarks, suggesting that they can perform general purpose reasoning across many domains.However, the mechanisms that underlie LM reasoning remain largely unknown (Räuker et al., 2023).The deployment of LMs in society has led to calls to better understand these mechanisms (Hendrycks et al., 2021), so as to know why they work and when they fail (Mu & Andreas, 2020;Hernandez et al., 2021;Vig et al., 2020b).",
    "next": "In this work, we seek to understand binding, a foundational skill that underlies reasoning.How humans solve binding, i.e."
  },
  {
    "index": 269,
    "source_corpus_id": 52893515,
    "ref_id": "b26",
    "citation_corpus_id": 4429876,
    "start": 3650,
    "end": 3654,
    "prev": "There is a large and rapidly expanding literature of optimization of neural networks.",
    "curr": "Some works focus on the loss surface [1,10,14,[16][17][18][19][20][21][24][25][26][27][28], while others study the convergence of gradient-based methods for optimizing this loss [3,6,22].",
    "next": "In particular, our focus is on the loss surface itself, independent of any algorithmic concerns; this is reflected in the works summarized below."
  },
  {
    "index": 270,
    "source_corpus_id": 51926976,
    "ref_id": "b14",
    "citation_corpus_id": 8820379,
    "start": 1483,
    "end": 1502,
    "prev": "Our model significantly outperforms previous models that were specifically designed for programming languages, as well as state-of-the-art NMT models.",
    "curr": "INTRODUCTION\n\nModeling the relation between source code and natural language can be used for automatic code summarization (Allamanis et al., 2016), documentation (Iyer et al., 2016), retrieval (Allamanis et al., 2015b), and even generation (Balog et al., 2016;Rabinovich et al., 2017;Yin and Neubig, 2017;Devlin et al., 2017;Murali et al., 2017).",
    "next": "In this work, we consider the general problem of generating a natural language sequence from a given snippet of source code."
  },
  {
    "index": 272,
    "source_corpus_id": 65455367,
    "ref_id": "b3",
    "citation_corpus_id": 6628106,
    "start": 2638,
    "end": 2657,
    "prev": "This problem is especially exacerbated in high dimensional problems arising in deep learning.",
    "curr": "To tackle this issue, several variants of ADAGRAD, such as RMSPROP (Tieleman & Hinton, 2012), ADAM (Kingma & Ba, 2015), ADADELTA (Zeiler, 2012), NADAM (Dozat, 2016), etc, have been proposed which mitigate the rapid decay of the learning rate using the exponential moving averages of squared past gradients, essentially limiting the reliance of the update to only the past few gradients.",
    "next": "While these algorithms have been successfully employed in several practical applications, they have also been observed to not converge in some other settings."
  },
  {
    "index": 273,
    "source_corpus_id": 3300406,
    "ref_id": "b8",
    "citation_corpus_id": 7774489,
    "start": 2328,
    "end": 2350,
    "prev": "This problem is known as the problem of option discovery.",
    "curr": "Option discovery has received ample attention over many years, with varied solutions being proposed (e.g., Bacon et al., 2017;Ş imsek & Barto, 2004;Daniel et al., 2016;Florensa et al., 2017;Konidaris & Barto, 2009;Mankowitz et al., 2016;McGovern & Barto, 2001).",
    "next": "Recently, Machado et al."
  },
  {
    "index": 274,
    "source_corpus_id": 219956317,
    "ref_id": "b25",
    "citation_corpus_id": 6628106,
    "start": 21833,
    "end": 21844,
    "prev": "Acceleration\n\nAs the last comparison, we discuss the combination of compression and acceleration/momentum.",
    "curr": "This setting is very important to consider as essentially all state-of-the-art methods for training deep learning models, including Adam [Kingma andBa, 2015, Reddi et al., 2018], rely on the use of momentum in one form or another.",
    "next": "One can treat the unbiased compressed gradient as a stochastic gradient [Gorbunov et al., 2020] and the theory for momentum SGD [Yang et al., 2016, Gadat et al., 2018, Loizou and Richtárik, 2017 would be applicable with an extra smoothness assumption."
  },
  {
    "index": 275,
    "source_corpus_id": 3720457,
    "ref_id": "b8",
    "citation_corpus_id": 6628106,
    "start": 2779,
    "end": 2798,
    "prev": "A related hyperparameter is momentum; typically fixed to a reasonable value such as 0.9, careful tuning can also give significant performance gains (Sutskever et al., 2013).",
    "curr": "While optimizers such as Adam (Kingma & Ba, 2015) are often described as adapting coordinate-specific learning rates, in fact they also have global learning rate and momentum hyperparameters analogously to SGD, and tuning at least the learning rate can be important to good performance.",
    "next": "In light of this, it is not surprising that there have been many attempts to adapt learning rates, either online during optimization (Schraudolph, 1999;Schaul et al., 2013), or offline by fitting a learning rate schedule (Maclaurin et al., 2015)."
  },
  {
    "index": 277,
    "source_corpus_id": 257405190,
    "ref_id": "b79",
    "citation_corpus_id": 232269984,
    "start": 7844,
    "end": 7848,
    "prev": "Besides, Zhao et al.",
    "curr": "[80] propose a novel architecture, bridging the gap between image-conditional and unconditional generation, improving free-form large-scale image completion.",
    "next": "There are also attempts to study the progressive generation."
  },
  {
    "index": 278,
    "source_corpus_id": 3502468,
    "ref_id": "b5",
    "citation_corpus_id": 5273326,
    "start": 20511,
    "end": 20533,
    "prev": "FearNet was implemented in Tensorflow.",
    "curr": "For mPFC and BLA, each fully connected layer uses an exponential linear unit activation function (Clevert et al., 2016).",
    "next": "The output of the encoder also connects to a softmax output layer."
  },
  {
    "index": 279,
    "source_corpus_id": 235899205,
    "ref_id": "b26",
    "citation_corpus_id": 9586240,
    "start": 14851,
    "end": 14873,
    "prev": "summary/article).",
    "curr": "We provide an example of auto-prompting for a sample from the Gigaword summarization dataset (Napoles et al., 2012) with the respective masking in Figure 2 .",
    "next": "For our generation experiments, we denote HTLM-Auto-NS (not-sized) as the auto-prompt without using size hints, where HTLM-Auto-S uses the size hints based policy described in the previous section."
  },
  {
    "index": 280,
    "source_corpus_id": 252668463,
    "ref_id": "b2",
    "citation_corpus_id": 6021932,
    "start": 5171,
    "end": 5174,
    "prev": "We summarize our contributions as follows:\n\nIntroduction\n\nThough neural networks (NNs) have experienced extraordinary success, understanding the generalizability of NNs and successfully using them in real-world contexts still faces a number of obstacles [1,2].",
    "curr": "It is a well-known enigma, for instance, why such NNs generalize well and do not suffer from overfitting [3,4,5].",
    "next": "Recent research on the loss landscape of NNs seeks to reduce these obstacles."
  },
  {
    "index": 281,
    "source_corpus_id": 173990564,
    "ref_id": "b13",
    "citation_corpus_id": 6706414,
    "start": 22678,
    "end": 22682,
    "prev": "Consider a worst-case scenario, in which an adversary can arbitrarily alter the output of the NN with minuscule changes to the input.",
    "curr": "It is well-known that there are NNs for which this is possible [14].",
    "next": "In this case, with the help of the I/O kernel, the model becomes highly uncertain with respect to the output kernel."
  },
  {
    "index": 283,
    "source_corpus_id": 236170938,
    "ref_id": "b31",
    "citation_corpus_id": 184486852,
    "start": 4552,
    "end": 4569,
    "prev": "Current methods are typically applied to a few dozens of variables and cannot scale so well, which is imperative for modern applications like learning causal relations with gene editing interventions (Dixit et al., 2016;Macosko et al., 2015).",
    "curr": "A promising new direction for scaling up DAG discovery methods are continuous-optimization methods (Zheng et al., 2018;2020;Zhu et al., 2020;Ke et al., 2019;Brouillard et al., 2020;Yu et al., 2019).",
    "next": "In contrast to score-based and constrained-based (Peters et al., 2017;Guo et al., 2020) methods, continuous-optimization methods reinterpret the search over discrete graph topologies as a continuous problem with neural networks as function approximators, for which efficient solvers are amenable."
  },
  {
    "index": 284,
    "source_corpus_id": 231648113,
    "ref_id": "b37",
    "citation_corpus_id": 12713052,
    "start": 2086,
    "end": 2102,
    "prev": "INTRODUCTION\n\nInstead of manually designing neural networks, neural architecture search (NAS) algorithms are used to automatically discover the best ones (Tan & Le, 2019a;Bender et al., 2018).",
    "curr": "Early work by Zoph & Le (2017) proposed using a reinforcement learning (RL) controller that constructs candidate architectures, these are evaluated and then feedback is provided to the controller based on the performance of the candidate.",
    "next": "One major problem with this basic NAS methodology is that each evaluation is very costly -typically on the order of hours or days to train a single neural network fully."
  },
  {
    "index": 286,
    "source_corpus_id": 202573030,
    "ref_id": "b25",
    "citation_corpus_id": 153313159,
    "start": 4662,
    "end": 4683,
    "prev": "Furthermore, NODE allows constructing multi-layer architectures, which resembles \"deep\" GBDT that is trained end-to-end, which was never proposed before.",
    "curr": "Besides the usage of oblivious decision tables, another important design choice is the recent entmax transformation (Peters et al., 2019), which effectively performs a \"soft\" splitting feature choice in decision trees inside the NODE architecture.",
    "next": "As discussed in the following sections, these design choices are critical to obtain state-of-the-art performance."
  },
  {
    "index": 289,
    "source_corpus_id": 13206339,
    "ref_id": "b25",
    "citation_corpus_id": 6018348,
    "start": 2374,
    "end": 2392,
    "prev": "We are interested in automated reasoning on large knowledge bases (KB) with rich and diverse semantics (Suchanek et al., 2007;Bollacker et al., 2008;Carlson et al., 2010).",
    "curr": "KBs are highly incomplete (Min et al., 2013), and facts not directly stored in a KB can often be inferred from those that are, creating exciting opportunities and challenges for automated reasoning.",
    "next": "For example, consider the small knowledge graph in figure 1."
  },
  {
    "index": 290,
    "source_corpus_id": 247058691,
    "ref_id": "b12",
    "citation_corpus_id": 12639289,
    "start": 2684,
    "end": 2714,
    "prev": "This search problem is a hard combinatorial optimization problem, and as a result, constraints are frequently imposed on the structure of the model to make solving or approximating the search problem easier.",
    "curr": "In neural machine translation, an autoregressive factorization of the output probability distribution is widely used (Kalchbrenner & Blunsom, 2013;Sutskever et al., 2014;Vaswani et al., 2017), and a variety of conditional independence assumptions are made in other model classes from statistical translation models (Brown et al., 1993;Koehn et al., 2003) to non-autoregressive neural models (Lee et al., 2018).",
    "next": "Although these assumptions enable fast and accurate approximations to the search problem with simple and efficient algorithms (e.g., beam search), which can be crucial for efficient production applications, they limit the form of the models and thereby restricting the kinds of architectures that can be used to address observed model failures."
  },
  {
    "index": 293,
    "source_corpus_id": 3481593,
    "ref_id": "b10",
    "citation_corpus_id": 17597823,
    "start": 22717,
    "end": 22736,
    "prev": "Each training sample x consists of a sentence s and its sentiment labelỹ.",
    "curr": "The student for the sentiment classification task is a convolutional model which has been shown to perform best on the dataset we used (Deriu et al., 2017;Severyn & Moschitti, 2015a;b;Deriu et al., 2016).",
    "next": "The first layer of the network learns the function ψ(.)"
  },
  {
    "index": 295,
    "source_corpus_id": 257219926,
    "ref_id": "b0",
    "citation_corpus_id": 246867209,
    "start": 11467,
    "end": 11490,
    "prev": "We refer to a variant of sparsify-during-training as early pruning here, which only exerts pruning to network in the early stage of training.",
    "curr": "It includes pruning at initialization, e.g., GraSP , SynFlow (Tanaka et al., 2020), SBP-SR (Hayou et al., 2021), ProsPr (Alizadeh et al., 2022), and the conventional early pruning methods which stop pruning after several epochs of training (You et al., 2020;Liu et al., 2021b;Rachwan et al., 2022).",
    "next": "Most of these works are inspired by the discovery of Lottery Ticket Hypothesis (LTH) (Frankle & Carbin, 2019) or SNIP (Lee et al., 2019), if not both."
  },
  {
    "index": 297,
    "source_corpus_id": 202750230,
    "ref_id": "b35",
    "citation_corpus_id": 44131019,
    "start": 1691,
    "end": 1708,
    "prev": "Moreover, we show that our approach leads to small BERT-like models of higher quality compared to training from scratch or using distillation.",
    "curr": "INTRODUCTION\n\nTransformer architectures (Vaswani et al., 2017) have become the dominant architecture in natural language processing, with state-of-the-art performance across a variety of tasks, including machine translation (Vaswani et al., 2017;Ott et al., 2018), language modeling Baevski & Auli, 2018) and sentence representation (Devlin et al., 2018;.",
    "next": "Each of its layers contains millions of parameters accessed during the forward pass, making it computationally demanding in terms of memory and latency during both training and inference."
  },
  {
    "index": 300,
    "source_corpus_id": 261530381,
    "ref_id": "b1",
    "citation_corpus_id": 208076137,
    "start": 2431,
    "end": 2455,
    "prev": "nciples of FL.We propose a novel method, Federated Orthogonal Training (FOT), to overcome these drawbacks and address the global catastrophic forgetting in CFL.Our algorithm extracts the global input subspace of each layer for old tasks and modifies the aggregated updates of new tasks such that they are orthogonal to the global principal subspace of old tasks for each layer.This decreases the interference between tasks, which is the main cause for forgetting.We empirically show that FOT outperforms state-of-the-art continual learning methods in the CFL setting, achieving an average accuracy gain of up to 15% with 27% lower forgetting while only incurring a minimal computation and communication cost.",
    "curr": "INTRODUCTION\n\nFederated learning (FL) is a decentralized training solution born from the need of keeping the local data of clients private to train a global model (McMahan et al., 2017).Most of the FL works focus on the global learning of a single task (Hard et al., 2018;Yang et al., 2021;Augenstein et al., 2020).However, in real life, new tasks might arrive to the clients over time while previous data disappear due to storage limitations.For instance, assume a malware classifier is trained over multiple FL clients.The emergence of new malware families (new tasks) is inevitable, making the update of the classifier a necessity.Another real-life scenario can be the emergence of new viruses in some clients due to epidemics.The global model also has to learn to classify these new viruses (new tasks) (Yoon et al., 2021).In both of these scenarios, the model should not forget its prior knowledge while learning new tasks.",
    "next": "Continual Learning (CL) addresses this issue in centralized machine learning (ML), the problem of learning sequentially arrived tasks without forgetting (Kirkpatrick et al., 2017).Learning a global model while new tasks appear in the clients in an online manner is a problem of Continual Federated Learning (CFL) (Ma et al., 2022).An ideal CFL algorithm solving global catastrophic fo"
  },
  {
    "index": 301,
    "source_corpus_id": 235606307,
    "ref_id": "b7",
    "citation_corpus_id": 233386821,
    "start": 3669,
    "end": 3686,
    "prev": "Prior approaches typically combine information through explicit specification as products (Wu & Goodman, 2018), mixtures (Shi et al., 2019), combinations of such (Sutter et al., 2021), or through additional regularisers on the representations (Suzuki et al., 2016;Sutter et al., 2020).",
    "curr": "Here, we explore an alternative approach that leverages advances in semi-supervised VAEs (Siddharth et al., 2017; Joy et al., 2021) to repurpose existing regularisation in the VAE framework as an implicit means by which information is combined across modalities (see Figure 1).",
    "next": "We develop a novel formulation for multimodal VAEs that views the combination of information through a semi-supervised lens, as mutual supervision between modalities."
  },
  {
    "index": 303,
    "source_corpus_id": 214002473,
    "ref_id": "b31",
    "citation_corpus_id": 76667896,
    "start": 2633,
    "end": 2650,
    "prev": "INTRODUCTION\n\nSystem identification or physical parameter estimation is commonly required for control or state estimation for physical modelling, and typically relies on dedicated sensing equipment and carefully constructed experiments.",
    "curr": "Current machine learning approaches to physical modeling from video either require training by supervised regression from video to object coordinates before estimating explicit physics (Watters et al., 2017;Wu et al., 2017b;Belbute-Peres et al., 2018), or are able to discover and segment objects from video in an unsupervised manner, but do not naturally integrate with a physics engine for long-term predictions or generation of interpretable locations and physical parameters for physical reasoning (Xu et al., 2019;van Steenkiste et al., 2018).",
    "next": "In this work, we bridge the gap between unsupervised discovery of objects from video and learning the physical dynamics of a system, by learning unknown physical parameters and explicit trajectory coordinates."
  },
  {
    "index": 305,
    "source_corpus_id": 220793552,
    "ref_id": "b6",
    "citation_corpus_id": 54101493,
    "start": 3214,
    "end": 3217,
    "prev": "While we are collecting datasets with millions of training samples, DNNs are still vulnerable to domain shift, small perturbations, and adversarial examples to which humans are remarkably robust [20,4].",
    "curr": "Recent research has shown that neural networks tend to use superficial features rather than global shape information for prediction even when trained on large scale datasets such as ImageNet [7].",
    "next": "These superficial features can be local textures or even patterns imperceptible to humans but detectable to the DNNs, as is the case for adversarial examples [11]."
  },
  {
    "index": 306,
    "source_corpus_id": 256105351,
    "ref_id": "b2",
    "citation_corpus_id": 67855617,
    "start": 3840,
    "end": 3857,
    "prev": "This issue is well known as the Open World Assumption (OWA) (Ji et al., 2021).",
    "curr": "Representation learning methods are employed to mitigate the incompleteness issue by learning representations from the observed KG triples and generalizing them to unseen triples (Bordes et al., 2013;Trouillon et al., 2016;Sun et al., 2018;Zhang et al., 2019;Chami et al., 2020).",
    "next": "When considering logical queries over incomplete knowledge graphs, the query answering models are required to not only predict the unseen knowledge but also execute logical operators, such as conjunction, disjunction, and negation (Ren & Leskovec, 2020;Wang et al., 2021b)."
  },
  {
    "index": 308,
    "source_corpus_id": 259075184,
    "ref_id": "b25",
    "citation_corpus_id": 245704504,
    "start": 3001,
    "end": 3020,
    "prev": " in various applications, including but not limited to object detection (Misra et al., 2021;Liu et al., 2021b;Wang et al., 2022a), segmentation (Qian et al., 2022;Tang et al., 2022;Zhao et al., 2021), and tracking (Qi et al., 2020;Zheng et al., 2021;Shan et al., 2021).",
    "curr": "Despite the significant advances in 3D point cloud processing, acquiring task-specific 3D annotations is a highly expensive and severely limited process due to the geometric complexity.The shortage of data annotations highlights the need for adapting pre-training paradigms.Instead of training the deep network from randomly initialized weights, prior work suggests that pre-training the network on a relevant but different pre-task and later fine-tuning the weights using task-specific labels often leads to superior performance.In natural language processing and 2D vision, pre-trained models are the backbones of many exciting applications, such as real-time chatbots (Touvron et al., 2023;OpenAI, 2023) and graphic designers (Meng et al., 2021;Wang et al., 2022b).However, pre-training on point clouds has yet to demonstrate a universal performance improvement.From-scratch training remains a common practice in 3D vision.",
    "next": "Initial attempts towards 3D point-cloud pre-training primarily leverage contrastive learning (Chopra et al., 2005), especially when the point clouds are collected from indoor scenes (Xie et al., 2020;Rao et al., 2021;Liu et al., 2021a;Zhang et al., 2021;Chen et al., 2022).However, the broad application of contrastive learning-based pre-training techniques is impeded by the requirement of large batch sizes and the necessity to carefully define positive and negative pairs.In contrast to natural language processing and 2D vision, pre-training on 3D point clouds presents two unique challenges.First, the data is extremely scarce, even without annotations.Public 3D datasets are orders of magnitude smaller than 2D image datasets.Second, the lack of data annotations necessitates 3D pre-training methods to adhere to t"
  },
  {
    "index": 310,
    "source_corpus_id": 258048805,
    "ref_id": "b16",
    "citation_corpus_id": 8968704,
    "start": 2926,
    "end": 2942,
    "prev": "Recently however, with the advent of large-scale datasets, generative modelling of chirographic data started to gain traction.",
    "curr": "Specifically, models have been trained on generic doodles/drawings data (Ha & Eck, 2018), or more \"specialized\" entities like fonts (Lopes et al., 2019), diagrams (Gervais et al., 2020;, SVG Icons (Carlier et al., 2020) etc.",
    "next": "Building unconditional neural generative models not only allows understanding the distribution of chirographic data but also enables further downstream tasks (e.g."
  },
  {
    "index": 311,
    "source_corpus_id": 221971169,
    "ref_id": "b18",
    "citation_corpus_id": 202539918,
    "start": 2223,
    "end": 2243,
    "prev": "For example, syntactic effects evolve at the timescale of words, whereas semantics, emotions, and narratives can evolve at much longer timescales of tens to hundreds or thousands of words.",
    "curr": "The importance of long timescale information is evident in results showing that neural networks have outperformed classical n-gram models on many language modeling benchmarks (Melis et al., 2019;Krause et al., 2019;Dai et al., 2019).",
    "next": "This difference is attributed to these networks' ability to capture long timescale dependencies that that are impossible for n-gram models."
  },
  {
    "index": 313,
    "source_corpus_id": 252715881,
    "ref_id": "b22",
    "citation_corpus_id": 235614244,
    "start": 9912,
    "end": 9916,
    "prev": "In terms of training implicit models, [25] also proposed a novel gradient estimate called phantom gradient which relies on fixed-point unrolling and a Neumann series to provide a new update direction; computation of precise gradient is forgone.",
    "curr": "Implicit models have also been extended to more complex learning frameworks, such as attention mechanisms [24] and Graph Neural Networks [30].",
    "next": "Preliminary: Differentiable optimization layers\n\nWe consider a parameterized convex optimization problems with polyhedral constraints:\nmin x f (x; θ) s.t."
  },
  {
    "index": 314,
    "source_corpus_id": 52894384,
    "ref_id": "b5",
    "citation_corpus_id": 17263016,
    "start": 4997,
    "end": 5017,
    "prev": "RELATED WORK\n\nMulti-task learning.",
    "curr": "The dominant approach to multi-task learning is to have a model that shares parameters in a soft (Duong et al., 2015;Yang & Hospedales, 2017) or hard way (Caruana, 1993).",
    "next": "Soft sharing refers to each task having independent weights that are constrained to be similar (e.g."
  },
  {
    "index": 317,
    "source_corpus_id": 257631995,
    "ref_id": "b22",
    "citation_corpus_id": 3687922,
    "start": 2887,
    "end": 2909,
    "prev": "This implies that a goal-conditioned policy is replaceable by a policy conditioned on a \"subgoal\" existing between the goal and the agent.",
    "curr": "Based on this insight, researchers have investigated graph-based planning to construct a goal-reaching path by (a) proposing a series of subgoals and (b) executing policies conditioned on the nearest subgoal (Savinov et al., 2018;Eysenbach et al., 2019;Huang et al., 2019).",
    "next": "Since the nearby subgoals are easier to reach than the faraway goal, such planning improves the success ratio of the agent reaching the target-goal during sample collection."
  },
  {
    "index": 318,
    "source_corpus_id": 231839495,
    "ref_id": "b28",
    "citation_corpus_id": 3162051,
    "start": 3130,
    "end": 3150,
    "prev": "To improve the generalization performance of the neural networks, a body of research has been proposed to develop regularizers based on priors or to augment the training data with task-dependent transforms (Bishop, 2006;Cubuk et al., 2019).",
    "curr": "Recently, a new taskindependent data augmentation technique, called mixup, has been proposed (Zhang et al., 2018).",
    "next": "The original mixup, called Input Mixup, linearly interpolates a given pair of input data and can be easily applied to various data and tasks, improving the generalization performance and robustness of neural networks."
  },
  {
    "index": 320,
    "source_corpus_id": 67856680,
    "ref_id": "b1",
    "citation_corpus_id": 2129889,
    "start": 1851,
    "end": 1874,
    "prev": "Based on our results, we argue that AQM+ is a general task-oriented dialog algorithm that can be applied for non-yes-or-no responses.",
    "curr": "INTRODUCTION\n\nRecent advances in deep learning have led an end-to-end neural approach to task-oriented dialog problems that can reduce a laborious labeling task on states and intents (Bordes & Weston, 2017).",
    "next": "Many researchers have applied sequence-to-sequence models (Vinyals & Le, 2015) that are trained in a supervised learning (SL) and a reinforcement learning (RL) fashion to generate an appropriate sentence for the task."
  },
  {
    "index": 323,
    "source_corpus_id": 222380524,
    "ref_id": "b13",
    "citation_corpus_id": 49411844,
    "start": 7315,
    "end": 7332,
    "prev": "AutoML is a long-pursued target of machine learning (He et al., 2019).",
    "curr": "Recently a sub-field of AutoML, neural architecture search (NAS), has attracted much attention due to its success in automating the process of neural network architecture design (Zoph & Le, 2017;Pham et al., 2018;Liu et al., 2018).",
    "next": "As an essential element, loss function has also raised the interest of researchers to automate its design process."
  },
  {
    "index": 325,
    "source_corpus_id": 238419701,
    "ref_id": "b17",
    "citation_corpus_id": 3568073,
    "start": 5182,
    "end": 5203,
    "prev": "Existing generative models are mostly designed to synthesize images of fixed sizes.",
    "curr": "A few methods (Karras et al., 2018; have been recently developed to train latent generative models on high-resolution images, up to 1024×1024 pixels.",
    "next": "However, latent generative models generate images from dense latent vectors that require synthesizing all structural contents at once."
  },
  {
    "index": 326,
    "source_corpus_id": 209202457,
    "ref_id": "b14",
    "citation_corpus_id": 108304275,
    "start": 2334,
    "end": 2357,
    "prev": "This realization is at the heart of imitation learning (Ho & Ermon, 2016;Pomerleau, 1989), in which one aims to learn a behavior policy from a set of expert demonstrations -logged experience data of a near-optimal policy interacting with the environment -without explicit knowledge of rewards.",
    "curr": "Distribution matching via adversarial learning, or Adversarial Imitation Learning (AIL), has recently become a popular approach for imitation learning (Ho & Ermon, 2016;Fu et al., 2017;Ke et al., 2019;Kostrikov et al., 2019).",
    "next": "These methods interpret the states and actions provided in the expert demonstrations as a finite sample from a target distribution."
  },
  {
    "index": 328,
    "source_corpus_id": 258999368,
    "ref_id": "b12",
    "citation_corpus_id": 6628106,
    "start": 25083,
    "end": 25102,
    "prev": "(2020).",
    "curr": "All models were trained for 200 epochs using Adam (Kingma & Ba, 2015) with the standard hyperparameters of β 1 = 0.9, β 2 = 0.999, a learning rate of 0.001, and a batch size of 500 on a single A5000 GPU using PyTorch (Paszke et al., 2019).",
    "next": "We did not use any additional training heuristics such as Dropout (Srivastava et al., 2014)."
  },
  {
    "index": 329,
    "source_corpus_id": 248239812,
    "ref_id": "b18",
    "citation_corpus_id": 233714270,
    "start": 28337,
    "end": 28354,
    "prev": "RELATED WORKS\n\nVisual model-based RL.",
    "curr": "Recent developments in video prediction and contrastive learning have enabled learning of world-models from images (Watter et al., 2015;Babaeizadeh et al., 2017;Hafner et al., 2019a;Ha & Schmidhuber, 2018;Hafner et al., 2019b;Xie et al., 2020).",
    "next": "All of these approaches learn latent representations through reconstruction objectives that are amenable for planning."
  },
  {
    "index": 330,
    "source_corpus_id": 219721308,
    "ref_id": "b13",
    "citation_corpus_id": 15329665,
    "start": 9319,
    "end": 9323,
    "prev": "As in prior DPP work, our decomposition has inner dimension K that could be as large as M , but is usually much smaller in practice.",
    "curr": "Our algorithms work well for modest values of K. In cases where the natural K is larger (e.g., natural language processing applications where K might be the number of words), random projections can often be used to significantly reduce K [14].",
    "next": "New kernel decomposition and scalable learning\n\nPrior work on NDPPs proposed a maximum likelihood estimation (MLE) learning algorithm [13]."
  },
  {
    "index": 331,
    "source_corpus_id": 204734215,
    "ref_id": "b4",
    "citation_corpus_id": 52889459,
    "start": 20843,
    "end": 20862,
    "prev": "(2019) attempt to solve the undesirable convergence issue of GDA by exploiting curvature information, but they focus on simultaneous game on finding local Nash and it is unclear how to extend their algorithm to sequential games.",
    "curr": "For GAN training, there is a rich literature on different strategies to make the GAN-game welldefined, e.g., by adding instance noise (Salimans et al., 2016), by using different objectives (Nowozin et al., 2016;Gulrajani et al., 2017;Mao et al., 2017) or by tweaking the architectures (Radford et al., 2015;Brock et al., 2019).",
    "next": "While these strategies try to make the overall optimization problem easily, our work deals with a specific optimization problem and convergence issues arise in theory and in practice; hence our algorithm is orthogonal to these work."
  },
  {
    "index": 334,
    "source_corpus_id": 234790212,
    "ref_id": "b26",
    "citation_corpus_id": 1957433,
    "start": 25519,
    "end": 25544,
    "prev": "Scene Prior (SP) (Yang et al., 2018) learns a graph neural network from the FastText database (Joulin et al., 2016) and leverages the scene prior knowledge and category relationships for navigation.",
    "curr": "Word Embedding (WE) uses GloVe embedding (Pennington et al., 2014) to indicate the target category rather than detection.",
    "next": "The association between object appearances and GloVe embeddings is learned through trail and error."
  },
  {
    "index": 335,
    "source_corpus_id": 236965836,
    "ref_id": "b7",
    "citation_corpus_id": 28202810,
    "start": 25760,
    "end": 25783,
    "prev": "SQIL (Reddy et al., 2020) denotes the SQIL heuristic.",
    "curr": "In order to ensure a fair comparison among algorithms, we re-implemented all of them using the same basis RL algorithm, SAC (Haarnoja et al., 2018).",
    "next": "All imitation learning agents were run for 500000 environment interactions."
  },
  {
    "index": 336,
    "source_corpus_id": 247518687,
    "ref_id": "b3",
    "citation_corpus_id": 231592851,
    "start": 3189,
    "end": 3215,
    "prev": "On the other hand, quantifying aleatoric uncertainty enables learning of dynamics models of stochastic processes (e.g.",
    "curr": "for model-based or offline reinforcement learning) (Chua et al., 2018;Yu et al., 2020), improves performance in semantic segmentation, depth regression and object detection (Kendall & Gal, 2017;Harakeh & Waslander, 2021), and allows for risk-sensitive decision making (Dabney et al., 2018;Vlastelica et al., 2021).",
    "next": "We examine a common approach for quantifying aleatoric uncertainty in neural network regression."
  },
  {
    "index": 337,
    "source_corpus_id": 247594344,
    "ref_id": "b8",
    "citation_corpus_id": 208309981,
    "start": 13058,
    "end": 13076,
    "prev": "REGRET ANALYSIS\n\nIn this section, we provide finite time regret analysis of NPR, where the time horizon T is set beforehand.",
    "curr": "The theoretical analysis is built on the recent studies about the generalization of DNN models Chen et al., 2020;Daniely, 2017;Arora et al., 2019), which illustrate that with (stochastic) gradient descent, the learned parameters of a DNN locate in a particular regime with the generalization error being characterized by the best function in the corresponding neural tangent kernel (NTK) space (Jacot et al., 2018).",
    "next": "We leave the background of NTK in the appendix and focus on the key steps of our proof in this section."
  },
  {
    "index": 339,
    "source_corpus_id": 5034059,
    "ref_id": "b27",
    "citation_corpus_id": 3626819,
    "start": 3984,
    "end": 4005,
    "prev": "We find that unified multi-task trained models slightly outperform comparable models trained on each task separately.",
    "curr": "Our best multitask model makes use of ELMo (Peters et al., 2018), a recently proposed pre-training technique.",
    "next": "However, this model still achieves a fairly low absolute score, indicating room for improved general NLU systems."
  },
  {
    "index": 341,
    "source_corpus_id": 233204603,
    "ref_id": "b37",
    "citation_corpus_id": 224705241,
    "start": 19699,
    "end": 19703,
    "prev": "In fully supervised learning, we simply applied ReCo on top of standard supervised learning.",
    "curr": "[38].",
    "next": "The percentage and the number of labelled data used are listed in the first row."
  },
  {
    "index": 342,
    "source_corpus_id": 208202124,
    "ref_id": "b9",
    "citation_corpus_id": 12730344,
    "start": 2902,
    "end": 2927,
    "prev": "Excessive adaptation could lead to inadvertent forgetting of how to perform earlier tasks.",
    "curr": "Indeed, catastrophic forgetting is one of the main pathologies in continual learning (McCloskey and Cohen, 1989;Ratcliff, 1990;Robins, 1993Robins, , 1995French, 1999;Pape et al., 2011;Goodfellow et al., 2014a;Achille et al., 2018;Kemker et al., 2018;Kemker and Kanan, 2018;Diaz-Rodriguez et al., 2018;Zeno et al., 2018;Ahn et al., 2019;Parisi et al., 2019;Pfulb and Gepperth, 2019;Rajasegaran et al., 2019).",
    "next": "Many approaches to continual learning employ an architecture which is divided a priori into (i) a slowly evolving, global part; and (ii) a quickly evolving, task-specific, local part."
  },
  {
    "index": 343,
    "source_corpus_id": 257365037,
    "ref_id": "b21",
    "citation_corpus_id": 3366315,
    "start": 3577,
    "end": 3598,
    "prev": " in our original ICLR paper are not accurate.We elaborate on the issue and provides some fix in Appendix D.\n\nINTRODUCTION\n\nThe robustness of deep neural networks is nowadays a great challenge to establish confidence in their decisions for real-life applications.Addressing this challenge requires guarantees on the stability of the prediction, with respect to adversarial attacks.In this context, the Lipschitz constant of neural networks is a key property at the core of many recent advances.Along with the margin of the classifier, this property allows us to certify the robustness against worst-case adversarial perturbations.This certification is based on a sphere of stability within which the decision remains the same for any perturbation inside the sphere (Tsuzuku et al., 2018).",
    "curr": "The design of 1-Lipschitz layers provides a successful approach to enforce this property for the whole neural network.For this purpose, many different techniques have been devised such as spectral normalization (Miyato et al., 2018;Farnia et al., 2019), orthogonal parameterization (Trockman et al., 2021;Li et al., 2019;Singla et al., 2021;Yu et al., 2022;Xu et al., 2022), Convex Potential Layers (CPL) (Meunier et al., 2022), and Almost-Orthogonal-Layers (AOL) (Prach et al., 2022).While all these techniques share the same goal, their motivations, and derivations can greatly differ, delivering different solutions.Nevertheless, their raw experimental comparison fails to really gain insight into their peculiar performance, soundness, and in the end their possible complementarity.Therefore a question acts as a barrier for an in-depth analysis and future development:\n\nAre there common principles underlying the developments of 1-Lipschitz Layers?",
    "next": "In this paper, we propose a novel perspective to answer this question based on a unified Semidefinite Programming (SDP) approach.We introduce a common algebraic condition underlying various types of methods like spectral normalization, orthogonality-based methods, AOL, and CPL"
  },
  {
    "index": 345,
    "source_corpus_id": 238354021,
    "ref_id": "b23",
    "citation_corpus_id": 12663716,
    "start": 7909,
    "end": 7932,
    "prev": "Order Agnostic ARDMs The main difficulty of parameterizing an autoregressive model from an engineering perspective, is the need to enforce the triangular or causal dependence.",
    "curr": "Especially for 2D signals, this triangular dependence is difficult to enforce for arbitrary orders (Jain et al., 2020) and tedious design is needed for multi-scale architectures (Salimans et al., 2017).",
    "next": "To relax this requirement, we take inspiration from modern diffusion-based generative models."
  },
  {
    "index": 346,
    "source_corpus_id": 218720006,
    "ref_id": "b10",
    "citation_corpus_id": 28202810,
    "start": 5107,
    "end": 5130,
    "prev": "We then show that if we define the trust-region w.r.t.",
    "curr": "the uniform policy, instead of the old one, our off-policy MDPO coincides with the popular soft actor-critic (SAC) algorithm (Haarnoja et al., 2018).",
    "next": "We discuss this connection in detail (Section 4.2) and empirically compare these algorithms using the same set of continuous control problems (Section 5.4)."
  },
  {
    "index": 347,
    "source_corpus_id": 247618912,
    "ref_id": "b13",
    "citation_corpus_id": 3633127,
    "start": 67142,
    "end": 67164,
    "prev": "A potential downside to modeling latent factors in generative models is a decrease in image quality of generated samples that has been noted when disentanglement terms are added(Burgess et al., 2018; Khrulkov et al., 2021).",
    "curr": "Prior work has studied how to integrate additional information into GAN training, in particular ground truth class labels(Mirza & Osindero, 2014;Salimans et al., 2016;Odena, 2016;Odena et al., 2017;  Brock et al., 2019;Thekumparampil et al., 2018;Miyato & Koyama, 2018; Lučić et al., 2019), also considering noisy scenarios(Kaneko et al., 2019).",
    "next": "However, in the programmatic weak supervision setting, having multiple noisy sources of imperfect labels that include abstains present large hurdles to similar conditional modeling."
  },
  {
    "index": 348,
    "source_corpus_id": 238857286,
    "ref_id": "b25",
    "citation_corpus_id": 14124313,
    "start": 13094,
    "end": 13122,
    "prev": "The pixel-wise loss L pixel combines 1 and 2 distance as:\nL pixel (p,p) = λ 1 1 (p,p) + λ 2 2 (p,p),(9)\nwhere λ 1 and λ 2 are two hyper-parameters to control the relative importance.",
    "curr": "For the perceptual loss L perceptual , we extract features from conv5 in a VGG-16 network (Simonyan & Zisserman, 2015) pretrained on ImageNet (Krizhevsky et al., 2012) and combine the 1 and 2 distance as:\nL perceptual (p,p) = λ 3 1 (φ(p), φ(p)) + λ 4 2 (φ(p), φ(p)),(10)\nwhere φ(·) represents the output of conv5 in the VGG-16 network, and λ 3 and λ 4 are two hyperparameters.",
    "next": "Compared to the pixel-wise loss, the perceptual loss is better to capture the region-wise structure, which reflects the waveform coherence."
  },
  {
    "index": 349,
    "source_corpus_id": 259287063,
    "ref_id": "b46",
    "citation_corpus_id": 213729382,
    "start": 2381,
    "end": 2398,
    "prev": "weight decay yields models suitable for averaging, sharing identical sparse connectivity by design.Averaging these models significantly enhances generalization and OOD performance over their individual counterparts.Building on this, we introduce SPARSE MODEL SOUPS (SMS), a novel method for merging sparse models by initiating each prune-retrain cycle with the averaged model from the previous phase.SMS preserves sparsity, exploits sparse network benefits, is modular and fully parallelizable, and substantially improves IMP's performance.We further demonstrate that SMS can be adapted to enhance state-of-the-art pruning-during-training approaches.",
    "curr": "INTRODUCTION\n\nState-of-the-art Neural Network architectures typically rely on extensive over-parameterization with millions or billions of parameters (Zhang et al., 2016).In consequence, these models have significant memory requirements and the training and inference process is computationally demanding.However, recent work (e.g.Han et al., 2015;Lin et al., 2020;Renda et al., 2020;Zimmer et al., 2022) has demonstrated that these resource demands can be significantly reduced by pruning the model, i.e., removing redundant structures such as individual parameters or groups thereof.The resulting sparse models demand considerably less storage and floating-point operations (FLOPs) during inference, while retaining performance comparable to dense models.",
    "next": "A different line of research has shown that the performance of a predictor can be significantly enhanced by leveraging multiple models, instead of selecting the best one on a hold-out validation dataset and discarding the rest.Such ensembles combine the predictions of m ∈ N individually trained models by averaging their output predictions (Ganaie et al., 2021;Mehrtash et al., 2020;Chandak et al., 2023;Fort et al., 2019).Prediction ensembles have been shown to improve the predictive performance and positively impact predictive uncertainty metrics such as calibration, out-of-distribution generalizatio"
  },
  {
    "index": 350,
    "source_corpus_id": 249097923,
    "ref_id": "b16",
    "citation_corpus_id": 3526769,
    "start": 6705,
    "end": 6724,
    "prev": "In addition to the attention terms, AoA and ATA also include the typical attack losses, e.g., logit output in their objective functions, and use a hyperparameter to balance the two terms.",
    "curr": "Adversarial Perturbations (TAP) (Zhou et al., 2018), Activation attack (AA) (Inkawhich et al., 2019) and Intermediate Level Attack (ILA) (Huang et al., 2019), which all directly maximize the distance between feature maps of benign images and adversarial examples, also belong to this category.",
    "next": "TAP and AA generate adversarial examples by employing multi-layer and single-layer feature maps respectively."
  },
  {
    "index": 351,
    "source_corpus_id": 252668422,
    "ref_id": "b52",
    "citation_corpus_id": 244954585,
    "start": 2842,
    "end": 2868,
    "prev": "Transformer, originally designed for natural language processing (Vaswani et al., 2017), has flourished for various tasks in computer vision, including image classification (Dosovitskiy et al., 2020;Yuan et al., 2021a), object detection (Carion et al., 2020;Zhu et al., 2020;, semantic segmentation Yuan et al., 2021b) and low-level image processing (Chen et al., 2021).",
    "curr": "Self-attention, the key part of Transformer, selectively focuses on information of interest, and is also an important feature of the human biological system (Whittington et al., 2022;Caucheteux & King, 2022).",
    "next": "Intuitively, it is intriguing to explore applying self-attention in SNNs for more advanced deep learning, considering the biological properties of the two mechanisms."
  },
  {
    "index": 355,
    "source_corpus_id": 3525232,
    "ref_id": "b13",
    "citation_corpus_id": 12639289,
    "start": 2536,
    "end": 2566,
    "prev": "Neural network architectures lie at the heart of a variety of applications.",
    "curr": "They are practically ubiquitous across vision tasks (LeCun et al., 1995;Krizhevsky et al., 2012;Simonyan & Zisserman, 2014) and natural language understanding, from machine translation (Kalchbrenner & Blunsom, 2013;Sutskever et al., 2014;Bahdanau et al., 2014) to textual entailment (Bowman et al., 2015;Rocktäschel et al., 2015) via sentiment analysis (Socher et al., 2013;Kalchbrenner et al., 2014) and reading comprehension Hill et al., 2015;Rajpurkar et al., 2016).",
    "next": "They have been used to synthesise programs (Ling et al., 2016;Parisotto et al., 2016;Devlin et al., 2017) or internalise algorithms (Graves et al., 2016;Joulin & Mikolov, 2015;Kaiser & Sutskever, 2015;Reed & De Freitas, 2015)."
  },
  {
    "index": 357,
    "source_corpus_id": 220280819,
    "ref_id": "b24",
    "citation_corpus_id": 14717992,
    "start": 1630,
    "end": 1633,
    "prev": "Introduction\n\nReinforcement learning (RL) provides a general solution for an agent to solve a given task by learning from its own experiences of interacting with the environment.",
    "curr": "In various domains of careful task designs and informative rewards, such as game playing [3,25,37] and robotics [1,11], RL algorithms have achieved significant progress on solving challenging sequential decision making problems.",
    "next": "However, the effectiveness of such a trial-and-error learning paradigm relies on the agent's ability of exploring the task environment and collecting informative experiences."
  },
  {
    "index": 359,
    "source_corpus_id": 3529936,
    "ref_id": "b28",
    "citation_corpus_id": 11758569,
    "start": 2445,
    "end": 2449,
    "prev": "These are omnipresent in scientific and engineering research involving data analysis, for instance ecology, climate science and geography, where simulators are used to fit real-world observations to produce forecasting results.",
    "curr": "Within the machine learning community there is a recent interest in a specific type of implicit models, generative adversarial networks (GANs) [10], which has been shown to be one of the most successful approaches to image and text generation [29,49,2,5].",
    "next": "Very recently, implicit distributions have also been considered as approximate posterior distributions for Bayesian inference, e.g."
  },
  {
    "index": 360,
    "source_corpus_id": 232478335,
    "ref_id": "b28",
    "citation_corpus_id": 67856276,
    "start": 1815,
    "end": 1832,
    "prev": "Finally, we broadly discuss several circumstances wherein label smoothing will indeed lose its effectiveness.",
    "curr": "INTRODUCTION\n\nLabel smoothing (Szegedy et al., 2016) and knowledge distillation (Hinton et al., 2015) are two commonly recognized techniques in training deep neural networks and have been applied in many state-of-the-art models, such as language translation (Vaswani et al., 2017;Tan et al., 2019;Zhou et al., 2020), image classification  and speech recognition (Chiu et al., 2018;Pereyra et al., 2017;Chorowski & Jaitly, 2017).",
    "next": "Recently a large body of studies is focusing on exploring the underlying relationships between these two methods, for instance, Müller et al."
  },
  {
    "index": 362,
    "source_corpus_id": 208857488,
    "ref_id": "b45",
    "citation_corpus_id": 3531730,
    "start": 17563,
    "end": 17588,
    "prev": "Experiments\n\n\nOverparamterized LQR\n\nWe first analyze the case of the LQR as a surrogate for what may occur in deep RL, which has been done before for various topics such as sample complexity (Dean et al., 2019) and model-based RL .",
    "curr": "This is analogous to analyzing linear/logistic regression (Kakade et al., 2008;McAllester, 2003) as a surrogate to understanding extensions to deep SL techniques (Neyshabur et al., 2018a;Bartlett et al., 2017).",
    "next": "In particular, this has numerous benefits -the cost (negative of reward) function is deterministic, and allows exact gradient descent (i.e."
  },
  {
    "index": 364,
    "source_corpus_id": 245117682,
    "ref_id": "b22",
    "citation_corpus_id": 28202810,
    "start": 2632,
    "end": 2655,
    "prev": "INTRODUCTION\n\nReinforcement learning (RL) has suffered for years from a curse of poor sample complexity.",
    "curr": "State-ofthe-art model-free reinforcement learning algorithms routinely take tens of thousands of sampled transitions to solve very simple tasks and millions to solve moderately complex ones (Haarnoja et al., 2018;Lillicrap et al., 2015).",
    "next": "The current best model-based reinforcement learning (MBRL) algorithms are better, requiring thousands of samples for simple problems and hundreds of thousands of samples for harder ones (Chua et al., 2018)."
  },
  {
    "index": 365,
    "source_corpus_id": 256390058,
    "ref_id": "b10",
    "citation_corpus_id": 3484654,
    "start": 21628,
    "end": 21647,
    "prev": "Meta learning is a learning framework that aims to adapt or generalize well on new tasks.",
    "curr": "There are three approaches in meta learning: metric-based (Koch et al., 2015;Vinyals et al., 2016;Sung et al., 2018;Snell et al., 2017), model-based (Santoro et al., 2016Munkhdalai & Yu, 2017;Grant et al., 2018) and optimization-based (Finn et al., 2017;Nichol et al., 2018).",
    "next": "Neural processes (NPs) is the model-based meta learning with stochasticity."
  },
  {
    "index": 366,
    "source_corpus_id": 67856276,
    "ref_id": "b26",
    "citation_corpus_id": 1998416,
    "start": 1685,
    "end": 1705,
    "prev": "* Authors contribute equally to this work.",
    "curr": "INTRODUCTION\n\nNeural Machine Translation (NMT) has witnessed rapid development in recent years (Bahdanau et al., 2015;Luong et al., 2015b;Wu et al., 2016;Gehring et al., 2017;Vaswani et al., 2017;Guo et al., 2018;Shen et al., 2018), including advanced model structures (Gehring et al., 2017;Vaswani et al., 2017; and human parity achievements .",
    "next": "While conventional NMT can well handle single pair translation, training a separate model for each language pair is resource consuming, considering there are thousands of languages in the world 1 ."
  },
  {
    "index": 367,
    "source_corpus_id": 235293986,
    "ref_id": "b8",
    "citation_corpus_id": 56657912,
    "start": 26709,
    "end": 26739,
    "prev": "Response to gradual domain shift.",
    "curr": "In order to assess how well models can cope with a gradual transition from their native domain, we evaluate their ECE performance on data perturbed by 19 types of corruption (Hendrycks & Dietterich, 2019) at five different severity levels.",
    "next": "Figure 3 depicts the performance of models averaged across all corruptions."
  },
  {
    "index": 371,
    "source_corpus_id": 235417023,
    "ref_id": "b37",
    "citation_corpus_id": 604334,
    "start": 28490,
    "end": 28511,
    "prev": "Before that we review adversarial attacks widely used for evaluating and improving robustness.",
    "curr": "Adversarial attack\n\nSince the realization of the adversarial example phenomenon (Biggio et al., 2013;Szegedy et al., 2014), tons of adversarial attacks have been proposed (Moosavi-Dezfooli et al., 2016;Goodfellow et al., 2014;Carlini & Wagner, 2017;Dong et al., 2018;Tu et al., 2019;Madry et al., 2017).",
    "next": "Among these attacks, PGD attack (Madry et al., 2017) is one of the most commonly used attacks."
  },
  {
    "index": 372,
    "source_corpus_id": 49473438,
    "ref_id": "b25",
    "citation_corpus_id": 11758569,
    "start": 5993,
    "end": 5996,
    "prev": "Various ideas have been proposed to further improve the quality of the learned distributions and the stability of the training.",
    "curr": "(See e.g., [1,24,15,26,33,28,16,9,38] and the reference therein.)",
    "next": "However, understanding of GANs is still in its infancy."
  },
  {
    "index": 373,
    "source_corpus_id": 54101493,
    "ref_id": "b1",
    "citation_corpus_id": 68222714,
    "start": 5709,
    "end": 5732,
    "prev": "On the other hand, some rather disconnected findings point to an important role of object textures for CNN object recognition.",
    "curr": "CNNs can still classify texturised images perfectly well, even if the global shape structure is completely destroyed Brendel & Bethge, 2019).",
    "next": "Conversely, standard CNNs are bad at recognising object sketches where object shapes are preserved yet all texture cues are missing (Ballester & de Araújo, 2016)."
  },
  {
    "index": 379,
    "source_corpus_id": 236912505,
    "ref_id": "b34",
    "citation_corpus_id": 3075448,
    "start": 2712,
    "end": 2715,
    "prev": "The value function is learned using experience of the agent acting in the environment via temporal difference (TD) methods [39], which regress the value function to a target based on a combination of groundtruth returns achieved in the environment and the approximate value function itself.",
    "curr": "The need to bootstrap learning of the value function on its own estimates is known to lead to difficulties in practice, where one must achieve a careful balance between bias and variance [15,49,35,23].",
    "next": "If a slight imbalance arises, the consequences can be disastrous for learning [45,46,41]."
  },
  {
    "index": 380,
    "source_corpus_id": 261823054,
    "ref_id": "b54",
    "citation_corpus_id": 204509033,
    "start": 23353,
    "end": 23357,
    "prev": "Models\n\nWe evaluate representations obtained using correlation maximization methods based either on linear projections (TICA) (43) or non-linear encoders (VAMPNet) (39)  in Appendix E.1.We use a conditional Flow++ architecture (25) to model the variational transition distribution q ϕ (z t |z t−τ ).This is because of the modeling flexibility, the tractability of the likelihood, and the possibility of directly sampling to unfold latent simulations.Multi-layer perceptrons (MLPs) are used to model q ψ (y t |z t ), mapping the representations z t into the logits of a categorical distribution over the target y t .For all objectives, we use the same encoder, transition, and predictive architectures.",
    "curr": "Quantitative evaluation We estimate the autoinformation of the representations AI([z t ] T t=s ; τ ) at several lag time τ using SMILE (54) and we measure the amount of information that the representations contain about the targets of interest I(z t ; y t ) by difference of discrete entropies: H(y t ) − H(y t |z t ) (47; 54; 41).These measurements aim to validate the theory presented in Section 2 and better characterize the representations obtained with the different models.",
    "next": "Following the graphical model in Figure 2, given an initial system state x s of a test trajectory [x t ] T t=s and the sequence of corresponding targets [y t ] T t=s , we use the trained encoder, transition, and prediction models to unfold trajectories [ỹ s+kτ ] K k=1 ∼ q LS ([y s+kτ ] K k=1 |x s ) that cover the same temporal span as the test trajectory (K = ⌊(T − s)/τ ⌋).Similarly to previous work (4), for evaluation purposes, we consider only discrete targets y t so that we can estimate the marginal and transition probabilities for the ground truth and unfolded trajectories by counting the frequency of each state and the corresponding transition m"
  },
  {
    "index": 381,
    "source_corpus_id": 225067567,
    "ref_id": "b23",
    "citation_corpus_id": 40100965,
    "start": 2198,
    "end": 2220,
    "prev": "2  We focus on encoder-only models, and do not consider encoder-decoder models like T5 (Raffel et al.,  2020)  where none of the embedding matrices are discarded after pre-training.",
    "curr": "Output embeddings may also be\n\nINTRODUCTION\n\nThe performance of models in natural language processing (NLP) has dramatically improved in recent years, mainly driven by advances in transfer learning from large amounts of unlabeled data (Howard & Ruder, 2018;Devlin et al., 2019).",
    "next": "The most successful paradigm consists of pre-training a large Transformer (Vaswani et al., 2017) model with a self-supervised loss and fine-tuning it on data of a downstream task (Ruder et al., 2019)."
  },
  {
    "index": 382,
    "source_corpus_id": 256358895,
    "ref_id": "b1",
    "citation_corpus_id": 222398658,
    "start": 6650,
    "end": 6668,
    "prev": "However, we find that most existing masking methods are not satisfactory from this perspective (see Section 2.1 and Table 1).",
    "curr": "The masking method based on conditional distribution (Covert et al., 2020b;Frye et al., 2021) performs a bit better but still has problems.",
    "next": "In particular, we notice that Shapley values can be derived from effects of causal patterns in theory, i.e., the causal patterns are proven to be elementary effects of Shapley values."
  },
  {
    "index": 383,
    "source_corpus_id": 259108558,
    "ref_id": "b35",
    "citation_corpus_id": 174802916,
    "start": 2625,
    "end": 2629,
    "prev": "These include generating the same token or series of token repeatedly, or generating gibberish outputs.",
    "curr": "This phenomenon of degeneration for longer continuations has also been observed in autoregressive models for video [36].",
    "next": "This problem of neural text degeneration has been linked to the training objective for LLMs, which trains a conditional distribution for the next token given a (partial) sentence [10]."
  },
  {
    "index": 384,
    "source_corpus_id": 211003696,
    "ref_id": "b14",
    "citation_corpus_id": 8393918,
    "start": 3806,
    "end": 3822,
    "prev": "Also, logic rules can only cover a small part of the possible combinations of knowledge graph relations, hence limiting the application of models that are purely based on logic rules.",
    "curr": "Graph neural networks (GNNs) have recently gained increasing popularity for addressing many graph related problems effectively (Dai et al., 2016;Li et al., 2016;Kipf & Welling, 2017;Schlichtkrull et al., 2018).",
    "next": "GNN-based methods typically require sufficient labeled instances on specific end tasks to achieve good performance, however, knowledge graphs have the long-tail nature (Xiong et al., 2018), i.e., a large portion the relations in only are a few triples."
  },
  {
    "index": 386,
    "source_corpus_id": 252967887,
    "ref_id": "b8",
    "citation_corpus_id": 28202810,
    "start": 6326,
    "end": 6349,
    "prev": "PRELIMINARIES\n\nSoft Actor-Critic.",
    "curr": "In this paper, our approach is built on Soft Actor-Critic (SAC) (Haarnoja et al., 2018).",
    "next": "SAC is a stable off-policy actor-critic algorithm based on the maximum entropy reinforcement learning framework, in which the actor maximizes both the returns and the entropy."
  },
  {
    "index": 387,
    "source_corpus_id": 249538510,
    "ref_id": "b2",
    "citation_corpus_id": 202749904,
    "start": 5114,
    "end": 5137,
    "prev": "INTRODUCTION\n\nDeep generative models have demonstrated noticeable successes for modeling raw audio.",
    "curr": "The successful methods include, autoregressive models (van den Oord et al., 2016;Mehri et al., 2017;Kalchbrenner et al., 2018), flow-based models (van den Oord et al., 2018;Ping et al., 2019;Prenger et al., 2019;Kim et al., 2019;Ping et al., 2020;Lee et al., 2020), GAN-based models Kumar et al., 2019;Bińkowski et al., 2020;Yamamoto et al., 2020;Kong et al., 2020), and diffusion models (Kong et al., 2021;Chen et al., 2021;Lee et al., 2022).",
    "next": "Among these methods, GAN-based vocoders (e.g., Kong et al., 2020) can generate high-fidelity raw audio conditioned on mel spectrogram, while synthesizing hundreds of times faster than real-time on a single GPU."
  },
  {
    "index": 388,
    "source_corpus_id": 258331519,
    "ref_id": "b12",
    "citation_corpus_id": 28202810,
    "start": 12124,
    "end": 12147,
    "prev": "We do not include HalfCheetah-v3 since the simulator does not terminate even when the robot has already fallen over, so there is no standard way to determine catastrophic failures.",
    "curr": "We first select three popular algorithms, including Soft Actor-Critic (SAC) (Haarnoja et al., 2018), Twin Delayed DDPG Table 1: Failure rates (%) of relay-evaluation using states generated by stranger agents trained with 4 algorithms, reported in the 4 rows for each environment.",
    "next": "The \"Reference\" column shows the failure rate of the stranger agents, serving as the baseline failure rate for these controllable states."
  },
  {
    "index": 391,
    "source_corpus_id": 220302636,
    "ref_id": "b6",
    "citation_corpus_id": 52055130,
    "start": 6736,
    "end": 6756,
    "prev": "Related Work\n\nContrastive loss Our work aims to encourage data-efficient multimodal generative-model learning using a popular representation learning metric-contrastive loss (Hadsell et al., 2006;Weinberger and Saul, 2009).",
    "curr": "Although learning robust generalisable representations have been an important desideratum in generative-model learning (Bengio et al., 2013;Tenenbaum and Freeman, 2000), most applications of contrastive methods skip the generative component, and directly learn representations for specific tasks, as seen in contrastive predictive coding for time series data (van den Oord et al., 2018), image classification (Hénaff et al., 2019), noise contrastive estimation for vector embeddings of words (Mnih and Kavukcuoglu, 2013), as well as a range of frameworks such as DIM (Hjelm et al., 2019), MoCo (He et al., 2019), SimCLR (Chen et al., 2020) for more general visual-representation learning.",
    "next": "These approaches, while effective for the particular task at hand, fall short in learning representations that are generalisable to different tasks."
  },
  {
    "index": 392,
    "source_corpus_id": 252716080,
    "ref_id": "b9",
    "citation_corpus_id": 56895453,
    "start": 1430,
    "end": 1449,
    "prev": "Self-play (Samuel, 1967;Tesauro, 1995) has not only demonstrated the ability to abstract highdimensional state spaces as typified by AlphaGo (Silver et al., 2017), but also improved exploration coverage in partially observable environments.",
    "curr": "Communication (Sukhbaatar et al., 2016;Singh et al., 2019) exchanges their internal representations such as explored observation and hidden state in RNNs.",
    "next": "Evolutionary learning is expected to be a general framework for creating superhuman AIs as such learning can generate a high-level abstract representation without any bias in supervision."
  },
  {
    "index": 394,
    "source_corpus_id": 237353080,
    "ref_id": "b6",
    "citation_corpus_id": 215814169,
    "start": 2957,
    "end": 2974,
    "prev": "Another solution is to compress a model with small accuracy degradation so that it takes less storage and reduces System-on-Chip (SoC) memory bandwidth utilization, which can minimize power-consumption and latency.",
    "curr": "To this end, various DNN compression techniques have been proposed (Wang et al., 2019b;Dong et al., 2020;Park et al., 2018;Rastegari et al., 2016;Fan et al., 2021;Stock et al., 2020;Zhou et al., 2019;Park et al., 2019;Yu et al., 2018;Polino et al., 2018).",
    "next": "Among them, weight-clustering/sharing (Han et al., 2016;Wu et al., 2018;Ullrich et al., 2017;Stock et al., 2020) has shown a high DNN compression ratio where weights are clustered into a few shareable weight values (or centroids) based on k-means clustering."
  },
  {
    "index": 395,
    "source_corpus_id": 204008396,
    "ref_id": "b40",
    "citation_corpus_id": 1957433,
    "start": 2782,
    "end": 2806,
    "prev": "1 Published as a conference paper at ICLR 2020The joint vocabulary is composed of three disjoint sets:is the shared vocabulary set and V i J is the set of tokens that appear in the ith language only.",
    "curr": "Note that a key difference of existing supervised joint training methods is that embeddings corresponding to V s J are not shared between E L1 and E L2 , meaning that they are disjoint, as in alignment methods.DISCUSSIONWhile alignment methods have had great success, there are still some critical downsides, among which we stress the following points:\n\nINTRODUCTION\n\nContinuous word representations (Mikolov et al., 2013a;Pennington et al., 2014;Bojanowski et al., 2017) have become ubiquitous across a wide range of NLP tasks.",
    "next": "In particular, methods for crosslingual word embeddings (CLWE) have proven a powerful tool for cross-lingual transfer for downstream tasks, such as text classification (Klementiev et al., 2012), dependency parsing (Ahmad et al., 2019), named entity recognition (NER) (Xie et al., 2018;Chen et al., 2019), natural language inference , language modeling (Adams et al., 2017), and machine translation (MT) (Zou et al., 2013;Artetxe et al., 2018b;."
  },
  {
    "index": 396,
    "source_corpus_id": 219708931,
    "ref_id": "b0",
    "citation_corpus_id": 53388625,
    "start": 1969,
    "end": 1994,
    "prev": "Introduction\n\nThe majority of pruning algorithms for Deep Neural Networks require training the dense models and often fine-tuning sparse sub-networks in order to obtain their pruned counterparts.",
    "curr": "In Frankle and Carbin [2019], the authors provide empirical evidence that there exist sparse sub-networks that can be trained from scratch to achieve similar performance as their dense counterparts.",
    "next": "However, their method to find such sub-networks requires training the full-sized model and intermediate sub-networks, making the process much more expensive."
  },
  {
    "index": 397,
    "source_corpus_id": 249191769,
    "ref_id": "b7",
    "citation_corpus_id": 203641746,
    "start": 3128,
    "end": 3130,
    "prev": "To alleviate the non-convexity of neural network optimization, recent studies focus on over-parameterization as one of the promising approaches.",
    "curr": "Indeed, it is fully exploited by (i) Neural Tangent Kernel (NTK) [27,5,6,18,53, 60] and (ii) mean field analysis [36,15,31,50,14,46].",
    "next": "In the setting of NTK, a relatively large-scale initialization is considered."
  },
  {
    "index": 398,
    "source_corpus_id": 256598122,
    "ref_id": "b0",
    "citation_corpus_id": 15978939,
    "start": 6162,
    "end": 6184,
    "prev": "Domain Adaptation Typically domain adaptations consider data distributions and deviations to search for mappings aligning domains.",
    "curr": "Early literature suggested that assuming data drawn from certain probabilities (Blitzer et al., 2006) can be used to model and compensate for the domain mismatch.",
    "next": "Some studies then looked for theoretical arguments when a successful adaptation can be yielded."
  },
  {
    "index": 399,
    "source_corpus_id": 203592088,
    "ref_id": "b17",
    "citation_corpus_id": 3366315,
    "start": 4120,
    "end": 4140,
    "prev": "As such, the distribution of training images can be estimated precisely without supervision.",
    "curr": "Recent interests in generative models pertain to their training stability Gulrajani et al., 2017;Miyato et al., 2018) and improvement in quality and diversity (Karras et al., 2018;Brock et al., 2019;Karras et al., 2019).",
    "next": "Furthermore, methods to learn 3D representation from 2D images by constructing a generative model conditioned on camera parameters have been proposed (Shen et al., 2018;Sitzmann et al., 2019;Nguyen-Phuoc et al., 2019)."
  },
  {
    "index": 401,
    "source_corpus_id": 243847413,
    "ref_id": "b6",
    "citation_corpus_id": 6628106,
    "start": 15985,
    "end": 16004,
    "prev": "We then approximate the integral over the δ-ball via Monte Carlo integration by sampling k points x 1 , ..., x k uniformly in the ball to obtain the objective:\n1 k k i=1 log p(g|x i ) + log p(x i ).",
    "curr": "(3)\nFinally, as the objective is differentiable, we can use gradient-based optimizer such as Adam (Kingma & Ba, 2015), and obtain the attack in Algorithm 1.",
    "next": "Fig."
  },
  {
    "index": 403,
    "source_corpus_id": 56657849,
    "ref_id": "b17",
    "citation_corpus_id": 6628106,
    "start": 37305,
    "end": 37326,
    "prev": "For external competitor methods, we used the hyperparameters from the respective publications where applicable and otherwise the default parameters from their packages.",
    "curr": "The models were implemented in TensorFlow (Abadi et al., 2016) and optimized using Adam (Kingma and Ba, 2015).",
    "next": "C CLUSTERING PERFORMANCE MEASURES\n\nGiven that one of our most interesting tasks at hand is the clustering of data, we need some performance measures to objectively compare the quality of this clustering with other methods."
  },
  {
    "index": 404,
    "source_corpus_id": 49876500,
    "ref_id": "b13",
    "citation_corpus_id": 8968704,
    "start": 3164,
    "end": 3167,
    "prev": "From another point of view, it also indicates that the autoencoder has uncovered some structure about the data and has captured it in its latent space.",
    "curr": "These characteristics have led interpolations to be a commonly reported experimental result in studies about autoencoders [5,11,14,26,27,32] and latent-variable generative models in general [10,30,38].",
    "next": "The connection between interpolation and a \"flat\" data manifold has also been explored in the context of unsupervised representation learning [3] and regularization [40]."
  },
  {
    "index": 408,
    "source_corpus_id": 263909446,
    "ref_id": "b40",
    "citation_corpus_id": 227209335,
    "start": 1900,
    "end": 1918,
    "prev": "s generalize DDIM, mapping the reverse diffusion dynamics to a more amenable space for sampling.In contrast, splitting-based integrators, commonly used in molecular dynamics, reduce the numerical simulation error by cleverly alternating between numerical updates involving the data and auxiliary variables.After extensively studying these methods empirically and theoretically, we present a hybrid method that leads to the best-reported performance for diffusion models in augmented spaces.Applied to Phase Space Langevin Diffusion[Pandey & Mandt, 2023]on CIFAR-10, our deterministic and stochastic samplers achieve FID scores of 2.11 and 2.36 in only 100 network function evaluations (NFE) as compared to 2.57 and 2.63 for the best-performing baselines, respectively.Our code and model checkpoints will be made publicly available at https://github.com/mandt-lab/PSLD.",
    "curr": "INTRODUCTION\n\nScore-based Generative models (or Diffusion models) (Sohl-Dickstein et al., 2015;Song & Ermon, 2019;Ho et al., 2020;Song et al., 2020) have demonstrated impressive performance on various tasks, such as image and video synthesis (Dhariwal & Nichol, 2021;Ho et al., 2022a;Rombach et al., 2022;Ramesh et al., 2022;Saharia et al., 2022a;Yang et al., 2022;Ho et al., 2022b;Harvey et al., 2022), image super-resolution (Saharia et al., 2022b), and audio and speech synthesis (Chen et al., 2021;Lam et al., 2021).",
    "next": "However, high-quality sample generation in standard diffusion models requires hundreds to thousands of expensive score function evaluations.While there have been recent advances in improving the sampling efficiency (Song et al., 2021;Lu et al., 2022;Zhang & Chen, 2023), most of these efforts have been focused towards a specific family of models that perform diffusion in the data space (Song et al., 2020;Karras et al., 2022).Interestingly, recent work (Dockhorn et al., 2022b;Pandey & Mandt, 2023;Singhal et al., 2023) indicates that performing diffusion in a joint space, where the data space is augmented with auxiliary v"
  },
  {
    "index": 410,
    "source_corpus_id": 264825556,
    "ref_id": "b34",
    "citation_corpus_id": 246823323,
    "start": 4366,
    "end": 4387,
    "prev": "n training, it is shown to empirically generate samples of comparable quality to the original model (Salimans & Ho, 2022;Meng et al., 2023).Unfortunately, the GPU time required to distill a model is comparable to the training time of the original model Salimans & Ho (2022), which is often considerable.",
    "curr": "(ii) Dedicated solvers: where the specific structure of the ODE is used to design a more efficient solver (Song et al., 2020a;Lu et al., 2022a;b) and/or employ a suitable solver family from the literature of numerical analysis (Zhang & Chen, 2022;Zhang et al., 2023).The main benefit of this approach is two-fold: First, it is consistent, i.e., as the number of steps (NFE) increases, the samples converge to those of the pre-trained model.Second, it does not require further training/fine-tuning of the pre-trained model, consequently avoiding long additional training times and access to training data.Related to our approach, some works have tried to learn an ODE solver within a certain class (Watson et al., 2021;Duan et al., 2023); however, they do not guarantee consistency and usually introduce moderate improvements over generic dedicated solvers.In this paper, we introduce Bespoke solvers, a framework for learning consistent ODE solvers custom-tailored to pre-trained flow models.The main motivation for Bespoke solvers is that different models exhibit sampling paths with different characteristics, leading to local truncation errors that are specific to each instance of a trained model.A key observation of this paper is that optimizing a solver for a particular model can significantly improve quality of samples for low NFE compared to existing dedicated solvers.Furthermore, Bespoke solvers use a very small number of learnable parameters and consequently are efficient to train.For example, we have trained n ∈ {5, 8, 10} steps Bespoke solvers for a pretrained ImageNet-64×64 flow model with {40, 64, 80} learnable parameters (resp.",
    "next": ")producing images with Fréchet Inception Distances (FID) of 2.2, 1."
  },
  {
    "index": 412,
    "source_corpus_id": 256827824,
    "ref_id": "b53",
    "citation_corpus_id": 5834589,
    "start": 25088,
    "end": 25092,
    "prev": "Figure 4 shows that for some rules, i.e.Trimmed Mean, the presence of even a single Byzantine worker has a catastrophic impact.For other rules, as the number of Byzantine workers increases, filtering out the outliers becomes more challenging because the amount of noise increases.Regardless, FA remains more robust compared to other approaches.",
    "curr": "Marginal utility of larger batch sizes under a fixed noise level:\n\nWe empirically verified the batch size required to identify our optimal Y * -the FA matrix at each iteration.In particular, we fixed the noise level to f = 3 Byzantine workers and varied batch sizes.We show the results in Figure 5.Our results indicate that, in cases where a larger batch size is a training requirement, FA achieves a significantly better accuracy compared to the existing state of the art aggregators.This may be useful in some large scale vision applications, see [54,55] for more details.Empirically, we can already see that our spectral relaxation to identify gradient subspace is effective in practice in all our experiments.",
    "next": "Tolerance to communication loss:\n\nTo analyze the effect of unreliable communication channels between the workers and the parameter server on convergence, we design an experiment where the physical link between some of the workers and the parameter server randomly drops a percentage of packets.Here, we set the loss rate of three links to 10% i.e., there are 3 Byzantine workers in our setting.The loss is introduced using the netem queuing discipline in Linux designed to emulate the properties of wide area networks [56].The two main takeaways in Figure 6a are:\n\n1."
  },
  {
    "index": 413,
    "source_corpus_id": 259375820,
    "ref_id": "b11",
    "citation_corpus_id": 241035330,
    "start": 2653,
    "end": 2671,
    "prev": "Introduction\n\nLarge language models (LLMs) demonstrate the surprising ability of in-context learning, where an LLM \"learns\" to solve a task by conditioning on a prompt containing input-output exemplars [Brown et al., 2020, Lieber et al., 2021, Radford et al., 2019, Wang and Komatsuzaki, 2021.",
    "curr": "Recent works have advanced the understanding of in-context learning via empirical analysis [Min et al., 2022, Akyürek et al., 2023, von Oswald et al., 2022, Dai et al., 2023, but theoretical analysis remains limited [Xie et al., 2022].",
    "next": "A recent line of work [Garg et al., 2022, Akyürek et al., 2023, von Oswald et al., 2022, Dai et al., 2023 empirically finds that transformers can be trained to implement algorithms that solve linear regression problems in-context."
  },
  {
    "index": 414,
    "source_corpus_id": 246485738,
    "ref_id": "b12",
    "citation_corpus_id": 235293778,
    "start": 30919,
    "end": 30944,
    "prev": "We ran the code from their open source repository 4 as is, hyper-parameters used are all defaults as suggested in their paper.",
    "curr": "TNC (Tonekaboni et al., 2021) TNC proposes a self-supervised framework for learning generalizable representations for non-stationary time series.",
    "next": "They make use of the augmented Dickey-Fuller test for stationarity to ensure positive samples come from the a neighborhood of similar signals."
  },
  {
    "index": 417,
    "source_corpus_id": 263909387,
    "ref_id": "b19",
    "citation_corpus_id": 215768182,
    "start": 23887,
    "end": 23909,
    "prev": "Following Leviathan et al.",
    "curr": "(2023), we evaluate two model types: 1) GPTlike decoder-only Transformer models trained using the standard autoregressive objective on LM1B task (Chelba et al., 2013), where the target and draft models have 234M and 33M parameters, respectively; and 2) Standard encoder-decoder T5 v1.1 models (Raffel et al., 2020) supervised fine-tuned on four different tasks, with T5-XL (3B) and T5-Small (77M) serving as the target and draft models, respectively.As for the four datasets, we utilize two datasets form the T5 paper, namely WMT EnDe (Bojar et al., 2014) and CNN/DM (Hermann et al., 2015) which deal with translation and text summarization, respectively.The remaining two tasks used to test T5 models are XSum (Narayan et al., 2018) and GSM8K (Cobbe et al., 2021), which deal with abstractive summarization and arithmetic reasoning, respectively.See Appendix C for more details.",
    "next": "Training data for KD.We study five KD algorithms outlined in Table 1.However, for Se-qKD (Kim & Rush, 2016) and f -Distill (Wen et al., 2023), we opt for an online data generation approach from the teacher instead of a conventional fixed offline teacher-generated dataset.This approach, while computationally expensive, yields a more diverse dataset.For GKD, we solely rely on the data generated by the online student model, excluding the static ground truth data.All data generated by either the teacher or the student is based on temperature sampling with a temperature of 1.0 (see Appendix D.1.3for an ablation study on sampling temperature)."
  },
  {
    "index": 418,
    "source_corpus_id": 53034786,
    "ref_id": "b42",
    "citation_corpus_id": 990233,
    "start": 2951,
    "end": 2972,
    "prev": "RNNs explicitly imposes a chain structure on the data.",
    "curr": "This chain structure may seem at odds with the latent non-sequential structure of language and poses several difficulties for the processing of natural language data with deep learning methods, such as capturing long-term dependencies (Bengio et al., 2009), achieving good generalization (Bowman et al., 2015), handling negation (Socher et al., 2013), etc.",
    "next": "Meanwhile, some evidence exists that an RNN with sufficient capacity has the potential to encode such a tree structure implicitly ."
  },
  {
    "index": 419,
    "source_corpus_id": 256827026,
    "ref_id": "b7",
    "citation_corpus_id": 52967399,
    "start": 2468,
    "end": 2471,
    "prev": "Notably, on the MSRVTT retrieval task, UniAdapter achieves 49.7% recall@1 with only 2.2% tunable model parameters, outperforming the latest competitors by 2.0%.",
    "curr": "The code and models are available at https\n\nIntroduction\n\nThe pretrain-finetune paradigm has achieved great success in natural language processing (NLP) [8,4], computer vision (CV) [41], and multimodal modeling [37,22], where models are first pre-trained with large-scale data, and then fully fine-tuned for each downstream task.",
    "next": "Recent research further finds that fine-tuning/adapting a foundation model to a new modality by introducing additional trainable modules significantly outperforms previous works, such as temporal modeling modules [13,23,33] for image-to-video transferring (see Figure 1 (a))."
  },
  {
    "index": 421,
    "source_corpus_id": 252715598,
    "ref_id": "b3",
    "citation_corpus_id": 11212020,
    "start": 2344,
    "end": 2367,
    "prev": "Code is publicly available.",
    "curr": "INTRODUCTION\n\nThe vision community has witnessed the prevalence of self-attention (Bahdanau et al., 2015) and Transformers (Vaswani et al., 2017).",
    "next": "The success of Transformers in natural language processing motivates the creation of their variants for vision recognition."
  },
  {
    "index": 423,
    "source_corpus_id": 237372712,
    "ref_id": "b18",
    "citation_corpus_id": 5034059,
    "start": 1429,
    "end": 1448,
    "prev": "INTRODUCTION\n\nShared benchmarks and datasets have historically played a crucial role in driving advances in largescale applications of deep learning, e.g.",
    "curr": "in computer vision (Deng et al., 2009) and natural language processing (Wang et al., 2019;Rajpurkar et al., 2016;Paperno et al., 2016).",
    "next": "Neural theorem proving is a rapidly developing area which aims to apply techniques from deep learning to interactive theorem proving."
  },
  {
    "index": 425,
    "source_corpus_id": 259187750,
    "ref_id": "b11",
    "citation_corpus_id": 232185279,
    "start": 12657,
    "end": 12661,
    "prev": "In particular, we consider the following three types of in-processing methods.",
    "curr": "Gap Regularization [12] simplifies optimization by offering a smooth approximation to real loss functions, which are often non-convex or difficult to optimize directly.",
    "next": "This approach includes DiffDP, DiffEodd, and DiffEopp."
  },
  {
    "index": 429,
    "source_corpus_id": 238419003,
    "ref_id": "b2",
    "citation_corpus_id": 52156206,
    "start": 8524,
    "end": 8556,
    "prev": "Finally, other domain adaptation and transfer learning settings use Gromov-Wasserstein variants, e.g.",
    "curr": "for transfer between word embedding spaces (Alvarez-Melis & Jaakkola, 2018) and image spaces (Vayer et al., 2020b).",
    "next": "PRELIMINARIES\n\nMetric Markov Decision Process."
  },
  {
    "index": 432,
    "source_corpus_id": 254877694,
    "ref_id": "b8",
    "citation_corpus_id": 220055784,
    "start": 4552,
    "end": 4574,
    "prev": "Then, we use active learning and crowdworker judgments to identify pairs that deserve similar treatment according to human intuition.",
    "curr": "In the context of text classification, most existing works have cast similarity in terms of word replacement (Dixon et al., 2018;Garg et al., 2019;Yurochkin & Sun, 2021;Liang et al., 2020).",
    "next": "Given a sentence s, a similar sentence s is generated by replacing each word in s, that belongs to a list of words A j indicative of a demographic group j, by a word from list A j , indicative of another demographic group j = j."
  },
  {
    "index": 435,
    "source_corpus_id": 253098972,
    "ref_id": "b5",
    "citation_corpus_id": 165163607,
    "start": 2447,
    "end": 2473,
    "prev": "We conduct experiments across a diverse set of eight NLP tasks using models of different scales (T5-{base, large, XL}) and find that SESoM consistently outperforms the existing models of the same as well as larger parametric scale by a large margin.",
    "curr": "INTRODUCTION\n\nRecent few years have witnessed the great success of large pre-trained language models (PLM) (Kenton & Toutanova, 2019;Liu et al., 2019;Radford et al., 2019;Raffel et al., 2020;Brown et al., 2020).",
    "next": "The size of pre-trained models which can easily go to billions of parameters (Brown et al., 2020;Raffel et al., 2020), however, hinder their real-world deployments and applications."
  },
  {
    "index": 437,
    "source_corpus_id": 254535963,
    "ref_id": "b8",
    "citation_corpus_id": 21529792,
    "start": 7416,
    "end": 7433,
    "prev": "te that the same finding holds even with real humans.Overall, our results suggest an optimistic perspective on the framework of reward learning, and that efforts in improving human models will further enhance the quality of the inferred rewards.",
    "curr": "RELATED WORK\n\nInverse reinforcement learning (IRL) aims to use expert demonstrations, often from a human, to infer a reward function (Ng & Russel, 2000;Ziebart et al., 2008).Maximum-entropy (MaxEnt) IRL is a popular IRL framework that models the demonstrator as noisily optimal, maximizing reward while also randomising actions as much as possible (Ziebart et al., 2008;2010).This is equivalent to modeling humans as Boltzmann rational.MaxEnt IRL is preferred in practice over Bayesian IRL (Ramachandran & Amir, 2007), which learns a posterior over reward functions rather than a point estimate, due to better scaling in high-dimensional environments (Wulfmeier et al., 2015).More recently, Guided Cost Learning (Finn et al., 2016) and Adversarial IRL (Fu et al., 2018) learn reward functions more robust to environment changes, but build off similar modeling assumptions as MaxEnt IRL.Gleave & Toyer (2022) connected MaxEnt IRL to maximum likelihood estimation (MLE), which is the framework that we consider in this work.One of the challenges with IRL is that rewards are not always uniquely identified from expert demonstrations (Cao et al., 2021;Kim et al., 2021).Since identifiability is orthogonal to the main message of our work-sensitivity to misspecified human models-we assume that the dataset avoids this ambiguity.",
    "next": "Recent IRL algorithms attempt to account for possible irrationalities in the expert (Evans et al., 2016;Reddy et al., 2018;Shah et al., 2019).Reddy et al."
  },
  {
    "index": 440,
    "source_corpus_id": 258865444,
    "ref_id": "b14",
    "citation_corpus_id": 248987078,
    "start": 3398,
    "end": 3420,
    "prev": "onto the users, who frequently either for convenience or lack of awareness simply do not choose to do so (Gleason et al., 2019).We find that as many as 98% of images uploaded to Twitter even after the widespread feature rollout do not have alt-text, to say nothing of the quality of those that do.When a screen reader encounters such an image, it will simply say \"Image\", leaving the user with no meaningful information about what the image is actually of.Even when images on Twitter do have accompanying user-written alt-text, the quality is inconsistent as not all users are well informed regarding best practices.",
    "curr": "Note that while there is a long line of existing research on the broader task of captioning images more generally, alt-text generation for social media is a special case of this task, which in turn comes with its own challenges.Well written alt-text is generally more explicitly descriptive than a high level caption and may emphasize specific details in the image based on context (Kreiss et al., 2022a).See Figure 1 for an example.Furthermore, the distribution of image types on Twitter differs substantially from those found in traditional captioning datasets, and may contain digital artwork, promotional graphics, or screenshots containing text.An additional challenge is that Twitter users are not well trained in the practice of writing alt-text, and therefore native \"gold\" examples can vary  ()   Alt-text: Delicious pink margarita sits on a table full of salsa and chips\n\nCaption: having cocktail at person who wants to join ?",
    "next": "Our Dataset Conceptual Captions\n\nFigure 1: Left: An image that requires textual context to write accurate alt-text for.Without conditioning on the tweet text, the election flyers are indistinguishable from books to a traditional captioning system.Right: Two similar images from our dataset and Conceptual Captions with their gold labels.The alt-text for the first image is literally descriptive while the second is more colloquial."
  },
  {
    "index": 441,
    "source_corpus_id": 257687492,
    "ref_id": "b26",
    "citation_corpus_id": 52908393,
    "start": 27139,
    "end": 27164,
    "prev": "While PBT-MAP-ELITES and PBT use similar strategies to update the population of agents, PBT only seeks the highest-performing agent by extracting the best one from the final population while PBT-MAP-ELITES aims to find a diverse collection of high-performing agents.",
    "curr": "Several methods such as CERL, ERL, and CEM-RL (Pourchot & Sigaud, 2019;Khadka & Tumer, 2018;Khadka et al., 2019) combine ES algorithms with PBRL methods to improve the asymptotic performance and sample efficiency of standard RL methods.",
    "next": "Other methods, such as DvD (Parker-Holder et al., 2020) and P3S-TD3 (Jung et al., 2020), train populations of agents and add terms in their loss functions to encourage the agents to explore different regions of the state-action space but always with the end goal of maximizing the performance of the best agent in the population."
  },
  {
    "index": 442,
    "source_corpus_id": 3497822,
    "ref_id": "b4",
    "citation_corpus_id": 51559,
    "start": 3308,
    "end": 3311,
    "prev": "Bradbury et al.",
    "curr": "[5], Kalchbrenner et al.",
    "next": "[12], Gehring et al."
  },
  {
    "index": 444,
    "source_corpus_id": 27494814,
    "ref_id": "b24",
    "citation_corpus_id": 2973141,
    "start": 6807,
    "end": 6824,
    "prev": "In Optimal Brain Surgeon (Hassibi et al., 1993), the saliency for each parameter was computed using the inverse Hessian matrix, and the low-saliency weights are pruned and all other weights in the network are updated using the second-order information.",
    "curr": "More recently, magnitude-based weight pruning methods have become popular techniques for network pruning (Han et al., 2015b,a;See et al., 2016;Narang et al., 2017).",
    "next": "Magnitude-based weight pruning techniques are computationally efficient, scaling to large networks and datasets."
  },
  {
    "index": 448,
    "source_corpus_id": 235417313,
    "ref_id": "b0",
    "citation_corpus_id": 235614315,
    "start": 4834,
    "end": 4852,
    "prev": "arXiv preprint arXiv:\n\nINTRODUCTION\n\nFederated learning (FL) (McMahan et al., 2017), which is a distributed learning framework with personalized data, has become an attractive field of research.",
    "curr": "From the early days of this field, improving a single global model across devices has been the main objective (Zhao et al., 2018;Duan et al., 2019;Li et al., 2018;Acar et al., 2021), where the global model suffers from data heterogeneity.",
    "next": "Many researchers have recently focused on multiple personalized models by leveraging data heterogeneity across devices as a blessing in disguise (Chen et al., 2018;Dinh et al., 2021;Zhang et al., 2020;Fallah et al., 2020;Shamsian et al., 2021;Smith et al., 2017)."
  },
  {
    "index": 449,
    "source_corpus_id": 220249871,
    "ref_id": "b3",
    "citation_corpus_id": 213152193,
    "start": 9504,
    "end": 9524,
    "prev": "(2017), a hand-craft positional encoding based on sinusoid function is proposed.",
    "curr": "But learnable positional encoding, i.e., treating p i as parameters, is often used in the recent works Devlin et al., 2018;Clark et al., 2019b).",
    "next": "Relative Positional Encoding."
  },
  {
    "index": 450,
    "source_corpus_id": 60440615,
    "ref_id": "b24",
    "citation_corpus_id": 16631020,
    "start": 1779,
    "end": 1798,
    "prev": "Experiments on a standard dataset of four low-resource languages show consistent improvements over strong multilingual NMT baselines, with gains of up to 2 BLEU on one of the tested languages, achieving the new state-of-the-art on all four language pairs 1 .",
    "curr": "INTRODUCTION\n\nMultilingual Neural Machine Translation (NMT) has shown great potential both in creating parameter-efficient MT systems for many languages (Johnson et al., 2016), and in improving translation quality of low-resource languages (Zoph et al., 2016;Firat et al., 2016;Gu et al., 2018;Neubig & Hu, 2018;Nguyen & Chiang, 2018).",
    "next": "Despite the success of multilingual NMT, it remains a research question how to represent the words from multiple languages in a way that is both parameter efficient and conducive to cross-lingual generalization."
  },
  {
    "index": 451,
    "source_corpus_id": 252762329,
    "ref_id": "b14",
    "citation_corpus_id": 238418995,
    "start": 7526,
    "end": 7544,
    "prev": "They show that the loss can non-monotonically decrease even when the sharpness λ > 2/η.",
    "curr": "The non-monotone property of the loss has also been observed in many other settings (Jastrzebski et al., 2020;Xing et al., 2018;Lewkowycz et al., 2020;Wang et al., 2022;Arora et al., 2018;Li et al., 2022a).",
    "next": "Recently several works try to understand the mechanism behind EoS with different loss functions under various assumptions (Ahn et al., 2022;Ma et al., 2022;Lyu et al., 2022;Li et al., 2022b)."
  },
  {
    "index": 452,
    "source_corpus_id": 253098739,
    "ref_id": "b33",
    "citation_corpus_id": 68137503,
    "start": 3821,
    "end": 3838,
    "prev": "To address these shortcomings, we take a new approach to formulating the meta-learning problem and represent prior knowledge in a novel way.",
    "curr": "We build on recent advances in functional approximate inference for BNNs that perform Bayesian inference in the function space rather than in the parameter space of neural networks (Wang et al., 2018;Sun et al., 2019).",
    "next": "When viewing the BNN prior and posterior as stochastic processes, the perfect Bayesian prior is the (true) data-generating stochastic process itself."
  },
  {
    "index": 453,
    "source_corpus_id": 251710555,
    "ref_id": "b6",
    "citation_corpus_id": 52967399,
    "start": 2072,
    "end": 2093,
    "prev": "Compared to Linformer, the accuracy can be as much as 12% higher while using similar FLOPs in the attention layer.",
    "curr": "INTRODUCTION\n\nSelf attention layer is the key component of Transformers (Vaswani et al., 2017), enabling them to achieve state of the art performance across tasks in Natural Language Processing (Devlin et al., 2019;Raffel et al., 2019) and Computer Vision (Dosovitskiy et al., 2021).",
    "next": "Attention computation scales quadratically (n 2 ) with the input sequence length (n), making it a key bottleneck in scaling Transformers to long inputs."
  },
  {
    "index": 457,
    "source_corpus_id": 248266388,
    "ref_id": "b9",
    "citation_corpus_id": 14717992,
    "start": 1555,
    "end": 1575,
    "prev": "* Correspondence to clare.",
    "curr": "INTRODUCTION\n\nDeep reinforcement learning has achieved remarkable successes in a variety of tasks (Mnih et al., 2015;Moravčík et al., 2017;Silver et al., 2017;Abreu et al., 2019), but its impressive performance is mirrored by its brittleness and sensitivity to seemingly innocuous design choices (Henderson et al., 2018).",
    "next": "In sparse-reward environments in particular, even different random seeds of the same algorithm can attain dramatically different performance outcomes."
  },
  {
    "index": 458,
    "source_corpus_id": 221447287,
    "ref_id": "b36",
    "citation_corpus_id": 49882757,
    "start": 2654,
    "end": 2672,
    "prev": "However, autoregressive models require a large number of sequential computational steps to generate one audio sample, which makes it challenging to deploy these models in real-world production applications, such as digital voice assistants on smart speakers, even using specialized hardware designed for neural networks.",
    "curr": "There has been a plethora of research into non-autoregressive models for audio generation, including normalizing flow (NF) models like inverse autoregressive flow (IAF) Ping et al., 2019), generative flow (Glow) (Prenger et al., 2019;Kim et al., 2019), and continuous normalizing flow (CNF) Wu & Ling, 2020), implicit generative models like generative adversarial network (GAN) (Donahue et al., 2018;Engel et al., 2019;Yamamoto et al., 2020;Bikowski et al., 2020;McCarthy & Ahmed, 2020) and energy score (Gritsenko et al., 2020), variational models like variational auto-encoder (VAE) (Peng et al., 2020), models inspired by digital signal processing (Ai & Ling, 2020;Engel et al., 2020), and those by the speech production mechanism (Juvela et al., 2019;Wang et al., 2020).",
    "next": "Although these models improve inference speed thanks to their architectures offering less sequential operations, they often yield lower quality samples than the autoregressive models."
  },
  {
    "index": 460,
    "source_corpus_id": 48361056,
    "ref_id": "b7",
    "citation_corpus_id": 848112,
    "start": 4514,
    "end": 4539,
    "prev": "Searching for a more stable function family (Radford et al., 2015) and utilizing better objectives Gulrajani et al., 2017) have been explored for training and instability issues.",
    "curr": "For tackling non-convergence, regularizing objective functions (Mescheder et al., 2017;Salimans et al., 2016), two time-scale techniques (Heusel et al., 2017) and extra-gradient (optimistic) methods (Daskalakis et al., 2018;Mertikopoulos et al., 2019) have been studied.",
    "next": "One reason for non-convergence is cycling around an optimal solution (Mertikopoulos et al., 2018;, or even slow outward spiraling (Bailey & Piliouras, 2018)."
  },
  {
    "index": 463,
    "source_corpus_id": 231719730,
    "ref_id": "b5",
    "citation_corpus_id": 213152193,
    "start": 20388,
    "end": 20408,
    "prev": "Unlike the training datasets, the evaluation document was translated to plain L A T E X manually using the PDF as a reference, in order to avoid possible spurious patterns in automatically expanded sT E X.",
    "curr": "S T E X-ANNOTATING WITH MACHINE LEARNING AS AN NMT TASK\n\nIn the course of our experiments, we considered our disambiguation task as a machine translation (NMT) problem, the models for which have been proven to be quite effective even beyond natural language translations (Clark et al., 2020).",
    "next": "In fact, the autoformalization projects mentiond in Section 3, which are spiritually closest to our task, all used NMT models with positive results."
  },
  {
    "index": 465,
    "source_corpus_id": 58014184,
    "ref_id": "b2",
    "citation_corpus_id": 11212020,
    "start": 10881,
    "end": 10904,
    "prev": "The queried values are invariant to the ordering of the key-value pairs; this permutation invariance property of attention is key in its application to NPs.",
    "curr": "The idea of using a differentiable addressing mechanism that can be learned from the data has been applied successfully in various areas of Deep Learning, namely handwriting generation and recognition (Graves, 2012) and neural machine translation (Bahdanau et al., 2015).",
    "next": "More recently, there has been work employing self-attention (where keys and queries are identical) to give expressive sequence-to-sequence mappings in natural language processing (Vaswani et al., 2017) and image modelling (Parmar et al., 2018)."
  },
  {
    "index": 470,
    "source_corpus_id": 264406180,
    "ref_id": "b6",
    "citation_corpus_id": 229923720,
    "start": 17860,
    "end": 17880,
    "prev": "Mean ablating the 1st or 2nd head slightly increased the average loss for BA questions from 0.05 to 0.08, whereas ablating the 3rd head substantially increased the loss to 3.7, confirming that the 3rd head is doing the BA task.",
    "curr": "The MLP can be thought of as a \"key-value pair\" memory [Meng et al., 2022, Geva et al., 2021] that can hold many bigrams and trigrams.We claim our MLP pulls together the two-state 1st head result, the tri-state 2nd head result and the ten-state 3rd head result value, treating them as a trigram with 60 (2 x 3 x 10) possible keys.For each digit, the MLP has memorised the mapping of these 60 keys to the 60 correct digit answers (0 to 9).We haven't proven this experimentally.Our MLP is sufficiently large to store this many mappings with zero interference between mappings [Elhage et al., 2022].",
    "next": "Despite being feasible, the model does not calculate the task MC1 in layers 7 to 11.Instead it completes each digit calculation in 1 layer, possibly because there are training optimisation benefits in generating a \"compact\" algorithm."
  },
  {
    "index": 471,
    "source_corpus_id": 59413817,
    "ref_id": "b12",
    "citation_corpus_id": 3286670,
    "start": 1973,
    "end": 1995,
    "prev": "The ability of these neural networks to efficiently represent a rich class of functions with a relatively small number of parameters is often referred to as depth efficiency, and the theory behind this phenomenon is not yet fully understood.",
    "curr": "A recent line of work Khrulkov et al., 2018;Cohen et al., 2018) focuses on comparing various deep learning architectures in terms of their expressive power.",
    "next": "It was shown in  that ConvNets with product pooling are exponentially more expressive than shallow networks, that is there exist functions realized by ConvNets which require an exponentially large number of parameters in order to be realized by shallow nets."
  },
  {
    "index": 473,
    "source_corpus_id": 238419359,
    "ref_id": "b5",
    "citation_corpus_id": 189762527,
    "start": 2107,
    "end": 2129,
    "prev": "Such knowledge can be implicitly encoded or explicitly stored in structured knowledge graphs (KGs).",
    "curr": "Large pre-trained language models (Devlin et al., 2018;Radford et al., 2018;Brown et al., 2020) are found to be effective in learning broad and rich implicit knowledge (Petroni et al., 2019;Bosselut et al., 2019;Talmor et al., 2020) and thus demonstrate much success for QA tasks.",
    "next": "Nevertheless, pretrained LMs struggle a lot with structured reasoning such as handling negation (Ribeiro et al., 2020;Yasunaga et al., 2021)."
  },
  {
    "index": 475,
    "source_corpus_id": 244714829,
    "ref_id": "b10",
    "citation_corpus_id": 85501317,
    "start": 27512,
    "end": 27529,
    "prev": "EXPERIMENTS\n\n\nEXPERIMENTAL SETTINGS\n\nWe train the conditional VAE described in Section 4.2 and all implicit functions using the Adam solver (Kingma & Ba, 2014) with a learning rate of 5 × 10 −5 .",
    "curr": "To balance the KL-divergence and the reconstruction objectives in the VAE, we adopt the Cyclical Annealing Schedule introduced in (Fu et al., 2019).",
    "next": "We evaluate the proposed environment field on agent navigation in 2D mazes (Section 5.2) and compare with the VIN (Tamar et al., 2016)."
  },
  {
    "index": 476,
    "source_corpus_id": 253510295,
    "ref_id": "b2",
    "citation_corpus_id": 53729760,
    "start": 3778,
    "end": 3780,
    "prev": "6\n\nINTRODUCTION\n\nSemantic image synthesis refers to generating photo-realistic images conditioned on pixel-level semantic labels.",
    "curr": "This task has a wide range of applications such as image editing and content generation (Chen & Koltun, 2017;Isola et al., 2017;Guo et al., 2022;Gu et al., 2019;Bau et al., 2019a;b;Liu et al., 2019;Qi et al., 2018;Jiang et al., 2020).",
    "next": "Although existing methods conducted interesting explorations, we still observe unsatisfactory aspects, mainly in the generated local structures and details, as well as small-scale objects, which we believe are mainly due to three reasons: 1) Conventional methods (Park et al., 2019;Wang et al., 2018;Liu et al., 2019) generally take the semantic label map as input directly."
  },
  {
    "index": 477,
    "source_corpus_id": 252873224,
    "ref_id": "b22",
    "citation_corpus_id": 189898036,
    "start": 2669,
    "end": 2675,
    "prev": "For gradient flow, we consider the standard leaky ReLU activation, φ(z) = max(γz, z).",
    "curr": "Our starting point in this setting is recent work by Lyu and Li [LL20] and Ji and Telgarsky [JT20] that show that, provided the network interpolates the training data at some time, gradient flow on homogeneous networks, such as two-layer leaky ReLU networks, converges (in direction) to a network that satisfies the Karush-Kuhn-Tucker (KKT) conditions for the margin-maximization problem,\nmin W 1 2 W 2 F s.t.",
    "next": "∀i ∈ [n], y i f (x i ; W ) ≥ 1 ."
  },
  {
    "index": 478,
    "source_corpus_id": 14298291,
    "ref_id": "b21",
    "citation_corpus_id": 806709,
    "start": 3895,
    "end": 3916,
    "prev": "(2014), who focus on learning expression representations to aid the search for computationally efficient identities.",
    "curr": "They use recursive neural networks (TREENN) 1 (Socher et al., 2012) for modeling homogenous, single-variable polynomial expressions.",
    "next": "While they present impressive results, we find that the TREENN model fails when applied to more complex symbolic polynomial and boolean expressions."
  },
  {
    "index": 479,
    "source_corpus_id": 228705808,
    "ref_id": "b12",
    "citation_corpus_id": 208248131,
    "start": 2018,
    "end": 2037,
    "prev": "INTRODUCTION\n\nGranger causality (GC) (Granger, 1969) is a popular practical approach for the analysis of multivariate time series and has become instrumental in exploratory analysis (McCracken, 2016) in various disciplines, such as neuroscience (Roebroeck et al., 2005), economics (Appiah, 2018), and climatology (Charakopoulos et al., 2018).",
    "curr": "Recently, the focus of the methodological research has been on inferring GC under nonlinear dynamics (Tank et al., 2018;Nauta et al., 2019;Wu et al., 2020;Khanna & Tan, 2020;Löwe et al., 2020), causal structures varying across replicates (Löwe et al., 2020), and unobserved confounding (Nauta et al., 2019;Löwe et al., 2020).",
    "next": "To the best of our knowledge, the latest powerful techniques for inferring GC do not target the effect sign detection (see Section 2.1 for a formal definition) or exploration of effect variability with time and, thus, have limited interpretability."
  },
  {
    "index": 481,
    "source_corpus_id": 256846551,
    "ref_id": "b19",
    "citation_corpus_id": 67855860,
    "start": 2396,
    "end": 2418,
    "prev": "Some recent works in the field of question-answering (QA) have demonstrated that language models can bypass some of these issues and learn to reason directly over natural language , allowing for more flexible and adaptable reasoning capabilities.",
    "curr": "Another advantage of performing multi-step reasoning over natural language is that it allows for more inspectable outputs, improving the explainability of models that are otherwise regarded as black box systems (Jain & Wallace, 2019;Rajani et al., 2019a;Danilevsky et al., 2020).",
    "next": "Despite the recent progress, we notice that there is still a gap in resources for training and evaluating general reasoning capabilities over natural language."
  },
  {
    "index": 482,
    "source_corpus_id": 252683429,
    "ref_id": "b0",
    "citation_corpus_id": 245334722,
    "start": 3939,
    "end": 3958,
    "prev": "Instead, we replace this step by performing a single forward pass with a large Transformer that has been pre-trained to solve artificially generated classification tasks from a tabular dataset prior.",
    "curr": "Our method builds on Prior-Data Fitted Networks (PFNs; Müller et al., 2022; see Section 2), which learn the training and prediction algorithm itself.",
    "next": "PFNs approximate Bayesian inference given any prior one can sample from and approximate the posterior predictive distribution (PPD) directly."
  },
  {
    "index": 483,
    "source_corpus_id": 53094405,
    "ref_id": "b15",
    "citation_corpus_id": 11445252,
    "start": 8928,
    "end": 8952,
    "prev": "Repertoire is mostly classical, including composers from the 17 th to early 20 th century.",
    "curr": "MusicNet (Thickstun et al., 2017) contains recordings of human performances, but separatelysourced scores.",
    "next": "As discussed in , the alignment between audio and score is not fully accurate."
  },
  {
    "index": 485,
    "source_corpus_id": 252693111,
    "ref_id": "b26",
    "citation_corpus_id": 6628106,
    "start": 17840,
    "end": 17858,
    "prev": "We resize the training batch to 32 for image SR, and 8 for image denoising and JPEG CAR in order to make a fair comparison.",
    "curr": "We choose ADAM Kingma & Ba (2015) to optimize our ART model with β 1 = 0.9, β 2 = 0.999, and zero weight decay.",
    "next": "The initial learning rate is set as 2×10 −4 and is reduced by half as the training iteration reaches a certain number."
  },
  {
    "index": 490,
    "source_corpus_id": 254044229,
    "ref_id": "b14",
    "citation_corpus_id": 225039882,
    "start": 3225,
    "end": 3251,
    "prev": "Recently, Saunshi et al.",
    "curr": "(2022) argue that the pre-training loss is not the only contributor to the performance of self-supervised learning, and that previous works which view neural networks as a black box cannot tell apart the differences in downstream performance between architectures (e.g., ResNet (He et al., 2015) vs vision transformers (Dosovitskiy et al., 2020)).",
    "next": "Furthermore, selfsupervised learning with an appropriate architecture can possibly work under more general condi-tions and/or with fewer pre-training data than predicted by these results on general architecture."
  },
  {
    "index": 494,
    "source_corpus_id": 231648391,
    "ref_id": "b11",
    "citation_corpus_id": 13123084,
    "start": 8985,
    "end": 9005,
    "prev": "Another technique is a consistency regularization which states that realistic perturbations of input examples from unlabeled dataset should not significantly change the output of a neural network.",
    "curr": "Consistency regularization is used in Π-model (Laine & Aila, 2017) and further improved by Temporal Ensembling (Laine & Aila, 2017) which maintains an exponential moving average prediction for each training example and Mean Teacher (Tarvainen & Valpola, 2017) that averages model weights instead of model predictions.",
    "next": "Recent methods UDA (Xie et al., 2019), ReMixMatch  and FixMatch ) use a combination of consistency loss, pseudo-labeling and advanced augmentation techniques in addition to color perturbations and spatial transformations."
  },
  {
    "index": 497,
    "source_corpus_id": 252683376,
    "ref_id": "b0",
    "citation_corpus_id": 49310753,
    "start": 24596,
    "end": 24621,
    "prev": "(Ahmed et al., 2018), and stronger robustness (Eysenbach & Levine, 2022).",
    "curr": "Generally, policies optimizing the MaxEnt RL objective sample actions that are proportional to the exponentiated reward, and alternatively can be viewed as a noise injection procedure for better exploration (Attias, 2003;Ziebart, 2010;Haarnoja et al., 2017;Nachum et al., 2017;Levine, 2018;Abdolmaleki et al., 2018;Haarnoja et al., 2018b;Vieillard et al., 2020b;Pan et al., 2022;2023;Lahlou et al., 2023).",
    "next": "However, this noise injection is commonly done directly in action space, leading to only local perturbations, whereas we inject noise through a nonlinear mapping."
  },
  {
    "index": 498,
    "source_corpus_id": 15816492,
    "ref_id": "b21",
    "citation_corpus_id": 12639289,
    "start": 1364,
    "end": 1394,
    "prev": "Experimental results on abstractive sentence summarisation, morphological inflection, and machine translation show that noisy channel models outperform direct models, and that they significantly benefit from increased amounts of unpaired output data that direct models cannot easily use.",
    "curr": "INTRODUCTION\n\nRecurrent neural network sequence to sequence models (Kalchbrenner & Blunsom, 2013;Sutskever et al., 2014;Bahdanau et al., 2015) are excellent models of p(output sequence y | input sequence x), provided sufficient input-output (x, y) pairs are available for estimating their parameters.",
    "next": "However, in many domains, vastly more unpaired output examples are available than input-output pairs (e.g., transcribed speech is relatively rare although non-spoken texts are abundant; Swahili-English translations are rare although English texts are abundant; etc.)."
  },
  {
    "index": 499,
    "source_corpus_id": 265038424,
    "ref_id": "b14",
    "citation_corpus_id": 3366315,
    "start": 2403,
    "end": 2423,
    "prev": "The original GAN is, however, highly unstable and often suffers from mode collapse.",
    "curr": "Much of recent researches has focused on improving the stability of GANs (Radford et al., 2015;Heusel et al., 2017;Miyato et al., 2018;Karras et al., 2018).",
    "next": "On the theoretical aspect, Nagarajan & Kolter (2017) proved that gradient based training of the original GAN is locally stable."
  },
  {
    "index": 500,
    "source_corpus_id": 220525352,
    "ref_id": "b5",
    "citation_corpus_id": 28202810,
    "start": 2288,
    "end": 2290,
    "prev": "The success of various off-policy RL algorithms largely attributes to the use of experience replay [5-7, 15, 16].",
    "curr": "However, most off-policy RL algorithms usually adopt a random sampling [5,6,16], which treats all past experiences equally.",
    "next": "It is questionable whether this simple strategy would always choose the most effective samples for training."
  },
  {
    "index": 502,
    "source_corpus_id": 261214750,
    "ref_id": "b2",
    "citation_corpus_id": 215416146,
    "start": 2521,
    "end": 2524,
    "prev": "The information of millions of academic papers can not be fully accessed because they are locked behind an unreadable format.",
    "curr": "Existing corpora, such as the S2ORC dataset [3], capture the text of 12M 2 papers using GROBID [4], but are missing meaningful representations of the mathematical equations.",
    "next": "To this end, we introduce Nougat, a transformer based model that can convert images of document pages to formatted markup text."
  },
  {
    "index": 503,
    "source_corpus_id": 247594694,
    "ref_id": "b6",
    "citation_corpus_id": 222272028,
    "start": 29905,
    "end": 29928,
    "prev": "More implementation details are in Appendix C.4.",
    "curr": "Moreover, we also conduct the visualization of relative loss change g over a linear path like (Mirzadeh et al., 2020) in Appendix C.4.",
    "next": "The loss change of SSL is slower than that of SL along the linear interpolation path, demonstrating the flatter minima of SSL."
  },
  {
    "index": 505,
    "source_corpus_id": 256105083,
    "ref_id": "b23",
    "citation_corpus_id": 218487034,
    "start": 1176,
    "end": 1197,
    "prev": "Introduction\n\nLarge language models, often trained with billions of parameters, have achieved impressive performance in recent years (Raffel et al., 2019) and are used in a wide variety of natural language generation tasks.",
    "curr": "However, their output is sometimes undesirable, with hallucinated content (Maynez et al., 2020;Filippova, 2020), and much work remains to fully understand their properties.",
    "next": "In many applications, such as healthcare, question-answering systems, or customer service, incorrect predictions are particularly costly and must be avoided."
  },
  {
    "index": 506,
    "source_corpus_id": 260316137,
    "ref_id": "b16",
    "citation_corpus_id": 53112107,
    "start": 11078,
    "end": 11099,
    "prev": "They also argued that momentum helps generalization from the lens of stability analysis for finite number of iterates, when the loss function is Lipschitz.",
    "curr": "Under the formulation of quasi-hyperbolic momentum [Ma and Yarats, 2019], Gitman et al.",
    "next": "[2019] proposed another unified analysis for momentum methods, in terms of the asymptotic and local convergence as well as the stability."
  },
  {
    "index": 507,
    "source_corpus_id": 102481167,
    "ref_id": "b3",
    "citation_corpus_id": 1844940,
    "start": 1790,
    "end": 1807,
    "prev": "INTRODUCTION\n\nAdvances in machine learning and the availability of large corpora of source code have led to growing interest in the development of neural representations of programs for performing program analyses.",
    "curr": "In particular, different representations based on token sequences (Gupta et al., 2017;Bhatia et al., 2018), program parse trees (Piech et al., 2015;Mou et al., 2016), program traces (Reed & de Freitas, 2015;Cai et al., 2017;Wang et al., 2018), and graphs (Allamanis et al., 2018) have been proposed for a variety of tasks including repair (Devlin et al., 2017b;Allamanis et al., 2018), optimization (Bunel et al., 2017), and synthesis (Parisotto et al., 2017;Devlin et al., 2017a).",
    "next": "In recent work, Allamanis et al."
  },
  {
    "index": 510,
    "source_corpus_id": 202750253,
    "ref_id": "b28",
    "citation_corpus_id": 53015027,
    "start": 7048,
    "end": 7051,
    "prev": "Due to its significant importance in wide range of application areas, its theoretical analysis is also getting much important.",
    "curr": "For example, it has been known that the deep neural network has universal approximation capability [13,24,44] and its expressive power grows up in an exponential order against the number of layers [37,8,12,11,42,47].",
    "next": "However, theoretical understandings are still lacking in several important issues."
  },
  {
    "index": 512,
    "source_corpus_id": 247613305,
    "ref_id": "b52",
    "citation_corpus_id": 216562627,
    "start": 7606,
    "end": 7626,
    "prev": "Data augmentation for RL.",
    "curr": "In the context of RL, data augmentation has been widely investigated for improving data-efficiency Yarats et al., 2021), or RL generalization (Cobbe et al., 2019;Lee et al., 2019).",
    "next": "For example, RAD  demonstrated that data augmentation, such as random crop, can improve both data-efficiency and generalization of RL algorithms."
  },
  {
    "index": 515,
    "source_corpus_id": 231879989,
    "ref_id": "b40",
    "citation_corpus_id": 231933751,
    "start": 8827,
    "end": 8844,
    "prev": "Conditional Computation.",
    "curr": "Many conditional computation methods have been recently proposed with the goal of improving computational efficiency (Bengio et al., 2015;Veit & Belongie, 2018;Wang et al., 2018b;Graves, 2016;Meng et al., 2020;Pan et al., 2021).",
    "next": "Several works have been Figure 1: A conceptual view for adaptive temporal fusion."
  },
  {
    "index": 520,
    "source_corpus_id": 245502568,
    "ref_id": "b22",
    "citation_corpus_id": 231807280,
    "start": 3252,
    "end": 3259,
    "prev": "In particular, Byrd and Lipton [BL19] empirically showed that when no regularization is used, overparameterized linear and nonlinear models trained with the importance weighted cross-entropy loss ignore the importance weights.",
    "curr": "Xu, Ye, and Ruan [XYR20] followed up and provided a theoretical justification for this observation in overparameterized linear and non-linear models.",
    "next": "To build intuition about why importance weighting fails, consider linear classifiers as an example."
  },
  {
    "index": 522,
    "source_corpus_id": 249431648,
    "ref_id": "b61",
    "citation_corpus_id": 6212000,
    "start": 2505,
    "end": 2509,
    "prev": "While significant efforts have been devoted in understanding and correcting biases in classical models such as logistic regressions and supported vector machines (SVM), see, e.g., [1,29], those derived tools are far less effective on modern over-parameterized models such as neural networks (NN).",
    "curr": "Over-parameterization can lead to poor generalization, as extensive efforts in both theoretical and empirical studies have exhibited ( [62,50,4]).",
    "next": "Furthermore, in large models, it is also difficult for measures of fairness (such as equalized odds to be introduced shortly) to generalize, as shown in Fig."
  },
  {
    "index": 523,
    "source_corpus_id": 1463401,
    "ref_id": "b5",
    "citation_corpus_id": 5590763,
    "start": 2354,
    "end": 2371,
    "prev": "The remarkable recent successes of the deep convolutional neural networks are particularly based on this ability to learn hierarchical representation for spatial data (Krizhevsky et al., 2012).",
    "curr": "For modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances (Mikolov et al., 2010;Graves, 2013;Cho et al., 2014;Vinyals et al., 2015).",
    "next": "However, unlike the spatial data, learning both hierarchical and temporal representation has been among the long-standing challenges of RNNs in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data (Schmidhuber, 1991;Mozer, 1993;El Hihi & Bengio, 1995;Lin et al., 1996;Koutník et al., 2014)."
  },
  {
    "index": 524,
    "source_corpus_id": 3535069,
    "ref_id": "b3",
    "citation_corpus_id": 19435386,
    "start": 3234,
    "end": 3236,
    "prev": "Machine understanding of this structured visual information could assist human analysts in extracting knowledge from the vast documentation produced by modern science.",
    "curr": "Thus motivated, and inspired by recent research in Visual Question Answering (VQA) [1,2] and relational reasoning [3,4], we introduce FigureQA.",
    "next": "FigureQA is a corpus of over one million question-answer pairs grounded in over 100, 000 figures, devised to study aspects of comprehension and reasoning in machines."
  },
  {
    "index": 525,
    "source_corpus_id": 3703428,
    "ref_id": "b22",
    "citation_corpus_id": 6628106,
    "start": 15185,
    "end": 15203,
    "prev": "We even show that the use of particular representation learning algorithms such as VAEs in the IMGEP-UGL architecture can produce exploration dynamics that match the one using engineered representations.",
    "curr": "Secondary contributions of this article:\n\n• We show that the IMGEP-UGL architecture can be successfully implemented (in terms of exploration efficiency) using various unsupervised learning algorithms for the goal space learning component: AutoEncoders (AEs) (Bourlard & Kamp, 1988), Variational AE (VAE) (Rezende et al., 2014;Kingma & Ba, 2015), VAE with Normalizing Flow (Rezende & Mohamed, 2015), Isomap (Tenenbaum et al., 2000), PCA (Pearson, 1901), and we quantitatively compare their performances in terms of exploration dynamics of the associated IMGEP-UGL architecture.",
    "next": "• We show that specifying more embedding dimensions than needed to capture the phenomenon manifold does not deteriorate the performance of these unsupervised learning algorithms."
  },
  {
    "index": 526,
    "source_corpus_id": 257255036,
    "ref_id": "b51",
    "citation_corpus_id": 227209335,
    "start": 2032,
    "end": 2052,
    "prev": "INTRODUCTION\n\nDiffusion probabilistic modeling has quickly become a central approach for learning data distributions, obtaining impressive empirical results across multiple domains like images (Nichol & Dhariwal, 2021), videos (Ho et al., 2022) or even 3D geometry (Luo & Hu, 2021).",
    "curr": "In particular, Denoising Diffusion Probabilistic Models (often referred to as DDPMs or diffusion generative models) (Ho et al., 2020;Nichol & Dhariwal, 2021) and their continuous-time extension (Song et al., 2021b) both present a training objective that is more stable than precursors like generative adversarial nets (GANs) (Goodfellow et al., 2014) or energy-based models (EBMs) (Du et al., 2020).",
    "next": "In addition, diffusion generative models have shown to empirically outperform GANs in the image domain (Dhariwal & Nichol, 2021) and to suffer less from mode-seeking pathologies during training (Kodali et al., 2017)."
  },
  {
    "index": 527,
    "source_corpus_id": 7902640,
    "ref_id": "b21",
    "citation_corpus_id": 16299141,
    "start": 15676,
    "end": 15698,
    "prev": "For machine translation, we use the extended DSL and construct candidate architectures incrementally using the RL generator without a ranking function.",
    "curr": "LANGUAGE MODELING USING RANDOM SEARCH WITH A RANKING FUNCTION\n\nFor evaluating architectures found during architecture search, we use the WikiText-2 dataset (Merity et al., 2017b).",
    "next": "When evaluating a proposed novel RNN cell c, we construct a two layer c-RNN with a 200 unit hidden size."
  },
  {
    "index": 528,
    "source_corpus_id": 263909107,
    "ref_id": "b66",
    "citation_corpus_id": 3693334,
    "start": 1863,
    "end": 1880,
    "prev": " analyze the connection between posterior quality and uncertainty quantification, delve into the impact of modes on the posterior, and explore methods for visualizing the posterior.Moreover, we uncover weight-space symmetries as a critical aspect for understanding the posterior.To this extent, we develop an in-depth assessment of the impact of both permutation and scaling symmetries that tend to obfuscate the Bayesian posterior.While the first type of transformation is known for duplicating modes, we explore the relationship between the latter and L2 regularization, challenging previous misconceptions.Finally, to help the community improve our understanding of the Bayesian posterior, we will release shortly the first large-scale checkpoint dataset, including thousands of real-world models, along with our codes.",
    "curr": "INTRODUCTION\n\nDespite substantial advancements in deep learning, Deep Neural Networks (DNNs) remain black box models.Various studies have sought to explore DNN loss landscapes (Li et al., 2018;Fort & Jastrzebski, 2019;Fort & Scherlis, 2019;Liu et al., 2022) to achieve a deeper understanding of these models.Recent works have, for instance, unveiled the interconnection of the modes obtained with Stochastic Gradient Descent (SGD) via narrow pathways that link pairs of modes, or through tunnels that connect multiple modes simultaneously (Garipov et al., 2018;Draxler et al., 2018).This mode connectivity primarily arises from scale and permutation invariances, which imply that numerous weights can represent the same exact function (e.g., Entezari et al.",
    "next": "(2022)).Several studies have delved into the relationship between these symmetries and the characteristics of the loss landscape (Entezari et al., 2022;Neyshabur et al., 2015;Brea et al., 2019).Our work investigates the connections between these symmetries and the distribution of DNN weights, a crucial aspect for uncertainty quantification.As shown in Figure 1, it is apparent that these symmetries also exert influence on DNN poster"
  },
  {
    "index": 531,
    "source_corpus_id": 209316097,
    "ref_id": "b1",
    "citation_corpus_id": 7771402,
    "start": 2049,
    "end": 2073,
    "prev": "So far, most of the existing methods focus on building statistical associations between textual inputs and semantic representations, e.g.",
    "curr": "using first-order logic (Manning et al., 1999) or other types of representations such as abstract meaning representation (Banarescu et al., 2013).",
    "next": "Recently, grounded language learning has gradually attracted attention in various domains, inspired by the hypothesis that early language learning was focused on problemsolving (Kirby & Hurford, 2002)."
  },
  {
    "index": 532,
    "source_corpus_id": 222133372,
    "ref_id": "b50",
    "citation_corpus_id": 3366315,
    "start": 16630,
    "end": 16634,
    "prev": "Compared with the conditions stated in Section 2 (namely, that it is locally homogeneous with a symmetric Jacobian), our requirement on the denoiser is milder.",
    "curr": "One can train a nonexpansive D σ by constraining the Lipschitz constant of D σ via the spectral normalization, which is an active area of research in deep learning [51][52][53].",
    "next": "We can now state the theorems on ASYNC-RED."
  },
  {
    "index": 534,
    "source_corpus_id": 240419913,
    "ref_id": "b47",
    "citation_corpus_id": 56475856,
    "start": 2418,
    "end": 2421,
    "prev": "While plenty of other work in model-based RL does measure generalization [e.g.",
    "curr": "18,48,70,76], each approach is typically evaluated on a bespoke task, making it difficult to ascertain the state of generalization in model-based RL more broadly.",
    "next": "Model-free RL, like model-based RL, has also suffered from both the \"train=test\" paradigm and a lack of standardization around how to measure generalization."
  },
  {
    "index": 536,
    "source_corpus_id": 262825568,
    "ref_id": "b48",
    "citation_corpus_id": 252715691,
    "start": 1769,
    "end": 1787,
    "prev": "-rank adaptation (QA-LoRA) algorithm.The motivation lies in the imbalanced degrees of freedom of quantization and adaptation, and the solution is to use group-wise operators which increase the degree of freedom of quantization meanwhile decreasing that of adaptation.QA-LoRA is easily implemented with a few lines of code, and it equips the original LoRA with two-fold abilities: (i) during fine-tuning, the LLM's weights are quantized (e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the LLM and auxiliary weights are naturally integrated into a quantized model without loss of accuracy.We apply QA-LoRA to the LLaMA and LLaMA2 model families and validate its effectiveness in different fine-tuning datasets and downstream scenarios.Code will be made available at https://github.com/yuhuixu1993/qa-lora.",
    "curr": "INTRODUCTION\n\nRecently, large language models (LLMs) (Brown et al., 2020;Scao et al., 2022;Zhang et al., 2022;Touvron et al., 2023a;Chowdhery et al., 2022;OpenAI, 2023;Zeng et al., 2023) have shown unprecedented performance across a wide range of language understanding tasks (Wei et al., 2022a) and served as the foundation of state-of-the-art chat systems (Bubeck et al., 2023).The diversity of real-world applications calls for a pipeline in which LLMs can be fine-tuned to fit different scenarios and quantized to be deployed onto edge devices (e.g., mobile phones), and the key issue is to get rid of the heavy computational burden brought by the large number of parameters of LLMs.",
    "next": "There are two lines of research for this purpose.The first one is parameter-efficient fine-tuning (PEFT) (Houlsby et al., 2019;Li & Liang, 2021;Liu et al., 2021;He et al., 2022;Hu et al., 2021) which introduced a small number of learnable parameters while keeping most pre-trained parameters unchanged.Among them, low-rank adaptation (LoRA) (Hu et al., 2021), a popular PEFT algorithm, proposed to fine-tune low-rank matrices to complement the pre-trained weights.Despite the comparable performanc"
  },
  {
    "index": 537,
    "source_corpus_id": 210164865,
    "ref_id": "b48",
    "citation_corpus_id": 26238954,
    "start": 3616,
    "end": 3635,
    "prev": "AR models based on recurrent (G)NNs can be viewed as special instantiations of SSMs in which the state transitions are restricted to being deterministic (Fraccaro, 2018, Section 4.2).",
    "curr": "Despite their simplicity, it has been pointed out that their modeling capability is bottlenecked by the deterministic state transitions (Chung et al., 2015;Fraccaro et al., 2016) and the oversimplified observation distributions (Yang et al., 2018).",
    "next": "In this study, we make the following contributions: (i) We propose the relational state-space model (R-SSM), a novel hierarchical deep SSM that simulates the stochastic state transitions of interacting objects with GNNs, extending GNN-based dynamics modeling to challenging stochastic multi-object systems."
  },
  {
    "index": 538,
    "source_corpus_id": 252715691,
    "ref_id": "b9",
    "citation_corpus_id": 237416585,
    "start": 11827,
    "end": 11845,
    "prev": "To improve FFNs in Transformer, we pick GLU with the GeLU (Hendrycks & Gimpel, 2016) activation as the replacement.",
    "curr": "2.2 GLM-130B'S PRE-TRAINING SETUP Inspired by recent works (Aribandi et al., 2022;Wei et al., 2022a;Sanh et al., 2022), the GLM-130B pre-training objective includes not only the self-supervised GLM autoregressive blank infilling) but also multi-task learning for a small portion of tokens.",
    "next": "This is expected to help boost its downstream zero-shot performance."
  },
  {
    "index": 541,
    "source_corpus_id": 52892477,
    "ref_id": "b1",
    "citation_corpus_id": 900029,
    "start": 1151,
    "end": 1172,
    "prev": "On the WIKITEXT-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the BILLION WORD benchmark, we achieve 23.02 perplexity.",
    "curr": "1\n\nINTRODUCTION\n\nLanguage modeling is a basic task in natural language processing, with many applications such as speech recognition (Arisoy et al., 2012) and statistical machine translation (Schwenk et al., 2012;Vaswani et al., 2013;Baltescu & Blunsom, 2015).",
    "next": "Recently, much progress has been made by neural methods (Bengio et al., 2003;Mikolov et al., 2010) based on LSTMs (Józefowicz et al., 2016), gated convolutional networks (Dauphin et al., 2017) and self-attentional networks (Al-Rfou et al., 2018)."
  },
  {
    "index": 543,
    "source_corpus_id": 247596648,
    "ref_id": "b24",
    "citation_corpus_id": 3144218,
    "start": 8028,
    "end": 8049,
    "prev": "For generalizing across parametric variations of a single morphology, various approaches have been proposed like using a learned hardware embedding (Chen et al., 2018), meta-learning for policy adaptation (Al-Shedivat et al., 2017;Ghadirzadeh et al., 2021), kinematics randomization (Exarchos et al., 2020), and dynamics randomization (Peng et al., 2018).",
    "curr": "In case of multiple different morphologies, one approach to tackle the challenge of differences in action and state spaces is to leverage Graph Neural Networks (Scarselli et al., 2008;Kipf & Welling, 2017;.",
    "next": "Wang et al."
  },
  {
    "index": 546,
    "source_corpus_id": 219530969,
    "ref_id": "b2",
    "citation_corpus_id": 3517962,
    "start": 2457,
    "end": 2460,
    "prev": "Under review.",
    "curr": "Introduction\n\nReinforcement learning (RL) algorithms have recently demonstrated impressive success in learning behaviors for a variety of sequential decision-making tasks [3,24,42].",
    "next": "Virtually all of these demonstrations have relied on highly-frequent online access to the environment, with the RL algorithms often interleaving each update to the policy with additional experience collection of that policy acting in the environment."
  },
  {
    "index": 548,
    "source_corpus_id": 259096096,
    "ref_id": "b37",
    "citation_corpus_id": 213695995,
    "start": 2481,
    "end": 2484,
    "prev": "Introduction\n\nA core research question in multimodal learning is to understand the nature of multimodal interactions across modalities in the context of a task: the emergence of new task-relevant information during learning from both modalities that was not present in either modality alone [6,65].",
    "curr": "In settings where labeled multimodal data is abundant, the study of multimodal interactions has inspired advances in theoretical analysis [1,43,66,84,94] and representation learning [51, 76,91,104] in language and vision [2], multimedia [9], healthcare [53], and robotics [57].",
    "next": "In this paper, we study the problem of interaction quantification in a setting where there is only unlabeled multimodal data D M = {(x 1 , x 2 )} but some labeled unimodal data D i = {(x i , y)} collected separately for each modality."
  },
  {
    "index": 549,
    "source_corpus_id": 248377437,
    "ref_id": "b12",
    "citation_corpus_id": 28202810,
    "start": 22619,
    "end": 22642,
    "prev": "2, the learner policy is trained with a MaxEnt RL algorithm with per-timestep entropy-regularized reward given by f ω (s t , a t ) − α log π(a t |s t ), where α is the entropy coefficient.",
    "curr": "In our experiments, we use the clipped-ratio PPO algorithm (Schulman et al., 2017) and adaptively tune α as suggested in prior work (Haarnoja et al., 2018).",
    "next": "RELATED WORK\n\nThere is a vast amount of literature on IL since it is a powerful framework to train agents to perform complex behaviors without a reward specification."
  },
  {
    "index": 551,
    "source_corpus_id": 227054461,
    "ref_id": "b9",
    "citation_corpus_id": 8968704,
    "start": 2176,
    "end": 2192,
    "prev": "Sketching is often used during brainstorming to help the creative process, and is a popular creative activity in itself.",
    "curr": "Sketch-related AI so far has primarily focused on mimicking the human ability to perceive rich visual information from simple line drawings (Yu et al., 2015;Li et al., 2018) and to generate minimal depictions that capture the salient aspects of our visual world (Ha & Eck, 2018;Isola et al., 2017).",
    "next": "Most existing datasets contain sketches drawn by humans to realistically mimic common objects (Eitz et al., 2012;Sangkloy et al., 2016;Jongejan et al., 2016;Wang et al., 2019)."
  },
  {
    "index": 552,
    "source_corpus_id": 220713471,
    "ref_id": "b6",
    "citation_corpus_id": 53018855,
    "start": 5740,
    "end": 5743,
    "prev": "In contrast, we rely on private GAN to reduce the support to only contain the set of privately generated samples, which makes PGB tractable even for high-dimensional data.",
    "curr": "We also provide an extension of the PGB method by incorporating the technique of discriminator rejection sampling [7,35].",
    "next": "We leverage the fact that the distinguisher's equilibrium strategy, which is a mixture of discriminators, can often accurately predict which samples are unlikely and thus can be used as a rejection sampler."
  },
  {
    "index": 557,
    "source_corpus_id": 263620293,
    "ref_id": "b65",
    "citation_corpus_id": 3352260,
    "start": 5477,
    "end": 5499,
    "prev": "Via our detailed ablation study, we construct ChemRLformer and find that pretraining on aligned datasets can significantly improve performance across all molecular design tasks, even exceeding the performance of agents pretrained on 100 times larger datasets.We also show that targeted algorithmic design, such as hill-climbing in the replay buffer and regularization, further increases the performance of ChemRLformer.To the best of our knowledge, ChemRLformer is the largest analysis of text-based RL methods for molecule discovery.",
    "curr": "[Yu et al., 2018, Schaff et al., 2019].Other methods have explicitly included the design process in the RL loop by training design problems together [Chen et al., 2021a, Ha, 2019, Luck et al., 2020, Kumar et al., 2022] with most prior work focusing on robot and agent design, not molecular design.Our molecular design work creates an autoregressive structure that grows the size of the state as the agent acts in the environment.",
    "next": "Molecular Discovery Using Sequence-Based Methods: Sequence-based methods treat molecular design as a sequence of tokens that get concatenated in order.Generative models for sequence-based methods span a diverse range, including variational autoencoders (VAEs) [Gómez-Bombarelli et al., 2018, Alperstein et al., 2019], recurrent neural networks (RNNs) [Gupta et al., 2018, Bjerrum and Threlfall, 2017, Grisoni et al., 2020, Flam-Shepherd et al., 2022] and transformer models [Wang et al., 2019, Fabian et al., 2020, Edwards et al., 2022a, Zeng et al., 2022, Taylor et al., 2022].The general procedure for all"
  },
  {
    "index": 558,
    "source_corpus_id": 240354066,
    "ref_id": "b12",
    "citation_corpus_id": 219965819,
    "start": 2855,
    "end": 2858,
    "prev": "Since LRDs are perhaps the foremost challenge for sequence models, all standard model families such as continuous-time models (CTMs), RNNs, CNNs, and Transformers include many specialized variants designed to address them.",
    "curr": "Modern examples include orthogonal and Lipschitz RNNs [1,13] to combat vanishing gradients, dilated convolutions to increase context size [3,28], and an increasingly vast family of efficient Transformers that reduce the quadratic dependence on sequence length [8,22].",
    "next": "Despite being designed for LRDs, these solutions still perform poorly on challenging benchmarks such as LRA [40] or raw audio classification [18]."
  },
  {
    "index": 559,
    "source_corpus_id": 108306725,
    "ref_id": "b9",
    "citation_corpus_id": 6715185,
    "start": 35139,
    "end": 35165,
    "prev": "Neural Turing Machine (NTM) (Graves et al., 2014; enables general-purpose neural problem solving such as sorting by introducing an external memory that mimics the execution of Turing Machine.",
    "curr": "Neural program induction and synthesis (Neelakantan et al., 2016;Reed & De Freitas, 2016;Kaiser & Sutskever, 2016;Parisotto et al., 2017;Devlin et al., 2017;Bunel et al., 2018;Sun et al., 2018) are recently introduced to solve problems by synthesizing computer programs with neural augmentations.",
    "next": "Some works tackle the issue of the systematical generalization by introducing extra supervision (Cai et al., 2017)."
  },
  {
    "index": 560,
    "source_corpus_id": 257039090,
    "ref_id": "b12",
    "citation_corpus_id": 173990158,
    "start": 7470,
    "end": 7488,
    "prev": "These datasets enable us to train a multimodal model y = f (x 1 , x 2 ; θ) which we are interested in visualizing.",
    "curr": "Modern parameterizations of multimodal models f are typically black-box neural networks, such as multimodal transformers (Hendricks et al., 2021;Tsai et al., 2019) and pretrained models (Li et al., 2019;Lu et al., 2019).",
    "next": "How can we visualize and understand the internal modeling of multimodal information and interactions in these models?"
  },
  {
    "index": 561,
    "source_corpus_id": 52055130,
    "ref_id": "b2",
    "citation_corpus_id": 18828233,
    "start": 41405,
    "end": 41429,
    "prev": "(13)\nSome intuition might help understand why this might work.",
    "curr": "As discussed in Arjovsky & Bottou (2017), if P and Q θ have support on a low-dimensional manifolds on X , unless they are perfectly aligned, there exists a discriminator that will be able to perfectly distinguish between samples coming from P and Q θ , which means that U ψ,P and U ψ,Q must also be disjoint.",
    "next": "However, to train the generator, U ψ,P and U ψ,Q need to share support on Y in order to ensure stable and non-zero gradients for the generator."
  },
  {
    "index": 562,
    "source_corpus_id": 263608118,
    "ref_id": "b24",
    "citation_corpus_id": 253098274,
    "start": 2169,
    "end": 2187,
    "prev": "es the attention matrix with linear complexity via kernel-based linear attention, then creates a sparse approximation to the full attention matrix with a top-k selection to perform a sparse attention operation.For language modeling tasks (Wikitext2), previous linear and sparse attention methods show a roughly two-fold worse perplexity scores over the quadratic OPT-125M baseline, while SEA achieves an even better perplexity than OPT-125M, using roughly half as much memory as OPT-125M.Moreover, SEA maintains an interpretable attention matrix and can utilize knowledge distillation to lower the complexity of existing pretrained transformers.We believe that our work will have a large practical impact, as it opens the possibility of running large transformers on resource-limited devices with less memory.",
    "curr": "INTRODUCTION\n\nThe transformer (Vaswani et al., 2017) architecture has revolutionized various fields of artificial intelligence, such as natural language understanding (Touvron et al., 2023;Wang et al., 2022) and computer vision (Dosovitskiy et al., 2021) due to its ability to learn pairwise relationships between all T tokens in a given sequence (O(T 2 )).This has ushered in the era of large transformer-based foundation models with impressive generalization abilities (Brown et al., 2020;Chiang et al., 2023).However, since the transformer's attention mechanism comes with a quadratic space and time complexity, it becomes untenable for handling long sequences which is essential for tasks such as dialogue generation (Chen et al., 2023).To overcome this limitation, previous works have suggested approaches with linear complexity by using static or dynamic sparse attention patterns (Beltagy et al., 2020;Zaheer et al., 2020;Tay et al., 2020a;Kitaev et al., 2020;Tay et al., 2020b;Liu et al., 2021), or by replacing quadratic attention with kernel or low-rank approximations (Choromanski et al., 2021;Chen et al., 2021;Qin et al., 2022).",
    "next": "However, despite their promising aspects, previous linear attent"
  },
  {
    "index": 564,
    "source_corpus_id": 260887189,
    "ref_id": "b7",
    "citation_corpus_id": 258461287,
    "start": 3078,
    "end": 3097,
    "prev": "1\n\nINTRODUCTION\n\nThe emergence of Large Language Models (LLMs) has played a pivotal role in driving the advancement of Artificial Intelligence (AI) systems.",
    "curr": "Noteworthy LLMs like ChatGPT (OpenAI, 2023a;b), Claude2 (Anthropic, 2023), Bard (Google, 2023), and Llama2 (Touvron et al., 2023a) have demonstrated their advanced capability to perform innovative applications, ranging from tool utilization, supplementing human evaluations, to stimulating human interactive behaviors Bubeck et al., 2023;Schick et al., 2023;Chiang & Lee, 2023;Park et al., 2023).",
    "next": "The outstanding competencies have fueled their widespread deployment, while the progression is shadowed by a significant challenge: ensuring the safety and reliability of the responses."
  },
  {
    "index": 565,
    "source_corpus_id": 51969884,
    "ref_id": "b4",
    "citation_corpus_id": 5590763,
    "start": 28291,
    "end": 28309,
    "prev": "−E(y, h; x) = f i xiw x if j yjw y jf k h k w h kf + k w h k h k + j w y j yj = x T W x • y T W y • h T W h 1 + h T w h + y T w y(21)\nSetting aside of bias terms, the I × J × K parameter tensor of unfactored Higher-Order Boltzmann Machines is replaced with three matrices, W x ∈ R I×F , W y ∈ R J×F , and W h ∈ R K×F .",
    "curr": "D.3 MULTIPLICATIVE INTEGRATION WITH RECURRENT NEURAL NETWORKS\n\nMost of recurrent neural networks, including vanilla RNNs, Long Short Term Memory networks (Hochreiter & Schmidhuber, 1997) and Gated Recurrent Units (Cho et al., 2014), share a common expression as follows:\nφ(Wx + Uh + b)(22)\nwhere φ is a non-linear function, W ∈ R d×n , x ∈ R n , U ∈ R d×m , h ∈ R m , and b ∈ R d is a bias vector.",
    "next": "Note that, usually, x is an input state vector and h is an hidden state vector in recurrent neural networks."
  },
  {
    "index": 566,
    "source_corpus_id": 256846836,
    "ref_id": "b24",
    "citation_corpus_id": 222140788,
    "start": 6355,
    "end": 6375,
    "prev": "Different works also studied different approximations of the unknown q(z t−1 |z t , z 0 ) used to perform sampling.",
    "curr": "For example, denoising diffusion implicit model (DDIM) (Song et al., 2021a) first computed a predicted clean data point\nz 0 = z t − ( √ 1 − α t ) θ (z t , t) √ α t ,(3)\nand sample z t−1 from q(z t−1 |z t ,ẑ 0 ) by replacing unknown z 0 withẑ 0 .",
    "next": "On the other hand, while the details of individual sampling methods vary, all sampling methods produce z t−1 based on current sample z t , current time step t and a predicted noiseˆ ."
  },
  {
    "index": 568,
    "source_corpus_id": 264288947,
    "ref_id": "b27",
    "citation_corpus_id": 254877603,
    "start": 2411,
    "end": 2432,
    "prev": "1 1 Our code and trained models are available at https://selfrag.github.io/.",
    "curr": "INTRODUCTION\n\nState-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023;Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022).Retrieval-Augmented Generation (RAG) methods (Figure 1 left; Lewis et al.",
    "next": "2020;Guu et al."
  },
  {
    "index": 570,
    "source_corpus_id": 252873467,
    "ref_id": "b14",
    "citation_corpus_id": 233289412,
    "start": 3502,
    "end": 3523,
    "prev": "To that end, several knowledge-editing methods have been proposed to insert new memories directly into specific model parameters.",
    "curr": "The approaches include constrained fine-tuning (Zhu et al., 2020), hypernetwork knowledge editing (De Cao et al., 2021;Hase et al., 2021;Mitchell et al., 2021;, and rank-one model editing (Meng et al., 2022).",
    "next": "However, this body of work is typically limited to updating at most a few dozen facts; a recent study evaluates on a maximum of 75 (Mitchell et al., 2022) whereas others primarily focus on single-edit cases."
  },
  {
    "index": 573,
    "source_corpus_id": 71145737,
    "ref_id": "b17",
    "citation_corpus_id": 49868626,
    "start": 27345,
    "end": 27363,
    "prev": "That said, in the past two years, there have been several other methods proposed for few-shot learning.",
    "curr": "Some bear similarity with MAML and correspond to a meta-learner trained to quickly adapt its parameters to various novel tasks (Ravi & Larochelle, 2017;Munkhdalai & Yu, 2017;Rusu et al., 2019;Yoon et al., 2018).",
    "next": "Others relate to Prototypical Networks by learning a data representation as well as a compact representation for a classifier of data under that representation (Bertinetto et al., 2019;Gidaris & Komodakis, 2018;Oreshkin et al., 2018;Gidaris & Komodakis, 2018)."
  },
  {
    "index": 574,
    "source_corpus_id": 238634584,
    "ref_id": "b30",
    "citation_corpus_id": 211842237,
    "start": 5855,
    "end": 5879,
    "prev": "The Open Catalyst Project (OCP) provides a platform for comparing different architectures by predicting energies and forces from the periodic structure of catalytic surfaces (Chanussot et al., 2021).",
    "curr": "Our encoder and decoder PGNNs directly use GNN architectures developed for the OCP (Klicpera et al., 2020b;Godwin et al., 2021), which are also closely related to SE(3) equivariant networks (Thomas et al., 2018;Fuchs et al., 2020).",
    "next": "Quantum mechanical search of stable materials."
  },
  {
    "index": 576,
    "source_corpus_id": 3518190,
    "ref_id": "b0",
    "citation_corpus_id": 11212020,
    "start": 1453,
    "end": 1475,
    "prev": "We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores up to 32.8, without using even a single parallel sentence at training time.",
    "curr": "INTRODUCTION\n\nThanks to recent advances in deep learning (Sutskever et al., 2014;Bahdanau et al., 2015) and the availability of large-scale parallel corpora, machine translation has now reached impressive performance on several language pairs .",
    "next": "However, these models work very well only when provided with massive amounts of parallel data, in the order of millions of parallel sentences."
  },
  {
    "index": 577,
    "source_corpus_id": 239009574,
    "ref_id": "b22",
    "citation_corpus_id": 213085920,
    "start": 3232,
    "end": 3236,
    "prev": "Falling broadly into the second category, our paper studies self-supervised molecular representation learning by leveraging the consistency between 3D geometry and 2D topology.",
    "curr": "Motivated by the prominent success of the pretraining-finetuning pipeline [17], unsupervisedly pretrained graph neural networks for molecules yields promising performance on downstream tasks and becomes increasingly popular [42,54,82,90,103,104].",
    "next": "The key to pre-training lies in finding an effective proxy task (i.e., training objective) to leverage the power of large unlabeled datasets."
  },
  {
    "index": 578,
    "source_corpus_id": 241035330,
    "ref_id": "b11",
    "citation_corpus_id": 2381275,
    "start": 2402,
    "end": 2424,
    "prev": "Introduction\n\nLarge language models (LMs) such as GPT-3 (Brown et al., 2020, Lieber et al., 2021, Radford et al., 2019, Wang and Komatsuzaki, 2021 are pretrained on massive text corpora to predict the next word given previous words.",
    "curr": "They demonstrate the surprising ability to do in-context learning, where an LM \"learns\" to do a task simply by conditioning on a prompt containing input-output pairs, achieving SOTA results on LAMBADA (Paperno et al., 2016) and TriviaQA (Joshi et al., 2017) tasks (18% and 3% over previous SOTA (Brown et al., 2020)).",
    "next": "For example, consider the task of predicting nationalities from names."
  },
  {
    "index": 582,
    "source_corpus_id": 263834782,
    "ref_id": "b6",
    "citation_corpus_id": 52967399,
    "start": 2301,
    "end": 2322,
    "prev": "Henceforth, we will refer to this masked language model as the auxiliary model 1 .",
    "curr": "This pre-training task, known as replaced token detection (RTD), has shown great advantages in training and data efficiency compared to other pre-training tasks such as masked language modeling (MLM) (Devlin et al., 2019).",
    "next": "ELECTRAstyle pre-training and its variations have been increasingly popular in advancing natural language understanding capabilities (Meng et al., 2021;Chi et al., 2021;Meng et al., 2022;He et al., 2021;."
  },
  {
    "index": 585,
    "source_corpus_id": 238419211,
    "ref_id": "b19",
    "citation_corpus_id": 216867841,
    "start": 2364,
    "end": 2383,
    "prev": "s.Moreover, we provide a theoretically-principled test of the distinguishability of PLRM for unseen labels, along with a generalization bound.On both image and text classification tasks as well as an industrial advertising application, we demonstrate the advantages of PLRM by outperforming baselines by a margin of 2%-9%.",
    "curr": "INTRODUCTION\n\nOne of the greatest bottlenecks of using modern machine learning models is the need for substantial amounts of manually-labeled training data.In real-world applications, such manual annotations are typically time-consuming, labor-intensive and static.To reduce the efforts of annotation, researchers have proposed Weak Supervision (WS) frameworks (Ratner et al., 2016;2018;2019;Fu et al., 2020) for synthesizing labels from multiple weak supervision sources, e.g., heuristics, knowledge bases, or pre-trained classifiers.These frameworks have been widely applied on various machine learning tasks (Dunnmon et al., 2020;Fries et al., 2021;Safranchik et al., 2020;Lison et al., 2020;Zhou et al., 2020;Hooper et al., 2021;Zhan et al., 2019;Varma et al., 2019) and industrial data (Bach et al., 2019).Among them, data programming (Ratner et al., 2016), one representative example that generalizes many approaches in the literature, represents weak supervision sources as labeling functions (LFs) and synthesizes training labels using Probabilistic Graphical Model (PGM).",
    "next": "Given both the increasing popularity of WS and the general increase in open-source availability of machine learning models and tools, there is a rising tide of available supervision sources that WS frameworks and practitioners could potentially leverage, including pre-trained machine learning models or prediction APIs (Chen et al., 2020;d'Andrea & Mintz, 2019;Yao et al., 2017).However, existing WS frameworks only utilize weak supervision sources with the same label space as the target task.This incompatibility largely limits the scope of usable sources, necessitating manual effort from domain experts to pr"
  },
  {
    "index": 588,
    "source_corpus_id": 248887351,
    "ref_id": "b16",
    "citation_corpus_id": 9192723,
    "start": 13304,
    "end": 13323,
    "prev": "To this end, we evaluated LLMs from the Gopher family in a 5-shot 1 setting on a larger set of 50 tasks that touch on different aspects of logical reasoning and vary in terms of the number of reasoning steps required, presence or absence of negation, whether the relevant context information was provided, and whether the model is required to evaluate the accuracy of multiple choices or generate the answer among others.",
    "curr": "The additional tasks were collected from six sources: bAbI (Weston et al., 2015), BigBench (Ghazal et al., 2017), AAC (Betz et al., 2020), Jeopardy (Tunguz, 2019), Proof Writer (Tafjord et al., 2020) and 2WikiMultiHop (Welbl et al., 2018) (see Fig.",
    "next": "S5a in Supplementary Information for raw results)."
  },
  {
    "index": 590,
    "source_corpus_id": 263310823,
    "ref_id": "b3",
    "citation_corpus_id": 225039882,
    "start": 25972,
    "end": 25973,
    "prev": "t is located with respect to the rest of the training set.This allows us to answer the question: Which data record S has the most significant influence and at which query Q?",
    "curr": "Proposition 4.1 (First-order optimality condition for influence).Consider a dataset D, a differing point S, and a kernel function K satisfying the same conditions of Theorem 3.1.Then, we have that\n∇ Q M (Q) | Q=S = −α −2 (1 − K SD M −1 D K DS )(y S − K SD M −1 D y D ) 2 KSD M −1 D K DS , where M (Q) is the mean distance LOOD in C.1, K SD , K DS , K DD are kernel matrices as defined in Section 2, KSD = ∂ ∂Q K QD | Q=S , M D = K DD + σ 2 I, and α = 1 − K SD M −1 D K DS + σ 2 .",
    "next": "Our first observation from Proposition 4.1 is that when S is far from the dataset D in the sense that K SD ≈ 0, the mean distance LOOD gradient ∇ Q M (Q) | Q=S is close to 0, i.e., the differing data S is a stationary point for the mean distance LOOD objective.We also analyze the second-order Hessian condition of mean distance LOOD in Lemma G. 4, and prove that the differing data S has locally maximal influence, as long as S far from the remaining training dataset D, and the data labels are bounded (which is the case in a classification task).In the opposite case where S is close to D in the sense that y S ≈ K SD M −1 D y D (i.e., S is perfectly predicted by the GP trained on remaining dataset D), then the gradient\n∇ Q M (Q) | Q=S of mean distance LOOD is also close to zero."
  },
  {
    "index": 591,
    "source_corpus_id": 3524184,
    "ref_id": "b2",
    "citation_corpus_id": 11383178,
    "start": 7570,
    "end": 7590,
    "prev": "(2015)).",
    "curr": "MARGINAL LOG-LIKELIHOOD ESTIMATION\n\nWe use two bounds to estimate the marginal log-likelihood of a model: IWAE (Burda et al., 2016) and AIS (Neal, 2001).",
    "next": "Here we describe the IWAE bound."
  },
  {
    "index": 593,
    "source_corpus_id": 237292758,
    "ref_id": "b24",
    "citation_corpus_id": 6628106,
    "start": 1545,
    "end": 1565,
    "prev": "This suggests that the inferior generalization performance of Adam is fundamentally tied to the nonconvex landscape of deep learning optimization.",
    "curr": "Introduction\n\nAdaptive gradient methods (Duchi et al., 2011;Hinton et al., 2012;Kingma and Ba, 2015;Reddi et al., 2018) such as Adam are very popular optimizers for training deep neural networks.",
    "next": "By adjusting the learning rate coordinate-wisely based on historical gradient information, they are known to be able to automatically choose appropriate learning rates to achieve fast convergence in training."
  },
  {
    "index": 595,
    "source_corpus_id": 254408767,
    "ref_id": "b33",
    "citation_corpus_id": 239768373,
    "start": 33117,
    "end": 33135,
    "prev": "We can observe that models in Region 1 have the highest test accuracies in each dataset.",
    "curr": "Computation (a) The Clothing-1M dataset (b) The Animal-10N dataset (c) The CIFAR-10N dataset Selection (d) The Clothing-1M dataset (e) The Animal-10N dataset (f) The CIFAR-10N dataset Evaluation (g) The Clothing-1M dataset (h) The Animal-10N dataset (i) The CIFAR-10N dataset\n\nExperiments on Real-world Datasets with noisy labels\n\nTo evaluate the performance of our approach on real-world datasets, we have conducted additional experiments on the Clothing-1M dataset [Xiao et al., 2015], which is a dataset with 1M images of clothes, on the Animal-10N dataset [Song et al., 2019], which is a dataset with 50k images of animals and on the CIFAR-10N dataset [Wei et al., 2022], which is the CIFAR-10 dataset with human-annotated noisy labels obtained from Amazon Mechanical Turk.",
    "next": "In the Clothing-1M dataset, the images have been labeled from the texts that accompany them, hence there are both clean and noisy labels in the set, and in the Animal-10N dataset, the images have been gathered and labeled from search engines."
  },
  {
    "index": 597,
    "source_corpus_id": 256358777,
    "ref_id": "b13",
    "citation_corpus_id": 3144218,
    "start": 1370,
    "end": 1392,
    "prev": "These new theoretical results are supported by corresponding numerical investigations.",
    "curr": "INTRODUCTION\n\nGraph Convolutional Networks (GCNs) (Kipf & Welling, 2017;Hammond et al., 2011;Defferrard et al., 2016) generalize Euclidean convolutional networks to the graph setting by replacing convolutional filters by functional calculus filters; i.e.",
    "next": "scalar functions applied to a suitably chosen graph-shift-oprator capturing the geometry of the underlying graph."
  },
  {
    "index": 598,
    "source_corpus_id": 252683295,
    "ref_id": "b3",
    "citation_corpus_id": 247475874,
    "start": 2131,
    "end": 2150,
    "prev": "1 Code and datasets are available in https://github.com/zjunlp/MKG_Analogy.",
    "curr": "INTRODUCTION\n\nAnalogical reasoning -the ability to perceive and use relational similarity between two situations or events -holds an important place in human cognition (Johnson-Laird, 2006;Bengio et al., 2021;Chen et al., 2022a) and can provide back-end support for various fields such as education (Thagard, 1992), creativity (Goel, 1997), thus appealing to the AI community.",
    "next": "Early, Mikolov et al."
  },
  {
    "index": 599,
    "source_corpus_id": 53514358,
    "ref_id": "b2",
    "citation_corpus_id": 30745030,
    "start": 2650,
    "end": 2672,
    "prev": "Yet these models have achieved state-ofthe-art generalization error on many real-world tasks.",
    "curr": "This observation has spurred an active line of research (Soudry et al., 2018;Brutzkus et al., 2018;Li & Liang, 2018) that has tried to understand what properties are possessed by stochastic gradient descent (SGD) training of deep networks that allows these networks to generalize well.",
    "next": "One particularly promising line of work in this area (Neyshabur et al., 2017;Arora et al., 2018) has been bounds that utilize the noise-resilience of deep networks on training data i.e., how much the training loss of the network changes with noise injected into the parameters, or roughly, how wide is the training loss minimum."
  },
  {
    "index": 600,
    "source_corpus_id": 244714571,
    "ref_id": "b15",
    "citation_corpus_id": 52920808,
    "start": 17523,
    "end": 17540,
    "prev": "The first part of Assumption 4.1 assures that H is non-singular and that the input data lies in the unit sphere S d−1 .",
    "curr": "Such assumption is commonly made in overparameterized neural network literature (Arora et al., 2019;Du et al., 2019b;a;Cao & Gu, 2019).",
    "next": "The non-singularity is satisfied when e.g."
  },
  {
    "index": 602,
    "source_corpus_id": 221041408,
    "ref_id": "b2",
    "citation_corpus_id": 213152193,
    "start": 9171,
    "end": 9191,
    "prev": "(2019) observes that parameters in different layers have similar attention distribution, and propose a parameter distillation method from shallow layers to deep layers.",
    "curr": "Another notable work is ELECTRA (Clark et al., 2019), which develops a new task using one discriminator and one generator.",
    "next": "The generator corrupts the sentence, and the discriminator is trained to predict whether each word in the corrupted sentence is replaced or not."
  },
  {
    "index": 603,
    "source_corpus_id": 247476419,
    "ref_id": "b8",
    "citation_corpus_id": 49667762,
    "start": 2546,
    "end": 2568,
    "prev": "One solution is cross-layer parameter sharing, which reduces the memory needed to store parameters, which can also reduce the cost of communicating model updates in distributed training (Lan et al., 2020;Jaegle et al., 2021) and federated learning (Konečný et al., 2016;McMahan et al., 2017), as the model is smaller, and can help avoid overfitting (Jaegle et al., 2021).",
    "curr": "However, prior work in parameter sharing (e.g., Dehghani et al., 2019;Savarese & Maire, 2019;Lan et al., 2020;Jaegle et al., 2021) has two significant limitations.",
    "next": "First, they rely on suboptimal hand-crafted techniques for deciding where and how sharing occurs."
  },
  {
    "index": 604,
    "source_corpus_id": 247597138,
    "ref_id": "b0",
    "citation_corpus_id": 61153666,
    "start": 6808,
    "end": 6813,
    "prev": "Forcing autoregressive models to generate longer texts exacerbates this incoherence because the models struggle to extrapolate beyond their expected text end point.",
    "curr": "Prior work has tried to address the problem of generating globally coherent text with planning-based approaches (Puduppully et al., 2019;Moryossef et al., 2019;Fan et al., 2019;Kiddon et al., 2016).",
    "next": "However, planning-based approaches rely on domain-specific heuristics for capturing text structure and dynamics."
  },
  {
    "index": 605,
    "source_corpus_id": 252683988,
    "ref_id": "b44",
    "citation_corpus_id": 23892230,
    "start": 2156,
    "end": 2178,
    "prev": "With SLiC, we exceed or match SOTA results on a wide range of generation tasks spanning abstractive summarization, question generation, abstractive question answering and data-to-text generation, even with modest-sized models.",
    "curr": "INTRODUCTION\n\nConditional language generation aims to generate natural language text based on input context, and includes many useful and hard tasks such as abstractive summarization (Mani, 2001;Nenkova and McKeown, 2011), generative question answering (Bajaj et al., 2016), question generation (Zhou et al., 2017) and data-to-text (Wiseman et al., 2017;Gardent et al., 2017) tasks.",
    "next": "Pretraining large Transformer encoder-decoder models and fine-tuning them on downstream tasks is the common paradigm to address these tasks (Raffel et al., 2020;Lewis et al., 2019;Tay et al., 2022;Zhang et al., 2019a)."
  },
  {
    "index": 606,
    "source_corpus_id": 263608698,
    "ref_id": "b8",
    "citation_corpus_id": 215717103,
    "start": 6356,
    "end": 6375,
    "prev": "age), and AL (audio-language) data pairs.The videos of previous datasets are always truncated segments from long videos (Miech et al., 2019;Xue et al., 2022), resulting in fragmented semantics.To avoid this problem, we construct our video-text pairs from short videos with complete stories.To ensure the quality of the central language modality, we perform multi-view text generation and enhancement on VIDAL-10M.",
    "curr": "The proposed LanguageBind ensures that we can extend vision-language to multiple (N) modalities, and our dataset VIDAL-10M benefits more downstream tasks beyond VL tasks, including video retrieval (Luo et al., 2022), depth classification (Cao et al., 2017), infrared classification (Baffa & Lattari, 2018) and audio classification (Palanisamy et al., 2020).In text-to-video zero-shot retrieval based on CLIP-Large, LanguageBind achieves state-of-the-art (SOTA) performance on four datasets, surpassing InterVideo (Wang et al., 2022c) by 1.9% on MSR-VTT (Xu et al., 2016), 8.8% on MSVD (Chen & Dolan, 2011), 6.3% on DiDeMo (Anne Hendricks et al., 2017), and 4.4% on ActivityNet (Caba Heilbron et al., 2015).For zero-shot classification on depth and infrared data, LanguageBind achieves a substantial performance advantage over ImageBind.LanguageBind attains top-1 accuracy of 87.2% and 65.1% on LLVIP and NYU-D, respectively, outperforming Im-ageBind by 23.8% and 11.1%.For zero-shot audio classification tasks, LanguageBind outperforms ImageBind with a 22.9% higher top-1 accuracy on the ESC50 dataset.",
    "next": "We summarize our primary contributions as follows:\n\n• We propose LanguageBind, the langauge-based multi-modal pretraining approach.During the pretraining process, all modalities gradually align with the language modality through contrastive learning, and these modalities are unified within a shared embedding space."
  },
  {
    "index": 608,
    "source_corpus_id": 247595075,
    "ref_id": "b21",
    "citation_corpus_id": 4606753,
    "start": 2916,
    "end": 2937,
    "prev": "First, learning to search works best when it exploits the symbolic scaffolding of existing search algorithms already proven useful for the problem domain.",
    "curr": "For example, AlphaGo exploits Monte Carlo Tree Search (Silver et al., 2016), while NGDS exploits top-down deductive search (Kalyan et al., 2018).",
    "next": "We engineer CROSSBEAM around bottom-up enumerative search (Udupa et al., 2013), a backbone of several successful recent program synthesis algorithms (Shi et al., 2020a;Odena et al., 2021;Barke et al., 2020)."
  },
  {
    "index": 610,
    "source_corpus_id": 245769552,
    "ref_id": "b20",
    "citation_corpus_id": 52967399,
    "start": 9232,
    "end": 9253,
    "prev": "Clusters of signalprocessing-based acoustic features, e.g., Mel-frequency cepstral coefficients (MFCC), exhibit nontrivial correlations with the inherent acoustic units of speech inputs.",
    "curr": "Using (A 1:T , z a 1:T ) pairs, the second step learns new feature representations by minimizing a masked prediction loss, similar to masked language modeling in BERT (Devlin et al., 2019).",
    "next": "The pressure to predict cluster assignments of masked audio regions forces the model to learn good local acoustic representations for unmasked regions and long-range temporal dependencies between latent features."
  },
  {
    "index": 612,
    "source_corpus_id": 252967802,
    "ref_id": "b9",
    "citation_corpus_id": 257232596,
    "start": 5074,
    "end": 5095,
    "prev": "A number of algorithms based on that framework, such as LargeVis (Tang et al., 2016), UMAP (McInnes et al., 2018), and TriMap (Amid & Warmuth, 2019) have been developed and got widespread adoption in recent years in a variety of application fields.",
    "curr": "All of them are closely related to SNE (Böhm et al., 2022;Damrich et al., 2023) and rely on the kNN graph of the data.",
    "next": "NE algorithms have been used to visualize latent representations of neural networks trained in a supervised setting (e.g."
  },
  {
    "index": 613,
    "source_corpus_id": 52877454,
    "ref_id": "b2",
    "citation_corpus_id": 4630420,
    "start": 6172,
    "end": 6201,
    "prev": "Contrastive methods are central to many popular word-embedding methods (Collobert & Weston, 2008;Mnih & Kavukcuoglu, 2013;Mikolov et al., 2013), but they are found in many unsupervised algorithms for learning representations of graphstructured input as well.",
    "curr": "There are many ways to score a representation, but in the graph literature the most common techniques use classification (Perozzi et al., 2014;Grover & Leskovec, 2016;Kipf & Welling, 2016b;Hamilton et al., 2017b), though other scoring functions are used (Duran & Niepert, 2017;Bojchevski & Günnemann, 2018).",
    "next": "DGI is also contrastive in this respect, as our objective is based on classifying local-global pairs and negative-sampled counterparts."
  },
  {
    "index": 615,
    "source_corpus_id": 209832425,
    "ref_id": "b8",
    "citation_corpus_id": 54443381,
    "start": 5528,
    "end": 5551,
    "prev": "Regularization and replay approaches fix the model architecture before training and prevent catastrophic forgetting by regularizing the change of a specific set of weights or replaying previously learned data.",
    "curr": "Hybrids of replay and regularization also exist, such as Gradient Episodic Memory (GEM) (Lopez-Paz & Ranzato, 2017;Chaudhry et al., 2019a).",
    "next": "On the other hand, methods based on expansion add new network components to learn new data."
  },
  {
    "index": 616,
    "source_corpus_id": 13019454,
    "ref_id": "b17",
    "citation_corpus_id": 629094,
    "start": 10109,
    "end": 10130,
    "prev": "E x ∈ R |V |×E is the embedding matrix, where |V | is the number of unique events (the vocabulary size) and E is the embedding dimension.",
    "curr": "The use of embedding provides a dense representation for an event that improves learning (Turian et al., 2010).",
    "next": "Through training, the embedding vector of an event encodes its meaning relative to other events."
  },
  {
    "index": 621,
    "source_corpus_id": 258999337,
    "ref_id": "b16",
    "citation_corpus_id": 6628106,
    "start": 32876,
    "end": 32895,
    "prev": "o the points within its intermediate predicted mask from the previous layer.Then in the click-to-click attention module, each click query self-attends to each other to realize inter-query communications (Eq.2).To make the point features click-aware, we let the point features cross-attend to the click queries in a scene-toclick attention module (Eq.3).In equations ( 1) to (3), we omit the layer normalization and dropout for simplicity.W Q , W K , W V are learnable weights for query, key and value as in the standard attention mechanism (Vaswani et al., 2017).In all attention modules, we add the positional part to their respective keys/queries.",
    "curr": "Q l+1 c = softmax W Q (Q l c + Q p ) • W K (F l c + F p ) + H √ D • W V F l c + Q l c\n(1)\nQ l+1 c = softmax W Q (Q l c + Q p ) • W K (Q l c + Q p √ D • W V Q l c + Q l c\n(2)    Training.We set the λ CE = 1 and the λ Dice = 2 in the loss function.The loss is applied to every intermediate layer of the click attention module.We use the Adam optimizer (Kingma & Ba, 2015) with a weight decay factor 1e-4.We train the model on ScanNet40 for 1100 epochs with an initial learning rate 1e-4, which is decayed by 0.1 after 1000 epochs.Due to the smaller data size, we train the model on ScanNet20 for 850 epochs with an initial learning rate 1e-4, which is decayed by 0.1 after 800 epochs.We use a single TITAN RTX GPU with 24GB memory for training.",
    "next": "F l+1 c = softmax W Q (F l c + F p ) • W K (Q l c + Q p ) √ D • W V Q l c + F l c(3)\n\nB ADDITIONAL RESULTS\n\nB.1 ABLATION ON QUERY FUSION STRATEGY In our query fusion module, we apply a per-point max operation to aggregate click-specific masks to region-specific masks (Fig."
  },
  {
    "index": 622,
    "source_corpus_id": 225076227,
    "ref_id": "b24",
    "citation_corpus_id": 52901777,
    "start": 7946,
    "end": 7968,
    "prev": "RELATED WORK\n\nSelective classification.",
    "curr": "Abstaining from uncertain predictions is a classic idea (Chow, 1957;Hellman, 1970), and uncertainty estimation is an active area of research, from the popular approach of using softmax probabilities (Geifman & El-Yaniv, 2017) to more sophisticated methods using dropout (Gal & Ghahramani, 2016), ensembles (Lakshminarayanan et al., 2017), or training snapshots (Geifman et al., 2018).",
    "next": "Others incorporate abstention into model training (Bartlett & Wegkamp, 2008;Geifman & El-Yaniv, 2019;Feng et al., 2019) and learn to abstain on examples human experts are more likely to get correct (Raghu et al., 2019;Mozannar & Sontag, 2020;De et al., 2020)."
  },
  {
    "index": 626,
    "source_corpus_id": 3292002,
    "ref_id": "b23",
    "citation_corpus_id": 3144218,
    "start": 5643,
    "end": 5664,
    "prev": "(2016) proposed to approximate the filters by means of a Chebyshev expansion of the graph Laplacian, removing the need to compute the eigenvectors of the Laplacian and yielding spatially localized filters.",
    "curr": "Finally, Kipf & Welling (2017) simplified the previous method by restricting the filters to operate in a 1-step neighborhood around each node.",
    "next": "However, in all of the aforementioned spectral approaches, the learned filters depend on the Laplacian eigenbasis, which depends on the graph structure."
  },
  {
    "index": 627,
    "source_corpus_id": 202888950,
    "ref_id": "b16",
    "citation_corpus_id": 3507990,
    "start": 21092,
    "end": 21110,
    "prev": "Detailed description of the architecture can be found in the appendix.",
    "curr": "Experiments are performed on three data sets: Omniglot (Lake et al., 2015), miniImageNet (Vinyals et al., 2016), and tieredImageNet (Ren et al., 2018).",
    "next": "We followed standard data processing procedures which are detailed in the appendix."
  },
  {
    "index": 628,
    "source_corpus_id": 91175758,
    "ref_id": "b34",
    "citation_corpus_id": 13751870,
    "start": 23320,
    "end": 23332,
    "prev": "If implementation details can cause the scores produced by a metric to vary significantly, this conflates comparison of when different implementations are used to compare different methods.",
    "curr": "This has caused issues in benchmarking machine translation (Post, 2018) and music information retrieval (Raffel et al., 2014).",
    "next": "This problem is particularly pronounced for NNDs because they require implementing a neural network architecture and training scheme, and the use of different software frameworks or even driver versions can cause results to vary (Henderson et al., 2018;Oliver et al., 2018)."
  },
  {
    "index": 629,
    "source_corpus_id": 257405483,
    "ref_id": "b6",
    "citation_corpus_id": 52967399,
    "start": 1699,
    "end": 1720,
    "prev": "INTRODUCTION\n\nHyperparameter optimization (HPO) and neural architecture search (NAS) yield state-of-the-art models, but often are a very costly endeavor, especially when working with large datasets and models.",
    "curr": "For example, using the results of (Sharir et al., 2020) we can estimate that evaluating 50 configurations for a 340-million-parameter BERT model (Devlin et al., 2019) on the 15GB Wikipedia and Book corpora would cost around $500,000.",
    "next": "To make HPO and NAS more efficient, researchers explored how we can learn from cheaper evaluations (e.g."
  },
  {
    "index": 632,
    "source_corpus_id": 204905143,
    "ref_id": "b16",
    "citation_corpus_id": 3633127,
    "start": 1617,
    "end": 1639,
    "prev": "Moreover, Our consistency regularized GAN (CR-GAN) improves stateof-the-art FID scores for conditional generation from 14.73 to 11.67 on CIFAR-10 and from 8.73 to 6.66 on ImageNet-2012.",
    "curr": "INTRODUCTION\n\nGenerative Adversarial Networks (GANs) (Goodfellow et al., 2014) have recently demonstrated impressive results on image-synthesis benchmarks Zhang et al., 2017;Miyato & Koyama, 2018;Zhang et al., 2018;Brock et al., 2018;Karras et al., 2019).",
    "next": "In the original setting, GANs are composed of two neural networks trained with competing goals: the generator is trained to synthesize realistic samples to fool the discriminator and the discriminator is trained to distinguish real samples from fake ones produced by the generator."
  },
  {
    "index": 635,
    "source_corpus_id": 85449634,
    "ref_id": "b41",
    "citation_corpus_id": 5592690,
    "start": 1968,
    "end": 1988,
    "prev": "A lot of effort has been put into designing sophisticated neural MRC architectures for reading short context (e.g.",
    "curr": "a single paragraph), with much success (Wang & Jiang, 2017;Seo et al., 2017;Xiong et al., 2017;Wang et al., 2018c;Yu et al., 2018, inter alia).",
    "next": "However, the performance of such systems degrades significantly when combined with a retriever in open domain settings."
  },
  {
    "index": 636,
    "source_corpus_id": 11324902,
    "ref_id": "b24",
    "citation_corpus_id": 14124313,
    "start": 22935,
    "end": 22962,
    "prev": "All evaluations are repeated 10 times based on different shuffles of the data, and the mean accuracies and standard deviations are analyzed.",
    "curr": "Office: Since the office dataset is rather small with only 2817 images in its largest domain, we use the latent representations of the convolution neural network VGG16 of Simonyan & Zisserman (2014).",
    "next": "In particular we train a classifier with one hidden layer, 256 hidden nodes and sigmoid activation function on top of the output of the first dense layer in the network."
  },
  {
    "index": 637,
    "source_corpus_id": 57759353,
    "ref_id": "b10",
    "citation_corpus_id": 7034786,
    "start": 9408,
    "end": 9433,
    "prev": "(2017) Learning to execute programs.",
    "curr": "Neural Program Interpreters (NPI) have been extensively studied for programs that abstract and execute tasks such as sorting, shape manipulation, and grade-school arithmetic (Reed & De Freitas, 2016;Cai et al., 2017;Bošnjak et al., 2017).",
    "next": "In NPI (Reed & De Freitas, 2016), the key insight is that a program execution trace can be decomposed into predefined operations that are more primitive; and at each step, an NPI learns to predict what operation to take next depending on the general environment, domain specific state , and previous actions."
  },
  {
    "index": 639,
    "source_corpus_id": 235368204,
    "ref_id": "b16",
    "citation_corpus_id": 4722462,
    "start": 2279,
    "end": 2296,
    "prev": "The success of deep learning (DL) models on complicated cognitive tasks (Krizhevsky et al., 2012;LeCun et al., 2015;Silver et al., 2016) then inspired researchers to apply DL-based models to language games to investigate the agents' ability to invent communication protocols without preset rules (e.g.",
    "curr": "Lee et al., 2018;Lazaridou et al., 2018).",
    "next": "In the existing works (e.g."
  },
  {
    "index": 640,
    "source_corpus_id": 236318292,
    "ref_id": "b25",
    "citation_corpus_id": 108300573,
    "start": 3303,
    "end": 3306,
    "prev": "The main challenges in dealing with such data include the presence of variable time gaps between the observation time points, partially observed feature vectors caused by the lack of temporal alignment across different dimensions, as well as different data cases, and variable numbers of observations across dimensions and data cases.",
    "curr": "Significant recent work has focused on developing specialized models and architectures to address these challenges in modeling irregularly sampled multivariate time series [17,18,19,9,3,26,24,12,16,27,7,30,14].",
    "next": "Recently, Shukla and Marlin [27] introduced the Multi-Time Attention Network (mTAN) model, a variational autoencoder (VAE) architecture for continuous-time interpolation of irregularly sampled time series."
  },
  {
    "index": 642,
    "source_corpus_id": 52903499,
    "ref_id": "b29",
    "citation_corpus_id": 4429876,
    "start": 4428,
    "end": 4432,
    "prev": "Bridging this gap between theory and practice is a very active area of research.",
    "curr": "There have been many attempts to understand why optimization works well for neural networks, by studying the loss surface [1,10,12,17,18,21,23,24,[26][27][28][29][30] and the role of (stochastic) gradient-based methods [4,8,25].",
    "next": "Besides nonconvexity, for ReLU networks significant additional challenges in the analysis arise due to nondifferentiability, and obtaining a precise understanding of the nondifferentiable points is still elusive."
  },
  {
    "index": 644,
    "source_corpus_id": 53215593,
    "ref_id": "b3",
    "citation_corpus_id": 11212020,
    "start": 12131,
    "end": 12154,
    "prev": "(1) Our key intuition is that the inverse dynamics model should attend to the most relevant part of the observation, which is controllable by the agent, to be able to classify the actions.",
    "curr": "We determine whether each region in a H × W grid is controllable, or in other words, useful for predicting the agent's action, by using a spatial attention mechanism (Bahdanau et al., 2015;Xu et al., 2015).",
    "next": "An overview of the model is shown in Figure 1."
  },
  {
    "index": 645,
    "source_corpus_id": 220347587,
    "ref_id": "b36",
    "citation_corpus_id": 196178484,
    "start": 5174,
    "end": 5178,
    "prev": "These methods attempt to separate nominal samples from anomalies in an unsupervised manner by concentrating nominal data in feature space while mapping anomalies to distant locations [36,6].",
    "curr": "In the domain of NLP, DSVDD [36] has been successfully applied to text, which yields a form of interpretation using attention mechanisms [37].",
    "next": "For images, Kauffmann et al."
  },
  {
    "index": 647,
    "source_corpus_id": 264128269,
    "ref_id": "b19",
    "citation_corpus_id": 21850704,
    "start": 2428,
    "end": 2449,
    "prev": "xplanation for why Goodhart's law occurs in Markov decision processes.We use these theoretical insights to propose an optimal early stopping method that provably avoids the aforementioned pitfall and derive theoretical regret bounds for this method.Moreover, we derive a training method that maximises worst-case reward, for the setting where there is uncertainty about the true reward function.Finally, we evaluate our early stopping method experimentally.Our results support a foundation for a theoretically-principled study of reinforcement learning under reward misspecification.",
    "curr": "INTRODUCTION\n\nTo solve a problem using Reinforcement Learning (RL), it is necessary first to formalise that problem using a reward function (Sutton & Barto, 2018).However, due to the complexity of many real-world tasks, it is exceedingly difficult to directly specify a reward function that fully captures the task in the intended way.However, misspecified reward functions will often lead to undesirable behaviour (Paulus et al., 2018;Ibarz et al., 2018;Knox et al., 2023;Pan et al., 2021).This makes designing good reward functions a major obstacle to using RL in practice, especially for safety-critical applications.",
    "next": "An increasingly popular solution is to learn reward functions from mechanisms such as human or automated feedback (e.g.Christiano et al., 2017;Ng & Russell, 2000).However, this approach comes with its own set of challenges: the right data can be difficult to collect (e.g.Paulus et al., 2018), and it is often challenging to interpret it correctly (e.g.Mindermann & Armstrong, 2018;Skalse & Abate, 2023).Moreover, optimising a policy against a learned reward model effectively constitutes a distributional shift (Gao et al., 2023); i.e., even if a reward function is accurate under the training distribution, it may fail to induce desirable behaviour from the RL agent."
  },
  {
    "index": 649,
    "source_corpus_id": 247476256,
    "ref_id": "b33",
    "citation_corpus_id": 15197911,
    "start": 3725,
    "end": 3741,
    "prev": "Such a strategy has been successful in other dense prediction tasks (Ronneberger et al., 2015) as well.",
    "curr": "Others propose different loss functions (Kokkinos, 2016;Kervadec et al., 2019) to address class imbalance.",
    "next": "Despite the improvements, we identify two issues regarding crisp boundary detection."
  },
  {
    "index": 650,
    "source_corpus_id": 220055921,
    "ref_id": "b36",
    "citation_corpus_id": 7034786,
    "start": 9963,
    "end": 9967,
    "prev": "Our proposed framework bears some resemblance to SPIRAL [7]; however, SPIRAL does not provide a density function, and it has no hierarchical structure, limiting its applications to image reconstruction and unconditional generation.",
    "curr": "Neuro-symbolic modeling has become an active area of research, with applications to learning inputoutput programs [37,16,3,34], question answering [44,29] and image description [4].",
    "next": "GNS modeling distinguishes from prior work through its focus on hybrid generative modeling, combining both structured program execution and neural networks directly in the probabilistic generative process."
  },
  {
    "index": 652,
    "source_corpus_id": 202712898,
    "ref_id": "b2",
    "citation_corpus_id": 14124313,
    "start": 1541,
    "end": 1568,
    "prev": "Nonetheless, these architectures may not necessarily lead to better generalization performance compared with other candidate architectures in the same search space, and therefore further improvement is possible by revising existing NAS algorithms.",
    "curr": "INTRODUCTION\n\nVarious neural network architectures (Krizhevsky et al., 2012;Simonyan & Zisserman, 2015;He et al., 2016;Huang et al., 2017) have been devised over the past decades, achieving superhuman performance for a wide range of tasks.",
    "next": "Designing these neural networks typically takes substantial efforts from domain experts by trial and error."
  },
  {
    "index": 654,
    "source_corpus_id": 13807351,
    "ref_id": "b45",
    "citation_corpus_id": 252796,
    "start": 5644,
    "end": 5665,
    "prev": "Actively biasing the optimization towards wide valleys in the energy landscape results in better generalization error.",
    "curr": "We present experimental results on fully-connected and convolutional neural networks (CNNs) on the MNIST and CIFAR-10 (Krizhevsky, 2009) datasets and recurrent neural networks (RNNs) on the Penn Tree Bank dataset (PTB) (Marcus et al., 1993) and character-level text prediction.",
    "next": "Our experiments show that Entropy-SGD scales to deep networks used in practice, obtains comparable generalization error as competitive baselines and also trains much more quickly than SGD (we get a 2x speed-up over SGD on RNNs)."
  },
  {
    "index": 655,
    "source_corpus_id": 3470596,
    "ref_id": "b39",
    "citation_corpus_id": 11445252,
    "start": 29665,
    "end": 29689,
    "prev": "The nature of an audio signal allows one to exploit complex operations as presented earlier in the paper.",
    "curr": "The experiments were performed on the MusicNet dataset [Thickstun et al., 2016].",
    "next": "For computational efficiency we resampled the original input from 44.1kHz to 11kHz using the algorithm described in Smith [2002]."
  },
  {
    "index": 656,
    "source_corpus_id": 222066778,
    "ref_id": "b2",
    "citation_corpus_id": 3708505,
    "start": 5522,
    "end": 5539,
    "prev": "Related work.",
    "curr": "Kernels for deep learning were originally derived by Neal (1996) for shallow networks, and later for deep networks (Cho & Saul, 2009;Daniely et al., 2016;Lee et al., 2018;Matthews et al., 2018).",
    "next": "Smola et al."
  },
  {
    "index": 657,
    "source_corpus_id": 250243645,
    "ref_id": "b46",
    "citation_corpus_id": 6628106,
    "start": 21932,
    "end": 21934,
    "prev": "Starting with Figure 1 We then plot the MI accuracy and precision as a function of time, both before injection and also for ten epochs after.",
    "curr": "We observe that MI is impossible until the injection step, after which precision remains very 46 47 48 49 50 51 52 53 54 55 56 57 58 59  high for many epochs, taking 10 epochs to decay to roughly 65%, which is permitted by DP 2 with ε ≈ 0.6.",
    "next": "This demonstrates that forgetting does occur for this setting."
  },
  {
    "index": 659,
    "source_corpus_id": 249848252,
    "ref_id": "b11",
    "citation_corpus_id": 196831582,
    "start": 2748,
    "end": 2751,
    "prev": "Under review.",
    "curr": "Introduction\n\nGenerative Adversarial Networks (GANs) [8] have achieved significant advancement over the past several years, enabling many computer vision tasks such as image manipulation [17,12,28,9,2], domain translation [11,34,6,7,20,18], and image or video generation [19,21,15,16,13,14,29,32,30,31].",
    "next": "In addition, the resolution and quality of images synthesized by generative models have seen rapid improvement recently in terms of quantitative metrics [3,10,27]."
  },
  {
    "index": 660,
    "source_corpus_id": 222125236,
    "ref_id": "b21",
    "citation_corpus_id": 207870430,
    "start": 1696,
    "end": 1721,
    "prev": "rity search.This approach requires no additional training and scales to give the decoder direct access to billions of examples at test time, resulting in a highly expressive model that consistently improves performance across many settings.Simply adding nearest neighbor search improves a state-of-the-art German-English translation model by 1.5 BLEU.kNN-MT allows a single model to be adapted to diverse domains by using a domain-specific datastore, improving results by an average of 9.2 BLEU over zero-shot transfer, and achieving new state-of-the-art results-without training on these domains.A massively multilingual model can also be specialized for particular language pairs, with improvements of 3 BLEU for translating from English into German and Chinese.Qualitatively, kNN-MT is easily interpretable; it combines source and target context to retrieve highly relevant examples.",
    "curr": "INTRODUCTION\n\nNon-parametric methods have recently been successfully applied to tasks such as language modeling (Khandelwal et al., 2020) and question answering (Guu et al., 2020;Lewis et al., 2020).They allow models that are (1) expressive, because they can use an arbitrary amount of data at test time; (2) adaptable, because predictions can be controlled by changing the datastore, and (3) interpretable, because the data used to make the prediction can be directly inspected.We introduce kNN-MT, a simple non-parametric method for machine translation (MT) using nearest neighbor retrieval.kNN-MT can be added to any pre-trained neural translation model without further training, and significantly improves performance for in-domain, out-of-domain, and multi-lingual evaluations.",
    "next": "More specifically, kNN-MT interpolates the target-token softmax distribution from a neural MT model with a multinomial generated using nearest neighbor search over examples cached in a data store.The cache is over translation contexts (i.e."
  },
  {
    "index": 661,
    "source_corpus_id": 85543148,
    "ref_id": "b37",
    "citation_corpus_id": 14124313,
    "start": 7304,
    "end": 7332,
    "prev": "It is challenging due to the complexity of features in high-dimensional space (Krizhevsky et al., 2012), the large intra-class variation and inter-class similarity across categories in benchmarks (Deng et al., 2009;Tsung-Yi Lin, 2015).",
    "curr": "Thanks to the development of deep networks structure (Simonyan & Zisserman, 2015;He et al., 2016) and modern GPU hardware acceleration, this community has witnessed a great bloom in both performance and efficiency.",
    "next": "The detection of small objects is addressed in concurrent literature mainly through two manners."
  },
  {
    "index": 663,
    "source_corpus_id": 251953412,
    "ref_id": "b39",
    "citation_corpus_id": 201646309,
    "start": 3249,
    "end": 3275,
    "prev": "As there are millions to billions of documents in a collection, efficiency is the most fundamental prerequisite for large-scale retrieval.",
    "curr": "To this end, query-agnostic document representations (i.e., indexing the collection independently) and lightweight relevance metrics (e.g., cosine similarity, dot-product) have become the common practices to meet the prerequisite -usually achieved by a two-tower structure (Reimers & Gurevych, 2019), a.k.a., bi-encoder and dual-encoder, in representation learning literature.",
    "next": "Besides the prevalent 'dense-vector retrieval' paradigm that encodes both queries and documents in the same low-dimension, real-valued latent semantic space , another retrieval paradigm, 'lexicon-weighting retrieval', aims to leverage weighted sparse representation in vocabulary space (Formal et al., 2021a;."
  },
  {
    "index": 666,
    "source_corpus_id": 3473900,
    "ref_id": "b16",
    "citation_corpus_id": 5959482,
    "start": 1479,
    "end": 1501,
    "prev": "INTRODUCTION\n\nLearning feature representations of natural data such as text and images has become increasingly important for understanding real-world concepts.",
    "curr": "These representations are useful for many tasks, ranging from semantic understanding of words and sentences (Mikolov et al., 2013;Kiros et al., 2015), image caption generation (Vinyals et al., 2015), textual entailment prediction (Rocktäschel et al., 2015), to language communication with robots (Bisk et al., 2016).",
    "next": "Meaningful representations of text and images capture visual-semantic information, such as hierarchical structure where certain entities are abstractions of others."
  },
  {
    "index": 668,
    "source_corpus_id": 225103201,
    "ref_id": "b8",
    "citation_corpus_id": 24029589,
    "start": 9634,
    "end": 9654,
    "prev": "Finally, we also go beyond the max-margin settings in these works to analyze the dynamics of a finite-time gradient-descent trained classifier.",
    "curr": "We also refer the reader to an orthogonal line of theory work (Fawzi et al., 2018;Gilmer et al., 2018) that has investigated the failure of standard classifiers in the adversarial perturbation setting.",
    "next": "Algorithms for OoD generalization."
  },
  {
    "index": 670,
    "source_corpus_id": 212874725,
    "ref_id": "b3",
    "citation_corpus_id": 22163777,
    "start": 11270,
    "end": 11273,
    "prev": "Therefore, B π describes how visitation flows from (ξ, α) to (s, a) and hence, we call it the backward flow operator.",
    "curr": "Note that similar forms of B π have appeared in the literature, usually used to encode constraints in a dual linear program for an MDP [4,37,38].",
    "next": "However, the application of B π for the off-policy estimation problem as considered here appears new to the best of our knowledge."
  },
  {
    "index": 674,
    "source_corpus_id": 263620583,
    "ref_id": "b38",
    "citation_corpus_id": 57825721,
    "start": 29466,
    "end": 29486,
    "prev": "(2022) for a survey).Since it is infeasible to precisely define real-world data and tasks, several works have studied it in stylized well-defined settings (Chan et al., 2022;Hahn & Goyal, 2023;Xie et al., 2021).Garg et al.",
    "curr": "(2022) presented a meta-learning-like Transformers and Sequence Models.The analysis of the capabilities and limitations of recurrent architectures dates back to a few decades ago (Kolen & Kremer, 2001).Given the recent success of Transformers, several works have sought to investigate their theoretical expressiveness (Pérez et al., 2019;Merrill et al., 2022;Chiang & Cholak, 2022;Hahn, 2020;Yun et al., 2020;Liu et al., 2022) as well as their empirical capabilities (Bhattamishra et al., 2023;2020b;Ebrahimi et al., 2020) and limitations (Bhattamishra et al., 2020a;Chiang & Cholak, 2022).Delétang et al.",
    "next": "( 2022) conduct a comprehensive study of the performance of various sequence models such as Transformers and RNNs on formal language tasks.While most of these prior works focus on classification or related tasks, our work complements these as we conduct a comprehensive study on in-context learning tasks."
  },
  {
    "index": 675,
    "source_corpus_id": 53483414,
    "ref_id": "b13",
    "citation_corpus_id": 9059612,
    "start": 2706,
    "end": 2728,
    "prev": "Various defensive methods have been proposed to mitigate the effect of the adversarial examples.",
    "curr": "Adversarial training which augments the training set with adversarial examples shows good defensive performance in terms of white box attacks (Kurakin et al., 2017;.",
    "next": "Apart from adversarial training, there are many other defensive approaches including defensive distillation (Papernot et al., 2016b), using randomization at inference time (Xie et al., 2018), and thermometer encoding (Buckman et al., 2018), etc."
  },
  {
    "index": 679,
    "source_corpus_id": 220265948,
    "ref_id": "b33",
    "citation_corpus_id": 3524184,
    "start": 20908,
    "end": 20912,
    "prev": "Figure 3  We evaluate the accuracy of mean reconstruction at different ratios of observed entries by measuring the peak signal to noise ratio (PSNR) between the ground truth data and mean recoveries ( Figure  3(b)).",
    "curr": "To evaluate probabilistic performance we approximately measure the likelihood assigned by the recovered posteriors to the ground truth data through a reconstruction ELBO, by training a new inference function with the clean ground truths, but leaving the posterior fixed, as is common for evaluating ELBOs in unsupervised settings [34,35,11].",
    "next": "Results are shown in figure 3(c)."
  },
  {
    "index": 680,
    "source_corpus_id": 51942590,
    "ref_id": "b14",
    "citation_corpus_id": 1428702,
    "start": 10417,
    "end": 10421,
    "prev": "It just sucks.",
    "curr": "( ) This sentence is contained in a movie review from the IMDB movie data set [15], and it is classified as negative sentiment by a machine learning model to be discussed in the sequel.",
    "next": "Now suppose we wish to quantify the importance of feature \"not\" in prediction."
  },
  {
    "index": 681,
    "source_corpus_id": 212633677,
    "ref_id": "b27",
    "citation_corpus_id": 5959482,
    "start": 10709,
    "end": 10713,
    "prev": ", x n ) in one go.",
    "curr": "An embedding is a mapping from plain input, for example words or characters, to a high dimensional vector, for which learning algorithms and toolkits exists, e.g., word2vec [26].",
    "next": "Given the encoders output z = (z 0 , ."
  },
  {
    "index": 683,
    "source_corpus_id": 249375359,
    "ref_id": "b1",
    "citation_corpus_id": 52922363,
    "start": 2697,
    "end": 2717,
    "prev": "From the theoretical perspective, understanding the roles of nonlinearity in deep neural networks is one critical part of understanding how modern deep models work.",
    "curr": "Currently, most works focus on linear variants of deep models (Jacot et al., 2018;Arora et al., 2019a;Kawaguchi, 2016;Jing et al., 2022;Tian et al., 2021;Wang et al., 2021).",
    "next": "When nonlinearity is involved, deep models are often treated as richer families of black-box functions than linear ones (Arora et al., 2019b;HaoChen et al., 2021)."
  },
  {
    "index": 684,
    "source_corpus_id": 257365130,
    "ref_id": "b13",
    "citation_corpus_id": 102350747,
    "start": 5336,
    "end": 5357,
    "prev": "PRELIMINARY\n\n\nESSENTIAL PROPERTIES OF STRUCTURED LANGUAGE MODELS\n\nStructured language models feature combining the powerful representation of neural networks with syntax structures.",
    "curr": "Though many attempts have been made about structured language models (Kim et al., 2019;Drozdov et al., 2019;Shen et al., 2021), three prerequisites need to be met before a model is selected as the backbone of our method.",
    "next": "Firstly, it should have the ability to learn reasonable syntax structure in an unsupervised manner."
  },
  {
    "index": 685,
    "source_corpus_id": 244117004,
    "ref_id": "b71",
    "citation_corpus_id": 52895589,
    "start": 17109,
    "end": 17125,
    "prev": "RELATED WORK\n\nGeometric Deep Learning.",
    "curr": "Graph Neural Networks (GNNs) are becoming the de facto choice for learning with graph data (Bruna et al., 2013;Defferrard et al., 2016;Kipf and Welling, 2016;Gilmer et al., 2017;Xu et al., 2018;.",
    "next": "Motivated by symmetries naturally occurring in different data types, architectures are tailored to explicitly incorporate such properties (Cohen and Welling, 2016a;Thomas et al., 2018;Fuchs et al., 2020;Finzi et al., 2020;Eismann et al., 2020;Satorras et al., 2021)."
  },
  {
    "index": 686,
    "source_corpus_id": 233033761,
    "ref_id": "b9",
    "citation_corpus_id": 54101493,
    "start": 3683,
    "end": 3705,
    "prev": "First, obtaining photorealistic appearance features at the micro-level, such as texture and illumination, is challenging due to the limits of simulation complexity and rendering granularity.",
    "curr": "Without special treatment, CNNs tend to be biased towards textures (Geirhos et al., 2019) and suffer from badly learned representations on synthetic data.",
    "next": "Second, the common lack of texture and shape variations on synthetic images often leads to collapsed and trivial representations without any diversity."
  },
  {
    "index": 687,
    "source_corpus_id": 235367997,
    "ref_id": "b20",
    "citation_corpus_id": 52967399,
    "start": 26528,
    "end": 26549,
    "prev": "Training: There are two steps of training involved for B2F: 1) model agnostic autoregressive BSEQ prediction task to pre-train BSEQENC; 2) model-specific training for BFRP.",
    "curr": "Autoregressive BSEQ prediction: Pre-training on auxiliary tasks to improve the quality of latent embedding is a well-known technique for deep learning methods (Devlin et al., 2019;Radford et al., 2018).",
    "next": "We pre-train BSEQENC to predict the next values of backfill sequences {x (t r +1) t ,i } i∈F ."
  },
  {
    "index": 688,
    "source_corpus_id": 220514300,
    "ref_id": "b32",
    "citation_corpus_id": 990233,
    "start": 5556,
    "end": 5560,
    "prev": "Structured Convolutional Models\n\nWe address neural models designed to learn from (multi-)relational, structured data, while exploiting some form of parameter sharing, as \"structured convolutional models\" 1 .",
    "curr": "Examples of such models include Recursive Neural Networks [33,23], Graph Neural Networks [16,28], and various Relational Neural Networks [34,26,5,31], designed for regular trees, graphs, and general relational structures, respectively.",
    "next": "They all utilize the idea of convolution (templating), where the same parameterized pattern is carried over different subparts of the data (representation) with the same local structure."
  },
  {
    "index": 691,
    "source_corpus_id": 4117071,
    "ref_id": "b27",
    "citation_corpus_id": 11758569,
    "start": 8398,
    "end": 8420,
    "prev": "However there is an assumption that the class conditional distributions share some commonality and so information can be transferred from the source domain to the one-shot target domain.",
    "curr": "Generative Adversarial Networks (GAN) , and specifically Deep Convolutional GANs (DCGAN) (Radford et al., 2015) use of the ability to discriminate between true and generated examples as an objective, GAN approaches can learn complex joint densities.",
    "next": "Recent improvements in the optimization process  have reduced some of the failure modes of the GAN learning process."
  },
  {
    "index": 693,
    "source_corpus_id": 255749563,
    "ref_id": "b62",
    "citation_corpus_id": 239616181,
    "start": 3323,
    "end": 3343,
    "prev": "Yet it is quite elusive what should be the appropriate structure and granularity of these tokens to support systematic generalization and how to obtain them, particularly in the unsupervised setting where the model should learn this ability only by observing.",
    "curr": "In visual scenes, binding has recently been pursued by object-centric learning methods through the spatial binding approach (Locatello et al., 2020;Singh et al., 2022a).",
    "next": "Spatial binding aims to divide a scene spatially into smaller areas so that each area contains a meaningful entity like an object."
  },
  {
    "index": 695,
    "source_corpus_id": 36060542,
    "ref_id": "b21",
    "citation_corpus_id": 1450294,
    "start": 3838,
    "end": 3842,
    "prev": "A third weakness, that both cases share, is inherited by the way in which they generate spatial filter-wise responses, i.e.",
    "curr": "either through deconvolutionbased heatmaps [22,23,28] or by up-scaling the activation maps at a given layer/filter to the image space [2,30].",
    "next": "On the one hand, deconvolution-based methods are able to produce heatmaps with high level of detail from any filter in the network."
  },
  {
    "index": 700,
    "source_corpus_id": 258833682,
    "ref_id": "b54",
    "citation_corpus_id": 604334,
    "start": 1738,
    "end": 1742,
    "prev": "Under review.",
    "curr": "Introduction\n\nDeep Neural Network (DNN) has been shown to exhibit susceptibility to adversarial attacks [55], wherein intentionally crafted imperceptible perturbations introduced into the original input cause the model's predictions to be altered.",
    "next": "Among various defense methods [32,37,60,9,42], Adversarial Training (AT) [39] stands out as one of the most effective techniques [3,59] to enhance DNN's adversarial robustness."
  },
  {
    "index": 704,
    "source_corpus_id": 222272028,
    "ref_id": "b5",
    "citation_corpus_id": 54443381,
    "start": 7151,
    "end": 7174,
    "prev": "With the trending popularity of deep learning, continual learning has gained a critical importance because the catastrophic forgetting problem imposes key challenges to deploy deep learning models in various applications (e.g Lange et al., 2019;.",
    "curr": "A growing body of research has attempted to tackle this problem in recent years (e.g Parisi et al., 2018;Toneva et al., 2018;Nguyen et al., 2019;Farajtabar et al., 2019;Hsu et al., 2018;Rusu et al., 2016;Kirkpatrick et al., 2017;Zenke et al., 2017;Shin et al., 2017;Rolnick et al., 2018;Lopez-Paz & Ranzato, 2017;Chaudhry et al., 2018b;Riemer et al., 2018;Wallingford et al., 2020).",
    "next": "Among these works, our proposed MC-SGD bares most similarities to rehearsal based methods such us (e.g."
  },
  {
    "index": 705,
    "source_corpus_id": 252408526,
    "ref_id": "b35",
    "citation_corpus_id": 247628243,
    "start": 3094,
    "end": 3098,
    "prev": "Prior work has developed a rough taxonomy of data properties, or metadata which different examples might exhibit, including but not limited to: noisy [68,71,62,63], atypical [25,10,21,60], challenging [24,3,8,49,2], prototypical or core subset selection [49,55,56,27] and out-of-distribution This approach can bring to light biases, mislabelled examples, and other dataset issues.",
    "curr": "[36].",
    "next": "While important progress has been made on some of these metadata categories individually, these categories are typically addressed in isolation reflecting an overly strong assumption that only one, known issue is at play in a given dataset."
  },
  {
    "index": 707,
    "source_corpus_id": 247222761,
    "ref_id": "b26",
    "citation_corpus_id": 3507990,
    "start": 25089,
    "end": 25107,
    "prev": "Furthermore to ensure that T > 0, we parametrized it with an exponential activation.",
    "curr": "FEW-SHOT IMAGE CLASSIFICATION\n\nWe evaluate COMLN on two standard few-shot image classification benchmarks: the miniImageNet (Vinyals et al., 2016) and the tieredImageNet datasets (Ren et al., 2018), both datasets being derived from ILSVRC-2012 (Russakovsky et al., 2015).",
    "next": "The process for creating tasks follows the standard procedure from the few-shot classification literature (Santoro et al., 2016), with distinct classes between the different splits."
  },
  {
    "index": 708,
    "source_corpus_id": 218889280,
    "ref_id": "b6",
    "citation_corpus_id": 848112,
    "start": 2128,
    "end": 2131,
    "prev": "The interactions between learning agents make for vastly more complex mechanics: naively applying gradient descent on each loss is known to diverge even in simple bilinear games.",
    "curr": "Related Work A large number of methods have been proposed recently to tackle the question of local convergence, or global convergence in the convex setting: adaptations of single-loss algorithms such as Extragradient (EG) [2] and Optimistic Mirror Descent (OMD) [7], Consensus Optimization (CO) for GAN training [20], Competitive Gradient Descent (CGD) based on solving a bilinear approximation of the loss functions [26], Local Symplectic Surgery (LSS) for finding local Nash equilibria [19], Symplectic Gradient Adjustment (SGA) based on a novel decomposition of game mechanics [4,16], and opponent-shaping algorithms including Learning with Opponent-Learning Awareness (LOLA) [10] and its convergent counterpart, Stable Opponent Shaping (SOS) [17].",
    "next": "Each has shown promising theoretical implications and empirical results, but none offers insight into global convergence in the non-convex setting, which includes the vast majority of machine learning applications."
  },
  {
    "index": 709,
    "source_corpus_id": 249209577,
    "ref_id": "b2",
    "citation_corpus_id": 219558760,
    "start": 10125,
    "end": 10144,
    "prev": "Modern approaches implement variations of this idea as differentiable neural architectures (Gori et al., 2005;Scarselli et al., 2008;Kipf & Welling, 2017;Gilmer et al., 2017).",
    "curr": "This allows to express more elaborate forms of propagation (Li et al., 2018;Alon & Yahav, 2021) and aggregation (Wu et al., 2019;Xu et al., 2019;Li et al., 2016), including attention-based mechanisms (Veličković et al., 2018;Brody et al., 2022).",
    "next": "Nonetheless, a key result by Wu et al."
  },
  {
    "index": 711,
    "source_corpus_id": 247292293,
    "ref_id": "b47",
    "citation_corpus_id": 3292002,
    "start": 25827,
    "end": 25852,
    "prev": "3(c), we compare the test accuracy averaged on eight graphs when using different GNNs e.g.",
    "curr": "GCN, SGC (Wu et al., 2019) and GAT (Velickovic et al., 2018), for data generation (See Appendix G for more results).",
    "next": "The results verify that our approach achieves consistently superior performance in different cases."
  },
  {
    "index": 713,
    "source_corpus_id": 245334722,
    "ref_id": "b6",
    "citation_corpus_id": 6628106,
    "start": 32657,
    "end": 32676,
    "prev": "The only exception of this is the fine-tuning done for the few-shot learning experiment in Section 7.",
    "curr": "We use Adam (Kingma & Ba, 2015) and cosine decay (Loshchilov & Hutter, 2016) with warmup.",
    "next": "For all experiments we used a embedding size of 512, only for few-shot classification we used 1024."
  },
  {
    "index": 719,
    "source_corpus_id": 208268589,
    "ref_id": "b26",
    "citation_corpus_id": 5037032,
    "start": 7227,
    "end": 7247,
    "prev": "• Combine the local alignment and global alignment components into a reinforcement learning framework by a regularized policy update objective.",
    "curr": "RELATED WORK\n\nImitation learning is widely used in solving complicated tasks where pure reinforcement learning might suffer from high sample complexity, like robotics control (Le et al., 2017;Ye & Alterovitz, 2017;Pathak et al., 2018), autonomous vehicle (Fu et al.",
    "next": ";Pomerleau, 1989), and playing video game Pohlen et al., 2018;Aytar et al., 2018a)."
  },
  {
    "index": 720,
    "source_corpus_id": 252280667,
    "ref_id": "b1",
    "citation_corpus_id": 7167114,
    "start": 67362,
    "end": 67382,
    "prev": "GWAE (GMP), AUC=1.0000.",
    "curr": "Figure 3 :\n3The ROC curves of the OoD detection inMNIST (LeCun et al., 1998) against Omniglot (Lake et al., 2015\n\nFrom\nthe Information Bottleneck (IB)(Tishby et al., 1999)  point of view, the β-VAE objective is re-interpreted as the following optimization problem(Alemi et al., 2018;Achille & Soatto, 2018): maximize θ,φ I φ (z; y)\n\n\nZhao et al., 2019)  is an extension of VAE to prevent posterior collapse by the retention of data information in the latent variables.",
    "next": "The InfoVAE objective is the sum of the ELBO and the inference model mutual information I φ in Eq."
  },
  {
    "index": 722,
    "source_corpus_id": 258480276,
    "ref_id": "b14",
    "citation_corpus_id": 238583580,
    "start": 12824,
    "end": 12840,
    "prev": "SegGPT (Wang et al., 2023) introduce a robust in-context learning paradigm and can segment any images by a given image-mask prompt.SEEM (Zou et al., 2023) further presents a general segmentation model prompted by multi-modal references, e.g., language and audio, incorporating versatile semantic knowledge.In this study, we introduce a new task termed personalized object segmentation, and annotate a new dataset PerSeg for evaluation.Instead of developing large segmentation models, our goal is to personalize them to segment user-provided objects in any poses or scenes.We propose two approaches, PerSAM and PerSAM-F, which efficiently customize SAM for personalized segmentation.",
    "curr": "Parameter-efficient Fine-tuning.Directly tuning the entire foundation models on downstream tasks can be computationally expensive and memory-intensive, posing challenges for resourceconstrained applications.To address this issue, recent works have focused on developing parameterefficient methods (Sung et al., 2022;He et al., 2022;Rebuffi et al., 2017;Qin & Eisner, 2021) to freeze the weights of foundation models and append small-scale modules for fine-tuning.Prompt tuning (Lester et al., 2021;Zhou et al., 2022;Jia et al., 2022;Liu et al., 2021) suggests using learnable soft prompts alongside frozen models to perform specific downstream tasks, achieving more competitive performance with scale and robust domain transfer compared to full model tuning.",
    "next": "Low-Rank Adaption (LoRA) (Hu et al., 2021;Cuenca & Paul, 2023;Zhang et al., 2023b;Hedegaard et al., 2022) injects trainable rank decomposition matrices concurrently to each pre-trained weight, which significantly reduces the number of learnable parameters required for downstream tasks.Adapters (Houlsby et al., 2019;Pfeiffer et al., 2020;Lin et al., 2020;Chen et al., 2022) are designed to be inserted between layers of the original transformer, introducing lightweight MLPs for feature transformation.Different from existing works, we adopt a more efficient adaption met"
  },
  {
    "index": 724,
    "source_corpus_id": 2721941,
    "ref_id": "b0",
    "citation_corpus_id": 748227,
    "start": 2947,
    "end": 2968,
    "prev": "We find particularly promising the opportunity to leverage deep generative models for search in high-dimensional discrete spaces (Gómez-Bombarelli et al., 2016b;Kusner et al., 2017).",
    "curr": "Discrete search is at the heart of problems in drug discovery (Gómez-Bombarelli et al., 2016a), natural language processing (Bowman et al., 2016;Guimaraes et al., 2017), and symbolic regression (Kusner et al., 2017).",
    "next": "The application of deep modeling to search involves 'lifting' the search from the discrete space to a continuous space, via an autoencoder (Rumelhart et al., 1985)."
  },
  {
    "index": 727,
    "source_corpus_id": 261100669,
    "ref_id": "b6",
    "citation_corpus_id": 246652381,
    "start": 2548,
    "end": 2550,
    "prev": "del, our defense, BaDExpert (Backdoor Input Detection with Backdoor Expert), effectively mitigates 17 SOTA backdoor attacks while minimally impacting clean utility.The effectiveness of BaDExpert has been verified on multiple datasets (CIFAR10, GTSRB, and ImageNet) across multiple model architectures (ResNet, VGG,\n\nIntroduction\n\nA prominent security concern of deep neural networks (DNNs) is the threat of backdoor attacks [1,2], wherein an adversary embeds hidden behaviors (backdoors) into a model through techniques such as data poisoning [3] or weights tampering [4].During inference, such a backdoor remains dormant when processing benign inputs but can be activated by trigger-planted backdoor samples devised by attackers.Upon activation, the compromised model produces anomalous outputs, which could lead to severe security breaches.",
    "curr": "The existing literature has extensively explored defensive strategies against backdoor attacks, with a significant focus on development-stage defenses [5,6,7,8].These defenses are operated before and during the model training process, primarily targeting data-poisoning-based attacks [3].",
    "next": "In this work, we rather focus on post-development defenses that operate after the model development [9,10,11,12].Given an arbitrary model that may potentially be backdoored, post-development defenses tackle the challenge of secure deployment head-on, without knowing how the model was generated.Implementing such defenses faces non-trivial technical challenges.From a methodological point of view, these defenses do not have access to the training dataset or information about training dynamics (such as gradient updates or loss information) and thus forfeit rich information that could aid in system defense.For example, approaches that directly analyze poisoned datasets [5,8] or the backdoor training dynamics [6,7] cannot be applied."
  },
  {
    "index": 733,
    "source_corpus_id": 13046179,
    "ref_id": "b7",
    "citation_corpus_id": 6706414,
    "start": 1596,
    "end": 1621,
    "prev": "INTRODUCTION\n\nWhen machine learning classifiers are employed in real-world tasks, they tend to fail when the training and test distributions differ.",
    "curr": "Worse, these classifiers often fail silently by providing highconfidence predictions while being woefully incorrect (Goodfellow et al., 2015;Amodei et al., 2016).",
    "next": "Classifiers failing to indicate when they are likely mistaken can limit their adoption or cause serious accidents."
  },
  {
    "index": 734,
    "source_corpus_id": 254877510,
    "ref_id": "b6",
    "citation_corpus_id": 52967399,
    "start": 1882,
    "end": 1903,
    "prev": "g data sets to obtain a single model that performs well both across all data set domains and can generalize on out-ofdomain data.We propose a dataless knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models.Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling.Further, we find that our method is a promising alternative to multi-task learning that can preserve or sometimes improve over the individual models without access to the training data.Finally, model merging is more efficient than training a multi-task model, thus making it applicable to a wider set of scenarios.",
    "curr": "1\n\nINTRODUCTION\n\nThe dominant paradigm for solving NLP tasks ranging from classification to sequence tagging involves fine-tuning a pretrained language model (PLM) using task-specific labeled data (Devlin et al., 2019;He et al., 2021).This results in specialized models that are explicitly trained to run inference over a single domain and task.Multi-task learning has shown that leveraging information across domains or tasks can be beneficial if the data sets, data set size and algorithms are well selected (Phang et al., 2018;Pruksachatkun et al., 2020;Poth et al., 2021;Weller et al., 2022).Combining knowledge of multiple data sets in a single model can lead to better overall performance on in-domain data (Poth et al., 2021), can better generalize on out-of-domain data (Wang et al., 2020b) and results in a model that is more practical and parameter efficient than maintaining specialized models.",
    "next": "However, the multi-task learning setup suffers from two practical limitations.First, the training process requires access to the original labeled data, which may not be realistic as annotated data may be private to the agent fine-tuning the model which can happen in order to ensure data or annotation privacy o"
  },
  {
    "index": 735,
    "source_corpus_id": 13298214,
    "ref_id": "b13",
    "citation_corpus_id": 8395799,
    "start": 12275,
    "end": 12299,
    "prev": "Recent work has developed on-policy RL methods such as advantage actor-critic that use asynchronous training of multiple agents in parallel (Mnih et al., 2016).",
    "curr": "Recurrent networks have also been successfully incorporated to enable state disambiguation in partially observable environments (Koutnik et al., 2013;Hausknecht & Stone, 2015;Mnih et al., 2016;Narasimhan et al., 2015).",
    "next": ", 1999)) to transfer between navigation tasks."
  },
  {
    "index": 736,
    "source_corpus_id": 1257772,
    "ref_id": "b0",
    "citation_corpus_id": 11212020,
    "start": 1804,
    "end": 1826,
    "prev": "We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.",
    "curr": "INTRODUCTION\n\nRecent advances in machine learning and deep neural networks enabled researchers to solve multiple important practical problems like image, video, text classification and others (Krizhevsky et al., 2012;Hinton et al., 2012;Bahdanau et al., 2015).",
    "next": "However, machine learning models are often vulnerable to adversarial manipulation of their input intended to cause incorrect classification (Dalvi et al., 2004)."
  },
  {
    "index": 738,
    "source_corpus_id": 259274820,
    "ref_id": "b0",
    "citation_corpus_id": 235436185,
    "start": 25378,
    "end": 25381,
    "prev": "is typically utilized for downstream initialization.",
    "curr": "Except for the common supervised pre-training [16,10,24], contrastive learning (CL) [4,14,6,12] and masked image modeling (MIM) [1,44,13] dominate the recent research.",
    "next": "The former is achieved by pulling close the features of two different augment views of the input image."
  },
  {
    "index": 739,
    "source_corpus_id": 265037895,
    "ref_id": "b14",
    "citation_corpus_id": 3366315,
    "start": 2403,
    "end": 2423,
    "prev": "The original GAN is, however, highly unstable and often suffers from mode collapse.",
    "curr": "Much of recent researches has focused on improving the stability of GANs (Radford et al., 2015;Heusel et al., 2017;Miyato et al., 2018;Karras et al., 2018).",
    "next": "On the theoretical aspect, Nagarajan & Kolter (2017) proved that gradient based training of the original GAN is locally stable."
  },
  {
    "index": 740,
    "source_corpus_id": 56895453,
    "ref_id": "b33",
    "citation_corpus_id": 2705742,
    "start": 8430,
    "end": 8449,
    "prev": "However, they have non-symmetric addition of agents in communication channel and are restricted to only cooperative scenarios.",
    "curr": "In contrast, a lot of work has focused on understanding agents' communication content; mostly in discrete settings with two agents (Wang et al., 2016;Havrylov & Titov, 2017;Kottur et al., 2017;Lazaridou et al., 2016;.",
    "next": "Lazaridou et al."
  },
  {
    "index": 741,
    "source_corpus_id": 252917667,
    "ref_id": "b6",
    "citation_corpus_id": 231698861,
    "start": 7500,
    "end": 7503,
    "prev": "Together with Dasoulas et al.",
    "curr": "[7] and Sahbi [33], our work can be listed as a theoretically grounded way to learn the GSO.",
    "next": "Justified by formulas grounded in Monte-Carlo analysis, we show how to compensate for the nonuniformity in the sampling when computing non-uniform geometric GSOs."
  },
  {
    "index": 745,
    "source_corpus_id": 259212224,
    "ref_id": "b5",
    "citation_corpus_id": 208547755,
    "start": 2027,
    "end": 2030,
    "prev": "Introduction\n\nThe ability to predict the outcome of an agent's action over long horizons is a crucial unresolved challenge in Reinforcement Learning (RL) [1][2][3][4].",
    "curr": "This is especially important in model-based RL and planning, where deriving a policy from the learned dynamics models allows one to efficiently accomplish a wide variety of tasks in an environment [5][6][7][8][9].",
    "next": "In fact, state-of-the-art model-free techniques also rely on dynamics models to learn a better representation for downstream value prediction tasks [30]."
  },
  {
    "index": 749,
    "source_corpus_id": 239016655,
    "ref_id": "b2",
    "citation_corpus_id": 231648113,
    "start": 23035,
    "end": 23062,
    "prev": "Experimental setup details are included in Appendix A.3.",
    "curr": "To align with the experimental setup of prior work (Abdelfattah et al., 2021b;Mellor et al., 2021), we use two criteria to evaluate the correlations between different metrics and test accuracies across approximately 20k networks:\n\nSpearman's ρ (Daniel et al., 1990) characterizes the monotonic relationships between two variables.",
    "next": "The correlation score is restricted in range [-1, 1], where ρ = 1 denotes a perfect positive monotonic relationship and ρ = −1 denotes a perfect negative monotonic relationship."
  },
  {
    "index": 752,
    "source_corpus_id": 232290577,
    "ref_id": "b43",
    "citation_corpus_id": 196183669,
    "start": 10921,
    "end": 10940,
    "prev": "Different from this work, MARS is built upon the general MCMC sampling framework, which allows further enhancement with adaptive proposal learning to edit molecular graphs efficiently.",
    "curr": "Actually, generating instances from a discrete space with MCMC sampling methods is previously employed in various other applications, e.g., generating natural language sentences under various constraints Zhang et al., 2019;.",
    "next": "PROPOSED MARS APPROACH\n\nIn this section, we present the MArkov moleculaR Sampling method (MARS) for multi-objective molecular design."
  },
  {
    "index": 753,
    "source_corpus_id": 43964415,
    "ref_id": "b10",
    "citation_corpus_id": 38796293,
    "start": 1781,
    "end": 1784,
    "prev": "Communication has been reported to be a major bottleneck for many large scale deep learning applications, see e.g.",
    "curr": "[3,11,24,31].",
    "next": "Mini-batch parallel SGD addresses this issue by increasing compute before communication."
  },
  {
    "index": 754,
    "source_corpus_id": 251765117,
    "ref_id": "b27",
    "citation_corpus_id": 218487034,
    "start": 3201,
    "end": 3222,
    "prev": "respects; for example, they are not able to retroactively modify or refine their own outputs.",
    "curr": "Beyond that, they are hard to control (Korbak et al., 2022) and verifying their outputs is challenging as they often hallucinate content (Maynez et al., 2020;Shuster et al., 2021;Nakano et al., 2021) and lack the ability to explain their intentions.",
    "next": "All of this makes it very difficult for humans to collaborate with such models for writing coherent, factual texts."
  },
  {
    "index": 755,
    "source_corpus_id": 235254358,
    "ref_id": "b43",
    "citation_corpus_id": 52895589,
    "start": 2361,
    "end": 2377,
    "prev": "In a GNN, each node iteratively updates its state by interacting with its neighbors.",
    "curr": "GNN variants (Wu et al., 2019;Xu et al., 2019;Li et al., 2016) mostly differ in how each node aggregates and combines the representations of its neighbors with its own.",
    "next": "pioneered the use of attention-based neighborhood aggregation, in one of the most common GNN variants -Graph Attention Network (GAT)."
  },
  {
    "index": 759,
    "source_corpus_id": 264289264,
    "ref_id": "b44",
    "citation_corpus_id": 247446904,
    "start": 9874,
    "end": 9893,
    "prev": "mising methods in the realm of protein design, which encompasses protein diffusion (Yim et al., 2023) and inverse folding (Gao et al., 2022a;Jendrusch et al., 2021;Wu et al., 2021;Ovchinnikov & Huang, 2021;Dauparas et al., 2022;Ingraham et al., 2019;Hsu et al., 2022;Gao et al., 2023b;Derevyanko et al., 2018).Specifically, a protein diffusion model first generates the backbone structure of a protein, followed by an inverse folding network that designs the corresponding sequence for this backbone.The feasibility of both these steps has been experimentally validated through cryo-electron microscopy (Watson et al., 2023;Dauparas et al., 2022), marking a significant breakthrough in the field of protein design.However, while protein diffusion methods based on frame representation achieve significant success, in these methods, atom representation is absent, rendering previous general purpose encoders unusable.",
    "curr": "GENERAL PURPOSE ENCODER\n\nIn the past, numerous encoders (Hermosilla et al., 2020;Zhang et al., 2022;Hermosilla & Ropinski, 2022;Veličković et al., 2017;Baldassarre et al., 2021;Li et al., 2022;Gao et al., 2022b;Shroff et al., 2019;Dumortier et al., 2022;McPartlon et al., 2022;Cao et al., 2021;Anishchenko et al., 2021;Karimi et al., 2020;Zhang et al., 2020;Wang et al., 2022b;Derevyanko et al., 2018) have been proposed for tasks such as model quality assessment (Townshend et al., 2021) and fold classification (Hou et al., 2018), where atomic information is available.However, these methods are not suitable for protein design tasks where atomic representations of proteins are unavailable.For instance, GVP (Jing et al., 2020) transforms input atomic coordinates into vector and scalars variables as the network input, facilitating the model's SE(3) invariance.Meanwhile, Wang et al.",
    "next": "(2022a); Jin et al."
  },
  {
    "index": 760,
    "source_corpus_id": 244527086,
    "ref_id": "b17",
    "citation_corpus_id": 3566136,
    "start": 12140,
    "end": 12169,
    "prev": "We minimize the pixel-wise squared reconstruction error (averaged over the batch), summed over both the temporal and spatial dimensions:\nL rec = T t=1 y t − y true t 2 .",
    "curr": "(4)\n\nRELATED WORK\n\nObject-centric representation learning There is a rich literature on learning object representations from static scenes (Greff et al., 2016;Eslami et al., 2016;Greff et al., 2017;2019;Burgess et al., 2019;Engelcke et al., 2020;Crawford & Pineau, 2019;Lin et al., 2020;Locatello et al., 2020;Du et al., 2021a) or videos (van Steenkiste et al., 2018;Kosiorek et al., 2018;Stelzner et al., 2019;Kipf et al., 2020;Crawford & Pineau, 2020;Creswell et al., 2021) without explicit supervision.",
    "next": "PSGNet (Bear et al., 2020) learns to decompose static images or individual frames from a video into hierarchical scene graphs using motion information estimated from neighboring video frames."
  },
  {
    "index": 761,
    "source_corpus_id": 258762142,
    "ref_id": "b10",
    "citation_corpus_id": 251252882,
    "start": 9708,
    "end": 9711,
    "prev": "This leads to the emergence of the recently popular topic, subject-driven text-to-image generation [8,27].",
    "curr": "Text-Guided Image Editing Text-guided image editing [2,3,11,15,17,20] aims to edit an input image according to the given textual descriptions.",
    "next": "SDEdit [20] and Blended-Diffusion [1] blend the noisy input to the generated image in the diffusion denoising process."
  },
  {
    "index": 764,
    "source_corpus_id": 108296236,
    "ref_id": "b42",
    "citation_corpus_id": 1957433,
    "start": 2187,
    "end": 2212,
    "prev": "INTRODUCTION\n\nLearned embeddings form the basis for many state-of-the-art learning systems.",
    "curr": "Word embeddings like word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), fastText (Bojanowski et al., 2017), and ELMo (Peters et al., 2018) are ubiquitous in natural language processing, where they are used for tasks like machine translation (Neubig et al., 2018), while graph embeddings (Nickel et al., 2016) like node2vec (Grover & Leskovec, 2016) are used to represent knowledge graphs and pre-trained image models (Simon et al., 2016) appear in many computer vision pipelines.",
    "next": "An effective embedding should capture the semantic structure of the data with high fidelity, in a way that is amenable to downstream tasks."
  },
  {
    "index": 768,
    "source_corpus_id": 245704504,
    "ref_id": "b9",
    "citation_corpus_id": 13890001,
    "start": 18181,
    "end": 18200,
    "prev": "GANs inversion and editing.",
    "curr": "Another mainstream approach to image editing involves GAN inversion (Zhu et al., 2016;Brock et al., 2017), where the input is first projected into the latent space of an unconditional GAN before synthesizing a new image from the modified latent code.",
    "next": "Several methods have been proposed in this direction, including fine-tuning network weights for each image (Bau et al., 2019a;Pan et al., 2020;Roich et al., 2021), choosing better or multiple layers to project and edit (Abdal et al., 2019;Gu et al., 2020;Wu et al., 2021), designing better encoders (Richardson et al., 2021;Tov et al., 2021), modeling image corruption and transformations (Anirudh et al., 2020;Huh et al., 2020), and discovering meaningful latent directions (Shen et al., 2020;Goetschalckx et al., 2019;Jahanian et al., 2020;Härkönen et al., 2020)."
  },
  {
    "index": 771,
    "source_corpus_id": 220265858,
    "ref_id": "b38",
    "citation_corpus_id": 6359641,
    "start": 40818,
    "end": 40822,
    "prev": "As a result, the run-time overhead is typically negligible, even for convolutional networks where masking and padding are heavily used.",
    "curr": "We chose multilingual neural machine translation (MT) [39,40,41] to validate our design for efficient training with GShard.",
    "next": "Multilingual MT, which is an inherently multi-task learning problem, aims at building a single neural network for the goal of translating multiple language pairs simultaneously."
  },
  {
    "index": 773,
    "source_corpus_id": 3047732,
    "ref_id": "b34",
    "citation_corpus_id": 3281198,
    "start": 3782,
    "end": 3809,
    "prev": "For example, one may want simultaneously predict a person's age (task one: multi-class classification or regression) as well as identify their gender (task two: binary classification) from a face image.",
    "curr": "Multi-Domain Learning: Each \"task\" corresponds to a dataset [Yang and Hospedales, 2015].",
    "next": "For example, one jointly can train a multi-class object recognition model for images captured by an HD camera (task/domain one) and for those captured by a webcam (task/domain two)."
  },
  {
    "index": 774,
    "source_corpus_id": 233474778,
    "ref_id": "b15",
    "citation_corpus_id": 14337532,
    "start": 27296,
    "end": 27323,
    "prev": "Our proposed principles are preferable guidance for designing an efficient module for temporal modeling.",
    "curr": "We use SGD with momentum 0.9 and cosine learning rate schedule (Loshchilov & Hutter, 2017) to train the entire network.",
    "next": "The first 10 epochs are used for warm-up (Goyal et al., 2017a) to overcome early optimization difficulty."
  },
  {
    "index": 775,
    "source_corpus_id": 247778993,
    "ref_id": "b4",
    "citation_corpus_id": 54458698,
    "start": 1952,
    "end": 1978,
    "prev": "Our experiments show that the proposed approach consistently improves existing methods, obtains good out-of-distribution generalization, and achieves new state-of-the-art results on widely used environments.",
    "curr": "INTRODUCTION\n\nText-based games (TBGs) have emerged as key benchmarks for studying how reinforcement learning (RL) agents can tackle the challenges of grounded language understanding, partial observability, large action spaces, and out-of-distribution generalization Ammanabrolu & Riedl, 2019).",
    "next": "While we have indeed made some progress on these fronts in recent years Adhikari et al., 2020;Murugesan et al., 2021b;a), these agents are still very inefficient and suffer from insufficient generalization to novel environments."
  },
  {
    "index": 779,
    "source_corpus_id": 14711954,
    "ref_id": "b2",
    "citation_corpus_id": 3130692,
    "start": 5513,
    "end": 5535,
    "prev": "We show that our approach substantially outperforms purely unsupervised methods that do not provide the learner with any task-specific guidance about how hierarchies should be deployed, and further that the specific use of sketches to parameterize modular subpolicies makes better use of sketches than conditioning on them directly.",
    "curr": "The present work may be viewed as an extension of recent approaches for learning compositional deep architectures from structured program descriptors (Andreas et al., 2016;Reed & de Freitas, 2016).",
    "next": "Here we focus on learning in interactive environments."
  },
  {
    "index": 781,
    "source_corpus_id": 247849778,
    "ref_id": "b15",
    "citation_corpus_id": 236459945,
    "start": 2063,
    "end": 2081,
    "prev": "INTRODUCTION\n\nText style transfer (TST) models learn how to transfer the style of text from source to target while preserving the style-independent content (John et al., 2019;Fu et al., 2018).",
    "curr": "Existing TST methods perform well when transferring simple styles, such as sentiment; however, they tend to do a poor job on more abstract and subtle styles, such as formality and political stance (Lee et al., 2021;Fu et al., 2019b).",
    "next": "The lack of parallel datasets is one of the main bottlenecks for text style transfer tasks."
  },
  {
    "index": 783,
    "source_corpus_id": 12130431,
    "ref_id": "b18",
    "citation_corpus_id": 14124313,
    "start": 2824,
    "end": 2851,
    "prev": "We believe that our method sheds new insights on how to make deep CNNs to be applicable on mobile or embedded devices.",
    "curr": "The code is available at https://github.com\n\nINTRODUCTION\n\nDeep convolutional neural networks (CNNs) have demonstrated record breaking results on a variety of computer vision tasks such as image classification (Krizhevsky et al., 2012;Simonyan & Zisserman, 2015), face recognition (Taigman et al., 2014;Sun et al., 2014), semantic segmentation (Long et al., 2015;Chen et al., 2015a) and object detection (Girshick, 2015;Ren et al., 2015).",
    "next": "Regardless of the availability of significantly improved training resources such as abundant annotated data, powerful computational platforms and diverse training frameworks, the promising results of deep CNNs are mainly attributed to the large number of learnable parameters, ranging from tens of millions to even hundreds of millions."
  },
  {
    "index": 785,
    "source_corpus_id": 204824061,
    "ref_id": "b3",
    "citation_corpus_id": 52967399,
    "start": 1224,
    "end": 1244,
    "prev": "* Work done during an internship at Facebook AI Research.",
    "curr": "INTRODUCTION\n\nThe size of modern neural sequence models (Gehring et al., 2017;Vaswani et al., 2017;Devlin et al., 2019) can amount to billions of parameters (Radford et al., 2019).",
    "next": "For example, the winning entry of the WMT'19 news machine translation task in English-German used an ensemble totaling two billion parameters ."
  },
  {
    "index": 786,
    "source_corpus_id": 238419267,
    "ref_id": "b6",
    "citation_corpus_id": 11212020,
    "start": 8264,
    "end": 8287,
    "prev": "THE PROBABILISTIC ATTENTION MECHANISM\n\nWe now give a high-level explanation of our results; the detailed formulations are in Section 2.",
    "curr": "Introduced in (Bahdanau et al., 2015) and later used to define the transformer architecture (Vaswani et al., 2017), in the NLP context, attention maps a matrix of queries Q, a matrix of keys K, and a matrix of values V to the quantity Softmax(QK )V , where the softmax function (defined below) is applied row-wise to QK .",
    "next": "Just as the authors of (Petersen & Voigtlaender, 2020;Zhou, 2020) focus on the simplified versions of practically implementable ConvNets in the study of approximation theory of deep ConvNets (e.g."
  },
  {
    "index": 787,
    "source_corpus_id": 88517649,
    "ref_id": "b8",
    "citation_corpus_id": 52889459,
    "start": 2223,
    "end": 2226,
    "prev": "By capturing the mechanisms behind the data generation process, one can reason about data probabilistically, access and traverse the low-dimensional manifold the data is assumed to live on, and ultimately generate new data.",
    "curr": "It is therefore not surprising that learning generative models has gained momentum in applications like chemistry [16,25], NLP [8,46] and computer vision [9,48].",
    "next": "Variational Autoencoders (VAEs) [27,38] allow for a principled probabilistic way to model high-dimensional distributions."
  },
  {
    "index": 789,
    "source_corpus_id": 252118863,
    "ref_id": "b12",
    "citation_corpus_id": 52967399,
    "start": 6941,
    "end": 6945,
    "prev": "Early practices revolve around contrastive learning [6-8, 18, 20] where the model output features of images transformed by different data augmentations are pulled together.",
    "curr": "With the development of Masked Language Modeling (MLM) in language pre-training [13], researchers also introduce the training strategy of masked reconstruction to visual pre-training.",
    "next": "BEiT [3] uses the DALL-E [32] to encode an image patch as the target for model reconstruction."
  },
  {
    "index": 793,
    "source_corpus_id": 204907203,
    "ref_id": "b7",
    "citation_corpus_id": 52967399,
    "start": 2655,
    "end": 2676,
    "prev": "Besides theft of intellectual property, extracted models may leak sensitive information about the training data (Tramèr et al., 2016) or be used to generate adversarial examples that evade the model served by the API (Papernot et al., 2017).",
    "curr": "With the recent success of contextualized pretrained representations for transfer learning, NLP models created by finetuning ELMo  and BERT (Devlin et al., 2019) have become increasingly popular .",
    "next": "Contextualized pretrained representations boost performance and reduce sample complexity (Yogatama et al., 2019), and typically require only a shallow task-specific network-sometimes just a single layer as in BERT."
  },
  {
    "index": 794,
    "source_corpus_id": 238408158,
    "ref_id": "b39",
    "citation_corpus_id": 3587087,
    "start": 1885,
    "end": 1904,
    "prev": "INTRODUCTION\n\nWe consider the most general form of entity linking (EL) in which a system, given a document, must both extract entity mentions and link the mentions to their corresponding entries in a knowledge base (KB).",
    "curr": "EL is a foundational building block in automatic text understanding with applications to question answering (QA) (Ferrucci, 2012), information retrieval (Xiong et al., 2017;Hasibi et al., 2016;Balog et al., 2013;Reinanda et al., 2015), and commercial recommendation systems (Yang et al., 2018;Slawski, 2015).",
    "next": "The output space in EL is intractably large."
  },
  {
    "index": 795,
    "source_corpus_id": 254096162,
    "ref_id": "b18",
    "citation_corpus_id": 235313882,
    "start": 10712,
    "end": 10731,
    "prev": "By using neural networks to learn the transformations from raw feature values to effects, Neural Additive Models (NAMs) maintain the interpretability of GAMs with more expressive effects (Agarwal et al., 2021).",
    "curr": "Pairwise interactions between effects can be included for more complex models (Yang et al., 2021;Chang et al., 2022), though complications arise involving degeneracy and the number of effect terms to inspect grows rapidly.",
    "next": "Regularization can be used to encourage sparsity in the ML model so that fewer components-weights in the network (Ng, 2004) or even features (Lemhadri et al., 2021)-conspire to produce the final prediction."
  },
  {
    "index": 796,
    "source_corpus_id": 257482484,
    "ref_id": "b13",
    "citation_corpus_id": 233169183,
    "start": 3858,
    "end": 3877,
    "prev": "INTRODUCTION\n\nDeformable object manipulation (DOM) is a crucial area of research with broad applications, from household (Maitin-Shepard et al., 2010;Miller et al., 2011;Ma et al., 2022) to industrial settings (Miller et al., 2012;.",
    "curr": "To aid in algorithm development and prototyping, several DOM benchmarks Huang et al., 2021) have been developed using deformable object simulators.",
    "next": "However, the high dimensional state and action spaces remain a significant challenge to DOM."
  },
  {
    "index": 798,
    "source_corpus_id": 252918265,
    "ref_id": "b34",
    "citation_corpus_id": 3725815,
    "start": 6802,
    "end": 6821,
    "prev": "(2006) (CTC) loss, identical SpecAugment (Park et al., 2019) parameters, and Adagrad optimizer (Duchi et al., 2011).",
    "curr": "The acoustic model is the same transformer architecture that was introduced in slimIPL, except that we encode positions with either absolute sinusoidal positional embedding (Vaswani et al., 2017) or the recently proposed CAPE (Likhomanenko et al., 2021b) instead of relative positional embedding (Shaw et al., 2018).",
    "next": "This allows us to speed up training (by 2-3x) and decrease the memory footprint significantly."
  },
  {
    "index": 799,
    "source_corpus_id": 240070972,
    "ref_id": "b30",
    "citation_corpus_id": 13900194,
    "start": 5143,
    "end": 5162,
    "prev": "The classic threat model of adversarial robustness considers an inductive setting where a model is learned at the training time and fixed, and then at the test time, an attacker attempts to thwart the fixed model with adversarially perturbed input.",
    "curr": "This gives rise to the adversarial training Sinha et al., 2018;Carmon et al., 2019) to enhance adversarial robustness.",
    "next": "Going beyond the inductive threat model, there has been emerging interest in using transductive learning (Vapnik, 1998) 1 for adversarial robustness (Goldwasser et al., 2020;Wu et al., 2020b;Wang et al., 2021)."
  },
  {
    "index": 800,
    "source_corpus_id": 235731899,
    "ref_id": "b9",
    "citation_corpus_id": 3708505,
    "start": 2281,
    "end": 2299,
    "prev": "Neal (1996) showed that a shallow but infinitely-wide Bayesian Neural Network (BNN) with random weights and biases corresponds to a GP.",
    "curr": "This result was extended to fully-connected deep neural networks of any depth (Lee et al., 2018;Matthews et al., 2018), which are shown to converge to GPs as the width grows.",
    "next": "Similar results were later obtained for deep Convolutional Neural Networks (CNNs) (Novak et al., 2018;Garriga-Alonso et al., 2019) and attention networks (Hron et al., 2020)."
  },
  {
    "index": 801,
    "source_corpus_id": 54203451,
    "ref_id": "b7",
    "citation_corpus_id": 20472740,
    "start": 2023,
    "end": 2042,
    "prev": "INTRODUCTION\n\nIn recent years, neural network based models have become the workhorse of natural language understanding and generation.",
    "curr": "They empower industrial systems in machine translation (Wu et al., 2016) and text generation (Kannan et al., 2016), also showing state-of-the-art performance on numerous benchmarks including Recognizing Textual Entailment (RTE) (Gong et al., 2017), Visual Question Answering (VQA) (Jiang et al., 2018), and Reading Comprehension (Wang et al., 2018).",
    "next": "Despite these successes, a growing body of literature suggests that these approaches do not generalize outside of the specific distributions on which they are trained, something that is necessary for a language understanding system to be widely deployed in the real world."
  },
  {
    "index": 802,
    "source_corpus_id": 221761540,
    "ref_id": "b26",
    "citation_corpus_id": 211146562,
    "start": 3602,
    "end": 3619,
    "prev": "While increasing the size of data does improve the model's robustness to noise, our method can substantially boost the representation learning performance by addressing noise.",
    "curr": "There exists a large body of literature on learning with label noise (Jiang et al., 2018;Han et al., 2018;Guo et al., 2018;Tanaka et al., 2018;Arazo et al., 2019;Li et al., 2020a).",
    "next": "However, existing methods have several limitations that make them less effective for webly-supervised representation learning."
  },
  {
    "index": 803,
    "source_corpus_id": 238408412,
    "ref_id": "b10",
    "citation_corpus_id": 52967399,
    "start": 2423,
    "end": 2444,
    "prev": "7a.",
    "curr": "INTRODUCTION\n\nMultilingual language models (Devlin et al., 2019;Conneau & Lample, 2019;Conneau et al., 2020;Lewis et al., 2020a;Xue et al., 2021) have achieved impressive performance on a variety of multilingual natural language processing (NLP) tasks.",
    "next": "Training a model with multiple languages jointly can be understood as a multi-task learning (MTL) problem where each language serves as a distinct task to be learned (Wang et al., 2021)."
  },
  {
    "index": 804,
    "source_corpus_id": 263909090,
    "ref_id": "b19",
    "citation_corpus_id": 253244266,
    "start": 11341,
    "end": 11358,
    "prev": "Take the \"Take nap\" task as an example, the robot needs to understand that napping can be done on a bed, and the bed is typically located in a bedroom.",
    "curr": "Many works hold the belief that LLMs trained on large-scale data encode commonsense knowledge about the real-world (Davison et al., 2019;Li et al., 2022b;Bian et al., 2023).",
    "next": "Recently, several studies have investigated the integration of LLMs into task planning, which aims to address language ambiguities and provide robots with background knowledge Li et al., 2022a;Ahn et al., 2022)."
  },
  {
    "index": 806,
    "source_corpus_id": 256358781,
    "ref_id": "b28",
    "citation_corpus_id": 236170938,
    "start": 2281,
    "end": 2300,
    "prev": "lems in protein-signaling and transcriptional network discovery, that our approach lies on the Pareto frontier of two key metrics, the SID and SHD.",
    "curr": "INTRODUCTION\n\nIn many domains, including cell biology (Sachs et al., 2005), finance (Sanford & Moosa, 2012), and genetics (Zhang et al., 2013), the data generating process is thought to be represented by an underlying directed acylic graph (DAG).Many models rely on DAG assumptions, e.g., causal modeling uses DAGs to model distribution shifts, ensure predictor fairness among subpopulations, or learn agents more sample-efficiently (Kaddour et al., 2022).A key question, with implications ranging from better modeling to causal discovery, is how to recover this unknown DAG from observed data alone.While there are methods for identifying the underlying DAG if given additional interventional data (Eberhardt, 2007;Hauser & Bühlmann, 2014;Shanmugam et al., 2015;Kocaoglu et al., 2017;Brouillard et al., 2020;Addanki et al., 2020;Squires et al., 2020;Lippe et al., 2022), it is not always practical or ethical to obtain such data (e.g., if one aims to discover links between dietary choices and deadly diseases).",
    "next": "Learning DAGs from observational data alone is fundamentally difficult for two reasons."
  },
  {
    "index": 807,
    "source_corpus_id": 252531622,
    "ref_id": "b15",
    "citation_corpus_id": 220347682,
    "start": 16537,
    "end": 16566,
    "prev": "As mentioned before, time series OOD algorithms are currently underexplored and there are only two recent strong approaches for comparison: GILE (Qian et al., 2021) and AdaRNN (Du et al., 2021).",
    "curr": "5 We further compare with 7 general OOD methods 6 from DomainBed (Gulrajani & Lopez-Paz, 2021): ERM, DANN (Ganin et al., 2016), CORAL (Sun & Saenko, 2016), Mixup (Zhang et al., 2018), GroupDRO (Sagawa et al., 2020), RSC (Huang et al., 2020), and ANDMask (Parascandolo et al., 2021).",
    "next": "More details of these methods are in Sec."
  },
  {
    "index": 808,
    "source_corpus_id": 52135921,
    "ref_id": "b26",
    "citation_corpus_id": 11217889,
    "start": 3258,
    "end": 3283,
    "prev": "It has been shown time and again ) that basically all defenses previously proposed did not increase model robustness but prevented existing attacks from finding minimal adversarial examples, the most common reason being masking of the gradients on which most attacks rely.",
    "curr": "The few verifiable defenses can only guarantee robustness within a small linear regime around the data points (Hein & Andriushchenko, 2017;Raghunathan et al., 2018).",
    "next": "The only defense currently considered effective  is a particular type of adversarial training ."
  },
  {
    "index": 810,
    "source_corpus_id": 259252546,
    "ref_id": "b3",
    "citation_corpus_id": 225039882,
    "start": 8853,
    "end": 8856,
    "prev": ",32].Pioneering deep global motion deblurring works utilize CNNs as basic layers and achieve promising improvements in image quality.Among them, DeepDeblur [16], a multi-scale convolutional neural network, performs residual blocks to increase convergence speed.DeblurGAN [8] and DeblurGAN-v2 [9] introduce GANs and a perceptual loss to improve subjective quality.HINet [2] applies Instance Normalization to boost performance.Recently, a CNN-based local motion deblurring method, LBAG [10], bridges the gap between global and local motion deblurring by inserting gate modules at the end of MIMO-UNet architecture [3].It predicts differentiable blur masks to reduce sharp backgrounds from modifications and guide the network to deblur locally.Although the performance is significantly improved, CNN-based methods suffer from the content-independent interactions between images and convolution kernels, as well as the limitations of long-range dependency modeling.",
    "curr": "Given the Vision Transformer's (ViT) [4] ability to capture long-range dependencies, its application to global deblurring tasks has seen a surge of interest.For example, Uformer [24] employs windowbased self-attention with a learnable multi-scale restoration modulator to capture both local and global dependencies.Restormer [28] utilizes multi-head attention and a feed-forward network to achieve long-range pixel interactions.In this paper, we build a Transformer-based local motion deblurring framework, LMD-ViT, that adaptively selects windows relevant to blurry regions for window-based self-attention and feed-forward operations, simultaneously benefiting from long-range modeling.",
    "next": "Vision Transformer acceleration.Transformers have proven valuable in deblurring tasks, yet their direct application in local motion deblurring for high-resolution images presents challenges concerning computational efficiency.To solve the heavy computation problem of global self-attention in Transformers, researchers have presented several technique"
  },
  {
    "index": 811,
    "source_corpus_id": 108296442,
    "ref_id": "b8",
    "citation_corpus_id": 13276568,
    "start": 2297,
    "end": 2319,
    "prev": "It also empowers applications including visual question answering and bidirectional image-text retrieval.",
    "curr": "INTRODUCTION\n\nHumans are capable of learning visual concepts by jointly understanding vision and language (Fazly et al., 2010;Chrupała et al., 2015;Gauthier et al., 2018).",
    "next": "Consider the example shown in Figure 1-I."
  },
  {
    "index": 812,
    "source_corpus_id": 54477714,
    "ref_id": "b13",
    "citation_corpus_id": 8495258,
    "start": 2224,
    "end": 2245,
    "prev": "To generate a coherent piece, a model needs to reference elements that came before, sometimes in the distant past, repeating, varying, and further developing them to create contrast and surprise.",
    "curr": "Intuitively, self-attention (Parikh et al., 2016) appears to be a good match for this task.",
    "next": "Self-attention over its own previous outputs allows an autoregressive model to access any part of the previously generated output at every step of generation."
  },
  {
    "index": 814,
    "source_corpus_id": 57189211,
    "ref_id": "b26",
    "citation_corpus_id": 3566136,
    "start": 26642,
    "end": 26670,
    "prev": "The insight that static observations are physically stable configurations of objects has been leveraged to improve 3D scene understanding algorithms.",
    "curr": "There has also been much attention on inferring object-factorized, or otherwise disentangled, representations of images (Eslami et al., 2016;Greff et al., 2017;van Steenkiste et al., 2018).",
    "next": "In contrast to works which aim to discover objects in a completely unsupervised manner, we focus on using object representations learned with minimal supervision, in the form of segmentation masks, for downstream tasks."
  },
  {
    "index": 818,
    "source_corpus_id": 2703040,
    "ref_id": "b19",
    "citation_corpus_id": 3144218,
    "start": 4782,
    "end": 4803,
    "prev": "This process is then repeated multiple times like in classical CNNs to build a deep graph representation.",
    "curr": "Other notable works on graph neural networks include (Li et al., 2015;Schütt et al., 2017;Battaglia et al., 2016;Kipf & Welling, 2017).",
    "next": "Very recently, (Gilmer et al., 2017) showed that many of these approaches can be seen to be specific instances of a general message passing formalism, and coined the term message passing neural networks (MPNNs) to refer to them collectively."
  },
  {
    "index": 819,
    "source_corpus_id": 250334789,
    "ref_id": "b32",
    "citation_corpus_id": 3526391,
    "start": 2934,
    "end": 2953,
    "prev": "INTRODUCTION\n\nLearning in the presence of Out-Of-Distribution (OOD) data has been a challenging task in machine learning, as the deployed classifier tends to fail if the unseen data drawn from unknown distributions are not properly handled (Hendrycks & Gimpel, 2017;Pan & Yang, 2009).",
    "curr": "Such a critical problem ubiquitously exists when deep models meet domain shift (Ganin et al., 2016;Tzeng et al., 2017) and unseen-class data (Hendrycks & Gimpel, 2017;Scheirer et al., 2012), which has drawn a lot of attention in some important fields such as OOD detection (Hein et al., 2019;Hendrycks & Gimpel, 2017;Lee et al., 2018;Liang et al., 2018;2023a;, Open-Set Domain Adaptation (DA) Saito et al., 2018), and Open-Set Semi-Supervised Learning (SSL) (Huang et al., 2021b;a;Oliver et al., 2018;Saito et al., 2021;.",
    "next": "In the above fields, OOD data can be divided into two types, namely benign OOD data 1 and malign OOD data."
  },
  {
    "index": 820,
    "source_corpus_id": 246634143,
    "ref_id": "b38",
    "citation_corpus_id": 14124313,
    "start": 4373,
    "end": 4401,
    "prev": "• Experimental results show that our efficient models outperform the previous efficient models and yield faster model speeds with further robustness ( §5).",
    "curr": "INTRODUCTION\n\nImage classification has been advanced with deep convolutional neural networks (Simonyan & Zisserman, 2015;Huang et al., 2017;He et al., 2016b) with the common design paradigm of the network building blocks with trainable spatial convolutions inside.",
    "next": "Such trainable layers with learnable parameters effectively grasp attentive signals to distinguish input but are computationally heavy."
  },
  {
    "index": 821,
    "source_corpus_id": 252070677,
    "ref_id": "b37",
    "citation_corpus_id": 2131938,
    "start": 2278,
    "end": 2282,
    "prev": "INTRODUCTION\n\nKnowledge graphs (KGs) represent a large amount of entities and their relationships via a collection of factual triplets, (ℎ, , ), where each triplet expresses the relationship between a head entity ℎ and a tail entity .",
    "curr": "Large-scale KGs [1,19,26,32] can provide powerful knowledge inference capabilities for many intelligent applications, including question answering [38], web search [8] and recommendation systems [33].",
    "next": "As KGs are often built semiautomatically from unstructured data, it has become a well-known fact that most existing KGs are far from complete [6]."
  },
  {
    "index": 822,
    "source_corpus_id": 54472058,
    "ref_id": "b1",
    "citation_corpus_id": 11212020,
    "start": 1821,
    "end": 1843,
    "prev": "INTRODUCTION\n\nDue to the power law distribution of word frequencies, rare words are extremely common in any language (Zipf, 1935).",
    "curr": "Yet, the majority of language generation tasks-including machine translation (Sutskever et al., 2014;Bahdanau et al., 2014;Luong et al., 2015), summarization (Rush et al., 2015;See et al., 2017;Paulus et al., 2018), dialogue generation (Vinyals & Le, 2015), question answering (Yin et al., 2015), speech recognition (Graves et al., 2013;Xiong et al., 2017), and others-generate words by sampling from a multinomial distribution over a closed output vocabulary.",
    "next": "This is done by computing scores for each candidate word and normalizing them to probabilities using a softmax layer."
  },
  {
    "index": 823,
    "source_corpus_id": 260497139,
    "ref_id": "b0",
    "citation_corpus_id": 18828233,
    "start": 5003,
    "end": 5005,
    "prev": "Negative Logarithm Alternative It is worth mentioning that in the above formulation we adopt − log(D r (x)) as an alternative of log(1−D r (x)) for the generator's loss [12].",
    "curr": "The rationale is that, when the discriminator perfectly distinguishes fake samples, the log(1−D r (x)) loss function of the generator may suffer from the gradient vanishing problem [12,1].",
    "next": "The motivation of using the − log(D r (x)) is that: though giving a difference gradient scale, it always preserves the same gradient direction as log(1−D r (x))."
  },
  {
    "index": 826,
    "source_corpus_id": 56895416,
    "ref_id": "b13",
    "citation_corpus_id": 49411844,
    "start": 3015,
    "end": 3033,
    "prev": "However, as NAS is modeled as a Markov Decision Process, credits are assigned to structural decisions with temporal-difference (TD) learning (Sutton et al., 1998), whose efficiency and interpretability suffer from delayed rewards (Arjona-Medina et al., 2018).",
    "curr": "To get rid of the architecture sampling process, DARTS (Liu et al., 2019) proposes deterministic attention on operations to analytically calculate expectation at each layer and remove operations with relatively weak attention after the convergence of the parent network.",
    "next": "Due to the pervasive non-linearity in neural operations, the performance of the child network is always inconsistent and parameter retraining comes up as necessary."
  },
  {
    "index": 827,
    "source_corpus_id": 235390444,
    "ref_id": "b13",
    "citation_corpus_id": 4986726,
    "start": 2046,
    "end": 2071,
    "prev": "Concretely, the goal is to ensure that representations have two properties: (i) they are informative for various prediction tasks of interest, (ii) sensitive attributes of the original data (e.g., race) cannot be recovered from the representations.",
    "curr": "Perhaps the most prominent approach for learning fair representations is adversarial training (Edwards & Storkey, 2016;Madras et al., 2018;Xie et al., 2017;Song et al., 2019;Roy & Boddeti, 2019), which jointly trains an encoder trying to transform data into a fair representation with an adversary attempting to recover sensitive attributes from the representation.",
    "next": "However, several recent lines of work (Feng et al., 2019;Moyer et al., 2018;Elazar & Goldberg, 2018;Xu et al., 2020;Gupta et al., 2021;Song & Shmatikov, 2020) have noticed that these approaches do not produce truly fair representations: stronger adversaries can in fact recover sensitive attributes."
  },
  {
    "index": 828,
    "source_corpus_id": 222291521,
    "ref_id": "b18",
    "citation_corpus_id": 3144218,
    "start": 3067,
    "end": 3071,
    "prev": "For example, in drug discovery, when viewing molecules as graphs with atoms as nodes and chemical bonds as edges, biochemists are interested in identifying the subgraphs that mostly represent certain properties of the molecules, namely the functional groups [17,11].",
    "curr": "In graph representation learning, the predictive subgraph highlights the vital substructure for graph classification, and provides an alternative way for yielding graph representation besides mean/sum aggregation [19,30,32] and pooling aggregation [35,21,4].",
    "next": "In graph attack and defense, it is vital to purify a perturbed graph and mine the robust structures for classification [16]."
  },
  {
    "index": 829,
    "source_corpus_id": 13352766,
    "ref_id": "b2",
    "citation_corpus_id": 1740355,
    "start": 8386,
    "end": 8429,
    "prev": "This allows our approach to find the opti-mal architecture for the given dataset and constraints instead of being limited to that of the original model.",
    "curr": "Knowledge Distillation: Knowledge distillation is the task of training a smaller network (a \"student\") to mimic a \"teacher\" network, performing comparably to the input network (a \"teacher\") (Bucilu, Caruana, and Niculescu-Mizil 2006), (Ba and Caruana 2014), (Hinton, Vinyals, and Dean 2015), (Romero et al.",
    "next": "2014), (Urban et al."
  },
  {
    "index": 830,
    "source_corpus_id": 222208633,
    "ref_id": "b20",
    "citation_corpus_id": 3608234,
    "start": 5721,
    "end": 5740,
    "prev": "The most straightforward paradigm is restricting the attention pattern to be fixed local windows.",
    "curr": "Most works (Liu et al., 2018a;Parmar et al., 2018;Child et al., 2019;Huang et al., 2019;Ho et al., 2019;Hu et al., 2019;Parmar et al., 2019;Qiu et al., 2019;Beltagy et al., 2020;Zaheer et al., 2020) follow this paradigm.",
    "next": "Although restricting the attention pattern to a local neighborhood can decrease the complexity, it loses global information."
  },
  {
    "index": 832,
    "source_corpus_id": 231847109,
    "ref_id": "b9",
    "citation_corpus_id": 220249831,
    "start": 4943,
    "end": 4962,
    "prev": "Robust SSL (Oliver et al., 2018;Guo Labeled set Unlabeled set \"elephant\" \"cheetah\" \"octopus\" class 1 class 2 class 3 seen classes novel classes + seen classes seen class seen class seen class novel class novel class novel class\n\nOpen-world SSL Figure 1: In the open-world SSL, the unlabeled dataset may contain classes that have never been encountered in the labeled set.",
    "curr": "Given unlabeled test set, the model needs to either assign instances to one of the classes previously seen in the labeled set, or form a novel class and assign instances to Chen et al., 2020b;Guo et al., 2020;Yu et al., 2020) assumes class distribution mismatch between labeled and unlabeled data, but in this setting the model only needs to be able to recognize (reject) instances belonging to novel classes in the unlabeled data as out-of-distribution instances.",
    "next": "In contrast, instead of rejecting instances belonging to novel classes, open-world SSL aims at discovering individual novel classes and then assigning instances to them."
  },
  {
    "index": 835,
    "source_corpus_id": 222140859,
    "ref_id": "b9",
    "citation_corpus_id": 52967399,
    "start": 2031,
    "end": 2052,
    "prev": "INTRODUCTION\n\nSelf-supervised representation learning pre-trains good feature extractors from massive unlabeled data, which show promising transferability to various downstream tasks.",
    "curr": "Recent success includes large-scale pre-trained language models (e.g., BERT, RoBERTa, and GPT-3 (Devlin et al., 2019;Liu et al., 2019;Brown et al., 2020)), which have advanced state of the art over a wide range of NLP tasks such as NLI and QA, even surpassing human performance.",
    "next": "Specifically, in the computer vision domain, many studies have shown that self-supervised representation learning is essentially solving the problem of maximizing the mutual information (MI) I(X; T ) between the input X and the representation T (van den Oord et al., 2018;Belghazi et al., 2018;Hjelm et al., 2019;."
  },
  {
    "index": 837,
    "source_corpus_id": 235669672,
    "ref_id": "b23",
    "citation_corpus_id": 1487550,
    "start": 19543,
    "end": 19559,
    "prev": "We use 5 iterations, t 1 = 0.8, and t 2 = 1.2.",
    "curr": "• Self-training (Yarowsky, 1995;McClosky et al., 2006).",
    "next": "A classical semi-supervised method -each iteration, we train on pseudo-labeled data (initialized to be the original labeled dataset) and add highly confident predictions to the training set using the prediction as the label."
  },
  {
    "index": 838,
    "source_corpus_id": 256697539,
    "ref_id": "b17",
    "citation_corpus_id": 238354201,
    "start": 24567,
    "end": 24592,
    "prev": "We tested the inference time and peak memory consumption on CPU (Intel(R) Xeon(R) Gold 6246R CPU with 3.40 GHz clock), GPU (Nvidia A100), and on CPU with XNNPACK optimization.",
    "curr": "For comparison, we also tested LSTM and MobileViT (Mehta & Rastegari, 2022) networks.",
    "next": "MODELS AND TRAINING PROCESS\n\nGhostNet Our classification network consist of 3 ghost bottlenecks and ends with a convolutional layer and average pooling."
  },
  {
    "index": 847,
    "source_corpus_id": 5037032,
    "ref_id": "b32",
    "citation_corpus_id": 14724343,
    "start": 202899,
    "end": 202920,
    "prev": "(2017) observe a sequence of images from the expert demonstration for performing rope manipulations.Sermanet et al.",
    "curr": "(2017;2018) imitate humans with robots by self-supervised learning but require expert supervision at training time.Third person imitation learning (Stadie et al., 2017) and the concurrent work of imitation-from-observation (Liu et al., 2018) learn to translate expert observations into agent observations such that they can do policy optimization to minimize the distance between the agent trajectory and the translated demonstration, but they require demonstrations for learning.Visual servoing is a standard problem in robotics (Koichi & Tom, 1993) that seeks to take actions that align the agent's observation with a target configuration of carefully-designed visual features (Wilson et al., 1996;Yoshimi & Allen, 1994) or raw pixel intensities (Caron et al., 2013).Classical methods rely on fixed features or policies, but more recently end-to-end learning has improved results (Lampe & Riedmiller, 2013;Lee et al., 2017).et al.",
    "next": "(1995) jointly learn forward and inverse dynamics model but do not optimize for consistency between the forward and inverse dynamics.We empirically show that learning models by our forward consistency loss significantly improves task p"
  },
  {
    "index": 850,
    "source_corpus_id": 232233563,
    "ref_id": "b17",
    "citation_corpus_id": 28202810,
    "start": 1892,
    "end": 1914,
    "prev": "Although RL has achieved impressive achievements on various tasks (Silver et al., 2017;Mnih et al., 2015;Berner et al., 2019), it is very expensive to provide dense rewards for every task we want the robot to learn.",
    "curr": "Intrinsically motivated reinforcement learning encourages the agent to explore by providing an \"internal motivation\" instead, such as curiosity (Schmidhuber, 1991;Pathak et al., 2017;Burda et al., 2018), diversity (Gregor et al., 2016;Haarnoja et al., 2018;Eysenbach et al., 2019) and empowerment (Klyubin et al., 2005;Salge et al., 2014;Mohamed & Rezende, 2015).",
    "next": "Those internal motivations can be computed on the fly when the agent is interacting with the environment, without any human engineered reward."
  },
  {
    "index": 853,
    "source_corpus_id": 211082893,
    "ref_id": "b29",
    "citation_corpus_id": 205514,
    "start": 3422,
    "end": 3444,
    "prev": "Videos of real-world interactions are plentiful and readily available, and a large generative model can be trained on large unlabeled datasets containing many video sequences, thereby learning about a wide range of real-world phenoma.",
    "curr": "Such a model could be useful for learning representations for further downstream tasks (Mathieu et al., 2016), or could even be used directly in applications where predicting the future enables effective decision making and control, such as robotics (Finn et al., 2016).",
    "next": "A central challenge in video prediction is that the future is highly uncertain: a short sequence of observations of the present can imply many possible futures."
  },
  {
    "index": 855,
    "source_corpus_id": 51952942,
    "ref_id": "b1",
    "citation_corpus_id": 65455367,
    "start": 8889,
    "end": 8892,
    "prev": "The most well-known first-order algorithms are gradient descent (GD) for deterministic optimization [6,7] and stochastic gradient descent (SGD) for stochastic optimization [8,9], where the former determines ∆ t using the full (batch) gradient of an objective function, and the latter uses a simpler but more computationally-efficient stochastic (unbiased) gradient estimate.",
    "curr": "• Different from recently developed theoretical analysis of AMSGrad [2], which has been focused on diminishing momentum controlling parameter, our convergence analysis is applicable to the more popular constant momentum parameter setting.",
    "next": "• Our work provides theoretical support for a generic class of adaptive momentum based methods, including existing algorithms such as Adam, AMSGrad, AdaGrad, as well as their new variants such as AdaGrad with momentum."
  },
  {
    "index": 856,
    "source_corpus_id": 244709059,
    "ref_id": "b28",
    "citation_corpus_id": 225094135,
    "start": 49901,
    "end": 49923,
    "prev": "This paradigm transforms the value decomposition to the structured prediction problem.",
    "curr": "A future work is integrating prior knowledge of the decomposition structure as many previous works for structured prediction (Chen et al., 2020;Tavakoli et al., 2021).",
    "next": "Figure 1 :\n1Learning curves on a suite of MuJoCo benchmark tasks with episodic rewards."
  },
  {
    "index": 857,
    "source_corpus_id": 251253049,
    "ref_id": "b58",
    "citation_corpus_id": 222140788,
    "start": 8761,
    "end": 8780,
    "prev": "(2021) improve inversion by conditioning the denoising process on noised low-pass filter data from the target image.",
    "curr": "demonstrate that the DDIM (Song et al., 2020) sampling process can be inverted in a closed-form manner, extracting a latent noise map that will produce a given real image.",
    "next": "In DALL-E 2 (Ramesh et al., 2022), they build on this method and demonstrate that it can be used to induce changes in the image, such as cross-image interpolations or semantic editing."
  },
  {
    "index": 858,
    "source_corpus_id": 52947979,
    "ref_id": "b12",
    "citation_corpus_id": 17306137,
    "start": 2373,
    "end": 2396,
    "prev": "Implementation of DReG estimators and code to reproduce experiments: sites.google.com/view/ dregs.",
    "curr": "INTRODUCTION\n\nFollowing the influential work by (Kingma & Welling, 2013;Rezende et al., 2014), deep generative models with latent variables have been widely used to model data such as natural images (Rezende & Mohamed, 2015;Gulrajani et al., 2016), speech and music time-series (Chung et al., 2015;Fraccaro et al., 2016;Krishnan et al., 2015), and video (Babaeizadeh et al., 2017;Ha & Schmidhuber, 2018;Denton & Fergus, 2018).",
    "next": "The power of these models lies in combining learned nonlinear function approximators with a principled probabilistic approach, resulting in expressive models that can capture complex distributions."
  },
  {
    "index": 859,
    "source_corpus_id": 262824542,
    "ref_id": "b30",
    "citation_corpus_id": 3608234,
    "start": 6444,
    "end": 6448,
    "prev": "Experimental methodology\n\nThis section details our experimental set-up (Section 2.1) and useful tools employed by our analysis: (i) measuring the relationship between learning rate and loss across scales (Section 2.2) and (ii) examining scaling trends for model characteristics (Section 2.3).",
    "curr": "Experimental set-up\n\nWe train small Transformer models [45] with a similar experimental set-up as GPT-2 [38] implemented in Flax [20]: the models are decoder-only [31] and trained with an auto-regressive loss (refer to Section A for more infrastructure details).While we experimentally manipulate many of the following hyperparameters, this section provides their default values, which we use unless otherwise specified.",
    "next": "By default, we use AdamW [33] with β 1 = 0.9, β 2 = 0.95, ϵ = 1e-8, and gradient clipping at global norm 1.The default warmup is 5e3 steps, and the default number of total steps is 1e5.We use a linear schedule for warmup and and a cosine-decay [32] schedule for the remainder, with minimum learning rate 1e-5.We use an independent weight decay of 1e-4 and auxiliary z-loss [6] with coefficient 1e-4.Sections 3.2.2 and 3.1.2respectively provide additional information and ablations on decoupled weight decay and z-loss."
  },
  {
    "index": 861,
    "source_corpus_id": 255186293,
    "ref_id": "b8",
    "citation_corpus_id": 218486942,
    "start": 28207,
    "end": 28229,
    "prev": "CIFAR-100 is a single-label image recognition task, which we perform in the challenging zero-shot classification setting using CLIP (Radford et al., 2021).",
    "curr": "In addition, we conduct a natural language processing experiment using the Go Emotions (Demszky et al., 2020) dataset and a fine-tuned BERT model (Devlin et al., 2018), where the goal is to recognize emotion in text and a single instance may have multiple labels.",
    "next": "Finally, we evaluate the interaction between our method and subgroup fairness using the UCI Nursery dataset (Dua and Graff, 2017), where applicants are ranked for admissions to school."
  },
  {
    "index": 862,
    "source_corpus_id": 256503523,
    "ref_id": "b17",
    "citation_corpus_id": 203836948,
    "start": 2454,
    "end": 2474,
    "prev": "Introduction\n\nLink prediction is a crucial task in graph machine learning.",
    "curr": "It has various real-world applications, such as recommender systems (Zhang & Chen, 2020), knowledge graph completion (Zhu et al., 2021), and drug interaction prediction (Souri et al., 2022).",
    "next": "Graph Neural Networks have been used in link prediction."
  },
  {
    "index": 863,
    "source_corpus_id": 227209335,
    "ref_id": "b14",
    "citation_corpus_id": 52908831,
    "start": 24092,
    "end": 24116,
    "prev": "4).",
    "curr": "(Dinh et al., 2016) 3.49 -iResNet  3.45 -Glow (Kingma & Dhariwal, 2018) 3.35 -MintNet (Song et al., 2019b) 3.32 -Residual Flow  3.28 46.37 FFJORD (Grathwohl et al., 2018) 3  Exact likelihood computation Leveraging the connection to neural ODEs, we can compute the density defined by Eq.",
    "next": "(12) via the instantaneous change of variables formula ."
  },
  {
    "index": 867,
    "source_corpus_id": 234357892,
    "ref_id": "b16",
    "citation_corpus_id": 209461008,
    "start": 2979,
    "end": 2983,
    "prev": "Without considering the prior of faces, the backbone, neck and head design of TinaFace is thus redundant and sub-optimal.",
    "curr": "Since directly taking the backbone of the classification network for object detection is sub-optimal, the recent CR-NAS [17] reallocates the computation across different resolutions.",
    "next": "It is based on the observation that the allocation of computation across different resolutions has a great impact on the Effective Receptive Field (ERF) and affects the detection performance."
  },
  {
    "index": 870,
    "source_corpus_id": 59608630,
    "ref_id": "b18",
    "citation_corpus_id": 3462549,
    "start": 14591,
    "end": 14610,
    "prev": "The concept of 'learning to learn' is not tied to a specific task and thus meta-learning algorithms have been successfully applied to a wide range of challenges like RL (Wang et al., 2016;Finn et al., 2017), program induction (Devlin et al., 2017) few-shot classification (Koch et al., 2015;Vinyals et al., 2016) and scene understanding .",
    "curr": "Some meta learning models generate predictions in an autoregressive fashion by predicting the next target point from the entire prior sequence of consecutive observations (Reed et al., 2018;Mishra et al., 2018).",
    "next": "Algorithms of this kind have delivered state-of-the art results in a range of tasks such as supervised learning to classification."
  },
  {
    "index": 871,
    "source_corpus_id": 263605885,
    "ref_id": "b8",
    "citation_corpus_id": 247315430,
    "start": 7124,
    "end": 7141,
    "prev": "z f 8 G g Y e l r w A N 6 z n 7 j T 0 o 2 d G d K y t P y X S O f u 0 o u a F m y 3 n M w u O E / d c m 5 F / 0 w Y V j p N h L U t T I Z R i M W h c K e o t m l l N R 9 K C Q D X 1 g A s r / a 5 U T L j l A v 2 H t L w J j y + l / w Y X n 6 P 4 M G L f D j o n p 0 s 7 1 s l 7 8 o H s k p g c k x N y R s 5 J n w j y K y D B R t A K f o e r 4 W a 4 t U g N g 2 X N W / J H h O 8 e A K i X u X s = < / l a t e x i t > p ✓,µ (x) / p ✓ (x)e Eµ(x)   Figure 1: The decoding distribution p θ,µ induced by DAEMON scales the input LM distribution p θ with a sequence-level energy function E µ , which leads to a more accurate recovery of the underlying data distribution p d .",
    "curr": "Although pre-trained on large corpora of human texts with scaled up sizes, existing autoregressive language models (LMs) (Radford et al., 2019;Brown et al., 2020;Zhang et al., 2022) are still struggling to produce humanlike texts measured in various aspects, such as repetition, coherence, and consistency (Pillutla et al., 2021;Dou et al., 2022).Existing decoding methods are mainly driven to address two main mis-specifications of an LM's distribution: (i) The long tail of the distribution is unreliable (Holtzman et al., 2020), such that sampling from these low-probability regions often produces low-quality contents that are incoherent.",
    "next": "(ii) The mode of the distribution is degenerated (Welleck et al., 2020), where samples with high probabilities exhibit low diversity with repetitive patterns.As a result, sampling-based decoding methods (Fan et al., 2018;Holtzman et al., 2020;Meister et al., 2022) use various truncation strategies to avoid sampling from the unreliable long tail of the distribution, while recent search-based methods (Li et al., 2022;Su et al., 2022) incorporate additional contrastive objectives to avoid the collapse of degenerated repetitions.Since these two mis-specifications reside at opposing extremes of the probability spectrum, current decoding methods inevitably concentrate on Preprint."
  },
  {
    "index": 874,
    "source_corpus_id": 263608822,
    "ref_id": "b43",
    "citation_corpus_id": 254877499,
    "start": 1940,
    "end": 1961,
    "prev": "at retrieval augmentation can sometimes have a negative effect on performance.In this work, we present a thorough analysis on five open-domain question answering benchmarks, characterizing cases when retrieval reduces accuracy.We then propose two methods to mitigate this issue.First, a simple baseline that filters out retrieved passages that do not entail question-answer pairs according to a natural language inference (NLI) model.This is effective in preventing performance reduction, but at a cost of also discarding relevant passages.Thus, we propose a method for automatically generating data to fine-tune the language model to properly leverage retrieved passages, using a mix of relevant and irrelevant contexts at training time.We empirically show that even 1,000 examples suffice to train the model to be robust to irrelevant contexts while maintaining high performance on examples with relevant ones.",
    "curr": "INTRODUCTION\n\nLarge Language Models (LLMs) (Brown et al., 2020;Chowdhery et al., 2022;Touvron et al., 2023) are the foundation on top of which modern language systems are built.However, open-domain question answering (ODQA; Chen et al.",
    "next": "2017) and other knowledge-intensive tasks (Thorne et al., 2018;Petroni et al., 2021) require vast amounts of up-to-date factual knowledge about rare entities that even very large models cannot memorize (Roberts et al., 2020;Dhingra et al., 2022).A dominant approach for combating this issue has been Retrieval Augmented Language Models (RALMs), which incorporate a retrieval mechanism to reduce the need for storing information in the LLM parameters (Guu et al., 2020;Lewis et al., 2020b;Izacard et al., 2022;Rubin & Berant, 2023).Furthermore, RALMs have also been shown to improve ODQA performance in an in-context setting (without any training), simply by prepending retrieved sentences to the input question (Ram et al., 2023).Nevertheless, retrievers are not perfect and past work has shown that noisy retrieval can negatively affect LLM performance (Petroni et al.,"
  },
  {
    "index": 875,
    "source_corpus_id": 261530996,
    "ref_id": "b24",
    "citation_corpus_id": 3338083,
    "start": 3076,
    "end": 3080,
    "prev": "That is, (P) minimizes f over x ∈ X and y ∈ Y (the upper-level problem) when y must be one of the minimizers of g(x, ·) over y ∈ Y (the lower-level problem).",
    "curr": "Scalable optimization methods for solving (P) are in high demand to handle increasingly large-scale applications in machine-learning, including meta-learning [50], hyper-parameter optimization [20,3], model selection [37,24], adversarial networks [25,23], game theory [55], and reinforcement learning [34,56].",
    "next": "There is particular interest in developing (stochastic) gradient-descent-based methods due to their simplicity and scalability to large-scale problems [22,13,28,33,12,15,27,54,30,62]."
  },
  {
    "index": 876,
    "source_corpus_id": 257766959,
    "ref_id": "b12",
    "citation_corpus_id": 249209614,
    "start": 3570,
    "end": 3573,
    "prev": "Despite its potential benefits, text-conditioned video prediction (TVP) is a challenging task because it requires a deep understanding of the initial frames, the natural language instruction, and the grounding between language and images, while predicting based upon all the information above.",
    "curr": "The traditional text-conditioned video generation task [35,36,13,12,26] does not condition on initial frames and thus a model could seemingly perform well if it only generates a few prototypical videos corresponding to the input text.",
    "next": "The TVP task is much more challenging as the initial frames are given and generating prototypical videos is no longer a solution."
  },
  {
    "index": 878,
    "source_corpus_id": 257254919,
    "ref_id": "b3",
    "citation_corpus_id": 21529792,
    "start": 2126,
    "end": 2142,
    "prev": "1\n\nINTRODUCTION\n\nInverse Reinforcement Learning (IRL) techniques have been developed to robustly extract behaviors from expert demonstration and solve the problems of classical Imitation Learning (IL) methods (Ng et al., 1999;Ziebart et al., 2008).",
    "curr": "Among the recent methods for IRL, the Adversarial Imitation Learning (AIL) approach (Ho & Ermon, 2016;Fu et al., 2018;Peng et al., 2021), which casts the optimization over rewards and policies into an adversarial setting, have been proven particularly successful.",
    "next": "These methods, inspired by Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), alternate between learning a discriminator, and improving the agent's policy w.r.t."
  },
  {
    "index": 889,
    "source_corpus_id": 252280353,
    "ref_id": "b15",
    "citation_corpus_id": 220347682,
    "start": 16868,
    "end": 16897,
    "prev": "Time series OOD algorithms are currently less studied and there are only two recent strong approaches for comparison: GILE (Qian et al., 2021) and AdaRNN (Du et al., 2021).",
    "curr": "5 We further compare with 7 general OOD methods 6 from DomainBed (Gulrajani & Lopez-Paz, 2021): ERM, DANN (Ganin et al., 2016), CORAL (Sun & Saenko, 2016), Mixup (Zhang et al., 2018), GroupDRO (Sagawa et al., 2020), RSC (Huang et al., 2020), and ANDMask (Parascandolo et al., 2021).",
    "next": "More details of these methods are in Sec."
  },
  {
    "index": 891,
    "source_corpus_id": 208202099,
    "ref_id": "b32",
    "citation_corpus_id": 53792719,
    "start": 4767,
    "end": 4787,
    "prev": "The virtual rodent affords a new opportunity to directly compare principles of artificial control to biological data from real-world rodents, which are more experimentally accessible than humans.",
    "curr": "We draw inspiration from emerging deep reinforcement learning algorithms which now allow artificial agents to perform complex and adaptive movement in physical environments with sensory information that is increasingly similar to that available to animals (Peng et al., 2016;Heess et al., 2017;Merel et al., 2019a;.",
    "next": "Similarly, our virtual rodent exists in a physical world, equipped with a set of actuators that must be coordinated for it to behave effectively."
  },
  {
    "index": 893,
    "source_corpus_id": 263835408,
    "ref_id": "b14",
    "citation_corpus_id": 44134226,
    "start": 9895,
    "end": 9913,
    "prev": "Greedy decoding, which simply selects the most probable token under IP θ (x i |x 1:i−1 ), is a special case of sampling when the temperature τ = 0.",
    "curr": "Variants of sampling-based decoding include top-p sampling (Holtzman et al., 2020) and top-k sampling (Fan et al., 2018), which restrict the sampling to the most probable tokens.",
    "next": "FIXED GENERATION CONFIGURATION FOR SAFETY EVALUATION\n\nWe notice that open-source LLMs are usually evaluated for alignment using only default generation methods, and this potentially leaves them vulnerable to misalignment when alternative strategies are used."
  },
  {
    "index": 896,
    "source_corpus_id": 219965999,
    "ref_id": "b33",
    "citation_corpus_id": 211532691,
    "start": 5026,
    "end": 5030,
    "prev": "We evaluate AMIGO on MiniGrid [6], a suite of fast-to-run procedurally-generated environments with a symbolic/discrete (expressed in terms of objects like walls, doors, keys, chests and balls) observation space which isolates the problem of exploration from that of visual perception.",
    "curr": "Furthermore, [34] found that MiniGrid presents a particular challenge for existing state-of-the-art intrinsic motivation approaches.",
    "next": "Here, AMIGO sets a new state-of-the-art on some of the hardest MiniGrid environments [6], being the only method based on intrinsic motivation capable of successfully obtaining extrinsic reward on some of them."
  },
  {
    "index": 902,
    "source_corpus_id": 229212743,
    "ref_id": "b20",
    "citation_corpus_id": 6706414,
    "start": 42302,
    "end": 42327,
    "prev": "Following the protocol in (Yu et al., 2019), we evaluate the robustness against five types of image perturbation: cropping and resizing, blurring with Gaussian kernel, JPEG compression, additive Gaussian noise, and random combination of them.We consider two versions of our model: the original version and the immunized version.An immunized model indicates that during training we augment generated images with the corresponding perturbation in random strengths before feeding them to the fingerprint decoder.",
    "curr": "It is worth noting that none of the encoder, decoder, and training data are accessible to the public.Therefore, the robustness against perturbation has to be experimented with the black-box assumption, as protocoled in (Yu et al., 2019).In other words, white-box perturbations such as adversarial image modifications (Goodfellow et al., 2015) and fingerprint overwriting, which requires access to the encoder, decoder, and/or training data, are not applicable in our scenario.",
    "next": "We plot in Figure 5 the comparisons of fingerprint detection accuracy among our original/immunized models and the models of (Yu et al., 2021) w.r.t."
  },
  {
    "index": 904,
    "source_corpus_id": 256615829,
    "ref_id": "b26",
    "citation_corpus_id": 221739314,
    "start": 2570,
    "end": 2586,
    "prev": "INTRODUCTION\n\nDeep Neural Networks (DNNs) have obtained great progress in the field of natural language processing (NLP) but are vulnerable to adversarial attacks, leading to security and safety concerns, and research on defense algorithms against such attacks is urgently needed.",
    "curr": "Specifically, the most common attack for NLP is word-level attack (Wang et al., 2019b;Garg & Ramakrishnan, 2020;Zang et al., 2020;Li et al., 2021), which is usually implemented by adding, deleting or substituting words within a sentence.",
    "next": "Such an attack often brings catastrophic performance degradation to DNN-based models."
  },
  {
    "index": 907,
    "source_corpus_id": 219965949,
    "ref_id": "b31",
    "citation_corpus_id": 14307651,
    "start": 13721,
    "end": 13725,
    "prev": "As also mentioned by Tran et al.",
    "curr": "[52], previous work on Gumbel-softmax distributions indicate that the gradient approximation works well when the number of classes is lower than 200 [32,23].",
    "next": "Finally, the question remains if a sequence of additive bijectors of the form of Eq."
  },
  {
    "index": 910,
    "source_corpus_id": 12200521,
    "ref_id": "b14",
    "citation_corpus_id": 252796,
    "start": 13724,
    "end": 13745,
    "prev": "Experiments and Discussion\n\nWe evaluate zoneout's performance on the following tasks:\n\n• Classification of hand-written digits on permuted sequential MNIST [Le et al., 2015].",
    "curr": "• Word-level language modeling on the Penn Treebank corpus [Marcus et al., 1993].",
    "next": "• Character-level language modeling on the Penn Treebank corpus\n\nWe first investigate zoneout with a shared zoneout mask on cells and hiddens on the above tasks and compare its performance with other regularizers."
  },
  {
    "index": 912,
    "source_corpus_id": 11480374,
    "ref_id": "b9",
    "citation_corpus_id": 7228830,
    "start": 5988,
    "end": 6007,
    "prev": "The ensemble model achieves an exact match (EM) score of 78.8% and F1 score of 85.9%.",
    "curr": "Furthermore, we have tested FusionNet against adversarial SQuAD datasets (Jia & Liang, 2017).",
    "next": "Results show that FusionNet outperforms existing state-of-the-art architectures in both datasets: on AddSent, FusionNet increases the best F1 metric from 46.6% to 51.4%; on AddOneSent, FusionNet boosts the best F1 metric from 56.0% to 60.7%."
  },
  {
    "index": 915,
    "source_corpus_id": 851777,
    "ref_id": "b7",
    "citation_corpus_id": 11383178,
    "start": 20317,
    "end": 20337,
    "prev": "RECENT METHODS IN VARIATIONAL INFERENCE\n\nWe demonstrate Edward's flexibility for experimenting with complex inference algorithms.",
    "curr": "We consider the VAE setup from Figure 2 and  ≤ 85.8 Hierarchical variational model  ≤ 85.4 Importance-weighted auto-encoders (K = 50) (Burda et al., 2016) ≤ 86.3 HVM with IWAE objective (K = 5) ≤ 85.2 Rényi divergence (α = −1) (Li & Turner, 2016) ≤ 140.5 Table 1: Inference methods for a probabilistic decoder on binarized MNIST.",
    "next": "The Edward PPL is a convenient research platform, making it easy to both develop and experiment with many algorithms."
  },
  {
    "index": 916,
    "source_corpus_id": 247158860,
    "ref_id": "b10",
    "citation_corpus_id": 67915085,
    "start": 9066,
    "end": 9086,
    "prev": "Differentiable sorting & ranking.",
    "curr": "Differentiable sorting and ranking algorithms (Adams & Zemel, 2011;Grover et al., 2018;Blondel et al., 2020;Petersen et al., 2021) can be used in training neural networks with sorting and ranking supervision.",
    "next": "Recent approach (Blondel et al., 2020) proposes to construct differentiable sorting and ranking operators as projections onto the permutahedron, i.e., the convex hull of permutations, and using a reduction to isotonic optimization."
  },
  {
    "index": 917,
    "source_corpus_id": 53576131,
    "ref_id": "b3",
    "citation_corpus_id": 38407095,
    "start": 5929,
    "end": 5942,
    "prev": "• We systematically analyze what information is captured by units in representation across multiple settings by varying network architectures, tasks, and datasets.",
    "curr": "We use VD- CNN (Conneau et al., 2017) for sentiment and topic classification tasks on Yelp Reviews, AG News (Zhang et al., 2015), and DBpedia ontology dataset (Lehmann et al., 2015) and ByteNet (Kalchbrenner et al., 2016) for translation tasks on Europarl (Koehn, 2005) and News Commentary (Tiedemann, 2012) datasets.",
    "next": "• We also analyze how aligned natural language concepts evolve as they get represented in deeper layers."
  },
  {
    "index": 921,
    "source_corpus_id": 219636236,
    "ref_id": "b6",
    "citation_corpus_id": 52889459,
    "start": 27586,
    "end": 27590,
    "prev": "CLUE's latent space optimization mechanism allows it to cope well with high dimensional data.",
    "curr": "Future work can leverage recent advances in scalable BNNs [37] and generative modeling [38] to explore using CLUE for more complex data, such as natural images and natural language.",
    "next": "Broader Impact\n\nAs machine learning models are deployed in high-stakes scenarios, there has been a call for algorithmic transparency into models' behavior."
  },
  {
    "index": 925,
    "source_corpus_id": 249953817,
    "ref_id": "b7",
    "citation_corpus_id": 52967399,
    "start": 2663,
    "end": 2666,
    "prev": "Predicting visual observations for embodied agents is however challenging and computationally demanding: the model needs to capture the complexity and inherent stochasticity of future events while maintaining an inference speed that supports the robot's actions.",
    "curr": "Therefore, recent advances in autoregressive generative models, which leverage Transformers [7] for building neural architectures and learn good representations via self-supervised generative pretraining [8], have not benefited video prediction or robotic applications.",
    "next": "We in particular identify three technical challenges."
  },
  {
    "index": 927,
    "source_corpus_id": 20140417,
    "ref_id": "b0",
    "citation_corpus_id": 11383178,
    "start": 1933,
    "end": 1953,
    "prev": "We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.",
    "curr": "INTRODUCTION\n\nWe build upon AESMC , a method for model learning that itself builds on variational auto-encoders (VAEs) (Kingma & Welling, 2014;Rezende et al., 2014) and importance weighted auto-encoders (IWAEs) (Burda et al., 2016).",
    "next": "AESMC is similarly based on maximizing a lower bound to the log marginal likelihood, but uses SMC (Doucet & Johansen, 2009) as the underlying marginal likelihood estimator instead of importance sampling (IS)."
  },
  {
    "index": 930,
    "source_corpus_id": 238856821,
    "ref_id": "b48",
    "citation_corpus_id": 227230646,
    "start": 2994,
    "end": 3011,
    "prev": "More recent methods attempt to learn from different types of tasks.",
    "curr": "These include LAMOL (Sun et al., 2019) and its improvements (Chuang et al., 2020;Sun et al., 2020;Kanwatchara et al., 2021).",
    "next": "Despite the effectiveness of these methods in LLL, there are several limitations."
  },
  {
    "index": 931,
    "source_corpus_id": 220042361,
    "ref_id": "b0",
    "citation_corpus_id": 202541346,
    "start": 4131,
    "end": 4152,
    "prev": "To address this issue, a class of algorithms have been developed that are based on learning a lowdimensional latent (embedding) space and a latent model (dynamics), and then using this model to control the system in the latent space.",
    "curr": "This class has been referred to as learning control-lable embedding (LCE) and includes recently developed algorithms, such as E2C [Watter et al., 2015], RCE [Banijamali et al., 2018], SOLAR [Zhang et al., 2019], PCC [Levine et al., 2020], Dreamer [Hafner et al., 2020], and PC3 .",
    "next": "The following two properties are extremely important in designing LCE models and algorithms."
  },
  {
    "index": 932,
    "source_corpus_id": 264590171,
    "ref_id": "b14",
    "citation_corpus_id": 246634193,
    "start": 1799,
    "end": 1818,
    "prev": " capture and learn structural information about objects and scenes.The intermediate feature representations obtained from MIM are suitable for fine-tuning on downstream tasks.In this paper, we propose an Image Modeling framework based on random orthogonal projection instead of binary masking as in MIM.Our proposed Random Orthogonal Projection Image Modeling (ROPIM) reduces spatially-wise token information under guaranteed bound on the noise variance and can be considered as masking entire spatial image area under locally varying masking degrees.Since ROPIM uses a random subspace for the projection that realizes the masking step, the readily available complement of the subspace can be used during unmasking to promote recovery of removed information.In this paper, we show that using random orthogonal projection leads to superior performance compared to crop-based masking.We demonstrate state-of-the-art results on several popular benchmarks.",
    "curr": "INTRODUCTION\n\nROPIM MAE (He et al., 2022) CIM (Fang et al., 2023) BEiT (Bao et al., 2022) MoCov3 (Chen et al., 2021b) MFM (Xie et.al., 2023) CAN (Mishra et al., 2022) GPL-MAE (Jiang et al., 2023) GPL-ROPIM Fast methods Best-performing Masked Image Modeling (MIM) (Bao et al., 2022;He et al., 2022;Xie et al., 2022) has achieved promising performance by pretraining backbones that are then fine-tuned on different downstream tasks such as image classification or semantic segmentation.",
    "next": "Most MIM techniques follow the general paradigm of self-prediction, i.e., they randomly mask out some regions in the input data and then learn to recover the missing data.Current MIM methods (Bao et al., 2022;He et al., 2022;Xie et al., 2022) mainly apply masking in the spatial domain by randomly excluding image patches.Since raw image pixels are highly correlated within their spatial neighbourhood, a high masking ratio (60%-75%) leads to high quality features (He et al., 2022;Xie et al., 2022)."
  },
  {
    "index": 936,
    "source_corpus_id": 252967732,
    "ref_id": "b9",
    "citation_corpus_id": 67855286,
    "start": 9462,
    "end": 9481,
    "prev": "In this way, the time variable is discretized and the ODE solution is computed by the successive application of an iterated map that operates on the discrete set of points between 0 and T , with a step size > 0.",
    "curr": "Crucially, as already observed for feed-forward and recurrent neural models (Haber & Ruthotto, 2017;Chang et al., 2019), each step of the ODE discretization process can be equated to one layer of a DGN network.",
    "next": "The whole neural architecture contains as many layers as the integration steps in the numerical method (i.e., L = T / ), and each layer = 1, ..., L computes nodes' states x u which approximates x u ( )."
  },
  {
    "index": 937,
    "source_corpus_id": 250144675,
    "ref_id": "b2",
    "citation_corpus_id": 208637407,
    "start": 2711,
    "end": 2713,
    "prev": "Introduction\n\nData augmentation is a crucial component of machine learning tasks as it prevents overfitting caused by a lack of training data and improves task performance without additional inference costs.",
    "curr": "Many data augmentation methods have been proposed across a broad range of research fields, including image recognition [1,2,3,4,5,6,7,8], image processing [9,10], language processing [11,12,13,14], and speech recognition [15,16].",
    "next": "In image recognition, each augmentation algorithm has become an essential component of the modern training recipe through various combinations [17,18,19,20]."
  },
  {
    "index": 941,
    "source_corpus_id": 257102638,
    "ref_id": "b21",
    "citation_corpus_id": 52901777,
    "start": 4009,
    "end": 4031,
    "prev": "A subset of ViT models is superior to all other models for all aspects of uncertainty estimation (\"ViT\" in the legend, marked as a red triangle facing upwards); the performance of EfficientNet-V2 and GENet models is worse.",
    "curr": "Among the various metrics proposed for evaluating the performance of uncertainty estimation are: Area Under the Receiver Operating Characteristic (AUROC or AUC), Area Under the Risk-Coverage curve (AURC) (Geifman et al., 2018), selective risk or coverage for a selective accuracy constraint (SAC), Negative Log-likelihood (NLL), Expected Calibration Error (ECE), which is often used for evaluating a model's calibration (see Section 2) and Brier score (Brier, 1950).",
    "next": "All these metrics are well known and are often used for comparing the uncertainty estimation performance of models (Moon et al., 2020;Nado et al., 2021;Maddox et al., 2019;Lakshminarayanan et al., 2017)."
  },
  {
    "index": 943,
    "source_corpus_id": 14911774,
    "ref_id": "b14",
    "citation_corpus_id": 6308361,
    "start": 25501,
    "end": 25526,
    "prev": "However, a very interesting effect of supervision is that it improves the interpretability of the code even when agents must communicate about images that do not contain objects in the supervised category set.",
    "curr": "This emerged in a follow-up experiment in which, during training, the sender was again exposed (with equal probability) to the same supervised classification task as above, but now the agents played the referential game on a different dataset of images derived from ReferItGame (Kazemzadeh et al., 2014).",
    "next": "In its general format, the ReferItGame contains annotations of bounding boxes in real images with referring expressions produced by humans when playing the game."
  },
  {
    "index": 944,
    "source_corpus_id": 228063969,
    "ref_id": "b11",
    "citation_corpus_id": 53477919,
    "start": 8724,
    "end": 8741,
    "prev": "This is indeed quite different from the setting considered in (Pilanci & Ergen, 2020) and demands a different treatment.",
    "curr": "It could also be useful to mention that there are works in (Amos et al., 2017;Chen et al., 2019) that customize the network architecture for convex inference, but they still require non-convex training.",
    "next": "In recent years, deep learning has been widely deployed in inverse problems to either learn effective priors for iterative algorithms (Bora et al., 2017;Heckel & Hand, 2018), or to directly learn the inversion map using feed-forward networks (Jin et al., 2017;Zhang et al., 2017)."
  },
  {
    "index": 946,
    "source_corpus_id": 259075723,
    "ref_id": "b6",
    "citation_corpus_id": 222378211,
    "start": 4845,
    "end": 4847,
    "prev": "In FL, a server trains a shared model by applying aggregated gradient updates, received from a large number of clients.",
    "curr": "Gradient leakage attacks A long line of work [5,6,7,8,9,10], primarily on images, has shown that even passive servers can reconstruct client data from gradients, breaking the key privacy promise of FL.",
    "next": "However, these attacks are only applicable to naive FL deployments [11]-in real-life settings with no unrealistic assumptions, they are limited to small batch sizes with no secure aggregation [12]."
  },
  {
    "index": 949,
    "source_corpus_id": 53216818,
    "ref_id": "b21",
    "citation_corpus_id": 997870,
    "start": 21913,
    "end": 21917,
    "prev": "A large number of time steps are not needed to manipulate the object, and a strong signal about progress is readily received.",
    "curr": "However, since the targets can change rapidly, the variance in gradient estimates can be very high for function approximation methods [22].",
    "next": "Trajectory optimization is particularly well suited for such types of problems, since it can efficiently compute near-optimal actions conditioned on the instance, facilitating function approximation."
  },
  {
    "index": 951,
    "source_corpus_id": 238857090,
    "ref_id": "b21",
    "citation_corpus_id": 204960716,
    "start": 37833,
    "end": 37852,
    "prev": "For direct comparison with existing methods that do not take direct advantage of this assumption, we trained a new Language Processing module that does not make use of templates but makes use of the subtasks sequences annotations ALFRED provides.",
    "curr": "5 Fine-tuning a pre-trained BART (Lewis et al., 2020) model, we directly learned a mapping from a high-level instruction to a sequence of subtasks (e.g.",
    "next": "\"Drop a clean pan on the table\" → \"(PickupObject, Pan), (PutObject, Sink), ...\")."
  },
  {
    "index": 953,
    "source_corpus_id": 219558836,
    "ref_id": "b4",
    "citation_corpus_id": 52967399,
    "start": 2234,
    "end": 2255,
    "prev": "Under review.",
    "curr": "Introduction\n\nPre-trained transformer-based masked language models such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and ALBERT (Lan et al., 2020) have had a dramatic impact on the NLP landscape in the recent year.",
    "next": "The standard recipe of using such models typically involves training a pre-trained model for few epochs on a supervised downstream dataset."
  },
  {
    "index": 961,
    "source_corpus_id": 256389917,
    "ref_id": "b14",
    "citation_corpus_id": 209977508,
    "start": 1969,
    "end": 1990,
    "prev": "re but also data that evolved in a similar fashion in the past.The time-varying propensity score is quite general: we demonstrate different ways of implementing it and evaluate it on a variety of problems ranging from supervised learning (e.g., image classification problems) where data undergoes a sequence of gradual shifts, to reinforcement learning tasks (e.g., robotic manipulation and continuous control) where data shifts as the policy or the task changes.",
    "curr": "INTRODUCTION\n\nMachine learning models are not expected to perform well when the test data is from a different distribution than the training data.There are many techniques to mitigate the consequent deterioration in performance (Heckman, 1979;Shimodaira, 2000;Huang et al., 2006;Bickel et al., 2007;Sugiyama et al., 2007b;2008;Gretton et al., 2008).These techniques use the propensity score between the train and test data distributions to reweigh the data-and they work well (Agarwal et al., 2011;Wen et al., 2014;Reddi et al., 2015b;Fakoor et al., 2020c;a;Tibshirani et al., 2019) when dealing with a single training and test dataset.But when machine learning models are deployed for real-world problems, they do not just undergo one distribution shift1 (from train to test), but instead suffer many successive distribution shifts (Lu et al., 2019) e.g., search queries to an online retailer or a movie recommendation service evolve as fashion and tastes of the population evolve, etc.Even for problems in healthcare, there are many situations where data drifts gradually, e.g., the different parts of the brain atrophy slowly as the brain ages for both healthy subjects and those with dementia; tracking these changes and distinguishing between them is very useful for staging the disease and deciding treatments (Saito et al., 2022;Wang et al., 2023).Viruses can mutate and drift over time, which can make them resistant to existing treatments (Russell, 2016;Callaway, 2020;Harvey et al., 2021).",
    "next": "To build a method that can account for changes in data,"
  },
  {
    "index": 962,
    "source_corpus_id": 199528271,
    "ref_id": "b9",
    "citation_corpus_id": 52967399,
    "start": 3151,
    "end": 3172,
    "prev": "For example, when training typical Transformers based neural machine translation models on the De-En IWSLT'14 dataset, removing the warmup stage increases the training loss from 3 to around 10, as shown in Figure 1.",
    "curr": "Similar phenomena are observed in other scenarios like BERT (a bidirectional transformer language model) pre-training (Devlin et al., 2019).",
    "next": "Due to the lack of the theoretical underpinnings, there is neither guarantee that warmup would bring consistent improvements for various machine learning settings nor guidance on how we should In this paper, we conduct both empirical and theoretical analysis of the convergence issue to identify its origin."
  },
  {
    "index": 963,
    "source_corpus_id": 238353966,
    "ref_id": "b0",
    "citation_corpus_id": 49310753,
    "start": 36797,
    "end": 36823,
    "prev": "Batch normalization and its variant are introduced in deep deterministic policy gradient (DDPG) (Lillicrap et al., 2015) and twin delayed DDPG (Fujimoto et al., 2018) (Bhatt et al., 2020).",
    "curr": "Layer normalization is introduced in the implementation of maximum a posteriori policy optimisation(Abdolmaleki et al., 2018; Hoffman et al.,  2020).",
    "next": "It is also introduced in SAC extensions(Ma et al., 2020; Zhang et al., 2021)\n\nFigure 4 :Figure 5 :\n45Average return and average/standard deviation of estimation bias for DroQ with different dropout rates."
  },
  {
    "index": 967,
    "source_corpus_id": 263608898,
    "ref_id": "b19",
    "citation_corpus_id": 233296292,
    "start": 6518,
    "end": 6536,
    "prev": "(2020); Kumar et al.",
    "curr": "(2022) for CL.In this paper, we use SimCLR (Chen et al., 2020) and SimSiam (Chen & He, 2021) for CIFAR10 dataset and SimCSE (Gao et al., 2021) for BIASBIOS dataset to learn CL representations.",
    "next": "Several works have theoretically studied the success of self-supervised learning (Arora et al., 2019;HaoChen et al., 2021;Lee et al., 2020;Tian et al., 2021;Tosh et al., 2021).Our theoretical analysis of CL loss is partly motivated by Fang et al."
  },
  {
    "index": 968,
    "source_corpus_id": 259076379,
    "ref_id": "b4",
    "citation_corpus_id": 52967399,
    "start": 2249,
    "end": 2256,
    "prev": "Specifically, we provide an efficient GPU inference algorithm for SpQR which yields faster inference than 16-bit baselines at similar accuracy, while enabling memory compression gains of more than 4x.",
    "curr": "Introduction\n\nPretrained large language models (LLMs) improved rapidly from task-specific performance [WSM + 18, DCLT19,RWC + 19], to performing well on general tasks if prompted with instructions [BMR + 20,WBZ + 21,Ope23].",
    "next": "While the improved performance can be attributed to scaling in training data and parameters [KMH + 20,CND + 22] recent trends focused on smaller models trained on more data, that are easier to use at inference time [HBM + 22, BSA + 23, TLI + 23]."
  },
  {
    "index": 970,
    "source_corpus_id": 227161986,
    "ref_id": "b36",
    "citation_corpus_id": 210064473,
    "start": 18945,
    "end": 18963,
    "prev": "An alternative to inferring object keypoints (and corresponding representations) is to learn entire object representations.",
    "curr": "Recent approaches can be broadly categorized into spatial mixture models (Greff et al., 2017;, sequential attention models (Eslami et al., 2016;Kosiorek et al., 2018) or hybrid models that are combinations of both (Lin et al., 2020).",
    "next": "While these methods have shown promising results, they have yet to scale to more complex visual settings."
  },
  {
    "index": 972,
    "source_corpus_id": 208006294,
    "ref_id": "b2",
    "citation_corpus_id": 11212020,
    "start": 2671,
    "end": 2694,
    "prev": " and metrics evaluation on both LSTM models and BERT Transformer models on multiple datasets show that our algorithms outperform prior hierarchical explanation algorithms.Our algorithms apply to hierarchical visualization of compositional semantics, extraction of classification rules and improving human trust of models.",
    "curr": "Introduction\n\nRecent advances in deep neural networks have led to impressive results on a range of natural language processing (NLP) tasks, by learning latent, compositional vector representations of text data (Peters et al., 2018;Devlin et al., 2018;Liu et al., 2019b).However, interpretability of the predictions given by these complex, \"black box\" models has always been a limiting factor for use cases that require explanations of the features involved in modeling (e.g., words and phrases) (Guidotti et al., 2018;Ribeiro et al., 2016).Prior efforts on enhancing model interpretability have focused on either constructing models with intrinsically interpretable structures (Bahdanau et al., 2015;Liu et al., 2019a), or developing post-hoc explanation algorithms which can explain model predictions without elucidating the mechanisms by which model works (Mohseni et al., 2018;Guidotti et al., 2018).Among these work, post-hoc explanation has come to the fore as they can operate over a variety of trained models while not affecting predictive performance of models.",
    "next": "Towards post-hoc explanation, a major line of work, additive feature attribution methods (Lundberg and Lee, 2017;Ribeiro et al., 2016;Binder et al., 2016;Shrikumar et al., 2017), explain a model prediction by assigning importance scores to individual input variables.However, these methods may not work for explaining compositional semantics in natural language (e.g., phrases or clauses), as the importance of a phrase often is non-linear combination of the importance of the words in the phrase.Contextual decomposition (CD) (Murdoch et al., 2018) and its hierarchical extension (Singh et al., 2019) go beyond the additive assu"
  },
  {
    "index": 973,
    "source_corpus_id": 251252927,
    "ref_id": "b17",
    "citation_corpus_id": 3144218,
    "start": 2452,
    "end": 2456,
    "prev": "A variety of applications characterized by this type of graph-structured data include works in the areas of social analysis [5], recommendation systems [56], computer vision [42], study of the properties of chemical compounds [26,48], statistical physics [8,10], and financial forensics [51,57].",
    "curr": "The most popular learning models for relational data use graph convolutions [33], where the idea is to aggregate the attributes of the set of neighbours of a node instead of only utilizing its own attributes.",
    "next": "Despite several empirical studies of various GCN-type models [13,38] that demonstrate that graph convolutions can improve the performance of traditional classification methods, such as a multi-layer perceptron (MLP), there has been limited progress in the theoretical understanding of the benefits of graph convolutions in multi-layer networks in terms of improving node classification tasks."
  },
  {
    "index": 974,
    "source_corpus_id": 3475375,
    "ref_id": "b0",
    "citation_corpus_id": 18828233,
    "start": 6604,
    "end": 6630,
    "prev": "(2012); Li et al.",
    "curr": "(2015) or the Wasserstein metric Arjovsky and Bottou (2017).",
    "next": "All of these divergence measures yield a minimax objective."
  },
  {
    "index": 976,
    "source_corpus_id": 256194054,
    "ref_id": "b52",
    "citation_corpus_id": 3162051,
    "start": 12877,
    "end": 12897,
    "prev": "In this case, the optimization objective for the student network is a weighted combination of dual losses: a standard supervised cross-entropy loss and a \"distribution-matching\" objective that aims to mimic the teacher's output.",
    "curr": "However, in our case there are no class-labels for the patches generated from a single image, so we solely use the second objective formulated as a Kullback-Leibler (KL) divergence between the student output p s and the teacher's output p t :\nL KL = c ∈ C −p t c log p s c + p t c log p t c(1)\nwhere c are the teachers' classes and the outputs of both student and teacher are temperature τ flattened probabilities, p = softmax(l/τ ), that are generated from logits l.\n\nFor training, we follow (Beyer et al., 2022) in employing a function matching strategy, where the teacher and student models are fed consistently augmented instances, that include heavy augmentations, such as MixUp (Zhang et al., 2018) or CutMix .",
    "next": "However, in contrast to (Beyer et al., 2022), we neither have access to TPUs nor can train 10K epochs on ImageNet-sized datasets."
  },
  {
    "index": 977,
    "source_corpus_id": 249431433,
    "ref_id": "b13",
    "citation_corpus_id": 14124313,
    "start": 2098,
    "end": 2126,
    "prev": "INTRODUCTION\n\nThe success of deep learning in computer vision is largely driven by Convolutional Neural Networks (CNNs).",
    "curr": "Starting from the milestone work AlexNet (Krizhevsky et al., 2012), CNNs keep pushing the frontier of computer vision (Simonyan & Zisserman, 2015;He et al., 2016;Tan & Le, 2019).",
    "next": "Interestingly, the recently emerged Vision Transformer (ViT) (Dosovitskiy et al., 2020) challenges the leading position of CNNs."
  },
  {
    "index": 978,
    "source_corpus_id": 261823404,
    "ref_id": "b14",
    "citation_corpus_id": 225039882,
    "start": 7252,
    "end": 7279,
    "prev": "RELATED WORK\n\nIn this section, we introduce the recent related improvements in 3D generative models including GAN-based and diffusion-based methods, as well as transformer structure.",
    "curr": "TRANSFORMER\n\nIn recent years, Transformer (Vaswani et al., 2017) has seen rapid progress in many fields including image recognition (Dosovitskiy et al., 2020a;Touvron et al., 2021), object detection (Carion et al., 2020;Zhu et al., 2020), tracking (Cao et al., 2021;2022;2023), segmentation (Zheng et al., 2021;Strudel et al., 2021), and image generation (Van den Oord et al., 2016;Jiang et al., 2021;Mo et al., 2023).Some works (Chen et al., 2020;Child et al., 2019) prove the remarkable of transformer when predicting the pixels autoregressively.Based on the masked token, MaskGIT.",
    "next": "(Chang et al., 2022) achieve promising generation performance.DiT (Peebles & Xie, 2022) adopts the transformer as the backbone of diffusion models of images.Based on the 2D version, Mo et al."
  },
  {
    "index": 981,
    "source_corpus_id": 258887825,
    "ref_id": "b23",
    "citation_corpus_id": 3488815,
    "start": 1233,
    "end": 1237,
    "prev": "Under review.",
    "curr": "Introduction\n\nMany noteworthy modern machine learning problems, such as generative adversarial networks (GANs) [13], adversarial training [24], and sharpness-aware minimization (SAM) [10], are instances of minimax problems, formulated as min x max y f (x, y).",
    "next": "First-order methods, such as gradient descent ascent (GDA) [1] and extragradient (EG) [20], are workhorses of minimax optimization in modern machine learning, but they still remain remarkably unreliable."
  },
  {
    "index": 982,
    "source_corpus_id": 263829263,
    "ref_id": "b10",
    "citation_corpus_id": 189928186,
    "start": 2333,
    "end": 2337,
    "prev": "it complexity class TC 0 , i.e., AC 0 -circuits extended by majority gates.We first show a negative result that there is an AC 0 -language that cannot be recognized by an UHAT encoder.On the positive side, we show that UHAT encoders can recognize a rich fragment of AC 0 -languages, namely, all languages definable in first-order logic with arbitrary unary numerical predicates.This logic, includes, for example, all regular languages from AC 0 .We then show that AHAT encoders can recognize all languages of our logic even when we enrich it with counting terms.We apply these results to derive new results on the expressive power of UHAT and AHAT up to permutation of letters (a.k.a.Parikh images).",
    "curr": "Introduction\n\nTransformers have revolutionized natural language processing by facilitating the efficient and effective modeling of intricate contextual relationships within text [19].This remarkable capability has sparked numerous investigations into the potential boundaries of transformers' power [11,22,17,21,12,6,5,7].One natural method for addressing this question is to explore the classes of formal languages that these architectures can recognize.This approach provides an insight into their strengths and limitations.The response to this question naturally relies on the specific features allowed within transformer encoders.These encompass the interplay between encoders and decoders, the kind of functions used for positional encodings and attention mechanisms, and considerations of fixed or unbounded precision, among other factors.",
    "next": "While the capacity of transformers that incorporate both encoders and decoders to recognize languages is well understood today (indeed, such architectures are Turing-complete and can thus recognize any computable language [17]), the expressive power of transformer encoders has not been fully elucidated to date.Unique Hard Attention Transformers (UHAT) are a class of transformer encoders that has been a subject of many recent papers.As was shown by [12]"
  },
  {
    "index": 983,
    "source_corpus_id": 3568073,
    "ref_id": "b23",
    "citation_corpus_id": 6628106,
    "start": 28804,
    "end": 28823,
    "prev": "The upsampling and downsampling operations in Table 2 correspond to 2 × 2 element replication and average pooling, respectively.",
    "curr": "We train the networks using Adam (Kingma & Ba, 2015) with α = 0.001, β 1 = 0, β 2 = 0.99, and = 10 −8 .",
    "next": "We do not use any learning rate decay or rampdown, but for visualizing generator output at any given point during the training, we use an exponential running average for the weights of the generator with decay 0.999."
  },
  {
    "index": 985,
    "source_corpus_id": 212414722,
    "ref_id": "b15",
    "citation_corpus_id": 210064473,
    "start": 56249,
    "end": 56267,
    "prev": "ng bilinear interpolation.This attention mechanism is motivated by the observation that only part of the image contains information for tracking an object and an inductive bias that objects cannot move a large distance within a short time span (i.e., objects do not teleport).",
    "curr": "Posterior Discovery.The posterior discovery also consists of proposal and rejection phases.The main difference is that we now compute the proposal in spatially-parallel manner by conditioning on the observations x ≤t , i.e., q(z D t |z P t , x ≤t ) = HW h,w=1 q(z D t,h,w |z P t , x ≤t ).Here, the observation x ≤t is encoded into the feature map of dimensionality H×W ×D using a Convolutional LSTM (Xingjian et al., 2015).Then, from each feature we obtain zD t,h,w .Importantly, this is done over all the feature cells h, w in parallel.A similar approach is used in SPAIR (Crawford & Pineau, 2019b), but it infers the object latent representations sequentially and thus is difficult to scale to a large number of objects (Lin et al., 2020).Even if this spatially-parallel proposal plays a key role in making our model scalable, we also observe another challenge due to this high capacity of the discovery module.The problem is that the discovery module tends to dominate the propagation module and thus most of the objects in an image are explained by the discovery module, i.e., objects are rediscovered at every time-step while nothing is propagated.We call this problem propagation collapse.",
    "next": "Why would the model tend to explain an image through discovery while suppressing propagation?First, the model does not care where-either from discovery or propagation-an object is sourced from as long as it can make an accurate reconstruction.Second, the propagation step performs a much harder task than the discovery.For the propagation to properly predict, tracker n needs to learn to find the matching object from an image containing many objects.Although the propagation attention plays an important role in balancing the discovery and pr"
  },
  {
    "index": 988,
    "source_corpus_id": 53831933,
    "ref_id": "b2",
    "citation_corpus_id": 3921978,
    "start": 2285,
    "end": 2305,
    "prev": "Yet, even though significant progress has been made thanks to better algorithms, training regimes, and computational infrastructure, the resulting behaviors still tend to exhibit significant idiosyncrasies (e.g.",
    "curr": "Bansal et al., 2018).",
    "next": "One advantage of working with humanoids in this context is that motion capture data is widely available and can serve to help design controllers that produce apparently humanlike movement."
  },
  {
    "index": 990,
    "source_corpus_id": 4862861,
    "ref_id": "b1",
    "citation_corpus_id": 9963298,
    "start": 5135,
    "end": 5162,
    "prev": "Static datasets such as visual question answering (Antol et al., 2015) provide grounding into images, but no possibility for language learning through interaction.",
    "curr": "Some works utilize a geographical environment such as a maze but still employ static datasets (Artzi & Zettlemoyer, 2013).",
    "next": "It has been argued that virtual embodiment of agents is a viable long-term strategy for artificial intelligence research and the learning of natural language semantics, particularly in the form of games which also contain human players (Kiela et al., 2016)."
  },
  {
    "index": 994,
    "source_corpus_id": 238744039,
    "ref_id": "b32",
    "citation_corpus_id": 44090489,
    "start": 5646,
    "end": 5671,
    "prev": "Although being highly effective in low-resource translation, backtranslation also has issues, as the model is trained on potentially invalid input-output pairs.",
    "curr": "Neural machine translation models being highly sensitive to input noise (Belinkov & Bisk, 2018;Khayrallah & Koehn, 2018), this can severely deteriorate the performance.",
    "next": "Fortunately, many programming languages come with relatively mature tools and technologies for automated test data generation."
  },
  {
    "index": 997,
    "source_corpus_id": 219401642,
    "ref_id": "b7",
    "citation_corpus_id": 202749904,
    "start": 5510,
    "end": 5513,
    "prev": "It is illustrated in Figure 1.",
    "curr": "The generator is inspired by GAN-TTS [8], a text-to-speech generative adversarial network operating on aligned linguistic features.",
    "next": "We employ the GAN-TTS generator as the decoder in our model, but instead of upsampling pre-computed linguistic features, its input comes from the aligner block."
  },
  {
    "index": 998,
    "source_corpus_id": 259137503,
    "ref_id": "b17",
    "citation_corpus_id": 225039882,
    "start": 1627,
    "end": 1631,
    "prev": "Code is available at https://github.com/NVlabs/FasterViT.",
    "curr": "Introduction\n\nVision Transformers (ViTs) [18] have recently become popular in computer vision and achieved superior performance in various applications such as image classification [38,17,35], object detection [77,21] and semantic segmentation [61,10].",
    "next": "In addition to learning more uniform local and global representations across their architecture when compared to Convolutional Neural Networks (CNNs), ViTs scale properly to large-scale data and model sizes [47,45]."
  },
  {
    "index": 1000,
    "source_corpus_id": 16299141,
    "ref_id": "b2",
    "citation_corpus_id": 11212020,
    "start": 17150,
    "end": 17173,
    "prev": "Other work has improved language modeling performance by modifying the RNN architecture to better handle increased recurrence depth (Zilly et al., 2016).",
    "curr": "In order to increase capacity and minimize the impact of vanishing gradients, some language and translation mod- WikiText-2  WikiText-103  Train  Valid  Test  Train  Valid  Test  Train  Valid  Test   Articles  ---600  60  60  28,475  60  60  Tokens  929,590 73,761 82, els have also added a soft attention or memory component (Bahdanau et al., 2015;Sukhbaatar et al., 2015;Cheng et al., 2016;Kumar et al., 2016;Xiong et al., 2016;Ahn et al., 2016).",
    "next": "These mechanisms allow for the retrieval and use of relevant previous hidden states."
  },
  {
    "index": 1002,
    "source_corpus_id": 252595881,
    "ref_id": "b3",
    "citation_corpus_id": 46899514,
    "start": 2583,
    "end": 2610,
    "prev": "Diffusion models have been used successfully in a variety of settings, outperforming all other methods on image and video (Dhariwal & Nichol, 2021;Ho et al., 2022).",
    "curr": "These successes raise hope for building powerful models for graph generation, a task with diverse applications such as molecule design (Liu et al., 2018), traffic modeling (Yu & Gu, 2019), and code completion (Brockschmidt et al., 2019).",
    "next": "However, generating graphs remains challenging due to their unordered nature and sparsity properties."
  },
  {
    "index": 1005,
    "source_corpus_id": 235446639,
    "ref_id": "b19",
    "citation_corpus_id": 62841605,
    "start": 8329,
    "end": 8355,
    "prev": "Learning-based algorithms Over the last few years, there has been a growing interest in using machine learning techniques to improve the performance of \"classical\" algorithms.",
    "curr": "This methodology found applications in similarity search (Wang et al., 2016;Sablayrolles et al., 2019;Dong et al., 2020), graph optimization (Khalil et al., 2017;Balcan et al., 2018), data structures (Kraska et al., 2018;Mitzenmacher, 2018), online algorithms (Lykouris & Vassilvitskii, 2018;Purohit et al., 2018), compressed sensing (Mousavi et al., 2015;Baldassarre et al., 2016;Bora et al., 2017) and streaming algorithms (Hsu et al., 2019;Jiang et al., 2019).",
    "next": "The last two papers are closest to our work, as they solve various computational problems over data streams, including distinct elements estimation in Jiang et al."
  },
  {
    "index": 1008,
    "source_corpus_id": 257636846,
    "ref_id": "b24",
    "citation_corpus_id": 229923720,
    "start": 31899,
    "end": 31918,
    "prev": "Being able to write SDM as an MLP with minor modifications is interesting in light of SDM's connection to Transformer Attention (Bricken & Pehlevan, 2021).",
    "curr": "This link converges with work showing that Transformer MLP layers perform associative memory-like operations that approximate Top-K by showing up to 90% activation sparsity in later layers (Geva et al., 2020;Sukhbaatar et al., 2019;Nelson et al., 2022).",
    "next": "Viewing both Attention and MLPs through the lens of SDM presents their tradeoffs: Attention operates on patterns in the model's current receptive field."
  },
  {
    "index": 1009,
    "source_corpus_id": 235829401,
    "ref_id": "b5",
    "citation_corpus_id": 168169888,
    "start": 14293,
    "end": 14311,
    "prev": "Again, CLIP-ViT-B variant leads to dramatically worse performance compared to other visual modules, that we will discuss in Section 5.",
    "curr": "Vision-and-Language Navigation\n\nVision-and-language navigation tests the agent's ability to take action according to human instructions, which recently gains popularity in embodied AI (Anderson et al., 2018b;Chen et al., 2019;Jain et al., 2019;Chen et al., 2019;Qi et al., 2020b;Krantz et al., 2020;Nguyen and Daumé III, 2019;.",
    "next": "Specifically, the agent is put at a location in the environment (Chang et al., 2017) and asked to reach a target by following the language instructions."
  },
  {
    "index": 1010,
    "source_corpus_id": 247518670,
    "ref_id": "b7",
    "citation_corpus_id": 2808403,
    "start": 3736,
    "end": 3761,
    "prev": "First, we can encode a known PDE into a loss function that encourages the model to fulfill the equation (Raissi et al., 2019).",
    "curr": "Another way to go about this is to derive the model structure itself from known laws such as the convection-diffusion equation (de Bézenac et al., 2018).",
    "next": "We will follow the second approach."
  },
  {
    "index": 1011,
    "source_corpus_id": 253264964,
    "ref_id": "b28",
    "citation_corpus_id": 1428702,
    "start": 21021,
    "end": 21040,
    "prev": "COMPARISON WITH BASELINES ON DIFFERENT BENCHMARKS\n\nSettings.",
    "curr": "In this section, we evaluate our MPCFormer framework with different approximations and compare it with baselines on the IMDb dataset and the GLUE benchmark (Maas et al., 2011;Wang et al., 2018).",
    "next": "For all experiments in this section, we use BERT BASE as the base model."
  },
  {
    "index": 1012,
    "source_corpus_id": 21731691,
    "ref_id": "b9",
    "citation_corpus_id": 2134321,
    "start": 5640,
    "end": 5643,
    "prev": "In the deep learning literature, low-rank factorization of convolutional filters has been previously used to remove redundancy in trained CNNs [6,17].",
    "curr": "The compression of deep networks has also been studied in [1,10,11], SqueezeNet [14], etc., where the low-rank factorization of filters can be utilized.",
    "next": "MobileNets [13] used depth-wise separable convolutions to obtain significant compression."
  },
  {
    "index": 1013,
    "source_corpus_id": 254823652,
    "ref_id": "b48",
    "citation_corpus_id": 233306870,
    "start": 4363,
    "end": 4380,
    "prev": "With increasing attention on this issue, researchers have been probing the generalisation performance of ML models by creating datasets that feature distribution shift tasks (Koh et al., 2021;Gulrajani and Lopez-Paz, 2020;Shah et al., 2020) and proposing algorithms that aim to improve generalisation performance under distribution shift (Ganin et al., 2016;Arjovsky et al., 2019;Sun and Saenko, 2016; Figure 1: Synthetic vs. realistic distribution shift: The distribution shift in synthetic datasets (left, MNIST-CIFAR and CdSprites) are usually extreme and controllable (adjusted via changing the correlation); for realistic datasets (right, WILDS-Camelyon17 and FMoW) distribution shift can be subtle, hard to identify and impossible to control.",
    "curr": "Sagawa et al., 2020;Shi et al., 2022).",
    "next": "In this work, we identify three specific problems with current approaches in distribution shift problems, in computer vision, and develop a suite of experiments to address them."
  },
  {
    "index": 1016,
    "source_corpus_id": 246822636,
    "ref_id": "b23",
    "citation_corpus_id": 8394195,
    "start": 1895,
    "end": 1914,
    "prev": "With the development of deep learning, learned methods have led to several breakthroughs in this task.",
    "curr": "Currently, the state-of-the-art (SOTA) deep image compression models are built on the auto-encoder framework [Hinton and Salakhutdinov, 2006] with an entropy-constrained bottleneck [Theis et al., 2017, Ballé et al., 2017, Ballé et al., 2018, Mentzer et al., 2018, Minnen et al., 2018a, Lee et al., 2019, Guo et al., 2021].",
    "next": "An entropy model estimates the conditional probability distribution of latents for compression by standard entropy coding algorithms."
  },
  {
    "index": 1017,
    "source_corpus_id": 246652106,
    "ref_id": "b16",
    "citation_corpus_id": 3144218,
    "start": 3286,
    "end": 3308,
    "prev": "As shown in Figure 1 (b), a noise edge means the connection between two faces of different classes.",
    "curr": "Unlike common graph datasets such as Citeseer, Cora and Pubmed with explicit link relation as edges (Kipf & Welling, 2017), face images do not contain explicit structural information, but only deep features extracted from a trained CNN model.",
    "next": "Therefore, face images are treated as vertices, and the edges between face images are usually constructed based on the kNN (Cover & Hart, 1967) relations when building the graph: Each face serves as a probe to retrieve its k nearest neighbours by deep features (Wang et al., 2019b;Guo et al., 2020;Shen et al., 2021)."
  },
  {
    "index": 1018,
    "source_corpus_id": 252596252,
    "ref_id": "b33",
    "citation_corpus_id": 227209335,
    "start": 1587,
    "end": 1607,
    "prev": "INTRODUCTION\n\nDiffusion models learn the implicit prior of the underlying data distribution by matching the gradient of the log density (i.e.",
    "curr": "Stein score; ∇ x log p(x)) (Song et al., 2021b).",
    "next": "The prior can be leveraged when solving inverse problems, which aim to recover x from the measurement y, related through the forward measurement operator A and the detector noise n. When we know such forward models, one can incorporate the gradient of the log likelihood (i.e."
  },
  {
    "index": 1019,
    "source_corpus_id": 198986015,
    "ref_id": "b33",
    "citation_corpus_id": 5273326,
    "start": 19465,
    "end": 19469,
    "prev": "The conditional distribution p θ (z c k | z m k ) is parameterised by a multilayer perceptron (MLP) with two hidden layers, 256 units per layer, and ELUs (Clevert et al.",
    "curr": "[34]).",
    "next": "We use the same component VAE featuring a spatial broadcast decoder as MONet to encode and decode z c k , but we replace ReLUs (Glorot et al."
  },
  {
    "index": 1022,
    "source_corpus_id": 239049848,
    "ref_id": "b43",
    "citation_corpus_id": 3626819,
    "start": 67576,
    "end": 67596,
    "prev": "Small weights final layer.Following Andrychowicz et al.",
    "curr": "(2020), we consider using smaller weights for the final layers of the actor and the critic.Specifically, we downscale these weights by a factor of 100 at initialization time.We refer to this strategy as scale down.Gradient clipping.Another stabilizing strategy is gradient clipping (Zhang et al., 2020).To avoid occasional exploding gradients, one simply clips the norm of gradients that are above some threshold.This strategy is popular in NLP (Gehring et al., 2017;Peters et al., 2018) and is sometimes used in RL (Raffin et al., 2019).We consider clipping the gradient when the norm, calculated independently for the actor, critic, and convolutional encoder, is larger than 1 or 10.We refer to these strategies as grad clip 1 and grad clip 10.",
    "next": "Results."
  },
  {
    "index": 1023,
    "source_corpus_id": 263831010,
    "ref_id": "b31",
    "citation_corpus_id": 3144218,
    "start": 24639,
    "end": 24661,
    "prev": "( 2022) is included in Graphium to reduce the cost of hyper-parameter tuning.More details are given in Appendix E.7.The library supports CPU, GPU and IPU [ §E.8] hardware to accelerate training.Further library optimisations are detailed in Appendix E.9.",
    "curr": "EXPERIMENTS ON BASELINE MODELS\n\nTo demonstrate the capabilities of the Graphium library in a multi-task setting with thousands of labels, a set of standard baselines were run with simple hyperparameter sweeps using 3 popular GNNs, namely GCN (Kipf & Welling, 2017), GIN (Xu et al., 2019), and GINE (Hu et al., 2020a).",
    "next": "A basic hyper-parameter sweep was conducted for each model and multiple random seeds used for initialisation to provide a performance baseline for future experiments to be evaluated against."
  },
  {
    "index": 1024,
    "source_corpus_id": 3609219,
    "ref_id": "b12",
    "citation_corpus_id": 11243593,
    "start": 7843,
    "end": 7864,
    "prev": "AMN does not use strong supervision but can solve tasks that require transitive logic by modeling sentence walks on the fly.",
    "curr": "EntNet constructs dynamic networks based on entities with tied weights for each entity (Henaff et al., 2017).",
    "next": "A key-value update system allows it to update relevant (learned) entities."
  },
  {
    "index": 1026,
    "source_corpus_id": 259095535,
    "ref_id": "b14",
    "citation_corpus_id": 174799399,
    "start": 51070,
    "end": 51089,
    "prev": "• Affine maps W len , W num : R → Rd and W node : R 2 → Rd.",
    "curr": "• A simple stack of five Transformer blocks with eight heads in the self-attention and layer normalization before the MHA and the FF (Wang et al., 2019).",
    "next": "The structure of a Transformer block is summarized in Figure 4."
  },
  {
    "index": 1027,
    "source_corpus_id": 253224188,
    "ref_id": "b0",
    "citation_corpus_id": 182953134,
    "start": 4058,
    "end": 4076,
    "prev": "With the proposed metric, we show that coresets selected by SOTA methods at high pruning rates have much worse data coverage than random pruning, suggesting a linkage between poor data coverage of SOTA methods and poor accuracy at high pruning rates.",
    "curr": "We note that data coverage has also been studied in active learning setting (Ash et al., 2019;Citovsky et al., 2021), but techniques from active learning do not trivially extend to one-shot coreset selection.",
    "next": "We discuss the similarity and differences in Section 5."
  },
  {
    "index": 1030,
    "source_corpus_id": 252595765,
    "ref_id": "b0",
    "citation_corpus_id": 30535508,
    "start": 11529,
    "end": 11550,
    "prev": "It has also be applied for feature selection (Zhao et al., 2015;Li et al., 2016;Scardapane et al., 2017;Lemhadri et al., 2021).",
    "curr": "While the LASSO is the most widely-used method for relaxing the ℓ 0 sparsity constraint in feature selection, several recent works have proposed new relaxations based on stochastic gates (Srinivas et al., 2017;Louizos et al., 2018;Balın et al., 2019;Trelin & Procházka, 2020;Yamada et al., 2020).",
    "next": "This approach introduces (learnable) Bernoulli random variables for each feature during training, and minimizes the expected loss over realizations of the 0-1 variables (accepting or rejecting features)."
  },
  {
    "index": 1031,
    "source_corpus_id": 58006571,
    "ref_id": "b40",
    "citation_corpus_id": 21946795,
    "start": 4326,
    "end": 4346,
    "prev": "For datasets that are relatively simple and have low intrinsic dimensions (MNIST, Fashion MNIST, etc), we can obtain enough training examples to make sure adversarial training covers most part of the data distribution.",
    "curr": "For high dimensional datasets (CIFAR, ImageNet), adversarial training have been shown difficult (Kurakin et al., 2016;Tramèr et al., 2018) and only limited success was obtained.",
    "next": "A recent attack proposed by  shows that adversarial training can be defeated when the input image is produced by a generative model (for example, a generative adversarial network) rather than selected directly from the test examples."
  },
  {
    "index": 1032,
    "source_corpus_id": 3463636,
    "ref_id": "b5",
    "citation_corpus_id": 6706414,
    "start": 2117,
    "end": 2141,
    "prev": "* indicates equal contributions\n\nINTRODUCTION\n\nDeep neural networks (DNNs) have demonstrated their outstanding performance in different domains, ranging from image processing (Krizhevsky et al., 2012;He et al., 2016), text analysis (Collobert & Weston, 2008) to speech recognition .",
    "curr": "Though deep networks have exhibited high performance for these tasks, recently they have been shown to be particularly vulnerable to adversarial perturbations added to the input images (Szegedy et al., 2013;Goodfellow et al., 2015).",
    "next": "These perturbed instances are called adversarial examples, which can lead to undesirable consequences in many practical applications based on DNNs."
  },
  {
    "index": 1033,
    "source_corpus_id": 254198961,
    "ref_id": "b5",
    "citation_corpus_id": 53115163,
    "start": 9077,
    "end": 9096,
    "prev": "This is motivated by applications in robotics and Embodied Intelligence where the state is not directly observable, but can be well-approximated through the combination: s = (x, q), where x denotes stacked RGB images observed by the agent's camera, and q denotes proprioceptive sensory information, e.g., the joint pose of a robot.",
    "curr": "Furthermore, shaped reward functions can be hard to script for real-world applications (Singh et al., 2019) or result in undesirable artifacts or behaviors (Amodei et al., 2016;Burda et al., 2019).",
    "next": "Thus, we desire to learn with simple sparse rewards that accurately capture task completion."
  },
  {
    "index": 1035,
    "source_corpus_id": 67855499,
    "ref_id": "b11",
    "citation_corpus_id": 3529936,
    "start": 2439,
    "end": 2458,
    "prev": "Traditional VI methods approximate the true posterior with oversimplified distribution families like factorized Gaussians, which can severely limit the approximation quality and induce pathologies such as over-pruning (Trippe & Turner, 2018).",
    "curr": "These limitations have motivated the recent development of implicit VI methods (Li & Turner, 2018;Shi et al., 2018b), which allow the use of flexible approximate distributions without a tractable density.",
    "next": "However, most of the implicit inference methods require to learn a \"generator network\" that maps a simple distribution to approximate the target posterior."
  },
  {
    "index": 1037,
    "source_corpus_id": 257219472,
    "ref_id": "b10",
    "citation_corpus_id": 52967399,
    "start": 2257,
    "end": 2278,
    "prev": "When applied to various LLMs, including GPT-3, our approach significantly improves the generated discourse compared to the base LLMs and other techniques in terms of both the overall language and detoxification performance.",
    "curr": "INTRODUCTION\n\nLarge-scale Transformer-based (Vaswani et al., 2017) language models (LMs) have shown tremendous progress and grown in importance across various NLP downstream tasks, often providing stateof-the-art performances over the last few years (Devlin et al., 2019;Yang et al., 2019;Raffel et al., 2020;Peters et al., 2018).",
    "next": "Despite their progress in learning linguistic knowledge, these models have been shown to capture and reproduce toxicity in the ever-larger pretraining datasets."
  },
  {
    "index": 1038,
    "source_corpus_id": 258180544,
    "ref_id": "b9",
    "citation_corpus_id": 49659296,
    "start": 26679,
    "end": 26707,
    "prev": "An important remark on the properties of our presented game is due.",
    "curr": "Existing literature tackles settings with (i) (weak-)monotonocity (Mertikopoulos et al., 2019;Diakonikolas et al., 2021), (ii) cocoercivity (Zhu and Marcotte, 1996), (iii) zero-duality gap (Von Neumann, 1928), (iv) unconstrained solution space (Golowich et al., 2020) .",
    "next": "Our game is carefully crafted and -although it has a distinct structure and is nonconvex-nonconcave only due to multilinearity-satisfies none of the latter properties."
  },
  {
    "index": 1040,
    "source_corpus_id": 51559,
    "ref_id": "b27",
    "citation_corpus_id": 1952530,
    "start": 1928,
    "end": 1947,
    "prev": "Recurrent layers can also be stacked, increasing network depth, representational power and often accuracy.",
    "curr": "RNN applications in the natural language domain range from sentence classification (Wang et al., 2015) to word-and character-level language modeling (Zaremba et al., 2014).",
    "next": "RNNs are also commonly the basic building block for more complex models for tasks such as machine translation (Bahdanau et al., 2015;Luong et al., 2015; or question answering (Kumar et al., 2016;."
  },
  {
    "index": 1042,
    "source_corpus_id": 245668925,
    "ref_id": "b18",
    "citation_corpus_id": 15986631,
    "start": 3620,
    "end": 3638,
    "prev": "On the other hand, since rewards are sparse, the agent only gets a few high-scoring trajectories to learn from, requiring vigorous exploitation in order to get back to the furthest point of the game and make progress thereon.",
    "curr": "Prior approaches to solving these games (He et al., 2016a;Guo et al., 2020) usually employ a single policy and action selection strategy, making it difficult to strike the right balance between exploration and exploitation.",
    "next": "In this paper, we propose eXploit-Then-eXplore (XTX), an algorithm for multi-stage control to explicitly decompose the exploitation and exploration phases within each episode."
  },
  {
    "index": 1046,
    "source_corpus_id": 251066616,
    "ref_id": "b9",
    "citation_corpus_id": 8768364,
    "start": 53746,
    "end": 53765,
    "prev": "B.6 LIMITATIONS AND FUTURE WORK\n\nThe proposed work focuses mainly on a mixture density network (MDN) as the generative channel model, which allows us to exploit some of their useful properties in our formulation.",
    "curr": "Generalizing the proposed few-shot domain adaptation to other types of generative channel models such as conditional GANs, VAEs, and normalizing flows (Dinh et al., 2017) could be an interesting direction.",
    "next": "These generative models can handle more high-dimensional structured inputs."
  },
  {
    "index": 1053,
    "source_corpus_id": 211082795,
    "ref_id": "b21",
    "citation_corpus_id": 604334,
    "start": 1673,
    "end": 1695,
    "prev": "∀x .",
    "curr": "x−x p ≤ =⇒F (x)=F (x )(1)\nThis problem carries practical significance, as such networks have been extensively shown to be vulnerable to adversarial examples (Szegedy et al., 2014;Papernot et al., 2016), wherein small-norm perturbations are chosen to cause aribtrary misclassifications.",
    "next": "Numerous solutions have been proposed to address variants of this problem."
  },
  {
    "index": 1054,
    "source_corpus_id": 233306870,
    "ref_id": "b6",
    "citation_corpus_id": 68222714,
    "start": 3462,
    "end": 3475,
    "prev": "Consider, for example, a model that is built to distinguish between cows and camels using photos collected in nature under different climates.",
    "curr": "Since CNNs are known to have a bias towards texture (Geirhos et al., 2018, Brendel andBethge, 2019), if we simply try to minimize the average loss across different domains, the classifier is prone to spuriously correlate \"cow\" with grass and \"camels\" with desert, and predict the species using only the background.",
    "next": "Such a classifier can be rendered useless when the animals are placed indoors or in a zoo."
  },
  {
    "index": 1055,
    "source_corpus_id": 252683303,
    "ref_id": "b16",
    "citation_corpus_id": 231632658,
    "start": 3133,
    "end": 3151,
    "prev": "This work studies example selection in chain-of-thoughts multi-step reasoning.",
    "curr": "Example selection is a central problem in the prompting literature (Liu et al., 2022;Rubin et al., 2022;Su et al., 2022;Lazaridou et al., 2022).",
    "next": "It asks what instances make the best prompts for solving the tasks of interest."
  },
  {
    "index": 1057,
    "source_corpus_id": 248965297,
    "ref_id": "b13",
    "citation_corpus_id": 52967399,
    "start": 2456,
    "end": 2477,
    "prev": "In ACL, 2021.",
    "curr": "INTRODUCTION\n\nLarge-scale pre-trained language models (PLMs) have achieved great success in promoting state of the art on various natural language understanding and generation tasks (Devlin et al., 2019;Radford et al., 2019;Yang et al., 2019;Brown et al., 2020;.",
    "next": "PLM self-supervision training largely benefits from harvesting local context information in the pre-training corpus."
  },
  {
    "index": 1059,
    "source_corpus_id": 255440465,
    "ref_id": "b14",
    "citation_corpus_id": 225039882,
    "start": 1724,
    "end": 1728,
    "prev": "Introduction\n\nThe transformer architecture [50] has become an important and highly influential model family, due to its simplicity, scalability, and its wide range of applications.",
    "curr": "While originally stemming from the domain of natural language processing (NLP), with the advent of the Vision transformer (ViT) [15], this has become a standard architecture in computer vision, setting various state-of-the-art (SoTA) performances on tasks ranging from representation learning, semantic segmentation, object detection and video understanding [4,5,18,30,31].",
    "next": "However, the original formulation of the transformer includes a quadratic computational complexity with respect to the number of input tokens."
  },
  {
    "index": 1061,
    "source_corpus_id": 226221794,
    "ref_id": "b29",
    "citation_corpus_id": 221878944,
    "start": 9619,
    "end": 9636,
    "prev": "Also, we consider a larger family of distribution shifts including covariate shifts.",
    "curr": "The above two categories are not exhaustivee.g., there are some recent works that characterize how some inductive biases favor extrapolation [Xu et al., 2021] and can be better for OOD generalization.",
    "next": "Sample Complexity of Invariant Risk Minimization\n\n\nInvariant Risk Minimization\n\nWe start with some background on IRM [Arjovsky et al., 2019]."
  },
  {
    "index": 1062,
    "source_corpus_id": 259298560,
    "ref_id": "b29",
    "citation_corpus_id": 3488815,
    "start": 12433,
    "end": 12453,
    "prev": "We also consider Voneresnet (biological V1 front-end (Dapello et al., 2020)) and ResNet-50 trained as a base network for action recognition (Chen et al., 2021) but include these as separate examples in the Appendix since they use a modified architecture.",
    "curr": "MODELS TRAINED FOR ROBUSTNESS\n\nWe consider two forms of models trained for minimizing a classification loss L ce in the face of input perturbations δ ∈ R h×w×c subject to constraints on the overall magnitude of perturbations in the input space, where x, y, θ are the network input, output, and classifier parameters, respectively:\nL ce (θ, x + δ, y)(1)\nIn adversarially trained networks, projected gradient descent from the output space finds maximal directions of perturbation in the input space limited to length ϵ, and training entails minimizing the effect of these perturbation directions on the network's output (Madry et al., 2018).",
    "next": "In random smoothing (Lecuyer et al., 2018;, a supervised network is trained but in the face of Gaussian noise added to the input space as the base classifier before performing a probabilistic inference."
  },
  {
    "index": 1068,
    "source_corpus_id": 235614244,
    "ref_id": "b4",
    "citation_corpus_id": 11212020,
    "start": 23811,
    "end": 23834,
    "prev": "ntic segmentation is a saturated task, and most contemporary published works have approximate performances, Hamburger shows considerable improvements over previous state-of-the-art attention modules.",
    "curr": "Method mIoU(%) PSPNet (Zhao et al., 2017) 82.6 DFN * (Yu et al., 2018) 82.7 EncNet  82.9 DANet * (Fu et al., 2019) 82.6 DMNet * (He et al., 2019a) 84.4 APCNet * (He et al., 2019b) 84.2 CFNet * (Zhang et al., 2019b) 84.2 SpyGR *  84.2 SANet * (Zhong et al., 2020) 83.2 OCR * (Yuan et al., 2020) 84.3 HamNet 85.9 Method mIoU(%) PSPNet (Zhao et al., 2017) 47.8 SGR * (Liang et al., 2018) 50.8 EncNet  51.7 DANet * (Fu et al., 2019) 52.6 EMANet * (Li et al., 2019a) 53.1 DMNet * (He et al., 2019a) 54.4 APCNet * (He et al., 2019b) 54.7 CFNet * (Zhang et al., 2019b) 54.0 SpyGR *  52.8 SANet * (Zhong et al., 2020) 53.0 OCR * (Yuan et al., 2020) 54.8 HamNet 55.2 \n\n\nIMAGE GENERATION\n\n\nRELATED WORK\n\nGeneral Survey for Attention The last five years have witnessed a roaring success of attention mechanisms (Bahdanau et al., 2015;Mnih et al., 2014;Xu et al., 2015;Luong et al., 2015) in deep learning.",
    "next": "Roughly speaking, the attention mechanism is a term of adaptively generating the targets' weights to be attended according to the requests."
  },
  {
    "index": 1069,
    "source_corpus_id": 222132819,
    "ref_id": "b12",
    "citation_corpus_id": 202565422,
    "start": 40425,
    "end": 40443,
    "prev": "(c) Role action spaces and role policy structure.",
    "curr": "Figure 2 :\n2Nachum et al., 2018b;a; Levy et al., 2018; Ghosh et al., 2018; Dwiel  et al., 2019;Nair & Finn, 2019;Nasiriany et al., 2019; Dilokthanakul et al., 2019)  or by discovering reusable skills(Daniel et al., 2012; Gregor et al., 2016;Warde-Farley et al., 2018;Shankar & Gupta, 2020;Thomas et al., 2018;Sharma et al., 2020).",
    "next": "In multi-agent settings, many challenges, such as efficient communication(Ossenkopf et al., 2019) and labor division(Wang et al., 2020c) in large systems, necessitate hierarchical learning along the second dimension -over agents(Zhang et al., 2010).Ahilan & Dayan (2019) propose a hierarchy in FeUdal(Dayan & Hinton, 1993;Vezhnevets et al., 2017) style for cooperative multi-agent tasks, where managers and workers are predefined and workers are expected to achieve the goal generated by managers."
  },
  {
    "index": 1074,
    "source_corpus_id": 253080708,
    "ref_id": "b23",
    "citation_corpus_id": 3144218,
    "start": 2175,
    "end": 2199,
    "prev": "GNNs broadly follow a message-passing framework, meaning that each layer of the GNN aggregates the representations of a node and its neighbors, and transforms these features into a new representation for that node.",
    "curr": "The aggregation function used by the GNN layer is taken to be locally permutationinvariant, since the ordering of the neighbors of a node is arbitrary, and its specific form is a key component of the GNN architecture; varying it gives rise to several common GNN variants (Kipf and Welling, 2017;Veličković et al., 2018;Li et al., 2015;Hamilton et al., 2017;Xu et al., 2019).",
    "next": "The output of a GNN can be used for tasks such as graph classification or node classification."
  },
  {
    "index": 1075,
    "source_corpus_id": 53113561,
    "ref_id": "b17",
    "citation_corpus_id": 1957433,
    "start": 6778,
    "end": 6802,
    "prev": "BASIC DESIGN OF MACHINE COMPREHENSION MODELS\n\nFor single-turn MC, many top-performing models share a similar architecture, consisting of four major components: (1) question encoding, (2) context encoding, (3) reasoning, and finally (4) answer prediction.",
    "curr": "Initially the word embeddings (e.g., Pennington et al., 2014;Peters et al., 2018) of question tokens Q and context tokens C are taken as input and fed into contextual integration layers, such as LSTMs (Hochreiter & Schmidhuber, 1997) or self attentions (Yu et al., 2018), to encode the question and context.",
    "next": "Multiple integration layers provide contextualized representations of context, and are often inter-weaved with attention, which inject question information."
  },
  {
    "index": 1077,
    "source_corpus_id": 229923128,
    "ref_id": "b18",
    "citation_corpus_id": 8822680,
    "start": 2025,
    "end": 2048,
    "prev": "INTRODUCTION\n\nWhen translating a word, translation models need to spend a substantial amount of its capacity in disambiguating its sense in the source language and choose a lexeme in the target language which adequately express its meaning (Choi et al., 2017;Tamchyna, 2017).",
    "curr": "However, neural machine translation (NMT) has a severe problem on lexical choice, since it usually has mistranslation errors on low-frequency words (Koehn & Knowles, 2017;Nguyen & Chiang, 2018).",
    "next": "Table 1: All samples that contain the source word \"纽 马 基特\" in raw and distilled training corpora, which are different in target sides (RAW-TGT vs. KD-TGT)."
  },
  {
    "index": 1078,
    "source_corpus_id": 238856778,
    "ref_id": "b30",
    "citation_corpus_id": 1957433,
    "start": 19469,
    "end": 19494,
    "prev": "(Even without having ever seen a white wolf, a typical English speaker can guess that a white wolf is more likely to resemble an arctic fox than a snorkel.)",
    "curr": "These kinds of relations are often captured by embeddings of class labels (or more detailed class descriptions) (Pennington et al., 2014).",
    "next": "When available, this kind of information about class semantics can be used to construct an improved subspace regularizer by encouraging new class representations to lie close to a convex combination of base classes weighted by their semantic similarity."
  },
  {
    "index": 1081,
    "source_corpus_id": 260351224,
    "ref_id": "b4",
    "citation_corpus_id": 29153681,
    "start": 11096,
    "end": 11121,
    "prev": "Bilevel optimization.",
    "curr": "Bilevel optimization is a powerful tool to study many machine learning applications such as hyperparameter optimization (Franceschi et al., 2018;Shaban et al., 2019), meta-learning (Bertinetto et al., 2018;Franceschi et al., 2018;Rajeswaran et al., 2019;Ji et al., 2020;, neural architecture search (Liu et al., 2018;Zhang et al., 2021a), etc.",
    "next": "Existing approaches are usually approximate implicit differentiation (AID) based (Domke, 2012;Pedregosa, 2016;Gould et al., 2016;Liao et al., 2018;Lorraine et al., 2020), or iterative differentiation (ITD) based (Domke, 2012;Maclaurin et al., 2015;Franceschi et al., 2017;Finn et al., 2017;Shaban et al., 2019;Rajeswaran et al., 2019;Liu et al., 2020)."
  },
  {
    "index": 1083,
    "source_corpus_id": 256663850,
    "ref_id": "b4",
    "citation_corpus_id": 229376913,
    "start": 8174,
    "end": 8191,
    "prev": "To distinguish it from our RLA, we rename the former as RLA g in the following.",
    "curr": "RealFormer (He et al., 2021) and EA-Transformer (Wang et al., 2021) both added attention scores in the previous layer to the current one, connecting the layers by residual attention.",
    "next": "Bapna et al."
  },
  {
    "index": 1086,
    "source_corpus_id": 212414027,
    "ref_id": "b6",
    "citation_corpus_id": 53464644,
    "start": 24902,
    "end": 24925,
    "prev": "In contrast to our work, these methods operate over small molecular graphs and were not applied to large macromolecules, like proteins.",
    "curr": "In parallel, recent work proposes that generative models pre-trained on protein sequences can transfer knowledge to downstream supervised tasks (Bepler & Berger, 2019;Alley et al., 2019;Yang et al., 2019;Rives et al., 2019).",
    "next": "These methods have also been explored for protein design (Wang et al., 2018)."
  },
  {
    "index": 1087,
    "source_corpus_id": 259203325,
    "ref_id": "b3",
    "citation_corpus_id": 199371687,
    "start": 66589,
    "end": 66610,
    "prev": "For our target X set, Jayanthi et al.",
    "curr": "(2020) provide 40k pairs of real mistakes by humans taken from the BEA grammar correction corpus (Bryant et al., 2019), which we use as our ideal target X.\n\n\nC.4.2 GIO\n\nWe first generate embeddings for the train and target sets using MPNet-Base-V2 (Song et al., 2020) on the input side of the data; see Appendix C.2 for an example, we use the same process as in our WMT experiments.",
    "next": "On an AWS p3dn.24xlarge machine with 8 NVIDIA Tesla V100 GPUs, it takes about 4 hours to generate the embeddings for the 15M data."
  },
  {
    "index": 1088,
    "source_corpus_id": 222140947,
    "ref_id": "b17",
    "citation_corpus_id": 4009713,
    "start": 2340,
    "end": 2362,
    "prev": "Several early methods propose pretext tasks that explore the inherent structures within a single image.",
    "curr": "For example, by identifying spatial arrangement (Doersch et al., 2015), orientation (Gidaris et al., 2018), or chromatic channels , models learn useful representations for downstream tasks.",
    "next": "Recently, another line of works Hjelm et al., 2018;Misra & van der Maaten, 2020;Chen et al., 2020a), e.g."
  },
  {
    "index": 1089,
    "source_corpus_id": 235485093,
    "ref_id": "b14",
    "citation_corpus_id": 212657453,
    "start": 37146,
    "end": 37164,
    "prev": "Another promising application of PDOs is an extension to manifolds.",
    "curr": "Gauge CNNs (Cohen et al., 2019c;Kicanaoglu et al., 2019;Haan et al., 2021;) are a rather general framework for convolutions on manifolds; see Weiler et al.",
    "next": "(2021) for a thorough treatment and literature review."
  },
  {
    "index": 1091,
    "source_corpus_id": 259138847,
    "ref_id": "b8",
    "citation_corpus_id": 247958465,
    "start": 5999,
    "end": 6002,
    "prev": "Moreover, unlike classical sparse methods where such a connection is established via explicit sparse regularization [8], the sparsity observed in Transformers is emergent without any explicit design.",
    "curr": "It is worth noting that the observation that Transformers produce sparse activations is previously reported in [9].",
    "next": "Our paper significantly extends upon results in [9] to demonstrate that sparsity emerges prevalently at all layers of Transformers, for both language and vision tasks, on both training and evaluation data, and for some architectures beyond Transformers."
  },
  {
    "index": 1092,
    "source_corpus_id": 236087421,
    "ref_id": "b34",
    "citation_corpus_id": 14124313,
    "start": 5101,
    "end": 5129,
    "prev": "In the MLP-Mixer , the model obtains the global receptive field through matrix transposition and token-mixing projection such that the long-range dependencies are covered.",
    "curr": "However, this rarely makes full use of the local information, which is very important in CNN-like architecture (Simonyan & Zisserman, 2015;He et al., 2016) because not all pixels need long-range dependencies, and the Therefore, a more ideal way to introduce locality is to directly model the relationship between a feature point and its surrounding feature points at any position, without the need to set a fixed window (and window size) in advance.",
    "next": "To aggregate the features of different spatial positions in the same position and model their relationships, inspired by (Wu et al., 2018;Lin et al., 2019;Wang et al., 2020;Ho et al., 2019), we propose an axial shift strategy for MLP-based architecture, where we spatially shift features in both horizontal and vertical directions."
  },
  {
    "index": 1094,
    "source_corpus_id": 256662465,
    "ref_id": "b37",
    "citation_corpus_id": 220363897,
    "start": 2927,
    "end": 2945,
    "prev": "If appropriate equivariance constraints are instead learned, the benefits of equivariance could extend to applications where the data may have unknown or imperfect symmetries.",
    "curr": "Learning approximate equivariance has been recently approached through novel layer operations (Wang et al., 2022;Finzi et al., 2021;Zhou et al., 2020;Yeh et al., 2022;Basu et al., 2021).",
    "next": "Separately, the field of neural architecture search (NAS) aims to optimize full neural network architectures (Zoph & Le, 2017;Real et al., 2017;Elsken et al., 2017;Liu et al., 2018;Lu et al., 2019)."
  },
  {
    "index": 1096,
    "source_corpus_id": 250048824,
    "ref_id": "b6",
    "citation_corpus_id": 240354066,
    "start": 2154,
    "end": 2157,
    "prev": "* Equal contribution.",
    "curr": "Introduction\n\nThe Structured State Space model (S4) is a recent deep learning model based on continuous-time dynamical systems that has shown promise on a wide variety of sequence modeling tasks [7].",
    "next": "It is defined as a linear time-invariant (LTI) state space model (SSM), which give it multiple properties [6]: as an SSM, S4 can be simulated as a discrete-time recurrence for efficiency in online or autoregressive settings, and as a LTI model, S4 can be converted into a convolution for parallelizability and computational efficiency at training time."
  },
  {
    "index": 1097,
    "source_corpus_id": 225103395,
    "ref_id": "b35",
    "citation_corpus_id": 2723173,
    "start": 6570,
    "end": 6590,
    "prev": "Further study of accuracy and error sets have been conducted in (Hacohen & Weinshall, 2020) (error sets over training), and (Hooker et al., 2019) (error after pruning).",
    "curr": "Other work has demonstrated that it is often possible for narrower or shallower neural networks to attain similar accuracy to larger networks when the smaller networks are trained to mimic the larger networks' predictions (Ba & Caruana, 2014;Romero et al., 2015).",
    "next": "We instead seek to study the impact of width and depth on network internal representations and (per-example) outputs, by applying techniques for measuring similarity of neural network hidden representations (Kornblith et al., 2019;Raghu et al., 2017a;Morcos et al., 2018)."
  },
  {
    "index": 1100,
    "source_corpus_id": 36483539,
    "ref_id": "b37",
    "citation_corpus_id": 12713052,
    "start": 2560,
    "end": 2577,
    "prev": "(2017)) is a class of approaches to reduce the size of Deep Neural Networks (DNNs) and accelerate inference.",
    "curr": "Meanwhile, Structure Learning (Zoph & Le (2017), Philipp & Carbonell (2017), Cortes et al.",
    "next": "(2017)) becomes an active research area for DNN structure exploration, which can potentially replace human labor with machine automation for design space exploration."
  },
  {
    "index": 1101,
    "source_corpus_id": 256662499,
    "ref_id": "b55",
    "citation_corpus_id": 13751870,
    "start": 2617,
    "end": 2629,
    "prev": "INTRODUCTION\n\nTraining of deep learning models utilizes randomness to improve generalization and training efficiency, thus causing an inherent nondeterminism that hampers the reliability of machine learning evaluation -the consistency of the measurement of evaluation scores across replicated training runs.Gundersen et al.",
    "curr": "(2022) list several sources of nondeterminism, e.g., implementation-level nondeterminism such as random ordering in floating-point accumulation in parallel GPU threads (Pham et al., 2021), algorithmic factors such as variations in meta-parameters and model architecture (Lucic et al., 2018;Henderson et al., 2018;D'Amour et al., 2020), or data-level factors such as variations in pre-processing and evaluation metrics (Post, 2018;Chen et al., 2022) or varying characteristics of data in different splits (Gorman & Bedrick, 2019;Søgaard et al., 2021).Zhuang et al.",
    "next": "(2022) show that implementation-level nondeterminism is partly irreducible, leading to variability in evaluation scores even for training runs on identical data, algorithmic settings and infrastructure.Furthermore, they point out strong effects of certain types of algorithm-level nondeterminism on certain subsets of the data."
  },
  {
    "index": 1102,
    "source_corpus_id": 263333942,
    "ref_id": "b10",
    "citation_corpus_id": 13046179,
    "start": 2171,
    "end": 2197,
    "prev": "INTRODUCTION\n\nIn deep neural networks, out-of-distribution (OOD) detection distinguishes samples which deviate from the training distribution.Standard OOD detection concerns semantic shifts (Yang et al., 2022;Zhang et al., 2023), where OOD data is defined as test samples from semantic categories unseen during training.Ideally, the neural network should be able to reject such samples as being OOD, while still maintaining strong performance on in-distribution (ID) test samples belonging to seen training categories.",
    "curr": "Methods for detecting OOD samples work by scoring network outputs such as logits or softmax values (Hendrycks & Gimpel, 2017;Hendrycks et al., 2022), post-hoc network adjustment during inference to improve OOD scoring (Sun & Li, 2022;Sun et al., 2021;Djurisic et al., 2023), or by adjusting model training (Wei et al., 2022;Ming et al., 2023;DeVries & Taylor, 2018).These approaches can be used either independently or in conjunction with one another.Typically, post-hoc adjustments together with OOD scoring is the preferred combination since it is highly effective at discerning OOD samples with minimal ID drop and can also be applied directly to already-trained models off-the-shelf.Examples include ReAct (Sun et al., 2021), DICE (Sun & Li, 2022) and more recently, ASH (Djurisic et al., 2023).",
    "next": "On the surface, each method takes different and sometimes even contradictory approaches.ReAct rectifies penultimate activations which exceed a threshold; ASH, on the other hand, prunes penultimate activations that are too low while amplifying remaining activations.While ASH currently achieves state-of-the-art performance,"
  },
  {
    "index": 1103,
    "source_corpus_id": 249712234,
    "ref_id": "b33",
    "citation_corpus_id": 6212000,
    "start": 6056,
    "end": 6076,
    "prev": "Introduction\n\nA central challenge of machine learning theory is understanding the generalization of overparameterized models.",
    "curr": "While in many real-world settings deep networks achieve low test loss, their high capacity makes theoretical analysis with classical tools difficult, or sometimes impossible (Zhang et al., 2017;Nagarajan and Kolter, 2019b).",
    "next": "Most classical theoretical tools are based on uniform convergence (UC), a property that, when it holds, guarantees that the test loss will be close to the training loss, uniformly over a class of candidate models."
  },
  {
    "index": 1104,
    "source_corpus_id": 67749672,
    "ref_id": "b46",
    "citation_corpus_id": 14687186,
    "start": 3842,
    "end": 3863,
    "prev": "One feature shared by many humandesigned representation systems is compositionality: the capacity to represent complex concepts (from objects to procedures to beliefs) by combining simple parts (Fodor & Lepore, 2002).",
    "curr": "While many machine learning approaches make use of human-designed compositional analyses for representation and prediction (Socher et al., 2013;Dong & Lapata, 2016), it is also natural to ask whether (and how) compositionality arises in learning problems where compositional structure has not been built in from the start.",
    "next": "Consider the example in Figure 1, which shows a hypothetical character-based encoding scheme learned for a simple communication task (similar to the one studied by Lazaridou et al., 2016)."
  },
  {
    "index": 1105,
    "source_corpus_id": 85501306,
    "ref_id": "b19",
    "citation_corpus_id": 6628106,
    "start": 20679,
    "end": 20698,
    "prev": "In each training batch, 16 low-quality (LQ) patches with the size of 48 × 48 are extracted as inputs.",
    "curr": "Our model is trained by ADAM optimizer (Kingma & Ba, 2014) with β 1 = 0.9, β 2 = 0.999, and = 10 −8 .",
    "next": "The initial learning rate is set to 10 −4 and then decreases to half every 2 × 10 5 iterations of back-propagation."
  },
  {
    "index": 1107,
    "source_corpus_id": 29153681,
    "ref_id": "b14",
    "citation_corpus_id": 3431470,
    "start": 8924,
    "end": 8945,
    "prev": "Despite its simplicity, this general strategy is particularly effective and it is at the core of several stateof-the-art few-shot classification algorithms (Vinyals et al., 2016;Snell et al., 2017;Sung et al., 2018).",
    "curr": "Interestingly, Garcia & Bruna (2018) interpret learning as information propagation from support (training) to query (test) images and propose a graph neural network that can generalize matching-based approaches.",
    "next": "Since this line of work relies on learning a similarity metric, one distinctive characteristic is that parameter updates only occur within the long time horizon of the outer training loop."
  },
  {
    "index": 1109,
    "source_corpus_id": 232307426,
    "ref_id": "b19",
    "citation_corpus_id": 52877454,
    "start": 1868,
    "end": 1893,
    "prev": "Self-supervised learning, a subset of unsupervised learning, learns representations by allowing the data to provide supervision (Devlin et al., 2018).",
    "curr": "Among its mainstream strategies, self-supervised contrastive learning has been successful in visual object recognition (He et al., 2020;Tian et al., 2019;Chen et al., 2020c), speech recognition (Oord et al., 2018;Rivière et al., 2020), language modeling (Kong et al., 2019), graph representation learning (Velickovic et al., 2019) and reinforcement learning (Kipf et al., 2019).",
    "next": "The idea of self-supervised contrastive learning is to learn latent representations such that related instances (e.g., patches from the same image; defined as positive pairs) will have representations within close distance, while unrelated instances (e.g., patches from two different images; defined as negative pairs) will have distant representations (Arora et al., 2019)."
  },
  {
    "index": 1110,
    "source_corpus_id": 244908617,
    "ref_id": "b12",
    "citation_corpus_id": 56657912,
    "start": 25448,
    "end": 25478,
    "prev": "First, we learn pixel classifiers on the clean images using the DDPM, SwAV and MAE representations on the Bedroom-28 and Horse-21 datasets.",
    "curr": "Then, 18 diverse corruption types, adopted from (Hendrycks & Dietterich, 2019), are applied to test images.",
    "next": "Each corruption has five levels of severity."
  },
  {
    "index": 1112,
    "source_corpus_id": 238531695,
    "ref_id": "b23",
    "citation_corpus_id": 52967399,
    "start": 2372,
    "end": 2392,
    "prev": " hand, CEL blends each embedding with multiple patches of different scales, providing the self-attention module itself with cross-scale features.On the other hand, LSDA splits the self-attention module into a short-distance one and a long-distance counterpart, which not only reduces the computational burden but also keeps both small-scale and large-scale features in the embeddings.Through the above two designs, we achieve cross-scale attention.Besides, we put forward a dynamic position bias for vision transformers to make the popular relative position bias apply to variable-sized images.Hinging on the cross-scale attention module, we construct a versatile vision architecture, dubbed CrossFormer, which accommodates variable-sized inputs.Extensive experiments show that CrossFormer outperforms the other vision transformers on image classification, object detection, instance segmentation, and semantic segmentation tasks.",
    "curr": "1\n\nINTRODUCTION\n\nIt turns out that transformer (Vaswani et al., 2017;Devlin et al., 2019;Brown et al., 2020) has achieved great success in the field of natural language processing (NLP).Benefitting from its selfattention module, transformer is born with the key ability to build long-distance dependencies.Since long-distance dependencies are also needed by a number of vision tasks (Zhang & Yang, 2021;Chu et al., 2021), a surge of research work (Dosovitskiy et al., 2021;Touvron et al., 2021;Wang et al., 2021) has been conducted to explore various transformer-based vision architectures.",
    "next": "A transformer requires a sequence of embeddings2 (e.g., word embeddings) as input.To adapt this requirement to typical vision tasks, most existing vision transformers (Dosovitskiy et al., 2021;Touvron et al., 2021;Wang et al., 2021;Liu et al., 2021b) produce embeddings by splitting an input image into equal-sized patches.For example, a 224 × 224 image can be split into 56 × 56 patches of size 4 × 4, and these patches are projected through a linear layer to yield an embedding sequence.Inside a"
  },
  {
    "index": 1113,
    "source_corpus_id": 263830433,
    "ref_id": "b15",
    "citation_corpus_id": 233296808,
    "start": 4926,
    "end": 4947,
    "prev": "ext descriptions of the desired tasks that are often significantly easier to obtain compared to full model weights or even a generic linear classifier over the last layer.The versatility and performance of prompting Table 1: Comparison with existing state-of-the-art generalization bounds for test error on different datasets.We report both data-independent and data-dependent bounds (⋆ indicates data-dependent prior and − indicates that the bounds are not available).Note that different works use different architectures and analytic tools so direct comparison can be more nuanced.Nonetheless, our bounds on prompt engineering are significantly tighter than the existing PAC-Bayes bounds in the literature, often within a few percent of the actual test error.",
    "curr": "Dataset\n\nZhou pretrained models have led to the rise of prompt engineering, an emergent paradigm in machine learning where practitioners carefully design the task specification in text or even learn the prompts in a data-driven fashion (Lester et al., 2021).For example, to obtain a two-class image classifier, one would write two sentences that describe the classes (e.g., \"This is a dog\" and \"This is a cat\"), and the two sentences are turned into text embeddings which can be used to classify image embeddings.Despite its empirical success, little is understood of how and why prompting these pretrained models work and, in particular, why the method seems to suffer little from overfitting: manually tuning or even greedily optimizing prompts on a given training set often performs nearly as well on the corresponding test set.",
    "next": "In this paper, we demonstrate that rather simple analysis tools capture this behavior surprisingly well (under some assumptions).In particular, we show that classical PAC-Bayes bounds (McAllester, 1999), when applied to the discrete hypothesis class defined by prompts (and specifically with a prior given by a large language model), are often remarkably tight, even for large domains: for example, we achieve a generalizati"
  },
  {
    "index": 1115,
    "source_corpus_id": 3464416,
    "ref_id": "b12",
    "citation_corpus_id": 6628106,
    "start": 29143,
    "end": 29162,
    "prev": "Dropout was applied to only fully-connected layers of the network with the probability of retaining the unit being 0.5.",
    "curr": "For all experiments, the softmax classifier is used, and each model is trained by optimizing the objective function using Adam learning rule (Kingma & Ba, 2014).",
    "next": "For each out-ofdistribution dataset, we randomly select 1,000 images for tuning the penalty parameter β, mini-batch size and learning rate."
  },
  {
    "index": 1116,
    "source_corpus_id": 252693131,
    "ref_id": "b22",
    "citation_corpus_id": 209439843,
    "start": 2844,
    "end": 2866,
    "prev": "This type of compositionality is central to the human ability to generalize from limited data to novel combinations (Lake et al., 2017).",
    "curr": "Recently, several datasets have been proposed to test systematic generalization of machine learning models-SCAN (Lake & Baroni, 2018), PCFG (Hupkes et al., 2020), CFQ (Keysers et al., 2020), and HINT (Li et al., 2021), to name a few.",
    "next": "While conventional neural networks fail dramatically on these datasets, certain inductive biases have been explored to improve systematic generalization."
  },
  {
    "index": 1120,
    "source_corpus_id": 256390109,
    "ref_id": "b26",
    "citation_corpus_id": 247244493,
    "start": 2830,
    "end": 2848,
    "prev": "Simultaneously, several works have aimed to rigorously formalize why KD can improve the student model performance.",
    "curr": "Some prominent observations from this line of work are that (self-)distillation induces certain favorable optimization biases in the training objective [Phuong andLampert, 2019, Ji andZhu, 2020], lowers variance of the objective [Menon et al., 2021, Dao et al., 2021, Ren et al., 2022, increases regularization towards learning \"simpler\" functions [Mobahi et al., 2020], transfers information from different data views [Allen-Zhu and Li, 2020], and scales per-example gradients based on the teacher's confidence [Furlanello et al., 2018, Tang et al., 2020.",
    "next": "Despite this remarkable progress, there are still many open problems and unexplained phenomena around knowledge distillation; to name a few: -Why do soft labels (sometimes) help?"
  },
  {
    "index": 1121,
    "source_corpus_id": 221818900,
    "ref_id": "b24",
    "citation_corpus_id": 49882757,
    "start": 1897,
    "end": 1915,
    "prev": "In previous work, likelihood-based models, including autoregressive models (van den Oord et al., 2016;Kalchbrenner et al., 2018; and flow-based models Prenger et al., 2019;, have predominated in audio synthesis because of the simple training objective and superior ability of modeling the fine details of waveform in real data.",
    "curr": "There are other waveform models, which often require auxiliary losses for training, such as flow-based models trained by distillation Ping et al., 2019), variational auto-encoder (VAE) based model , and generative adversarial network (GAN) based models Yamamoto et al., 2020).",
    "next": "Most of previous waveform models focus on audio synthesis with informative local conditioner (e.g., mel spectrogram or aligned linguistic features), with only a few exceptions in unconditional generation ."
  },
  {
    "index": 1124,
    "source_corpus_id": 247996510,
    "ref_id": "b4",
    "citation_corpus_id": 235368289,
    "start": 4765,
    "end": 4786,
    "prev": "(2020) showed that SATNet (Wang et al., 2019) could not solve visual Sudoku without using intermediate labels to identify individual Sudoku digit images.",
    "curr": "Similar limitations were observed in language related compounded tasks, including commonsense reasoning (Liu et al., 2022;Wei et al., 2022;Zelikman et al., 2022), math word problems (Piękos et al., 2021;Wei et al., 2022), and programs execution (Nye et al., 2022).",
    "next": "The go-to architectures in this domain are powerful language models, which are trained as sequence-to-sequence models over text."
  },
  {
    "index": 1129,
    "source_corpus_id": 257757426,
    "ref_id": "b20",
    "citation_corpus_id": 238634325,
    "start": 7554,
    "end": 7578,
    "prev": "This setting is particularly attractive for applications where there is previous logged experience available but online data collection is expensive (e.g., robotics, healthcare).",
    "curr": "Recently, the field of offline RL has made significant progress, and many offline RL algorithms have been proposed to learn improved policies from diverse and sub-optimal offline demonstrations Fujimoto & Gu, 2021;Kostrikov et al., 2022c;.",
    "next": "Offline RL research typically assumes that the offline dataset is reward-annotated."
  },
  {
    "index": 1130,
    "source_corpus_id": 239769065,
    "ref_id": "b30",
    "citation_corpus_id": 3687922,
    "start": 2541,
    "end": 2544,
    "prev": "Some prior methods approach this problem by performing search or optimization over subgoals at test time.",
    "curr": "However, these test-time planning methods either rely on graph search [7,31], which scales poorly with dimensionality [14], or continuous optimization over subgoals [26], which is expensive and can result in model exploitation.",
    "next": "In this paper, we take a different tack and instead use search at training time to automatically generate a curriculum."
  },
  {
    "index": 1131,
    "source_corpus_id": 248810913,
    "ref_id": "b30",
    "citation_corpus_id": 108300988,
    "start": 2602,
    "end": 2624,
    "prev": "A large number of studies have been devoted towards interpreting DNNs.",
    "curr": "A major line of research work has focused on DNNs in interpreting deep Natural Language Processing (NLP) models and their ability to learn various pre-defined linguistic concepts (Tenney et al., 2019b;Liu et al., 2019a).",
    "next": "More specifically, they rely on pre-defined linguistic concepts such as: parts-of-speech tags and semantic tags, and probe whether the specific linguistic knowledge is learned in various parts of the network."
  },
  {
    "index": 1132,
    "source_corpus_id": 52881014,
    "ref_id": "b25",
    "citation_corpus_id": 10082291,
    "start": 14948,
    "end": 14975,
    "prev": "This method has shown promise in robotics (Finn et al., 2017b;Yu et al., 2018).",
    "curr": "In generative modeling, few-shot learning has been addressed from several perspectives, including matching networks (Bartunov and Vetrov, 2017) and variable inference for memory addressing (Bornschein et al., 2017).",
    "next": "Rezende et al."
  },
  {
    "index": 1133,
    "source_corpus_id": 1091965,
    "ref_id": "b22",
    "citation_corpus_id": 14124313,
    "start": 2130,
    "end": 2158,
    "prev": "(2012) developed AlexNet, an ImageNet-winning CNN with more than 1.1 × 10 9 multiplies.",
    "curr": "In 2014, ImageNetwinning and runner up CNNs increased the number of multiplies to 1.4 × 10 9 (Szegedy et al., 2015) and 1.6 × 10 10 (Simonyan & Zisserman, 2015) respectively.",
    "next": "Despite the powerful representational ability of large scale CNNs, their computational workload prohibits deployment on mobile devices."
  },
  {
    "index": 1134,
    "source_corpus_id": 218901036,
    "ref_id": "b9",
    "citation_corpus_id": 6706414,
    "start": 2472,
    "end": 2476,
    "prev": "Motivation and Contributions\n\nThe outputs of deep networks are known to be very sensitive to small perturbations to the input.",
    "curr": "This sensitivity can be exploited to create adversarial examples that undermine robustness by causing trained networks to produce defective results from input changes that are imperceptible to a human [10].",
    "next": "The adversarial scenarios studied in this paper are primarily untargeted white-box attacks on image classification networks."
  },
  {
    "index": 1135,
    "source_corpus_id": 237593074,
    "ref_id": "b10",
    "citation_corpus_id": 225039882,
    "start": 7314,
    "end": 7339,
    "prev": "Architecture We use an encoder-decoder architecture.",
    "curr": "The encoder can be a general image encoder that perceives pixels and encodes them into hidden representations, such as a ConvNet (LeCun et al., 1989;Krizhevsky et al., 2012;He et al., 2016), Transformer (Vaswani et al., 2017;Dosovitskiy et al., 2020), or their combination (Carion et al., 2020).",
    "next": "For generation we use a Transformer decoder, widely used in modern language modeling (Radford et al., 2018;Raffel et al., 2019)."
  },
  {
    "index": 1138,
    "source_corpus_id": 235358707,
    "ref_id": "b14",
    "citation_corpus_id": 4009713,
    "start": 1860,
    "end": 1881,
    "prev": "The source code is available at https://github.com/tsaishien-chen/IFND.",
    "curr": "INTRODUCTION\n\nSelf-supervised learning of visual representation (Doersch et al., 2015;Pathak et al., 2016;Noroozi & Favaro, 2016;Oord et al., 2018;Gidaris et al., 2018;Chen et al., 2020a) aims to learn a semanticaware embedding space based on the image data without the supervision of human-labeled annotations.",
    "next": "Recently, significant advances have been made by contrastive learning approaches (Hjelm et al., 2019;Tian et al., 2019;Chen et al., 2020b;He et al., 2020;Chen et al., 2020d) to reduce the performance gap with the supervised counterparts."
  },
  {
    "index": 1139,
    "source_corpus_id": 248377648,
    "ref_id": "b11",
    "citation_corpus_id": 28202810,
    "start": 33316,
    "end": 33340,
    "prev": "In our experiments we use expert policies to collect offline datasets for the pre-training tasks of standing, forward walking, and backward walking respectively.",
    "curr": "To collect these datasets, we pre-train polices with SAC (Haarnoja et al., 2018a) in the state space and collect rollouts of the visual observation.",
    "next": "We test our representation on the downstream task of \"running\"."
  },
  {
    "index": 1140,
    "source_corpus_id": 210932183,
    "ref_id": "b31",
    "citation_corpus_id": 12713052,
    "start": 1608,
    "end": 1625,
    "prev": "To showcase the framework, we compare several state-of-the-art one-shot NAS methods, examine how sensitive they are to their hyperparameters and how they can be improved by tuning their hyperparameters, and compare their performance to that of blackbox optimizers for NAS-Bench-101.",
    "curr": "INTRODUCTION\n\nWhile neural architecture search (NAS) has attracted a lot of attention due to the effectiveness in automatically designing state-of-the-art neural networks (Zoph & Le, 2017;Real et al., 2017;, the focus has recently shifted to making the search process more efficient (Pham et al., 2018;Elsken et al., 2019;Xie et al., 2019;Cai et al., 2019;Casale et al., 2019).",
    "next": "The most crucial concept which led to a reduction in search costs to the order of a single function evaluation is certainly the weight-sharing paradigm: Training only a single large architecture (the one-shot model) subsuming all the possible architectures in the search space (Brock et al., 2018;Pham et al., 2018)."
  },
  {
    "index": 1141,
    "source_corpus_id": 49667227,
    "ref_id": "b66",
    "citation_corpus_id": 22014305,
    "start": 41491,
    "end": 41494,
    "prev": "Finding such structure is important for generalization because it shields various parts of the representation or program from changing when one component changes.",
    "curr": "For example, [24,42,67] have begun to examine how to route through an fixed architecture to encapsulate computation, but the hard limitation that prevents fixed architectures from scaling to problems with complexity beyond that which they were trained is their limited number of computation steps.",
    "next": "A parallel line of work runs through the hierarchical reinforcement learning (HRL) literature [9], with recent work [8,27,50,58,84] attempting to learn both lower-level policies as well as a higher-level policy that calls them."
  },
  {
    "index": 1142,
    "source_corpus_id": 219792972,
    "ref_id": "b24",
    "citation_corpus_id": 3568073,
    "start": 7236,
    "end": 7240,
    "prev": "MSGAN (aka Miss-GAN) (23) aims to solve the missing mode problem of conditional GANs through a regularization term that maximizes the distance between the generated images with respect to the distance between their corresponding input latent codes.",
    "curr": "Progressive GANs (24) are growing both the generator and discriminator progressively, and approach resembling the layer-wise training of autoencoders.",
    "next": "Method\n\n\nPreliminaries\n\nWe define an N -way, K (tr) -shot supervised classification task, T , as a set D (tr) T composed of i ∈ {1, ."
  },
  {
    "index": 1146,
    "source_corpus_id": 258988005,
    "ref_id": "b11",
    "citation_corpus_id": 237605600,
    "start": 7790,
    "end": 7806,
    "prev": "However, we have been particularly motivated by the research on Langevin MCMC which is a class of gradientbased sampling algorithms obtained by discretizing the Langevin diffusion (Parisi, 1981).",
    "curr": "There is a growing body of work on the analysis of Langevin MCMC algorithms of various complexity (overdampled, Metropolis-adjusted, underdamped, higher-order) for sampling from log-concave distributions (Dalalyan, 2017;Durmus & Moulines, 2017;Cheng et al., 2018;Dwivedi et al., 2018;Shen & Lee, 2019;Cao et al., 2020;Mou et al., 2021;Li et al., 2022).",
    "next": "In our experiments, we compare several recent Langevin MCMC methods, which may be of independent interest."
  },
  {
    "index": 1151,
    "source_corpus_id": 227054483,
    "ref_id": "b6",
    "citation_corpus_id": 52967399,
    "start": 2962,
    "end": 2983,
    "prev": "In other fields of machine learning, the use of large prior datasets to bootstrap acquisition of new capabilities has been studied extensively to good effect.",
    "curr": "For example, language models trained on large, diverse datasets offer representations that drastically improve the efficiency of learning downstream tasks (Devlin et al., 2019).",
    "next": "What would be the analogue of this kind of pre-training in robotics and RL?"
  },
  {
    "index": 1153,
    "source_corpus_id": 235390683,
    "ref_id": "b27",
    "citation_corpus_id": 3292002,
    "start": 2577,
    "end": 2601,
    "prev": "Due to their prevalence and rich descriptive capacity, pattern mining and discovery on graph data is a prominent research area with powerful implications.",
    "curr": "As the generalization of deep neural networks on graph data, graph neural networks (GNNs) have proved to be powerful in learning representations for graphs and associated entities (nodes, edges, subgraphs), and they have been employed in various applications such as node classification (Kipf & Welling, 2016a;Veličković et al., 2018), node clustering (Pan et al., 2018), recommender systems (Ying et al., 2018) and drug discovery (Duvenaud et al., 2015).",
    "next": "In recent years, the explosive interest in self-supervised learning (SSL) has suggested its great potential in empowering stronger neural networks in an unsupervised manner Kolesnikov et al., 2019;Doersch et al., 2015)."
  },
  {
    "index": 1154,
    "source_corpus_id": 244773230,
    "ref_id": "b17",
    "citation_corpus_id": 204206524,
    "start": 35508,
    "end": 35534,
    "prev": "In the future, we would like to extend this work to multi-task settings, employ vision and move towards the context of batch RL, e.g.",
    "curr": "through employing distance learning techniques (Hartikainen et al., 2020) to build on more informative goal spaces.",
    "next": "We measure the performance of the learnt representations to encode progress by running a KNN classification."
  },
  {
    "index": 1158,
    "source_corpus_id": 54445324,
    "ref_id": "b3",
    "citation_corpus_id": 3503217,
    "start": 25596,
    "end": 25621,
    "prev": "More relevant to our framework are approaches that parameterize the training process through a recurrent neural network that takes the gradient as input and produces a new set of parameters (Ravi & Larochelle, 2016;Santoro et al., 2016;Andrychowicz et al., 2016;Hochreiter et al., 2001).",
    "curr": "The approach most closely related to us learns an initialization such that the model can adapt to a new task through one or a few gradient updates (Finn et al., 2017;Nichol et al., 2018;Al-Shedivat et al., 2017;Lee & Choi, 2018).",
    "next": "In contrast to our work, these methods focus exclusively on few-shot learning, where the gradient path is trivial as only a single or a handful of training steps are allowed, limiting them to settings where the current task is closely related to previous ones."
  },
  {
    "index": 1160,
    "source_corpus_id": 3290366,
    "ref_id": "b1",
    "citation_corpus_id": 3495200,
    "start": 3105,
    "end": 3108,
    "prev": "Examples include prediction of properties of chemical molecules [9], answering questions about knowledge graphs [25], natural language processing with parsestructured inputs (trees or richer structures like Abstract Meaning Representations) [4], predicting properties of data structures or source code in programming * Work done partially while author was at Microsoft Research.",
    "curr": "languages [2,22], and making predictions from scene graphs [39].",
    "next": "Sequence data can be seen as a special case of a simple chain-structured graph."
  },
  {
    "index": 1161,
    "source_corpus_id": 254926905,
    "ref_id": "b2",
    "citation_corpus_id": 17682909,
    "start": 2924,
    "end": 2926,
    "prev": "Can we quantitatively model the effect of applying a finite number of graph convolutions and theoretically predict the \"sweet spot\" for the choice of depth?In this paper, we propose a non-asymptotic analysis framework to study the effects of graph convolutions and oversmoothing using the Contextual Stochastic Block Model (CSBM)[18].",
    "curr": "The CSBM mimics the community structure\n\nIntroduction\n\nGraph Neural Networks (GNNs) are a powerful framework for learning with graph-structured data and have shown great promise in diverse domains such as molecular chemistry, physics, and social network analysis [1,2,3,4,5,6,7].",
    "next": "Most GNN models are built by stacking graph convolutions or message-passing layers [8], where the representation of each node is computed by recursively aggregating and transforming the representations of its neighboring nodes."
  },
  {
    "index": 1162,
    "source_corpus_id": 235613482,
    "ref_id": "b21",
    "citation_corpus_id": 214220671,
    "start": 10830,
    "end": 10848,
    "prev": "(2019) proposed a Logic Embedding Network (LEN) with distracting features, which also uses the auxiliary annotations to boost the performance and designed a teacher model to control the learning trajectories.",
    "curr": "MXGNet (Wang et al., 2020) is a multi-layer graph neural network for multi-panel diagrammatic reasoning tasks.",
    "next": "For better performance, MXGNet also uses auxiliary annotations for model training."
  },
  {
    "index": 1164,
    "source_corpus_id": 247922261,
    "ref_id": "b18",
    "citation_corpus_id": 8348149,
    "start": 2513,
    "end": 2534,
    "prev": "For this we draw inspiration from overcomplete representations: representations of an input that are nonunique combinations of a number of basis vectors greater than the input's dimensionality (Lewicki & Sejnowski, 2000).",
    "curr": "Mostly studied in the context of the sparse coding literature (Gregor & LeCun, 2010;Goodfellow et al., 2012;Olshausen, 2013), sparse overcomplete representations have been shown to increase stability in the presence of noise (Donoho et al., 2006), have applications in neuroscience (Olshausen & Field, 1996;Lee et al., 2007), and lead to more interpretable representations (Murphy et al., 2012;Fyshe et al., 2015;Faruqui et al., 2015).",
    "next": "However, the choice of basis vectors is generally assumed to be learned using traditional methods such as ICA (Teh et al., 2003) or fitting linear models (Lewicki & Sejnowski, 2000), limiting the expressive power of the encoding function."
  },
  {
    "index": 1165,
    "source_corpus_id": 222133257,
    "ref_id": "b4",
    "citation_corpus_id": 5590763,
    "start": 6162,
    "end": 6165,
    "prev": "By unrolling this algorithm, the authors demonstrate superior recovery over ALISTA for a specific setting of M, N and s.\n\nIn a related approach [20] identify undershooting, meaning that reconstructed components are smaller than target components, as a shortcoming of LISTA and propose Gated-LISTA to address these issues.",
    "curr": "The authors introduce gain and overshoot gates to LISTA, which can amplify the reconstruction after each iteration before and after thresholding, yielding an architecture resembling GRU cells [5].",
    "next": "The authors demonstrate better sparse reconstruction than previous LISTA-variants and also show that adding their proposed gates to ALISTA, named AGLISTA, it is possible to improve its performance in the same setting of M, N and s as ALISTA-AT."
  },
  {
    "index": 1166,
    "source_corpus_id": 231698498,
    "ref_id": "b58",
    "citation_corpus_id": 202888885,
    "start": 2681,
    "end": 2700,
    "prev": "INTRODUCTION\n\nThe record-breaking performance of modern deep neural networks (DNNs) comes at a prohibitive training cost due to the required massive training data and parameters, limiting the development of the highly demanded DNN-powered intelligent solutions for numerous applications Wu et al., 2018).",
    "curr": "As an illustration, training ResNet-50 involves 10 18 FLOPs (floating-point operations) and can take 14 days on one state-of-the-art (SOTA) GPU (You et al., 2020b).",
    "next": "Meanwhile, the large DNN training costs have raised increasing financial and environmental concerns."
  },
  {
    "index": 1168,
    "source_corpus_id": 259298217,
    "ref_id": "b42",
    "citation_corpus_id": 246652474,
    "start": 34913,
    "end": 34929,
    "prev": "CLASSIFICATION\n\nDatasets.",
    "curr": "DG-15 (Xu et al., 2022) is a synthetic binary classification dataset with 15 groups.",
    "next": "Each group contains 100 data points."
  },
  {
    "index": 1170,
    "source_corpus_id": 231719359,
    "ref_id": "b29",
    "citation_corpus_id": 52900371,
    "start": 3378,
    "end": 3391,
    "prev": " this SDE analogy assumes that each minibatch is randomly sampled from the full dataset, which implies that some examples will be sampled multiple times in one epoch.",
    "curr": "Furthermore, the most common SDE analogy holds only for vanishing learning rates (Yaida, 2019) and therefore misses the generalization benefits of finite learning rates which we identify in this work.",
    "next": "An important exception is Li et al."
  },
  {
    "index": 1172,
    "source_corpus_id": 257496038,
    "ref_id": "b11",
    "citation_corpus_id": 208547755,
    "start": 3566,
    "end": 3586,
    "prev": "Predictions of the reward r t , discount factor γ t , and next latent state z t+1 are computed based on h t using MLPs.",
    "curr": "o t− z t− ô t− a t− h t− r t− · · · · · ·\n1 arXiv:2303.07109v1 [cs.LG] 13 Mar 2023\n\nPublished as a conference paper at ICLR 2023 A promising model-based concept is learning in imagination (Ha & Schmidhuber, 2018;Kaiser et al., 2020;Hafner et al., 2020;Hafner et al., 2021): instead of learning behaviors from the collected experience directly, a generative model of the environment dynamics is learned in a (self-)supervised manner.",
    "next": "Such a so-called world model can create new trajectories by iteratively predicting the next state and reward."
  },
  {
    "index": 1176,
    "source_corpus_id": 52917627,
    "ref_id": "b6",
    "citation_corpus_id": 5763832,
    "start": 6756,
    "end": 6777,
    "prev": "RELATED WORK\n\nDifferentiable physics simulators.",
    "curr": "Researchers have developed many differentiable physics simulators (Ehrhardt et al., 2017;Degrave et al., 2016).",
    "next": "In particular, Battaglia et al."
  },
  {
    "index": 1178,
    "source_corpus_id": 239616181,
    "ref_id": "b31",
    "citation_corpus_id": 210064473,
    "start": 5294,
    "end": 5312,
    "prev": "(4) While achieving better performance, the proposed model is much simpler than previous approaches.",
    "curr": "PRELIMINARIES\n\n\nOBJECT-CENTRIC REPRESENTATION LEARNING WITH PIXEL-MIXTURE DECODER\n\nA common framework for learning object-centric representations is via a form of auto-encoders (Locatello et al., 2020;Lin et al., 2020b).",
    "next": "In this framework, an encoder takes an input image to return a set of object representation vectors or slot vectors {s 1 , ."
  },
  {
    "index": 1179,
    "source_corpus_id": 252668432,
    "ref_id": "b11",
    "citation_corpus_id": 229297705,
    "start": 5683,
    "end": 5699,
    "prev": "In contrast to the so-called kernel regime where the implicit bias is described by the Neural Tangent Kernel (Jacot et al., 2018), there are several active regimes (also called rich or feature-learning regimes), whose implicit bias often feature a form sparsity that is absent from the kernel regime.",
    "curr": "Such active regimes have been observed for example in DNNs with small initialization (Chizat & Bach, 2018;Rotskoff & Vanden-Eijnden, 2018;Li et al., 2020;Jacot et al., 2022a), with L 2regularization (Savarese et al., 2019;Ongie et al., 2020;Jacot et al., 2022b) or when trained on exponentially decaying losses (Gunasekar et al., 2018a;b;Soudry et al., 2018;Du et al., 2018;Ji & Telgarsky, 2018;Chizat & Bach, 2020;Ji & Telgarsky, 2020).",
    "next": "In the latter two cases, the implicit bias is described by the representation cost:\nR(f ) = min W:f W =f W 2\nwhere f is a function that can be represented by the network and the minimization is over all parameters W that result in a network function f W equal to f , the parameters W form a vector and W is the L 2 -norm."
  },
  {
    "index": 1180,
    "source_corpus_id": 53332150,
    "ref_id": "b20",
    "citation_corpus_id": 6628106,
    "start": 2376,
    "end": 2395,
    "prev": "In comparison, the optimization routines used to train deep nets have arguable changed only little.",
    "curr": "Comparably simple first-order methods like SGD (Robbins & Monro, 1951), its momentum variants (MOMENTUM) (Polyak, 1964;Nesterov, 1983) and ADAM (Kingma & Ba, 2015) remain standards (Goodfellow et al., 2016;Karpathy, 2017).",
    "next": "The low practical relevance of more advanced optimization methods is not for lack of research, though."
  },
  {
    "index": 1186,
    "source_corpus_id": 232013402,
    "ref_id": "b21",
    "citation_corpus_id": 3144218,
    "start": 2403,
    "end": 2425,
    "prev": "INTRODUCTION\n\nGraph neural networks (GNNs) (Zhou et al., 2018;Wu et al., 2019), which work with graph structured data, have recently attracted considerable attention, as they can learn expressive representations for various graph-related tasks such as node classification, link prediction, and graph classification.",
    "curr": "While the majority of the existing works on GNNs focus on the message passing strategies for neighborhood aggregation (Kipf & Welling, 2017;Hamilton et al., 2017), which aims to encode the nodes in a graph accurately, graph pooling Ying et al., 2018) that maps the set of nodes into a compact representation is crucial in capturing a meaningful structure of an entire graph.",
    "next": "As a simplest approach for graph pooling, we can average or sum all node features in the given graph (Atwood & Towsley, 2016;Xu et al., 2019) (Figure 1 (B))."
  },
  {
    "index": 1187,
    "source_corpus_id": 264288929,
    "ref_id": "b2",
    "citation_corpus_id": 53115163,
    "start": 5411,
    "end": 5430,
    "prev": "23b]'s K 6 /7 regret through a computationally efficient algorithm.Still, there remain significant gaps between the current upper bounds and the √ K lower bound.In this work, we push the frontiers both on the information theoretical limits and the achievable bounds under computational constraints: 1) we present the first (computationally inefficient) algorithm that provably obtains O( √ K) regret, showing that this is the minimax K dependence (Section 3); 2) we obtain O(K 3 /4 ) regret with a polynomial-time algorithm (Section 4).Below, we briefly describe the elements in our approaches.",
    "curr": "Inefficient\n\n√ K algorithm.We convert the linear MDP problem to a linear bandit problem by mapping each policy to a single dH-dimensional feature vector, where d is the ambient dimension of the linear MDP and H K 3 /4 in this paper.Beyond theoretical advancement, exploration in policy optimization has also showcased its potential in addressing real-world challenges, as evidenced by empirical studies [Burda et al., 2018, Pan et al., 2019].",
    "next": "Preliminaries\n\nNo-Regret Learning in MDPs.An (episodic) MDP is specified by a tuple M = (S, A, P ) where S is the state space (possibly infinite), A is the action space (assumed to be finite with size A = |A|), P : S × A → ∆(S) is the transition kernal.The state space is assumed to be layered, i.e., S = S 1 ∪ S 2 ∪ • • • ∪ S H where S h ∩ S h ′ = ∅ for any 1 ≤ h < h ′ ≤ H, and transition is only possible from one layer to the next, that is, P (s ′ | s, a) = 0 only when s ∈ S h and s ′ ∈ S h+1 .Without loss of generality, we assume S 1 = {s 1 }."
  },
  {
    "index": 1189,
    "source_corpus_id": 264436566,
    "ref_id": "b51",
    "citation_corpus_id": 222140788,
    "start": 27408,
    "end": 27412,
    "prev": "Dataset.We collect meshes in 4 categories (T-shirt, Top, Skirts, Trousers) from the Cloth3D dataset [4] and regroup them into two new categories: upper garments (including T-shirt and Top) and lower garments (including Skirts and Trousers).For both MeshDiffusion (using the DMTet representation) G-MeshDiffusion (using the G-SHELL representation), we run inverse rendering on meshes with known environment lightmaps and known materials using RGB, binary mask, and depth supervision.We generally follow the same settings of [35] for G-MeshDiffusion.For GET3D, we follow the same training setting as [11] and render multiview RGB images for training.",
    "curr": "Evaluation metrics.For each model, we sample a set of meshes, with the size of the test sets, using 100 steps of DDIM [52] and apply standard Laplacian smoothing to these meshes.Similar to [11,35], we evaluate point cloud metrics between the point clouds sampled from generated meshes and those from ground truth meshes.To compensate the lack of perceptual measure in the point cloud metrics, we also evaluate the generated results with multiview FID (MV-FID) [35,63], which is computed by an average of FID (Fréchet Inception Distance) scores of 20 views (rendered with fixed light sources and a diffuse-only mesh material).During rendering, we do not re-orient the face normals towards the camera so that the difference between watertight and open surfaces can be taken into account.",
    "next": "Qualitative and quantitative results.The quantitative results are given in Table 4.We observe that G-MeshDiffusion generally achieves better performance than the watertight mesh generation methods (MeshDiffusion and GET3D), but more importantly, G-MeshDiffusion can better capture the single-sided nature of non-watertight meshes as it"
  },
  {
    "index": 1191,
    "source_corpus_id": 231986303,
    "ref_id": "b23",
    "citation_corpus_id": 3509777,
    "start": 5768,
    "end": 5771,
    "prev": "For example, it has been shown that it is possible to set the weights of a deep ReLU network such that the number of linear regions computed by the network grows exponentially in the depth [7,9,20,26,27].",
    "curr": "Other works consider the degree of polynomials approximable by networks of different depths [19,24] and the topological invariants of networks [5].",
    "next": "While such work has sometimes been used to explain the utility of different neural network architectures (especially deeper ones), a second strand of prior work has shown that a significant gap can exist between the functions expressible by a given architecture and those which may be learned in practice."
  },
  {
    "index": 1192,
    "source_corpus_id": 259937490,
    "ref_id": "b9",
    "citation_corpus_id": 244130146,
    "start": 3443,
    "end": 3447,
    "prev": "notes some forward measurement operator (can be linear or nonlinear) and η ∈ R m is additive noise.Usually, we are interested in the case when m < n, which follows many real-world scenarios.When m < n, the problem is ill-posed and some kind of regularizer (or prior) is necessary to obtain a meaningful solution.",
    "curr": "In the literature, the traditional approach of using hand-crafted priors (e.g.sparsity) is slowly being replaced by rich, learned priors such as deep generative models.Recently, there has been a lot of interests in using diffusion models as structural priors due to their state-of-the-art performance in image generation [7][8][9].Compared to GANs, diffusion models are generally easier and more stable to train as they do not rely on an adversarial training scheme, making them a generative prior that is more readily accessible [7].The most common approach for using diffusion models as priors is to resort to posterior sampling, which has been extensively explored in the literature [10][11][12][13][14][15][16][17].However, despite their remarkable success, these techniques exhibit several limitations.The primary challenge is that the majority of existing works that employ diffusion models as priors train these models directly in the pixel space, which requires substantial computational resources and a large volume of training data.In real-world applications such as computed tomography (CT) and magnetic resonance imaging (MRI) reconstruction, where images are inherently either 3D or even 4D [18], training diffusion models directly in the pixel space is often infeasible.",
    "next": "CT Reconstruction\n\nLatent diffusion models (LDMs), which embed data in order to operate in a lower-dimensional space, present a potential solution to this challenge, along with considerable improvements in computational efficiency [19,20].They also can provide a great amount of flexibility, as they can enable one to transfer and generalize these models to different domains by fine-tuning on small amo"
  },
  {
    "index": 1193,
    "source_corpus_id": 252873172,
    "ref_id": "b8",
    "citation_corpus_id": 238744187,
    "start": 2727,
    "end": 2744,
    "prev": "Unlike image classification, object detection is more challenging and has a larger gap than the pre-training tasks.",
    "curr": "Several methods, such as RegionCLIP (Zhong et al., 2022), ViLD (Gu et al., 2021), and Detic , have been suggested to transfer knowledge from pre-trained CLIP  model to some modules of object detectors.",
    "next": "However, real images and annotations are still required for some key modules of the object detectors, such as RPNs or RoI heads."
  },
  {
    "index": 1195,
    "source_corpus_id": 258557287,
    "ref_id": "b22",
    "citation_corpus_id": 227209335,
    "start": 11292,
    "end": 11311,
    "prev": "BACKGROUND\n\nIn this section, we first review diffusion models in Section 3.1 and we discuss how they are used for solving inverse problems in Section 3.2.",
    "curr": "DENOISING DIFFUSION MODELS\n\nDiffusion models (Sohl-Dickstein et al., 2015;Ho et al., 2020;Song et al., 2021b) consist of two processes: a forward process that gradually adds noise to input images and a reverse process that learns to generate images by iterative denoising.",
    "next": "Formally the forward process can be expressed by the variance preserving stochastic differential equation (VP-SDE) (Song et al., 2021b) dx \" 1 2 βptqxdt`aβptqdw for t P r0, T s where βptq :\" β min`p β max´βmin q t T rescales the time variable, and dw is the standard Wiener process."
  },
  {
    "index": 1196,
    "source_corpus_id": 3463660,
    "ref_id": "b6",
    "citation_corpus_id": 5590763,
    "start": 23522,
    "end": 23540,
    "prev": "We then employ a meta controller called interpreter in an iterative manner.",
    "curr": "For the ith interpretation step, the interpreter computes the word attention as:\nτ˚$ ' ' ' ' ' & ' ' ' ' ' % Word attention: o i l 9 exp \" S cos pp i´1 , w l q ‰ Attended context: w i \" ÿ l o i l w l Attended word: s i \" ÿ l o i l w l Interpreter state: p i \" GRUpp i´1 , w i q(6)\nwhere S cos is cosine similarity and GRU is the gated recurrent unit (Cho et al., 2014).",
    "next": "Here we use τ˚to represent an approximation of τ via soft word attention."
  },
  {
    "index": 1197,
    "source_corpus_id": 211296452,
    "ref_id": "b20",
    "citation_corpus_id": 6018348,
    "start": 2313,
    "end": 2331,
    "prev": "\", one can identify the entity Grateful Dead and the path of relations LeadSinger, BirthDate to efficiently extract the answer-provided that this information is present in the KB.",
    "curr": "Unfortunately, KBs are often incomplete (Min et al., 2013).",
    "next": "While relation extraction methods can be used to populate KBs, this process is inherently error-prone, expensive and slow."
  }
]